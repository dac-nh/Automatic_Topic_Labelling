ID|Title|Summary
1|Testing MapReduce-Based Systems |Abstract. MapReduce (MR) is the most popular solution to build applications for large-scale data processing. These applications are often deployed on large clusters of commodity machines, where failures happen constantly due to bugs, hardware problems, and outages. Testing MR-based systems is hard, since it is needed a great effort of test harness to execute distributed test cases upon failures. In this paper, we present a novel testing solution to tackle this issue called HadoopTest. This solution is based on a scalable harness approach, where distributed tester components are hung around each map and reduce worker (i.e., node). Testers are allowed to stimulate each worker to inject failures on them, monitor their behavior, and validate testing results. HadoopTest was used to test two applications bundled into Hadoop, the Apache open source MapReduce implementation. Our initial implementation demonstrates promising results, with HadoopTest coordinating test cases across distributed MapReduce workers, and finding bugs.
2|MapReduce: Simplified Data Processing on Large Clusters|MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.  
3|ASM: A code manipulation tool to implement adaptable systems|ABSTRACT. ASM is a Java class manipulation tool designed to dynamically generate and manipulate Java classes, which are useful techniques to implement adaptable systems. ASM is based on a new approach, compared to equivalent existing tools, which consists in using the &amp;quot;visitor&amp;quot; design pattern without explicitly representing the visited tree with objects. This new approach gives much better performances than those of existing tools, for most of practical needs. RÉSUMÉ. ASM est un outil de manipulation de classes Java conçu pour la génération et la manipulation dynamiques de code, qui sont des techniques très utiles pour la réalisation de systèmes adaptables. ASM est basé sur une approche originale, par rapport aux outils existants équivalents, qui consiste à utiliser le patron de conception « visiteur » sans représenter explicitement l’arborescence visitée sous forme d’objets. Cette nouvelle approche permet d’obtenir des performances bien supérieures à celles des outils existants, pour la plupart des besoins courants.
4|Boom analytics: exploring data-centric, declarative programming for the cloud|Building and debugging distributed software remains extremely difficult. We conjecture that by adopting a datacentric approach to system design and by employing declarative programming languages, a broad range of distributed software can be recast naturally in a data-parallel programming model. Our hope is that this model can significantly raise the level of abstraction for programmers, improving code simplicity, speed of development, ease of software evolution, and program correctness. This paper presents our experience with an initial largescale experiment in this direction. First, we used the Overlog language to implement a “Big Data ” analytics stack that is API-compatible with Hadoop and HDFS and provides comparable performance. Second, we extended the system with complex distributed features not yet available in Hadoop, including high availability, scalability, and unique monitoring and debugging facilities. We present both quantitative and anecdotal results from our experience, providing some concrete evidence that both data-centric design and declarative languages can substantially simplify distributed systems programming.
5|Test Oracles|All software testing methods depend on the availability of an oracle, that is,  some method for checking whether the system under test has behaved correctly  on a particular execution. An ideal oracle would provide an unerring pass/fail  judgment for any possible program execution, judged against a natural specification  of intended behavior. Practical approaches must make compromises to balance  trade-offs and provide useful capabilities. This report surveys proposed approaches  to the oracle problem that are general in the sense that they require neither  pre-computed input/output pairs nor a previous version of the system under test.  The survey is not encyclopedic, but discusses representative examples of the main  approaches and tactics for solving common problems.  Partially supported by the Italian National Research Council (CNR). This work has also been supported  by the Defense Advanced Research Projects Agency and Rome Laboratory, Air Force Materiel Command,  USAF, under agreement number F30602-97-2-0034. The U.S. Government is authorized to reproduce and  distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views  and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing  the official policies or endorsements, either expressed or implied, of the Defense Advanced Research  Projects Agency, Rome Laboratory, or the U.S. Government.  1  Contents  1 
6|Location-aware topology matching in p2p systems|Abstract—Peer-to-Peer (P2P) computing has emerged as a popular model aiming at further utilizing Internet information and resources, complementing the available client-server services. However, the mechanism of peers randomly choosing logical neighbors without any knowledge about underlying physical topology can cause a serious topology mismatching between the P2P overlay network and the physical underlying network. The topology mismatching problem brings a great stress in the Internet infrastructure and greatly limits the performance gain from various search or routing techniques. Meanwhile, due to the inefficient overlay topology, the flooding-based search mechanisms cause a large volume of unnecessary traffic. Aiming at alleviating the mismatching problem and reducing the unnecessary traffic, we propose a locationaware topology matching (LTM) technique, an algorithm of building an efficient overlay by disconnecting low productive connections and choosing physically closer nodes as logical neighbors while still retaining the search scope and reducing response time for queries. LTM is scalable and completely distributed in the sense that it does not require any global knowledge of the whole overlay network when each node is optimizing the organization of its logical neighbors. The effectiveness of LTM is demonstrated through simulation studies. Keywords-peer-to-peer; topology mismatching; blind flooding; location-aware topology matching; search efficiency I.
7|A Practical System for Mutation Testing: Help for the Common Programmer |Mutation testing is a technique for unit testing software that, although powerful, is computationally expensive. Recent engineering advances have given us techniques and algorithms for significantly reducing the cost of mutation testing. These techniques include a new algorithmic execution technique called schema-based mutation, an approximation technique called weak mutation, a reduction technique called selective mutation, and algorithms for automatic test data generation. This paper outlines a design for a system that will approximate mutation, but in a way that will be accessible to everyday programmers. We envision a system to which a programmer can submit a program unit, and get back a set of input/output pairs that are guaranteed to form an effective test of the unit by being close to mutation adequate.  1 
8|Mujava: a mutation system for java|Mutation testing is a valuable experimental research tech-nique that has been used in many studies. It has been experimentally compared with other test criteria, and also used to support experimental comparisons of other test cri-teria, by using mutants as a method to create faults. In effect, mutation is often used as a “gold standard ” for ex-perimental evaluations of test methods. Although mutation testing is powerful, it is a complicated and computationally expensive testing method. Therefore, automated tool sup-port is indispensable for conducting mutation testing. This demo presents a publicly available mutation system for Java that supports both method-level mutants and class-level mu-tants. MuJava can be freely downloaded and installed with relative ease under both Unix and Windows. MuJava is of-fered as a free service to the community and we hope that it will promote the use of mutation analysis for experimental research in software testing.
9|Test Architectures for Distributed Systems - State of the Art and Beyond|A generic test architecture for conformance, interoperability and performance testing of distributed systems is presented. The generic test architecture extends current test architectures with respect to the types of systems that can be tested. Whereas in the conformance testing methodology and framework the focus is on testing protocol implementations, the generic architecture focuses on testing real distributed systems whose communication functions are implemented on different real systems and whose correctness can only be assessed when tested as a whole. In support of the latter requirement, a test system itself is regarded as being a distributed system whose behaviour is determined by the behaviour of components and their interaction using a flexible and dynamic communication structure.  Keywords  CTMF, TTCN, test architecture, types of testing, advanced distributed systems  1 INTRODUCTION  A distributed processing system is a system which can exploit a physical architecture consis...
10|Ganesha: Black-box diagnosis of mapreduce systems|Ganesha aims to diagnose faults transparently (in a blackbox manner) in MapReduce systems, by analyzing OS-level metrics. Ganesha’s approach is based on peer-symmetry under fault-free conditions, and can diagnose faults that manifest asymmetrically at nodes within a MapReduce system. We evaluate Ganesha by diagnosing Hadoop problems for the Gridmix Hadoop benchmark on 10-node and 50-node MapReduce clusters on Amazon’s EC2. We also candidly highlight faults that escape Ganesha’s diagnosis. 1.
11|SHAHED: A MapReduce-based System for Querying and Visualizing Spatio-temporal Satellite Data |Abstract—Remote sensing data collected by satellites are now made publicly available by several space agencies. This data is very useful for scientists pursuing research in several applications including climate change, desertification, and land use change. The benefit of this data comes from its richness as it provides an archived history for over 15 years of satellite observations for natural phenomena such as temperature and vegetation. Unfortunately, the use of such data is very limited due to the huge size of archives (&gt; 500TB) and the limited capabilities of traditional applications. This paper introduces SHAHED; a MapReduce-based system for querying, visualizing, and mining large scale satellite data. SHAHED considers both the spatial and temporal aspects of the data to provide efficient query processing at large scale. The core of SHAHED is composed of four main components. The uncertainty component recovers missing data in the input which comes from cloud coverage and satellite mis-alignment. The indexing component provides a novel multi-resolution quad-tree-based spatio-temporal index structure, which indexes satellite data efficiently with minimal space overhead. The querying component answers selection and aggregate queries in real-time using the constructed index. Finally, the visualization component uses MapReduce programs to generate heat map images and videos for user queries. A set of experiments running on a live system deployed on a cluster of machines show the efficiency of the proposed design. All the features supported by SHAHED are made accessible through an easy to use web interface that hides the complexity of the system and provides a nice user experience. I.
12|The quadtree and related hierarchical data structures|A tutorial survey is presented of the quadtree and related hierarchical data structures. They are based on the principle of recursive decomposition. The emphasis is on the representation of data used in applications in image processing, computer graphics, geographic information systems, and robotics. There is a greater emphasis on region data (i.e., two-dimensional shapes) and to a lesser extent on point, curvilinear, and threedimensional data. A number of operations in which such data structures find use are examined in greater detail.
13|Parallel Construction of Quadtrees and Quality Triangulations|We describe e#cient PRAM algorithms for constructing unbalanced quadtrees, balanced  quadtrees, and quadtree-based finite element meshes. Our algorithms take time  O(log n) for point set input and O(log n log k) time for planar straight-line graphs, using  O(n + k/ log n) processors, where n measures input size and k output size.  1. Introduction  A crucial preprocessing step for the finite element method is mesh generation, and the most general and versatile type of two-dimensional mesh is an unstructured triangular mesh. Such a mesh is simply a triangulation of the input domain (e.g., a polygon), along with some extra vertices, called Steiner points. Not all triangulations, however, serve equally well; numerical and discretization error depend on the quality of the triangulation, meaning the shapes and sizes of triangles. A typical quality guarantee gives a lower bound on the minimum angle in the triangulation.  Baker et al.  1  first proved the existence of quality triangulations fo...
14|A Demonstration of SpatialHadoop: An Efficient MapReduce Framework for Spatial Data|This demo presents SpatialHadoop as the first full-fledged MapReduce framework with native support for spatial data. Spatial-Hadoop is a comprehensive extension to Hadoop that pushes spatial data inside the core functionality of Hadoop. SpatialHadoop runs existing Hadoop programs as is, yet, it achieves order(s) of magnitude better performance than Hadoop when dealing with spatial data. SpatialHadoop employs a simple spatial high level language, a two-level spatial index structure, basic spatial components built inside the MapReduce layer, and three basic spatial operations: range queries, k-NN queries, and spatial join. Other spatial operations can be similarly deployed in SpatialHadoop. We demonstrate a real system prototype of SpatialHadoop running on an Amazon EC2 cluster against two sets of real spatial data obtained from Tiger Files and OpenStreetMap with sizes 60GB and 300GB, respectively. 1.
15|Change Detection from Temporal Sequences of Class Labels: Application to Land Cover Change Mapping |Mapping land cover change is an important problem for the scientific community as well as policy makers. Traditionally, bi-temporal classification of satellite data is used to identify areas of land cover change. However, these classification products often have errors due to classifier inaccuracy or poor data, which poses significant issues when using them for land cover change detection. In this paper, we propose a generative model for land cover label sequences and use it to reassign a more accurate sequence of land cover labels to every pixel. Empirical evaluation on real and synthetic data suggests that the proposed approach is effective in capturing the characteristics of land cover classification and change processes, and produces significantly improved classification and change detection products. 1
16|TAREEG: A MapReduce-Based System for Extracting Spatial Data from OpenStreetMap |Real spatial data, e.g., detailed road networks, rivers, build-ings, parks, are not easily available for most of the world. This hinders the practicality of many research ideas that need a real spatial data for testing and experiments. Such data is often available for governmental use, or at major soft-ware companies, but it is prohibitively expensive to build or buy for academia or individual researchers. This paper presents TAREEG; a web-service that makes real spatial data, from anywhere in the world, available at the fingertips of every researcher or individual. TAREEG gets all its data by leveraging the richness of OpenStreetMap data set; the most comprehensive available spatial data of the world. Yet, it is still challenging to obtain OpenStreetMap data due to the size limitations, special data format, and the noisy na-ture of spatial data. TAREEG employs MapReduce-based techniques to make it efficient and easy to extract Open-StreetMap data in a standard form with minimal effort. Experimental results show that TAREEG is highly accurate and efficient.
17|The grid file: an adaptable, symmetric multikey file structure|Traditional file structures that provide multikey access to records, for example, inverted files, are extensions of file structures originally designed for single-key access. They manifest various deficiencies in particular for multikey access to highly dynamic files. We study the dynamic aspects of tile structures that treat all keys symmetrically, that is, file structures which avoid the distinction between primary and secondary keys. We start from a bitmap approach and treat the problem of file design as one of data compression of a large sparse matrix. This leads to the notions of a grid partition of the search space and of a grid directory, which are the keys to a dynamic file structure called the grid file. This tile system adapts gracefully to its contents under insertions and deletions, and thus achieves an upper hound of two disk accesses for single record retrieval; it also handles range queries and partially specified queries efficiently. We discuss in detail the design decisions that led to the grid file, present simulation results of its behavior, and compare it to other multikey access file structures.
18|Nearest neighbor queries in road networks|With wireless communications and geo-positioning being widely available, it becomes possible to offer new e-services that provide mobile users with information about other mobile objects. This pa-per concerns active, ordered k-nearest neighbor queries for query and data objects that are moving in road networks. Such queries may be of use in many services. Specifically, we present an easily implementable data model that serves well as a foundation for such queries. We also present the design of a prototype system that implements the queries based on the data model. The algorithm used for the nearest neighbor search in the prototype is presented in detail. In addition, the paper reports on results from experiments with the prototype system.
19|Continuous Nearest Neighbor Monitoring in Road Networks|Recent research has focused on continuous monitoring of nearest neighbors (NN) in highly dynamic scenarios, where the queries and the data objects move frequently and arbitrarily. All existing methods, however, assume the Euclidean distance metric. In this paper we study k-NN monitoring in road networks, where the distance between a query and a data object is determined by the length of the shortest path connecting them. We propose two methods that can handle arbitrary object and query moving patterns, as well as fluctuations of edge weights. The first one maintains the query results by processing only updates that may invalidate the current NN sets. The second method follows the shared execution paradigm to reduce the processing time. In particular, it groups together the queries that fall in the path between two consecutive intersections in the network, and produces their results by monitoring the NN sets of these intersections. We experimentally verify the applicability of the proposed techniques to continuous monitoring of large data and query sets.  
20|Aggregate nearest neighbor queries in road networks|Abstract—Aggregate nearest neighbor queries return the object that minimizes an aggregate distance function with respect to a set of query points. Consider, for example, several users at specific locations (query points) that want to find the restaurant (data point), which leads to the minimum sum of distances that they have to travel in order to meet. We study the processing of such queries for the case where the position and accessibility of spatial objects are constrained by spatial (e.g., road) networks. We consider alternative aggregate functions and techniques that utilize Euclidean distance bounds, spatial access methods, and/or network distance materialization structures. Our algorithms are experimentally evaluated with synthetic and real data. The results show that their relative performance depends on the problem characteristics. Index Terms—Query processing, spatial databases, spatial databases and GIS, location-dependent and sensitive. 1
21|Monitoring path nearest neighbor in road networks|This paper addresses the problem of monitoring the k nearest neighbors to a dynamically changing path in road networks. Given a destination where a user is going to, this new query returns the k-NN with respect to the shortest path connecting the destination and the user’s current location, and thus provides a list of nearest candidates for reference by considering the whole coming journey. We name this query the k-Path Nearest Neighbor query (k-PNN). As the user is moving and may not always follow the shortest path, the query path keeps changing. The challenge of monitoring the k-PNN for an arbitrarily moving user is to dynamically determine the update locations and then refresh the k-PNN efficiently. We propose a three-phase Best-first Network Expansion (BNE) algorithm for monitoring the k-PNN and the corresponding shortest path. In the searching phase, the BNE finds the shortest path to the destination, during which a candidate set that guarantees to include the k-PNN is generated at the same time. Then in the verification phase, a heuristic algorithm runs for examining candidates’ exact distances to the query path, and it achieves significant reduction in the number of visited nodes. The monitoring phase deals with computing update locations as well as refreshing the k-PNN in different user movements. Since determining the network distance is a costly process, an expansion tree and the candidate set are carefully maintained by the BNE algorithm, which can provide efficient update on the shortest path and the k-PNN results. Finally, we conduct extensive experiments on real road networks and show that our methods achieve satisfactory performance.
22|Graph Indexing of Road Networks for Shortest Path Queries with Label Restrictions |The current widespread use of location-based services and GPS technologies has revived interest in very fast and scalable shortest path queries. We introduce a new shortest path query type in which dynamic constraints may be placed on the allowable set of edges that can appear on a valid shortest path (e.g., dynamically restricting the type of roads or modes of travel which may be considered in a multimodal transportation network). We formalize this problem as a specific variant of formal language constrained shortest path problems, which we call the Kleene Language Constrained Shortest Paths problem. To efficiently support this type of dynamically constrained shortest path query for large-scale datasets, we extend the hierarchical graph indexing technique known as Contraction Hierarchies. Our experimental evaluation using the North American road network dataset (with over 50 million edges) shows an average query speed and search space improvement of over 3 orders of magnitude compared to the naïve adaptation of the standard Dijkstra’s algorithm to support this query type. We also show an improvement of over 2 orders of magnitude compared to the only previously-existing indexing technique which could solve this problem without additional preprocessing. 1.
23|Shortest Path and Distance Queries on Road Networks: An Experimental Evaluation |Computing the shortest path between two given locations in a road network is an important problem that finds applications in various map services and commercial navigation products. The stateof-the-art solutions for the problem can be divided into two categories: spatial-coherence-based methods and vertex-importancebased approaches. The two categories of techniques, however, have not been compared systematically under the same experimental framework, as they were developed from two independent lines of research that do not refer to each other. This renders it difficult for a practitioner to decide which technique should be adopted for a specific application. Furthermore, the experimental evaluation of the existing techniques, as presented in previous work, falls short in several aspects. Some methods were tested only on small road networks with up to one hundred thousand vertices; some approaches were evaluated using distance queries (instead of shortest path queries), namely, queries that ask only for the length of the shortest path; a state-of-the-art technique was examined based on a faulty implementation that led to incorrect query results. To address the above issues, this paper presents a comprehensive comparison of the most advanced spatial-coherence-based and vertex-importance-based approaches. Using a variety of real road networks with up to twenty million vertices, we evaluated each technique in terms of its preprocessing time, space consumption, and query efficiency (for both shortest path and distance queries). Our experimental results reveal the characteristics of different techniques, based on which we provide guidelines on selecting appropriate methods for various scenarios. 1.
24|DISKs: A System for Distributed Spatial Group Keyword Search on Road Networks |Query (e.g., shortest path) on road networks has been exten-sively studied. Although most of the existing query process-ing approaches are designed for centralized environments, there is a growing need to handle queries on road network-s in distributed environments due to the increasing query workload and the challenge of querying large networks. In this demonstration, we showcase a distributed system called DISKs (DI stributed Spatial K eyword search) that is capa-ble of efficiently supporting spatial group keyword search (S-GKS) on road networks. Given a group of keywords X and a distance r, an SGKS returns locations on a road network, such that for each returned location p, there exists a set of nodes (on the road network), which are located within a network distance r from p and collectively contains X. We will demonstrate the innovative modules, performance and interactive user interfaces of DISKs. 1.
25|Finding the Most Accessible Locations: Reverse Path Nearest Neighbor Query in Road Networks|In this paper, we propose and investigate a novel spatial query called Reverse Path Nearest Neighbor (R-PNN) search to find the most accessible locations in road networks. Given a trajectory data-set and a list of location candidates spec-ified by users, if a location o is the Path Nearest Neighbor (PNN) of k trajectories, the influence-factor of o is defined as k and the R-PNN query returns the location with the highest influence-factor. The R-PNN query is an extension of the conventional Reverse Nearest Neighbor (RNN) search. It can be found in many important applications such as ur-ban planning, facility allocation, traffic monitoring, etc. To answer the R-PNN query efficiently, an effective trajectory data pre-processing technique is conducted in the first place. We cluster the trajectories into several groups according to their distribution. Based on the grouped trajectory data, a two-phase solution is applied. First, we specify a tight search range over the trajectory and location data-sets. The effi-ciency study reveals that our approach defines the minimum search area. Second, a series of optimization techniques are adopted to search the exact PNN for trajectories in the can-didate set. By combining the PNN query results, we can retrieve the most accessible locations. The complexity anal-ysis shows that our solution is optimal in terms of time cost. The performance of the proposed R-PNN query processing is verified by extensive experiments based on real and syn-thetic trajectory data in road networks.
26|TAREEG: A MapReduce-Based Web Service for Extracting Spatial Data from OpenStreetMap (System Demonstration  (2014) |Real spatial data, e.g., detailed road networks, rivers, build-ings, parks, are not really available in most of the world. This hinders the practicality of many research ideas that need a real spatial data for testing experiments. Such data is often available for governmental use, or at major software companies, but it is prohibitively expensive to build or buy for academia or individual researchers. This demo presents TAREEG; a web-service that makes real spatial data, from anywhere in the world, available at the fingertips of every researcher or individual. TAREEG gets all its data by lever-aging the richness of OpenStreetMap dataset; the most com-prehensive available spatial data of the world. Yet, it is still challenging to obtain OpenStreetMap data due to the size limitations, special data format, and the noisy nature of spa-tial data. TAREEG employs MapReduce-based techniques to make it efficient and easy to extract OpenStreetMap data in a standard form with minimal effort. TAREEG is acces-sible via
27|Pigeon: A spatial mapreduce language|Abstract—With the huge amounts of spatial data collected everyday, MapReduce frameworks, such as Hadoop, have become a common choice to analyze big spatial data for scientists and peo-ple from industry. Users prefer to use high level languages, such as Pig Latin, to deal with Hadoop for simplicity. Unfortunately, these languages are designed for primitive non-spatial data and have no support for spatial data types or functions. This demonstration presents Pigeon, a spatial extension to Pig which provides spatial functionality in Pig. Pigeon is implemented through user defined functions (UDFs) making it easy to use and compatible with all recent versions of Pig. This also allows it to integrate smoothly with existing non-spatial functions and operations such as Filter, Join and Group By. Pigeon is compatible with the Open Geospatial Consortium (OGC) standard which makes it easy to learn and use for users who are familiar with existing OGC-compliant tools such as PostGIS. This demonstrations shows to audience how to work with Pigeon through some interesting applications running on large scale real datasets extracted from OpenStreetMap. I.
28|A Demonstration of Shahed: A MapReduce-based System for Querying and Visualizing Satellite Data |Abstract—Several space agencies such as NASA are continu-ously collecting datasets of earth dynamics—e.g., temperature, vegetation, and cloud coverage—through satellites. This data is stored in a publicly available archive for scientists and re-searchers and is very useful for studying climate, desertification, and land use change. The benefit of this data comes from its richness as it provides an archived history for over 15 years of satellite observations. Unfortunately, the use of such data is very limited due to the huge size of archives (&gt; 500TB) and the limited capabilities of traditional applications. In this demo, we present Shahed, an interactive system which provides an efficient way to index, query, and visualize satellite datasets available in NASA archive. Shahed is composed of four main modules. The uncertainty module resolves data uncertainty imposed by the satellites. The indexing module organizes the data in a novel multi-resolution spatio-temporal index designed for satellite data. The querying module uses the indexes to answer both spatio-temporal selection and aggregate queries provided by the user. The visualization module generates images, videos, and multi-level images which gives an insight of data distribution and dynamics over time. This demo gives users a hands-on experience with Shahed through a map-based web interface in which users can browse the available datasets using the map, issue spatio-temporal queries, and visualize the results as images or videos. I.
29|HadoopDB: An Architectural Hybrid of MapReduce and DBMS Technologies for Analytical Workloads |The production environment for analytical data management applications is rapidly changing. Many enterprises are shifting away from deploying their analytical databases on high-end proprietary machines, and moving towards cheaper, lower-end, commodity hardware, typically arranged in a shared-nothing MPP architecture, often in a virtualized environment inside public or private “clouds”. At the same time, the amount of data that needs to be analyzed is exploding, requiring hundreds to thousands of machines to work in parallel to perform the analysis. There tend to be two schools of thought regarding what technology to use for data analysis in such an environment. Proponents of parallel databases argue that the strong emphasis on performance and efficiency of parallel databases makes them wellsuited to perform such analysis. On the other hand, others argue that MapReduce-based systems are better suited due to their superior scalability, fault tolerance, and flexibility to handle unstructured data. In this paper, we explore the feasibility of building a hybrid system that takes the best features from both technologies; the prototype we built approaches parallel databases in performance and efficiency, yet still yields the scalability, fault tolerance, and flexibility of MapReduce-based systems. 1.
30|Xen and the art of virtualization|Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100 % binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service. This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort. Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead — at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests.
31|Pig Latin: A Not-So-Foreign Language for Data Processing |There is a growing need for ad-hoc analysis of extremely large data sets, especially at internet companies where innovation critically depends on being able to analyze terabytes of data collected every day. Parallel database products, e.g., Teradata, offer a solution, but are usually prohibitively expensive at this scale. Besides, many of the people who analyze this data are entrenched procedural programmers, who find the declarative, SQL style to be unnatural. The success of the more procedural map-reduce programming model, and its associated scalable implementations on commodity hardware, is evidence of the above. However, the map-reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse. We describe a new language called Pig Latin that we have designed to fit in a sweet spot between the declarative style of SQL, and the low-level, procedural style of map-reduce. The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. We give a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the development and execution of their data analysis tasks, compared to using Hadoop directly. We also report on a novel debugging environment that comes integrated with Pig, that can lead to even higher productivity gains. Pig is an open-source, Apache-incubator project, and available for general use. 1.
32|SCOPE: Easy and Efficient Parallel Processing of Massive Data Sets |Companies providing cloud-scale services have an increasing need to store and analyze massive data sets such as search logs and click streams. For cost and performance reasons, processing is typically done on large clusters of shared-nothing commodity machines. It is imperative to develop a programming model that hides the complexity of the underlying system but provides flexibility by allowing users to extend functionality to meet a variety of requirements. In this paper, we present a new declarative and extensible scripting language, SCOPE (Structured Computations Optimized for Parallel Execution), targeted for this type of massive data analysis. The language is designed for ease of use with no explicit parallelism, while being amenable to efficient parallel execution on large clusters. SCOPE borrows several features from SQL. Data is modeled as sets of rows composed of typed columns. The select statement is retained with inner joins, outer joins, and aggregation allowed. Users can easily define their own functions and implement their own versions of operators: extractors (parsing and constructing rows from a file), processors (row-wise processing), reducers (group-wise processing), and combiners (combining rows from two inputs). SCOPE supports nesting of expressions but also allows a computation to be specified as a series of steps, in a manner often preferred by programmers. We also describe how scripts are compiled into efficient, parallel execution plans and executed on large clusters. 1.
33|Efficient dispersal of information for security, load balancing, and fault tolerance|Abstract. An Information Dispersal Algorithm (IDA) is developed that breaks a file F of length L =  ( F ( into n pieces F,, 1 5 i 5 n, each of length ( F, 1 = L/m, so that every m pieces suffice for reconstructing F. Dispersal and reconstruction are computationally efficient. The sum of the lengths ( F, 1 is (n/m). L. Since n/m can be chosen to be close to I, the IDA is space eflicient. IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communi-cations between processors in parallel computers. For the latter problem provably time-efftcient and highly fault-tolerant routing on the n-cube is achieved, using just constant size buffers. Categories and Subject Descriptors: E.4 [Coding and Information Theory]: nonsecret encoding schemes
34|Distributed Computing in Practice: The Condor Experience|Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational grid. In this chapter, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course traveled by research ideas as they grow into production systems.
35|Cluster-Based Scalable Network Services|This paper has benefited from the detailed and perceptive comments of our reviewers, especially our shepherd Hank Levy. We thank Randy Katz and Eric Anderson for their detailed readings of early drafts of this paper, and David Culler for his ideas on TACC&#039;s potential as a model for cluster programming. Ken Lutz and Eric Fraser configured and administered the test network on which the TranSend scaling experiments were performed. Cliff Frost of the UC Berkeley Data Communications and Networks Services group allowed us to collect traces on the Berkeley dialup IP network and has worked with us to deploy and promote TranSend within Berkeley. Undergraduate researchers Anthony Polito, Benjamin Ling, and Andrew Huang implemented various parts of TranSend&#039;s user profile database and user interface. Ian Goldberg and David Wagner helped us debug TranSend, especially through their implementation of the rewebber
36|Parallel Prefix Computation|ABSTRACT The prefix problem is to compute all the products x t o x2.... o xk for i ~ k. ~ n, where o is an associative operation A recurstve construction IS used to obtain a product circuit for solving the prefix problem which has depth exactly [log:n] and size bounded by 4n An application yields fast, small Boolean ctrcmts to simulate fimte-state transducers. By simulating a sequentml adder, a Boolean clrcmt which has depth 2[Iog2n] + 2 and size bounded by 14n Is obtained for n-bit binary addmon The size can be decreased significantly by permitting the depth to increase by an addmve constant
37|Cluster I/O with River: Making the Fast Case Common|We introduce River, a data-flow programming environment and I/O substrate for clusters of computers. River is designed to provide maximum performance in the common case --- even in the face of nonuniformities in hardware, software, and workload. River is based on two simple design features: a high-performance distributed queue, and a storage redundancy mechanism called graduated declustering. We have implemented a number of data-intensive applications on River, which validate our design with near-ideal performance in a variety of non-uniform performance scenarios. 
38|Diamond: A storage architecture for early discard in interactive search|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
39|Explicit Control in a Batch-Aware Distributed File System |We present the design, implementation, and evaluation of the Batch-Aware Distributed File System (BAD-FS), a system designed to orchestrate large, I/O-intensive batch workloads on remote computing clusters distributed across the wide area. BAD-FS consists of two novel components: a storage layer which exposes control of traditionally fixed policies such as caching, consistency, and replication; and a scheduler that exploits this control as needed for different users and workloads. By extracting these controls from the storage layer and placing them in an external scheduler, BAD-FS manages both storage and computation in a coordinated way while gracefully dealing with cache consistency, fault-tolerance, and space management issues in an application-specific manner. Using both microbenchmarks and real applications, we demonstrate the performance benefits of explicit control, delivering excellent end-to-end performance across the wide-area.  
40|MapDupReducer: Detecting Near Duplicates over Massive |Near duplicate detection benefits many applications, e.g., on-line news selection over the Web by keyword search. The purpose of this demo is to show the design and implementation of MapDupReducer, a MapReduce based system capable of detecting near duplicates over massive datasets efficiently.
41|Duplicate record detection: A survey|Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a dif cult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats or any combination of these factors. In this article, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar eld entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the ef ciency and scalability of approximate duplicate detection algorithms. We conclude with a coverage of existing tools and with a brief discussion of the big open problems in the area.  
42|Efficient similarity joins for near duplicate detection|With the increasing amount of data and the need to integrate data from multiple data sources, one of the challenging issues is to identify near duplicate records efficiently. In this paper, we focus on efficient algorithms to find pair of records such that their similarities are no less than a given threshold. Several existing algorithms rely on the prefix filtering principle to avoid computing similarity values for all possible pairs of records. We propose new filtering techniques by exploiting the token ordering information; they are integrated into the existing methods and drastically reduce the candidate sizes and hence improve the efficiency. We have also studied the implementation of our proposed algorithm in stand-alone and RDBMSbased settings. Experimental results show our proposed algorithms can outperforms previous algorithms on several real datasets.
43|Brute force and indexed approaches to pairwise document similarity comparisons with mapreduce|This paper explores the problem of computing pairwise similarity on document collections, focusing on the application of “more like this ” queries in the life sciences domain. Three MapReduce algorithms are introduced: one based on brute force, a second where the problem is treated as large-scale ad hoc retrieval, and a third based on the Cartesian product of postings lists. Each algorithm supports one or more approximations that trade effectiveness for efficiency, the characteristics of which are studied experimentally. Results show that the brute force algorithm is the most efficient of the three when exact similarity is desired. However, the other two algorithms support approximations that yield large efficiency gains without significant loss of effectiveness.
44|The design and implementation of an intentional naming system|This paper presents the design and implementation of the Intentional Naming System (INS), a resource discovery and service location system for dynamic and mobile networks of devices and computers. Such environments require a naming system that is (i) expressive, to describe and make requests based on specific properties of services, (ii) responsive, to track changes due to mobility and performance, (iii) robust, to handle failures, and (iv) easily configurable. INS uses a simple language based on attributes and values for its names. Applications use the language to describe what they are looking for (i.e., their intent), not where to find things (i.e., not hostnames). INS implements a late binding mechanism that integrates name resolution and message routing, enabling clients to continue communicating with end-nodes even if the name-to-address mappings change while a session is in progress. INS resolvers self-configure to form an application-level overlay network, which they use to discover new services, perform late binding, and maintain weak consistency of names using soft-state name exchanges and updates. We analyze the performance of the INS algorithms and protocols, present measurements of a Java-based implementation, and describe three applications we have implemented that demonstrate the feasibility and utility of INS.
45|A Performance Comparison of Multi-Hop Wireless Ad Hoc Network Routing Protocols|An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Due to the limited transmission range of wireless network interfaces, multiple network &amp;quot;hops &amp;quot; may be needed for one node to exchange data with another across the network. In recent years, a variety of new routing protocols targeted specifically at this environment have been developed, but little performance information on each protocol and no realistic performance comparison between them is available. This paper presents the results of a detailed packet-level simulation comparing four multi-hop wireless ad hoc network routing protocols that cover a range of design choices: DSDV, TORA, DSR, and AODV. We have extended the ns-2 network simulator to accurately model the MAC and physical-layer behavior of the IEEE 802.11 wireless LAN standard, including a realistic wireless transmission channel model, and present the results of simulations of networks of 50 mobile nodes. 1
46|Next century challenges: Scalable coordination in sensor networks|Networked sensors-those that coordinate amongst them-selves to achieve a larger sensing task-will revolutionize information gathering and processing both in urban envi-ronments and in inhospitable terrain. The sheer numbers of these sensors and the expected dynamics in these environ-ments present unique challenges in the design of unattended autonomous sensor networks. These challenges lead us to hypothesize that sensor network coordination applications may need to be structured differently from traditional net-work applications. In particular, we believe that localized algorithms (in which simple local node behavior achieves a desired global objective) may be necessary for sensor net-work coordination. In this paper, we describe localized al-gorithms, and then discuss directed diffusion, a simple com-munication model for describing localized algorithms. 1
47|Multicast Routing in Datagram Internetworks and Extended LANs|Multicasting, the transmission of a packet to a group of hosts, is an important service for improving the efficiency and robustness of distributed systems and applications. Although multicast capability is available and widely used in local area networks, when those LANs are interconnected by store-and-forward routers, the multicast service is usually not offered across the resulting internetwork. To address this limitation, we specify extensions to two common internetwork routing algorithms-distance-vector routing and link-state routing-to support low-delay datagram multicasting beyond a single LAN. We also describe modifications to the single-spanning-tree routing algorithm commonly used by link-layer bridges, to reduce the costs of multicasting in large extended LANs. Finally, we discuss how the use of multicast scope control and hierarchical multicast routing allows the multicast service to scale up to large internetworks.
48|Supporting Real-Time Applications in an Integrated Services Packet Network: Architecture and Mechanism|This paper considers the support of real-time applications in an
49|A Survey of active network Research|Active networks are a novel approach to network architecture in which the switches of the network perform customized computations on the messages flowing through them. This approach is motivated by both lead user applications, which perform user-driven computation at nodes within the network today, and the emergence of mobile code technologies that make dynamic network service innovation attainable. In this paper, we discuss two approaches to the realization of active networks and provide a snapshot of the current research issues and activities. Introduction – What Are Active Networks? In an active network, the routers or switches of the network perform customized computations on the messages flowing through them. For example, a user of an active network could send a “trace ” program to each router and arrange for the program to be executed when their packets are processed. Figure 1 illustrates how the routers of an IP
50|An Architecture for a Secure Service Discovery Service|The widespread deployment of inexpensive communications technology, computational resources in the networking infrastructure, and network-enabled end devices poses an interesting problem for end users: how to locate a particular network service or device out of hundreds of thousands of accessible services and devices. This paper presents the architecture and implementation of a secure Service Discovery Service (SDS). Service providers use the SDS to advertise complex descriptions of available or already running services, while clients use the SDS to compose complex queries for locating these services. Service descriptions and queries use the eXtensible Markup Language (XML) to encode such factors as cost, performance, location, and device- or service-specific capabilities. The SDS provides a highlyavailable, fault-tolerant, incrementally scalable service for locating services in the wide-area. Security is a core component of the SDS and, where necessary, communications are both encrypt...
51|Development of the Domain Name System|(Originally published in the Proceedings of SIGCOMM ‘88,
52|Designing a Global Name Service|A name service maps a name of an individual, organization or facility into a set of labeled properties, each of which is a string. It is the basis for resource location, mail addressing, and authentication in a distributed computing system. The global name service described here is meant to do this for billions of names distributed throughout the world. It addresses the problems of high availability, large size, continuing evolution, fault isolation and lack of global trust. The non-deterministic behavior of the service is specified rather precisely to allow a wide range of client and server implementations.  Introduction  There are already enough names. One must know when to stop. Knowing when to stop averts trouble.  Tao Te Ching The name service I am describing in this talk is intended to be the basis for resource location, mail addressing, and authentication in a distributed computing system. The system I have in mind is a large one, large enough to encompass all the computers in t...
53|A model, analysis, and protocol framework for soft state-based communication|&#034;Soft state&#034; is an often cited yet vague concept in network protocol design in which two or more network entities intercommunicate in a loosely coupled, often anonymous fashion. Researchers often define this concept operationally (if at all) rather than analytically: a source of soft state transmits periodic &#034;refresh messages&#034; over a (lossy) communication channel to one or more receivers that maintain a copy of that state, which in turn &#034;expires&#034; if the periodic updates cease. Though a number of crucial Internet protocol building blocks are rooted in soft state-based designs | e.g., RSVP refresh messages, PIM membership updates, various routing protocol updates, RTCP control messages, directory services like SAP, and so forth | controversy is building as to whether the performance overhead of soft state refresh messages justify their qualitative benefit of enhanced system &#034;robustness&#034;. We believe that this controversy has risen not from fundamental performance tradeo s but rather from our lack of a comprehensive understanding of soft state. To better understand these tradeoffs, we propose herein a formal model for soft state communication based on a probabilistic delivery model with relaxed reliability. Using this model, we conduct queueing analysis and simulation to characterize the data consistency and performance tradeo s under a range of workloads and network loss rates. We then extend our model with feedback and show, through simulation, that adding feedback dramatically improves data consistency (by up to 55%) without increasing network resource consumption. Our model not only provides a foundation for understanding soft state, but also induces a new fundamental transport protocol based on probabilistic delivery. Toward this end, we sketch our design of the &#034;Soft State Transport Protocol&#034; (SSTP), which enjoys the robustness of soft state while retaining the performance benefit of hard state protocols like TCP through its judicious use of feedback.  
54|Active Names: Flexible Location and Transport of Wide-Area Resources|In this paper, we explore flexible name resolution as a way of supporting extensibility for wide-area distributed services. Our approach, called Active Names, maps names to a chain of mobile programs that can customize how a service is located and how its results are transformed and transported back to the client. To illustrate the properties of our system, we implement prototypes of server selection based on end-to-end performance measurements, location-independent data transformation, and caching of composable active objects and demonstrate up to a five-fold performance improvement to end users. We show how these new services are developed, composed, and secured in our framework. Finally, we develop a set of algorithms to control how mobile Active Name programs are mapped onto available wide-area resources to optimize performance and availability.  
55|Application-Layer Anycasting|The anycasting communication paradigm is designed to support server replication by allowing applications to easily select and communicate with the &#034;best&#034; server, according to some performance or policy criteria, in a group of content-equivalent servers. We examine the definition and support of the anycasting paradigm at the application layer, providing a service that maps anycast domain names into one or more IP addresses using anycast resolvers. In addition to being independent from network-layer support, our definition includes the notion of filters, functions that are applied to groups of addresses to affect the selection process. We consider both metric-based filters (e.g., server response time) and policy-based filters. An expanded version of this work can be found as a technical report. 
56|Salamander: a push-based distribution substrate for Internet applications|The Salamander distribution system is a widearea network data dissemination substrate that has been used daily for overayear by several groupware and webcasting Internet applications. Speci-cally, Salamander is designed to support push-based applications and provides a variety of delivery semantics. These semantics range from basic data delivery, used by the Internet Performance Measurement and Analysis (IPMA) project, to collaborative group communication used by the Upper Atmospheric Research Collaboratory (UARC) project. The Salamander substrate is designed to accommodate the large variation in Internet connectivity and client resources through the use of applicationspeci
57|Discover: A Resource Discovery System based on Content Routing| We have built an HTTP based resource discovery system called Discover that provides a single point
58|Cilk: An Efficient Multithreaded Runtime System|Cilk (pronounced &#034;silk&#034;) is a C-based runtime system for multithreaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the &#034;work&#034; and &#034;critical-path length&#034; of a Cilk computation can be used to model performance accurately. Consequently, a Cilk programmer can focus on reducing the computation&#039;s work and critical-path length, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of &#034;fully strict&#034; (well-structured) programs, the Cilk scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The Cilk
59|PVM: A Framework for Parallel Distributed Computing|The PVM system is a programming environment for the development and execution of large concurrent or parallel applications that consist of many interacting, but relatively independent, components. It is intended to operate on a collection of heterogeneous computing elements interconnected by one or more networks. The participating processors may be scalar machines, multiprocessors, or special-purpose computers, enabling application components to execute on the architecture most appropriate to the algorithm. PVM provides a straightforward and general interface that permits the description of various types of algorithms (and their interactions), while the underlying infrastructure permits the execution of applications on a virtual computing environment that supports multiple parallel computation models. PVM contains facilities for concurrent, sequential, or conditional execution of application components, is portable to a variety of architectures, and supports certain forms of error dete...
60|Scheduling Multithreaded Computations by Work Stealing|This paper studies the problem of efficiently scheduling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is “work stealing,&#034; in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies. Specifically, our analysis shows that the ezpected time Tp to execute a fully strict computation on P processors using our work-stealing scheduler is Tp = O(TI/P + Tm), where TI is the minimum serial eze-cution time of the multithreaded computation and T, is the minimum ezecution time with an infinite number of processors. Moreover, the space Sp required by the execution satisfies Sp 5 SIP. We also show that the ezpected total communication of the algorithm is at most O(TmS,,,P), where S, is the site of the largest activation record of any thread, thereby justify-ing the folk wisdom that work-stealing schedulers are more communication eficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.
61|MULTILISP: a language for concurrent symbolic computation|Multilisp is a version of the Lisp dialect Scheme extended with constructs for parallel execution. Like Scheme, Multilisp is oriented toward symbolic computation. Unlike some parallel programming languages, Multilisp incorporates constructs for causing side effects and for explicitly introducing parallelism. The potential complexity of dealing with side effects in a parallel context is mitigated by the nature of the parallelism constructs and by support for abstract data types: a recommended Multilisp programming style is presented which, if followed, should lead to highly parallel, easily understandable programs. Multilisp is being implemented on the 32-processor Concert multiprocessor; however, it is ulti-mately intended for use on larger multiprocessors. The current implementation, called Concert Multilisp, is complete enough to run the Multilisp compiler itself and has been run on Concert prototypes including up to eight processors. Concert Multilisp uses novel techniques for task scheduling and garbage collection. The task scheduler helps control excessive resource utilization by means of an unfair scheduling policy; the garbage collector uses a multiprocessor algorithm based on the incremental garbage collector of Baker.
63|Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism|Threads are the vehicle,for concurrency in many approaches to parallel programming. Threads separate the notion of a sequential execution stream from the other aspects of traditional UNIX-like processes, such as address spaces and I/O descriptors. The objective of this separation is to make the expression and control of parallelism sufficiently cheap that the programmer or compiler can exploit even fine-grained parallelism with acceptable overhead. Threads can be supported either by the operating system kernel or by user-level library code in the application address space, but neither approach has been fully satisfactory. This paper addresses this dilemma. First, we argue that the performance of kernel threads is inherently worse than that of user-level threads, rather than this being an artifact of existing implementations; we thus argue that managing par- allelism at the user level is essential to high-performance parallel computing. Next, we argue that the lack of system integration exhibited by user-level threads is a consequence of the lack of kernel support for user-level threads provided by contemporary multiprocessor operating systems; we thus argue that kernel threads or processes, as currently conceived, are the wrong abstraction on which to support user- level management of parallelism. Finally, we describe the design, implementation, and performance of a new kernel interface and user-level thread package that together provide the same functionality as kernel threads without compromis- ing the performance and flexibility advantages of user-level management of parallelism.
64|The parallel evaluation of general arithmetic expressions|ABSTRACT. It is shown that arithmetic expressions with n&gt; 1 variables and constants; operations of addition, multiplication, and division; and any depth of parenthesis nesting can be evaluated in time 4 log2n + 10(n- 1)/p using p&gt; 1 processors which can independently perform arithmetic operations in unit time. This bound is within a constant factor of the best possible. A sharper result is given for expressions without the division operation, and the question of numerical stability is discussed. KEY WORDS AND PHRASES: arithmetic expressions, compilation of arithmetic expressions, compu-tational complexity, general arithmetic expressions, numerical stability, parallel computatioR,
65|Lazy Task Creation: A Technique for Increasing the Granularity of Parallel Programs|Many parallel algorithms are naturally expressed at a fine level of granularity, often finer than a MIMD parallel system can exploit efficiently. Most builders of parallel systems have looked to either the programmer or a parallelizing compiler to increase the granularity of such algorithms. In this paper we explore a third approach to the granularity problem by analyzing two strategies for combining parallel tasks dynamically at run-time. We reject the simpler load-based inlining method, where tasks are combined based on dynamic load level, in favor of the safer and more robust lazy task creation method, where tasks are created only retroactively as processing resources become available. These strategies grew out of work on Mul-T [15], an efficient parallel implementation of Scheme, but could be used with other languages as well. We describe our Mul-T implementations of lazy task creation for two contrasting machines, and present performance statistics which show the method&#039;s effectiveness. Lazy task creation allows efficient execution of naturally expressed algorithms of a substantially finer grain than possible with previous parallel Lisp systems. 
66|Programming Parallel Algorithms|In the past 20 years there has been treftlendous progress in developing and analyzing parallel algorithftls. Researchers have developed efficient parallel algorithms to solve most problems for which efficient sequential solutions are known. Although some ofthese algorithms are efficient only in a theoretical framework, many are quite efficient in practice or have key ideas that have been used in efficient implementations. This research on parallel algorithms has not only improved our general understanding ofparallelism but in several cases has led to improvements in sequential algorithms. Unf:ortunately there has been less success in developing good languages f:or prograftlftling parallel algorithftls, particularly languages that are well suited for teaching and prototyping algorithms. There has been a large gap between languages
67|The Amber System: Parallel Programming on a Network of Multiprocessors|Microprocessor-based shared-memory multiprocessors are becoming widely available and promise to provide cost-effective high-performance computing. This paper describes a programming system called Amber which permits a single application program to use a homogeneous network of multiprocessors in a uniform way, making the network appear to the application as an integrated, non-uniform memory access, shared-memory multiprocessor. This simplifies the development of applications and allows compute-intensive parallel programs to effectively harness the potential of multiple nodes. Amber programs are written using an object-oriented subset of the C++ programming language, supplemented with primitives for managing concurrency and distribution. Amber provides a network-wide shared-object virtual memory in which coherence is provided by hardware means for locally-executing threads, and by software means for remote accesses. Amber runs on top of the Topaz operating system on a network of DEC SRC ...
68|Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine|Abstract: In this paper, we present a relatively primitive execution model for ne-grain parallelism, in which all synchronization, scheduling, and storage management is explicit and under compiler control. This is de ned by a threaded abstract machine (TAM) with a multilevel scheduling hierarchy. Considerable temporal locality of logically related threads is demonstrated, providing an avenue for e ective register use under quasi-dynamic scheduling. A prototype TAM instruction set, TL0, has been developed, along with a translator to a variety of existing sequential and parallel machines. Compilation of Id, an extended functional language requiring ne-grain synchronization, under this model yields performance approaching that of conventional languages on current uniprocessors. Measurements suggest that the net cost of synchronization on conventional multiprocessors can be reduced to within a small factor of that on machines with elaborate hardware support, such as proposed data ow architectures. This brings into question whether tolerance to latency and inexpensive synchronization require speci c hardware support or merely an appropriate compilation strategy and program representation. 1
69|Jade: A High-Level, Machine-Independent Language for Parallel Programming|this memory is called a shared object. Pointers to shared objects are identified in a Jade program using the shared type qualifier. For example:
70|The Network Architecture of the Connection Machine CM-5|The Connection Machine Model CM-5 Supercomputer is a massively parallel computer system designed to offer performance in the range of 1 teraflops (10  12  floating-point operations per second). The CM-5 obtains its high performance while offering ease of programming, flexibility, and reliability. The machine contains three communication networks: a data network, a control network, and a diagnostic network. This paper describes the organization of these three networks and how they contribute to the design goals of the CM-5. 1 Introduction  In the design of a parallel computer, the engineering principle of economy of mechanism suggests that the machine should employ only a single communication network to convey information among the processors in the system. Indeed, many parallel computers contain only a single network: typically, a hypercube or a mesh. The Connection Machine Model CM-5 Supercomputer has three networks, however, and none is a hypercube or a mesh. This paper describes the...
71|A simple load balancing scheme for task allocation in parallel machines |A collection of local workpiles (task queues) and a simple load balancing scheme is well suited for scheduling tasks in shared memory parallel machines. Task scheduling on such machines has usually been done through a single, globally accessible, workpile. The scheme introduced in this paper achieves a balancing comparable to that of a global workpile, while minimizing the overheads. In many parallel computer architectures, each processor has some memory that it can access more efficiently, and so it is desirable that tasks do not mirgrate frequently. The load balancing is simple and distributed: Whenever a processor accesses its local workpile, it performs a balancing operation with probability inversely proportional to the size of its workpile. The balancing operation consists of examining the workpile of a random processor and exchanging tasks so as to equalize the size of the two workpiles. The probabilistic analysis of the performance of the load balancing scheme proves that each tasks in the system receives its fair share of computation time. Specifically, the expected size of each local task queue is within a small constant factor of the average, i.e. total number of tasks in the system divided by the number of processors. 1
72|Distributed Filaments: Efficient Fine-Grain Parallelism on a Cluster of Workstations|A fine-grain parallel program is one in which processes are typically small, ranging from a few to a few hundred instructions. Fine-grain parallelism arises naturally in many situations, such as iterative grid computations, recursive fork/join programs, the bodies of parallel FOR loops, and the implicit parallelism in functional or dataflow languages. It is useful both to describe massively parallel computations and as a target for code generation by compilers. However, fine-grain parallelism has long been thought to be inefficient due to the overheads of process creation, context switching, and synchronization. This paper describes a software kernel, Distributed Filaments (DF), that implements fine-grain parallelism both portably and efficiently on a workstation cluster. DF runs on existing, off-the-shelf hardware and software. It has a simple interface, so it is easy to use. DF achieves efficiency by using stateless threads on each node, overlapping communication and computation, emp...
74|Concert -- Efficient Runtime Support for Concurrent Object-Oriented Programming Languages on Stock Hardware|Inefficient implementations of global namespaces, message passing, and thread scheduling on stock multicomputers have prevented concurrent object-oriented programming (COOP) languages from gaining widespread acceptance. Recognizing that the architectures of stock multicomputers impose a hierarchy of costs for these operations, we have described a runtime system which provides different versions of each primitive, exposing performance distinctions for optimization. We confirm the advantages of a cost-hierarchy based runtime system organization by showing a variation of two orders of magnitude in version costs for a CM5 implementation. Frequency measurements based on COOP application programs demonstrate that a 39 % invocation cost reduction is feasible by simply selecting cheaper versions of runtime operations.  
75|  Computation Migration: Enhancing Locality for Distributed-Memory Parallel Systems |We describe computation migration, a new technique that is based on compile-time program transformations, for accessing remote data in a distributed-memory parallel system. In contrast with RPC-style access, where the access is performed remotely, and with data migration, where the data is moved so that it is local, computation migration moves part of the current thread to the processor where the data resides. The access is performed at the remote processor, and the migrated thread portion continues to run on that same processor; this makes subsequent accesses in the thread portion local. We describe an implementation of computation migration that consists of two parts: an implementation that migrates single activation frames, and a high-level language annotation that allows a programmer to express when migration is desired. We performed experiments using two applications; these experiments demonstrate that computation migration is a valuable alternative to RPC and data migration.  
76|Scheduling Large-Scale Parallel Computations on Networks of Workstations|Workstation networks are an underutilized yet valuable resource for solving large-scale parallel problems. In this paper, we present &#034;idle-initiated&#034; techniques for efficiently scheduling large-scale parallel computations on workstation networks. By &#034;idle-initiated,&#034; we mean that idle computers actively search out work to do rather than wait for work to be assigned. The idleinitiated scheduler operates at both the macro and the micro levels. On the macro level, a computer without work joins an ongoing parallel computation as a participant. On the micro level, a participant without work &#034;steals&#034; work from some other participant of the same computation. We have implemented these scheduling techniques in Phish, a portable system for running dynamic parallel applications on a network of workstations. 1 Introduction Even with the annual exponential improvements in microprocessor speed, a large body of problems cannot be solved in a reasonable time on a single computer. One method of reduc...
78|Studying Overheads in Massively Parallel Min/Max-Tree Evaluation (Extended Abstract)  (1994) |)  y Rainer Feldmann and Peter Mysliwietz and Burkhard Monien Email: chess@uni-paderborn.de Department of Mathematics and Computer Science, University of Paderborn, Germany Abstract  In this paper we study the overheads arising in our algorithm for distributed evaluation of Min/Max trees. The overheads are classified into search overhead, performance loss, and decrease of work load. Several mechanisms are investigated to cope with these overheads in order to achieve a high performance. We study a combination of local, medium range, and global load distribution strategies that does not only show a good behavior in terms of work load, but also has a positive influence on the search overhead. The efficient use of a virtual shared memory, that is distributed among the processors, shows also a big contribution to the overall performance of the system. A carefully restricted application of parallelism using an improved version of the Young Brothers Wait Concept (YBWC) leads to a perfect beha...
79|Communication Complexity for Parallel Divide-and-Conquer|This paper studies the relationship between parallel computation cost and communication cost for performing divide-and-conquer (D&amp;C) computations on a parallel system of p processors. The parallel computation cost is the maximal number of the D&amp;C nodes that any processor in the parallel system may expand, whereas the communication cost is the total number of cross nodes. A cross node is a node which is generated by one processor but expanded by another processor. A new scheduling algorithm is proposed, whose parallel computation cost and communication cost are at most  dN=pe and pdh, respectively, for any D&amp;C computation tree with N nodes, height h, and degree d. Also, lower bounds on the communication cost are derived. In particular, it is shown that for each scheduling algorithm and for each positive ffl C ! 1, which can be arbitrarily close to 0, there are values of N , h, d, p,  and ffl T (? 0), for which if the parallel computation cost is between N=p (the minimum) and (1 + ffl T ...
80|Massively Parallel Chess|Computer chess provides a good testbed for understanding dynamic MIMD-style computations. To investigate the programming issues, we engineered a parallel chess program called *Socrates, which running on the NCSA&#039;s 512 processor CM-5, tied for third in the 1994 ACM International Computer Chess Championship. *Socrates uses the Jamboree algorithm to search game trees in parallel and uses the Cilk 1.0 language and run-time system to express and to schedule the computation. In order to obtain good performance for chess, we use several mechanisms not directly provided by Cilk, such as aborting computations and directly accessing the active message layer to implement a global transposition table distributed across the processors. We found that we can use the critical path C and the total work W to predict the performance of our chess programs. Empirically *Socrates runs in time T ß 0:95C+1:09W=P on P processors. For best-ordered uniform trees of height  h and degree d the average available pa...
81|Enabling Primitives For Compiling Parallel Languages|This paper presents three novel languageimplementation primitives---lazy threads,stacklets, and synchronizers---andshows how they combine to provide a parallel call at nearly the efficiency of a sequential call. The central idea is to transform parallel calls into parallel-ready sequential calls. Excess parallelism degrades into sequential calls with the attendant efficient stack management and direct transfer of control and data, unless a call truly needs to execute in parallel, in which case it gets its own thread of control. We show how these techniques can be applied to distribute work efficiently on multiprocessors.
82|A Customizable Substrate for Concurrent Languages|We describe an approach to implementing a wide-range of concurrency paradigms in high-level (symbolic) programming languages. The focus of our discussion is sting,  a dialect of Scheme, that supports lightweight threads of control and virtual processors as first-class objects. Given the significant degree to which the behavior of these objects may be customized, we can easily express a variety of concurrency paradigms and linguistic structures within a common framework without loss of efficiency. Unlike parallel systems that rely on operating system services for managing concurrency, sting implements concurrency management entirely in terms of Scheme objects and procedures. It, therefore, permits users to optimize the runtime behavior of their applications without requiring knowledge of the underlying runtime system. This paper concentrates on (a) the implications of the design for building asynchronous concurrency structures, (b) organizing large-scale concurrent computations, and (c)...
83|Programming a Distributed System Using Shared Objects |Building the hardware for a high-performance distributed computer system is a lot easier than building its software. In this paper we describe a model for programming distributed systems based on abstract data types that can be replicated on all machines that need them. Read operations are done locally, without requiring network traffic. Writes can be done using a reliable broadcast algorithm if the hardware supports broadcasting; otherwise, a point-to-point protocol is used. We have built such a system based on the Amoeba microkernel, and implemented a language, Orca, on top of it. For Orca applications that have a high ratio of reads to writes, we have measured good speedups on a system with 16 processors.  
84|Formal Ontology and Information Systems|Research on ontology is becoming increasingly widespread in the computer science community, and its importance is being recognized in a multiplicity of research fields and application areas, including knowledge engineering, database design and integration, information retrieval and extraction. We shall use the generic term information systems, in its broadest sense, to collectively refer to these application perspectives. We argue in this paper that so-called ontologies present their own methodological and architectural peculiarities: on the methodological side, their main peculiarity is the adoption of a highly interdisciplinary approach, while on the architectural side the most interesting aspect is the centrality of the role they can play in an information system, leading to the perspective of ontology-driven information systems.
85|A translation approach to portable ontology specifications|To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse — definitions of classes, relations, functions, and other objects — is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms. 
86|WordNet: A Lexical Database for English|Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet 1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].
87|Toward Principles for the Design of Ontologies Used for Knowledge Sharing|Recent work in Artificial Intelligence is exploring the use of formal ontologies as a way of specifying content-specific agreements for the sharing and reuse of knowledge among software entities. We take an engineering perspective on the development of such ontologies. Formal ontologies are viewed as designed artifacts, formulated for specific purposes and evaluated against objective design criteria. We describe the role of ontologies in supporting knowledge sharing activities, and then present a set of criteria to guide the development of ontologies for these purposes. We show how these criteria are applied in case studies from the design of ontologies for engineering mathematics and bibliographic data. Selected design decisions are discussed, and alternative representation choices and evaluated against the design criteria.
88|Ontologies: Principles, methods and applications|This paper is intended to serve as a comprehensive introduction to the emerging field concerned with the design and use of ontologies. We observe that disparate backgrounds, languages, tools, and techniques are a major barrier to effective communication among people, organisations, and/or software systems. We show how the development and implementation of an explicit account of a shared understanding (i.e. an `ontology&#039;) in a given subject area, can improve such communication, which in turn, can give rise to greater reuse and sharing, inter-operability, and more reliable software. After motivating their need, we clarify just what ontologies are and what purposes they serve. We outline a methodology for developing and evaluating ontologies, first discussing informal techniques, concerning such issues as scoping, handling ambiguity, reaching agreement and producing de nitions. We then consider the bene ts of and describe, a more formal approach. We re-visit the scoping phase, and discuss the role of formal languages and techniques in the specification, implementation and evaluation of ontologies. Finally, we review the state of the art and practice in this emerging field,
89|Ontologies and knowledge bases: Towards a terminological clarification|The word “ontology ” has recently gained a good popularity within the knowledge engineering community. However, its meaning tends to remain a bit vague, as the term is used in very different ways. Limiting our attention to the various proposals made in the current debate in AI, we isolate a number of interpretations, which in our opinion deserve a suitable clarification. We elucidate the implications of such various interpretations, arguing for the need of clear terminological choices regarding the technical use of terms like “ontology”, “conceptualization ” and “ontological commitment”. After some comments on the use “Ontology ” (with the capital “o”) as a term which denotes a philosophical discipline, we analyse the possible confusion between an ontology intended as a particular conceptual framework at the knowledge level and an ontology intended as a concrete artifact at the symbol level, to be used for a given purpose. A crucial point in this clarification effort is the careful analysis of Gruber’ s definition of an ontology as a specification of a conceptualization. 1
90|Formal Ontology, Conceptual Analysis and Knowledge Representation|The purpose of this paper is to defend the systematic introduction of formal ontological principles in the current practice of knowledge engineering, to explore the various relationships between ontology and knowledge representation, and to present the recent trends in this promising research area. According to the &#034;modelling view&#034; of knowledge acquisition proposed by Clancey, the modeling activity must establish a correspondence between a knowledge base and two separate subsystems: the agent&#039;s behavior (i.e. the problem-solving expertize) and its own environment (the problem domain). Current knowledge modelling methodologies tend to focus on the former subsystem only, viewing domain knowledge as strongly dependent on the particular task at hand: in fact, AI researchers seem to have been much more interested in the nature of reasoning rather than in the nature of the real world. Recently, however, the potential value of task-independent knowlege bases (or &#034;ontologies&#034;) suitable to large scale integration has been underlined in many ways.  In this paper, we compare the dichotomy between reasoning and representation to the philosophical distinction between epistemology and ontology. We introduce the notion of the ontological level, intermediate between the epistemological and the conceptual level discussed by Brachman, as a way to characterize a knowledge representation formalism taking into account the intended meaning of its primitives. We then discuss some formal ontological distinctions which may play an important role for such purpose.   
91|Towards distributed use of large-scale ontologies|Large scale knowledge bases systems are difficult and expensive to construct. If we could share knowledge across systems, costs would be reduced. However, because knowledge bases are typically constructed from scratch, each with their own idiosyncratic structure, sharing is difficult. Recent research has focused on the use of ontologies to promote sharing. An ontology is a hierarchically structured set of terms for describing a domain that can be used as a skeletal foundation for a knowledge base. If two knowledge bases are built on a common ontology, knowledge can be more readily shared, since they share a common underlying structure. This paper outlines a set of desiderata for ontologies, and then describes how we have used a large-scale (50,000+ concept) ontology develop a specialized, domain-specific ontology semiautomatically. We then discuss the relation between ontologies and the process of developing a system, arguing that to be useful, an ontology needs to be created as a &#034;living document&#034;, whose development is tightly integrated with the system’s. We conclude with a discussion of Web-based ontology tools we are developing to support this approach.
92|Enterprise modeling|... This article motivates the need for enterprise models and introduces the concepts of generic and deductive enterprise models. It  reviews research to date on enterprise modeling and considers in detail the Toronto virtual enterprise effort at the University of Toronto.
93|Part-Whole Relations in Object-Centered Systems: An Overview|Knowledge bases, data bases and object-oriented systems (referred to in the paper as Object-Centered systems) all rely on attributes as the main construct used  to associate properties to objects; among these, a fundamental role is played by  the so-called part-whole relation. The representation of such a structural information usually requires a particular semantics together with specialized inference and  update mechanisms, but rarely do current modeling formalisms and methodologies  give it a specific  &#034;first-class&#034; dignity.  The main thesis of this paper is that the part-whole relation cannot simply be  considered as an ordinary attribute, its specific ontological nature requires to be  understood and integrated within data modeling formalisms and methodologies.  On the basis of such an ontological perspective, we survey the conceptual modeling  issues involving part-whole relations, and the various modeling frameworks provided  by knowledge representation and object-oriented formalisms.   
94|Semantic Matching: Formal Ontological Distinctions for Information Organization, Extraction, and Integration|The task of information extraction can be seen as a problem of semantic  matching between a user-defined template and a piece of information written  in natural language. To this purpose, the ontological assumptions of the  template need to be suitably specified, and compared with the ontological implications  of the text. So-called &#034;ontologies&#034;, consisting of theories of various  kinds expressing the meaning of shared vocabularies, begin to be used for this  task. This paper addresses the theoretical issues related to the design and use of  such ontologies for purposes of information retrieval and extraction. After a discussion  on the nature of semantic matching within a model-theoretical framework,  we introduce the subject of Formal Ontology, showing how the notions of  parthood, integrity, identity, and dependence can be of help in understanding,  organizing and formalizing fundamental ontological distinctions. We present  then some basic principles for ontology design, and we illustrate a preliminary  proposal for a top-level ontology develped according to such principles. As a  concrete example of ontology-based information retrieval, we finally report an  ongoing experience of use of a large linguistic ontology for the retrieval of object-oriented software components. 
95|The MOMIS approach to Information Integration|Introduction The web explosion, both at internet and intranet level, has transformed the electronic information system from single isolated node to an entry points into a worldwide network of information exchange and business transactions. Business and commerce has taken the opportunity of the new technologies to define the e-commerce activity. An electronic marketplace represents a virtual place where buyers and sellers meet to exchange goods and services, by sharing information that is often obtained as hypertext catalogs from different companies. Companies have equipped themselves with data storing systems building up informative systems containing data which are related one another, but which are often redundant, heterogeneous and not always substantial. The problems that have to be faced in this field are mainly due to both structural and application heterogeneity, as well as to the lack of a common ontology, causing semantic differences between information sources. Moreo
97|Ontology Reuse and Application|In this paper, we describe an investigation into the reuse and application  of an existing ontology for the purpose of specifying and formally  developing software for aircraft design. Our goals were to clearly identify  the processes involved in the task, and assess the cost-effectiveness  of reuse. Our conclusions are that (re)using an ontology is far from  an automated process, and instead requires significant effort from the  knowledge engineer. We describe and illustrate some intrinsic properties  of the ontology translation problem and argue that fully automatic  translators are unlikely to be forthcoming in the foreseeable future. Despite  the effort involved, our subjective conclusions are that in this case  knowledge reuse was cost-effective, and that it would have taken significantly  longer to design the knowledge content of this ontology from  scratch in our application. Our preliminary results are promising for  achieving larger-scale knowledge reuse in the future.
98|Domain Specific Ontologies for Semantic Information Brokering on the Global Information Infrastructure|Recent emerging technologies such as internetworking and the World Wide Web (WWW) have significantly expanded the types, availability, and volume of data accessible to an information management system. In this new environment it is imperative to view an information source at the level of its relevant semantic concepts. We propose that these semantic concepts be chosen from pre-existing domain specific ontologies. Domain specific ontologies are used as tools/mechanisms for specifying the ontological commitments or agreements between information users and providers on the information infrastructure. We use domain specific ontologies to tackle the information explosion by the: (a) Re-use and organization of knowledge in pre-existing real world ontologies, achieved by mapping semantic concepts in the ontologies to data structures in the underlying repositories; and (b) Knowledge integration and development of mechanisms to translate information requests across ontologies. We thus provide s...
99|A Connection Based Approach to Commonsense Topological Description and Reasoning|The standard mathematical approaches to topology, point-set topology and algebraic  topology, treat points as the fundamental, undefined entities, and construct extended  spaces as sets of points with additional structure imposed on them. Point-set topology  in particular generalises the concept of a `space&#039; far beyond its intuitive meaning. Even  algebraic topology, which concentrates on spaces built out of `cells&#039; topologically equivalent  to n-dimensional discs, concerns itself chiefly with rather abstract reasoning concerning  the association of algebraic structures with particular spaces, rather than the kind of  topological reasoning which is required in everyday life, or which might illuminate the  metaphorical use of topological concepts such as `connection&#039; and `boundary&#039;.  This paper explores an alternative to these approaches, RCC theory, which takes  extended spaces (`regions&#039;) rather than points as fundamental. A single relation, C (x; y)  (read `Region x connects with reg...
100|Ontological Tools for Geographic Representation|Abstract. This paper is concerned with certain ontological issues in the foundations of geographic representation. It sets out what these basic issues are, describes the tools needed to deal with them, and draws some implications for a general theory of spatial representation. Our approach has ramifications in the domains of mereology, topology, and the theory of location, and the question of the interaction of these three domains within a unified spatial representation theory is addressed. In the final part we also consider the idea of nonstandard geographies, which may be associated with geography under a classical conception in the same sense in which non-standard logics are associated with classical logic. 1.
101|The Basic Tools of Formal Ontology|The term ‘formal ontology ’ was first used by the philosopher Edmund Husserl in his Logical Investigations to signify the study of those formal structures and relations – above all relations of part and whole – which are exemplified in the subject-matters of the different material sciences. We follow Husserl in presenting the basic concepts of formal ontology as falling into three groups: the theory of part and whole, the theory of dependence, and the theory of boundary, continuity and contact. These basic concepts are presented in relation to the problem of providing an account of the formal ontology of the mesoscopic realm of everyday experience, and specifically of providing an account of the concept of individual substance.
102|An Ontological Theory of Physical Objects|We discuss an approach to a theory of physical objects and present a logical theory based on a fundamental distinction between objects and their substrates, i.e. chunks of matter and regions of space. The purpose is to establish the basis of a general ontology of space, matter and physical objects for the domain of mechanical artifacts. An extensional mereological framework is assumed for substrates, whereas physical objects are allowed to change their spatial and material substrate while keeping their identity. Besides the parthood relation, simple self-connected region and congruence (or sphere) are adopted as primitives for the description of space. Only threedimensional regions are assumed in the domain. This paper is a revision and slight modification of [Borgo et al. 1996]. 1.
103|Spatial Entities|this paper. However one basic motivation seems easily available. Without going into much detail (see Varzi [1994]), the point is simply that mereological reasoning by itself cannot do justice to the notion of a whole---a self-connected whole, such as a stone or a rope, as opposed to a scattered entity made up of several disconnected parts, such as a broken glass or an archipelago. Parthood is a relational concept, wholeness a global property. And in spite of a widespread tendency to present mereology as a theory of parts and wholes, the latter notion (in its ordinary understanding) cannot be explained in terms of the former. For every whole there is a set of (possibly potential) parts; for every set of parts (i.e., arbitrary objects) there is in principle a complete whole, viz. its mereological sum, or fusion. But there is no way, mereologically, to draw a distinction between &#034;good&#034; and &#034;bad&#034; wholes; there is no way one can rule out wholes consisting of widely scattered or ill assorted entities (the sum consisting of our four eyes and Caesar&#039;s left foot) by reasoning exclusively in terms of parthood. If we allow for the possibility of scattered entities, then we lose the possibility of discriminating them from integral, connected wholes. On the other hand, we cannot just keep the latter without some means of discriminating them from the former.
104|The Ontological Nature of Subject Taxonomies|. Subject based classification is an important part of information retrieval, and has a long history in libraries, where a subject taxonomy was used to determine the location of books on the shelves. We have been studying the notion of subject itself, in order to determine a formal ontology of subject for a large scale digital library card catalog system. Deep analysis reveals a lot of ambiguity regarding the usage of subjects in existing systems and terminology, and we attempt to formalize these notions into a single framework for representing it. 1 Introduction Until recently, library card catalog systems have worked successfully because the amount of material referenced by the system was fairly small. Digital libraries, both formal as in the United States National Digital Library, or informal as in the World Wide Web, promise the potential of billions of electronic documents, and will render the existing card catalog paradigm useless. It has begun already, as web users find themsel...
105|Logical Modelling of Product Knowledge: Towards a Well-Founded Semantics for STEP|The main purpose of the STEP standard is to make possible the integration of product knowledge within the whole enterprise. Under this perspective, the mere exchange of geometric data is not enough, and qualitative knowledge of different kinds needs to be acquired and represented. Here, however, serious semantic problems arise, since the interpretation of the modelling primitives proposed by the standard heavily relies on implicit background knowledge. This problem has been recently underlined in [Metzger 1996], where it is argued that this background knowledge is stable enough and well agreed-upon only in the case of low-level geometric concepts. In the case of more abstract geometric concepts like design features, or non-geometric concepts like part or action, their meaning is not clear enough to be effectively shared by different application protocols. As a result, different interpretations are assumed for the same term in differ
106|Basic Problems of Mereotopology|Mereotopology is today regarded as a major tool for ontological analysis,  and for many good reasons. There are, however, a number of open questions that call  for an answer. Some of them are philosophical, others have direct import for applications,  but all are crucial for a proper assessment of the strengths and limits of  mereotopology. This paper is an attempt to put some order into this still untamed area  of research. I will not attempt any answers. But I shall try to give an idea of the problems,  and of their relevance for the systematic development of formal ontological  theories.
107|The Neutral Representation Project|The evolving complexity of many modern artifacts,  such as aircraft, has led to a serious fragmentation of  knowledge among software systems required for their  design and manufacture. In the case of aircraft design,  views of the same generic design knowledge are redundantly  encoded in multiple software systems, each  system using its own idiosyncractic ontology, and each  system containing that knowledge in an implicit, taskand  vendor-specific form. This situation is expensive,  due to high cost of developing from scratch, maintaining  and keeping synchronized the many systems used  in design.  Boeing&#039;s &#034;Neutral Representation&#034; project aims to address  these concerns by prototyping languages and  methods for making these underlying ontologies and  knowledge explicit, and hence more sharable and  maintainable. We are approaching this goal through  three tasks: Building explicit, neutral, machinesensible  representations of design knowledge; structuring  that knowledge into reusable components, indexed  by the ontologies which they use; and linking those  representations with existing design systems. In this  paper we present the work done this year, and discuss  issues related to ontological engineering and knowledge  sharing which have arisen.  
108|The Cricket Location-Support System|This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, locationdependent applications. It allows applications running on mobile and static nodes to learn their physical location by using listeners that hear and analyze information from beacons spread throughout the building. Cricket is the result of several design goals, including user privacy, decentralized administration, network heterogeneity, and low cost. Rather than explicitly tracking user location, Cricket helps devices learn where they are and lets them decide whom to advertise this information to; it does not rely on any centralized management or control and there is no explicit coordination between beacons; it provides information to devices regardless of their type of network connectivity; and each Cricket device is made from off-the-shelf components and costs less than U.S. $10. We describe the randomized algorithm used by beacons to transmit information, the use of concurrent radio and ultrasonic signals to infer distance, the listener inference algorithms to overcome multipath and interference, and practical beacon configuration and positioning techniques that improve accuracy. Our experience with Cricket shows that several location-dependent applications such as in-building active maps and device control can be developed with little effort or manual configuration.  1 
109|RADAR: an in-building RF-based user location and tracking system|The proliferation of mobile computing devices and local-area wireless networks has fostered a growing interest in location-aware systems and services. In this paper we present RADAR, a radio-frequency (RF) based system for locating and tracking users inside buildings. RADAR operates by recording and processing signal strength information at multiple base stations positioned to provide overlapping coverage in the area of interest. It employs techniques that combine empirical measurements with signal propagation modeling to enable location-aware services and applications. We present concrete experimental results that demonstrate the feasibility of using RADAR to estimate user location with a high degree of accuracy. 1
110|The Active Badge Location System|cation is the `pager system&#039;. In order to locate a person a signal is sent out by a central facility that addresses a particular receiver unit (beeper) and produces an audible signal. In addition, it may display a number to which the called-party should phone back (some systems allow a vocal message to be conveyed about the call-back number). It is then up to the recipient to use the conventional telephone system to call-back confirming the signal and determine the required action. Although useful in practice there are still circumstances where it is not ideal. For instance, if the called party does not reply the controller has no idea if they: 1) are in an area where the signal does not penetrate 2) have been completely out of the area for some time 3) have been too busy to reply or 4) have misheard or misread the call-back number. Moreover, in the case where there are a number of people who could respond to a crisis situation, it is not known which one is the nearest to the crisis an
111|The Anatomy of a Context-Aware Application|We describe a platform for context-aware computing which enables applications to follow mobile users as they move around a building. The platform is particularly suitable for richly equipped, networked environments. The only item a user is required to carry is a small sensor tag, which identifies them to the system and locates them accurately in three dimensions. The platform builds a dynamic model of the environment using these location sensors and resource information gathered by telemetry software, and presents it in a form suitable for application programmers. Use of the platform is illustrated through a practical example, which allows a user&#039;s current working desktop to follow them as they move around the environment.
112|A New Location Technique for the Active Office|Configuration of the computing and communications systems found at home and in the workplace is a complex task that currently requires the  attention of the user. Recently, researchers have begun to examine computers that would autonomously change their functionality based on  observations of who or what was around them. By determining their context, using input from sensor systems distributed throughout the  environment, computing devices could personalize themselves to their current user, adapt their behavior according to their location, or react  to their surroundings. The authors present a novel sensor system, suitable for large-scale deployment in indoor environments, which allows the  locations of people and equipment to be accurately determined. We also describe some of the context-aware applications that might make use  of this fine-grained location information.
113|A Self-Configuring Resolver Architecture for Resource Discovery and Routing in Device Networks|Network environments of the future will be characterized by a variety of mobile and wireless devices in addition to general-purpose computers. Such environments display a degree of dynamism not usually seen in traditional wired networks due to mobility of nodes and services, rapid uctuations in performance, and node failures. This combination of heterogeneity and dynamism makes it hard for applications to discover the network locations of services that best satisfy their needs. This thesis presents a communication system that allows network applications to send messages by describing the attributes of the intended destinations for their messages, rather than by explicitly listing the network locations (e.g., IP addresses) of the message destinations. Senders are thus relieved from having to know in priori the destination network locations, and receiver nodes are determined during message delivery time by the system, allowing them to be mobile, grouped, or dynamically adapted to differ...
114|The Unix Time-Sharing System|Unix is a general-purpose, multi-user, interactive operating system for the larger Digital Equipment Corporation PDP-11 and the Interdata 8/32 computers. It offers a number of features seldom found even in larger operating systems, including i A hierarchical file system incorporating demountable volumes, ii Compatible file, device, and inter-process I/O, iii The ability to initiate asynchronous processes, iv System command language selectable on a per-user basis, v Over 100 subsystems including a dozen languages, vi High degree of portability. This paper discusses the nature and implementation of the file system and of the user command interface. I.
115|A System for Typesetting Mathematics|This paper describes the design and implementation of a system for typesetting mathematics.  The language has been designed to be easy to learn and to use by people (for example,  secretaries and mathematical typists) who know neither mathematics nor typesetting. Experience  indicates that the language can be learned in an hour or so, for it has few rules and fewer exceptions.  For typical expressions, the size and font changes, positioning, line drawing, and the like  necessary to print according to mathematical conventions are all done automatically. For example,  the input  sum from i=0 to infinity x sub i = pi over 2  produces  i =0  S  x i =  2  p __  The syntax of the language is specified by a small context-free grammar; a compilercompiler  is used to make a compiler that translates this language into typesetting commands.  Output may be produced on either a phototypesetter or on a terminal with forward and reverse  half-line motions. The system interfaces directly with text for...
116|Portability of C Programs and the UNIX System|Computer programs are portable to the extent that they can be moved to new computing environments with much less effort than it would take to rewrite them. In the limit, a program is perfectly portable if it can be moved at will with no change whatsoever. Recent C language extensions have made it easier to write portable programs. Some tools have also been developed that aid in the detection of nonportable constructions. With these tools many programs have been moved from the PDP-11 on which they were developed to other machines. In particular, the UNIX operating system and most of its software have been transported to the Interdata 8/32. The source-language representation of most of the code involved is identical in all environments. 
117|The Protection of Information in Computer Systems|This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures--whether hardware or software--that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysis of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading. Glossary The following glossary provides, for reference, brief definitions for several terms as used in this paper in the context of protecting information in computers. Access The ability to make use of information stored in a computer system. Used frequently as a verb, to the horror of grammarians. Access control list A list of principals that are authorized to have access to some object. Authenticate To verify the identity of a person (or other agent external to the protection system) making a request.
118|A Note on the Confinement Problem|This not explores the problem of confining a program during its execution so that it cannot transmit information to any other program except its caller. A set of examples attempts to stake out the boundaries of the problem. Necessary conditions for a solution are stated and informally justified.
119|Programming Semantics for Multiprogrammed Computations|... between an assembly language and an advanced algebraic language.  
120|Protection and the control of information sharing in Multics|This document was originally prepared off-line. This file is the result of scan, OCR, and manual touchup, starting
121|Protection|The following paper by Butler Lampson has been frequently referenced. Because the original is not widely available, we are reprinting it here. If the paper is referenced in published work,
122|A hardware architecture for implementing protection rings|Protection of computations and information is an important aspect of a computer utility. In a system which usessegmentation as a memory addressing scheme, protection can be achieved in part by associating concentric rings of decreasing access privilege with a computation. This paper describes hardware processor mechanisms for implementing these rings of protection. The mechanisms allow cross-ring calls and subsequent returns to occur without trapping to the supervisor. Automatic hardware validation of referencesacross ring boundaries is also performed. Thus, a call by a user procedure to a protected subsystem (including the the supervisor) is identical to a call to a companion user procedure. The mechanisms of passing and referencing arguments are the same in both cases as well.
123|The Multics Virtual Memory: Concepts and Design|As experience with use of on-line operating systems has grown, the need to share information among system users has become increasingly apparent. Many contemporary systems permit some degree of sharing. Usually, sharing is accomplished by allowing several users to share data via input and output of information stored in files kept in secondary storage. Through the use of segmentation, however, Multics provides direct hardware addressing by user and system programs of all information, independent of its physical storage location. Information is stored in segments each of which is potentially sharable and carries its own independent attributes of size and access privilege. Here, the design and implementation considerations of segmentation and sharing in Multics are first discussed under the assumption that all information resides in a large, segmented main memory. Since the size of main memory on contemporary systems is rather limited, it is then shown how the Multics software achieves the effect of a large segmented main memory through the use of the Honeywell 645 segmentation and paging hardware.
124|A User Machine in a Time-Sharing System|This paper describes the design of the computer seen by a machine-language programmer in a timesharing system developed at the University of California at Berkeley. Some of the instructions in this machine are executed by the hardware, and some are implemented by software. The user, however, thinks of them all as part of his machine, a machine having extensive and unusual capabilities, many of which might be part of the hardware of a (considerably more expensive) computer. Among the important features of the machine are the arithmetic and string manipulation instructions, the very general memory allocation and configuration mechanism, and the multiple processes which can be created by the program. Facilities are provided for communication among these processes and for the control of exceptional conditions. The input-output system is capable of handling all of the peripheral equipment in a uniform and convenient manner through files having symbolic names. Programs can access files belonging to a number of people, but each person can protect his own files from unauthorized access by others. Some mention is made at various points of the techniques of implementation, but the main emphasis is on the appearance of the user&#039;s machine. 
125|Ongoing research and development on information protection|Many individuals involved in the projects described here spent time patiently explaining their activities to me, putting together written descriptions for me to work from, and reviewing early drafts of my (often muddled) writeups of their research. Thanks are due all of them, but responsibility for mistakes and omissions is my own.
126|Term Rewriting Systems|Term Rewriting Systems play an important role in various areas, such as abstract data type specifications, implementations of functional programming languages and automated deduction. In this chapter we introduce several of the basic comcepts and facts for TRS&#039;s. Specifically, we discuss Abstract Reduction Systems
128|Process algebra for synchronous communication|Within the context of an algebraic theory of processes, an equational specification of process cooperation is provided. Four cases are considered: free merge or interleaving, merging with communication, merging with mutual exclusion of tight regions, and synchronous process cooperation. The rewrite system behind the communication algebra is shown to be confluent and terminating (modulo its permutative reductions). Further, some relationships are shown to hold between the four concepts of merging. © 1984 Academic Press, Inc.
129|PROVING TERMINATION WITH MULTISET ORDERINGS  |A common tool for proving the termination of programs is the well-founded set, a set ordered in such a way as to admit no infinite descending sequences. The basic approach is to find a termination function that maps the values of the program variables into some well-founded set, such that the value of the termination function is continually reduced throughout the computation. All too often, the termination functions required are difficult to find and are of a complexity out of proportion to the program under consideration. However, by providing more sophisticated well-founded sets, the corresponding termination functions can be simplified. Given a well-founded set S, we consider multisets over S, &#034;sets&#034; that admit multiple occurrences of elements taken from S. We define an ordering on all finite multisets over S that is induced by the given ordering on S. This multiset ordering is shown to be well-founded. The value of the multiset ordering is that it permits the use of relatively simple and intuitive termination functions in otherwise difficult termination proofs. In particular, we apply the multiset ordering to prove the termination of production systems, programs defined in terms of sets of rewriting rules.
130|Equations and rewrite rules: a survey|bY
131|Confluence of Conditional Rewrite Systems |Conditional rewriting has been studied both from the point of view of algebraic data type specifications and as a computational paradigm combining logic and functional programming. An important issue, in either case, is determining whether a rewrite system has the Church-Rosser, or confluence, property. In this paper, we settle negatively the question whether &#034;joinabihty of critical pairs&#034; is, in general, sufficient for confluence of terminating conditional systems. We review known sufficient conditions for confluence, and also prove two new positive results for systems having critical pairs and arbitrarily big terms in conditions. 
132|What&#039;s so special about Kruskal&#039;s Theorem AND THE ORDINAL G0? A SURVEY OF SOME RESULTS IN PROOF THEORY| This paper consists primarily of a survey of results of Harvey Friedman about some proof theoretic aspects of various forms of Kruskal’s tree theorem, and in particular the connection with the ordinal G0. We also include a fairly extensive treatment of normal functions on the countable ordinals, and we give a glimpse of Veblen hierarchies, some subsystems of second-order logic, slow-growing and fast-growing hierarchies including Girard’s result, and Goodstein sequences. The central theme of this paper is a powerful theorem due to Kruskal, the “tree theorem”, as well as a “finite miniaturization ” of Kruskal’s theorem due to Harvey Friedman. These versions of Kruskal’s theorem are remarkable from a proof-theoretic point of view because they are not provable in relatively strong logical systems. They are examples of so-called “natural independence phenomena”, which are considered by most logicians as more natural than the metamathematical incompleteness results first discovered by Gödel. Kruskal’s tree theorem also plays a fundamental role in computer science, because it is one of the main tools for showing that certain orderings on trees are well founded. These orderings play a crucial role in proving the termination of systems of rewrite rules and the correctness of Knuth-Bendix completion procedures. There is also a close connection between a certain infinite countable ordinal called G0 and Kruskal’s theorem. Previous definitions of the function involved in this connection are known to be incorrect, in that, the function is not monotonic. We offer a repaired definition of this function, and explore briefly the consequences of its existence.  
133|The Google File System |We have designed and implemented the Google File Sys-tem, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous dis-tributed file systems, our design has been driven by obser-vations of our application workloads and technological envi-ronment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore rad-ically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our ser-vice as well as research and development efforts that require large data sets. The largest cluster to date provides hun-dreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use. Categories and Subject Descriptors D [4]: 3—Distributed file systems
134|Scale and performance in a distributed file system|The Andrew File System is a location-transparent distributed tile system that will eventually span more than 5000 workstations at Carnegie Mellon University. Large scale affects performance and complicates system operation. In this paper we present observations of a prototype implementation, motivate changes in the areas of cache validation, server process structure, name translation, and low-level storage representation, and quantitatively demonstrate Andrew’s ability to scale gracefully. We establish the importance of whole-file transfer and caching in Andrew by comparing its performance with that of Sun Microsystem’s NFS tile system. We also show how the aggregation of files into volumes improves the operability of the system.
135|GPFS: A Shared-Disk File System for Large Computing Clusters|GPFS is IBM&#039;s parallel, shared-disk file system for cluster computers, available on the RS/6000 SP parallel supercomputer and on Linux clusters. GPFS is used on many of the largest supercomputers in the world. GPFS was built on many of the ideas that were developed in the academic community over the last several years, particularly distributed locking and recovery technology. To date it has been a matter of conjecture how well these ideas scale. We have had the opportunity to test those limits in the context of a product that runs on the largest systems in existence. While in many cases existing ideas scaled well, new approaches were necessary in many key areas. This paper describes GPFS, and discusses how distributed locking and recovery techniques were extended to scale to large clusters.
136|Serverless Network File Systems|In this paper, we propose a new paradigm for network file system design, serverless network file systems. While traditional network file systems rely on a central server machine, a serverless system utilizes workstations cooperating as peers to provide all file system services. Any machine in the system can store, cache, or control any block of data. Our approach uses this location independence, in combination with fast local area networks, to provide better performance and scalability than traditional file systems. Further, because any machine in the system can assume the responsibilities of a failed component, our serverless design also provides high availability via redundant data storage. To demonstrate our approach, we have implemented a prototype serverless network file system called xFS. Preliminary performance measurements suggest that our architecture achieves its goal of scalability. For instance, in a 32-node xFS system with 32 active clients, each client receives nearly as much read or write throughput as it would see if it were the only active client. 
137| Frangipani: A Scalable Distributed File System |The ideal distributed file system would provide all its users with coherent, shared access to the same set of files,yet would be arbitrarily scalable to provide more storage space and higher performance to a growing user community. It would be highly available in spite of component failures. It would require minimal human administration, and administration would not become more complex as more components were added. Frangipani is a new file system that approximates this ideal, yet was relatively easy to build because of its two-layer structure. The lower layer is Petal (described in an earlier paper), a distributed storage service that provides incrementally scalable, highly available, automatically managed virtual disks. In the upper layer, multiple machines run the same Frangipani file system code on top of a shared Petal virtual disk, using a distributed lock service to ensure coherence. Frangipaniis meant to run in a cluster of machines that are under a common administration and can communicate securely. Thus the machines trust one another and the shared virtual disk approach is practical. Of course, a Frangipani file system can be exported to untrusted machines using ordinary network file access protocols. We have implemented Frangipani on a collection of Alphas running DIGITAL Unix 4.0. Initial measurements indicate that Frangipani has excellent single-server performance and scales well as servers are added.  
138|A cost-effective, high-bandwidth storage architecture|(NASD) storage architecture, prototype implementations oj NASD drives, array management for our architecture, and three,filesystems built on our prototype. NASD provides scal-able storage bandwidth without the cost of servers used primarily,fijr trut&amp;rring data from peripheral networks (e.g. SCSI) to client networks (e.g. ethernet). Increasing datuset sizes, new attachment technologies, the convergence of peripheral and interprocessor switched networks, and the increased availability of on-drive transistors motivate and enable this new architecture. NASD is based on four main principles: direct transfer to clients, secure interfaces via cryptographic support, asynchronous non-critical-path oversight, and variably-sized data objects. Measurements of our prototype system show that these services can be cost-#ectively integrated into a next generation disk drive ASK. End-to-end measurements of our prototype drive andfilesys-terns suggest that NASD cun support conventional distrib-uted filesystems without per$ormance degradation. More importantly, we show scaluble bandwidth for NASD-special-ized filesystems. Using a parallel data mining application, NASD drives deliver u linear scaling of 6.2 MB/s per client-drive pair, tested with up to eight pairs in our lab.
140|Swift: Using distributed disk striping to provide high I/O data rates|We present an I/O architecture, called Swift, that addresses the problem of data rate mismatches between the requirements of an application, storage devices, and the interconnection medium. The goal of Swift is to support high data rates in general purpose distributed systems. Swift uses a high-speed interconnection medium to provide high data rate transfers by using multiple slower storage devices in parallel. It scales well when using multiple storage devices and interconnections, and can use any appropriate storage technology, including high-performance devices such as disk arrays. To address the problem of partial failures, Swift stores data redundantly. Using the UNIX operating system, we have constructed a simplified prototype of the Swift architecture. The prototype provides data rates that are significantly faster than access to the local SCSI disk, limited by the capacity of a single Ethernet segment, or in the case of multiple Ethernet segments by the ability of the client to drive them. We have constructed a simulation model to demonstrate how the Swift architecture can exploit advances in processor, communication and storage technology. We consider the effects of processor speed, interconnection capacity, and multiple storage agents on the utilization of the components and the data rate of the system. We show that the data rates scale well in the number of storage devices, and that by replacing the most highly stressed components by more powerful ones the data rates of the entire system increase significantly.
141|The Global File System|The Global File System (GFS) is a prototype design for a distributed file system in which cluster nodes physically share storage devices connected via a network like Fibre Channel. Networks and network attached storage devices have advanced to a level of performance and extensibility that the once believed disadvantages of “shared disk ” architectures are no longer valid. This shared storage architecture attempts to exploit the sophistication of device technologies where as the client–server architecture diminishes a device’s role to a simple components. GFS distributes the file system responsibilities across the processing nodes, storage across the devices, and file system resources across the entire storage pool. GFS caches data on the storage devices instead of the main memories of the machines. Consistency is established by using a locking mechanism maintained by the storage device controllers to facilitate atomic read–modify– write operations. The locking mechanism is being prototyped on Seagate disks drives and Ciprico disk arrays. GFS is implemented in the Silicon Graphics IRIX operating system and is accessed using standard Unix commands and utilities.
142|Distributed Database Systems |this article, we discuss the fundamentals of distributed DBMS technology. We address the data distribution and architectural design issues as well as the algorithms that need to be implemented to provide the basic DBMS functions such as query processing, concurrency control, reliability, and replication control.
143|An almost ideal demand system|prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at
144|The empirical case for two  systems of reasoning|Distinctions have been proposed between systems of reasoning for centuries. This article distills  properties shared by many of these distinctions and characterizes the resulting systems in light of  recent findings and theoretical developments. One system is associative because its computations reflect similarity structure and relations of temporal contiguity. The other is “rule based” because it operates on symbolic structures that have logical content and variables and because its computations have the properties that are normally assigned to rules. The systems serve complementary functions and can simultaneously generate different solutions to a reasoning problem. The rule-based system can suppress the associative system but not completely inhibit it. The article reviews evidence in favor of the distinction and its characterization. 
145|The principles of psychology|This Thesis is brought to you for free and open access. It has been accepted for inclusion in University Honors Theses by an authorized administrator of
146|Hierarchical mixtures of experts and the EM algorithm|We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM’s). Learning is treated as a max-imum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parame-ters of the architecture. We also develop an on-line learning algorithm in which the pa-rameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain. 
147|Controlled and automatic human information processing|A two-process theory of human information processing is proposed and applied to detection, search, and attention phenomena. Automatic processing is activa-tion of a learned sequence of elements in long-term memory that is initiated by appropriate inputs and then proceeds automatically—without subject control, without stressing the capacity limitations of the system, and without necessarily demanding attention. Controlled processing is a temporary activation of a se-quence of elements that can be set up quickly and easily but requires attention, is capacity-limited (usually serial in nature), and is controlled by the subject. A series of studies using both reaction time and accuracy measures is presented, which traces these concepts in the form of automatic detection and controlled, search through the areas of detection, search, and attention. Results in these areas are shown to arise from common mechanisms. Automatic detection is shown to develop following consistent mapping of stimuli to responses over trials. Controlled search is utilized in varied-mapping paradigms, and in our studies, it takes the form of serial, terminating search. The approach resolves a number of apparent conflicts in the literature.
148|Toward an instance theory of automatization|This article presents a theory in which automatization is construed as the acquisition of a domain-specific knowledge base, formed of separate representations, instances, of each exposure to the task. Processing is considered automatic if it relies on retrieval of stored instances, which will occur only after practice in a consistent environment. Practice is important because it increases the amount retrieved and the speed of retrieval; consistency is important because it ensures that the retrieved instances will be useful. The theory accounts quantitatively for the power-function speed-up and predicts a power-function reduction in the standard deviation that is constrained to have the same exponent as the power function for the speed-up. The theory accounts for qualitative properties as well, explaining how some may disappear and others appear with practice. More generally, it provides an alternative to the modal view of automaticity, arguing that novice performance is limited by a lack of knowledge rather than a scarcity of resources. The focus on learning avoids many problems with the modal view that stem from its focus on resource limitations. Automaticity is an important phenomenon in everyday men-tal life. Most of us recognize that we perform routine activities quickly and effortlessly, with little thought and conscious aware-ness--in short, automatically (James, 1890). As a result, we of-ten perform those activities on &amp;quot;automatic pilot &amp;quot; and turn our minds to other things. For example, we can drive to dinner while conversing in depth with a visiting scholar, or we can make coffee while planning dessert. However, these benefits may be offset by costs. The automatic pilot can lead us astray, caus-ing errors and sometimes catastrophes (Reason &amp; Myceilska, 1982). If the conversation is deep enough, we may find ourselves and the scholar arriving at the office rather than the restaurant, or we may discover that we aren&#039;t sure whether we put two or three scoops of coffee into the pot. Automaticity is also an important phenomenon in skill acqui-sition (e.g., Bryan &amp; Harter, 1899). Skills are thought to consist largely of collections of automatic processes and procedures
149|Analogical Mapping by Constraint Satisfaction|A theory of analogical mopping between source and target analogs based upon interacting structural, semantic, and pragmatic constraints is proposed here. The structural constraint of fsomorphfsm encourages mappings that maximize the consistency of relational corresondences between the elements of the two analogs. The constraint of semantic similarity supports mapping hypotheses to the degree that mapped predicates have similar meanings. The constraint of pragmatic centrality fovors mappings involving elements the analogist believes to be important in order to achieve the purpose for which the anology Is being used. The theory is implemented in a computer progrom called ACME (Analogical Constraint Mapping Engine), which represents constraints by means of a network of supporting and competing hypotheses regarding what elements to map. A coop erative algorithm for parallel constraint satisfaction identifies mapping hypotheses that collectively represent the overall mapping that best fits the interactlng constraints. ACME has been applied to a wide range of examples that include problem analogies, analogical arguments, explanatory analogies, story analogies, formal analogies, and metaphors. ACME is sensitive to semantic and prag matic information if it is available,.and yet able to compute mappings between formally isomorphic analogs without any similar or identical elements. The theory Is able to account for empirical findings regarding the impact of consistenty and similarity on human processing of analogies.
151|Similarity and induction|An argument is categorical if its premises and conclusion are of the form All members ofC have property F, where C is a natural category like FALCON or BIRD, and P remains the same across premises and conclusion. An example is Grizzly bears love onions. Therefore, all bears love onions. Such an argument is psychologically strong to the extent that belief in its premises engenders belief in its conclusion. A subclass of categorical arguments is examined, and the following hypothesis is advanced: The strength of a categorical argument increases with (a) the degree to which the premise categories are similar to the conclusion category and (b) the degree to which the premise categories are similar to members of the lowest level category that includes both the premise and the conclusion categories. A model based on this hypothesis accounts for 13 qualitative phenomena and the quantitative results of several experiments. The Problem of Argument Strength Fundamental to human thought is the confirmation relation, joining sentences P,... Pn to another sentence C just in case belief in the former leads to belief in the latter. Theories of confirmation may be cast in the terminology of argument strength,
152|A rational analysis of the selection task as optimal data selection|Human reasoning in hypothesis-testing tasks like Wason&#039;s (1966, 1968) selection task has been depicted as prone to systematic biases. However, performance on this task has been assessed against a now outmoded falsificationist philosophy of science. Therefore, the experimental data is reassessed in the light of a Bayesian model of optimal data selection in inductive hypothesis testing. The model provides a rational analysis (Anderson, 1990) of the selection task that fits well with people&#039;s performance on both abstract and thematic versions of the task. The model suggests that reasoning in these tasks may be rational rather than subject to systematic bias. Over the past 30 years, results in the psychology of reasoning have raised doubts about human rationality. The assumption of human rationality has a long history. Aristotle took the capacity for rational thought to be the defining characteristic of human beings, the capacity that separated us from the animals. Descartes regarded the ability to use language and to reason as the hallmarks of the mental that separated it from the merely physical. Many contemporary philosophers of mind also appeal to a basic principle of rationality in accounting for everyday, folk psychological explanation whereby we explain each other&#039;s behavior in terms of our beliefs and desires (Cherniak, 1986; Cohen, 1981; Davidson, 1984; Dennett, 1987; but see Stich, 1990). These philosophers, both ancient and modern, share a common view of rationality: To be rational is to reason according to rules (Brown, 1989). Logic and mathematics provide the normative rules that tell us how we should reason. Rationality therefore seems to demand that the human cognitive system embodies the rules of logic and mathematics. However, results in the psychology of reasoning appear to show that people do not reason according to these rules. In both deductive (Evans, 1982, 1989;
153|Distributed Memory and the Representation of General and Specific Information|We describe a distributed model of information processing and memory and apply it to the representation of general and specific information. The model consists of a large number of simple processing elements which send excitatory and inhibitory signals to each other via modifiable connections. Information processing is thought of as the process whereby patterns of activation are formed over the units in the model through their excitatory and inhibitory interactions. The memory trace of a processing event is the change or increment to the strengths of the interconnections that results from the processing event. The traces of separate events are superimposed on each other in the values of the connection strengths that result from the entire set of traces stored in the memory. The model is applied to a number of findings related to the question of whether we store abstract representations or an enumeration of specific experiences in memory. The model simulates the results of a number of important experiments which have been taken as evidence for the enumeration of specific experiences. At the same time, it shows how the functional equivalent of abstract representations—prototypes, logogens
154|The role of Similarity in Categorization: Providing a Groundwork. Cognition|The relation between similarity and categorization has recently come under scrutiny from several sectors. The issue provides an important inroad to questions about the contributions of high-level thought and lower-level perception in the development of people’s concepts. Many psychological models base categorization on similarity, assuming that things belong in the same category because of their similarity. Empirical and in-principle arguments have recently raised objections to this connection, on the grounds that similarity is too unconstrained to provide an explanation of categorization, and similarity is not sufficiently sophisticated to ground most categories. Although these objections have merit, a reassessment of evidence indicates that similarity can be sufficiently constrained and sophisticated to provide at least a partial account of many categories. Principles are discussed for
155|The Logic Of Plausible Reasoning: A Core Theory|this paper. In particular, the protocols we have collected often involve picturing different situations (e.g., a mental map of South America, images of savannas, or an advertisement showing Juan Valdez on his coffee plantation in Colombia). These im- ages can be taken as evidence for the manipulation of mental models in Johnson-Laird&#039;s terms. But overlaying this manipulation of mental models are the systematic patterns in which they are deployed to support one&#039;s con- clusions (cf. Rips, 1986). So while mental models may be part of the story of plausible reasoning, there is another critical part which the theory we pro- pose addresses. The theory does not address the issue of whether people make systematic errors in their reasoning, as the psychological literature on decision making (Kahneman, Slovic, &amp; Tversky, 1982) attempts to document. This issue does not arise in the theory because we are developing a formalism for representing the kinds of inferences people make and the parameters that affect their certainty, rather than a theory about how people make particular inferences. People may systematically ignore some kinds of information or undervalue particular certainty parameters--we have not attempted to determine whether they do or not. Instead we have tried to represent all the kinds of reasoning patterns and the kinds of certainty parameters that appear in the protocols we have analyzed (Collins, 1978a, 1978b). In this regard it is worth pointing out that certain fallacles in logic, such as affirming the consequent (Havi- land, 1974), become plausible inference patterns in the theory.&#039; The theory was developed to account for protocols where. a question drives the search fo relevant information; in Artificial Intelligence this is called backward inferencing. One qu...
156|From tools to theories: A heuristic of discovery in cognitive psychology|The study of scientific discovery—where do new ideas come from?—has long been denigrated by philosophers as irrelevant to analyzing the growth of scientific knowledge. In particular, little is known about how cognitive theories are discovered, and neither the classical accounts of discovery as either probabilistic induction (e.g., Reichenbach, 1938) or lucky guesses (e.g., Popper, 1959), nor the stock anecdotes about sudden “eureka ” moments deepen the insight into discovery. A heuristics approach is taken in this review, where heuristics are understood as strategies of discovery less general than a supposed unique logic of discovery but more general than lucky guesses. This article deals with how scientists’ tools shape theories of mind, in particular with how methods of statistical inference have turned into metaphors of mind. The tools-to-theories heuristic explains the emergence of a broad range of cognitive theories, from the cognitive revolution of the 1960s up to the present, and it can be used to detect both limitations and new lines of development in current cognitive theories that investigate the mind as an “intuitive statistician.” Scientific inquiry can be viewed as “an ocean, continuous everywhere and without a break or division ” (Leibniz, 1690/1951, p. 73). Hans Reichenbach (1938) nonetheless divided this ocean into two great seas, the context of discovery and the context of justification. Philosophers, logicians,
157|Similarity involving attributes and relations : Judgments of similarity and difference are not inverses|Abstract-Conventional wisdom and previous research suggest that similarity judgments and difference judgments are inverses of one another. An exception to this rule arises when both relational similarity and attributional similarity are considered. When presented with choices that are relationally or attributionally similar to a standard, human subjects tend to pick the relationally similar choice as more similar to the standard and as more different from the standard. These results not only reinforce the general distinction between attributes and relations but also show that attributes and relations are dynamically distinct in the processes that give rise to similarity and difference judgments. The question of what makes things seem alike or seem different is fundamental to cognition. Models of learning imply that the learning of a task is facilitated if it is similar to another task that is already part of a learner&#039;s repertoire (Thorndike, 1966). Stimulus generalization occurs as a function of how similar the new stimulus is to the conditioned stimulus (Pavlov, 1927; Shepard, 1987). In memory models it has been assumed that X reminds people of Y if it is similar to Y (Kolodner, 1984; Schank, 1982), and in theories of categorization it has been assumed that classification of new examples is based on their similarity to known examples or to a category prototype
158|Role of specific similarity in a medical diagnostic task|Three experiments are reported showing that diagnosis of skin disorders by medical residents and general practitioners was facilitated by similar eases previously seen in the same context. Diagnosis of similar cases was facilitated more than that of dissimilar cases in the same diagnostic
159|Rule Induction through Integrated Symbolic and Subsymbolic Processing|We describe a neural network, called RuleNet, that learns explicit, symbolic condition-action rules in a formal string manipulation domain. RuleNet discovers functional categories over elements of the domain, and, at various points during learning, extracts rules that operate on these categories. The rules are then injected back into RuleNet and training continues, in a process called iterative projection. By incorporating rules in this way, RuleNet exhibits enhanced learning and generalization performance over alternative neural net approaches. By integrating symbolic rule learning and subsymbolic category learning, RuleNet has capabilities that go beyond a purely symbolic system. We show how this architecture can be applied to the problem of case-role assignment in natural language processing, yielding a novel rule-based solution.  1 INTRODUCTION  We believe that neural networks are capable of more than pattern recognition; they can also perform higher cognitive tasks which are funda...
160|A Connectionist Symbol Manipulator That Discovers the Structure of Context-Free Languages|We present a neural net architecture that can discover hierarchical and recursive structure in symbol strings. To detect structure at multiple levels, the architecture has the capability of reducing symbols substrings to single symbols, and makes use of an external stack memory. In terms of formal languages, the architecture can learn to parse strings in an LR(0) contextfree grammar. Given training sets of positive and negative exemplars, the architecture has been trained to recognize many different grammars. The architecture has only one layer of modifiable weights, allowing for a straightforward interpretation of its behavior. Many cognitive domains involve complex sequences that contain hierarchical or recursive structure, e.g., music, natural language parsing, event perception. To illustrate, &#034;the spider that ate the hairy fly&#034; is a noun phrase containing the embedded noun phrase &#034;the hairy fly.&#034; Understanding such multilevel structures requires forming reduced descriptions (Hinton...
161|Cognitive processes in prepositional reasoning|Propositional reasoning is the ability to draw conclusions on the basis of sentence connectives such as and, if, or, and not. A psychological theory of prepositional reasoning explains the mental operations that underlie this ability. The ANDS (A Natural Deduction System) model, described in the following pages, is one such theory that makes explicit assumptions about memory and control in deduction. ANDS uses natural deduction rules that manipulate propositions in a hierarchically structured working memory and that apply in either a forward or a backward direction (from the premises of an argument to its conclusion or from the conclusion to the premises). The rules also allow suppositions to be introduced during the deduction process. A computer simulation incorporating these ideas yields proofs that are similar to those of untrained subjects, as assessed by their decisions and explanations concerning the validity of arguments. The model also provides an account of memory for proofs in text and can be extended to a theory of causal connectives. The importance of deductive reasoning lo
162|Parallel database systems: the future of high performance database systems|Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1.
163|Access path selection in a relational database management system|ABSTRACT: In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research&#039;Laboratory. 1.
164|The Gamma database machine project|This paper describes the design of the Gamma database machine and the techniques employed in its implementation. Gamma is a relational database machine currently operating on an Intel iPSC/2 hypercube with 32 processors and 32 disk drives. Gamma employs three key technical ideas which enable the architecture to be scaled to 100s of processors. First, all relations are horizontally partitioned across multiple disk drives enabling relations to be scanned in parallel. Second, novel parallel algorithms based on hashing are used to implement the complex relational operators such as join and aggregate functions. Third, dataflow scheduling techniques are used to coordinate multioperator queries. By using these techniques it is possible to control the execution of very complex queries with minimal coordination- a necessity for configurations involving a very large number of processors. In addition to describing the design of the Gamma software, a thorough performance evaluation of the iPSC/2 hypercube version of Gamma is also presented. In addition to measuring the effect of relation size and indices on the response time for selection, join, aggregation, and update queries, we also analyze the performance of Gamma relative to the number of processors employed when the sizes of the input relations are kept constant (speedup) and when the sizes of the input relations are increased proportionally to the number of processors (scaleup). The speedup results obtained for both selection and join queries are linear; thus, doubling the number of processors
165|A Performance Evaluation of Four Parallel Join Algorithms in a Shared-Nothing Multiprocessor Environment|The join operator has been a cornerstone of relational database systems since their inception. As such, much time and effort has gone into making joins efficient. With the obvious trend towards multiprocessors, attention has focused on efficiently parallelizing the join operation. In this paper we analyze and compare four parallel join algorithms. Grace and Hybrid hash represent the class of hash-based join methods, Simple hash represents a looping algorithm with hashing, and our last algorithm is the more traditional sort-merge. The Gamma database machine serves as the host for the performance comparison. Gamma’s shared-nothing architecture with commercially available components is becoming increasingly common, both in research and in industry. 1.
166|Dynamic Query Evaluation Plans|Traditional query optimizers assume accurate knowledge of run-time parameters such as selectivities and resource availability during plan optimization, i.e., at compile-time. In reality, however, this assumption is often not justified. Therefore, the “static ” plans produced by traditional optimizers may not be optimal for many of their actual run-time invocations. Instead, we propose a novel optimization model that assigns the bulk of the optimization effort to compile-time and delays carefully selected optimization decisions until run-time. Our previous work defined the run-time primitives, “dynamic plans ” using “choose-plan” operators, for executing such delayed decisions, but did not solve the problem of constructing dynamic plans at compile-time. The present paper introduces techniques that solve this problem. Experience with a working prototype optimizer demonstrates (i) that the additional optimization and start-up overhead of dynamic plans compared to static plans is dominated by their advantage at run-time, (ii) that dynamic plans are as robust as the “brute-force” remedy of run-time optimization, i.e., dynamic plans maintain their optimality even if parameters change between compile-time and run-time, and (iii) that the start-up overhead of dynamic plans is signifmantly less than the time required for complete optimization at run-time. In other words, our proposed techniques are superior to both techniques considered to-date, namely compile-time optimization into a single static plan as well as run-time optimization. Finally, we believe that the concepts and technology described can be transferred to commercial query optimizers in order to improve the performance of embedded queries with host variables in the query predicate and to adapt to run-time system loads unpredictable at compile-time. 1.
167|Data Placement in Bubba|Thus paper examines the problem of data placement in Bubba, a highly-parallel system for data-intensive applications bemg developed at MCC “Highly-parallel” lmplres that load balancmng IS a cntlcal performance issue “Data-mtenave” means data IS so large that operatrons should be executed where the data resides As a result, data placement becomes a cntlcal performance issue In general, determmmg the optimal placement of data across processmg nodes for performance IS a difficult problem We describe our heuristic approach to solvmg the data placement problem w Bubba We then present expenmental results using a specific workload to provide msrght into the problem Several researchers have argued the benefits of deelustering (1 e, spreading each base relation over many nodes) We show that as declustermg IS increased. load balancing contmues to improve However, for transactions mvolvmg complex Joins, further declusterrng reduces throughput because of communications, startup and termmatron overhead We argue that data placement, especially declustermg, m a highly-parallel system must be considered early in the design, so that mechanrsms can be included for supportmg variable declustermg, for mmtmlzmg the most significant overheads associated with large-scale declustenng, and for gathering the required statistics.
168|The Design Of Xprs|This paper presents an overview of the techniques we are using to build a DBMS at Berkeley that will simultaneously provide high performance and high availability in transaction processing environments, in applications with complex ad-hoc queries and in applications with large objects such as images or CAD layouts. We plan to achieve these goals using a general purpose DBMS and operating system and a shared memory multiprocessor. The hardware and software tactics which we are using to accomplish these goals are described in this paper and include a novel &#034;fast path&#034; feature, a special purpose concurrency control scheme, a two-dimensional file system, exploitation of parallelism and a novel method to efficiently mirror disks. 
169|A benchmark of NonStop SQL release 2 demonstrating near-linear speedup and scaleup on large databases|its second release, NonStop SQL transparently and automatically implements parallelism within an SQL statement. This parallelism allows query execution speed to increase almost linearly as processors and discs are added to the system-- speedup. In addition, this parallelism can help jobs restricted to a fIxed &amp;quot;batch window&amp;quot;. When the job doubles in size, its elapsed processing time will not change ifproportionately more equipment is available to process the job-- scaleup. This paper describes the parallelism features of NonStop SQL and an audited benchmark that demonstrates these speedup and scaleup claims.
170|Mediators in the architecture of future information systems|The installation of high-speed networks using optical fiber and high bandwidth messsage forwarding gateways is changing the physical capabilities of information systems. These capabilities must be complemented with corresponding software systems advances to obtain a real benefit. Without smart software we will gain access to more data, but not improve access to the type and quality of information needed for decision making. To develop the concepts needed for future information systems we model information processing as an interaction of data and knowledge. This model provides criteria for a high-level functional partitioning. These partitions are mapped into information process-ing modules. The modules are assigned to nodes of the distributed information systems. A central role is assigned to modules that mediate between the users &#039; workstations and data re-sources. Mediators contain the administrative and technical knowledge to create information needed for decision-making. Software which mediates is common today, but the structure, the interfaces, and implementations vary greatly, so that automation of integration is awkward. By formalizing and implementing mediation we establish a partitioned information sys-
171|PVS: A Prototype Verification System|PVS is a prototype system for writing specifications and constructing proofs. Its development has been shaped by our experiences studying or using several other systems and performing a number of rather substantial formal verifications (e.g., [5,6,8]). PVS is fully implemented and freely available. It has been used to construct proofs of nontrivial difficulty with relatively modest amounts of human effort. Here, we describe some of the motivation behind PVS and provide some details of the system. Automated reasoning systems typically fall in one of two classes: those that provide powerful automation for an impoverished logic, and others that feature expressive logics but only limited automation. PVS attempts to tread the middle ground between these two classes by providing mechanical assistance to support clear and abstract specifications, and readable yet sound proofs for difficult theorems. Our goal is to provide mechanically-checked specificati
172|The Byzantine Generals Problem|Reliable computer systems must handle malfunctioning components that give conflicting information to different parts of the system. This situation can be expressed abstractly in terms of a group of generals of the Byzantine army camped with their troops around an enemy city. Communicating only by messenger, the generals must agree upon a common battle plan. However, one of more of them may be traitors who will try to confuse the others. The problem is to find an algorithm to ensure that the loyal generals will reach agreement. It is shown that, using only oral messages, this problem is solvable if and only if more than two-thirds of the generals are loyal; so a single traitor can confound two loyal generals. With unforgeable written messages, the problem is solvable for any number of generals and possible traitors. Applications of the solutions to reliable computer systems are then discussed.
173|Deciding Combinations of Theories|Abstract. A method ~s g~ven for dec~dlng formulas in combinations ofunquantified first-order theories. Rather than couphng separate decision procedures for the contributing theories, the method makes use of a single, uniform procedure that minimizes the code needed to accommodate each additional theory. It ~s apphcable to theories whose semantics an be encoded within a certain class of purely equational canonical form theories that ~s closed under combination. Examples are given from the equational theories of integer and real anthmeUc, a subtheory of monadic set theory, the theory of cons, car, and cdr, and others. A discussion of the speed performance of the procedure and a proof of the theorem that underhes ~ts completeness are also g~ven. The procedure has been used extensively asthe deductive core of a system for program specificaUon a d verifcation.
174|A Fast Majority Vote Algorithm|A new algorithm is presented for determining which, if any, of an arbitrary number of candidates has received a majority of the votes cast in an election. The number of comparisons required is at most twice the number of votes. Furthermore, the algorithm uses storage in a way that permits an efficient use of magnetic tape.
175|Mechanical Verification of a Generalized Protocol for Byzantine Fault Tolerant Clock Synchronization|Schneider [Sch87] generalizes a number of protocols for Byzantine fault-tolerant clock synchronization and presents a uniform proof for their correctness. We present a mechanical verification of Schneider&#039;s protocol leading to several significant clarifications and revisions. The verification was carried out with the Ehdm system [RvHO91] developed at the SRI Computer Science Laboratory. The mechanically checked proofs include the verification that the egocentric mean function used in Lamport and Melliar-Smith&#039;s Interactive Convergence Algorithm [LMS85] satisfies the requirements of Schneider&#039;s protocol. Our mechanical verification raises a number of issues regarding the verification of fault-tolerant, distributed, real-time protocols that are germane to the design of a special-purpose logic for such problems.   
176|Formal Specification and Verification of a Fault-Masking and Transient-Recovery Model for Digital Flight-Control Systems|We present a formal model for fault-masking and transient-recovery among the replicated computers of digital flight-control systems. We establish conditions under which majority voting causes the same commands to be sent to the actuators as those that would be sent by a single computer that suffers no failures. The model and its analysis have been subjected to formal specification and mechanically checked verification using the Ehdm system. Keywords: digital flight control systems, formal methods, formal specification and verification, proof checking, fault tolerance, transient faults, majority voting, modular redundancy  Contents 1 Introduction 1  1.1 Digital Flight-Control Systems : : : : : : : : : : : : : : : : : : : : : 2 1.2 Fault Tolerance for DFCS : : : : : : : : : : : : : : : : : : : : : : : : 3 1.3 Formal Models for DFCS : : : : : : : : : : : : : : : : : : : : : : : : 11 1.3.1 Overview of the Fault-Masking Model Employed : : : : : : : 12  2 The Fault-Masking Model 17  2.1 A M...
177|Disconnected Operation in the Coda File System|Disconnected operation is a mode of operation that enables a client to continue accessing critical data during temporary failures of a shared data repository. An important, though not exclusive, application of disconnected operation is in supporting portable computers. In this paper, we show that disconnected operation is feasible, efficient and usable by describing its design and implementation in the Coda File System. The central idea behind our work is that caching of data, now widely used for performance, can also be exploited to improve availability.
178|Coda: A Highly available File System for a Distributed Workstation Environment|Abstract- Coda is a file system for a large-scale distributed computing environment composed of Unix workstations. It provides resiliency to server and network failures through the use of two distinct but complementary mechanisms. One mechanism, server replication, stores copies of a file at multiple servers. The other mechanism, disconnected operation, is a mode of execution in which a caching site temporarily assumes the role of a replication site. Disconnected operation is particularly useful for supporting portable workstations. The design of Coda optimizes for availability and performance, and strives to provide the highest degree of consistency attainable in the light of these objectives. Measurements from a prototype show that the performance cost of providing high availability in Coda is reasonable. Index Terms- Andrew, availability, caching, disconnected operation, distributed file system, performance, portable computers, scalability, server replication. I.
179|Design and Implementation or the Sun Network Filesystem|this paper we discuss the design and implementation of the/&#039;fiesystem interface in the kernel and the NF$ virtual/&#039;fiesystem. We describe some interesting design issues and how they were resolved, and point out some of the shortcomings of the current implementation. We conclude with some ideas for future enhancements
180|Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency|Caching introduces the overbead and complexity of ensur-ing consistency, reducing some of its performance bene-fits. In a distributed system, caching must deal,wit.h the additional complications of communication and host fail-ures. Leases are proposed as a time-based mechanism that provides efficient consistent access to cached data in dis-tributed systems. Non-Byzantine failures affect perfor-mance, not correctness, with their effect minimized by short leases. An analytic model and an evaluation for file access in the V system show that leases of short duration provide good performance. The impact of leases on per-formance grows more significant in systems of lar;ger scale and higher processor performance. 
181|Vnodes: An architecture for multiple file system types|sun!srk
182|The locus distributed operating system|LOCUS Is a distributed operating system which supports transparent access to data through a network wide fllesystem, permits automatic replication of storaget supports transparent distributed process execution, supplies a number of high reliability functions such as nested transactions, and is upward compatible with Unix. Partitioned operation of subnetl and their dynamic merge is also supported. The system has been operational for about two years at UCLA and extensive experience In its use has been obtained. The complete system architecture is outlined in this paper, and that experience is summarized. 1
183|Optimism and consistency in partitioned distributed database systems|A protocol for transaction processing during partition failures is presented which guarantees mutual consistency between copies of data-items after repair is completed. The protocol is “optimistic ” in that transactions are processed without restrictions during failure; conflicts are then detected at repair time using a precedence graph, and are resolved by backing out transactions according to some backout strategy. The resulting database state then corresponds to a serial execution of some subset of transactions run during the failure. Results from simulation and probabilistic modeling show that the optimistic protocol is a reasonable alternative in many cases. Conditions under which the protocol performs well are noted, and suggestions are made as to how performance can be improved. In particular, a backout strategy is presented which takes into account individual transaction costs and attempts to minimize total backout cost. Although the problem of choosing transactions to minimize total backout cost is, in general, NP-complete, the backout strategy is efficient and produces very good results.
184|Logbased directory resolution in the Coda file system|optimistic replicltion is m imparrrnt technique for achieving high avdability in distributed file systans. A key problem m optimistic rephtim is using d c bwledge of objects to resolve co&#034;mt updates from multiple partitions. In this paper, we describe how the Coda File System resolves pllrtitionsd updareg to dirCCtOIkS. The CUUd Idt Of Our Work is th.t &amp;g&amp;g Of updates is a simple yet efficient and powerful teclmique for directory resolution m Unix file systems. Mersurementr from our implementatkm show that the time for resolution is typically less than 1096 of the time for paforming the original set of partitioned updates. Analysis based on file traces fnnn our envinm &#034; indicate that a log size of 2 MB per hour of padtion should be ample for typical smers. 1.
185|A Caching File System for a Programmer&#039;s Workstation|This paper describes a workstation file system that supports a group of cooperating programmers by allowing them both to manage local naming environments and to share consistent versions of collections of software. The file system has access to the workstation&#039;s local disk and to remote file servers, and provides a hierarchical name space that includes the files on both. Local names can refer to local files or be attached to remote files. Remote files, which also may be referred to directly, are immutable and cached on the local disk. The file system is part of the Cedar experimental programming environment at Xerox PARC and has been in use since late 1983.
186|Availability and consistency tradeoffs in the Echo distributed file system|Workstations typically depend on remote servers accessed over a network for such services as mail, printing, storing files, booting, and time. The availability of these remote services has a major impact on the usability of the workstation. Availability can be increased by repli-cating the servers. In the Echo distributed file system at DEC SRC, two different replication techniques are employed, one at the upper levels of our hierarchical name space, the name service, and another at the lower levels of the name space, the file volume service. The two replication techniques provide different guarantees of consistency be-tween their replicas and, therefore, different levels of availability. Echo also caches data from the name service and file volume service in client machines (e.g., workstations), with the cache for each service having its own cache consistency guarantee that mimics the guarantee on the consistency of the replicas for that service. The replication and caching consistency guarantees provided by each service are appropriate for its intended use.
187|Fine-grained Mobility in the Emerald System|Emerald is an object-based language and system designed for the construction of distributed programs. An explicit goal of Emerald is support for object mobility; objects in Emerald can freely move within the system to take advantage of distribution and dynamically changing environments. We say that Emerald has fine-grained mobility because Emerald objects can be small data objects as well as process objects. Fine-grained mobility allows us to apply mobility in new ways but presents imple-mentation problems as well. This paper discusses the benefits of tine-grained mobility, the Emerald language and run-time mechanisms that support mobility, and techniques for implementing mobility that do not degrade the performance of local operations. Performance measurements of the current implementation are included.
188|Object Structure in the Emerald System|Emerald is an object.based language for the construction of distributed applications. The principal features of Emerald lnehtde a uniform object model appropriate for programming both private local objects and shared remote objects, and a type system that permits multiple user.defined and compiler-defined implementations. Emerald objects are fully mobile and can move from node to node within the network, even during an invocation. This paper discusses the structure, programming, and inq~lementation of Emerald objects, and Emerald&#039;s use of abstract types. 1.
189|Accent: A Communication Oriented Network Operating System Kernel|Accent is a communication oriented operating system kernel being built at Carnegie-Mellon University to support the distributed personal computing project, Spice, and the development of a fault-tolerant distributed sensor network (DSN). Accent is. built around a single, powerful abstraction of communication between processes, with all kernel functions, such as device access and virtual memory management accessible through messages and distributable throughout a network. In this paper, specific attention is given to system supplied facilities which support transparent network access and fault-tolerant behavior. Many of these facilities are already being provided under a modified version of VAX/UNIX. The Accent system itself is currently being implemented on the Three Rivers Corp. PERQ. Keywords: Inter-process communication, networking,
190|Transparent Process Migration in the Sprite Operating System|The Sprite operating system allows executing processes to be moved between hosts at any time. We use this process migration mechanism to offload work onto idle machines, and also to evict migrated processes when idle workstations are reclaimed by their owners. Sprite&#039;s migration mechanism provides a high degree of transparency both for migrated processes and for users. Transparency is ensured by managing shared data structures on a single site and redirecting operations on those structures to the host managing them. Idle machines are identified, and eviction is invoked, automatically by daemon processes. On Sprite it takes up to a few hundred milliseconds on SPARCstation 1 or DECstation 3100 workstations to perform a remote exec, while evictions typically occur in a few seconds. The pmake program uses remote invocation to invoke tasks concurrently. Compilations commonly obtain speedup factors in the range of three to six; they are limited primarily by contention for centralized resourc...
191|A Value Transmission Method for Abstract Data Types|Abstract data types have proved to be a useful technique for structuring systems. In large systems it is sometimes useful to have different regions of the system use different representations for the abstract data values. A technique is described for communicating abstract values between such regions. The method was developed for use in constructing distributed systems, where the regions exist at different computers and the values are communicated over a network. The method defines a call-by-value semantics; it is also useful in nondistributed systems wherever call by value is the desired semantics. An important example of such a use is a repository, such as a file system, for storing long-lived data.
192|Supporting distributed applications: Experience with Eden|The Eden distributed system has been running at the University of Washington for over two years. Most of the principles and implementation ideas of Eden have been adequately discussed in the literature [4]. This paper presents some of the experience that has been gained from the implementation and use of Eden. Much of this experience is relevant to other distributed systems, even though they may be based on different assumptions. 1.
193|Models and issues in data stream systems|In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues. 
194|Randomized Algorithms|Randomized algorithms, once viewed as a tool in computational number theory, have by now found widespread application. Growth has been fueled by the two major benefits of randomization: simplicity and speed. For many applications a randomized algorithm is the fastest algorithm available, or the simplest, or both. A randomized algorithm is an algorithm that uses random numbers to influence the choices it makes in the course of its computation. Thus its behavior (typically quantified as running time or quality of output) varies from
195|The space complexity of approximating the frequency moments|The frequency moments of a sequence containing mi elements of type i, for 1 = i = n, are the numbers Fk = ?n i=1 mki. We consider the space complexity of randomized algorithms that approximate the numbers Fk, when the elements of the sequence are given one by one and cannot be stored. Surprisingly, it turns out that the numbers F0, F1 and F2 can be approximated in logarithmic space, whereas the approximation of Fk for k = 6 requires n?(1) space. Applications to data bases are mentioned as well. 
196|Multiparty Communication Complexity|A given Boolean function has its input distributed among many parties. The aim is to determine which parties to tMk to and what information to exchange with each of them in order to evaluate the function while minimizing the total communication. This paper shows that it is possible to obtain the Boolean answer deterministically with only a polynomial increase in communication with respect to the information lower bound given by the nondeterministic communication complexity of the function.
197|NiagaraCQ: A Scalable Continuous Query System for Internet Databases|Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system’s performance and scalability. 1.
198|Fast subsequence matching in time-series databases|We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an ecient and eective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.  
199|Eddies: Continuously Adaptive Query Processing|In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.  In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry  during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates prom...
200|Mining high-speed data streams|Categories and Subject ?????????? ? ?¨?????????????????????????¦???¦??????????¡¤?? ¡  ? ¡????????????????¦¡¤????§?£????
201|Online Aggregation|Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in postgres. 1 Introduction  Aggregation is an incre...
202|Efficient Filtering of XML Documents for Selective Dissemination of Information|Information Dissemination applications are gaining increasing  popularity due to dramatic improvements in  communications bandwidth and ubiquity. The sheer  volume of data available necessitates the use of selective  approaches to dissemination in order to avoid overwhelming  users with unnecessaryinformation. Existing  mechanisms for selective dissemination typically rely  on simple keyword matching or &#034;bag of words&#034; information  retrieval techniques. The advent of XML as a  standard for information exchangeand the development  of query languages for XML data enables the development  of more sophisticated filtering mechanisms that  take structure information into account. We have developed  several index organizations and search algorithms  for performing efficient filtering of XML documents for  large-scale information dissemination systems. In this  paper we describe these techniques and examine their  performance across a range of document, workload, and  scale scenarios.  1 
203|External Memory Algorithms and Data Structures|Data sets in large applications are often too massive to fit completely inside the computer&#039;s internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this paper, we survey the state of the art in the design and analysis of external memory algorithms and data structures (which are sometimes referred to as &#034;EM&#034; or &#034;I/O&#034; or &#034;out-of-core&#034; algorithms and data structures). EM algorithms and data structures are often designed and analyzed using the parallel disk model (PDM). The three machine-independent measures of performance in PDM are the number of I/O operations, the CPU time, and the amount of disk space. PDM allows for multiple disks (or disk arrays) and parallel CPUs, and it can be generalized to handle tertiary storage and hierarchical memory. We discuss several important paradigms for how to solve batched and online problems efficiently in external memory. Programming tools and environments are available for simplifying the programming task. The TPIE system (Transparent Parallel I/O programming Environment) is both easy to use and efficient in terms of execution speed. We report on some experiments using TPIE in the domain of spatial databases. The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.  
204|Random sampling with a reservoir|We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O(n(1 + log(N/n))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.
205|Stable Distributions, Pseudorandom Generators, Embeddings and Data Stream Computation|In this paper we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular: ffl we show how to maintain (using only O(log n=ffl  2  ) words of storage) a sketch C(p) of a point p 2 l  n  1 under dynamic updates of its coordinates, such that given sketches C(p) and C(q) one can estimate jp \Gamma qj 1 up to a factor of (1 + ffl) with large probability. This solves the main open problem of [10].  ffl we obtain another sketch function C  0  which maps l  n  1 into a normed space l  m  1 (as opposed to C), such that m = m(n) is much smaller than n; to our knowledge this is the first dimensionality reduction lemma for l 1 norm  ffl we give an explicit embedding of l  n  2 into l  n  O(log n) 1  with distortion (1 + 1=n  \Theta(1)  ) and a nonconstructive embedding of l  n  2 into l  O(n)  1 with distortion  (1 + ffl) such that the embedding can be represented using only O(n log  2  n) bits (as opposed to at least...
206|Mining time-changing data streams|Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a station-ary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying pro-cesses generating them changed during this time, sometimes radically. Although a number of algorithms have been pro-posed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes ques-tionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.
207|Continuous Queries over Data Streams|In many recent applications, data may take the form of continuous data streams, rather than finite stored data sets. Several aspects of data management need to be re-considered in the presence of data streams, offering a new research direction for the database community. In this pa-per we focus primarily on the problem of query processing, specifically on how to define and evaluate continuous queries over data streams. We address semantic issues as well as efficiency concerns. Our main contributions are threefold. First, we specify a general and flexible architecture for query processing in the presence of data streams. Second, we use our basic architecture as a tool to clarify alternative semantics and processing techniques for continuous queries. The architecture also captures most previous work on continuous queries and data streams, as
209|Fjording the Stream: An Architecture for Queries over Streaming Sensor Data|If industry visionaries are correct, our lives will soon be full of sensors, connected together in loose conglomerations via wireless networks, each monitoring and collecting data about the environment at large. These sensors behave very differently from traditional database sources: they have intermittent connectivity, are limited by severe power constraints, and typically sample periodically and push immediately, keeping no record of historical information. These limitations make traditional database systems inappropriate for queries over sensors. We present the Fjords architecture for managing multiple queries over many sensors, and show how it can be used to limit sensor resource demands while maintaining high query throughput. We evaluate our architecture using traces from a network of traffic sensors deployed on Interstate 80 near Berkeley and present performance results that show how query throughput, communication costs, and power consumption are necessarily coupled in sensor environments.
210|Maintaining Stream Statistics over Sliding Windows (Extended Abstract)  (2002) |Mayur Datar  Aristides Gionis  y  Piotr Indyk  z  Rajeev Motwani  x  Abstract  We consider the problem of maintaining aggregates and statistics over data streams, with respect to the last N data elements seen so far. We refer to this model as the sliding window model. We consider the following basic problem: Given a stream of bits, maintain a count of the number of 1&#039;s in the last N elements seen from the stream. We show that using O(  1  ffl log  2  N) bits of memory, we can estimate the number of 1&#039;s to within a factor of 1 + ffl. We also give a matching lower bound of \Omega\Gamma  1  ffl log  2  N) memory bits for any deterministic or randomized algorithms. We extend our scheme to maintain the sum of the last N positive integers. We provide matching upper and lower bounds for this more general problem as well. We apply our techniques to obtain efficient algorithms for the Lp norms (for p 2 [1; 2]) of vectors under the sliding window model. Using the algorithm for the basic counting problem, one can adapt many other techniques to work for the sliding window model, with a multiplicative overhead of O(  1  ffl log N) in memory and a 1 + ffl factor loss in accuracy. These include maintaining approximate histograms, hash tables, and statistics or aggregates such as sum and averages.
211|Continuously Adaptive Continuous Queries over Streams|We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive crossquery sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators -- both selections and join state -- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.
212|Multiple-Query Optimization|Some recently proposed extensions to relational database systems, as well as to deductive database systems, require support for multiple-query processing. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions may expand to more than one actual query that has to be run over the database. It is an interesting problem then to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of the paper is a systematic look at the problem, along with the presentation and analysis of algorithms that can be used for multiple-query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably.
213|Trajectory Sampling for Direct Traffic Observation|Traffic measurement is a critical component for the control and engineering of communication networks. We argue that traffic measurement should make it possible to obtain the spatial flow of traffic through the domain, i.e., the paths followed by packets between any ingress and egress point of the domain. Most resource allocation and capacity planning tasks can benefit from such information. Also, traffic measurements should be obtained without a routing model and without knowledge of network state. This allows the traffic measurement process to be resilient to network failures and state uncertainty.  We propose a method that allows the direct inference of traffic flows through a domain by observing the trajectories of a subset of all packets traversing the network. The key advantages of the method are that (i) it does not rely on routing state, (ii) its implementation cost is small, and (iii) the measurement reporting traffic is modest and can be controlled precisely. The key idea of the method is to sample packets based on a hash function computed over the packet content. Using the same hash function will yield the same sample set of packets in the entire domain, and enables us to reconstruct packet trajectories.  I. 
214|  Wavelet-Based Histograms for Selectivity Estimation |Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations. In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data values give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using
215|An Adaptive Query Execution System for Data Integration|Query processing in data integration occurs over networkbound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the  Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila  to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that the Tukwila architecture extends previous innovations in adaptive execution (such as...
216|Surfing wavelets on streams: One-pass summaries for approximate aggregate queries|Abstract We present techniques for computing small spacerepresentations of massive data streams. These are inspired by traditional wavelet-based approx-imations that consist of specific linear projections of the underlying data. We present general&amp;quot;sketch &amp;quot; based methods for capturing various linear projections of the data and use them to pro-vide pointwise and rangesum estimation of data streams. These methods use small amounts ofspace and per-item time while streaming through the data, and provide accurate representation asour experiments with real data streams show.
217|Approximate Query Processing Using Wavelets|Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today’s decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing
218|Space-Efficient Online Computation of Quantile Summaries|An &amp;epsilon;-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of &amp;epsilon;N . We present a new online...
219| Approximate Computation of Multidimensional Aggregates of Sparse Data Using Wavelets |Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries. In this paper, we present anovel method that provides approximate answers to high-dimensional OLAP aggregation queries in massive sparse data sets in a time-efficient and space-efficient manner. We construct a compact data cube, which is an approximate and space-efficient representation of the underlying multidimensional array, based upon a multiresolution wavelet decomposition. In the on-line phase, each aggregation query can generally be answered using the compact data cube in one I/O or a small number of I/Os, depending upon the desired accuracy. We present two I/O-efficient algorithms to construct the compact data cube for the important case of sparse high-dimensional arrays, which often arise in practice. The traditional histogram methods are infeasible for the massive high-dimensional data sets in OLAP applications. Previously developed wavelet techniques are efficient only for dense data. Our on-line query processing algorithm is very fast and capable of refining answers as the user demands more accuracy. Experiments on real data show that our method provides significantly more accurate results for typical OLAP aggregation queries than other efficient approximation techniques such as random sampling.
220|Computing on Data Streams|In this paper we study the space requirement of algorithms that make  only one (or a small number of) pass(es) over the input data. We study such  algorithms under a model of data streams that we introduce here. We give  a number of upper and lower bounds for problems stemming from queryprocessing,  invoking in the process tools from the area of communication  complexity.
221|Processing Complex Aggregate Queries over Data Streams|Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (i.e., data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed.
222|Continual Queries for Internet Scale Event-Driven Information Delivery|In this paper we introduce the concept of continual queries, describe the design of a distributed  event-driven continual query system -- OpenCQ, and outline the initial implementation of OpenCQ  on top of the distributed interoperable information mediation system DIOM [21, 19]. Continual  queries are standing queries that monitor update of interest and return results whenever the update  reaches specified thresholds. In OpenCQ, users may specify to the system the information they would  like to monitor (such as the events or the update thresholds they are interested in). Whenever the  information of interest becomes available, the system immediately delivers it to the relevant users;  otherwise, the system continually monitors the arrival of the desired information and pushes it to the  relevant users as it meets the specified update thresholds. In contrast to conventional pull-based data  management systems such as DBMSs and Web search engines, OpenCQ exhibits two important featu...
223|Join synopses for approximate query answering|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses (join samples) as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known. We also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations. One of our key contributions is a detailed analysis of the error bounds obtained for approximate answers that demonstrates the trade-offs in various methods, as well as the advantages in certain scenarios of a new subsampling method we propose. Our extensive set of experiments on the TPC-D benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper. 1
224|An efficient cost-driven index selection tool for Microsoft SQL Server|In this paper we describe novel techniques that make it possible to build an industrial-strength tool for automating the choice of indexes in the physical design of a SQL database. The tool takes as input a workload of SQL queries, and suggests a set of suitable indexes. We ensure that the indexes chosen are effective in reducing the cost of the workload by keeping the index selection tool and the query optimizer &amp;quot;in step&amp;quot;. The number of index sets that must be evaluated to find the optimal configuration is very large. We reduce the complexity of this problem using three techniques. First, we remove a large number of spurious indexes from consideration by taking into account both query syntax and cost information. Second, we introduce optimizations that make it possible to cheaply evaluate the “goodness ” of an index set. Third, we describe an iterative approach to handle the complexity arising from multicolumn indexes. The tool has been implemented on Microsoft SQL Server 7.0. We performed extensive experiments over a range of workloads, including TPC-D. The results indicate that the tool is efficient and its choices are close to optimal. 1.
225|Reductions in Streaming Algorithms, with an Application to Counting Triangles in Graphs |We introduce reductions in the streaming model as a tool in the design of streaming algorithms. We develop
226|Computing iceberg queries efficiently|Many applications compute aggregate functions...
227|Data-Streams and Histograms|Histograms have been used widely to capture data distribution, to represent the data by a small number of step functions. Dynamic programming algorithms which provide optimal construction of these histograms exist, albeit running in quadratic time and linear space. In this paper we provide linear time construction of 1 + epsilon approximation of optimal histograms, running in polylogarithmic space. Our results extend to the context of data-streams, and in fact generalize to give 1 + epsilon approximation of several problems in data-streams which require partitioning the index set into intervals. The only assumptions required are that the cost of an interval is monotonic under inclusion (larger interval has larger cost) and that the cost can be computed or approximated in small space. This exhibits a nice class of problems for which we can have near optimal data-stream algorithms.
228|XJoin: A Reactively-Scheduled Pipelined Join Operator|Wide-area distribution raises significant performance problems for traditional query processing techniques as data access becomes less predictable due to link congestion, load imbalances, and temporary outages. Pipelined query execution is a promising approach to coping with unpredictability in such environments as it allows scheduling to adjust to the arrival properties of the data. We have developed a non-blocking join operator, called XJoin, which has a small memory footprint, allowing many such operators to be active in parallel. XJoin is optimized to produce initial results quickly and can hide intermittent delays in data arrival by reactively scheduling background processing. We show that XJoin is an effective solution for providing fast query responses to users even in the presence of slow and bursty remote sources.  
229|Rate-Based Query Optimization for Streaming Information Sources|Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences. We present a preliminary validation of our rate-based optimization framework on a prototype XML query engine, though it is generic enough to be used in other database contexts. The results show that rate-based optimization is feasible and can indeed yield correct decisions.
230|Making Views Self-Maintainable for Data Warehousing|A data warehouse stores materialized views over data from one or more sources in order to provide fast access to the integrated data, regardless of the availability of the data sources. Warehouse views need to be maintained inresponse to changes to the base data in the sources. Except for very simple views, maintaining a warehouse view requires access to data that is not available in the view itself. Hence, to maintain the view, one either has to query the data sources or store auxiliary data in the warehouse. We show that by using key and referential integrity constraints, we often can maintain a select-project-join view without going to the data sources or replicating the base relations in their entirety in the warehouse. We derive a set of auxiliary views such that the warehouse view and the auxiliary views together are self-maintainable|they can be maintained without going to the data sources or replicating all base data. In addition, our technique can be applied to simplify traditional materialized view maintenance by exploiting key and referential integrity constraints. 1
231|Approximate Medians and other Quantiles in One Pass and with Limited Memory|We present new algorithms for computing approximate quantiles of large datasets in a single pass. The approximation guarantees are explicit, and apply without regard to the value distribution or the arrival distributions of the dataset. The main memory requirements are smaller than those reported earlier by an order of magnitude.  We also discuss methods that couple the approximation algorithms with random sampling to further reduce memory requirements. With sampling, the approximation guarantees are explicit but probabilistic, i.e., they apply with respect to a (user controlled) confidence parameter.  We present the algorithms, their theoretical analysis and simulation results.  1 Introduction  This article studies the problem of computing order statistics  of large sequences of online or disk-resident data using as little main memory as possible. We focus on computing  quantiles, which are elements at specific positions in the sorted order of the input. The OE-quantile, for OE 2 [0; ...
232|Random Sampling for Histogram Construction: How much is enough?|Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining &#034;How much sampling is enough?&#034; We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a  conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose ...
233|Sampling From a Moving Window Over Streaming Data|We introduce the problem of sampling from a moving window of recent items from a data stream and develop the &#034;chain-sample&#034; and &#034;priority-sample&#034; algorithms for this problem.
234|Tracking join and self-join sizes in limited storage|This paper presents algorithms for tracking (approximate) join and self-join sizes in limited storage, in the presence of insertions and deletions to the data set(s). Such algorithms detect changes in join and self-join sizes without an expensive recomputation from the base data, and without the large space overhead required to maintain such sizes exactly. Query optimizers rely on fast, high-quality estimates of join sizes in order to select between various join plans, and estimates of self-join sizes are used to indicate the degree of skew in the data. For self-joins, we considertwo approaches proposed in [Alon, Matias, and Szegedy. The Space Complexity of Approximating the Frequency Moments. JCSS, vol. 58, 1999, p.137-147], which we denote tug-of-war and sample-count. Wepresent fast algorithms for implementing these approaches, and extensions to handle deletions as well as insertions. We also report on the rst experimental study of the two approaches, on a range of synthetic and real-world data sets. Our study shows that tug-of-war provides more accurate estimates for a given storage limit than sample-count, which in turn is far more accurate than a standard sampling-based approach. For example, tug-of-war needed only 4{256 memory words, depending on the data set, in order to estimate the self-join size
235|Data Cube Approximation and Histograms via Wavelets (Extended Abstract)  (1998) |) Jeffrey Scott Vitter   Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  jsv@cs.duke.edu Min Wang  y Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  minw@cs.duke.edu Bala Iyer  Database Technology Institute IBM Santa Teresa Laboratory P.O. Box 49023 San Jose, CA 95161 USA  balaiyer@vnet.ibm.com Abstract  There has recently been an explosion of interest in the analysis of data in data warehouses in the field of On-Line Analytical Processing (OLAP). Data warehouses can be extremely large, yet obtaining quick answers to queries is important. In many situations, obtaining the exact answer to an OLAP query is prohibitively expensive in terms of time and/or storage space. It can be advantageous to have fast, approximate answers to queries. In this paper, we present an I/O-efficient technique based upon a multiresolution wavelet decomposition that yields an approximate a...
236|The design and implementation of a sequence database system|This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. There are three distinct contributions made in this paper. (1) We describe the specification of sequence queries using the SEQUIN query language. (2) We quantitatively demonstrate the importance of various storage and optimization techniques by studying their effect on performance. (3) We present a novel nested design paradigm used in PREDATOR to combine sequence ‘and relational data.
237|Random sampling techniques for space efficient online computation of order statistics of large datasets|In a recent paper [MRL98], we had described a general framework for single pass approximate quantile nding algorithms. This framework included several known algorithms as special cases. We had identi ed a new algorithm, within the framework, which had a signi cantly smaller requirement for main memory than other known algorithms. In this paper, we address two issues left open in our earlier paper. First, all known and space e cient algorithms for approximate quantile nding require advance knowledge of the length of the input sequence. Many important database applications employing quantiles cannot provide this information. In this paper, we present anovel non-uniform random sampling scheme and an extension of our framework. Together, they form the basis of a new algorithm which computes approximate quantiles without knowing the input sequence length. Second, if the desired quantile is an extreme value (e.g., within the top 1 % of the elements), the space requirements of currently known algorithms are overly pessimistic. We provide a simple algorithm which estimates extreme values using less space than required by the earlier more general technique for computing all quantiles. Our principal observation here is that random sampling is quanti ably better when estimating extreme values than is the case with the median.  
238|Characterizing Memory Requirements for Queries over Continuous Data|This paper deals with continuous conjunctive queries with arithmetic comparisons and optional aggregation over multiple data streams. An algorithm is presented for determining whether or not any given query can be evaluated using a bounded amount of memory for all possible instances of the data streams. For queries that can be evaluated using bounded memory, an execution strategy based on constant-sized synopses of the data streams is proposed. For queries that cannot be evaluated using bounded memory, data stream scenarios are identified in which evaluating the queries requires memory linear in the size of the unbounded streams
239|Congressional samples for approximate answering of group-by queries|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex decision support queries using precomputed summary statistics, such as samples. Decision support queries routinely segment the data into groups and then aggregate the information in each group (group-by queries). Depending on the data, there can be a wide disparity between the number of data items in each group. As a result, approximate answers based on uniform random samples of the data can result in poor accuracy for groups with very few data items, since such groups will be represented in the sample by very few (often zero) tuples. In this paper, we propose a general class of techniques for obtaining fast, highly-accurate answers for group-by queries. These techniques rely on precomputed non-uniform (biased) samples of the data. In particular, we proposecongressional samples, ahybrid union of uniform and biased samples. Given a xed amount of space, congressional samples seek to maximize the accuracy for all possible group-by queries on a set of columns. We present a one pass algorithm for constructing a congressional sample and use this technique to also incrementally maintain the sample up-to-date without accessing the base relation. We also evaluate query rewriting strategies for providing approximate answers from congressional samples. Finally, we conduct an extensive set of experiments on the TPC-D database, which demonstrates the e cacy of the techniques proposed. 1
240|Histogram-Based Approximation of Set-Valued Query Answers|Answering queries approximately has recently  been proposed as a way to reduce query response  times in on-line decision support systems,  when the precise answer is not necessary  or early feedback is helpful. Most of the  work in this area uses sampling-based techniques  and handles aggregate queries, ignoring  queries that return relations as answers. In  this paper, we extend the scope of approximate  query answering to general queries. We propose  a novel and intuitive error measure for  quantifying the error in an approximate query  answer, which can be a multiset in general.
241|Data Integration using Self-Maintainable Views|.  In this paper we de#ne the concept of self-maintainable views  # these are views that can be maintained using only the contents of  the view and the database modi#cations, without accessing any of the  underlying databases. We derive tight conditions under which several  types of select-project-join are self-maintainable upon insertions, deletions  and updates. Self-Maintainability is a desirable property for e#-  ciently maintaining large views in applications where fast response and  high availability are important. One example of suchanenvironment  is data warehousing wherein views are used for integrating data from  multiple databases.  1 Introduction  Most large organizations have related data in distinct databases. Many of these databases may be legacy systems, or systems separated for organizational reasons like funding and ownership. Integrating data from such distinct databases is a pressing business need. A common approach for integration is to de#ne an integrated view and...
242|Monitoring XML Data on the Web|We consider the monitoring of a flow of incoming documents. More precisely, we present here the monitoring used in a very large warehouse built from XML documents found on the web. The flow of documents consists in XML pages (that are warehoused) and HTML pages (that are not). Our contributions are the following:  ffl a subscription language which specifies the monitoring of pages when fetched, the periodical evaluation of continuous queries and the production of XML reports. ffl the description of the architecture of the system we implemented that makes it possible to monitor a flow of millions of pages per day with millions of subscriptions on a single PC, and scales up by using more machines. ffl a new algorithm for processing alerts that can be used in a wider context. We support
244|Expiring data in a warehouse|Data warehouses collect data into materi-alized views for analysis. After some time, some of the data may no longer be needed or may not be of interest. In this pa-per, we handle this by expiring or remov-ing unneeded materialized view tuples. A framework supporting such expiration is presented. Within it, a user or adminis-trator can declaratively request expirations and can specify what type of modifications are expected from external sources. The lat-ter can significantly increase the amount of data that can be expired. We present effi-cient algorithms for determining what data can be expired (data not needed for main-tenance of other views), taking into account the types of updates that may occur. 1
246|Sequence Query Processing|Many applications require the ability to manipulate sequences of data. We motivate the importance of sequence query processing, and present a framework for the optimization of sequence queries based on several novel techniques. These include query transformations, optimizations that utilize meta--data, and caching of intermediate results. We present a bottom-up algorithm that generates an efficient query evaluation plan based on cost estimates. This work also identifies a number of directions in which future research can be directed.  1 Introduction  Many real life applications manipulate data that is inherently sequential. Such data is logically viewed and queried in terms of a sequence abstraction and is often physically stored as a sequence. Databases should (a) allow sequences to be queried in a declarative manner, utilizing the ordered semantics of the data, and (b) take advantage of the opportunities available for query optimization. Relational databases are inadequate in this re...
247|SEQ: A Model for Sequence Databases|This paper presents the model which is the basis for a system to manage various kinds of sequence data. The model separates the data from the ordering information, and includes operators based on two distinct abstractions of a sequence. The main contributions of the model are: (a) it can deal with different types of sequence data, (b) it supports an expressive range of sequence queries, (c) it draws from many of the diverse existing approaches to modeling sequence data. 1
248|Sampling Algorithms: Lower Bounds and Applications (Extended Abstract)  (2001) |]  Ziv Bar-Yossef  y  Computer Science Division  U. C. Berkeley  Berkeley, CA 94720  zivi@cs.berkeley.edu  Ravi Kumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  ravi@almaden.ibm.com  D. Sivakumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  siva@almaden.ibm.com  ABSTRACT  We develop a framework to study probabilistic sampling algorithms that approximate general functions of the form f : A  n  ! B, where A and B are arbitrary sets. Our goal is to obtain lower bounds on the query complexity of functions, namely the number of input variables x i that any sampling algorithm needs to query to approximate f(x1 ; : : : ; xn ).  We define two quantitative properties of functions --- the block sensitivity and the minimum Hellinger distance --- that give us techniques to prove lower bounds on the query complexity. These techniques are quite general, easy to use, yet powerful enough to yield tight results. Our applications include the mean and higher statistical moments, the median and other selection functions, and the frequency moments, where we obtain lower bounds that are close to the corresponding upper bounds.  We also point out some connections between sampling and streaming algorithms and lossy compression schemes.  1. 
249|Approximating a Data Stream for Querying and Estimation: Algorithms and Performance Evaluation|Obtaining fast and good quality approximations to data distributions is a problem of central interest to database management. A variety of popular database applications including, approximate querying, similarity searching and data mining in most application domains, rely on such good quality approximations. Histogram based approximation is a very popular method in database theory and practice to succinctly represent a data distribution in a space efficient manner. In this paper, we place the problem of histogram construction into perspective and we generalize it by raising the requirement of a finite data set and/or known data set size. We consider the case of an infinite data set on which data arrive continuously forming an infinite data stream. In this context, we present the first single pass algorithms capable of constructing histograms of provable good quality. We present algorithms for the fixed window variant of the basic histogram construction problem, supporting incremental maintenance of the histograms. The proposed algorithms trade accuracy for speed and allow for a graceful tradeoff between the two, based on application requirements. In the case of approximate queries on infinite data streams, we present a detailed experimental evaluation comparing our algorithms with other applicable techniques using real data sets, demonstrating the superiority of our proposal. 1
250|A robust, optimization-based approach for approximate answering of aggregate queries|The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are “similar ” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work. 1
251|Online Dynamic Reordering for Interactive Data Processing|Abstract We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing.
253|On Sampling and Relational Operators|A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. We highlight the primary difficulties, summarize the results of some recent work in this area, and indicate directions for future work.   
255|Bathtub dynamics: initial results of a systems thinking inventory|Fund. Nelson Repenning graciously permitted us to administer the tasks in his introductory system dynamics class. We also thank Jim Doyle, Michael Radzicki, Terry Tivnan the referees for helpful comments. Christopher Hunter assisted with data entry.
256|Monitors: An Operating System Structuring Concept|This is a digitized copy derived from an ACM copyrighted work. It is not guaranteed to be an accurate copy of the author&#039;s original work. This paper develops Brinch-Hansen&#039;s concept of a monitor as a method of structuring an operating system. It introduces a form of synchronization, describes a possible rnctltotl of implementation in terms of semaphorcs and gives a suitable proof rule. Illustrative examples include a single rcsourcc scheduler, a bounded buffer, an alarm clock, a buffer pool, a disk head optimizer, and a version of the problem of readers and writers. Key Words and Phrases: monitors, operating systems,schcduling, mutual exclusion, synchronization, system implementation langua yes, structured multiprogramming CR Categories:
258|Modern Information Retrieval|Information retrieval (IR) has changed considerably in the last years with the expansion of the Web (World Wide Web) and the advent of modern and inexpensive graphical user interfaces and mass storage devices. As a result, traditional IR textbooks have become quite out-of-date which has led to the introduction of new IR books recently. Nevertheless, we believe that there is still great need of a book that approaches the field in a rigorous and complete way from a computer-science perspective (in opposition to a user-centered perspective). This book is an effort to partially fulfill this gap and should be useful for a first course on information retrieval as well as for a graduate course on the topic. The book
259|GroupLens: An Open Architecture for Collaborative Filtering of Netnews|Collaborative filters help people make choices based on the opinions of other people. GroupLens is a system for collaborative filtering of netnews, to help people find articles they will like in the huge stream of available articles. News reader clients display predicted scores and make it easy for users to rate articles after they read them. Rating servers, called Better Bit Bureaus, gather and disseminate the ratings. The rating servers predict scores based on the heuristic that people who agreed in the past will probably agree again. Users can protect their privacy by entering ratings under pseudonyms, without reducing the effectiveness of the score prediction. The entire architecture is open: alternative software for news clients and Better Bit Bureaus can be developed independently and can interoperate with the components we have developed.
260|Empirical Analysis of Predictive Algorithm for Collaborative Filtering|1
262|Social Information Filtering: Algorithms for Automating &#034;Word of Mouth&#034;|This paper describes a technique for making personalized recommendations from any type of database to a user based on similarities between the interest profile of that user and those of other users. In particular, we discuss the implementation of a networked system called Ringo, which makes personalized recommendations for music albums and artists. Ringo&#039;s database of users and artists grows dynamically as more people use the system and enter more information. Four different algorithms for making recommendations by using social information filtering were tested and compared. We present quantitative and qualitative results obtained from the use of Ringo by more than 2000 people.
263|Using collaborative filtering to weave an information tapestry|predicated on the belief that information filtering can be more effective when humans are involved in the filtering process. Tapestry was designed to support both content-based filtering and collaborative filtering, which entails people collaborating to help each other perform filtering by recording their reactions to documents they read. The reactions are called annotations; they can be accessed by other people’s filters. Tapestry is intended to handle any incoming stream of electronic documents and serves both as a mail filter and repository; its components are the indexer, document store, annotation store, filterer, little box, remailer, appraiser and reader/browser. Tapestry’s client/server architecture, its various components, and the Tapestry query language are described.
264|Grouplens: Applying collaborative filtering to usenet news|... a collaborative filtering system for Usenet news—a high-volume, high-turnover discussion list service on the Internet. Usenet newsgroups—the individual discussion lists—may carry hundreds of messages each day. While in theory the newsgroup organization allows readers to select the content that most interests them, in practice most newsgroups carry a wide enough spread of messages to make most individuals consider Usenet news to be a high noise information resource. Furthermore, each user values a different set of messages. Both taste and prior knowledge are major factors in evaluating news articles. For example, readers of the rec.humor newsgroup, a group designed for jokes and other humorous postings, value articles based on whether they perceive them to be funny. Readers of technical groups, such as comp.lang.c? ? value articles based
265|Mining the Network Value of Customers|One of the major applications of data mining is in helping companies determine which potential customers to market to. If the expected pro t from a customer is greater than the cost of marketing to her, the marketing action for that customer is executed. So far, work in this area has considered only the intrinsic value of the customer (i.e, the expected pro t from sales to her). We propose to model also the customer&#039;s network value: the expected pro t from sales to other customers she may inuence to buy, the customers those may inuence, and so on recursively. Instead of viewing a market as a set of independent entities, we view it as a social network and model it as a Markov random eld. We show the advantages of this approach using a social network mined from a collaborative ltering database. Marketing that exploits the network value of customers|also known as viral marketing|can be extremely eective, but is still a black art. Our work can be viewed as a step towards providing a more solid foundation for it, taking advantage of the availability of large relevant databases. Categories and Subject Descriptors H.2.8 [Database Management]: Database Applications| data mining
266|Learning Collaborative Information Filters|Predicting items a user would like on the basis of other users’ ratings for these items has become a well-established strategy adopted by many recommendation services on the Internet. Although this can be seen as a classification problem, algo-rithms proposed thus far do not draw on results from the ma-chine learning literature. We propose a representation for collaborative filtering tasks that allows the application of virtually any machine learning algorithm. We identify the shortcomings of current collaborative filtering techniques and propose the use of learning algorithms paired with feature extraction techniques that specifically address the limitations of previous approaches. Our best-performing algorithm is based on the singular value decomposition of an initial matrix of user ratings, exploiting latent structure that essentially eliminates the need for users to rate common items in order to become predictors for one another&#039;s preferences. We evaluate the proposed algorithm on a large database of user ratings for motion pictures and find that our approach significantly out-performs current collaborative filtering algorithms.
267|Recommendation as Classification: Using Social and Content-Based Information in Recommendation|Recommendation systems make suggestions about artifacts to a user. For instance, they may predict whether a user would be interested in seeing a particular movie. Social recomendation methods collect ratings of artifacts from many individuals and use nearest-neighbor techniques to make recommendations to a user concerning new artifacts. However, these methods do not use the significant amount of other information that is often available about the nature of each artifact --- such as cast lists or movie reviews, for example. This paper presents an inductive learning approach to recommendation that is able to use both ratings information and other forms of information about each artifact in predicting user preferences. We show that our method outperforms an existing social-filtering method in the domain of movie recommendations on a dataset of more than 45,000 movie ratings collected from a community of over 250 users.  Introduction  Recommendations are a part of everyday life. We usually...
268|Application of Dimensionality Reduction in Recommender System -- A Case Study|We investigate the use of dimensionality reduction to improve performance for a new class of data analysis  software called &#034;recommender systems&#034;. Recommender systems apply knowledge discovery techniques to the problem of making product recommendations during a live customer interaction. These systems are achieving widespread success in E-commerce nowadays, especially with the advent of the Internet. The tremendous growth of customers and products poses three key challenges for recommender systems in the E-commerce domain. These are: producing high quality recommendations, performing many recommendations per second for millions of customers and products, and achieving high coverage in the face of data sparsity. One successful recommender system technology is collaborative filtering , which works by matching customer preferences to other customers in making recommendations. Collaborative filtering has been shown to produce high quality recommendations, but the performance degrades with ...
269|Combining collaborative filtering with personal agents for better recommendations|Information filtering agents and collaborative filtering both attempt to alleviate information overload by identifying which items a user will find worthwhile. Information filtering (IF) focuses on the analysis of item content and the development of a personal user interest profile. Collaborative filtering (CF) focuses on identification of other users with similar tastes and the use of their opinions to recommend items. Each technique has advantages and limitations that suggest that the two could be beneficially combined. This paper shows that a CF framework can be used to combine personal IF agents and the opinions of a community of users to produce better recommendations than either agents or users can produce alone. It also shows that using CF to create a personal combination of a set of agents produces better results than either individual agents or other combination mechanisms. One key implication of these results is that users can avoid having to select among agents; they can use them all and let the CF framework select the best ones for them.
270|Collaborative Filtering by Personality Diagnosis: A Hybrid Memory- and Model-Based Approach|The growth of Internet commerce has stimulated  the use of collaborative filtering (CF) algorithms  as recommender systems. Such systems leverage  knowledge about the known preferences of  multiple users to recommend items of interest to  other users. CF methods have been harnessed  to make recommendations about such items as  web pages, movies, books, and toys. Researchers  have proposed and evaluated many approaches  for generating recommendations. We describe  and evaluate a new method called personality  diagnosis (PD). Given a user&#039;s preferences for  some items, we compute the probability that he  or she is of the same &#034;personality type&#034; as other  users, and, in turn, the probability that he or she  will like new items. PD retains some of the advantages  of traditional similarity-weighting techniques  in that all data is brought to bear on each  prediction and new data can be added easily and  incrementally. Additionally, PD has a meaningful  probabilistic interpretation, which ma...
271|Footprints: History-Rich Tools for Information Foraging|Inspired by Hill and Hollan&#039;s original work [6], we have been developing a theory of interaction history and building tools to apply this theory to navigation in a complex information space. We have built a series of tools --- map, trails, annotations and signposts --- based on a physical-world navigation metaphor. These tools have been in use for over a year. Our user study involved a controlled browse task and showed that users were able to get the same amount of work done with significantly less effort.  Keywords  information navigation, information foraging, interaction history, Web browsing  INTRODUCTION  Digital information has no history. It comes to us devoid of the patina that forms on physical objects as they are used. In the non-digital world we make extensive use of these traces to guide our actions, to make choices, and to find things of importance or interest. We call this area interaction history; that is, the records of the interactions of people and objects. Physical o...
272|Dependency networks for inference, collaborative filtering, and data visualization |We describe a graphical model for probabilistic relationships|an alternative tothe Bayesian network|called a dependency network. The graph of a dependency network, unlike aBayesian network, is potentially cyclic. The probability component of a dependency network, like aBayesian network, is a set of conditional distributions, one for each nodegiven its parents. We identify several basic properties of this representation and describe a computationally e cient procedure for learning the graph and probability components from data. We describe the application of this representation to probabilistic inference, collaborative ltering (the task of predicting preferences), and the visualization of acausal predictive relationships.
273|Collaborative filtering with privacy via factor analysis|Collaborative filtering is valuable in e-commerce, and for direct recommendations for music, movies, news etc. But today’s systems use centralized databases and have several disadvantages, including privacy risks. As we move toward ubiquitous computing, there is a great potential for individuals to share all kinds of information about places and things to do, see and buy, but the privacy risks are severe. In this paper we introduce a peer-to-peer protocol for collaborative filtering which protects the privacy of individual data. A second contribution of this paper is a new collaborative filtering algorithm based on factor analysis which appears to be the most accurate method for CF to date. The new algorithm has other advantages in speed and storage over previous algorithms. It is based on a careful probabilistic model of user choice, and on a probabilistically sound approach to dealing with missing data. Our experiments on several test datasets show that the algorithm is more accurate than previously reported methods, and the improvements increase with the sparseness of the dataset. Finally, factor analysis with privacy is applicable to other kinds of statistical analyses of survey or questionaire data scientists (e.g. web surveys or questionaires).
274|Information Filtering Based on User Behavior Analysis|Information filtering systems have potential power that may provide an efficient means of navigating through large and diverse data space. However, current information filtering technology heavily depends on a user’s active participation for describing the user’s interest to information items, forcing the user to accept extra load to overcome thealready loaded situation. Fumhemo~, because theuser&#039;s interests weoften expressed indiscrete fomat such as a set of keywords sometimes augmented with if-then rules, it is difficult to express ambiguous interests, which users often want to do. We propose a technique that uses user behavior monitonng to transparently capture the user’sinterest in information, andatechnique to use this interest to fikerincoming information in avery efficient way. The proposed techniques are verified to perform very well by having conducted a field experiment and a series of simulation. 1
275|Using Filtering Agents to Improve Prediction Quality in the GroupLens Research Collaborative Filtering System|Collaborative filtering systems help address information overload by using the opinions of users in a community to make personal recommendations for documents to each user. Many collaborative filtering systems have few user opinions relative to the large number of documents available. This sparsity problem can reduce the utility of the filtering system by reducing the number of documents for which the system can make recommendations and adversely affecting the quality of recommendations.  This paper defines and implements a model for integrating content-based ratings into a collaborative filtering system. The filterbot model allows collaborative filtering systems to address sparsity by tapping the strength of content filtering techniques. We identify and evaluate metrics for assessing the effectiveness of filterbots specifically, and filtering system enhancements in general. Finally, we experimentally validate the filterbot approach by showing that even simple filterbots such as spell ...
276|Getting to Know You: Learning New User Preferences in Recommender Systems|Recommender systems have become valuable resources for users seeking intelligent ways to search through the enormous volume of information available to them. One crucial unsolved problem for recommender systems is how best to learn about a new user. In this paper we study six techniques that collaborative filtering recommender systems can use to learn about new users. These techniques select a sequence of items for the collaborative filtering system to present to each new user for rating. The techniques include the use of information theory to select the items that will give the most value to the recommender system, aggregate statistics to select the items the user is most likely to have an opinion about, balanced techniques that seek to maximize the expected number of bits learned per presented item, and personalized techniques that predict which items a user will have an opinion about. We study the techniques thru offline experiments with a large preexisting user data set, and thru a live experiment with over 300 users. We show that the choice of learning technique significantly affects the user experience, in both the user effort and the accuracy of the resulting predictions.
277|Horting Hatches an Egg: A New Graph-Theoretic Approach to Collaborative Filtering|This paper introduces a new and novel approach to ratingbased collaborative filtering. The new technique is most appropriate for e-commerce merchants offering one or more groups of relatively homogeneous items such as compact disks, videos, books, software and the like. In contrast with other known collaborative filtering techniques, the new algorithm is graph-theoretic, based on the twin new concepts of horting and predictability. As is demonstrated in this paper, the technique is fast, scalable, accurate, and requires only a modest learning curve. It makes use of a hierarchical classification scheme in order to introduce context into the rating process, and uses so-called creative links in order to find surprising and atypical items to recommend, perhaps even items which cross the group boundaries. The new technique is one of the key engines of the Intelligent Recommendation Algorithm (IRA) project, now being developed at IBM Research. In addition to several other recommendation engines, IRA contains a situation analyzer to determine the most appropriate mix of engines for a particular e-commerce merchant, as well as an engine for optimizing the placement of advertisements.
278|An empirical analysis of design choices in neighborhoodbased collaborative filtering algorithms|Abstract. Collaborative filtering systems predict a user’s interest in new items based on the recommendations of other people with similar interests. Instead of performing content indexing or content analysis, collaborative filtering systems rely entirely on interest ratings from members of a participating community. Since predictions are based on human ratings, collaborative filtering systems have the potential to provide filtering based on complex attributes, such as quality, taste, or aesthetics. Many implementations of collaborative filtering apply some variation of the neighborhood-based prediction algorithm. Many variations of similarity metrics, weighting approaches, combination measures, and rating normalization have appeared in each implementation. For these parameters and others, there is no consensus as to which choice of technique is most appropriate for what situations, nor how significant an effect on accuracy each parameter has. Consequently, every person implementing a collaborative filtering system must make hard design choices with little guidance. This article provides a set of recommendations to guide design of neighborhood-based prediction systems, based on the results of an empirical study. We apply an analysis framework that divides the neighborhood-based prediction approach into three components and then examines variants of the key parameters in each component. The three components identified are similarity computation, neighbor selection, and rating combination.
279|Overview of the Seventh Text REtrieval Conference TREC-7|This paper serves as an introduction to the research described in detail in the remainder of the volume. It concentrates mainly on the main task, ad hocretrieval, which is de#ned the next section. Details regarding the test collections and evaluation methodology used in TREC followinsections 3 and 4, while section 5 provides an overview of the ad hoc retrieval results. In addition to the main ad hoc task, TREC-7 contained seven #tracks,&#034; tasks that focus research on particular subproblems of text retrieval. Taken together, the tracks represent the bulk of the experiments performed in TREC-7. However, each track has its own overview paper included in the proceedings, so this paper presents only a short summary of each track in section 6. The #nal section looks forward to future TREC conferences
280|Beyond Algorithms: An HCI Perspective on Recommender Systems|The accuracy of recommendations made by an online Recommender System (RS) is mostly dependent on the underlying collaborative filtering algorithm. However, the ultimate effectiveness of an RS is dependent on factors that go beyond the quality of the algorithm. The goal of an RS is to introduce users to items that might interest them, and convince users to sample those items. What design elements of an RS enable the system to achieve this goal? To answer this question, we examined the quality of recommendations and usability of three book RS (Amazon.com, RatingZone &amp; Sleeper) and three movie RS (Amazon.com, MovieCritic, Reel.com). Our findings indicate that from a user&#039;s perspective, an effective recommender system inspires trust in the system; has system logic that is at least somewhat transparent; points users towards new, not-yet-experienced items; provides details about recommended items, including pictures and community ratings; and finally, provides ways to refine recommendations by including or excluding particular genres. Users expressed willingness to provide more input to the system in return for more effective recommendations. 
281|Variations in relevance assessments and the measurement of retrieval effectiveness|The purpose of this article is to bring attention to the prob-lem of variations in relevance assessments and the effects that these may have on measures of retrieval effective-ness. Through an analytical review of the literature, I show that despite known wide variations in relevance assess-ments in experimental test collections, their effects on the measurement of retrieval performance are almost com-pletely unstudied. I will further argue that what we know about the many variables that have been found to affect relevance assessments under experimental conditions, as well as our new understanding of psychological, situa-tional, user-based relevance, point to a single conclusion. We can no longer rest the evaluation of information re-trieval systems on the assumption that such variations do not significantly affect the measurement of information re-trieval performance. A series of thorough, rigorous, and extensive tests is needed, of precisely how, and under what conditions, variations in relevance assessments do, and do not, affect measures of retrieval performance. We need to develop approaches to evaluation that are sensi-tive to these variations and to human factors and individual differences more generally. Our approaches to evaluation must reflect the real world of real users.
282|Let&#039;s stop pushing the envelope and start addressing it: a Reference Task Agenda for HCI|We identify a problem with the process of research in the HCI community -- an overemphasis on &#034;radical invention&#034; at the price of achieving a common research focus. Without such a focus, it is difficult to build on previous work, to compare different interaction techniques objectively, and to make progress in developing theory. These problems at the research level have implications for practice, too; as
283|Why batch and user evaluations do not give the same results |Much system-oriented evaluation of information retrieval systems has used the Cranfield approach based upon queries run against test collections in a batch mode. Some researchers have questioned whether this approach can be applied to the real world, but little data exists for or against that assertion. We have studied this question in the context of the TREC Interactive Track. Previous results demonstrated that improved performance as measured by relevance-based metrics in batch studies did not correspond with the results of outcomes based on real user searching tasks. The experiments in this paper analyzed those results to determine why this occurred. Our assessment showed that while the queries entered by real users into systems yielding better results in batch studies gave comparable gains in ranking of relevant documents for those users, they did not translate into better performance on specific tasks. This was most likely due to users being able to adequately find and utilize relevant documents ranked further down the output list.
284|Measuring retrieval effectiveness based on user preference of documents|The notion of user preference is adopted for the representation, interpretation and measurement of the relevance or usefulness of documents. User judgments on documents may be formally described by a weak order (i.e., user ranking) and measured using an ordinal scale. Within this framework, a new measure of system performance is suggested based on the distance between user ranking and system ranking. It only uses the relative order of documents and therefore confirms to the valid use of an ordinal scale measuring relevance. It is also applicable to multi-level relevance judgments and ranked system output. The appropriateness of the proposed measure is demonstrated through an axiomatic approach. The inherent relationships between the new measure and many existing measures provide further supporting evidence. 1 1
285|Effectiveness of information retrieval methods|Distribution of this document is unlimited. It may be released to the Clearinghouse, Department of Commerce, for sale to the general public.
286|Better or Just Different? On the Benefits of Designing Interactive Systems in Terms of Critical Parameters|Critical parameters are quantitative measures of performance that may be used to determine the overall ability of a design to serve its purpose. Although critical parameters figure in almost every field of design where there is a demand for progressive improvement, they do not appear to figure significantly in the design of interactive systems. As a result, systems are designed that are recognizably different from other systems but not necessarily better at doing the job intended. This paper discusses the role of critical parameters in design, and illustrates their lack of use in interactive system design by presenting a number of of examples drawn from the HCI literature. It identifies a consequent need for research to establish critical parameters for applications and to build models of the performance of designs against these parameters. Some ideas are presented on how critical parameters might be established for specific applications, and the paper concludes by summarising some of the benefits that might be gained from moving in this direction.
287|Inferring User Interest|Recommender systems provide personalized suggestions about items that users will find interesting. Typically, recommender systems require a user interface that can determine the interest of a user and use this information to make suggestions. The common solution, explicit ratings, where users tell the system what they think about a piece of information, is well-understood and fairly precise. However, having to stop to enter explicit ratings can alter normal patterns of browsing and reading. A less intrusive method is to use implicit ratings, where a rating is obtained by a method other than obtaining it directly from the user. This research studies the correlation between various implicit ratings and the explicit rating for a single Web page, and the impact of implicit interest indicators on user privacy. We developed a Web browser that records a user&#039;s actions (implicit ratings) and the explicit rating for each page visited. The browser was used by over 70 people that browsed more than 2500 Web pages. We find that the time spent on a page, the amount of scrolling on a page and the combination of time and scrolling has a strong correlation with explicit interest, while individual scrolling methods and mouse-clicks are ineffective in predicting explicit interest.
288|Evaluating Expertise Recommendations|Finding a person who has the experience to solve a specific precif is an imporKRI application ofr1SSWRI-1W systems to a difficultorcultRA1AWAR pr11Rr Pr11 systems have made attempts to implement solutions to thisprsRDAK but few systems haveunderC12 systematicuser evaluation. This wor descrDWA a systematic evaluation of theExperWAW Recommender (ER), a system thatrtRAKAC1R people who ar likely to haveexper1Ain a specific prcific ER and the or2-1CRICCW11 contextfor which it was designedar descredR toprAA2R a basisfor underDM-2RIC this evaluation.Pral t o conducting the evaluation, a baselineexpereRA2 showed that peoplear rpleRWKSgood at judging coworRD1- experRD when given an appr1RIM2W context. This finding prdingR a way to demonstrKW the effectiveness of ER bycompar-R ER&#039;s per-1MRIMK torRAAMM by coworARIM The evaluation, the design, andrdR2SW ar descrSRI in detail. ThereRDMD suggest that the parDMARIM-W agr with therRAKWAKRIM-W11 made by ER, and that ER significantly outpercant other experrc rperrcan systems when compared using similar metrics.
289|An Empirical Evaluation of User Interfaces for Topic Management of Web Sites|Topic management is the task of gathering, evaluating, organizing, and sharing a set of web sites for a specific topic. Current web tools do not provide adequate support for this task. We created the TopicShop system to address this need. TopicShop includes (1) a webcrawler that discovers relevant web sites and builds site profiles, and (2) user interfaces for exploring and organizing sites. We conducted an empirical study comparing user performance with TopicShop vs. Yahoo. TopicShop subjects found over 80% more high-quality sites (where quality was determined by independent expert judgements) while browsing only 81% as many sites and completing their task in 89% of the time. The site profile data that TopicShop provides -- in particular, the number of pages on a site and the number of other sites that link to it -- was the key to these results, as users exploited it to identify the most promising sites quickly and easily.  KEYWORDS  information access, information retrieval, informat...
290|Is Seeing Believing? How Recommender Interfaces Affect Users&#039; Opinions|Recommender systems use people&#039;s opinions about items in an information domain to help people choose other items. These systems have succeeded in domains as diverse as movies, news articles, Web pages, and wines. The psychological literature on conformity suggests that in the course of helping people make choices, these systems probably affect users&#039; opinions of the items. If opinions are influenced by recommendations, they might be less valuable for making recommendations for other users. Further, manipulators who seek to make the system generate artificially high or low recommendations might benefit if their efforts influence users to change the opinions they contribute to the recommender. We study two aspects of recommender system interfaces that may affect users&#039; opinions: the rating scale and the display of predictions at the time users rate items. We find that users rate fairly consistently across rating scales. Users can be manipulated, though, tending to rate toward the prediction the system shows, whether the prediction is accurate or not. However, users can detect systems that manipulate predictions. We discuss how designers of recommender systems might react to these findings.
291|MovieLens Unplugged: Experiences with a recommender system on four mobile devices|Recommender systems have changed the way people shop online. Recommender systems on wireless mobile devices may have the same impact on the way people shop in stores. There are several important challenges that interface designers must overcome on mobile devices: Providing sufficient value to attract prospective wireless users, handling occasionally connected devices, privacy and security, and surmounting the physical limitations of the devices. We present our experience with the implementation of a wireless movie recommender system on a cell phone browser, an AvantGo channel, a wireless PDA, and a voice-only phone interface. These interfaces help MovieLens users select movies to rent, buy, or see while away from their computer. The results of a nine month field study show that although wireless has still not arrived for the majority of users, mobile recommender systems have the potential to provide value to their users today.
292|Generative Models for Cold-Start Recommendations|Systems for automatically recommending items (e.g., movies, products, or information) to users are becoming increasingly important in e-commerce applications, digital libraries, and other domains where mass personalization is highly valued. Such recommender systems typically base their suggestions on (1) collaborative data encoding which users like which items, and/or (2) content data describing item features and user demographics. Systems that rely solely on collaborative data fail when operating from a cold start|that is, when recommending items (e.g., rst-run movies) that no member of the community has yet seen. We develop several generative probabilistic models that circumvent the cold-start problem by mixing content data with collaborative data in a sound statistical manner. We evaluate the algorithms using MovieLens movie ratings data, augmented with actor and director information from the Internet Movie Database. We nd that maximum likelihood learning with the expectation maximization (EM) algorithm and variants tends to over t complex models that are initialized randomly. However, by seeding parameters of the complex models with parameters learned in simpler models, we obtain greatly improved performance. We explore both methods that exploit a single type of content data (e.g., actors only) and methods that leverage multiple types of content data (e.g., both actors and directors) simultaneously.
293|Experiments in social data mining: The topicshop system|Social data mining systems enable people to share opinions and benefit from each other’s experience. They do this by mining and redistributing information from computational records of social activity such as Usenet messages, system usage history, citations, or hyperlinks. Some general questions for evaluating such systems are: (1) is the extracted information valuable? and (2) do interfaces based on the information improve user task performance? We report here on TopicShop, a system that mines information from the structure and content of Web pages and provides an exploratory information workspace interface. We carried out experiments that yielded positive answers to both evaluation questions. First, a number of automatically computable features about Web sites do a good job of predicting expert quality judgments about sites. Second, compared to popular Web search interfaces, the TopicShop interface to this information lets users select significantly more high-quality sites, in less time and with less effort, and to organize the sites they select into personally meaningful collections more quickly and easily. We conclude by discussing how our results may be applied and considering how they touch on general issues concerning quality, expertise, and consensus.
294|An Examination of Trust Production in Computer-Mediated Exchange|In this paper, we apply principles of trust derived mostly from interpersonal communication and human-computer interaction research to computer-mediated exchange (CME). We define key terms and synthesize relevant literature identifying four sources and seven dimensions of trust. Combining these sources and dimensions, we offer a trust taxonomy enabling trust analysis of exchange partners in CME. To demonstrate the usefulness of the taxonomy, we report on a case study in which we examined trust production methods in three exchange sites and compared the results.
295|A Framework for Uplink Power Control in Cellular Radio Systems|In cellular wireless communication systems, transmitted power is regulated to provide each user an acceptable connection by limiting the interference caused by other users. Several models have been considered including: (1) fixed base station assignment where the assignment of users to base stations is fixed, (2) minimum power assignment where a user is iteratively assigned to the base station at which its signal to interference ratio is highest, and (3) diversity reception, where a user&#039;s signal is combined from several or perhaps all base stations. For the above models, the uplink power control problem can be reduced to finding a vector p of users&#039; transmitter powers satisfying p  I(p) where the  jth constraint p j  I j (p) describes the interference that user j must overcome to achieve an acceptable connection. This work unifies results found for these systems by identifying common properties of the interference constraints. It is also shown that systems in which transmitter powers ...
296|A simple distributed autonomous power control algorithm and its convergence| For wireless cellular communication systems, one seeks a simple effective means of power control of signals associated with randomly dispersed users that are reusing a single channel in different cells. By effecting the lowest interference environment, in meeting a required minimum signal-to-interference ratio of p per user, channel reuse is maximized. Distributed procedures for doing this are of special interest, since the centrally administered alternative requires added infrastructure, latency, and network vulnerability. Successful distributed powering entails guiding the evolution of the transmitted power level of each of the signals, using only local measurements, so that eventually all users meet the p requirement. The local per channel power measurements include that of the intended signal as well as the undesired interference from other users (plus receiver noise). For a certain simple distributed type of algorithm, whenever power settings exist for which all users meet the p requirement, we demonstrate exponentially fast convergence to these settings.  
297|Erlang capacity of a power controlled CDMA system|Abstract-This paper presents an approach to the evaluation of the reverse link capacity of a CDMA cellular voice system which employs power control and a variable rate vocoder based on voice activity. It is shown that the Erlang capacity of CDMA is many times that of conventional analog systems and several times that of other digital multiple access systems. I.
298|Integrated Power Control and Base Station Assignment|In cellular wireless communication systems, transmitted power is regulated to provide each user an acceptable connection while limiting the interference seen by other users. Previous work has focused on maximizing the minimum carrier to interference ratio (CIR) or attaining a common CIR over all radio links. However, previous work has assumed the assignment of mobiles to base stations is known and fixed. In this work, we integrate power control and base station assignment. In the context of a CDMA system, we consider the minimization of the total transmitted uplink power subject to maintaining an individual target CIR for each mobile. This minimization occurs over the set of power vectors and base station assignments. We show that this problem has special structure and identify synchronous and asynchronous distributed algorithms that find the optimal power vector and base station assignment.  Keywords--- Power control, Cellular radio, CDMA, Handoff, Base station assignment I. Introduct...
299|Radio Link Admission Algorithms for Wireless Networks with Power Control and Active Link Quality Protection|In this paper we present a distributed power control scheme, which maintains the SIRs of operational (active) links above their required thresholds at all time (link quality protection), while new users are being admitted; furthermore, when new users cannot be successfully admitted, existing ones do not suffer fluctuations of their SIRs below their required thresholds values. We also present two admission /rejection control algorithms, which exercise voluntary drop-out of links inadmissible to the network, so as to reduce interference and possibly facilitate the admission of other links.
300|Transmitter Power Control for Co-channel Interference Management in Cellular Radio Systems|Transmitter power control is a necessity to combat &#034;near-far&#034; problems in radio systems using receivers with limited &#034;dynamic range&#034; (such as conventional DSCDMA systems). Transmitter power control, however, can also be used to control cochannel interference, i.e. interference from other users using the same channel (code or time slot). For this purpose, it has been shown that the optimum transmitter power configuration is found by solving an eigenvalue problem. The paper reviews some recent results in this area. The basic models as well as the concepts of achievable C/I, up/down-link equivalence and C/Ibalancing are introduced. Both the interference limited (noise-less) case as well as models including thermal noise are treated. Results show that substantial improvements in system capacity can be achieved, particulary in conjunction with Dynamic Channel Allocation. The optimum power control schemes are shown to be robust against implementational shortcomings.  1. Introduction  Control...
301|Simultaneous Optimization and Evaluation of Multiple Dimensional Queries|Database researchers have made significant progress on several research issues related to multidimensional data analysis, including the development of fast cubing algorithms, efficient schemes for creating and maintaining precomputed group-bys, and the design of efficient storage structures for multidimensional data. However, to date there has been little or no work on multidimensional query optimization. Recently, Microsoft has proposed &#034;OLE DB for OLAP&#034; as a standard multidimensional interface for databases. OLE DB for OLAP defines Multi-Dimensional Expressions (MDX), which have the interesting and challenging feature of allowing clients to ask several related dimensional queries in a single MDX expression. In this paper, we present three algorithms to optimize multiple related dimensional queries. Two of the algorithms focus on how to generate a global plan from several related local plans. The third algorithm focuses on generating a good global plan without first generating local p...
302|Selection Predicate Indexing for Active Databases Using Interval Skip Lists|A new, efficient selection predicate indexing scheme for active database systems is introduced.  The selection predicate index proposed uses an interval index on an attribute of a relation or  object collection when one or more rule condition clauses are defined on that attribute. The  selection predicate index uses a new type of interval index called the interval skip list (IS-list).  The IS-list is designed to allow efficient retrieval of all intervals that overlap a point, while allowing  dynamic insertion and deletion of intervals. IS-list algorithms are described in detail. The IS-list  allows efficient on-line searches, insertions, and deletions, yet is much simpler to implement than  other comparable interval index data structures such as the priority search tree and balanced  interval binary search tree (IBS-tree). IS-lists require only one third as much code to implement  as balanced IBS-trees. The combination of simplicity, performance, and dynamic updateability  of the IS-li...
303|A Trace-Driven Analysis of the UNIX 4.2 BSD File System|We analyzed the UNIX 4.2 BSD file system by recording userlevel activity in trace files and writing programs to analyze the traces. The tracer did not record individual read and write operations, yet still provided tight bounds on what information was accessed and when. The trace analysis shows that the average file system bandwidth needed per user is low (a few hundred bytes per second). Most of the files accessed are open only a short time and are accessed sequentially. Most new information is deleted or overwritten within a few minutes of its creation. We also wrote a simulator that uses the traces to predict the performance of caches for disk blocks. The moderate-sized caches used in UNIX reduce disk traffic for file blocks by about 50%, but larger caches (several megabytes) can eliminate 90% or more of all disk traffic. With those large caches, large block sizes (16 kbytes or more) result in the fewest disk accesses.  Trace-Driven Analysis of 4.2 BSD File System January 2, 1993 1...
304|Bigtable: A distributed storage system for structured data|Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.  
305|Space/Time Trade-offs in Hash Coding with Allowable Errors|this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash- coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency
306|The ubiquitous B-tree|B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and nnplemented using B-trees This paper reviews B-trees and shows why they have been so successful It discusses the major variations of the B-tree, especially the B+-tree,
307|Resource Containers: A New Facility for Resource Management in Server Systems|General-purpose operating systems provide inadequate support for resource management in large-scale servers. Applications lack sufficient control over scheduling and management of machine resources, which makes it difficult to enforce priority policies, and to provide robust and controlled service. There is a fundamental mismatch between the original design assumptions underlying the resource management mechanisms of current general-purpose operating systems, and the behavior of modern server applications. In particular, the operating system’s notions of protection domain and resource principal coincide in the process abstraction. This coincidence prevents a process that manages large numbers of network connections, for example, from properly allocating system resources among those connections. We propose and evaluate a new operating system abstraction called a resource container, which separates the notion of a protection domain from that of a resource principal. Resource containers enable fine-grained resource management in server systems and allow the development of robust servers, with simple and firm control over priority policies. 1
308|Operating System Support for Planetary-Scale Network Services|PlanetLab is a geographically distributed overlay network designed to support the deployment and evaluation of planetary-scale network services. Two high-level goals shape its design. First, to enable a large research community to share the infrastructure, PlanetLab provides distributed virtualization, whereby each service runs in an isolated slice of PlanetLab’s global resources. Second, to support competition among multiple network services, PlanetLab decouples the operating system running on each node from the networkwide services that define PlanetLab, a principle referred to as unbundled management. This paper describes how Planet-Lab realizes the goals of distributed virtualization and unbundled management, with a focus on the OS running on each node. 1
309|Paxos made live: an engineering perspective|We describe our experience building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected algorithmic and engineering problems encountered, and the solutions we found for them. Our measurements indicate that we have built a competitive system. 1
310|Integrating compression and execution in column-oriented database systems|Column-oriented database system architectures invite a reevaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads. In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in roworiented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.
311|Weaving Relations for Cache Performance|Relational database systems have traditionally optimzed for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM) (a.k.a., slotted pages). Recent research, however, indicates that cache utilization and performance is becoming increasingly important on modern platforms. In this paper, we first demonstrate that in-page data placement is the key to high cache performance and that NSM exhibits low cache utilization on modern platforms. Next, we propose a new data organization model called PAX (Partition Attributes Across), that significantly improves cache performance by grouping together all values of each attribute within each page. Because PAX only affects layout inside the pages, it incurs no storage penalty and does not affect I/O behavior. According to our experimental results, when compared to NSM (a) PAX exhibits superior cache and memory bandwidth utilization, saving at least 75% of NSM&#039;s stall time due to data cache accesses, (b) range selection queries and updates on memoryresident relations execute 17-25% faster, and (c) TPC-H queries involving I/O execute 11-48% faster.
312|DB2 Parallel Edition|The rate of increase in database size and response time requirements has outpaced  advancements in processor and mass storage technology. One way to satisfy the increasing  demand for processing power and I/O bandwidth in database applications is to have a  number of processors, loosely or tightly coupled, serving database requests concurrently.  Technologies developed during the last decade have made commercial parallel database  systems a reality and these systems have made an inroad into the stronghold of traditionally  mainframe based large database applications. This paper describes the parallel database  project initiated at IBM Research at Hawthorne and the DB2/AIX-PE product based on  it.  1 Introduction  Large scale parallel processing technology has made giant strides in the past decade and there is no doubt that it has established a place for itself. However, almost all of the applications harnessing this technology are scientific or engineering applications. The lack of com...
313|Tinydb: An acquisitional query processing system for sensor networks|We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. Categories and Subject Descriptors: H.2.3 [Database Management]: Languages—Query languages; H.2.4 [Database Management]: Systems—Distributed databases; query processing
314|Directed Diffusion: A scalable and robust communication paradigm for sensor networks|Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.
315|System architecture directions for networked sensors|Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world or spread throughout our environment. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances. 
316|ANALYSIS OF WIRELESS SENSOR NETWORKS FOR HABITAT MONITORING|  We provide an in-depth study of applying wireless sensor networks (WSNs) to real-world habitat monitoring. A set of system design requirements were developed that cover the hardware design of the nodes, the sensor network software, protective enclosures, and system architecture to meet the requirements of biologists. In the summer of 2002, 43 nodes were deployed on a small island off the coast of Maine streaming useful live data onto the web. Although researchers anticipate some challenges arising in real-world deployments of WSNs, many problems can only be discovered through experience. We present a set of experiences from a four month long deployment on a remote island. We analyze the environmental and node health data to evaluate system performance. The close integration of WSNs with their environment provides environmental data at densities previously impossible. We show that the sensor data is also useful for predicting system operation and network failures. Based on over one million 2 Polastre et. al. data readings, we analyze the node and network design and develop network reliability profiles and failure models.
318|The nesC language: A holistic approach to networked embedded systems|We present nesC, a programming language for networked embedded systems that represent a new design space for application developers. An example of a networked embedded system is a sensor network, which consists of (potentially) thousands of tiny, lowpower “motes, ” each of which execute concurrent, reactive programs that must operate with severe memory and power constraints. nesC’s contribution is to support the special needs of this domain by exposing a programming model that incorporates event-driven execution, a flexible concurrency model, and component-oriented application design. Restrictions on the programming model allow the nesC compiler to perform whole-program analyses, including data-race detection (which improves reliability) and aggressive function inlining (which reduces resource consumption). nesC has been used to implement TinyOS, a small operating system for sensor networks, as well as several significant sensor applications. nesC and TinyOS have been adopted by a large number of sensor network research groups, and our experience and evaluation of the language shows that it is effective at supporting the complex, concurrent programming style demanded by this new class of deeply networked systems.
319|Timing-Sync Protocol for Sensor Networks|Wireless ad-hoc sensor networks have emerged as an interesting and important research area in the last few years. The applications envisioned for such networks require collaborative execution of a distributed task amongst a large set of sensor nodes. This is realized by exchanging messages that are timestamped using the local clocks on the nodes. Therefore, time synchronization becomes an indispensable piece of infrastructure in such systems. For years, protocols such as NTP have kept the clocks of networked systems in perfect synchrony. However, this new class of networks has a large density of nodes and very limited energy resource at every node; this leads to scalability requirements while limiting the resources that can be used to achieve them. A new approach to time synchronization is needed for sensor networks.
320|TelegraphCQ: Continuous Dataflow Processing for an Uncertan World|Increasingly pervasive networks are leading towards a world where data is constantly in motion. In such a world, conventional techniques for query processing, which were developed under the assumption of a far more static and predictable computational environment, will not be sufficient. Instead, query processors based on adaptive dataflow will be necessary. The Telegraph project has developed a suite of novel technologies for continuously adaptive query processing. The next generation Telegraph system, called TelegraphCQ, is focused on meeting the challenges that arise in handling large streams of continuous queries over high-volume, highly-variable data streams. In this paper, we describe the system architecture and its underlying technology, and report on our ongoing implementation effort, which leverages the PostgreSQL open source code base. We also discuss open issues and our research agenda.
321|The Cougar Approach to In-Network Query Processing in Sensor Networks|The widespread distribution and availability of smallscale sensors, actuators, and embedded processors is transforming the physical world into a computing platform. One such example is a sensor network consisting of a large number of sensor nodes that combine physical sensing capabilities such as temperature, light, or seismic sensors with networking and computation capabilities. Applications range from environmental control, warehouse inventory, and health care to military environments. Existing sensor networks assume that the sensors are preprogrammed and send data to a central frontend where the data is aggregated and stored for offline querying and analysis. This approach has two major drawbacks. First, the user cannot change the behavior of the system on the fly. Second, conservation of battery power is a major design factor, but a central system cannot make use of in-network programming, which trades costly communication for cheap local computation.
322|A Transmission Control Scheme for Media Access in Sensor Networks|We study the problem of media access control in the novel regime of sensor networks, where unique application behavior and tight constraints in computation power, storage, energy resources, and radio technology have shaped this design space to be very different from that found in traditional mobile computing regime. Media access control in sensor networks must not only be energy efficient but should also allow fair bandwidth allocation to the infrastructure for all nodes in a multihop network. We propose an adaptive rate control mechanism aiming to support these two goals and find that such a scheme is most effective in achieving our fairness goal while being energy effcient for both low and high duty cycle of network traffic.
323|Routing indices for peer-to-peer systems|Finding information in a peer-to-peer system currently requires either a costly and vulnerable central index, or ooding the network with queries. In this paper we introduce the concept of Routing Indices (RIs), which allow nodes to forward queries to neighbors that are more likely to have answers. If a node cannot answer a query, it forwards the query to a subset of its neighbors, based on its local RI, rather than by selecting neighbors at random or by ooding the network by forwarding the query to all neighbors. We present three RI schemes: the compound, the hop-count, and the exponential routing indices. We evaluate their performance via simulations, and nd that RIs can improve performance by one or two orders of magnitude vs. a ooding-based system, and by up to 100 % vs. a random forwarding system. We also discuss the tradeo s between the di erent RIschemes and highlight the e ects of key design variables on system performance.
325|Optimization of nonrecursive queries|State-of-the-art optimization approaches for relational database systems, e.g., those used in systems such as OBE, SQL/DS, and commercial INGRES. when used for queries in non-traditional database applications, suffer from two problems. First, the time complexity of their optimization algorithms, being combinatoric, is exponential in the number of relations to be joined in the query. Their cost is therefore prohibitive in situa-tions such as deductive databases and logic oriented languages for knowledge bases, where hundreds of joins may be required. The second problem with the traditional approaches is that, al-beit effective in their specific domain, it is not clear whether they can be generalized to different scenarios (e.g. parallel evalua-tion) since they lack a formal model to define the assumptions and critical factors on which their valiclity depends. This paper
326|Extensible/Rule Based Query Rewrite Optimization in Starburst|This paper describes the Query Rewrite facility of the Starburst extensible database system, a novel phase of query optimization. We present a suite of rewrite rules used in Starburst to transform queries into equivalent queries for faster execution, and also describe the production rule engine which is used by Starburst to choose and execute these rules. Examples are provided demonstrating that these Query Rewrite transformations lead to query execution time improvements of orders of magnitude, suggesting that Query Rewrite in general --- and these rewrite rules in particular --- are an essential step in query optimization for modern database systems.  1 Introduction  In traditional database systems, query optimization typically consists of a single phase of processing in which access methods, join orders and join methods are chosen to provide an efficient plan for executing a user&#039;s declarative query. We refer to this phase as plan optimization. In this paper we present a distinct ph...
327|Query Processing, Approximation, and Resource Management In a Data Stream Management System|This paper describes our ongoing work developing the Stanford Stream Data Manager (STREAM), a system for executing continuous queries over multiple continuous data streams. The STREAM system supports a declarative query language, and it copes with high data rates and query workloads by providing approximate answers when resources are limited. This paper describes specific contributions made so far and enumerates our next steps in developing a general-purpose Data Stream Management System.
328|Querying in highly mobile distributed environments|New challenges to the database field brought about by the rapidly growing area of wireless personal communication are presented. Preliminary solutions to two of the major problems, control of update volume and integration of query processing and data acquisition, are discussed along with the simulation results. 1
329|Adaptive Query Processing: Technology in Evolution|As query engines are scaled and federated, they must cope with highly unpredictable and changeable environments. In the Telegraph project, we are attempting to architect and implement a continuously adaptive query engine suitable for global-area systems, massive parallelism, and sensor networks. To set the stage for our research, we present a survey of prior work on adaptive query processing, focusing on three characterizations of adaptivity: the frequency of adaptivity, the effects of adaptivity, and the extent of adaptivity. Given this survey, we sketch directions for research in the Telegraph project.  
330|Database System Issues in Nomadic Computing|Mobile computers and wireless networks are emerging technologies that will soon be available to a  wide variety of computer users. Unlike earlier generations of laptop computers, the new generation  of mobile computers can be an integrated part of a distributed computing environment, one in  which users change physical location frequently. The result is a new computing paradigm, nomadic  computing. This paradigm will affect the design of much of our current systems software, including  that of database systems.  This paper discusses in some detail the impact of nomadic computing on a number of traditional  database system concepts. In particular, we point out how the reliance on short-lived batteries  changes the cost assumptions underlying query processing. In these systems, power consumption  competes with resource utilization in the definition of cost metrics. We also discuss how the likelihood  of temporary disconnection forces consideration of alternative transaction processing pr...
331|Optimization techniques for queries with expensive methods|Object-Relational database management systems allow knowledgeable users to de ne new data types, as well as new methods (operators) for the types. This exibility produces an attendant complexity, which must be handled in new ways for an Object-Relational database management system to be e cient. In this paper we study techniques for optimizing queries that contain time-consuming methods. The focus of traditional query optimizers has been on the choice of join methods and orders; selections have been handled by \pushdown &#034; rules. These rules apply selections in an arbitrary order before as many joins as possible, using the assumption that selection takes no time. However, users of Object-Relational systems can embed complex methods in selections. Thus selections may take signi cant amounts of time, and the query optimization model must be enhanced. In this paper, we carefully de ne a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial Object-Relational database management system Illustra, and discuss practical issues that a ect our earlier assumptions. We compare Predicate Migration to a variety of simpler optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we presentmaybe useful for constrained workloads.
332|Cost-based Query Scrambling for Initial Delays|Remote data access from disparate sources across a wide-area network such as the Internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites. Traditional, static, query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays. Query scrambling has been proposed to address this problem. Scrambling modifies query execution plans on-the-fly when delays are encountered during runtime. In its original formulation, scrambling was based on simple heuristics, which although providing good performance in many cases, were also shown to be susceptible to problems resulting from bad scrambling decisions. In this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices. We propose three different approaches to using query optimization for scramblin...
333|Adaptive Parallel Aggregation Algorithms|Aggregation and duplicate removal are common in SQL queries. However, in the parallel query processing literature, aggregate processing has received surprisingly little attention; furthermore, for each of the traditional parallel aggregation algorithms, there is a range of grouping selectivities where the algorithm performs poorly. In this work, we propose new algorithms that dynamically adapt, at query evaluation time, in response to observed grouping selectivities. Performance analysis via analytical modeling and an implementation on a workstation-cluster shows that the proposed algorithms are able to perform well for all grouping selectivities. Finally, we study the effect of data skew and show that for certain data sets the proposed algorithms can even outperform the best of traditional approaches. 1 Introduction  SQL queries are replete with aggregate and duplicate elimination operations. One measure of the perceived importance of aggregation is that in the proposed TPCD benchmark...
335|Bluetooth and Sensor Networks: A Reality Check|The current generation of sensor nodes rely on commodity components. The choice of the radio is particularly important as it impacts not only energy consumption but also software design (e.g., network self-assembly, multihop routing and in-network processing). Bluetooth is one of the most popular commodity radios for wireless devices. As a representative of the frequency hopping spread spectrum radios, it is a natural alternative to broadcast radios in the context of sensor networks. The question is whether Bluetooth can be a viable alternative in practice. In this paper, we report our experience using Bluetooth for the sensor network regime. We describe our tiny Bluetooth stack that allows TinyOS applications to run on Bluetooth-based sensor nodes, we present a multihop network assembly procedure that leverages Bluetooth&#039;s device discovery protocol, and we discuss how Bluetooth favorably impacts in-network query processing. Our results show that despite obvious limitations the Bluetooth sensor nodes we studied exhibit interesting properties, such as a good energy per bit sent ratio. This reality check underlies the limitations and some promises of Bluetooth for the sensor network regime.
336|Aggregation and Relevance in Deductive Databases|In this paper we present a technique to optimize queries on deductive databases that use aggregate operations such as min, max, and “largest Ic values. ” Our approach is based on an extended notion of relevance of facts to queries that takes aggregate operations into account. The approach has two parts: a rewriting part that labels predica.tes with “ag-gregate selections, ” and an evaluat,ion part. t.hat, makes use of “aggregate selections ” to detect that facts are irrelevant and discards them. The rewriting complements standard rewrit-ing algorithms like Magic sets, and the evaluation essentially refines Semi-Naive evaluation. 1
337|Query Optimization In Compressed Database Systems|Over the lastd ecad es, improvements in CPU speed have outpaced improvements in main memory and d isk access rates by ord ers of magnitud , enabling the use ofd ata compression techniques to improve the performance ofd atabase systems. Previous work d scribes the benefits of compression for numerical attributes, whered8 a is stored in compressed  format ond isk. Despite the abund3&amp; e of stringvalued  attributes in relational schemas there is little work on compression for string attributes in ad atabase context. Moreover, none of the previous work suitablyad2 esses the role of the query optimizer: During query execution, dD a is either eagerly d compressed when it is read into main memory, or dD a lazily stays compressed in main memory and is d compressed ond emand only. In this paper, we present an e#ective approach for dD abase compression based on lightweight, attribute-level compression techniques. We propose a Hierarchical ictionary Encod  ing strategy that intelligently selects the most e#ective compression method for string-valued attributes. We show that eager and lazy d compression strategies prod1 e suboptimal plans for queries involving compressed string attributes. We then formalize the problem of compressionaware query optimizationand propose one provably optimal  and two fast heuristic algorithms for selecting a query plan for relational schemas with compressed attributes; our algorithms can easily be integrated into existing cost-based query optimizers. Experiments using TPC-Hd atad emonstrate the impact of our string compression method s and show the importance of compression-aware query optimization. Our approach results in up to an or d r speed up over existing approaches.  1. 
338|The Design and Implementation of the Ariel Active Database Rule System|This paper describes the design and implementation of the Ariel DBMS and it&#039;s tightlycoupled forward-chaining rule system. The query language of Ariel is a subset of POSTQUEL, extended with a new production-rule sublanguage. Ariel supports traditional relational database query and update operations efficiently, using a System Rlike query processing strategy. In addition, the Ariel rule system is tightly coupled with query and update processing. Ariel rules can have conditions based on a mix of patterns, events, and transitions. For testing rule conditions, Ariel makes use of a discrimination network composed of a special data structure for testing single-relation selection conditions efficiently, and a modified version of the TREAT algorithm, called A-TREAT, for testing join conditions. The key modification to TREAT (which could also be used in the Rete algorithm) is the use of virtual ff-memory nodes which save storage since they contain only the predicate associated with the memory n...
339|DOMINO: Databases fOr MovINg Objects tracking|Consider a database that represents information about moving objects and their location. For example, for a database representing the location of taxi-cabs a typical query may be: retrieve the free cabs that are currently within 1 mile of 33 N. Michigan Ave., Chicago (to pick-up a customer); or for a trucking company database a typical query may be: retrieve the trucks that are currently within 1 mile of truck ABT312 (which needs assistance); or for a database representing the current location of objects in a battlefield a typical query may be: retrieve the friendly helicopters that are in a given region, or, retrieve the friendly helicopters that are expected to enter the region within the next 10 minutes. The queries may originate from the moving objects, or from stationary users. We will refer to applications with the above characteristics as moving-objects-database (MOD) applications, and to queries as the ones mentioned above as MOD queries.
340|From ethnography to design in a vineyard, in|This paper summarizes the process from ethnographic study of a vineyard to concept development and interaction design for a ubiquitous computing solution. It provides examples of vineyard interfaces and the lessons learned that could be generally applied to the interaction design of ubiquitous systems. These are: design for multiple perspectives on data, design for multiple access points, and design for varying levels of attention.
341|Query Optimization in Mobile Environments|We consider the issue of optimizing queries for a distributed processing in mobile environment. An interesting characteristic of mobile machines is that they depend on battery as a source of energy which may not be substantial enough. Hence, the appropriate optimization criterion in a mobile environment considers both resource utilization and energy consumption at the mobile client. In this scenario, the optimal plan for a query depends on the residual battery level of the mobile client and the load at the server. We approach this problem by compiling a query into a sequence of candidate plans, such that for any state of the client-server system, the optimal plan is one of the candidate plans. A general solution is proposed by adapting the partial order dynamic programming search algorithm [17, 18] (p.o dp) such that the coverset of the query is the set of candidate plans. We propose two novel algorithms, namely, the linear combinations algorithm and the linearset algorithm (referred t...
342|Online Dynamic Reordering|We present a mechanism for providing dynamic user control during long running, data-intensive operations. Users  can see partial results and dynamically direct the processing to suit their interests, thereby enhancing the interactivity  in several contexts such as online aggregation and large-scale spreadsheets.  In this paper, we develop a pipelining, dynamically tunable reorder operator for providing user control. Users  specify preferences for different data items based on prior results, so that data of interest is prioritized for early  processing. The reordering mechanism is efficient and non-blocking, and can be used over arbitrary data streams  from files and indexes, as well as continuous data feeds. We also investigate several policies for the reordering  based on the performance goals of various typical applications. We present performance results for reordering in the  context of an Online Aggregation implementation in Informix, and in the context of sorting and scrolling in...
343|Kerberos: An Authentication Service for Open Network Systems|In an open network computing environment, a workstation cannot be trusted to identify its users correctly to network services.  Kerberos provides an alternative approach whereby a trusted third-party authentication service is used to verify users’ identities. This paper gives an overview of the Kerberos authentication model as implemented for MIT’s Project Athena. It describes the protocols used by clients, servers, and Kerberos to achieve authentication. It also describes the management and replication of the database required.   The views of Kerberos as seen by the user, programmer, and administrator are described.  Finally, the role of Kerberos in the larger Athena picture is given, along with a list of applications that presently use Kerberos for user authentication.  We describe the addition of Kerberos authentication to the Sun Network File System as a case study for integrating Kerberos with an existing application.
344|Berkeley UNIX+ on 1000 Workstations: Athena Changes to 4.3BSD |4.3BSD UNIX as shipped is designed for use on individually-managed, networked  timesharing systems. A large network of individual workstations and server machines,  all managed centrally, has many important differences from such a model. This paper  discusses some of the changes necessary for 4.3 in this new world, including the file system  layout, configuration files, and software. The integration with Athena&#039;s authentication  system, name service, and service management system are also discussed.  1. Overview  &#034;By 1988, create a new educational computing environment environment at MIT built around high-performance graphics workstations, high-speed networking, and servers of various types.&#034; This one-sentence statement is a highlevel description of the technical goals of Project Athena. While the primary goals are to enhance education, attaining them has required a significant effort to engineer a software environment for use in a large network of workstations and servers.  The Athena...
345|Reality Mining: Sensing Complex Social Systems|We introduce a system for sensing complex social systems with data collected from one hundred mobile phones over the course of six months. We demonstrate the ability to use standard Bluetooth-enabled mobile telephones to measure information access and use in different contexts, recognize social patterns in daily user activity, infer relationships, identify socially significant locations, and model organizational rhythms. 
346|Learning and inferring transportation routines| This paper introduces a hierarchical Markov model that can learn and infer a user’s daily movements through the community. The model uses multiple levels of abstraction in order to bridge the gap between raw GPS sensor measurements and high level information such as a user’s mode of transportation or her goal. We apply Rao-Blackwellised particle filters for efficient inference both at the low level and at the higher levels of the hierarchy. Significant locations such as goals or locations where the user frequently changes mode of transportation are learned from GPS data logs without requiring any manual labeling. We show how to detect abnormal behaviors (e.g. taking a wrong bus) by concurrently tracking his activities with a trained and a prior model. Experiments show that our model is able to accurately predict the goals of a person and to recognize situations in which the user performs unknown activities.
347|Using GPS to Learn Significant Locations and Predict Movement across Multiple Users|Wearable computers have the potential to act as intelligent agents in everyday life and assist the user in a variety of tasks, using context to determine how to act. Location is the most common form of context used by these agents to determine the user&#039;s task. However, another potential use of location context is the creation of a predictive model of the user&#039;s future movements. We present a system that automatically clusters GPS data taken over an extended period of time into meaningful locations at multiple scales. These locations are then incorporated into a Markov model that can be consulted for use with a variety of applications in both single--user and collaborative scenarios. 1 
348|Place lab: Device positioning using radio beacons in the wild|Abstract. Location awareness is an important capability for mobile computing. Yet inexpensive, pervasive positioning—a requirement for wide-scale adoption of location-aware computing—has been elusive. We demonstrate a radio beacon-based approach to location, called Place Lab, that can overcome the lack of ubiquity and high-cost found in existing location sensing approaches. Using Place Lab, commodity laptops, PDAs and cell phones estimate their position by listening for the cell IDs of fixed radio beacons, such as wireless access points, and referencing the beacons ’ positions in a cached database. We present experimental results showing that 802.11 and GSM beacons are sufficiently pervasive in the greater Seattle area to achieve 20-40 meter median
349|The familiar stranger: anxiety, comfort, and play in public places|As humans we live and interact across a wildly diverse set of physical spaces. We each formulate our own personal meaning of place using a myriad of observable cues such as public-private, large-small, daytime-nighttime, loud-quiet, and crowded-empty. Unsurprisingly, it is the people with which we share such spaces that dominate our perception of place. Sometimes these people are friends, family and colleagues. More often, and particularly in public urban spaces we inhabit, the individuals who affect us are ones that we repeatedly observe and yet do not directly interact with – our Familiar Strangers. This paper explores our often ignored yet real relationships with Familiar Strangers. We describe several experiments and studies that lead to a design for a personal, body-worn, wireless device that extends the Familiar Stranger relationship while respecting the delicate, yet important, constraints of our feelings and relationships with strangers in pubic places.
350|Rhythm Modeling, Visualizations, and Applications|People use their awareness of others &#039; temporal patterns to plan work activities and communication. This paper presents algorithms for programatically detecting and modeling temporal patterns from a record of online presence data. We describe analytic and end-user visualizations of rhythmic patterns and the tradeoffs between them. We conducted a design study that explored the accuracy of the derived rhythm models compared to user perceptions, user preference among the visualization alternatives, and users&#039; privacy preferences. We also present a prototype application based on the rhythm model that detects when a person is “away ” for an extended period and predicts their return. We discuss the implications of this technology on the design of computer-mediated communication.
351|Tracking Mobile Users in Wireless Communications Networks|Tracking strategies for mobile wireless networks are studied. We assume a cellular architecture where base stations that are interconnected by a wired network communicate with mobile units via wireless links. Previous works focused on the cost of utilizing the  wired links for management of directories. In this paper, the issue considered is the cost of utilizing the wireless links for the actual tracking of mobile users. We propose a novel tracking strategy in which a subset of all base stations is selected and designated as reporting centers. Mobile users transmit update messages only upon entering cells of reporting centers, while every search for a mobile user is restricted to the vicinity of the reporting center to which the user lastly reported. We first show that for an arbitrary topology of the cellular network (represented by the  mobility graph), finding an optimal set of reporting centers is an NP-complete problem. We then present optimal and near optimal solutions for impor...
352|Using Handheld Devices in Synchronous Collaborative Scenarios|. In this paper we present a platform specially designed for groupware  applications running on handheld devices. Common groupware platforms  request desktop computers as underlying hardware platforms. The fundamental  different nature of handheld devices has a great impact on the platform, e.g.  resource limitations have to be considered, the network is slow and unstable.  Often, personal data are stored on handheld devices, thus mechanisms have to  ensure privacy. These considerations lead to the QuickStep platform. Sample  applications developed with QuickStep demonstrate the strengths of the  QuickStep environment.  1 Introduction  Collaborative applications help a group to, e.g., collaboratively create documents, write agendas or schedule appointments. A common taxonomy [3] classifies collaborative applications by time and space, with &#039;same place&#039; and &#039;different places&#039; attributes on the space axis and &#039;same time&#039; (synchronous) and &#039;different time&#039; (asynchronous) ones on the time ...
353|Consensus and cooperation in networked multi-agent systems | This paper provides a theoretical framework for analysis of consensus algorithms for multi-agent networked systems with an emphasis on the role of directed information flow, robustness to changes in network topology due to link/node failures, time-delays, and performance guarantees. An overview of basic concepts of information consensus in networks and methods of convergence and performance analysis for the algorithms are provided. Our analysis framework is based on tools from matrix theory, algebraic graph theory, and control theory. We discuss the connections between consensus problems in networked dynamic systems and diverse applications including synchronization of coupled oscillators, flocking, formation control, fast consensus in small-world networks, Markov processes and gossip-based algorithms, load balancing in networks, rendezvous in space, distributed sensor fusion in sensor networks, and belief propagation. We establish direct connections between spectral and structural properties of complex networks and the speed of information diffusion of consensus algorithms. A brief introduction is provided on networked systems with nonlocal information flow that are considerably faster than distributed systems with latticetype nearest neighbor interactions. Simulation results are presented that demonstrate the role of small-world effects on the speed of consensus algorithms and cooperative control of multi-vehicle formations.
356|The structure and function of complex networks|Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.
357|Coordination of Groups of Mobile Autonomous Agents Using Nearest Neighbor Rules|In a recent Physical Review Letters paper, Vicsek et. al. propose a simple but compelling  discrete-time model of n autonomous agents fi.e., points or particlesg all moving in the plane  with the same speed but with dierent headings. Each agent&#039;s heading is updated using a local  rule based on the average of its own heading plus the headings of its \neighbors.&#034; In their paper,  Vicsek et. al. provide simulation results which demonstrate that the nearest neighbor rule they  are studying can cause all agents to eventually move in the same direction despite the absence of  centralized coordination and despite the fact that each agent&#039;s set of nearest neighbors change  with time as the system evolves. This paper provides a theoretical explanation for this observed  behavior. In addition, convergence results are derived for several other similarly inspired models.
358|Consensus Problems in Networks of Agents with Switching Topology and Time-Delays|In this paper, we discuss consensus problems for a network of dynamic agents with fixed and switching topologies. We analyze three cases: i) networks with switching topology and no time-delays, ii) networks with fixed topology and communication time-delays, and iii) max-consensus problems (or leader determination) for groups of discrete-time agents. In each case, we introduce a linear/nonlinear consensus protocol and provide convergence analysis for the proposed distributed algorithm. Moreover, we establish a connection between the Fiedler eigenvalue of the information flow in a network (i.e. algebraic connectivity of the network) and the negotiation speed (or performance) of the corresponding agreement protocol. It turns out that balanced digraphs play an important role in addressing average-consensus problems. We introduce disagreement functions that play the role of Lyapunov functions in convergence analysis of consensus protocols. A distinctive feature of this work is to address consensus problems for networks with directed information flow. We provide analytical tools that rely on algebraic graph theory, matrix theory, and control theory. Simulations are provided that demonstrate the effectiveness of our theoretical results.
359|Coverage Control for Mobile Sensing Networks|This paper presents control and coordination algorithms for groups of vehicles. The focus is on autonomous vehicle networks performing distributed sensing tasks where each vehicle plays the role of a mobile tunable sensor. The paper proposes gradient descent algorithms for a class of utility functions which encode optimal coverage and sensing policies. The resulting closed-loop behavior is adaptive, distributed, asynchronous, and verifiably correct.
360|Gossip-Based Computation of Aggregate Information|between computers, and a resulting paradigm shift from centralized to highly distributed systems. With massive scale also comes massive instability, as node and link failures become the norm rather than the exception. For such highly volatile systems, decentralized gossip-based protocols are emerging as an approach to maintaining simplicity and scalability while achieving fault-tolerant information dissemination.
361|Fast Linear Iterations for Distributed Averaging|We consider the problem of finding a linear iteration that yields distributed averaging  consensus over a network, i.e., that asymptotically computes the average of  some initial values given at the nodes. When the iteration is assumed symmetric, the  problem of finding the fastest converging linear iteration can be cast as a semidefinite  program, and therefore efficiently and globally solved. These optimal linear iterations  are often substantially faster than several common heuristics that are based on the  Laplacian of the associated graph.
362|Flocking for Multi-Agent Dynamic Systems: Algorithms and Theory| In this paper, we present a theoretical framework for design and analysis of distributed flocking algorithms. Two cases of flocking in free-space and presence of multiple obstacles are considered. We present three flocking algorithms: two for free-flocking and one for constrained flocking. A comprehensive analysis of the first two algorithms is provided. We demonstrate the first algorithm embodies all three rules of Reynolds. This is a formal approach to extraction of interaction rules that lead to the emergence of collective behavior. We show that the first algorithm generically leads to regular fragmentation, whereas the second and third algorithms both lead to flocking. A systematic method is provided for construction of cost functions (or collective potentials) for flocking. These collective potentials penalize deviation from a class of lattice-shape objects called alattices. We use a multi-species framework for construction of collective potentials that consist of flock-members, or a-agents, and virtual agents associated with a-agents called ß- and ?-agents. We show that migration of flocks can be performed using a peer-to-peer network of agents, i.e. “flocks need no leaders.” A “universal” definition of flocking for particle systems with similarities to Lyapunov stability is given. Several simulation results are provided that demonstrate performing 2-D and 3-D flocking, split/rejoin maneuver, and squeezing maneuver for hundreds of agents using the proposed algorithms.
363|Consensus Seeking in Multi-agent Systems under Dynamically Changing Interaction Topologies|This note considers the problem of information consensus among multiple agents in the presence  of limited and unreliable information exchange with dynamically changing interaction topologies. Both  discrete and continuous update schemes are proposed for information consensus. The note shows that  information consensus under dynamically changing interaction topologies can be achieved asymptotically  if the union of the directed interaction graphs across some time intervals has a spanning tree frequently  enough as the system evolves. Simulation results show the effectiveness of our update schemes.
366|Expander Codes|We present a new class of asymptotically good, linear error-correcting codes based upon expander graphs. These codes have linear time sequential decoding algorithms, logarithmic time parallel decoding algorithms with a linear number of processors, and are simple to understand. We present both randomized and explicit constructions for some of these codes. Experimental results demonstrate the extremely good performance of the randomly chosen codes.  
367|From kuramoto to crawford: exploring the onset of synchronization in populations of coupled oscillators|The Kuramoto model describes a large population of coupled limit-cycle oscillators whose natural frequencies are drawn from some prescribed distribution. If the coupling strength exceeds a certain threshold, the system exhibits a phase transition: some of the oscillators spontaneously synchronize, while others remain incoherent. The mathematical analysis of this bifurcation has proved both problematic and fascinating. We review 25 years of research on the Kuramoto model, highlighting the false turns as well as the successes, but mainly following the trail leading from Kuramoto’s work to Crawford’s recent contributions. It is a lovely winding road, with excursions through mathematical biology, statistical physics, kinetic theory, bifurcation theory, and plasma physics. © 2000 Elsevier Science B.V. All rights reserved.
368|Kalman filtering with intermittent observations|Abstract—Motivated by navigation and tracking applications within sensor networks, we consider the problem of performing Kalman filtering with intermittent observations. When data travel along unreliable communication channels in a large, wireless, multihop sensor network, the effect of communication delays and loss of information in the control loop cannot be neglected. We address this problem starting from the discrete Kalman filtering formulation, and modeling the arrival of the observation as a random process. We study the statistical convergence properties of the estimation error covariance, showing the existence of a critical value for the arrival rate of the observations, beyond which a transition to an unbounded state error covariance occurs. We also give upper and lower bounds on this expected state error covariance. Index Terms—Kalman estimation, missing observation, online adaptive filtering, sensor networks, stability. I.
369|Convergence in multiagent coordination, consensus, and flocking|We discuss an old distributed algorithm for reaching consensus that has received a fair amount of recent attention. In this algorithm, a number of agents exchange their values asynchronously and form weighted averages with (possibly outdated) values possessed by their neighbors. We overview existing convergence results, and establish some new ones, for the case of unbounded intercommunication intervals.
370|The Laplacian spectrum of graphs|Abstract. The paper is essentially a survey of known results about the spectrum of the Laplacian matrix of graphs with special emphasis on the second smallest Lapla-cian eigenvalue ?2 and its relation to numerous graph invariants, including connectivity, expanding properties, isoperimetric number, maximum cut, independence number, genus, diameter, mean distance, and bandwidth-type parameters of a graph. Some new results and generalizations are added. † This article appeared in “Graph Theory, Combinatorics, and Applications”, Vol. 2,
371|Gossip algorithms: Design, analysis and applications| Motivated by applications to sensor, peer-to-peer and ad hoc networks, we study distributed asynchronous algorithms, also known as gossip algorithms, for computation and information exchange in an arbitrarily connected network of nodes. Nodes in such networks operate under limited computational, communication and energy resources. These constraints naturally give rise to &#034;gossip&#034; algorithms: schemes which distribute the computational burden and in which a node communicates with a randomly chosen neighbor. We analyze the averaging problem under the gossip constraint for arbitrary network, and find that the averaging time of a gossip algorithm depends on the second largest eigenvalue of a doubly stochastic matrix characterizing the algorithm. Using recent results of Boyd, Diaconis and Xiao
372|On robust rendezvous for mobile autonomous agents| This paper presents coordination algorithms for networks of mobile autonomous agents. The objective of the proposed algorithms is to achieve rendezvous, that is, agreement over the location of the agents in the network. We provide analysis and design results for multi-agent networks in arbitrary dimensions under weak requirements on the switching and failing communication topology. The correctness proof relies on proximity graphs and their properties and on a LaSalle Invariance Principle for nondeterministic discrete-time systems. 
373|Local control strategies for groups of mobile autonomous agents|Abstract — The problem is studied of achieving a specified formation among a group of mobile autonomous agents by distributed control. If convergence to a point is feasible, then more general formations are achievable too, so the focus is on convergence to a point (the agreement problem). Three formation strategies are studied and convergence is proved under certain conditions. Also, motivated by the question of whether collisions occur, formation evolution is studied. I.
374|Distributed Kalman filtering in sensor networks with quantifiable performance|We analyze the performance of a distributed Kalman filter proposed in recent work on distributed dynamical systems. This approach to distributed estimation is novel in that it admits a systematic analysis of its performance as various network quantities such as connection density, topology, and bandwidth are varied. Our main contribution is a frequency-domain characterization of the distributed estimator’s performance; this is quantified in terms of a special matrix associated with the connection topology called the graph Laplacian, and also the rate of message exchange between immediate neighbors in the communication network. We present simulations for an array of sonar-like sensors to verify our analysis results. 1.
375|Agreement over random networks|Abstract—We consider the agreement problem over random information networks. In a randomnetwork, the existence of an information channel be-tween a pair of units at each time instance is probabilistic and independent of other channels; hence, the topology of the network varies over time. In such a framework, we address the asymptotic agreement for the networked units via notions from stochastic stability. Furthermore, we delineate on the rate of convergence as it relates to the algebraic connectivity of random graphs. Index Terms—Agreement problem, networked systems, random graphs, stochastic stability, supermartingales. I.
376|Distibuted cooperative control of multiple vehicle formations using structural potential functions| In this paper, we propose a framework for formation stabilization of multiple autonomous vehicles in a distributed fashion. Each vehicle is assumed to have simple dynamics, i.e. a double-integrator, with a directed (or an undirected) information flow over the formation graph of the vehicles. Our goal is to find a distributed control law (with an efficient computational cost) for each vehicle that makes use of limited information regarding the state of other vehicles. Here, the key idea in formation stabilization is the use of natural potential functions obtained from structural constraints of a desired formation in a way that leads to a collision-free, distributed, and bounded state feedback law for each vehicle.
377|Necessary and sufficient graphical conditions for formation control of unicycles| The feasibility problem is studied of achieving a specified formation among a group of autonomous unicycles by local distributed control. The directed graph defined by the information flow plays a key role. It is proved that formation stabilization to a point is feasible if and only if the sensor digraph has a globally reachable node. A similar result is given for formation stabilization to a line and to more general geometric arrangements. 
378|Consensus filters for sensor networks and distributed sensor fusion|Abstract — Consensus algorithms for networked dynamic systems provide scalable algorithms for sensor fusion in sensor networks. This paper introduces a distributed filter that allows the nodes of a sensor network to track the average of n sensor measurements using an average consensus based distributed filter called consensus filter. This consensus filter plays a crucial role in solving a data fusion problem that allows implementation of a scheme for distributed Kalman filtering in sensor networks. The analysis of the convergence, noise propagation reduction, and ability to track fast signals are provided for consensus filters. As a byproduct, a novel critical phenomenon is found that relates the size of a sensor network to its tracking and sensor fusion capabilities. We characterize this performance limitation as a tracking uncertainty principle. This answers a fundamental question regarding how large a sensor network must be for effective sensor fusion. Moreover, regular networks emerge as efficient topologies for distributed fusion of noisy information. Though, arbitrary overlay networks can be used. Simulation results are provided that demonstrate the effectiveness of consensus filters for distributed sensor fusion. Index Terms — sensor networks, sensor fusion, consensus algorithms, distributed Kalman filters, networked embedded systems, graph Laplacians, complex networks I.
379|Consensus Protocols for Networks of Dynamic Agents |In this paper, we introduce linear and nonlinear consensus protocols for networks of dynamic agents that allow the agents to agree in a distributed and cooperative fashion. We consider the cases of networks with communication time-delays and channels that have  ltering eects. We  nd a tight upper bound on the maximum  xed time-delay that can be tolerated in the network. It turns out that the connectivity of the network is the key in reaching a consensus. The case of agreement with bounded inputs is considered by analyzing the convergence of a class of nonlinear protocols. A Lyapunov function is introduced that quanti  es the total disagreement among the nodes of a network. Simulation results are provided for agreement in networks with communication time-delays and constrained inputs.
380|On the Stability of the Kuramoto Model of Coupled Nonlinear Oscillators|We provide an analysis of the classic Kuramoto model of coupled nonlinear oscillators that goes beyond the existing results for all-to-all networks of identical oscillators. Our work is applicable to oscillator networks of arbitrary interconnection topology with uncertain natural frequencies. Using tools from spectral graph theory and control theory, we prove that for couplings above a critical value, the synchronized state is locally asymptotically stable, resulting in convergence of all phase differences to a constant value, both in the case of identical natural frequencies as well as uncertain ones. We further explain the behavior of the system as the number of oscillators grows to infinity. 
381|Graph Rigidity and Distributed Formation Stabilization of Multi-Vehicle Systems|In this paper, we provide a graph theoretical framework that allows us to formally de  ne formations of multiple vehicles and the issues arising in uniqueness of graph realizations and its connection to stability of formations. The notion of graph rigidity is crucial in identifying the shape variables of a formation and an appropriate potential function associated with the formation. This allows formulation of meaningful optimization or nonlinear control problems for formation stabilization/tacking, in addition to formal representation of split, rejoin, and recon  guration maneuvers for multi-vehicle formations. We introduce an algebra that consists of performing some basic operations on graphs which allow creation of larger rigidby -construction graphs by combining smaller rigid subgraphs. This is particularly useful in performing and representing rejoin/split maneuvers of multiple formations in a distributed fashion.
382|Programmable self-assembly using biologically-inspired multiagent control|This paper presents a programming language that specifies a robust process for shape formation on a sheet of identicallyprogrammed agents, by combining local organization primitives from epithelial cell morphogenesis and Drosophila cell differentiation with combination rules from geometry. This work represents a significantly different approach to the design of self-organizing systems: the desired global shape is specified using an abstract geometry-based language, and the agent program is directly compiled from the global specification. The resulting self-assembly process is extremely reliable in the face of random agent distributions, random agent death and varying agent numbers, without relying on global coordinates or centralized control.
383|Controlling Connectivity of Dynamic Graphs|The control of mobile networks of multiple agents raises fundamental and novel problems in controlling the structure of the resulting dynamic graphs. In this paper, we consider the problem of controlling a network of agents so that the resulting motion always preserves various connectivity properties. In particular, we consider preserving k-hop connectivity, where agents are allowed to move while maintaining connections to agents that are no more than k-hops away. The connectivity constraint is translated to constrains on individual agent motion by considering the dynamics of the adjacency matrix and related constructs from algebraic graph theory. As special cases, we obtain motion constraints that can preserve the exact structure of the initial dynamic graph, or may simply preserve the usual notion connectivity while the structure of the graph changes over time. We conclude by illustrating various interesting problems that can be achieved while preserving connectivity constraints.
384|Robust computation of aggregates in wireless sensor networks: distributed randomized algorithms and analysis|A wireless sensor network consists of a large number of small, resource-constrained devices and usually operates in hostile environments that are prone to link and node failures. Computing aggregates such as average, minimum, maximum and sum is fundamental to various primitive functions of a sensor network like system monitoring, data querying, and collaborative information processing. In this paper we present and analyze a suite of randomized distributed algorithms to efficiently and robustly compute aggregates. Our Distributed Random Grouping (DRG) algorithm is simple and natural and uses probabilistic grouping to progressively converge to the aggregate value. DRG is local and randomized and is naturally robust against dynamic topology changes from link/node failures. Although our algorithm is natural and simple, it is nontrivial to show that it converges to the correct aggregate value and to bound the time needed for convergence. Our analysis uses the eigen-structure of the underlying graph in a novel way to show convergence and to bound the running time of our algorithms. We also present simulation results of our algorithm and compare its performance to various other known distributed algorithms. Simulations show that DRG needs much less transmissions than other distributed localized schemes. Author names appear in alphabetical order.
385|Belief consensus and distributed hypothesis testing in sensor networks|Summary. In this paper, we address distributed hypothesis testing (DHT) in sensor networks and Bayesian networks using the average-consensus algorithm of Olfati-Saber &amp; Murray. As a byproduct, we obtain a novel belief propagation algorithm called Belief Consensus. This algorithm works for connected networks with loops and arbitrary degree sequence. Belief consensus allows distributed computation of products of n beliefs (or conditional probabilities) that belong to n different nodes of a network. This capability enables distributed hypothesis testing for a broad variety of applications. We show that this belief propagation admits a Lyapunov function that quantifies the collective disbelief in the network. Belief consensus benefits from scalability, robustness to link failures, convergence under variable topology, asynchronous features of average-consensus algorithm. Some connections between small-word networks and speed of convergence of belief consensus are discussed. A detailed example is provided for distributed detection of multi-target formations in a sensor network. The entire network is capable of reaching a common set of beliefs associated with correctness of different hypotheses. We demonstrate that our DHT algorithm successfully identifies a test formation in a network of sensors with self-constructed statistical models. Key words: distributed hypothesis testing, multi-target tracking, Bayesian networks, average consensus, belief propagation, sensor networks, small-world networks 1
386|Convergence and asymptotic agreement in distributed decision problems|Abstract-We consider a distributed team decision problem in nrhich A related-and much more general situation-is the subject of different agents obtain from the environment different stochastic measure- this paper; we assume that the agents are not just interested in ments, possibly at different random times, related to the same uncertain obtaining an optimal estimate or a likelihood ratio, but their random vector. Each agent has the same objective function and prior objective is to try to minimize some common cost function, given probability distribution. We assume that each agent can compute an the available information. (Clearly, if each agent has a different optimal tentative decision based upon his om observation and that these cost function, no agreement is possible even if each agent had tentative decisions are communicated and received, possibly at random identical information.) In th~s setting. we assume that agents times, by- a subset of other agents. Conditions for asymptotic convergence communicate to each other tentative decisions (which initially of each agent’s decision sequence and asymptotic agreement of all agents ’ will be different). That is, at any time, an agent computes an decisions are derived. optimal decision given the information he possesses and communicates it to other agents. Whenever an agent receives such a I.
387|Closing Ranks in Vehicle Formations Based on Rigidity|In this paper, a systematic way of maintaining rigidity in case of vehicle removals in formations for coordinating mobile autonomous vehicles with limited communication /sensing links is presented. The main concern is the minimal rearrangement of links in such a way that the links that have not been removed are preserved as they are, and the minimum number of new links are created in the formation to maintain rigidity. The problem is solved by the tools used in the Henneberg method. This method gives us provably rigid formations with the minimum number of links. The proposed framework meets this challenge for removed vehicles with any number of links in both 2 and 3-space. The same problem is also considered for the minimal Delaunay edge formations, which is a special class of the formations created by the Henneberg method with some geometrical properties.
388|A lower bound on convergence of a distributed network consensus algorithm|Abstract — This paper gives a lower bound on the convergence rate of a class of network consensus algorithms. Two different approaches using directed graphs as a main tool are introduced: one is to compute the “scrambling constants ” of stochastic matrices associated with “neighbor shared graphs ” and the other is to analyze random walks on a sequence of graphs. Both approaches prove that the time to reach consensus within a dynamic network is logarithmic in the relative error and is in worst case exponential in the size of the network. I.
389|On synchronous robotic networks Part I: models, tasks, and complexity notions|This paper proposes a formal model for a network of robotic agents that move and communicate. Building on concepts from distributed computation, robotics and control theory, we define notions of robotic network, control and communication law, coordination task, and time and communication complexity. We illustrate our model and compute the proposed complexity measures in the example of a network of locally connected agents on a circle that agree upon a direction of motion and pursue their immediate neighbors. I.
391|Limited communication control|SystemsandControlLetters,July1999 This work in limited communication control is directed towards bringing together classical control theory and communication theoretical issues that are of practical importance in the design of control systems. It is common to “decouple ” the communication aspects from the underlying dynamics of a system, as this simplifies the analysis and generally works well for classical models. However, in situations where a single decision maker controls many subsystems over a communication channel of finite capacity, the computation of control signals and their transmission to each system may take significant amounts of time. To address such cases, we consider a class of discrete-time models that jointly optimize over control and communication goals. Real-world examples where these models play a role include remotely controlled unmanned autonomous vehicles (UAVs), planetary rovers, arrays of microactuators and power control in mobile communication.
392|Collective motion and oscillator synchronization|Summary. This paper studies connections between phase models of coupled oscillators and kinematic models of groups of self-propelled particles. These connections are exploited in the analysis and design of feedback control laws for the individuals that stabilize collective motions for the group. 1
393|Distributed geodesic control laws for flocking of nonholonomic agents|Abstract—We study the problem of flocking and velocity alignment in a group of kinematic nonholonomic agents in 2 and 3 dimensions. By analyzing the velocity vectors of agents on a circle (for planar motion) or sphere (for 3-D motion), we develop a geodesic control law that minimizes a misalignment potential and results in velocity alignment and flocking. The proposed control laws are distributed and will provably result in flocking when the underlying proximity graph which represents the neighborhood relation among agents is connected. We further show that flocking is possible even when the topology of the proximity graph changes over time, so long as a weaker notion of joint connectivity is preserved. Index Terms—Cooperative control, distributed coordination, flocking, multiagent systems. I.
394|Connectivity graphs as models of local interactions|In this paper, we study graphs that arise from certain sensory and com-munication limitations on the local interactions in multi-agent systems. In particular, we show that the set of graphs that can represent formations corresponds to a proper subset of all graphs and we denote such graphs as connectivity graphs. These graphs have a special structure that allows them to be composed from a small number of atomic generators using a certain kind of graph amalgamation. This structure moreover allows us to give connectivity graphs a topological characterization in terms of their simplicial complexes. Finally, we outline some applications of this topo-logical characterization to the construction of decentralized algorithms. 1
395|Distributed estimation and control of swarm formation statistics|Abstract — We describe distributed estimation algorithms that allow robots in a communication network to maintain estimates of summary statistics describing the shape of the swarm. We show that these estimators, combined with motion controllers implemented on each robot, result in the swarm formation statistics being driven to desired values in the presence of a changing network topology and the addition and deletion of robots. I.
396|Sensor and Network Topologies of Formations with  Direction, Bearing and  Angle . . .| Sensor and network topologies of formations of autonomous agents are considered. The aim of the paper is to suggst an approach for such topologies for formations with direction, bearing and angle information between agents in the plane and in 3-space. A number of results are translated fmm prior work in this field and in the study of constraints in CAD programming, in rigidity theory, in structural engineering and in discrete mathematics. Some new results are presented both for the plane and for 3-space. A number of unsolved problem are also mentioned.  
397|On synchronous robotic networks – Part II: Time complexity of rendezvous and deployment algorithms |This paper analyzes a number of basic coordination algorithms running on synchronous robotic networks. We provide upper and lower bounds on the time complexity of the move-toward average and circumcenter laws, both achieving rendezvous, and of the centroid law, achieving deployment over a region of interest. The results are derived via novel analysis methods, including a set of results on the convergence rates of linear dynamical systems defined by tridiagonal Toeplitz and circulant matrices. I.
398|Approximate Distributed Kalman Filtering in Sensor Networks with Quantifiable Performance |Abstract — We analyze the performance of an approximate distributed Kalman filter proposed in recent work on distributed coordination. This approach to distributed estimation is novel in that it admits a systematic analysis of its performance as various network quantities such as connection density, topology, and bandwidth are varied. Our main contribution is a frequency-domain characterization of the distributed estimator’s steady-state performance; this is quantified in terms of a special matrix associated with the connection topology called the graph Laplacian, and also the rate of message exchange between immediate neighbors in the communication network. I.
399|Cooperative Control of Multi-Vehicle Systems Using Cost Graphs and Optimization|We introduce a class of triangulated graphs for algebraic representation of formations that allows us to specify a mission cost for a group of vehicles. This representation plus the navigational information allows us to formally specify and solve tracking problems for groups of vehicles in formations using an optimization-based approach. The approach is illustrated using a collection of six underactuated vehicles that track a desired trajectory in formation.
400|Flocking with Obstacle Avoidance: Cooperation with Limited Communication in Mobile Networks|In this paper, we provide a dynamic graph theoretic framework for ocking in presence of multiple obstacles. In particular, we give formal de  nitions of nets and ocks as spatially induced graphs and de- ne ocking. We introduce the notion of framenets and describe a procedure for automatic construction of an energy function for groups of agents. The task of ocking is achieved via dissipation of this energy according to a protocol that only requires the use of local information. We show that all three rules of Reynolds are hidden in this single protocol. Three types of agents called ,  , and  agents are used to create ocking. Simulation results are provided that demonstrate ocking by 100 dynamic agents.
401|A study on decentralized receding horizon control for decoupled systems|Abstract — We consider a set of decoupled dynamical systems and an optimal control problem where cost function and constraints couple the dynamical behavior of the systems. The coupling is described through a connected graph where each system is a node and, cost and constraints of the optimization problem associated to each node are only function of its state and the states of its neighbors. For such scenario, we propose a framework for designing decentralized Receding Horizon Control (RHC) control schemes. In these decentralized schemes, a centralized RHC controller is broken into distinct RHC controllers of smaller sizes. Each RHC controller is associated to a different node and computes the local control inputs based only on the states of the node and of its neighbors. The proposed decentralized control schemes are formulated in a rigorous mathematical framework. Moreover, we highlight the main issues involved in guaranteeing stability and constraint fulfillment for such schemes and the degree of conservativeness that the decentralized approach introduces. I.
402|Asynchronous Consensus Protocols: Preliminary Results, Simulations and Open Questions|  Consensus is well accepted as being a fundamental paradigm for coordination of groups of autonomous agents. Recently, we casted previous work on multi-agent consensus protocols into an asynchronous framework, where each agent updates on its own pace, and uses the most recently received information from other agents. Asynchronous consensus protocols encompass those synchronous ones with various communication patterns. In this paper, we study via simulation a number of open new problems introduced by the asynchronous operation of multi-agent systems. More interestingly, the existing consensus results are classified by their communication assumptions and future research directions are proposed. To facilitate our study, we develop a multi-vehicle simulator in Java, built on top of JProwler; JProwler is a discrete event simulator for prototyping, verifying and analyzing communication protocols of ad-hoc wireless networks. Implementation issues with their implications are also discussed.  
403|Synchronization in Oscillator Networks: Switching Topologies and Non-homogeneous Delays| In this paper we investigate the problem of synchronization in oscillator networks when the delay inherent in such systems is taken into account. We first investigate a general Kuramoto-type model with heterogeneous time delays, both with a nearest neighbor as well as a more general network interaction, for which we propose conditions for synchronization around a rotating frequency. Then, we turn our attention to the problem of synchronization when the topologies are allowed to change. We show that synchronization is possible in the presence of delay, using a common Lyapunov functional argument.  
404|Distributed averaging on asynchronous communication networks|Abstract — Distributed algorithms for averaging have attracted interest in the control and sensing literature. However, previous works have not addressed some practical concerns that will arise in actual implementations on packet-switched communication networks such as the Internet. In this paper, we present several implementable algorithms that are robust to asynchronism and dynamic topology changes. The algorithms do not require global coordination and can be proven to converge under very general asynchronous timing assumptions. Our results are verified by both simulation and experiments on a real-world TCP/IP network. I.
405|Formation control and collision avoidance for multi-agent systems and a connection between formation infeasibility and flocking behavior|Abstract — A feedback control strategy that achieves con-vergence of a multi-agent system to a desired formation configuration avoiding at the same time collisions is proposed. The collision avoidance objective is handled by a decentralized navigation function that vanishes when the desired formation tends to be realized. When inter-agent objectives that specify the desired formation cannot occur simultaneously in the state space the desired formation is infeasible. It is shown that under certain assumptions, formation infeasibility forces the agents velocity vectors to a common value at steady state. This provides a connection between formation infeasibility and flocking behavior for the multi-agent system. I.
406|Assessing Rater Performance without a “Gold Standard” Using Consensus Theory’. Medical Decision Making|This study illustrates the use of consensus theory to assess the diagnostic perf rm-ances of raters and to estimate case diagnoses in the absence of a criterion or “gold” standard. A description is provided of how consensus theory “pools ” information pro-vided by raters, estimating rater competencies and differentially weighting their re-sponses. Although the model assumes that raters respond without bias (i.e., sensitivity = specificity), a Monte Carlo simulation with 1,200 data sets shows that model esti-mates appear to be robust even with bias. The model is illustrated on a set of elbow radiographs, and consensus-model estimates are compared with those obtained from follow-up data. Results indicate that with high rater competencies, the model retrieves accurate estimates of competency and case diagnoses even when raters ’ responses are biased. Key words: clinical competence; interobserver variation; diagnostic evalu-ation; models-mathematical: consensus theory. (Med Decis Making 1997;17:71-
407|On sensor fusion in the presence of packet-dropping communication channels|Abstract — In this paper we look at the problem of multisensor data fusion when data is being communicated over channels that drop packets randomly. We are motivated by the use of wireless links for communication among nodes in upcoming sensor networks. We wish to identify the information that should be communicated by each node to others given that some of the information it had transmitted earlier might have been lost. We solve the problem exactly for the case of two sensors and study the performance of the algorithm when more sensors are present. For the two-sensor case, the performance of our algorithm is optimal in the sense that if a packet is received from the other sensor, it is equivalent to receiving all previous measurements, irrespective of the packet drop pattern. I.
408|Interesting conjugate points in formation constrained multi-agent coordination|Abstract — In this paper, an optimal coordinated motion planning problem is formulated where multiple agents have to reach given destination positions starting from given initial positions, subject to constraints on the admissible formation patterns. Solutions to the problem are reinterpreted as distanceminimizing geodesics on a certain manifold with boundary. A geodesic on this manifold that possesses conjugate points will not be distance-minimizing beyond its first conjugate point. We study a particular instance of the problem, where a number of initially aligned agents tries to switch positions by rotating around their common centroid. We characterize analytically the complete set of conjugate points of a geodesic that naturally arises as a candidate solution. This allows us to prove that the geodesic does not correspond to an optimal coordinated motion when the angle of rotation exceeds a threshold that decreases to zero as the number of agents increases. Moreover, infinitesimal perturbations that improve the performance of the geodesic after it passes the conjugate points are characterized by a family of orthogonal polynomials. I.
409|A Transformation System for Developing Recursive Programs|A system of rules for transforming programs is described, with the programs in the form of recursion equations An initially very simple, lucid. and hopefully correct program IS transformed into a more efficient one by altering the recursion structure Illustrative examples of program transformations are given, and a tentative implementation !s described Alternative structures for programs are shown, and a possible initial phase for an automatic or semiautomatic program manipulation system is lndmated  KEY WORDS AND PHRASES program transformation, program mampulatlon, optimization, recursion  CR CATEGORIES&#039; 3 69, 4 12, 4 22, 5 24, 5 25  1. 
410|Ptolemy: A Framework for Simulating and Prototyping Heterogeneous Systems|Ptolemy is an environment for simulation and prototyping of heterogeneous systems. It uses modern object-oriented software technology (C++) to model each subsystem in a natural and efficient manner, and to integrate these subsystems into a whole. Ptolemy encompasses practically all aspects of designing signal processing and communications systems, ranging from algorithms and communication strategies, simulation, hardware and software design, parallel computing, and generating real-time prototypes. To accommodate this breadth, Ptolemy must support a plethora of widely-differing design styles. The core of Ptolemy is a set of object-oriented class definitions that makes few assumptions about the system to be modeled; rather, standard interfaces are provided for generic objects and more specialized, application-specific objects are derived from these. A basic abstraction in Ptolemy is the Domain, which realizes a computational model appropriate for a particular type of subsystem. Current e...
411|Sketchpad: A man-machine graphical communication system|The Sketchpad system uses drawing as a novel communication medium for a computer. The system contains input, output, and computation programs which enable it to interpret information drawn directly on a computer display. It has been used to draw electrical, mechanical, scientific, mathematical, and animated drawings; it is a general purpose system. Sketchpad has shown the most usefulness as an aid to the understanding of processes, such as the notion of linkages, which can be described with pictures. Sketchpad also makes it easy to draw highly repetitive or highly accurate drawings and to change drawings previously drawn with it. The many drawings in this thesis were all made with Sketchpad.
412|Virtual Time and Global States of Distributed Systems|A distributed system can be characterized by the fact that the global state is distributed and that a common time base does not exist. However, the notion of time is an important concept in every day life of our decentralized &#034;real world&#034; and helps to solve problems like getting a consistent population census or determining the potential causality between events. We argue that a linearly ordered structure of time is not (always) adequate for distributed systems and propose a generalized non-standardmodel of time which consists of vectors of clocks. These clock-vectors arepartially orderedand  form a lattice. By using timestamps and a simple clock update mechanism the structure of causality is represented in an isomorphic way. The new model of time has a close analogy to Minkowski&#039;s relativistic spacetime and leads among others to an interesting characterization of the global state problem. Finally, we present a new algorithm to compute a consistent global snapshot of a distributed system where messages may bereceived out of order.
413|Distributed Snapshots: Determining Global States of Distributed Systems|This paper presents an algorithm by which a process in a distributed system determines a global state of the system during a computation. Many problems in distributed systems can be cast in terms of the problem of detecting global states. For instance, the global state detection algorithm helps to solve an important class of problems: stable property detection. A stable property is one that persists: once a stable property becomes true it remains true thereafter. Examples of stable properties are “computation has terminated,”  “the system is deadlocked” and “all tokens in a token ring have disappeared.” The stable property detection problem is that of devising algorithms to detect a given stable property. Global state detection can also be used for checkpointing. 
414|ON DISTRIBUTED SNAPSHOTS|We develop an efficient snapshot algorithm that needs no control messages and does not require channels to be first-in-first-out. We also show that several stable properties (e.g., termination, deadlock) can be detected with uncoordinated distributed snapshots. For such properties, our algorithm can be further simplified.
416|Proof-Carrying Code|This paper describes proof-carrying code (PCC), a mechanism by which a host system can determine with certainty that it is safe to execute a program supplied (possibly in binary form) by an untrusted source. For this to be possible, the untrusted code producer must supply with the code a safety proof that attests to the code&#039;s adherence to a previously defined safety policy. The host can then easily and quickly validate the proof without using cryptography and without consulting any external agents. In order to gain preliminary experience with PCC, we have performed several case studies. We show in this paper how proof-carrying code might be used to develop safe assembly-language extensions of ML programs. In the context of this case study, we present and prove the adequacy of concrete representations for the safety policy, the safety proofs, and the proof validation. Finally, we briefly discuss how we use proof-carrying code to develop network packet filters that are faster than similar filters developed using other techniques and are formally guaranteed to be safe with respect to a given operating system safety policy. 
417|Light Linear Logic |The abuse of structural rules may have damaging complexity effects.
418|Extensibility, safety and performance in the SPIN operating system|This paper describes the motivation, architecture and performance of SPIN, an extensible operating system. SPIN provides an extension infrastructure, together with a core set of extensible services, that allow applications to safely change the operating system&#039;s interface and implementation. Extensions allow an application to specialize the underlying operating system in order to achieve a particular level of performance and functionality. SPIN uses language and link-time mechanisms to inexpensively export ne-grained interfaces to operating system services. Extensions are written in a type safe language, and are dynamically linked into the operating system kernel. This approach o ers extensions rapid access to system services, while protecting the operating system code executing within the kernel address space. SPIN and its extensions are written in Modula-3 and run on DEC Alpha workstations. 1
419|Safe Kernel Extensions Without Run-Time Checking |Abstract This paper describes a mechanism by which an operating system kernel can determine with certainty that it is safe to execute a binary supplied by an untrusted source. The kernel first defines a safety policy and makes it public. Then, using this policy, an application can provide binaries in a special form called proof-carrying code, or simply PCC. Each PCC binary contains, in addition to the native code, a formal proof that the code obeys the safety policy. The kernel can easily validate the proof without using cryptography and without consulting any external trusted entities. If the validation succeeds, the code is guaranteed to respect the safety policy without relying on run-time checks. The main practical difficulty of PCC is in generating the safety proofs. In order to gain some preliminary experience with this, we have written several network packet filters in hand-tuned DEC Alpha assembly language, and then generated PCC binaries for them using a special prototype assembler. The PCC binaries can be executed with no run-time overhead, beyond a one-time cost of 1 to 3 milliseconds for validating the enclosed proofs. The net result is that our packet filters are formally guaranteed to be safe and are faster than packet filters created using Berkeley Packet Filters, Software Fault Isolation, or safe languages such as Modula-3.
420|Standard ML of New Jersey|The Standard ML of New Jersey compiler has been under development for five years now. We have developed a robust and complete environment for Standard ML that supports the implementation of large software systems and generates efficient code. The compiler has also served as a laboratory for developing novel implementation techniques for a sophisticated type and module system, continuation based code generation, efficient pattern matching, and concurrent programming features.
421|ORBIT: An optimizing compiler for Scheme|In this paper we describe an optimizing compiler for Scheme [3, 131 called Orbit that incorporates our experience with an earlier Scheme compiler called TC [IO, II], together with some ideas from Steele’s Rabbit compiler [14]. The three main design goals have been correctness, generating very efficient compiled code, and portability. In spirit, Orbit is similar to the Rabbit compiler in that it depends on a translation of source code into
422|Unboxed Objects and Polymorphic Typing|This paper presents a program transformation that allows languages with polymorphic typing (e.g. ML) to be implemented with unboxed, multi-word data representations, more efficient than the conventional boxed representations. The transformation introduces coercions between various representations, based on a typing derivation. A prototype ML compiler utilizing this transformation demonstrates important speedups.
423|TALx86: A Realistic Typed Assembly Language|The goal of typed assembly language (TAL) is to provide a low-level, statically typed target language that is better suited than Java bytecodes for supporting a wide variety of source languages and a number of important optimizations. In previous work, we formalized idealized versions of TAL and proved important safety properties about them. In this paper, we present our progress in defining and implementing a realistic typed assembly language called TALx86. The  TALx86 instructions comprise a relatively complete fragment of the Intel IA32 (32-bit 80x86 flat model) assembly language and are thus executable on processors such as the Intel Pentium. The type system for the language incorporates a number of advanced features necessary for safely compiling large programs to good code. To motivate the design of the type system, we demonstrate how various high-level language features are compiled to TALx86. For this purpose, we present a type-safe C-like language called Popcorn. 1 Introductio...
424|Typed closure conversion|The views and conclusions contained in this document are those of the authors and should not be interpreted as representing o cial policies, either expressed or implied, of the Advanced Research Projects Agency or the U.S. Government. Any opinions, ndings, and conclusions or recommendations expressed in this material are those of the We study the typing properties of closure conversion for simply-typed and polymorphic-calculi. Unlike most accounts of closure conversion, which only treat the untyped-calculus, we translate well-typed source programs to well-typed target programs. This allows later compiler phases to take advantage of types for representation analysis and tag-free garbage collection, and it facilitates correctness proofs. Our account of closure conversion for the simply-typed language takes advantage of a simple model of objects by mapping closures to existentials. Closure conversion for the polymorphic language requires additional type machinery, namely translucency in the style of Harper and Lillibridge&#039;s module calculus, to express the type of a closure.
425|A Linearly Typed Assembly Language|Today&#039;s type-safe low-level languages rely on garbage collection to recycle heap-allocated objects safely. We present LTAL, a safe, low-level, yet simple language that &#034;stands on its own&#034;: it guarantees safe execution within a fixed memory space, without relying on external run-time support. We demonstrate the expressiveness of LTAL by giving a type-preserving compiler for the functional core of ML. But this independence comes at a steep price: LTAL&#039;s type system imposes a draconian discipline of linearity that ensures that memory can be reused safely, but prohibits any useful kind of sharing. We present the results of experiments with a prototype LTAL system that show just how high the price of linearity can be.
426|Compiling with Proofs|One of the major challenges of building software systems is to ensure that the  various components fit together in a well-defined manner. This problem is exacerbated  by the recent advent of software components whose origin is unknown  or inherently untrusted, such as mobile code or user extensions for operatingsystem  kernels or database servers. Such extensions are useful for implementing  an e#cient interaction model between a client and a server because several data  exchanges between them can be saved at the cost of a single code exchange.  In this dissertation, I propose to tackle such system integrity and security problems  with techniques from mathematical logic and programming-language semantics.  I propose a framework, called proof-carrying code, in which the extension  provider sends along with the extension code a representation of a formal  proof that the code meets certain safety and correctness requirements. Then,  the code receiver can ensure the safety of executing the...
427|Intensional Polymorphism in Type-Erasure Semantics|Intensional polymorphism, the ability to dispatch to di#erent routines based on types at run time, enables a variety of advanced implementation techniques for polymorphic languages, including tag-free garbage collection, unboxed function arguments, polymorphic marshalling, and flattened data structures. To date, languages that support intensional polymorphism have required a type-passing (as opposed to type-erasure) interpretation where types are constructed and passed to polymorphic functions at run time. Unfortunately, type-passing su#ers from a number of drawbacks: it requires duplication of run-time constructs at the term and type levels, it prevents abstraction, and it severely complicates polymorphic closure conversion.
428|The Glasgow Haskell compiler: a technical overview|We give an overview of the Glasgow Haskell compiler, focusing especially on way in which we have been able to exploit the rich theory of functional languages to give very practical improvements in the compiler. The compiler is portable, modular, generates good code, and is freely available.  
429|Representing control: a study of the CPS transformation|This paper investigates the transformation of  v -terms into continuation-passing style  (CPS). We show that by appropriate j-expansion of Fischer and Plotkin&#039;s two-pass  equational specification of the CPS transform, we can obtain a static and context-free  separation of the result terms into &#034;essential&#034; and &#034;administrative&#034; constructs. Interpreting  the former as syntax builders and the latter as directly executable code,  we obtain a simple and efficient one-pass transformation algorithm, easily extended to  conditional expressions, recursive definitions, and similar constructs. This new transformation  algorithm leads to a simpler proof of Plotkin&#039;s simulation and indifference  results.  Further we show how CPS-based control operators similar to but more general than  Scheme&#039;s call/cc can be naturally accommodated by the new transformation algorithm.  To demonstrate the expressive power of these operators, we use them to present  an equivalent but even more concise formulation of t...
430|Flexible Type Analysis|Run-time type dispatch enables a variety of advanced optimization techniques for polymorphic languages, including tag-free garbage collection, unboxed function arguments, and flattened data structures. However, modern type-preserving compilers transform types between stages of compilation, making type dispatch prohibitively complex at low levels of typed compilation. It is crucial therefore for type analysis at these low levels to refer to the types of previous stages. Unfortunately, no current intermediate language supports this facility. To fill this gap, we present the language LX, which provides a rich language of type constructors supporting type analysis (possibly of previous-stage types) as a programming idiom. This language is quite flexible, supporting a variety of other applications such as analysis of quantified types, analysis with incomplete type information, and type classes. We also show that LX is compatible with a type-erasure semantics. 1 Introduction Type-directed co...
431|Explicit Polymorphism and CPS Conversion|We study the typing properties of CPS conversion for an extension of F ! with control operators. Two classes of evaluation strategies are considered, each with call-by-name and call-by-value variants. Under the &#034;standard&#034; strategies, constructor abstractions are values, and constructor applications can lead to non-trivial control effects. In contrast, the &#034;ML-like&#034; strategies evaluate beneath constructor abstractions, reflecting the usual interpretation of programs in languages based on implicit polymorphism. Three continuation passing style sub-languages are considered, one on which the standard strategies coincide, one on which the ML-like strategies coincide, and one on which all the strategies coincide. Compositional, type-preserving CPS transformation algorithms are given for the standard strategies, resulting in terms on which all evaluation strategies coincide. This has as a corollary the soundness and termination of well-typed programs under the standard evaluation strategies. A similar result is obtained for the ML-like call-by-name strategy. In contrast, such results are obtained for the call-by value ML-like strategy only for a restricted sub-language in which constructor abstractions are limited to values.
432|Type-Safe Linking and Modular Assembly Language|Linking is a low-level task that is usually vaguely specified, if at all, by language definitions. However, the security of web browsers and other extensible systems depends crucially upon a set of checks that must be performed at link time. Building upon the simple, but elegant ideas of Cardelli, and module constructs from high-level languages, we present a formal model of typed object files and a set of inference rules that are sufficient to guarantee that type safety is preserved by the linking process.
433|Semantics of memory management for polymorphic languages|The views and conclusions contained in this document arethose of the authors and should not be interpreted as representing o cial policies, either expressed or implied, of the Advanced We present a static and dynamic semantics for an abstract machine that evaluates expressions of a polymorphic programming language. Unlike traditional semantics, our abstract machine exposes many important issues of memory management, such as value sharing and control representation. We prove the soundness of the static semantics with respect to the dynamic semantics using traditional techniques. We then show how these same techniques may be used to establish the soundness of various memory management strategies, including type-based, tag-free garbage collection? tail-call elimination ? and environment strengthening. Keywords: management Type theory and operational semantics are remarkably e ective tools for programming
434|Type Dispatch for Named Hierarchical Types|Type dispatch constructs are an important feature of many programming languages. Scheme has predicates for testing the runtime type of a value. Java has a class cast expression and a try statement for switching on an exception&#039;s class. Crucial to these mechanisms, in typed languages, is type refinement: The static type system will use type dispatch to refine types in successful dispatch branches. Existing work in functional languages has addressed certain kinds of type dispatch, namely, intensional type analysis. However, this work does not extend to languages with subtyping nor to named types. This paper describes a number of type dispatch constructs that share a common theme: class cast and class case constructs in object oriented languages, ML style exceptions, hierarchical extensible sums, and multimethods. I describe a unifying mechanism, tagging, that abstracts the operation of these constructs, and formalise a small tagging language. After discussing how to implement the tagging...
435|Symbolic Model Checking for Real-time Systems|We describe finite-state programs over real-numbered time in a guarded-command  language with real-valued clocks or, equivalently, as finite automata with  real-valued clocks. Model checking answers the question which states of a real-time  program satisfy a branching-time specification (given in an extension of CTL with clock  variables). We develop an algorithm that computes this set of states symbolically as a  fixpoint of a functional on state predicates, without constructing the state space.  For this purpose, we introduce a -calculus on computation trees over real-numbered  time. Unfortunately, many standard program properties, such as response for all  nonzeno execution sequences (during which time diverges), cannot be characterized  by fixpoints: we show that the expressiveness of the timed -calculus is incomparable  to the expressiveness of timed CTL. Fortunately, this result does not impair the  symbolic verification of &#034;implementable&#034; real-time programs---those whose safety...
436|Hybrid Automata: An Algorithmic Approach to the Specification and Verification of Hybrid Systems|We introduce the framework of hybrid automata as a model and specification language for hybrid systems. Hybrid automata can be viewed as a generalization of timed automata, in which the behavior of variables is governed in each state by a set of differential equations. We show that many of the examples considered in the workshop can be defined by hybrid automata. While the reachability problem is undecidable even for very restricted classes of hybrid automata, we present two semidecision procedures for verifying safety properties of piecewise-linear hybrid automata, in which all variables change at constant rates. The two procedures are based, respectively, on minimizing and computing fixpoints on generally infinite state spaces. We show that if the procedures terminate, then they give correct answers. We then demonstrate that for many of the typical workshop examples, the procedures do terminate and thus provide an automatic way for verifying their properties. 1 Introduction  More and...
438|Classification in  the KL-ONE knowledge representation system|KL-ONE lets one define and use a class of descriptive terms called Concepts, where each Concept denotes a set of objects A subsumption relation between Concepts is defined which is related to set inclusion by way of a semantics for Concepts. This subsumption relation defines a partial order on Concepts, and KL-ONE organizes all Concepts into a taxonomy that reflects this partial order. Classification is a process that takes a new Concept and determines other Concepts that either subsume it or that it subsumes, thereby determining the location for the new Concept within a given taxonomy. We discuss these issues and demonstrate some uses of the classification algorithm.  
439|An Information Presentation System|AIPS is a system for graphically presenting information. It promotes a high degree of interactivity between a user and a knowledge base or knowlege-based system, and is designed to be utmost domain independent and extensible. This paper describes the concept of an Information Presentation System (IPS), the intimate relationship between IPS goals and knowledge representation issues, and some of the architecture of AIPS. I Information Presentation Interactive graphics is an indispensible technique for putting people in touch with a large knowledge base or knowledge-based system. Graphic output is the best way to communicate a substantial amount of information to a human user because it exploits the high-bandwidth human visual channel. Graphic input (i.e. user input which points at or otherwise indicates components of a graphic display) is an extremely economical way to describe something; it is much easier to designate an existing depiction than to generate some other descriptor. The descriptional economy of graphic input promotes a feeling of immediacy; the user has the sense of interacting directly with information rather than dealinq with an intermediary. For these reasons, interactive graphics can play an important role in larqe knowledge-based systems, whether at the interface to the end user or at the interface to the implementor or maintainer. That role is not diminished by progress in natural
440|Vivaldi: A Decentralized Network Coordinate System|Large-scale Internet applications can benefit from an ability to predict round-trip times to other hosts without having to contact them first. Explicit measurements are often unattractive because the cost of measurement can outweigh the benefits of exploiting proximity information. Vivaldi is a simple, light-weight algorithm that assigns synthetic coordinates to hosts such that the distance between the coordinates of two hosts accurately predicts the communication latency between the hosts.
441|Wide-area cooperative storage with CFS|The Cooperative File System (CFS) is a new peer-to-peer readonly storage system that provides provable guarantees for the efficiency, robustness, and load-balance of file storage and retrieval. CFS does this with a completely decentralized architecture that can scale to large systems. CFS servers provide a distributed hash table (DHash) for block storage. CFS clients interpret DHash blocks as a file system. DHash distributes and caches blocks at a fine granularity to achieve load balance, uses replication for robustness, and decreases latency with server selection. DHash finds blocks using the Chord location protocol, which operates in time logarithmic in the number of servers. CFS is implemented using the SFS file system toolkit and runs on Linux, OpenBSD, and FreeBSD. Experience on a globally deployed prototype shows that CFS delivers data to clients as fast as FTP. Controlled tests show that CFS is scalable: with 4,096 servers, looking up a block of data involves contacting only seven servers. The tests also demonstrate nearly perfect robustness and unimpaired performance even when as many as half the servers fail.
442|Surface reconstruction from unorganized points|We describe and demonstrate an algorithm that takes as input an unorganized set of points fx1?:::?xng IR 3 on or near an unknown manifold M, and produces as output a simplicial surface that approximates M. Neither the topology, the presence of boundaries, nor the geometry of M are assumed to be known in advance — all are inferred automatically from the data. This problem naturally arises in a variety of practical situations such as range scanning an object from multiple view points, recovery of biological shapes from two-dimensional slices, and interactive surface sketching.
443|Predicting Internet Network Distance with Coordinates-Based Approaches|In this paper, we propose to use coordinates-based mechanisms in a peer-to-peer architecture to predict Internet network distance (i.e. round-trip propagation and transmission delay) . We study two mechanisms. The first is a previously proposed scheme, called the triangulated heuristic, which is based on relative coordinates that are simply the distances from a host to some special network nodes. We propose the second mechanism, called Global Network Positioning (GNP), which is based on absolute coordinates computed from modeling the Internet as a geometric space. Since end hosts maintain their own coordinates, these approaches allow end hosts to compute their inter-host distances as soon as they discover each other. Moreover coordinates are very efficient in summarizing inter-host distances, making these approaches very scalable. By performing experiments using measured Internet distance data, we show that both coordinates-based schemes are more accurate than the existing state of the art system IDMaps, and the GNP approach achieves the highest accuracy and robustness among them.
444|Modeling Internet Topology|The topology of a network, or a group of networks such as the Internet, has a strong bearing on many management and performance issues. Good models of the topological structure of a network are essential for developing and analyzing internetworking technology. This article discusses how graph-based models can be used to represent the topology of large networks, particularly aspects of locality and hierarchy present in the Internet. Two implementations that generate networks whose topology resembles that of typical internetworks are described, together with publicly available source code.  
445|Geographic Routing without Location Information|For many years, scalable routing for wireless communication systems was a compelling but elusive goal. Recently, several routing algorithms that exploit geographic information (e.g., GPSR) have been proposed to achieve this goal. These algorithms refer to nodes by their location, not address, and use those coordinates to route greedily, when possible, towards the destination. However, there are many situations where location information is not available at the nodes, and so geographic methods cannot be used. In this paper we define a scalable coordinate-based routing algorithm that does not rely on location information, and thus can be used in a wide variety of ad hoc and sensornet environments.  
446|IDMaps: A Global Internet Host Distance Estimation Service|There is an increasing need to quickly and efficiently learn network distances, in terms of metrics such as latency or bandwidth, between Internet hosts. For example, Internet content providers often place data and server mirrors throughout the Internet to improve access latency for clients, and it is necessary to direct clients to the closest mirrors based on some distance metric in order to realize the benefit of mirrors. We suggest a scalable Internet-wide architecture, called IDMaps, which measures and disseminates distance information on the global Internet. Higher-level services can collect such distance information to build a virtual distance map of the Internet and estimate the distance between any pair of IP addresses. We present our solutions to the measurement server placement and distance map construction problems in IDMaps. We show that IDMaps can indeed provide useful distance estimations to applications such as closest-mirror selection.
447|Locationing in distributed ad-hoc wireless sensor networks|Evolving networks of ad-hoc, wireless sensing nodes rely heavily on the ability to establish position information. The algorithms presented herein rely on range measurements between pairs of nodes and the aprioricoordinates of sparsely located anchor nodes. Clusters of nodes surrounding anchor nodes cooperatively establish confident position estimates through assumptions, checks, and iterative refinements. Once established, these positions are propagated to more distant nodes, allowing the entire network to create an accurate map of itself. Major obstacles include overcoming inaccuracies in range measurements as great as ±50%, as well as the development of initial guesses for node locations in clusters with few or no anchor nodes. Solutions to these problems are presented and discussed, using position error as the primary metric. Algorithms are compared according to position error, scalability, and communication and computational requirements. Early simulations yield average position errors of 5 % in the presence of both range and initial position inaccuracies. 1.
448|Designing a DHT for low latency and high throughput|Designing a wide-area distributed hash table (DHT) that provides high-throughput and low-latency network storage is a challenge. Existing systems have explored a range of solutions, including iterative routing, recursive routing, proximity routing and neighbor selection, erasure coding, replication, and server selection. This
449|Virtual Landmarks for the Internet|Internet coordinate schemes have been proposed as a method for estimating minimum round trip time between hosts without direct measurement. In such a scheme, each host is assigned a set of coordinates, and Euclidean distance is used to form the desired estimate. Two key questions are: How accurate are coordinate schemes across the Internet as a whole? And: are coordinate assignment schemes fast enough, and scalable enough, for large scale use? In this paper we make contributions toward answering both those questions. Whereas the coordinate assignment problem has in the past been approached by nonlinear optimization, we develop a faster method based on dimensionality reduction of the Lipschitz embedding. We show that this method is reasonably accurate, even when applied to measurements spanning the Internet, and that it naturally leads to a scalable measurement strategy based on the notion of virtual landmarks.
450|PIC: Practical Internet Coordinates for Distance Estimation|mechanism to estimate Internet network distance (i.e., round-trip delay or network hops). Network distance estimation is important in many applications, for example, network-aware overlay construction and server selection. There are several proposals for distance estimation in the Internet but they all suffer from problems that limit their benefit. Most rely on a small set of infrastructure nodes that are a single point of failure and limit scalability. Others use sets of peers to compute coordinates but these coordinates can be arbitrarily wrong if one of these peers is malicious. While it may be reasonable to secure a small set of infrastructure nodes, it is unreasonable to secure all peers. PIC addresses these problems: it does not rely on infrastructure nodes and it can compute accurate coordinates even when some peers are malicious. We present PIC&#039;s design, experimental evaluation, and an application to network-aware overlay construction and maintenance.
451|Lighthouses for Scalable Distributed Location|This paper introduces Lighthouse, a scalable location mechanism  for wide-area networks. Unlike existing vector-based systems such  as GNP, we show how network-location can be established without using  a  xed set of reference points. This lets us avoid the communication bottlenecks  and single-points-of-failure that otherwise limit the practicality  of such systems.
452|Big-Bang Simulation for Embedding Network Distances in Euclidean Space|Embedding of a graph metric in Euclidean space efficiently and accurately is an important problem in general with applications in topology aggregation, closest mirror selection, and application level routing. We propose a new graph embedding scheme called Big-Bang Simulation (BBS), which simulates an explosion of particles under force field derived from embedding error. BBS is shown to be significantly more accurate, compared to all other embedding methods including GNP. We report an extensive simulation study of BBS compared with several known embedding schemes and show its advantage for distance estimation (as in the IDMaps project), mirror selection and topology aggregation.
453|Anchor-Free Distributed Localization in Sensor Networks|Many sensor network applications require that each node&#039;s sensor stream be annotated with its physical location in some common coordinate system. Manual measurement and configuration methods for obtaining location don&#039;t scale and are error-prone, and equipping sensors with GPS is often expensive and does not work in indoor and urban deployments. Sensor networks can therefore benefit from a self-configuring method where nodes cooperate with each other, estimate local distances to their neighbors, and converge to a consistent coordinate assignment. This paper describes a fully decentralized algorithm called AFL (Anchor-Free Localization) where nodes start from a random initial coordinate assignment and converge to a consistent solution using only local node interactions. The key idea in AFL is fold-freedom, where nodes first configure into a topology that resembles a scaled and unfolded version of the true configuration, and then run a force-based relaxation procedure.We show using extensive simulations under a variety of network sizes, node densities, and distance estimation errors that our algorithm is superior to previously proposed methods that incrementally compute the coordinates of nodes in the network, in terms of its ability to compute correct coordinates under a wider variety of conditions and its robustness to measurement errors.
454|Efficient Topology-Aware Overlay Network|Peer-to-peer (P2P) networking has become a household word in the past few years, being marketed as a work-around for server scalability problems and ``wonder drug&#039;&#039; to achieve resilience. Current widely-used P2P networks rely on central directory servers or massive message flooding, clearly not scalable solutions. Distributed Hash Tables (DHT) are expected to eliminate flooding and central servers, but can require many long-haul message deliveries. We introduce Mithos, an content-addressable overlay network that only uses minimal routing information and is directly suitable as an underlay network for P2P systems, both using traditional and DHT addressing. Unlike other schemes, it also efficiently provides locality-aware connectivity, thereby ensuring that a message reaches its destination with minimal overhead. Mithos provides for highly efficient forwarding, making it suitable for use in high-throughput applications. Paired with its ability to have addresses directly mapped into a subspace of the IPv6 address space, it provides a potential candidate for native deployment. Additionally, Mithos can be used to support third-party triangulation to quickly select a close-by replica of data or services.
455|A Network Positioning System for the Internet|Network positioning has recently been demonstrated to be a viable concept to represent the network distance relationships among Internet end hosts. Several subsequent studies have examined the potential benefits of using network position in applications, and proposed alternative network positioning algorithms. In this paper, we study the problem of designing and building a network positioning system (NPS). We identify several key system building issues such as the consistency, adaptivity and stability of host network positions over time. We propose a hierarchical network positioning architecture that maintains consistency while enabling decentralization, a set of adaptive decentralized algorithms to compute and maintain accurate, stable network positions, and finally present a prototype system deployed on PlanetLab nodes that can be used by a variety of applications. We believe our system is a viable first step to provide a network positioning capability in the Internet. 
456|Practical, Distributed Network Coordinates|Vivaldi is a distributed algorithm that assigns synthetic coordinates to Internet hosts, so that the Euclidean distance between two hosts&#039; coordinates predicts the network latency between them. Each node in Vivaldi computes its coordinates by simulating its position in a network of physical springs. Vivaldi is both distributed and efficient: no fixed infrastructure need be deployed and a new host can compute useful coordinates after collecting latency information from only a few other hosts. Vivaldi can rely on piggy-backing latency information on application traffic instead of generating extra traffic by sending its own probe packets. This paper
457|On the Curvature of the Internet and its usage for Overlay Construction and Distance Estimation|It was noted in recent years that the Internet structure resembles a star with a highly connected core and long stretched tendrils. In this work we present a new quantity, the Internet geometric curvature, that captures the above observation by a single number. We embed the Internet distance metric in a hyperbolic space with an optimal curvature and achieve an accuracy better than achieved before for the Euclidean space. This proves our hypothesis regarding the internet curvature. We demonstrate the strength of our embedding with two applications: selecting the closest server and building an application level multicast tree.
458|Efficient use of workstations for passive monitoring of local area networks|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
459|Routing and Data Location in Overlay Peer-to-Peer Networks|Peer-to-peer overlay networks offer a novel platform for a variety of scalable and decentralized distributed applications. Systems known as Distributed Hash Tables provide efficient and fault-tolerant routing, object location and load balancing within a self-organizing overlay network. The alternative solution we propose is an overlay location and routing infrastructure that efficiently uses minimal local information to achieve global routing. The main novelty of our approach consists in fitting the overlay network in a hyper-toroidal space and building it with *locality awareness.* Thanks to this specific network construction phase, forwarding decisions always take into account *locality preservation* in an implicit manner, leading to significant improvements in end-to-end delays and path lengths. With this overlay network it is possible to obtain global routing by adding minimal information to each single host and by making only local forwarding decisions. Our analysis shows how the average path length coming from the overlay routing is close to the optimal average path length of the underlaying network: on average, they only differ by a factor of 2. Furthermore, *locality preservation* has a significant impact on the end-to-end latency of the routing process as well. Such a system can be viewed as novel in the field of peer-to-peer data location and addressing, allowing the development of new applications in a real *low-latency* environment.
460|Peer-to-peer simulator|the development of a general force field for the molecular
461|Identity-Based Encryption from the Weil Pairing|We propose a fully functional identity-based encryption scheme (IBE). The scheme has chosen ciphertext security in the random oracle model assuming an elliptic curve variant of the computational Diffie-Hellman problem. Our system is based on bilinear maps between groups. The Weil pairing on elliptic curves is an example of such a map. We give precise definitions for secure identity based encryption schemes and give several applications for such systems.
462|Handbook of Applied Cryptography|As we draw near to closing out the twentieth century, we see quite clearly that the information-processing and telecommunications revolutions now underway will continue vigorously into the twenty-first. We interact and transact by directing flocks of digital packets towards each other through cyberspace, carrying love notes, digital cash, and secret corporate documents. Our personal and economic lives rely more and more on our ability to let such ethereal carrier pigeons mediate at a distance what we used to do with face-to-face meetings, paper documents, and a firm handshake. Unfortunately, the technical wizardry enabling remote collaborations is founded on broadcasting everything as sequences of zeros and ones that one&#039;s own dog wouldn&#039;t recognize. What is to distinguish a digital dollar when it is as easily reproducible as the spoken word? How do we converse privately when every syllable is bounced off a satellite and smeared over an entire continent? How should a bank know that it really is Bill Gates requesting from his laptop in Fiji a transfer of $10,000,000,000 to another bank? Fortunately, the magical mathematics of cryptography can help. Cryptography provides techniques for keeping information secret, for determining that information
463|How to leak a secret|In this paper we formalize the notion of a ring signature, which makes it possible to specify a set of possible signers without revealing which member actually produced the signature. Unlike group signatures, ring signatures have no group managers, no setup procedures, no revocation procedures, and no coordination: any user can choose any set of possible signers that includes himself, and sign any message by using his secret key and the others ’ public keys, without getting their approval or assistance. Ring signatures provide an elegant way to leak authoritative secrets in an anonymous way, to sign casual email in a way which can only be verified by its intended recipient, and to solve other problems in multiparty computations. The main contribution of this paper is a new construction of such signatures which is unconditionally signer-ambiguous, provably secure in the random oracle model, and exceptionally efficient: adding each ring member increases the cost of signing or verifying by a single modular multiplication and a single symmetric encryption.
464|Random Oracles are Practical: A Paradigm for Designing Efficient Protocols|We argue that the random oracle model -- where all parties have access to a public random oracle -- provides a bridge between cryptographic theory and cryptographic practice. In the paradigm we suggest, a practical protocol P is produced by first devising and proving correct a protocol P  R  for the random oracle model, and then replacing oracle accesses by the computation of an &#034;appropriately chosen&#034; function h. This paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security. We illustrate these gains for problems including encryption, signatures, and zero-knowledge proofs.
465|A public key cryptosystem and a signature scheme based on discrete logarithms|Abstract-A new signature scheme is proposed, together with an imple-mentation of the Diffie-Hellman key distribution scheme that achieves a public key cryptosystem. The security of both systems relies on the difficulty of computing discrete logarithms over finite fields. I.
466|How To Prove Yourself: Practical Solutions to Identification and Signature Problems|In this paper we describe simple identification and signature schemes which enable any user to prove his identity and the authenticity of his messages to any other user without shared or public keys. The schemes are provably secure against any known or chosen message attack ff factoring is difficult, and typical implementations require only 1% to 4% of the number of modular multiplications required by the RSA scheme. Due to their simplicity, security and speed, these schemes are ideally suited for microprocessor-based devices such as smart cards, personal computers, and remote control system.q.
467|Short signatures from the Weil pairing|Abstract. We introduce a short signature scheme based on the Computational Diffie-Hellman assumption on certain elliptic and hyper-elliptic curves. The signature length is half the size of a DSA signature for a similar level of security. Our short signature scheme is designed for systems where signatures are typed in by a human or signatures are sent over a low-bandwidth channel. 1
468|A practical public key cryptosystem provably secure against adaptive chosen ciphertext attack| A new public key cryptosystem is proposed and analyzed. The scheme is quite practical, and is provably secure against adaptive chosen ciphertext attack under standard intractability assumptions. There appears to be no previous cryptosystem in the literature that enjoys both of these properties simultaneously. 
469|Relations among notions of security for public-key encryption schemes|Abstract. We compare the relative strengths of popular notions of security for public key encryption schemes. We consider the goals of privacy and non-malleability, each under chosen plaintext attack and two kinds of chosen ciphertext attack. For each of the resulting pairs of definitions we prove either an implication (every scheme meeting one notion must meet the other) or a separation (there is a scheme meeting one notion but not the other, assuming the first notion can be met at all). We similarly treat plaintext awareness, a notion of security in the random oracle model. An additional contribution of this paper is a new definition of non-malleability which we believe is simpler than the previous one.
470|Non-Malleable Cryptography|The notion of non-malleable cryptography, an extension of semantically secure cryptography, is defined. Informally, in the context of encryption the additional requirement is that given the ciphertext it is impossible to generate a different ciphertext so that the respective plaintexts are related. The same concept makes sense in the contexts of string commitment and zero-knowledge proofs of possession of knowledge. Non-malleable schemes for each of these three problems are presented. The schemes do not assume a trusted center; a user need not know anything about the number or identity of other system users. Our cryptosystem is the first proven to be secure against a strong type of chosen ciphertext attack proposed by Rackoff and Simon, in which the attacker knows the ciphertext she wishes to break and can query the decryption oracle on any ciphertext other than the target.
471|Efficient algorithms for pairing-based cryptosystems|Abstract. We describe fast new algorithms to implement recent cryptosystems based on the Tate pairing. In particular, our techniques improve pairing evaluation speed by a factor of about 55 compared to previously known methods in characteristic 3, and attain performance comparable to that of RSA in larger characteristics. We also propose faster algorithms for scalar multiplication in characteristic 3 and square root extraction over Fpm, the latter technique being also useful in contexts other than that of pairing-based cryptography. 1
472|Lower Bounds for Discrete Logarithms and Related Problems|. This paper considers the computational complexity of the discrete logarithm and related problems in the context of &#034;generic algorithms&#034;---that is, algorithms which do not exploit any special properties of the encodings of group elements, other than the property that each group element is encoded as a unique binary string. Lower bounds on the complexity of these problems are proved that match the known upper bounds: any generic algorithm must perform\Omega (p  1=2  ) group operations, where p is the largest prime dividing the order of the group. Also, a new method for correcting a faulty Diffie-Hellman oracle is presented. 1 Introduction  The discrete logarithm problem plays an important role in cryptography. The problem is this: given a generator g of a cyclic group G, and an element g  x  in  G, determine x. A related problem is the Diffie-Hellman problem: given g  x  and  g  y  , determine g  xy  . In this paper, we study the computational power of &#034;generic algorithms&#034;--- that is, ...
473|An identity based encryption scheme based on quadratic residues| We present a novel public key cryptosystem in which the public key of a subscriber can be chosen to be a publicly known value, such as his identity. We discuss the security of the proposed scheme, and show that this is related to the difficulty of solving the quadratic residuosity problem.
474|The Decision Diffie-Hellman Problem|The Decision Diffie-Hellman assumption (ddh) is a gold mine. It enables one to construct efficient cryptographic systems with strong security properties. In this paper we survey the recent applications of DDH as well as known results regarding its security. We describe some open problems in this area. 1 Introduction  An important goal of cryptography is to pin down the exact complexity assumptions used by cryptographic protocols. Consider the Diffie-Hellman key exchange protocol [12]: Alice and Bob fix a finite cyclic group G and a generator g. They respectively pick random a; b 2 [1; jGj] and exchange g  a  ; g  b  . The secret key is g  ab  . To totally break the protocol a passive eavesdropper, Eve, must compute the Diffie-Hellman function defined as: dh g (g  a  ; g  b  ) = g  ab  . We say that the group G satisfies the Computational Diffie-Hellman assumption (cdh) if no efficient algorithm can compute the function dh g (x; y)  in G. Precise definitions are given in the next sectio...
475|Secure Integration of Asymmetric and Symmetric Encryption Schemes|This paper shows a generic and simple conversion from weak asymmetric and symmetric encryption schemes into an asymmetric encryption scheme which is secure in a very strong sense - indistinguishability against adaptive chosen-ciphertext attacks in the random oracle model. In particular, this conversion can be applied efficiently to an asymmetric encryption scheme that provides a large enough coin space and, for every message, many enough variants of the encryption, like the ElGamal encryption scheme.
476|Secure Distributed Key Generation for Discrete-Log Based Cryptosystems|Abstract. Distributed key generation is a main component of threshold cryptosystems and distributed cryptographic computing in general. Solutions to the distributed generation of private keys for discrete-log based cryptosystems have been known for several years and used in a variety of protocols and in many research papers. However, these solutions fail to provide the full security required and claimed by these works. We show how an active attacker controlling a small number of parties can bias the values of the generated keys, thus violating basic correctness and secrecy requirements of a key generation protocol. In particular, our attacks point out to the places where the proofs of security fail. Based on these findings we designed a distributed key generation protocol which we present here together with a rigorous proof of security. Our solution, that achieves optimal resiliency, can be used as a drop-in replacement for key generation modules as well as other components of threshold or proactive discrete-log based cryptosystems.
477|Implementing Tate Pairing|  The Weil and Tate pairings have found several new applications in cryptography. To efficiently implement these cryptosystems it is necessary to optimise the computation time for the Tate pairing. This paper provides methods to achieve fast computation of the Tate pairing. We also give division-free formulae for point tripling on a family of elliptic curves in characteristic three. Examples of the running time for these methods are given.
478|On the Exact Security of Full Domain Hash|The Full Domain Hash (FDH) scheme is a RSA-based signature  scheme in which the message is hashed onto the full domain of the  RSA function. The FDH scheme is provably secure in the random oracle  model, assuming that inverting RSA is hard. In this paper we exhibit a  slightly di#erent proof which provides a tighter security reduction. This  in turn improves the e#ciency of the scheme since smaller RSA moduli  can be used for the same level of security. The same method can be used  to obtain a tighter security reduction for Rabin signature scheme, Paillier  signature scheme, and the Gennaro-Halevi-Rabin signature scheme.
479|Public-Key Encryption in a Multi-user Setting: Security Proofs and Improvements  |This paper addresses the security of public-key cryptosystems in a &#034;multi-user&#034; setting, namely in the presence of attacks involv-ing the encryption of related messages under different public keys, as exemplified by Hºastad&#039;s classical attacks on RSA. We prove that security in the single-user setting implies security in the multi-user setting as long as the former is interpreted in the strong sense of &#034;indistinguishability,&#034; thereby pin-pointing many schemes guaranteed to be secure against Hºastad-type attacks. We then highlight the importance, in practice, of considering and improving the concrete security of the general reduction, and present such improvements for two Diffie-Hellman based schemes, namely El Gamal and Cramer-Shoup. 
480|New explicit conditions of elliptic curve traces for FR-reduction|In this paper, we aim at characterizing elliptic curve traces by FR-reduction and investigate explicit conditions of traces vulnerable or secure against FR-reduction. We show new explicit conditions of elliptic curve traces for FRreduction. We also present algorithms to construct such elliptic curves, which have relation to famous number theory problems. key words: elliptic curve cryptosystems, trace, FRreduction 1. Introduction Koblitzand Miller proposed ind end tly a public key cryptosystembased on an elliptic curve E d2EO8 over a finitefield F q (q = p r )([19], [ 5]). If elliptic curve cryptosystems satisfy socalled FRcondO0GO2 ([11], [17], [ 4])and avoid anomalous elliptic curve over F q ([3], [33], [35]), then the only known attacks are the Pollard #-method ([ 7])and the Pohlig-Hellman method ([ 6]). Hence with current knowledEL we can construct elliptic curve cryptosystems over a smallerdaller2L field than thede2OAOS logarithm problem(DLP)-based cryptosystems like the ElGamal cryptosystems ([13]) or the DSA ([1 ])and RSA cryptosystems ([ 8]). Elliptic curve cryptosystems with a 160-bit key are Manuscript received August 31, 2000. Manuscript revised August 31, 2000. The author is with Japan Advanced Institute of Science and Technology, Ishikawa-ken, 923-1292 Japan. The author is with Matsushita Communication Industrial Co., Ltd., Kanagawa-ken, 223-8639 Japan. This work was conducted when he was with JAIST. thus believed to have the same security as both the ElGamal cryptosystemsand RSA cryptosystems with a 1,0 4-bit key. Recently some researches on comparing MOV and FR-redgAGSSE have been reported in [15], [18]. These attacks imbed a subgroup # E(F q )toF # q k for an extensionfield F q kand red -2 ECDLPbased on # E(F q ) to DLP based ...
481|Supersingular curves in cryptography|Frey and Rück gave a method to map the discrete logarithm problem in the divisor class group of a curve over  ¢¡ into a finite field discrete logarithm problem in some extension. The discrete logarithm problem in the divisor class group can therefore be solved as long ¥ as is small. In the elliptic curve case it is known that for supersingular curves one ¥§¦© ¨ has. In this paper curves of higher genus are studied. Bounds on the possible values ¥ for in the case of supersingular curves are given. Ways to ensure that a curve is not supersingular are also given. 1.
482|Evidence that XTR is more secure than supersingular elliptic curve cryptosystems|Abstract. We show that finding an efficiently computable injective homomorphism from the XTR subgroup into the group of points over GF(p 2) of a particular type of supersingular elliptic curve is at least as hard as solving the Diffie-Hellman problem in the XTR subgroup. This provides strong evidence for a negative answer to the question posed by S. Vanstone and A. Menezes at the Crypto 2000 Rump Session on the possibility of efficiently inverting the MOV embedding into the XTR subgroup. As a side result we show that the Decision Diffie-Hellman problem in the group of points on this type of supersingular elliptic curves is efficiently computable, which provides an example of a group where the Decision Diffie-Hellman problem is simple, while the Diffie-Hellman and discrete logarithm problem are presumably not. The cryptanalytical tools we use also lead to cryptographic applications of independent interest. These applications are an improvement of Joux’s one round protocol for tripartite Diffie-Hellman key exchange and a non refutable digital signature scheme that supports escrowable encryption. We also discuss the applicability of our methods to general elliptic curves defined over finite fields. 1
483|Separating Decision Diffie-Hellman from Diffie-Hellman in cryptographic groups|In many cases, the security of a cryptographic scheme based on Diffie-Hellman does in fact rely on the hardness of...
484|Supersingular abelian varieties in cryptology |Abstract. For certain security applications, including identity based encryption and short signature schemes, it is useful to have abelian varieties with security parameters that are neither too small nor too large. Supersingular abelian varieties are natural candidates for these applications. This paper determines exactly which values can occur as the security parameters of supersingular abelian varieties (in terms of the dimension of the abelian variety and the size of the finite field), and gives constructions of supersingular abelian varieties that are optimal for use in cryptography. 1
485|Self-Delegation with Controlled Propagation - or - What If You Lose Your Laptop| We introduce delegation schemes wherein a user may delegate certain rights to himself, but may not safely delegate these rights to others. In our motivating application, a user has a primary (longterm) key that receives some personalized access rights, yet the user may reasonably wish to delegate these rights to new secondary (short-term) keys he creates to use on his laptop when traveling, to avoid having to store his primary secret key on the vulnerable laptop. We propose several cryptographic schemes, both generic ones under general assumptions and more specific practical ones, that fulfill these somewhat conflicting requirements, without relying on special-purpose (e.g., tamper-proof) hardware. This is an extended abstract of our work [19]. 
486|Towards Practical Non-interactive Public Key Cryptosystems Using Non-maximal Imaginary Quadratic Orders|Abstract. We present a new non-interactive public key distribution system based on the class group of a non-maximal imaginary quadratic order Cl(?p). The main advantage of our system over earlier proposals based on (Z/nZ)  * [19,21] is that embedding id information into group elements in a cyclic subgroup of the class group is easy (straight-forward embedding into prime ideals suffices) and secure, since the entire class group is cyclic with very high probability. In order to compute discrete logarithms in the class group, the KGC needs to know the prime factorization of ?p = ?1p 2. We present an algorithm for computing discrete logarithms in Cl(?p) by reducing the problem to computing discrete logarithms in Cl(?1) and either F * p or F * p2. We prove that a similar reduction works for arbitrary non-maximal orders, and that it has polynomial complexity if the factorization of the conductor is known.
487|Conditional oblivious transfer and timed-release encryption|Abstract. We consider the problem of sending messages “into the future.” Previous constructions for this task were either based on heuristic assumptions or did not provide anonymity to the sender of the message. In the public-key setting, we present an efficient and secure timed-release encryption scheme using a “time server ” which inputs the current time into the system. The server has to only interact with the receiver and never learns the sender’s identity. The scheme’s computational and communicational cost per request are only logarithmic in the time parameter. The construction of our scheme is based on a novel cryptographic primitive: a variant of oblivious transfer which we call conditional oblivious transfer. We define this primitive (which may be of independent interest) and show an efficient construction for an instance of this new primitive based on the quadratic residuosity assumption. 1
488|Exokernel: An Operating System Architecture for Application-Level Resource Management|We describe an operating system architecture that securely multiplexes machine resources while permitting an unprecedented degree of application-specific customization of traditional operating system abstractions. By abstracting physical hardware resources, traditional operating systems have significantly limited the performance, flexibility, and functionality of applications. The exokernel architecture removes these limitations by allowing untrusted software to implement traditional operating system abstractions entirely at application-level. We have implemented a prototype exokernel-based system that includes Aegis, an exokernel, and ExOS, an untrusted application-level operating system. Aegis defines the low-level interface to machine resources. Applications can allocate and use machine resources, efficiently handle events, and participate in resource revocation. Measurements show that most primitive Aegis operations are 10–100 times faster than Ultrix,a mature monolithic UNIX operating system. ExOS implements processes, virtual memory, and inter-process communication abstractions entirely within a library. Measurements show that ExOS’s application-level virtual memory and IPC primitives are 5–50 times faster than Ultrix’s primitives. These results demonstrate that the exokernel operating system design is practical and offers an excellent combination of performance and flexibility. 1
489|Active Messages: a Mechanism for Integrated Communication and Computation|The design challenge for large-scale multiprocessors is (1) to minimize communication overhead, (2) allow communication to overlap computation, and (3) coordinate the two without sacrificing processor cost/performance. We show that existing message passing multiprocessors have unnecessarily high communication costs. Research prototypes of message driven machines demonstrate low communication overhead, but poor processor cost/performance. We introduce a simple communication mechanism, Active Messages, show that it is intrinsic to both architectures, allows cost effective use of the hardware, and offers tremendous flexibility. Implementations on nCUBE/2 and CM-5 are described and evaluated using a split-phase shared-memory extension to C, Split-C. We further show that active messages are sufficient to implement the dynamically scheduled languages for which message driven machines were designed. With this mechanism, latency tolerance becomes a programming/compiling concern. Hardware support for active messages is desirable and we outline a range of enhancements to mainstream processors.  
490|Efficient Software-Based Fault Isolation|One way to provide fault isolation among cooperating software modules is to place each in its own address space. However, for tightly-coupled modules, this solution incurs prohibitive context switch overhead. In this paper, we present a software approach to implementing fault isolation within a single address space. Our approach has two parts. First, we load the code and data for a distrusted module into its own fault domain, a logically separate portion of the application&#039;s address space. Second, we modify the object code of a distrusted module to prevent it from writing or jumping to an address outside its fault domain. Both these software operations are portable and programming language independent. Our approach poses a tradeo relative to hardware fault isolation: substantially faster communication between fault domains, at a cost of slightly increased execution time for distrusted modules. We demonstrate that for frequently communicating modules, implementing fault isolation in software rather than hardware can substantially improve end-to-end application performance.
491|Lottery Scheduling: Flexible Proportional-Share Resource Management|This paper presents lottery scheduling, a novel randomized resource allocation mechanism. Lottery scheduling provides efficient, responsive control over the relative execution rates of computations. Such control is beyond the capabilities of conventional schedulers, and is desirable in systems that service requests of varying importance, such as databases, media-based applications, and networks. Lottery scheduling also supports modular resource management by enabling concurrent modules to insulate their resource allocation policies from one another. A currency abstraction is introduced to flexibly name, share, and protect resource rights. We also show that lottery scheduling can be generalized to manage many diverse resources, such as I/O bandwidth, memory, and access to locks. We have implemented a prototype lottery scheduler for the Mach 3.0 microkernel, and found that it provides flexible and responsive control over the relative execution rates of a wide range of applications. The overhead imposed by our unoptimized prototype is comparable to that of the standard Mach timesharing policy. 
492|Lightweight remote procedure call|Lightweight Remote Procedure Call (LRPC) is a communication facility designed and optimized for communication between protection domains on the same machine. In contemporary small-kernel operating systems, existing RPC systems incur an unnecessarily high cost when used for the type of communication that predominates-between protection domains on the same machine. This cost leads system designers to coalesce weakly related subsystems into the same protection domain, trading safety for performance. By reducing the overhead of same-machine communication, LRPC encourages both safety and performance. LRPC combines the control transfer and communication model of capability systems with the programming semantics and large-grained protection model of RPC. LRPC achieves a factor-of-three performance improvement over more traditional approaches based on independent threads exchanging messages, reducing the cost of same-machine communication to nearly the lower bound imposed by conventional hardware. LRPC has been integrated into the Taos operating system of the DEC SRC Firefly multiprocessor workstation.
493|Experiences with the amoeba distributed operating system|The Amoeba distributed operating system has been in development and use for over eight years now. In this paper we describe the present system and our experience with it—what we did right, but also what we did wrong. Among the things done right were basing the system on objects, using a single uniform mechanism (capabilities) for naming and protecting them in a location independent way, and designing a completely new, and very fast file system. Among the things done wrong were having threads not be pre-emptable, initially building our own homebrew window system, and not having a multicast facility at the outset.
494|The Packet Filter: An Efficient Mechanism for User-level Network Code|Code to implement network protocols can be either inside the kernel of an operating system or in user-level processes. Kernel-resident code is hard to develop, debug, and maintain, but user-level implementations typically incur significant overhead and perform poorly.  The performance of user-level network code depends on the mechanism used to demultiplex received packets. Demultiplexing in a user-level process increases the rate of context switches and system calls, resulting in poor performance. Demultiplexing in the kernel eliminates unnecessary overhead.  This paper describes the packet filter, a kernel-resident, protocolindependent packet demultiplexer. Individual user processes have great flexibility in selecting which packets they will receive. Protocol implementations using the packet filter perform quite well, and have been in production use for several years.  
495|Hints for Computer Systems Design|Studying the design and implementation of a number of computer has led to some general hints for system design. They are described here and illustrated by many examples, ranging from hardware such as the Alto and the Dorado to application programs such as Bravo and Star. 1.
496|Overview of the CHORUS Distributed Operating Systems|The CHORUS technology has been designed for building new generations of open, distributed, scalable operating systems. CHORUS has the following main characteristics:  # a communication-based architecture, relying on a minimal Nucleus which integrates distributed processing and communication at the lowest level, and which implements generic services used by a set of subsystem servers to extend standard operating system interfaces. A UNIX subsystem has been developed; other subsystems such as objectoriented systems are planned;  # a real-time Nucleus providing real-time services which are accessible to system programmers;   # a modular architecture providing scalability, and allowing, in particular, dynamic configuration of the system and its applications over a wide range of hardware and network configurations, including parallel and multiprocessor systems. CHORUS-V3 is the current version of the CHORUS Distributed Operating System, developed by Chorus systemes. Earlier versions were st...
497|Virtual Memory Primitives for User Programs|Memory Management Units (MMUs) are traditionally used by operating systems to implement disk-paged virtual memory. Some operating systems allow user programs to specify the protection level (inaccessible, readonly. read-write) of pages, and allow user programs t.o handle protection violations. bur. these mechanisms are not. always robust, efficient, or well-mat. ched to the needs of applications.
498|Improving IPC by kernel design|Inter-process communication (ipc) has to be fast and e ective, otherwise programmers will not use remote procedure calls (RPC), multithreading and multitasking adequately. Thus ipc performance is vital for modern operating systems, especially µ-kernel based ones. Surprisingly, most µ-kernels exhibit poor ipc performance, typically requiring 100 µs for a short message transfer on a modern processor, running with 50 MHz clock rate. In contrast, we achieve 5 µs; a twentyfold improvement. This paper describes the methods and principles used, starting from the architectural design and going down to the coding level. There is no single trick to obtaining this high performance; rather, a synergetic approach in design and implementation on all levels is needed. The methods and their synergy are illustrated by applying them to a concrete example, the L3-kernel (an industrial-quality operating system in daily use at several hundred sites). The main ideas are to guide the complete kernel design by the ipc requirements, and to make heavy use of the concept of virtual address space inside the-kernel itself. As the L3 experiment shows, significant performance gains are possible: compared with Mach, they range from a factor of 22 (8-byte messages) to 3 (4-Kbyte messages). Although hardware specific details in uence both the design and implementation, these techniques are applicable to the whole class of conventional general
499|Experiences with a High-Speed Network Adaptor: A Software Perspective|This paper describes our experiences, from a software perspective, with the OSIRIS network adaptor. It first identifies the problems we encountered while programming OSIRIS and optimizing network performance, and outlines how we either addressed them in the software, or had to modify the hardware. It then describes the opportunities provided by OSIRIS that we were able to exploit in the host operating system (OS); opportunities that suggested techniques for making the OS more effective in delivering network data to application programs. The most novel of these techniques, called application device channels, gives application programs running in user space direct access to the adaptor. The paper concludes with the lessons drawn from this work, which we believe will benefit the designers of future network adaptors. 1 Introduction  With the emergence of high-speed network facilities, several research efforts are focusing on the design and implementation of network adaptors [5, 2, 3, 16, 2...
500|Stride Scheduling: Deterministic Proportional-Share Resource Management|This paper presents stride scheduling, a deterministic scheduling technique that efficiently supports the same flexible resource management abstractions introduced by lottery scheduling. Compared to lottery scheduling, stride scheduling achieves significantly improved accuracy over relative throughput rates, with significantly lower response time variability. Stride scheduling implements proportional-share control over processor time and other resources by cross-applying elements of rate-based flow control algorithms designed for networks. We introduce new techniques to support dynamic changes and higher-level resource management abstractions. We also introduce a novel hierarchical stride scheduling algorithm that achieves better throughput accuracy and lower response time variability than prior schemes. Stride scheduling is evaluated using both simulations and prototypes implemented for the Linux kernel.
501|Implementation and Performance of Application-Controlled File Caching|Traditional file system implementations do not allow applications to control file caching replacement decisions.  We have implemented two-level replacement, a scheme that allows applications to control their own cache replacement, while letting the kernel control the allocation of cache space among processes. We designed an interface to let applications exert control on replacement via a set of directives to the kernel. This is effective and requires low overhead.  We demonstrate that for applications that do not perform well under traditional caching policies, the combination of good application-chosen replacement strategies, and our kernel allocation policy LRU-SP, can reduce the number of block I/Os by up to 80%, and can reduce the elapsed time by up to 45%. We also show that LRU-SP is crucial to the performance improvement for multiple concurrent applications: LRUSP fairly distributes cache blocks and offers protection against foolish applications.   
502|PATHFINDER: A Pattern-Based Packet Classifier|This paper describes a pattern-based approach to building packet classifiers. One novelty of the approach is that it can be implemented efficiently in both software and hardware. A performance study shows that the software implementation is about twice as fast as existing mechanisms, and that the hardware implementation is currently able to keep up with OC-12 (622Mbps) network links and is likely to operate at gigabit speeds in the near future. 1 Introduction A packet classifier is a mechanism that inspects incoming network packets, and based on the values found in select header fields, determines how each is to be processed. A classifier can be thought of as assigning a tag (type) to each packet, or alternatively, identifying the flow or path to which the packet belongs. We call this mechanism a packet classifier rather than a packet filter because it more accurately describes the function being performed---it classifies all packets, rather than filtering out select packets. Packet ...
503|Efficient Packet Demultiplexing for Multiple Endpoints and Large Messages|This paper describes a new packet filter mechanism that efficiently dispatches incoming network packets to one of multiple endpoints, for example address spaces. Earlier packet filter systems iteratively applied each installed filter against every incoming packet, resulting in high processing overhead whenever multiple filters existed. Our new packet filter provides an associative match function that enables similar but not identical filters to be combined together into a single filter. The filter mechanism, which we call the Mach Packet Filter (MPF), has been implemented for the Mach 3.0 operating system and is being used to support endpoint-based protocol processing, whereby each address space implements its own suite of network protocols. With large numbers of registered endpoints, MPF outperforms the earlier BSD Packet Filter (BPF) by over a factor of four. MPF also allows a filter program to dispatch fragmented packets, which was quite difficult with previous filter mechanisms.  
504|Sharing and protection in a single-address-space operating system|This article explores memory sharing and protection support in Opal, a single-address-space operating system designed for wide-address (64-bit) architectures. Opal threads execute within protection domains in a single shared virtual address space. Sharing is simplified, because addresses are context independent. There is no loss of protection, because addressability and access are independent; the right to access a segment is determined by the protection domain in which a thread executes. This model enables beneficial code- and data-sharing patterns that are currently prohibitive, due in part to the inherent restrictions of multiple address spaces, and in part to Unix programming style. We have designed and implemented an Opal prototype using the Mach 3.0 microkernel as a base. Our implementation demonstrates how a single-address-space structure can be supported alongside of other environments on a modern microkernel operating system, using modern wide-address architectures. This article justifies the opal model and its goals for sharing and protection, presents the system and its abstractions, describes the prototype implementation,
505|Threads and Input/Output in the Synthesis kernel|The Synthesis operating system kernel combines several techniques to provide high performance, including kernel code synthesis, ne-grain scheduling, and optimistic synchronization. Kernel code synthesis reduces the execution path for frequently used kernel calls. Optimistic synchronization increases concurrency within the kernel. Their combination results in signi cant performance improvement over traditional operating system implementations. Using hardware and software emulating a SUN 3/160 running SUNOS, Synthesis achieves several times to several dozen times speedup for Unix kernel calls and context switch times of 21 microseconds or faster. 1
506|Limits to Low-Latency Communication on High-Speed Networks|The throughput of local area networks is rapidly increasing. For example, the bandwidth of new ATM networks and FDDI token rings is an order of magnitude greater than that of Ethernets. Other network technologies promise a bandwidth increase of yet another order of magnitude in a few years. However, in distributed systems, lowered latency rather than increased throughput is often of primary concern. This paper examines the system-level effects of newer high-speed network technologies on low-latency, cross-machine communications. To evaluate a number of influences, both hardware and software, we designed and imple-mented a new remote procedure call system targeted at providing low latency. We then ported this system to several hardware platforms (DECstation and SPARCstation) with several differ-ent networks and controllers (ATM, FDDI, and Ethernet). Comparing these systems allows us to explore the performance impact of alternative designs in the communication system with respect to achieving low latency, e.g., the network, the network controller, the host architecture and cache system, and the kernel and user-level runtime software. Our RPC system, which achieves substantially reduced call times (170 pseconds on an ATM network using DECstation 5000/200 hosts), allow us to isolate those components of next-
507|Pilot: An Operating System for a Personal Computer|this paper was presented at the 7th ACM Symposium on Operating Systems Principles, Pacific Grove, Calif., Dec. 10-12, 1979
508|Synthesis: An Efficient Implementation of Fundamental Operating System Services|This dissertation shows that operating systems can provide fundamental services an order of magnitude more efficiently than traditional implementations. It describes the implementation of a new operating system kernel, Synthesis, that achieves this level of performance. The Synthesis kernel combines several new techniques to provide high performance without sacrificing the expressive power or security of the system. The new ideas include: ffl Run-time code synthesis --- a systematic way of creating executable machine code at runtime to optimize frequently-used kernel routines --- queues, buffers, context switchers, interrupt handlers, and system call dispatchers --- for specific situations, greatly reducing their execution time. ffl Fine-grain scheduling --- a new process-scheduling technique based on the idea of feedback that performs frequent scheduling actions and policy adjustments (at submillisecond intervals) resulting in an adaptive, self-tuning system that can support real-ti...
509|Architectural support for translation table management in large address space machines|Virtual memoy page translation tables provide mappings from virtual to physical addresses. When the hardware controlled Tratmlation L.ookaside Buffers (TLBs) do not contain a translation, these tables provide the translation. Approaches to the structure and management of these tables vary from full hardware implementations to complete software based algon”thms. The size of the virtual aaliress space used by processes is rapidly growing beyond 32 bits of address. As the utilized address space increases, new problems and issues surjace. Traditional methoak for managing the page translation tables are inappropriate for large address space architectures. The Hashed Page Table (HPI’), described here, provides a very fast and space ejicient translation table that reduces ovdwad by splitting TLB management responsibilities between hardware and software. Measurements demonstrate its applicability to a diverse range of operating systems and workloads and, in particular, to large virtual address space machines. In simulations of over 4 billion instructions, improvements of 5 to IO % were observed. 1.
510|Design Tradeoffs for Software-Managed TLBs|this paper appeared in the Proceedings of the 20th Annual International Symposium on Computer Architecture, San Diego, May 1993. Authors&#039; address: Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109-2122 This work was supported by Defense Advanced Research Projects Agency under DARPA/ARO Contract Number DAAL03-90-C-0028 and a National Science Foundation Graduate Fellowship.  Uhlig et al. . 2 within the kernel. These and related operating system trends place greater stress upon the TLB by increasing miss rates and, hence, decreasing overall system performance. This paper explores these issues by examining design trade-offs for software-managed TLBs and their impact, in conjunction with various operating systems, on overall system performance. To examine issues which cannot be adequately modeled with simulation, we have developed a system analysis tool called Monster, which enables us to monitor actual systems. We have also developed a novel TLB simulator called Tapeworm, which is compiled directly into the operating system so that it can intercept all TLB misses caused by both user process and OS kernel memory references. The information that Tapeworm extracts from the running system is used to obtain TLB miss counts and to simulate different TLB configurations. The remainder of this paper is organized as follows: Section 2 examines previous TLB and OS research related to this work. Section 3 describes our analysis tools, Monster and Tapeworm. The MIPS R2000 TLB structure and its performance under Ultrix, OSF/1 and Mach 3.0 are explored in Section 4. Hardware- and software-based performance improvements are presented in Section 5. Section 6 summarizes our conclusions. 2 RELATED WORK By caching page table entries, TLBs g...
511|SPIN - an extensible microkernel for application-specific operating system services|Application domains, such as multimedia, databases, and parallel computing, require operating system services with high performance and high functionality. Existing operating systems provide fixed interfaces and implementations to system services and resources. This makes them inappropriate for applications whose resource demands and usage patterns are poorly matched by the services provided. The SPIN operating system enables system services to be defined in an application-specific fashion through an extensible microkernel. It offers fine-grained control over a machine&#039;s logical and physical resources to applications through run-time adaptation of the system to application requirements. 1
512|Hardware and Software Support for Efficient Exception Handling|Program-synchronous exceptions, for example, breakpoints, watchpoints, illegal opcodes, and memory access violations, provide information about exceptional conditions, interrupting the program and vectoring to an operating system handler. Over the last decade, however, programs and run-time systems have increasingly employed these mechanisms as a performance optimization to detect normal and expected conditions. Unfortunately, current architecture and operating system structures are designed for exceptional or erroneous conditions, where performance is of secondary importance, rather than normal conditions. Consequently, this has limited the practicality of such hardware-based detection mechanisms. We propose both hardware and software structures that permit efficient handling of synchronous exceptions by user-level code. We demonstrate a software implementation that reduces exceptiondelivery cost by an order-of-magnitude on current RISC processors, and show the performance benefits o...
513|Fast Mutual Exclusion for Uniprocessors|In this paper we describe restartable atomic sequences, an optimistic mechanism for implementing simple atomic operations (such as Test-And-Set) on a uniprocessor. A thread that is suspended within a restartable atomic sequence is resumed by the operating system at the beginning of the sequence, rather than at the point of suspension. This guarantees that the thread eventually executes the sequence atomically. A restartable atomic sequence has signi cantly less overhead than other software-based synchronization mechanisms, such askernel emulation or software reservation. Consequently, it is an attractive alternative for use on uniprocessors that do not support atomic operations. Even on processors that do support atomic operations in hardware, restartable atomic sequences can have lower overhead. We describe di erent implementations of restartable atomic sequences for the Mach 3.0 and Taos operating systems. These systems &#039; thread management packages
514|Software Prefetching and Caching for Translation Lookaside Buffers|A number of interacting trends in operating system structure, processor architecture, and memory systems are increasing both the rate of translation lookaside buffer (TLB) misses and the cost of servicing a miss. This paper presents two novel software schemes, implemented under Mach 3.0, to decrease both the number and the cost of kernel TLB misses (i.e., misses on kernel data structures, including user page tables). The first scheme is a new use of prefetching for TLB entries on the IPC path, and the second scheme is a new use of software caching of TLB entries for hierarchical page table organizations.  For a range of applications, prefetching decreases the number of kernel TLB misses by 40% to 50%, and caching decreases TLB penalties by providing a fast path for over 90% of the misses. Our caching scheme also decreases the number of nested TLB traps due to the page table hierarchy, reducing the number of kernel TLB miss traps for applications by 20% to 40%. Prefetching and caching, ...
515|Tools for the Development of Application-Specific Virtual Memory Management|While many applications incur few page faults, some scientific and database applications perform poorly when running on top of a traditional virtual memory implementation. To help address this problem, several systems have been built to allow each program the flexibility to use its own application-specific page replacement policy, in place of the generic policy provided by the operating system. This has the potential to improve performance for the class of applications limited by virtual memory behavior; however, to realize this performance gain, application developers must re-implement much of the virtual memory system, a non-trivial programming task. Our goal is to make it easy for programmers to develop new application-specific page replacement policies. To do this, we have implemented (i) an extensible object-oriented user-level virtual memory system and (ii) a graphical performance monitor for virtual memory behavior. Together, these help the user to identify problems with an appl...
517|The Operating System Kernel as a Secure Programmable Machine|To provide modularity and performance, operating system kernels should have only minimal embedded functionality. Today&#039;s operating systems are large, inefficient and, most importantly, inflexible. In our view, most operating system performance and flexibility problems can be eliminated simply by pushing the operating system interface lower. Our goal is to put abstractions traditionally implemented by the kernel out into user-space, where user-level libraries and servers abstract the exposed hardware resources. To achieve this goal, we have defined a new operating system structure, exokernel, that safely exports the resources defined by the underlying hardware. To enable applications to benefit from full hardware functionality and performance, they are allowed to download additions to the supervisor-mode execution environment. To guarantee that these extensions are safe, techniques such as code inspection, inlined cross-domain procedure calls, and secure languages are used. To test and ...
518|An open operating system for a single-user machine|The file system and modularization of a single-user operating system are described. The main points of interest are the openness of the system, which establishes no sharp boundary between itself and the user&#039;s programs, and the techniques used to make the system robust. 1.
519|How to Use a 64-Bit Virtual Address Space|Most operating systems execute programs in private address spaces communicating through messages or files. The traditional private address space model was developed for 16- and 32-bit architectures, on which virtual addresses are a scarce resource. The recent appearance of architectures with flat 64-bit virtual addressing opens an opportunity to reconsider our use of virtual address spaces. In this paper we argue for an alternative addressing model, in which all programs and data reside in a single global virtual address space shared by multiple protection domains. Hardware-based memory protection exists within the single address space, providing firewalls as strong as in conventional systems. We explore the tradeoffs in the use of a global virtual address space relative to the private address space model. We contend that a shared address space can eliminate obstacles to efficient sharing and exchange of data structures that are inherent in private address space systems. The shared add...
