ID|Title|Summary
1|Formalization of the Data Encryption Standard|  In this article we formalize DES (the Data Encryption Standard), that was the most widely used symmetric cryptosystem in the world. DES is a block cipher which was selected by the National Bureau of Standards
2|Functions and their basic properties| The definitions of the mode Function and the graph of a function are introduced. The graph of a function is defined to be identical with the function. The following concepts are also defined: the domain of a function, the range of a function, the identity function, the composition of functions, the 1-1 function, the inverse function, the restriction of a function, the image and the inverse image. Certain basic facts about functions and the notions defined in the article are proved.
3|Functions from a set to a set|function from a set X into a set Y, denoted by “Function of X,Y ”, the set of all functions from a set X into a set Y, denoted by Funcs(X,Y), and the permutation of a set (mode Permutation of X, where X is a set). Theorems and schemes included in the article are reformulations of the theorems of [1] in the new terminology. Also some basic facts about functions of two variables are proved.
4|The ordinal numbers|Summary. We present the choice function rule in the beginning of the article. In the main part of the article we formalize the base of cardinal theory. In the first section we introduce the concept of cardinal numbers and order relations between them. We present here Cantor-Bernstein theorem and other properties of order relation of cardinals. In the second section we show that every set has cardinal number equipotence to it. We introduce notion of alephs and we deal with the concept of finite set. At the end of the article we show two schemes of cardinal induction. Some definitions are based on [9] and [10].
5|The fundamental properties of natural numbers|Summary. Some fundamental properties of addition, multiplication, order relations, exact division, the remainder, divisibility, the least common multiple, the greatest common divisor are presented. A proof of Euclid algorithm is also given. MML Identifier:NAT_1. WWW:http://mizar.org/JFM/Vol1/nat_1.html The articles [4], [6], [1], [2], [5], and [3] provide the notation and terminology for this paper. A natural number is an element of N. For simplicity, we use the following convention: x is a real number, k, l, m, n are natural numbers, h, i, j are natural numbers, and X is a subset of R. The following proposition is true (2) 1 For every X such that 0 ? X and for every x such that x ? X holds x+1 ? X and for every k holds k ? X. Let n, k be natural numbers. Then n+k is a natural number. Let n, k be natural numbers. Note that n+k is natural. In this article we present several logical schemes. The scheme Ind concerns a unary predicate P, and states that: For every natural number k holdsP[k] provided the parameters satisfy the following conditions: • P[0], and • For every natural number k such thatP[k] holdsP[k+1]. The scheme Nat Ind concerns a unary predicateP, and states that: For every natural number k holdsP[k] provided the following conditions are satisfied: • P[0], and • For every natural number k such thatP[k] holdsP[k+1]. Let n, k be natural numbers. Then n · k is a natural number. Let n, k be natural numbers. Observe that n · k is natural. Next we state several propositions: (18) 2 0 = i. (19) If 0 ? = i, then 0 &lt; i. (20) If i = j, then i · h = j · h. 1 The proposition (1) has been removed. 2 The propositions (3)–(17) have been removed.
6|Partial Functions |this article we prove some auxiliary theorems and schemes related to the articles: [1] and [2]. MML Identifier: PARTFUN1.  WWW: http://mizar.org/JFM/Vol1/partfun1.html The articles [4], [6], [3], [5], [7], [8], and [1] provide the notation and terminology for this paper. We adopt the following rules: x, y, y 1 , y 2 , z, z 1 , z 2 denote sets, P , Q, X , X  0  , X 1 , X 2 ,  Y , Y  0  , Y 1 , Y 2 , V , Z denote sets, and C, D denote non empty sets. We now state three propositions: (1) If P ` [: X 1 
7|Binary operations|Summary. In this paper we define binary and unary operations on domains. We also define the following predicates concerning the operations:... is commutative,... is associative,... is the unity of..., and... is distributive wrt.... A number of schemes useful in justifying the existence of the operations are proved. MML Identifier:BINOP_1. WWW:http://mizar.org/JFM/Vol1/binop_1.html The articles [4], [3], [5], [6], [1], and [2] provide the notation and terminology for this paper. Let f be a function and let a, b be sets. The functor f(a, b) yielding a set is defined by: (Def. 1) f(a, b)  = f(<a, b>). In the sequel A is a set. Let A, B be non empty sets, let C be a set, let f be a function from [:A, B:] into C, let a be an element of A, and let b be an element of B. Then f(a, b) is an element of C. The following proposition is true (2) 1 Let A, B, C be non empty sets and f1, f2 be functions from [:A, B:] into C. Suppose that for every element a of A and for every element b of B holds f1(a, b)  = f2(a, b). Then f1 = f2. Let A be a set. A unary operation on A is a function from A into A. A binary operation on A is a
8|Finite Sequences and Tuples of Elements of a Non-empty Sets|this article is the definition of tuples. The element of a set of all sequences of the length n of D is called a tuple of a non-empty set D and it is denoted by element of D
9|Pigeon hole principle|Summary. We introduce the notion of a predicate that states that a function is one-toone at a given element of its domain (i.e. counterimage of image of the element is equal to its singleton). We also introduce some rather technical functors concerning finite sequences: the lowest index of the given element of the range of the finite sequence, the substring preceding (and succeeding) the first occurrence of given element of the range. At the end of the article we prove the pigeon hole principle.
10|Functions and Finite Sequences of Real Numbers|this paper.
11|Algebraic Cryptanalysis of the Data Encryption Standard|In spite of growing importance of AES, the Data Encryption Standard is  by no means obsolete. DES has never been broken from the practical point of view. The triple
12|Differential Cryptanalysis of DES-like Cryptosystems|The Data Encryption Standard (DES) is the best known and most widely used cryptosystem for civilian applications. It was developed at IBM and adopted by the National Buraeu of Standards in the mid 70&#039;s, and has successfully withstood all the attacks published so far in the open literature. In this paper we develop a new type of cryptanalytic attack which can break the reduced variant of DES with eight rounds in a few minutes on a PC and can break any reduced variant of DES (with up to 15 rounds) in less than 2 56 operations. The new attack can be applied to a variety of DES-like substitution/permutation cryptosystems, and demonstrates the crucial role of the (unpublished) design rules. 
13|Algebraic Attacks on Stream Ciphers with Linear Feedback|A classical construction of stream ciphers is to combine several LFSRs and a  highly non-linear Boolean function f . Their security is usually studied in terms of correlation  attacks, that can be seen as solving a system of multivariate linear equations, true  with some probability. At ICISC&#039;02 this approach is extended to systems of higher-degree  multivariate equations, and gives an attack in 2    for Toyocrypt, a Cryptrec submission.
14|Cryptanalysis of block ciphers with overdefined systems of equations|Abstract. Several recently proposed ciphers, for example Rijndael and Serpent, are built with layers of small S-boxes interconnected by linear key-dependent layers. Their security relies on the fact, that the classical methods of cryptanalysis (e.g. linear or differential attacks) are based on probabilistic characteristics, which makes their security grow exponentially with the number of rounds Nr. In this paper we study the security of such ciphers under an additional hypothesis: the S-box can be described by an overdefined system of algebraic equations (true with probability 1). We show that this is true for both Serpent (due to a small size of S-boxes) and Rijndael (due to unexpected algebraic properties). We study general methods known for solving overdefined systems of equations, such as XL from Eurocrypt’00, and show their inefficiency. Then we introduce a new method called XSL that uses the sparsity of the equations and their specific structure. The XSL attack uses only relations true with probability 1, and thus the security does not have to grow exponentially in the number of rounds. XSL has a parameter P, and from our estimations is seems that P should be a constant or grow very slowly with the number of rounds. The XSL attack would then be polynomial (or subexponential) in Nr, with a huge constant that is double-exponential in the size of the S-box. The exact complexity of such attacks is not known due to the redundant equations. Though the presented version of the XSL attack always gives always more than the exhaustive search for Rijndael, it seems to (marginally) break 256-bit Serpent. We suggest a new criterion for design of S-boxes in block ciphers: they should not be describable by a system of polynomial equations that is too small or too overdefined.
15|Efficient Algorithms for Solving Overdefined Systems of Multivariate Polynomial Equations|The security of many recently proposed cryptosystems is based on the difficulty of solving large systems of quadratic multivariate polynomial equations. This problem is NP-hard over any field. When the number of equations m is the same as the number of unknowns n the best known algorithms are exhaustive search for small fields, and a Gröbner base algorithm for large fields. Gröbner base algorithms have large exponential complexity and cannot solve in practice systems with n = 15. Kipnis and Shamir [9] have recently introduced a new algorithm called ”relinearization”. The exact complexity of this algorithm is not known, but for sufficiently overdefined systems it was expected to run in polynomial time. In this paper we analyze the theoretical and practical aspects of relinearization. We ran a large number of experiments for various values of n and m, and analysed which systems of equations were actually solvable. We show that many of the equations generated by relinearization are linearly dependent, and thus relinearization is less efficient that one could expect. We then develop an improved algorithm called XL which is both simpler and more powerful than relinearization. For all 0 &lt; ? = 1/2, and m = ?n 2, XL and relinearization are expected to run in polynomial time of approximately n O(1/ v ?). Moreover, we provide strong evidence that relinearization and XL can solve randomly generated systems of polynomial equations in subexponential time when m exceeds n by a number that increases slowly with n. 
16|Essential algebraic structure within the AES|Abstract. One difficulty in the cryptanalysis of the Advanced Encryption Standard AES is the tension between operations in the two fields GF (2 8) and GF (2). This paper outlines a new approach that avoids this conflict. We define a new block cipher, the BES, that uses only simple algebraic operations in GF (2 8). Yet the AES can be regarded as being identical to the BES with a restricted message space and key space, thus enabling the AES to be realised solely using simple algebraic operations in one field GF (2 8). This permits the exploration of the AES within a broad and rich setting. One consequence is that AES encryption can be described by an extremely sparse overdetermined multivariate quadratic system over GF (2 8), whose solution would recover an AES key.
17|Efficient Methods for Conversion and Solution of Sparse Systems of Low-Degree Multivariate Polynomials over GF(2) via SAT-Solvers   (2007) | The computational hardness of solving large systems of sparse and low-degree multivariate equations is a necessary condition for the security of most modern symmetric cryptographic schemes. Notably, most cryptosystems can be implemented with inexpensive hardware, and have a low gate counts, resulting in a sparse system of equations, which in turn renders such attacks feasible. On one hand, numerous recent papers on the XL algorithm and more sophisticated Gröbner-bases techniques [5, 7, 13, 14] demonstrate that systems of equations are efficiently solvable when they are sufficiently overdetermined or have a hidden internal algebraic structure that implies the existence of some useful algebraic relations. On the other hand, most of this work, as well as most successful algebraic attacks, involve dense, not sparse systems, at least until linearization by XL or a similar algorithm. No polynomial-system-solving algorithm we are aware of, demonstrates that a significant benefit is obtained from the extreme sparsity of some systems of equations. In this paper, we study methods for efficiently converting systems of low-degree sparse multivariate equations into a conjunctive normal form satisfiability (CNF-SAT) problem, for which excellent heuristic algorithms have been developed in recent years. A direct application of this method gives very efficient results: we show that sparse multivariate quadratic systems (especially if over-defined) can be solved much faster than by exhaustive search if ß = 1/100. In particular, our method requires no additional memory beyond that required to store the problem, and so often terminates with an answer for problems that cause Magma and Singular to crash. On the other hand, if Magma or Singular do not crash, then they tend to be faster than our method, but this case includes only the smallest sample problems.
18|Algebraic Attacks on Combiners with Memory and Several Outputs|Abstract. Algebraic attacks on stream ciphers [9] recover the key by solving an overdefined system of multivariate equations. Such attacks can break several interesting cases of LFSR-based stream ciphers, when the output is obtained by a Boolean function, see [9– 11]. Recently this approach has been successfully extended also to combiners with memory, provided the number of memory bits is small, see [1, 11, 2]. In [2] it is shown that, for ciphers built with LFSRs and an arbitrary combiner using a subset of k LFSR state bits, and with l state/memory bits, a polynomial attack always do exist when k and l are fixed. Yet this attack becomes very quickly impractical: already when k and l exceed about 4. In this paper we give a much simpler proof of this result from [2], and prove a more general theorem. We show that much better algebraic attacks exist for ciphers that (in order to be fast) output several bits at a time. In practice our result substantially reduces the complexity of the best attack known on three well known constructions of stream ciphers when the number of outputs is increased. We present attacks on modified versions of Snow, E0 and LILI-128 that are apparently the fastest known. Key Words: LFSR-based stream ciphers, algebraic attacks on stream ciphers, pseudorandom generators, multivariate equations, overdefined problems, linearization, XL algorithm,
19|Logical cryptanalysis as a SAT-problem: Encoding and analysis|Abstract. Cryptographic algorithms play a key role in computer security and the formal analysis of their robustness is of utmost importance. Yet, logic and automated reasoning tools are seldom used in the analysis of a cipher, and thus one cannot often get the desired formal assurance that the cipher is free from unwanted properties that may weaken its strength. In this paper, we claim that one can feasibly encode the low-level properties of state-of-theart cryptographic algorithms as SAT problems and then use efficient automated theorem-proving systems and SAT-solvers for reasoning about them. We call this approach logical cryptanalysis. In this framework, for instance, finding a model for a formula encoding an algorithm is equivalent to finding a key with a cryptanalytic attack. Other important properties, such as cipher integrity or algebraic closure, can also be captured as SAT problems or as quantified boolean formulae. SAT benchmarks based on the encoding of cryptographic algorithms can be used to effectively combine features of “real-world ” problems and randomly generated problems. Here we present a case study on the U.S. Data Encryption Standard (DES) and show how to obtain a manageable encoding of its properties. We have also tested three SAT provers, TABLEAU by Crawford and Auton, SATO by Zhang, and rel-SAT by Bayardo and Schrag, on the encoding of DES, and we discuss the reasons behind their different performance. A discussion of open problems and future research concludes the paper. Key words: cipher verification, Data Encryption Standard, logical cryptanalysis, propositional satisfiability, quantified boolean formulae, SAT benchmarks.
20|Cryptanalysis of Block Ciphers with Probabilistic Non-Linear Relations of Low Degree|Using recent results from coding theory, it is shown how to break block ciphers operating on GF(q) where the ciphertext is expressible as evaluations of an unknown univariate polynomial of low degree m over the plaintext with a typically low but non-negligible probability µ. The method employed is essentially Sudan’s algorithm for decoding Reed-Solomon codes beyond the error-correction diameter. The known plaintext attack needs n=2m/µ^2 plaintext/ciphertext pairs and the running time is polynomial in n. Furthermore, it is shown how to discover more general non-linear relations p(x,y)=0 between plaintext x and ciphertext y that hold with small probability µ. The second attack needs access to n=(2m/µ)^2 plaintext/ciphertext pairs where m =deg(p) and its running time is also polynomial in n. As a demonstration, we break up to 10 rounds of a cipher constructed by Nyberg and Knudsen provably secure against differential and linear cryptanalysis.
21|Using Walk-SAT and Rel-SAT for cryptographic key search|Computer security depends heavily on the strength of cryptographic algorithms. Thus, cryptographic key search is often THE search problem for many governments and corporations. In the recent years, Al search techniques have achieved notable successes in solving &amp;quot;real world&amp;quot; problems. Following a recent result which showed that the properties of the U.S. Data Encryption Standard can be encoded in propositional logic, this paper advocates the use of cryptographic key search as a benchmark for propositional reasoning and search. Benchmarks based on the encoding of cryptographic algorithms optimally share the features of &amp;quot;real world &amp;quot; and random problems. In this paper, two state-of-the-art Al search algorithms, Walk-SAT by Kautz &amp; Selman and Rel-SAT by Bayardo &amp; Schrag, have been tested on the encoding of the Data Encryption Standard, to see whether they are up the task, and we discuss what lesson can be learned from the analysis on this benchmark to improve SAT solvers. New challenges in this field conclude the paper. 1
22|General Principles of Algebraic Attacks and New Design Criteria for Components of Symmetric Ciphers|Abstract. This paper is about the design of multivariate public key schemes, as well as block and stream ciphers, in relation to recent attacks that exploit various types of multivariate algebraic relations. We survey these attacks focusing on their common fundamental principles and on how to avoid them. From this we derive new very general design criteria, applicable for very different cryptographic components. These amount to avoiding (if possible) the existence of, in some sense “too simple” algebraic relations. Though many ciphers that do not satisfy this new paradigm probably still remain secure, the design of ciphers will never be the same again. Key Words: algebraic attacks, polynomial relations, multivariate equations, finite fields, design of cryptographic primitives, generalised linear cryptanalysis, multivariate public key encryption and signature schemes, HFE, Quartz, Sflash, stream ciphers, Boolean functions, combiners with memory, block ciphers, AES, Rijndael, Serpent, elimination methods, Gröbner bases. 1
23|The Inverse S-box, Non-linear Polynomial Relations and Cryptanalysis of Block Ciphers|Abstract. This paper is motivated by the design of AES. We consider a broader question of cryptanalysis of block ciphers having very good non-linearity and diffusion. Can we expect anyway, to attacks such ciphers, clearly designed to render hopeless the main classical attacks? Recently a lot of attention have been drawn to the existence of multivariate algebraic relations for AES (and other) S-boxes. Then, if the XSL-type algebraic attacks on block ciphers [11] are shown to work well, the answer would be positive. In this paper we show that the answer is certainly positive for many other constructions of ciphers. This is not due to an algebraic attack, but to new types of generalised linear cryptanalysis, highly-nonlinear in flavour. We present several constructions of somewhat special practical block ciphers, seemingly satisfying all the design criteria of AES and using similar S-boxes, and yet being extremely weak. They can be generalised, and evolve into general attacks that can be applied- potentially- to any block cipher. Key Words: block ciphers, AES, Rijndael, interpolation attack on block ciphers, fractional transformations, homographic functions, multivariate equations,
24|The Best Differential Characteristics and Subtleties of the Biham-Shamir Attacks on DES|In about every book about cryptography, we learn that the plaintext complexity of differential cryptanalysis on DES is 2^47, as reported by Biham and Shamir in [2]. Yet few people realise that in a typical setting this estimation is not exact and too optimistic. In this note we show...
25|Data Encryption Standard|its strength against attacks The Data Encryption Standard (DES) was developed by an IBM team around 1974 and adopted as a national standard in 1977. Since that time, many cryptanalysts have attempted to find shortcuts for breaking the system. In this paper, we examine one such attempt, the method of differential cryptanalysis, published by Blham and Shamir. We show some of the safeguards against differential cryptanalysis that were built into the system from the beginning, with the result that more than 1015 bytes of chosen plaintext are required for this attack to succeed.
26|Differential Cryptanalysis of the Full 16-round DES|In this paper we develop the first known attack which is capable of breaking the full 16 round DES in less than the 2  55  complexity of exhaustive search. The data analysis phase computes the key by analyzing about 2  36  ciphertexts in 2  37  time. The 2  36  usable ciphertexts are obtained during the data collection phase from a larger pool of 2  47  chosen plaintexts by a simple bit repetition criteria which discards more than 99.9% of the ciphertexts as soon as they are generated. While earlier versions of differential attacks were based on huge counter arrays, the new attack requires negligible memory and can be carried out in parallel on up to 2  33  disconnected processors with linear speedup. In addition, the new attack can be carried out even if the analyzed ciphertexts are derived from up to 2  33  different keys due to frequent key changes during the data collection phase. The attack can be carried out incrementally with any number of available ciphertexts, and its probabil...
27|Novel Data Encryption Algorithm|We always strive to get better algorithms for securing data. A variety of such algorithms are being used in cryptography. Manly block and stream ciphers are available and one of them is International Data Encryption Algorithm (IDEA), which was regarded arguably as one of the best for encryption purposes. A considerable time has elapsed since its advent and this period has witnessed a wide development in process approaches and applications. The number of transactions and exchanges of data has increased exponentially. Consequently, better and novel attacks on data evolved. Researchers believe that the security of the algorithm needs to be improved keeping a check on the time and space complexity. Within this research work we are looking for a robust algorithm known as NDEA which can be applied for securing modern environment applications.
28|Attribute-based encryption for fine-grained access control of encrypted data|As more sensitive data is shared and stored by third-party sites on the Internet, there will be a need to encrypt data stored at these sites. One drawback of encrypting data, is that it can be selectively shared only at a coarse-grained level (i.e., giving another party your private key). We develop a new cryptosystem for fine-grained sharing of encrypted data that we call Key-Policy Attribute-Based Encryption (KP-ABE). In our cryptosystem, ciphertexts are labeled with sets of attributes and private keys are associated with access structures that control which ciphertexts a user is able to decrypt. We demonstrate the applicability of our construction to sharing of audit-log information and broadcast encryption. Our construction supports delegation of private keys which subsumes Hierarchical Identity-Based Encryption (HIBE). E.3 [Data En-
29|How to leak a secret|In this paper we formalize the notion of a ring signature, which makes it possible to specify a set of possible signers without revealing which member actually produced the signature. Unlike group signatures, ring signatures have no group managers, no setup procedures, no revocation procedures, and no coordination: any user can choose any set of possible signers that includes himself, and sign any message by using his secret key and the others ’ public keys, without getting their approval or assistance. Ring signatures provide an elegant way to leak authoritative secrets in an anonymous way, to sign casual email in a way which can only be verified by its intended recipient, and to solve other problems in multiparty computations. The main contribution of this paper is a new construction of such signatures which is unconditionally signer-ambiguous, provably secure in the random oracle model, and exceptionally efficient: adding each ring member increases the cost of signing or verifying by a single modular multiplication and a single symmetric encryption.
30|Random Oracles are Practical: A Paradigm for Designing Efficient Protocols|We argue that the random oracle model -- where all parties have access to a public random oracle -- provides a bridge between cryptographic theory and cryptographic practice. In the paradigm we suggest, a practical protocol P is produced by first devising and proving correct a protocol P  R  for the random oracle model, and then replacing oracle accesses by the computation of an &#034;appropriately chosen&#034; function h. This paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security. We illustrate these gains for problems including encryption, signatures, and zero-knowledge proofs.
32|An identity based encryption scheme based on quadratic residues| We present a novel public key cryptosystem in which the public key of a subscriber can be chosen to be a publicly known value, such as his identity. We discuss the security of the proposed scheme, and show that this is related to the difficulty of solving the quadratic residuosity problem.
33|Hierarchical ID-Based Cryptography|We present hierarchical identity-based encryption schemes and signature schemes that have total collusion resistance on an arbitrary number of levels and that have chosen ciphertext security in the random oracle model assuming the difficulty of the Bilinear Diffie-Hellman problem.
34|Collusion resistant broadcast encryption with short ciphertexts and private keys  |  We describe two new public key broadcast encryption systems for stateless receivers. Both systems are fully secure against any number of colluders. In our first construction both ciphertexts and private keys are of constant size (only two group elements), for any subset of receivers. The public key size in this system is linear in the total number of receivers. Our second system is a generalization of the first that provides a tradeoff between ciphertext size and public key size. For example, we achieve a collusion resistant broadcast system for n users where both ciphertexts and public keys are of size O (vn) for any subset of receivers. We discuss several applications of these systems. 
35|Non-Malleable Non-Interactive Zero Knowledge and Adaptive Chosen-Ciphertext Security|We introduce the notion of non-malleable noninteractive zero-knowledge (NIZK) proof systems. We show how to transform any ordinary NIZK proof system into one that has strong non-malleability properties. We then show that the elegant encryption scheme of Naor and Yung [NY] can be made secure against the strongest form of chosen-ciphertext attack by using a non-malleable NIZK proof instead of a standard NIZK proof.  Our encryption scheme is simple to describe and works in the standard cryptographic model under general assumptions. The encryption scheme can be realized assuming the existence of trapdoor permutations.  1 Introduction  Modern cryptography provides us with several fundamental tools, from encryption schemes to zeroknowledge proofs. For each of these tools, we have some intuition about what they &#034;should&#034; achieve. But we must be careful to understand the gap between our intuition and what we can actually achieve. Indeed, a major goal of cryptography is to refine our tools to br...
36|Generalized secret sharing and monotone functions|Secret Sharing from the perspective of threshold schemes has been well-studied over the past decade. Threshold schemes, however, can only handle a small fraction of the secret sharing functions which we may wish to form. For example, if it is desirable to divide a secret among four participants A,  B:  C, and D in such a way that either A together with B can reconstruct the secret or C together with D can reconstruct the secret, then threshold schemes (even with weighting) are provably insufficient. This paper will present general methods for constructing secret sharing schemes for any given secret sharing function. There is a natural correspon-dence between the set of “generitlized ” secret sharing functions and the set of monotone functions, and tools developed for simplifying the latter set can be applied equally well t o the former set. 1
37|On span programs|We introduce a linear algebraic model of computation, the Span Program, and prove several upper and lower bounds on it. These results yield the following applications in complexity and cryptography: • SL ? ?L (a weak Logspace analogue of N P ? ?P). • The first super-linear size lower bounds on branching programs that count. • A broader class of functions which posses information-theoretic secret sharing schemes. The proof of the main connection, between span programs and counting branching programs, uses a variant of Razborov’s general approximation method. 1
38|Efficient Selective-ID Secure Identity Based Encryption without Random Oracles|We construct two efficient Identity Based Encryption (IBE) systems that are selective identity secure without the random oracle model. Selective identity secure IBE is a slightly weaker security model than the standard security model for IBE. In this model the adversary must commit ahead of time to the identity that it intends to attack, whereas in the standard model the adversary is allowed to choose this identity adaptively. Our first secure IBE system extends to give a selective identity Hierarchical IBE secure without random oracles.
39|Towards hierarchical identity-based encryption|Abstract. We introduce the concept of hierarchical identity-based encryption (HIBE) schemes, give precise definitions of their security and mention some applications. A two-level HIBE (2-HIBE) scheme consists of a root private key generator (PKG), domain PKGs and users, all of which are associated with primitive IDs (PIDs) that are arbitrary strings. A user’s public key consists of their PID and their domain’s PID (in whole called an address). In a regular IBE (which corresponds to a 1-HIBE) scheme, there is only one PKG that distributes private keys to each user (whose public keys are their PID). In a 2-HIBE, users retrieve their private key from their domain PKG. Domain PKGs can compute the private key of any user in their domain, provided they have previously requested their domain secret key from the root PKG (who possesses a master secret). We can go beyond two levels by adding subdomains, subsubdomains, and so on. We present a two-level system with total collusion resistance at the upper (domain) level and partial collusion resistance at the lower (user) level, which has chosen-ciphertext security in the random-oracle model. 1
40|The LSD Broadcast Encryption Scheme|Abstract. Broadcast Encryption schemes enable a center to broadcast encrypted programs so that only designated subsets of users can decrypt each program. The stateless variant of this problem provides each user with a fixed set of keys which is never updated. The best scheme published so far for this problem is the “subset difference ” (SD) technique of Naor Naor and Lotspiech, in which each one of the n users is initially given O(log 2 (n)) symmetric encryption keys. This allows the broadcaster to define at a later stage any subset of up to r users as “revoked”, and to make the program accessible only to their complement by sending O(r) short messages before the encrypted program, and asking each user to perform an O(log(n)) computation. In this paper we describe the “Layered Subset Difference ” (LSD) technique, which achieves the same goal with O(log 1+? (n)) keys, O(r) messages, and O(log(n)) computation. This reduces the number of keys given to each user by almost a square root factor without affecting the other parameters. In addition, we show how to use the same LSD keys in order to address any subset defined by a nested combination of inclusion and exclusion conditions with a number of messages which is proportional to the complexity of the description rather than to the size of the subset. The LSD scheme is truly practical, and makes it possible to broadcast an unlimited number of programs to 256,000,000 possible customers by giving each new customer a smart card with one kilobyte of tamper-resistant memory. It is then possible to address any subset defined by t nested inclusion and exclusion conditions by sending less than 4t short messages, and the scheme remains secure even if all the other users form an adversarial coalition. 1
41|Improved Efficiency for CCA-Secure Cryptosystems Built Using Identity-Based Encryption|Recently, Canetti, Halevi, and Katz showed a general method for constructing CCA-secure encryption schemes from identity-based encryption schemes in the standard model. We improve the efficiency of their construction, and show two specific instantiations of our resulting scheme which offer the most efficient encryption (and, in one case, key generation) of any CCA-secure encryption scheme to date.
42|A Unified Scheme for Resource Protection in Automated Trust Negotiation|Automated trust negotiation is an approach to establishing trust between strangers through iterative disclosure of digital credentials. In automated trust negotiation, access control policies play a key role in protecting resources from unauthorized access. Unlike in traditional trust management systems, the access control policy for a resource is usually unknown to the party requesting access to the resource, when trust negotiation starts. The negotiating parties can rely on policy disclosures to learn each other&#039;s access control requirements. However, a policy itself may also contain sensitive information. Disclosing policies&#039; contents unconditionally may leak valuable business information or jeopardize individuals&#039; privacy. In this paper, we propose UniPro, a unified scheme to model protection of resources, including policies, in trust negotiation. UniPro improves on previous work by modeling policies as first-class resources, protecting them in the same way as other resources, providing fine-grained control over policy disclosure, and clearly distinguishing between policy disclosure and policy satisfaction, which gives users more flexibility in expressing their authorization requirements. We also show that UniPro can be used with practical negotiation strategies without jeopardizing autonomy in the choice of strategy, and present criteria under which negotiations using UniPro are guaranteed to succeed in establishing trust.
43|No Registration Needed: How to Use Declarative Policies and Negotiation to Access Sensitive Resources on the Semantic Web|Gaining access to sensitive resources on the Web usually involves an explicit registration step, where the client has to provide a predetermined set of information to the server. The registration process yields a login/password combination, a cookie, or something similar that can be used to access the sensitive resources. In this paper we show how an explicit registration step can be avoided on the Semantic Web by using appropriate semantic annotations, rule-oriented access control policies, and automated trust negotiation. After presenting the PeerTrust language for policies and trust negotiation, we describe our implementation of implicit registration and authentication that runs under the Java-based MINERVA Prolog engine. The implementation includes a PeerTrust policy applet and evaluator, facilities to import local metadata, policies and credentials, and secure communication channels between all parties.
44|Direct Chosen Ciphertext Security from Identity-Based Techniques|We describe a new encryption technique that is secure in the standard model against adaptive  chosen ciphertext (CCA2) attacks. We base our method on two very e#cient Identity-Based  Encryption (IBE) schemes without random oracles due to Boneh and Boyen, and Waters.
45|Concealing complex policies with hidden credentials|Hidden credentials are useful in protecting sensitive resource requests, resources, policies, and credentials. We propose a significant performance improvement when implementing hidden credentials using Boneh/Franklin Identity Based Encryption. We also propose a substantially improved secret splitting scheme for enforcing complex policies, and show how it improves concealment of policies from nonsatisfying recipients. Categories and Subject Descriptors
46|Access Control Mechanisms for Inter-organizational Workflow|As more businesses engage in globalization, inter-organizational  collaborative computing grows in importance. Since we cannot  expect homogeneous computing environments in participating  organizations, heterogeneity and Internet-based technology are  prevalent in inter-organizational collaborative computing  environments. One technology that provides solutions for data  sharing and work coordination at the global level is interorganizational  workflow. In this paper, we investigate the access  control requirements for inter-organizational workflow. We then  present access control solutions for inter-organizational workflow  based on our implementation. Many of the requirements and  solutions in this paper address the scalability of existing security  solutions, the separation of inter-organizational workflow security  from concrete organization level security enforcement, and the  enforcement of fine-grained access control for inter-organizational workflow.  Keywords  Access control, ...
47|AUTOMATED TRUST NEGOTIATION USING CRYPTOGRAPHIC CREDENTIALS|In automated trust negotiation (ATN), two parties exchange digitally signed credentials that contain attribute information to establish trust and make access control decisions. Because the information in question is often sensitive, credentials are protected according to access control policies. In traditional ATN, credentials are transmitted either in their entirety or not at all. This approach can at times fail unnecessarily, either because a cyclic dependency makes neither negotiator willing to reveal her credential before her opponent, because the opponent must be authorized for all attributes packaged together in a credential to receive any of them, or because it is necessary to fully disclose the attributes, rather than merely proving they satisfy some predicate (such as being over 21 years of age). Recently, several cryptographic credential schemes and associated protocols have been developed to address these and other problems. However, they can be used only as fragments of an ATN process. This paper introduces a framework for ATN in which the diverse credential schemes and protocols can be combined, integrated, and used as needed. A policy language is introduced that enables negotiators to specify authorization requirements that must be met by an opponent to receive various amounts of information about certified attributes and the credentials that contain it. The language also supports the use of uncertified attributes, allowing them to be required as part of policy satisfaction, and to place their (automatic) disclosure under policy control.
48|Fuzzy Identity Based Encryption|We introduce a new type of Identity Based Encryption (IBE) scheme that we call Fuzzy Identity Based Encryption. A Fuzzy IBE scheme allows for a private key for an identity id to decrypt a ciphertext encrypted with another identity id # if and only if the identities id and id # are close to each other as measured by some metric (e.g. Hamming distance). A Fuzzy IBE scheme can be applied to enable encryption using biometric measurements as identities. The error-tolerance of a Fuzzy IBE scheme is precisely what allows for the use of biometric identities, which inherently contain some amount of noise during each measurement.
49|Methods and Limitations of Security Policy Reconciliation|A security policy is a means by which participant session requirements are specified. However, existing frameworks provide limited facilities for the automated reconciliation of participant policies. This paper considers the limits and methods of reconciliation in a general-purpose policy model. We identify an algorithm for efficient two-policy reconciliation, and show that, in the worst-case, reconciliation of three or more policies is intractable. Further, we suggest efficient heuristics for the detection and resolution of intractable reconciliation. Based upon the policy model, we describe the design and implementation of the Ismene policy language. The expressiveness of Ismene, and indirectly of our model, is demonstrated through the representation and exposition of policies supported by existing policy languages. We conclude with brief notes on the integration and enforcement of Ismene policy within the Antigone communication system.
50|ID-Based Encryption for Complex Hierarchies with Applications to Forward Security and Broadcast Encryption|A forward-secure encryption scheme protects secret keys from exposure by evolving the keys with time. Forward security has several unique requirements in hierarchical identity-based encryption (HIBE) scheme: (1) users join dynamically; (2) encryption is joining-time-oblivious; (3) users evolve secret keys autonomously. We present a scalable forward-secure HIBE (fs-HIBE) scheme satisfying the above properties. We also show how our fs-HIBE scheme can be used to construct a forward-secure public-key broadcast encryption scheme, which protects the secrecy of prior transmissions in the broadcast encryption setting. We further generalize fs-HIBE into a collusion-resistant multiple hierarchical ID-based encryption scheme, which can be used for secure communications with entities having multiple roles in role-based access control. The security of our schemes is based on the bilinear Diffie-Hellman assumption in the random oracle model. 1
51|Principles of Policy in Secure Groups|Security policy is increasingly being used as a vehicle for specifying complex entity relationships. When used to define group security, policy must be extended to state the entirety of the security context. For this reason, the policy requirements of secure groups are more complex than found in traditional peer communication; group policies convey information about associations greater and more abstract than their pair-wise counterparts. This paper identifies and illustrates universal requirements of secure group policy and reasons about the adherence of the Group Security Association Key Management Protocol (GSAKMP) to these principles.
53|Data Encryption Techniques for USB |Universal Serial Bus (USB) external devices are high speed flash/external drives which can boast data transfer speeds, about as large as 5Gbps. Host controlled USB is the fastest way of transferring material to an external device available today. It is easy, convenient and fast to access these USB devices as compared to other external devices such as CD, DVD, Floppy drive etc. USB is easy to use as they’re plug and play devices and do not require any additional installation. The other big advantage of USB is that the data transmission is done at the same speeds, irrespective of the size of the data being copied. Thus USB and USB devices have become most popular interface standard for hardware connection. USB devices however lacks in security as any user can access the USB device and the data associated with it, leaving the data at huge potential risk. Thus to make the system more reliable and fast, we propose methods namely, Mutual Encryption and Key Match in order to provide security to the sensitive information on the device as well as to make the transactions or communication done using USB more encrypted.
54|New Directions in Cryptography|Two kinds of contemporary developments in cryptography are examined. Widening applications of teleprocessing have given rise to a need for new types of cryptographic systems, which minimize the need for secure key distribution channels and supply the equivalent of a written signature. This paper suggests ways to solve these currently open problems. It also discusses how the theories of communication and computation are beginning to provide the tools to solve cryptographic problems of long standing.
55|Symmetric and asymmetric encryption|All cryptosystems currently m use are symmetrm m the sense that they require the transmitter and receiver to share, m secret, either the same pmce of reformation (key) or one of a paLr of related keys easdy computed from each other, the key is used m the encryption process to introduce uncertainty to an unauthorized receiver. Not only is an
56|Chaotic Algorithms for Data Encryption|A system of encryption based on chaotic algorithms is described. The system is used for encrypting text and image files for the purpose of creating secure data bases and for sending secure email messages. The system is also implemented on an FPGA for real-time applications. Levels of security several orders of magnitude better than published systems have been achieved.
57|(Data Encryption Standard). |Implementation of cryptographic standards and cryptanalysis using
58|Selective data encryption in outsourced |dynamic environments
59|Executing SQL over Encrypted Data in the Database-Service-Provider Model|Rapid advances in networking and Internet technologies have fueled the emergence of the &#034;software as a service&#034; model for enterprise computing. Successful examples of commercially viable software services include rent-a-spreadsheet, electronic mail services, general storage services, disaster protection services. &#034;Database as a Service&#034; model provides users power to create, store, modify, and retrieve data from anywhere in the world, as long as they have access to the Internet. It introduces several challenges, an important issue being data privacy. It is in this context that we specifically address the issue of data privacy.
60|Providing Database as a Service|In this paper, we explore a new paradigm for data management in which a third party service provider hosts &#034;database as a service&#034; providing its customers seamless mechanisms to create, store, and access their databases at the host site. Such a model alleviates the need for organizations to purchase expensive hardware and software, deal with software upgrades, and hire professionals for administrative and maintenance tasks which are taken over by the service provider. We have developed and deployed a database service on the Internet, called NetDB2, which is in constant use. In a sense, data management model supported by NetDB2 provides an effective mechanism for organizations to purchase data management as a service, thereby freeing them to concentrate on their core businesses. Among the primary challenges introduced by &#034;database as a service&#034; are additional overhead of remote access to data, an infrastructure to guarantee data privacy, and user interface design for such a service. These issues are investigated in the study. We identify data privacy as a particularly vital problem and propose alternative solutions based on data encryption. This paper is meant as a challenges paper for the database community to explore a rich set of research issues that arise in developing such a service.
61|Cryptographic Solution to a Problem of Access Control in a Hierarchical|A scheme based on cryptography is proposed for access control in a system where hierarchy is represented by a partially ordered set (or poset). Straightforward implementation f the scheme requires users highly placed in the hierarchy to store a large number of cryptographic keys. A time-versus-storage trade-off is then described for addressing this key management problem.
62|Balancing Confidentiality and Efficiency In Untrusted Relational DBMSs|The scope and character of today&#039;s computing environments are progressively shifting from traditional, one-on-one clientserver interaction to the new cooperative paradigm. It then becomes of primary importance to provide means of protecting the secrecy of the information, while guaranteeing its availability to legitimate clients. Operating on-line querying services securely on open networks is very difficult; therefore many enterprises outsource their data center operations to external application service providers. A promising direction towards prevention of unauthorized access to outsourced data is represented by encryption. However, data encryption is often supported for the sole purpose of protecting the data in storage and assumes trust in the server, that decrypts data for query execution. In this paper,
63|Key management for multiuser encrypted databases|Database outsourcing is becoming increasingly popular introducing a new paradigm, called database-as-a-service (DAS), where an organization’s database is stored at an external service provider. In such a scenario, access control is a very important issue, especially if the data owner wishes to publish her data for external use. In this paper, we first present our approach for the implementation of access control through selective encryption. The focus of the paper is then the presentation of the experimental results, which demonstrate the applicability of our proposal.
64|Hierarchy-Based Access Control In Distributed Environments|Access control is a fundamental concern in any system that manages resources, e.g., operating systems, file systems, databases and communications systems. The problem we address is how to specify, enforce, and implement access control in distributed environments. This problem occurs in many applications such as management of distributed project resources, e-newspaper and payTV subscription services.
65|P.: Metadata Management in Outsourced Encrypted Databases|Abstract. Database outsourcing is becoming increasingly popular in-troducing a new paradigm, called database-as-a-service, where a client’s database is stored at an external service provider. Outsourcing databases to external providers promises higher availability and more effective dis-aster protection than in-house operations. This scenario presents new research challenges on which the usability of the system is based. In par-ticular, one important aspect is the metadata that must be provided to support the proper working of the system. In this paper, we illustrate the metadata that are needed, at the client and server, to store and retrieve mapping information for processing a query issued by a client application to the server storing the outsourced database. We also present an approach to develop an efficient access control technique and the corresponding metadata needed for its en-forcement. 1
66|Implementation of a Storage Mechanism for Untrusted DBMSs|Several architectures have been recently proposed that store relational data in encrypted form on untrusted relational databases. Such architectures permit the creation of novel Internet services and also offer an opportunity for a better construction of ASP solutions. Environments where there are limited resources that do not permit an efficient management of databases or where it is critical to offer a robust Internet access to private data may all benefit from the above architectures. In this paper we analyze the impact that this architecture has on the typical services of a database. The analysis is based on the experience gained in the construction of a prototype of a complete architecture for the management of encrypted databases. Specifically, we illustrate the impact on query translation and optimization, and the main components of the software architecture of the prototype.
67|Data Encryption in Unreliable Clouds |The aim is to secure the data stored on the third party. For this, the document uploaded on the cloud by the server is encrypted. The client on the other side needs to download the document and decrypt it in order to view the contents. Thus, in cloud computing environment there are many cloud servers, so the data will be not secured due to unreliable network communications. This problem can be solved by RSA algorithm.
69|Achieving secure, scalable, and fine-grained data access control in cloud computing|Abstract—Cloud computing is an emerging computing paradigm in which resources of the computing infrastructure are provided as services over the Internet. As promising as it is, this paradigm also brings forth many new challenges for data security and access control when users outsource sensitive data for sharing on cloud servers, which are not within the same trusted domain as data owners. To keep sensitive user data confidential against untrusted servers, existing solutions usually apply cryptographic methods by disclosing data decryption keys only to authorized users. However, in doing so, these solutions inevitably introduce a heavy computation overhead on the data owner for key distribution and data management when fine-grained data access control is desired, and thus do not scale well. The problem of simultaneously achieving fine-grainedness, scalability, and data confidentiality of access control actually still remains unresolved. This paper addresses this challenging open issue by, on one hand, defining and enforcing access policies based on data attributes, and, on the other hand, allowing the data owner to delegate most of the computation tasks involved in fine-grained data access control to untrusted cloud servers without disclosing the underlying data contents. We achieve this goal by exploiting and uniquely combining techniques of attribute-based encryption (ABE), proxy re-encryption, and lazy re-encryption. Our proposed scheme also has salient properties of user access privilege confidentiality and user secret key accountability. Exten-sive analysis shows that our proposed scheme is highly efficient and provably secure under existing security models. I.
70|Identity-based Encryption with Efficient Revocation|Identity-based encryption (IBE) is an exciting alternative to public-key encryption, as IBE eliminates the need for a Public Key Infrastructure (PKI). Any setting, PKI- or identity-based, must provide a means to revoke users from the system. Efficient revocation is a well-studied problem in the traditional PKI setting. However in the setting of IBE, there has been little work on studying the revocation mechanisms. The most practical solution requires the senders to also use time periods when encrypting, and all the receivers (regardless of whether their keys have been compromised or not) to update their private keys regularly by contacting the trusted authority. We note that this solution does not scale well – as the number of users increases, the work on key updates becomes a bottleneck. We propose an IBE scheme that significantly improves key-update efficiency on the side of the trusted party (from linear to logarithmic in the number of users), while staying efficient for the users. Our scheme builds on the ideas of the Fuzzy IBE primitive and binary tree data structure, and is provably secure.
71|A PASS Scheme in Cloud Computing- Protecting Data Privacy by Authentication and Secret Sharing |Abstract- Cloud computing is an emerging IT service paradigm. Instead of developing their own IT departments, business sectors purchase on-demand IT service from external providers in a per-use basis. From business cost perspective, companies are shifting capital expenses (CapExp) on hardware equipments to operational expenses (OpExp). Many companies, especially those startups, found this cloud IT service is economically beneficial. However, this IT service paradigm still requires to overcome some security concerns before it can be fully deployed. One of the main security issues is how to protect client’s data privacy from cloud employees. This paper proposes a PASS scheme for this purpose using Authentication
72|On Compression of Data Encrypted with Block Ciphers |This paper investigates compression of encrypted data. It has been previously shown that data encrypted with Vernam’s scheme [1], also known as the one-time pad, can be compressed without knowledge of the secret key, therefore this result can be applied to stream ciphers used in practice. However, it was not known how to compress data encrypted with non-stream ciphers. In this paper, we address the problem of compressing data encrypted with block ciphers, such as the Advanced Encryption Standard (AES) used in conjunction with one of the commonly employed chaining modes. We show that such data can be feasibly compressed without knowledge of the key. We present performance results for practical code constructions used to compress binary sources. 1
73|A public key cryptosystem and a signature scheme based on discrete logarithms|Abstract-A new signature scheme is proposed, together with an imple-mentation of the Diffie-Hellman key distribution scheme that achieves a public key cryptosystem. The security of both systems relies on the difficulty of computing discrete logarithms over finite fields. I.
74|A Concrete Security Treatment of Symmetric Encryption|We study notions and schemes for symmetric (ie. private key) encryption in a concrete security framework. We give four di erent notions of security against chosen plaintext attack and analyze the concrete complexity ofreductions among them, providing both upper and lower bounds, and obtaining tight relations. In this way we classify notions (even though polynomially reducible to each other) as stronger or weaker in terms of concrete security. Next we provide concrete security analyses of methods to encrypt using a block cipher, including the most popular encryption method, CBC. We establish tight bounds (meaning
75|Compression of binary sources with side information using low-density parity-check codes|Abstract—We show how low-density parity-check (LDPC) codes can be used to compress close to the Slepian–Wolf limit for cor-related binary sources. Focusing on the asymmetric case of com-pression of an equiprobable memoryless binary source with side information at the decoder, the approach is based on viewing the correlation as a channel and applying the syndrome concept. The encoding and decoding procedures are explained in detail. The per-formance achieved is seen to be better than recently published re-sults using turbo codes and very close to the Slepian–Wolf limit. Index Terms—Channel coding, distributed source coding, LDPC codes, Slepian–Wolf theorem. I.
76|Regular and Irregular Progressive Edge-Growth Tanner Graphs|We propose a general method for constructing Tanner graphs having a large girth by progressively establishing edges or connections between symbol and check nodes in an edge-by-edge manner, called progressive edge-growth (PEG) construction. Lower bounds on the girth of PEG Tanner graphs and on the minimum distance of the resulting low-density parity-check (LDPC) codes are derived in terms of parameters of the graphs. The PEG construction attains essentially the same girth as Gallager&#039;s explicit construction for regular graphs, both of which meet or exceed the Erdos--Sachs bound. Asymptotic analysis of a relaxed version of the PEG construction is presented. We describe an empirical approach using a variant of the &#034;downhill simplex&#034; search algorithm to design irregular PEG graphs for short codes with fewer than a thousand of bits, complementing the design approach of &#034;density evolution&#034; for larger codes. Encoding of LDPC codes based on the PEG construction is also investigated. We show how to exploit the PEG principle to obtain LDPC codes that allow linear time encoding. We also investigate regular and irregular LDPC codes using PEG Tanner graphs but allowing the symbol nodes to take values over GF(q), q &gt; 2. Analysis and simulation demonstrate that one can obtain better performance with increasing field size, which contrasts with previous observations.
77|Guessing and Entropy|It is shown that the average number of successive guesses, E#G#, required with an optimum strategy until one correctly guesses the value of a discrete random X, is underbounded by the entropy H#X# in the manner E#G# # #1=4#2  H#X#  +1 provided that H#X# # 2 bits. This bound is tight within a factor of #4=e# when X is geometrically distributed. It is further shown that E#G# may be arbitrarily large when H#X# is an arbitrarily small positive number so that there is no interesting upper bound on E#G# in terms of H#X#.  I. Introduction  Consider the problem of guessing the value taken on by a discrete random variable X in one trial of a random experiment by asking questions of the form &#034;Did X take on its i-th possible value?&#034; until the answer is &#034;Yes!&#034;. This problem arises for instance when a cryptanalyst must try out possible secret keys one at a time after narrowing the possibilities by some cryptanalysis. Let G be the number of guesses used in the guessing strategy that minimizes E#G#, ...
78|On Compressing Encrypted Data| When it is desired to transmit redundant data over an insecure and bandwidth-constrained channel, it is customary to first compress the data and then encrypt it. In this paper, we investigate the novelty of reversing the order of these steps, i.e., first encrypting and then compressing, without compromising either the compression efficiency or the information-theoretic security. Although counter-intuitive, we show surprisingly that, through the use of coding with side information principles, this reversal of order is indeed possible in some settings of interest without loss of either optimal coding efficiency or perfect secrecy. We show that in certain scenarios our scheme requires no more randomness in the encryption key than the conventional system where compression precedes encryption. In addition to proving the theoretical feasibility of this reversal of operations, we also describe a system which implements compression of encrypted data. 
79|How to compress rabin ciphertexts and signatures (and more  (2004) |Abstract. Ordinarily, RSA and Rabin ciphertexts and signatures are log N bits, where N is a composite modulus; here, we describe how to “compress ” Rabin ciphertexts and signatures (among other things) down to about (2/3) log N bits, while maintaining a tight provable reduction from factoring in the random oracle model. The computational overhead of our compression algorithms is small. We also improve upon Coron’s results regarding partial-domain-hash signature schemes, reducing by over 300 bits the hash output size necessary to prove adequate security. 1
80|On Compressing Encrypted Data without the Encryption Key |When it is desired to transmit redundant data over an insecure  and bandwidth-constrained channel, it is customary to  rst compress  the redundant data and then encrypt it for security reasons. In this  paper, we investigate the novelty of reversing the order of these steps,  i.e.
81|Reversible Data Encryption Algorithm |Abstract:  — IDEA is a Block Cipher working on 64 bit Plain Text Block with 128 bits key. Reversible logic implementation of IDEA implements a novel means of reversible cryptic implementation proposed in this paper. An RDEA algorithm with a REV module in the classical IDEA algorithm with 4 bits Plain Text Block and a 6 bits key is proposed. The proposed algorithm is further tested and analyzed for non-linearity in this paper. This algorithm is proposed to improve the complexity and nonlinearity of classical crypto-circuits.
82|Fully homomorphic encryption using ideal lattices|We propose a fully homomorphic encryption scheme – i.e., a scheme that allows one to evaluate circuits over encrypted data without being able to decrypt. Our solution comes in three steps. First, we provide a general result – that, to construct an encryption scheme that permits evaluation of arbitrary circuits, it suffices to construct an encryption scheme that can evaluate (slightly augmented versions of) its own decryption circuit; we call a scheme that can evaluate its (augmented) decryption circuit bootstrappable. Next, we describe a public key encryption scheme using ideal lattices that is almost bootstrappable. Lattice-based cryptosystems typically have decryption algorithms with low circuit complexity, often dominated by an inner product computation that is in NC1. Also, ideal lattices provide both additive and multiplicative homomorphisms (modulo a public-key ideal in a polynomial ring that is represented as a lattice), as needed to evaluate general circuits. Unfortunately, our initial scheme is not quite bootstrappable – i.e., the depth that the scheme can correctly evaluate can be logarithmic in the lattice dimension, just like the depth of the decryption circuit, but the latter is greater than the former. In the final step, we show how to modify the scheme to reduce the depth of the decryption circuit, and thereby obtain a bootstrappable encryption scheme, without reducing the depth that the scheme can evaluate. Abstractly, we accomplish this by enabling the encrypter to start the decryption process, leaving less work for the decrypter, much like the server leaves less work for the decrypter in a server-aided cryptosystem.
84|Public-key cryptosystems based on composite degree residuosity classes| This paper investigates a novel computational problem, namely the Composite Residuosity Class Problem, and its applications to public-key cryptography. We propose a new trapdoor mechanism and derive from this technique three encryption schemes: a trapdoor permutation and two homomorphic probabilistic encryption schemes computationally comparable to RSA. Our cryptosystems, based on usual modular arithmetics, are provably secure under appropriate assumptions in the standard model.  
85|Factoring polynomials with rational coefficients|In this paper we present a polynomial-time algorithm to solve the following problem: given a non-zero polynomial fe Q[X] in one variable with rational coefficients, find the decomposition of f into irreducible factors in Q[X]. It is well known that this is equivalent to factoring primitive polynomials feZ[X] into irreducible factors in Z[X]. Here we call f ~ Z[X] primitive if the greatest common divisor of its coefficients (the content of f) is 1. Our algorithm performs well in practice, cf. [8]. Its running time, measured in bit operations, is O(nl2+n9(log[fD3). Here f~Tl[X] is the polynomial to be factored, n = deg(f) is the degree of f, and for a polynomial ~ a ~ i with real coefficients a i. i An outline of the algorithm is as follows. First we find, for a suitable small prime number p, a p-adic irreducible factor h of f, to a certain precision. This is done with Berlekamp&#039;s algorithm for factoring polynomials over small finite fields, combined with Hensel&#039;s lemma. Next we look for the irreducible factor h o of f in
86|Bounded-width polynomial-size branching programs recognize exactly those languages|We show that any language recognized by an NC ’ circuit (fan-in 2, depth O(log n)) can be recognized by a width-5 polynomial-size branching program. As any bounded-width polynomial-size branching program can be simulated by an NC ’ circuit, we have that the class of languages recognized by such programs is exactly nonuniform NC’. Further, following
87|Evaluating 2-dnf formulas on ciphertexts|Abstract. Let ? be a 2-DNF formula on boolean variables x1,..., xn ? {0, 1}. We present a homomorphic public key encryption scheme that allows the public evaluation of ? given an encryption of the variables x1,..., xn. In other words, given the encryption of the bits x1,..., xn, anyone can create the encryption of ?(x1,..., xn). More generally, we can evaluate quadratic multi-variate polynomials on ciphertexts provided the resulting value falls within a small set. We present a number of applications of the system: 1. In a database of size n, the total communication in the basic step of the Kushilevitz-Ostrovsky PIR protocol is reduced from v n to 3 v n. 2. An efficient election system based on homomorphic encryption where voters do not need to include non-interactive zero knowledge proofs that their ballots are valid. The election system is proved secure without random oracles but still efficient. 3. A protocol for universally verifiable computation. 1
88|A new public-key cryptosystem as secure as factoring|Abstract. This paper proposes a novel public-key cryptosystem, which is practical, provably secure and has some other interesting properties as follows: 1. Its trapdoor technique is essentially different from any other previous schemes including RSA-Rabin and Diffie-Hellman. 2. It is a probabilistic encryption scheme. 3. It can be proven to be as secure as the intractability of factoring n = p 2 q (in the sense of the security of the whole plaintext) against passive adversaries. 4. It is semantically secure under the p-subgroup assumption, which is comparable to the quadratic residue and higher degree residue assumptions. 5. Under the most practical environment, the encryption and decryp-tion speeds of our scheme are comparable to (around twice slower than) those of elliptic curve cryptosystems. 6. It has a homomorphic property: E(m 0
89|Divertible protocols and atomic proxy cryptography|Abstract. First, we introduce the notion of divertibility as a protocol property as opposed to the existing notion as a language property (see Okamoto, Ohta [OO90]). We give a definition of protocol divertibility that applies to arbitrary 2-party protocols and is compatible with Okamoto and Ohta’s definition in the case of interactive zero-knowledge proofs. Other important examples falling under the newdefinition are blind signature protocols. We propose a sufficiency criterion for divertibility that is satisfied by many existing protocols and which, surprisingly, generalizes to cover several protocols not normally associated with divertibility (e.g., Diffie-Hellman key exchange). Next, we introduce atomic proxy cryptography, in which an atomic proxy function, in conjunction with a public proxy key, converts ciphertexts (messages or signatures) for one key into ciphertexts for another. Proxy keys, once generated, may be made public and proxy functions applied in untrusted environments. We present atomic proxy functions for discrete-log-based encryption, identification, and signature schemes. It is not clear whether atomic proxy functions exist in general for all public-key cryptosystems. Finally, we discuss the relationship between divertibility and proxy cryptography. 1
90|Lossy Trapdoor Functions and Their Applications| We propose a new general primitive called lossy trapdoor functions (lossy TDFs), and realize it under a variety of different number theoretic assumptions, including hardness of the decisional Diffie-Hellman (DDH) problem and the worst-case hardness of standard lattice problems. Using lossy TDFs, we develop a new approach for constructing many important cryptographic primitives, including standard trapdoor functions, CCA-secure cryptosystems, collisionresistant hash functions, and more. All of our constructions are simple, efficient, and black-box. Taken all together, these results resolve some long-standing open problems in cryptography. They give the first known (injective) trapdoor functions based on problems not directly related to integer factorization, and provide the first known CCA-secure cryptosystem based solely on worst-case lattice assumptions.
91|Non-Interactive CryptoComputing for NC1|The area of &#034;computing with encrypted data&#034; has been studied by numerous authors in the past twenty years since it is fundamental to understanding properties of encryption and it has many practical applications. The related fundamental area of &#034;secure function evaluation&#034; has been studied since the mid 80&#039;s. In its basic two-party case, two parties (Alice and Bob) evaluate a known circuit over private inputs (or a private input and a private circuit). Much attention has been paid to the important issue of minimizing rounds of computation in this model. Namely, the number of communication rounds in which Alice and Bob need to engage in to evaluate a circuit on encrypted data securely. Advancements in these areas have been recognized as open problems and have remained open for a number of years. In this paper we give a one round, and thus round optimal, protocol for secure evaluation of circuits which is in polynomialtime for NC
92|A New Public-Key Cryptosystem Based on Higher Residues|This paper describes a new public-key cryptosystem based  on the hardness of computing higher residues modulo a composite RSA  integer. We introduce two versions of our scheme, one deterministic and  the other probabilistic. The deterministic version is practically oriented:  encryption amounts to a single exponentiation w.r.t. a modulus with at  least 768 bits and a 160-bit exponent. Decryption can be suitably optimized  so as to become less demanding than a couple RSA decryptions.
93|Circular-Secure Encryption from Decision Diffie-Hellman|Let E be a public-key encryption system and let (pk i, ski) be public/private key pairs for E for i = 0,..., n. A natural question is whether E remains secure once an adversary obtains an encryption cycle, which consists of the encryption of ski under pk (i mod n)+1 for all i = 1,..., n. Surprisingly, even strong notions of security such as chosen-ciphertext security appear to be insufficient for proving security in these settings. Since encryption cycles come up naturally in several applications, it is desirable to construct systems that remain secure in the presence of such cycles. Until now, all known constructions have only be proved secure in the random oracle model. We construct an encryption system that is circular-secure under the Decision Diffie-Hellman assumption, without relying on random oracles. Our proof of security holds even if the adversary obtains an encryption clique, that is, encryptions of ski under pk j for all 0 = i, j = n. We also construct a circular counterexample: a one-way secure encryption scheme that becomes completely insecure if an encryption cycle of length 2 is published. 1
94|Generalized compact knapsacks, cyclic lattices, and efficient one-way functions|We investigate the average-case complexity of a generalization of the compact knapsack problem to arbitrary rings: given m (random) ring elements a1,..., am ? R and a (random) target value b ? R, find coefficients x1,..., xm ? S (where S is an appropriately chosen subset of R) such that P ai · xi = b. We consider compact versions of the generalized knapsack where the set S is large and the number of weights m is small. Most variants of this problem considered in the past (e.g., when R = Z is the ring of the integers) can be easily solved in polynomial time even in the worst case. We propose a new choice of the ring R and subset S that yields generalized compact knapsacks that are seemingly very hard to solve on the average, even for very small values of m. Namely, we prove that for any unbounded function m = ?(1) with arbitrarily slow growth rate, solving our generalized compact knapsack problems on the average is at least as hard as the worst-case instance of various approximation problems over cyclic lattices. Specific worst-case lattice problems considered in this paper are the shortest independent vector problem SIVP and the guaranteed distance decoding problem GDD (a variant of the closest vector problem, CVP) for approximation factors n 1+o almost linear in the dimension of the lattice. Our results yield very efficient and provably secure one-way functions (based on worst-case complexity assumptions) with key size and time complexity almost linear in the security parameter n. Previous constructions with similar security guarantees required quadratic key size and computation time. Our results can also be formulated as a connection between the worst-case and average-case complexity of various lattice problems over cyclic and quasi-cyclic lattices.
95|Encryption-Scheme Security in the Presence of Key-Dependent Messages|Encryption that is only semantically secure should not be used on messages that depend on the  underlying secret key; all bets are o# when, for example, one encrypts using a shared key K the  value K. Here we introduce a new notion of security, KDM security, appropriate for key-dependent  messages. The notion makes sense in both the public-key and shared-key settings. For the latter we  show that KDM security is easily achievable within the random-oracle model. By developing and  achieving stronger notions of encryption-scheme security it is hoped that protocols which are proven  secure under &#034;formal&#034; models of security can, in time, be safely realized by generically instantiating  their primitives.
96|Efficient collision-resistant hashing from worst-case assumptions on cyclic lattices|Abstract The generalized knapsack function is defined as fa(x)  = Pi ai * xi, where a = (a1,..., am)consists of m elements from some ring R, and x = (x1,..., xm) consists of m coefficients froma specified subset S ` R. Micciancio (FOCS 2002) proposed a specific choice of the ring R andsubset S for which inverting this function (for random a, x) is at least as hard as solving certainworst-case problems on cyclic lattices. We show that for a different choice of S ae R, the generalized knapsack function is in factcollision-resistant, assuming it is infeasible to approximate the shortest vector in n-dimensionalcyclic lattices up to factors ~ O(n). For slightly larger factors, we even get collision-resistancefor any m&gt; = 2. This yields very efficient collision-resistant hash functions having key size andtime complexity almost linear in the security parameter n. We also show that altering S isnecessary, in the sense that Micciancio&#039;s original function is not collision-resistant (nor even universal one-way).Our results exploit an intimate connection between the linear algebra of n-dimensional cycliclattices and the ring Z [ ff]/(ffn- 1), and crucially depend on the factorization of ffn- 1 intoirreducible cyclotomic polynomials. We also establish a new bound on the discrete Gaussian distribution over general lattices, employing techniques introduced by Micciancio and Regev(FOCS 2004) and also used by Micciancio in his study of compact knapsacks. 1 Introduction A function family {fa}a2A is said to be collision-resistant if given a uniformly chosen a 2 A, it is infeasible to find elements x1 6 = x2 so that fa(x1)  = fa(x2). Collision-resistant hash functions are one of the most widely-employed cryptographic primitives. Their applications include integrity checking, user and message authentication, commitment protocols, and more. Many of the applications of collision-resistant hashing tend to invoke the hash function only a small number of times. Thus, the efficiency of the function has a direct effect on the efficiency of the application that uses it. This is in contrast to primitives such as one-way functions, which typically must be invoked many times in their applications (at least when used in a black-box way) [9].
97|QUANTUM ALGORITHMS FOR SOME HIDDEN SHIFT PROBLEMS|Almost all of the most successful quantum algorithms discovered to date exploit the ability of the Fourier transform to recover subgroup structures of functions, especially periodicity. The fact that Fourier transforms can also be used to capture shift structure has received far less attention in the context of quantum computation. In this paper, we present three examples of “unknown shift” problems that can be solved efficiently on a quantum computer using the quantum Fourier transform. For one of these problems, the shifted Legendre symbol problem, we give evidence that the problem is hard to solve classically, by showing a reduction from breaking algebraically homomorphic cryptosystems. We also define the hidden coset problem, which generalizes the hidden shift problem and the hidden subgroup problem. This framework provides a unified way of viewing the ability of the Fourier transform to capture subgroup and shift structure.
98|Generalized compact knapsacks are collision resistant|n.A step in the direction of creating efficient cryptographic functions based on worst-case hardness was
99|Improving Lattice Based Cryptosystems Using the Hermite Normal Form|We describe a simple technique that can be used to substantially reduce the key and ciphertext size of various lattice based cryptosystems and trapdoor functions of the kind proposed by Goldreich, Goldwasser and Halevi (GGH). The improvement is signi cant both from the theoretical and practical point of view, reducing the size of both key and ciphertext by a factor n equal to the dimension of the lattice (i.e., several hundreds for typical values of the security parameter.) The eciency improvement is obtained without decreasing the security of the functions: we formally prove that the new functions are at least as secure as the original ones, and possibly even better as the adversary gets less information in a strong information theoretical sense. The increased eciency of the new cryptosystems allows the use of bigger values for the security parameter, making the functions secure against the best cryptanalytic attacks, while keeping the size of the key even below the smallest key size for which lattice cryptosystems were ever conjectured to be hard to break.
101|Evaluating branching programs on encrypted data|Abstract. We present a public-key encryption scheme with the following properties. Given a branching program P and an encryption c of an input x, it is possible to efficiently compute a succinct ciphertext c ' from which P (x) can be efficiently decoded using the secret key. The size of c ' depends polynomially on the size of x and the length of P, but does not further depend on the size of P. As interesting special cases, one can efficiently evaluate finite automata, decision trees, and OBDDs on encrypted data, where the size of the resulting ciphertext c ' does not depend on the size of the object being evaluated. These are the first general representation models for which such a feasibility result is shown. Our main construction generalizes the approach of Kushilevitz and Ostrovsky (FOCS 1997) for constructing single-server Private Information Retrieval protocols. We also show how to strengthen the above so that c ' does not contain additional information about P (other than P (x) for some x) even if the public key and the ciphertext c are maliciously formed. This yields a two-message secure protocol for evaluating a length-bounded branching program P held by a server on an input x held by a client. A distinctive feature of this protocol is that it hides the size of the server’s input P from the client. In particular, the client’s work is independent of the size of P. 1
102|Security under key-dependent inputs|In this work we re-visit the question of building cryptographic primitives that remain secure even when queried on inputs that depend on the secret key. This was investigated by Black, Rogaway, and Shrimpton in the context of randomized encryption schemes and in the random oracle model. We extend the investigation to deterministic symmetric schemes (such as PRFs and block ciphers) and to the standard model. We term this notion “security against keydependent-input attack”, or KDI-security for short. Our motivation for studying KDI security is the existence of significant real-world implementations of deterministic encryption (in the context of storage encryption) that actually rely on their building blocks to be KDI secure. We consider many natural constructions for PRFs, ciphers, tweakable ciphers and randomized encryption, and examine them with respect to their KDI security. We exhibit inherent limitations of this notion and show many natural constructions that fail to be KDI secure in the standard model, including some schemes that have been proven in the random oracle model. On the positive side, we demonstrate examples where some measure of KDI security can be provably achieved (in particular, we show such examples in the standard model). 1
103|Some baby-step giant-step algorithms for the low hamming weight discrete logarithm problem |Abstract. In this paper, we present several baby-step giant-step algorithms for the low hamming weight discrete logarithm problem. In this version of the discrete log problem, we are required to find a discrete logarithm in a finite group of order approximately 2m, given that the unknown logarithm has a specified number of 1’s, say t, in its binary representation. Heiman and Odlyzko presented the first algorithms for this problem. Unpublished improvements ?  ? ? ? by Coppersmith include a deterministic algorithm with ? complexity m/2 vt ? ?? m/2 O m, and a Las Vegas algorithm with complexity O t/2 t/2 We perform an average-case analysis of Coppersmith’s deterministic algorithm. The average-case complexity achieves only a constant factor speed-up
104|Asymptotically efficient lattice-based digital signatures|We give a direct construction of digital signatures based on the complexity of approximating the shortest vector in ideal (e.g., cyclic) lattices. The construction is provably secure based on the worst-case hardness of approximating the shortest vector in such lattices within a polynomial factor, and it is also asymptotically efficient: the time complexity of the signing and verification algorithms, as well as key and signature size is almost linear (up to poly-logarithmic factors) in the dimension n of the underlying lattice. Since no sub-exponential (in n) time algorithm is known to solve lattice problems in the worst case, even when restricted to cyclic lattices, our construction gives a digital signature scheme with an essentially optimal performance/security trade-off.  
105|Attacks on Protocols for Server-Aided RSA Computation|. On Crypto &#039;88, Matsumoto, Kato, and Imai presented protocols to speed up secret computations with insecure auxiliary devices. The two most important protocols enable a smart card to compute the secret RSA operation faster with the help of a server that is not necessarily trusted by the card holder. It was stated that if RSA is secure, the protocols could only be broken by exhaustive search in certain spaces. Our main attacks show that much smaller search spaces suffice. These attacks are passive and therefore undetectable. It was already known that one of the protocols is vulnerable to active attacks. We show that this holds for the other protocol, too. More importantly, we show that our attack may still work if the smart card checks the correctness of the result; this was previously believed to be an easy measure excluding all active attacks. Finally, we discuss attacks on related protocols. 1 Introduction  1.1 The Model  Smart cards are often considered as appropriate for carrying ...
106|On The Multiplicative Complexity of Boolean Functions over the Basis ...|. The multiplicative complexity c(f) of a Boolean function f is the minimum number of AND gates in a circuit representing f which employs only AND, XOR and NOT gates. A constructive upper bound, c(f) = 2  n  2 +1  \Gamma n=2 \Gamma 2, for any Boolean function f on n  variables (n even) is given. A counting argument gives a lower bound of c(f) = 2  n  2 \Gamma O(n).  Thus we have shown a separation, by an exponential factor, between worst-case Boolean complexity (which is known to be \Theta(2  n  n  \Gamma1  )) and worst-case multiplicative complexity. A construction of circuits for symmetric Boolean functions on n variables, requiring less than  n + 3  p  n AND gates, is described. 1 Introduction. A fair amount of research in Boolean circuit complexity is devoted to the following problem: Given a Boolean function and a supply of gates that perform certain basic operations, construct a circuit which corresponds (in some way) to the function and is optimal (in some sense). A well-studied...
107|Lattices that admit logarithmic worst-case to averagecase connection factors|We demonstrate an average-case problem which is as hard as finding ?(n)-approximate shortest vectors in certain n-dimensional lattices in the worst case, where ?(n)  = O( log n). The previously best known factor for any class of lattices was ?(n)  = O~(n). To obtain our results, we focus on families of lattices having special algebraic structure. Specifically, we consider lattices that correspond to ideals in the ring of integers of an algebraic number field. The worst-case assumption we rely on is that in some `p length, it is hard to find approximate shortest vectors in these lattices, under an appropriate form of preprocessing of the number field. Our results build upon prior works by Micciancio (FOCS 2002), Peikert and Rosen (TCC 2006), and Lyubashevsky and Micciancio (ICALP 2006). For the connection factors ?(n) we achieve, the corresponding decisional promise problems on ideal lattices are not known to be NP-hard; in fact, they are in P. However, the search approximation problems still appear to be very hard. Indeed, ideal lattices are well-studied objects in computational number theory, and the best known algorithms for them seem to perform no better than the best known algorithms for general lattices. To obtain the best possible connection factor, we instantiate our constructions with infinite families of number fields having constant root discriminant. Such families are known to exist and are computable, though no efficient construction is yet known. Our work motivates the search for such constructions. Even constructions of number fields having root discriminant up to O(n2/3-) would yield connection factors better than the current best of O~(n).
108|Minimal-latency secure function evaluation|Abstract. Sander, Young and Yung recently exhibited a protocol for computing on encrypted inputs, for functions computable in NC 1. In their variant of secure function evaluation, Bob (the “CryptoComputer”) accepts homomorphically-encrypted inputs (x) from client Alice, and then returns a string from which Alice can extract f(x, y) (where y is Bob’s input, or e.g. the function f itself). Alice must not learn more about y than what f(x, y) reveals by itself. We extend their result to encompass NLOGSPACE (nondeterministic log-space functions). In the domain of multiparty computations, constant-round protocols have been known for years [BB89,FKN95]. This paper introduces novel parallelization techniques that, coupled with the [SYY99] methods, reduce the constant to 1 with preprocessing. This resolves the conjecture that NLOGSPACE subcomputations (including log-slices of circuit computation) can be evaluated with latency 1 (as opposed to just O(1)). 1
109|The Béguin-Quisquater Server-Aided RSA Protocol from Crypto &#039;95 is not Secure|A well-known cryptographic scenario is the following: a smart card wishes to compute an RSA signature with the help of an untrusted powerful server. Several protocols have been proposed to solve this problem, and many have been broken. There exist two kinds of attacks against such protocols: passive attacks (where the server follows the instructions) and active attacks (where the server may return false values). An open question in this field is the existence of efficient protocols (without expensive precomputations) provably secure against both passive and active attacks. At Crypto &#039;95, B&#039;eguin and Quisquater tried to answer this question by proposing an efficient protocol which was resistant against all known passive and active attacks. In this paper, we present a very effective lattice-based passive attack against this protocol. An implementation is able to recover the secret factorization of an RSA-512 or RSA-768 key in less than 5 minutes once the card has produced about 50 signa...
110|A new approach for algebraically homomorphic encryption|The existence of an efficient and provably secure algebraically homomorphic scheme (AHS), i.e., one that supports both addition and multiplication operations, is a long stated open problem. All proposals so far are either insecure or not provable secure, inefficient, or allow only for one multiplication (and arbitrary additions). As only very limited progress has been made on the existing approaches in the recent years, the question arises whether new methods can lead to more satisfactory solutions. In this paper we show how to construct a provably secure AHS based on a coding theory problem. It allows for arbitrary many additions and for a fixed, but arbitrary number of multiplications and works over arbitrary finite fields. Besides, it possesses some useful properties: i) the plaintext space can be extended adaptively without the need for re-encryption, ii) it operates over arbitrary infinite fields as well, e.g., rational numbers, but the hardness of the underlying decoding problem in such cases is less studied, and iii) depending on the parameter choice, the scheme has inherent error-correcting up to a certain number of transmission errors in the ciphertext. However, since our scheme is symmetric and its ciphertext size grows exponentially with the expected total number of encryptions, its deployment is limited to specific client/server applications with few number of multiplications. Nevertheless, we believe room for improvement due to the huge number of alternative coding schemes that can serve as the underlying hardness problem. For these reasons and because of the interesting properties of our scheme, we believe that using coding theory to design AHS is a promising approach and hope to encourage further investigations.
111|Searching for Elements in Black Box Fields and Applications|We introduce the notion of a black box field and discuss the problem of explicitly exposing field elements given in a black box form. We present several sub-exponential algorithms for this problem using a technique due to Maurer. These algorithms make use of elliptic curves over finite fields in a crucial way. We present three applications for our results: (1) We show that any algebraically homomorphic encryption scheme can be broken in expected sub-exponential time. The existence of such schemes has been open for a number of years. (2) We give an expected sub-exponential time reduction from the problem of finding roots of polynomials over finite fields with low straight line complexity (e.g. sparse polynomials) to the problem of testing whether such polynomials have a root in the field. (3) We show that the hardness of computing discrete-log over elliptic curves implies the security of the Diffie-Hellman protocol over elliptic curves. Finally in the last section of the paper we prove ...
112|Black-Box Extension Fields and the Inexistence of Field-Homomorphic One-Way Permutations |The black-box field (BBF) extraction problem is, for a given field?, to determine a secret field element hidden in a black-box which allows to add and multiply values in?in the box and which reports only equalities of elements in the box. This problem is of cryptographic interest for two reasons. First, for ???Ôit corresponds to the generic reduction of the discrete logarithm problem to the computational Diffie-Hellman problem in a group of prime orderÔ. Second, an efficient solution to the BBF problem proves the inexistence of certain field-homomorphic encryption schemes whose realization is an interesting open problems in algebra-based cryptography. BBFs are also of independent interest in computational algebra. In the previous literature, BBFs had only been considered for the prime field case. In this paper we consider a generalization of the extraction problem to BBFs that are extension fields. More precisely we discuss the representation problem defined as follows: For given generators????????algebraically generating a BBF and an additional elementÜ, all hidden in a black-box, expressÜalgebraically in terms of ????????. We give an efficient algorithm for this representation problem and related problems for fields with small characteristic (e.g.???Òfor someÒ). We also consider extension fields of large characteristic and show how to reduce the representation problem to the extraction problem for the underlying prime field. These results imply the inexistence of field-homomorphic (as opposed to only group-homomorphic, like RSA) one-way permutations for fields of small characteristic.
113|Homomorphic Encryption with CCA Security *|We address the problem of constructing public-key encryption schemes that meaningfully combine useful computability features with non-malleability. In particular, we investigate schemes in which anyone can change an encryption of an unknown message m into an encryption of T (m) (as a feature), for a specific set of allowed functions T, but the scheme is “non-malleable ” with respect to all other operations. We formulate precise definitions that capture these intuitive requirements and also show relationships among our new definitions and other more standard ones (IND-CCA, gCCA, and RCCA). We further justify our definitions by showing their equivalence to a natural formulation of security in the Universally Composable framework. We also consider extending the definitions to features which combine multiple ciphertexts, and show that a natural definition is unattainable for a useful class of features. Finally, we describe a new family of encryption schemes that satisfy our definitions for a wide variety of allowed transformations T,
114|Multi-Round Passive Attacks on Server-Aided RSA Protocols|At Crypto&#039;88, Matsumoto, Kato, and Imai presented two server-aided RSA protocols, RSA-S1 and RSA-S2, which speed up a clients RSA signature generation by interacting with a computationally strong but untrusted server. These protocolls are quite attractive by their efficiency, but unfortunately they are susceptible to multi-round active attacks. Therefore, on Eurocrypt&#039;92, Pfitzmann and Waidner suggested to renew the decomposition of the secret key after each signature generation. In this paper we show that in this case the non-binary version of RSA-S1 becomes totally insecure. Our experiments show that the secret key can be reconstructed very efficiently by lattice reduction using the data obtained by the server during some executions of the protocol. On the other hand we show that if the decomposition of the secret key is slightly modified, our attacks become inefficient. This modification does not significantly affect the efficiency of the protocol. Furthermore, we present a very sim...
115|Additively Homomorphic Encryption with t-Operand Multiplications |Abstract. Homomorphic encryption schemes are an essential ingredient to design protocols where different users interact in order to obtain some information from the others, at the same time that each user keeps private some of his information. When the algebraic structure underlying these protocols is complicated, then standard homomorphic encryption schemes are not enough, because they do not allow to compute at the same time additions and products of plaintexts through the manipulation of ciphertexts. In this work we define a theoretical object, t-chained encryption schemes, which can be used to compute additions and products of t integer values, by ciphertext manipulation. Efficient solutions have been previously proposed only for the case t = 2. Our solution is not only theoretical: we show that some existing IND-CPA secure (pseudo)homomorphic encryption schemes (some of them based on lattices) can be used to implement in practice the concept of t-chained encryption scheme.
116|Data Encryption with Linear Feedback Shift |Abstract — A data encryption technology which ensures secrecy of the data while being transferred over a long distance. It can provide about 80-85% data security as decoding of data involves inverting the feedback function or generating the binary sequence which will help in retrieving the data after some recombination operation. Index Terms — octal word time generation, linear feedback shift register, feedback function, data security, priority encoder, email server, SMTP(simple mail transfer protocol),POP(post office protocol) , device sensitive password check. 1
117|Data Security|The rising abuse of computers and increasing threat to personal privacy through data banks have stimulated much interest m the techmcal safeguards for data. There are four kinds of safeguards, each related to but distract from the others. Access controls regulate which users may enter the system and subsequently whmh data sets an active user may read or wrote. Flow controls regulate the dissemination of values among the data sets accessible to a user. Inference controls protect statistical databases by preventing questioners from deducing confidential information by posing carefully designed sequences of statistical queries and correlating the responses. Statlstmal data banks are much less secure than most people beheve. Data encryption attempts to prevent unauthorized disclosure of confidential information in transit or m storage. This paper describes the general nature of controls of each type, the kinds of problems they can and cannot solve, and their inherent limitations and weaknesses. The paper is intended for a general audience with little background in the area.
118|The Protection of Information in Computer Systems|This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures--whether hardware or software--that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysis of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading. Glossary The following glossary provides, for reference, brief definitions for several terms as used in this paper in the context of protecting information in computers. Access The ability to make use of information stored in a computer system. Used frequently as a verb, to the horror of grammarians. Access control list A list of principals that are authorized to have access to some object. Authenticate To verify the identity of a person (or other agent external to the protection system) making a request.
119|A Lattice Model of Secure Information Flow|This paper investigates mechanisms that guarantee secure information flow in a computer system. These mechanisms are examined within a mathematical framework suitable for formulating the requirements of secure information flow among security classes. The central component of the model is a lattice structure derived from the security classes and justified by the semantics of information flow. The lattice properties permit concise formulations of the security requirements of different existing systems and facilitate the construction of mechanisms that enforce security. The model provides a unifying view of all systems that restrict information flow, enables a classification of them according to security objectives, and suggests some new approaches. It also leads to the construction of automatic program certification mechanisms for verifying the secure flow of information through a program.
120|A Note on the Confinement Problem|This not explores the problem of confining a program during its execution so that it cannot transmit information to any other program except its caller. A set of examples attempts to stake out the boundaries of the problem. Necessary conditions for a solution are stated and informally justified.
121|Certification of Programs for Secure Information Flow|This paper presents a certification mechanism for verifying the secure flow of information through a program. Because it exploits the properties of a lattice structure among security classes, the procedure is sufficiently simple that it can easily be included in the analysis phase of most existing compilers. Appropriate semantics are presented and proved correct. An important application is the confinement problem: The mechanism can prove that a program cannot cause supposedly nonconfidential results to depend on confidential input data.
122|Password Security: A Case History|This paper describes the history of the design of the password security scheme on a  remotely accessed time-sharing system. The present design was the result of countering  observed attempts to penetrate the system. The result is a compromise between extreme  security and ease of use.
123|A hardware architecture for implementing protection rings|Protection of computations and information is an important aspect of a computer utility. In a system which usessegmentation as a memory addressing scheme, protection can be achieved in part by associating concentric rings of decreasing access privilege with a computation. This paper describes hardware processor mechanisms for implementing these rings of protection. The mechanisms allow cross-ring calls and subsequent returns to occur without trapping to the supervisor. Automatic hardware validation of referencesacross ring boundaries is also performed. Thus, a call by a user procedure to a protected subsystem (including the the supervisor) is identical to a call to a companion user procedure. The mechanisms of passing and referencing arguments are the same in both cases as well.
124|The tracker: a threat to statistical database security|The query programs of certain databases report raw statistics for query sets, which are groups of records specified implicitly by a characteristic formula. The raw statistics include query set size and sums of powers of values in the query set. Many users and designers believe that the individual records will remain confidential as long as query programs refuse to report the statistics of query sets which are too small. It is shown that the compromise of small query sets can in fact almost always be accomplished with the help of characteristic formulas called trackers. Schlorer’s individual tracker is reviewed, it is derived from known characteristics of a given individual and permits deducing additional characteristics he may have. The general tracker is introduced: It permits calculating statistics for arbitrary query sets, without requiring preknowledge of anything in the database. General trackers always exist if there are enough distinguishable classes of individuals in the database, in which case the trackers have a simple form. Almost all databases have a general tracker, and general trackers are almost always easy to find. Security is not guaranteed by the lack of a general tracker.
125|Operating Systems|Introduction Early operating systems were control programs a few thousand bytes long that scheduled jobs, drove peripheral devices, and kept track of system usage for billing purposes. Modern operating systems are much larger, ranging from hundreds of thousands of bytes for personal computers (e.g., MS/DOS, Xenix) to tens of millions of bytes for mainframes (e.g., Honeywell&#039;s Multics, IBM&#039;s MVS, AT
126|Third generation computer systems|The common features of third generation operating systems are surveyed from a general view, with emphasis on the common abstractions that constitute at least the basis for a &amp;quot;theory &amp;quot; of operating systems. Properties of specific systems are not discussed except where examples are useful. The technical aspects of issues and concepts are stressed, the nontechnical aspects mentioned only briefly. A perfunctory knowledge of third generation systems is presumed. Key words and phrases: multiprogramming systems, operating systems, supervisory systems, time-sharing systems, programming, storage allocation, memory allocation, processes, concurrency, parallelism, resource allocation, protection CR categories: 1.3, 4.0, 4.30, 6.20 It has been the custom to divide the era of electronic computing into &amp;quot;generations&amp;quot; whose approximate dates are:
127|Practical Techniques for Searches on Encrypted Data|It is desirable to store data on data storage servers such as mail servers and file servers in encrypted form to reduce security and privacy risks. But this usually implies that one has to sacrifice functionality for security. For example, if a client wishes to retrieve only documents containing certain words, it was not previously known how to let the data storage server perform the search and answer the query without loss of data confidentiality.
128|Practice-Oriented Provable-Security|This article is intended to provide some background and tell you about the bigger picture.  the plaintext M to create a ciphertext C, which is transmitted to the receiver. The latter applies
129|Checking Linked Data Structures|In the program checking paradigm, the original program is run on the desired input, and its output is checked by another program called achecker. Recently, the notion of program checking has been extended from its original formulation of checking functions to checking a sequence of operations which query and alter the state of an object external to the program, e.g., checking the interactions between a client and the manager (server) of a data structure. In this expanded paradigm, the checker acts as an intermediary between the client, which generates the requests, and the server, which processes them. The checker is allowed a small amount of reliable memory and may provide a probabilistic guarantee of correctness for the client. We present o-line and on-line checkers for data structures such as linked lists, trees, and graphs. Previously, the only data structures for which such checkers existed were random access memories, stacks, and queues. 
130|A Technique for Data Encryption and Decryption |A day-to-day use of cryptography in our life is increasing tremendously; this is because of necessity of our multimedia documents to be protected from unauthorized person. As the days are passing the old algorithms are not remained so strong as cryptanalyst are familiar with them. Today the computers are faster and in feature its speed will increase more and more. Brute force attacks are made to break the encryption and they are growing so faster. These attacks are the main drawbacks of older algorithm. But with feature this algorithms will be replaced by new techniques that will provide better protection. In this paper we are going to proposed new encryption technique which is more faster, better immune to attacks, more complex, easy to encrypt and many more advanced security feature included. This Document displays the comparison between PSR algorithm and RSA Algorithm which are used in the encryption of plaintext into cipher text that are generally used in cryptography.
131|An Application of a Fast Data Encryption Standard Implementation|ABSTRACT: The Data Encryption Standard is used as the basis for the UNIX password encryption scheme. Some of the security of that scheme depends on the speed of the implementation. This paper presents a mathematical formulation of a fast implementation of the DES in software, discusses how the mathematics can be translated into code, and then analyzes the UNIX password scheme to show how these results can be used to implement it. Experimental results are provided for several computers to show that the given method speeds up the computation of a password by roughly 20 times (depending on the specifrc computer).
132|Data Encryption Using Mrf with an RSA|In a digital multimedia era, the security of multimedia over network transmission becomes a challenging issue. A strategy, combining cryptography with steganography, is investigated to overcome the problems in hand. This paper proposes hiding secret messages, represented as a binary image, by covering a binary random texture synthesized from a 2D Ising Markov random field with an authorized RSA key as the seed. Experiments show that an unauthorized key may never recover the message even it is close to the authorized one. 1
133|Secure spread spectrum watermarking for multimedia| This paper presents a secure (tamper-resistant) algorithm for watermarking images, and a methodology for digital watermarking that may be generalized to audio, video, and multimedia data. We advocate that a watermark should be constructed as an independent and identically distributed (i.i.d.) Gaussian random vector that is imperceptibly inserted in a spread-spectrum-like fashion into the perceptually most significant spectral components of the data. We argue that insertion of a watermark under this regime makes the watermark robust to signal processing operations (such as lossy compression, filtering, digital-analog and analog-digital conversion, requantization, etc.), and common geometric transformations (such as cropping, scaling, translation, and rotation) provided that the original image is available and that it can be succesfully registered against the transformed watermarked image. In these cases, the watermark detector unambiguously identifies the owner. Further, the use of Gaussian noise, ensures strong resilience to multiple-document, or collusional, attacks. Experimental results are provided to support these claims, along with an exposition of pending open problems. 
134|Information Hiding -- A Survey|Information hiding techniques have recently become important in a number of application areas. Digital audio, video, and pictures are increasingly furnished with distinguishing but imperceptible marks, which may contain a hidden copyright notice or serial number or even help to prevent unauthorised copying directly. Military communications systems make increasing use of traffic security techniques which, rather than merely concealing the content of a message using encryption, seek to conceal its sender, its receiver or its very existence. Similar techniques are used in some mobile phone systems and schemes proposed for digital elections. Criminals try to use whatever traffic security properties are provided intentionally or otherwise in the available communications systems, and police forces try to restrict their use. However, many of the techniques proposed in this young and rapidly evolving field can trace their history back to antiquity; and many of them are surprisingly easy to circumvent. In this article, we try to give an overview of the field; of what we know, what works, what does not, and what are the interesting topics for research.
135|Abstract?International Data Encryption Algorithm |(IDEA) is one of the most popular and considered the most secure cryptography algorithm in date since the characteristic of IDEA is suitable for hardware implementation. Here we propose an efficient hardware structure for modulo (2 n +1) multiplier, which is the most time and space consuming operation of IDEA. Compared with the former designs, our design of modulo multiplier saves more time and area cost. For instance, our structure is 9.78 % faster than that proposed by Zimmermann in 1999, while the area reduction of our design also reaches about 6.88%. With the new design, we can implement IDEA on hardware with high performance and low cost. The simulation result gotten from the CPLD system developed by Altera shows the new design has 66Mb/sec encryption/decryption rate under 8.25MHz system clock rate with 4 pipeline stages for each round.
136|Data Encryption Techniques in Multinode Networks |On behalf of a secured system, it has been preferred to make the arrangement of data and keys protected. One can hack the information by perceptive  the information about the radiations of a machine, key length, encryption time, number of stations, block size many more. A small number of algorithms create a replica file which has been generate beside with the encrypted data in order to misguide the hacker and act as outlay. The short extent key is usually exposed to the hacker very simply that’s why huge key lengths have been favored. Meant for the encryption and spread of data on Multinode Network (MN) various techniques are used. This paper covers a variety of techniques and algorithms used for the figures security in MN.
137|A cryptographic key generation scheme for multilevel data security. Computers &amp; Security 9(6  (1990) |In 1982, and Taylor proposed an elegant solution to rhe partially ordered multilevel key distribution problem, using a cryptographic approach. Since then, continuing research has been conducred to try to realize and simplify their scheme. Generally speaking, there are two problems associated with their scheme. First, a large value associated with each security class needs to be made public. Secondly, new security classes are not permitted co be added into the system once all the security keys have been issued. Our paper presents a very simi-lar approach. But, instead of using the top-down design approach as in their scheme, our scheme is using a bottom-up key generating procedure. The result is that the published values for most security classes can be much smaller than in rheir scheme. This property becomes more obvious for a broad and shallow hierarchical graph. In addition, our scheme can accommodate the changes of adding new security classes into the system.
138|AN ORDEAL RANDOMIZED SECURE DATA ENCRYPTION SCHEME (ORSDES)  (2012) |In this paper, we present a new Data encryption scheme named as Ordeal Randomized Secure Data Encryption Scheme (ORSDES). The theoretical security measures are also discussed and ORSDES advocates its competency. Through this paper, we encourage and motivate the user to use DES as ORSDES with more efficiency and security. Mainly we emphasis on secrecy of key because all knows, In cryptography key always have important role. Using a variable pseudo random number and operational function, the new generated key for each block of message make ORSDES more attractive and usable. ORSDES motivates itself the user to use it with new destiny of confidence, integrity and authentication.
139|Pairs and Triplets of DES S-Boxes|This paper describes an investigation of a potential weakness in DES which leads to a statistical property observable in plaintextciphertext pairs and dependent on the key. However, the number of encryptions of known plaintext needed to exploit this property is comparable to the number of encryptions of an exhaustive key search, so the \weakness &amp;quot; is mainly of theoretical interest.
140|An Improvement of Davies&#039; Attack on DES|In this paper we improve Davies&#039; attack [2] on DES to become capable of breaking the full 16-round DES faster than the exhaustive search. Our attack requires 2  50  complexity of the data collection and 2  50  the complexity of analysis. An alternative approach nds 24 key bits of DES with 2  52  known plaintexts and the data analysis requires only several minutes on a SPARC. Therefore, this is the third successful attack on DES, faster than brute force, after dierential cryptanalysis [1] and linear cryptanalysis [5]. We also suggest criteria which make the S-boxes immune to this attack. 1 
141|A Novel Approach for Block-Based Data Encryption |Abstract—With the growth of data communication over the internet, the need for security has obtained utmost importance. Data needs to be kept hidden from all the internet users apart from the authorized ones. Data should be encrypted before sending it through the internet. Here, an algorithm based on Block-Based data encryption is introduced, where the encryption and decryption process is done on binary data, so it will be applicable to all data in the field of computer science.
142|Why Johnny can’t encrypt: A usability evaluation of PGP 5.0|User errors cause or contribute to most computer security failures, yet user interfaces for security still tend to be clumsy, confusing, or near-nonexistent. Is this simply due to a failure to apply standard user interface design techniques to security? We argue that, on the contrary, effective security requires a different usability standard, and that it will not be achieved through the user interface design techniques appropriate to other types of consumer software. To test this hypothesis, we performed a case study of a security program which does have a good user interface by general standards: PGP 5.0. Our case study used a cognitive walkthrough analysis together with a laboratory user test to evaluate whether PGP 5.0 can be successfully used by cryptography novices to achieve effective electronic mail security. The analysis found a number of user interface design flaws that may contribute to security failures, and the user test demonstrated that when our test participants were given 90 minutes in which to sign and encrypt a message using PGP 5.0, the majority of them were unable to do so successfully. We conclude that PGP 5.0 is not usable enough to provide effective security for most computer users, despite its attractive graphical user interface, supporting our hypothesis that user interface design for effective security remains an open problem. We close with a brief description of our continuing work on the development and application of user interface design principles and techniques for security.  
143|Why Cryptosystems Fail|Designers of cryptographic systems are at a disadvantage to most other engineers, in that information on how their systems fail is hard to get: their major users have traditionally been government agencies, which are very secretive about their mistakes. In this article, we present the results of a survey of the failure modes of retail banking systems, which constitute the next largest application of cryptology. It turns out that the threat model commonly used by cryptosystem designers was wrong: most frauds were not caused by cryptanalysis or other technical attacks, but by implementation errors and management failures. This suggests that a paradigm shift is overdue in computer security; we look at some of the alternatives, and see some signs that this shift may begetting under way.
144|Access Control for Collaborative Environments|Access control is an indispensable part of any information sharing system. Collaborative environments introduce new requirements for access control, which cannot be met by using existing models developed for non-collaborative domains. We have developed a new access control model for meeting these requirements. The model is based on a generalized editing model of collaboration, which assumes that users interact with a collaborative application by concurrently editing its data structures. It associates fine-grained data displayed by a collaborative application with a set of collaboration rights and provides programmers and users a multi-dimensional, inheritance-based scheme for specifying these rights. The collaboration rights include traditional read and write rights and several new rights such as viewing rights and coupling rights. The inheritance-based scheme groups subjects, protected objects, and access rights; allows each component of an access specification to refer to both groups...
145|Compliance defects in public key cryptography|Public-key cryptographyhaslowinfrastructural overhead because public-key users bear a substantial but hidden administrative burden. A public-key security system trusts its users to validate each others &#039; public keys rigorously and to manage their own private keys securely. Both tasks are hard to do well, but publickey security systems lack a centralized infrastructure for enforcing users &#039; discipline. A compliance defect in a cryptosystem is such a rule of operation that is both di cult to follow and unenforceable. This paper presents ve compliance defects that are inherent in public-key cryptography ? these defects make publickey cryptography more suitable for server-to-server security than for desktop applications. 1
146|Usability of security: A case study|Human factors are perhaps the greatest current barrier to effective computer security. Most security mechanisms are simply too difficult and confusing for the average computer user to manage correctly. Designing security software that is usable enough to be effective is a specialized problem, and user interface design strategies that are appropriate for other types of software will not be sufficient to solve it. In order to gain insight and better define this problem, we studied the usability of PGP 5.0, which is a public key encryption program mainly intended for email privacy and authentication. We chose PGP 5.0 because it has a good user interface by conventional standards, and we wanted to discover whether that was sufficient to enable non-programmers who know little about security to actually use it effectively. After performing both user testing and a cognitive walkthrough analysis, we conclude that PGP 5.0 is not sufficiently usable to provide effective security for most users. In the course of our study, we developed general principles for evaluating the usability of computer security utilities and systems. This study is of interest not only because of the conclusions that we reach, but also because it can serve as an example of how to evaluate the usability of computer security software. 
147|Privacy Preserving Data Mining|In this paper we address the issue of privacy preserving data mining. Specifically, we consider a  scenario in which two parties owning confidential databases wish to run a data mining algorithm on  the union of their databases, without revealing any unnecessary information. Our work is motivated  by the need to both protect privileged information and enable its use for research or other purposes. The
148|Induction of Decision Trees| The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.  
149|A randomized protocol for signing contracts|Two parties, A and B, want to sign a contract C over a communication network. To do so, they must “simultaneously” exchange their commitments to C. Since simultaneous exchange is usually impossible in practice, protocols are needed to approximate simultaneity by exchanging partial commitments in piece by piece manner. During such a protocol, one party or another may have a slight advantage; a “fair” protocol keeps this advantage within acceptable limits. We present a new protocol that is fair in the sense that, at any stage in its execution, the conditional probability that one party cannot commit both parties to the contract given that the other party can, is close to zero. This is true even if A and B have vastly different computing powers, and is proved under very weak cryptographic assumptions. Our protocol has the following additional properties: 4 during the procedure the parties exchange probadilistic options for committing both parties to the contract; the protocol never terminates in an asymmetric situation where party A knows that party B is committed to the contract while he is not; the protocol makes use of a weak form of a third party (judge). If both A and B are honest, the judge will never be called upon. Otherwise, the judge rules by performing a simple computation. No bookkeeping is required of the judge. 
150|Multiparty unconditionally secure protocols|Under the assumption that each pair of participants em communieatc secretly, we show that any reasonable multiparty protwol can be achieved if at least Q of the Participants am honest. The secrecy achieved is unconditional, It does not rely on any assumption about computational intractability. 1.
151|Security and Composition of Multi-party Cryptographic Protocols|We present general definitions of security for multi-party cryptographic protocols, with focus  on the task of evaluating a probabilistic function of the parties&#039; inputs. We show that, with  respect to these definitions, security is preserved under a natural composition operation.  The definitions follow the general paradigm of known definitions; yet some substantial modifications  and simplifications are introduced. The composition operation is the natural `subroutine  substitution&#039; operation, formalized by Micali and Rogaway.  We consider several standard settings for multi-party protocols, including the cases of eavesdropping,  Byzantine, non-adaptive and adaptive adversaries, as well as the information-theoretic  and the computational models. In particular, in the computational model we provide the first  definition of security of protocols that is shown to be preserved under composition.  
152|Efficient generation of shared RSA keys|We describe efficient techniques for a number of parties to jointly generate an RSA key. At the end of the protocol an RSA modulus N = pq is publicly known. None of the parties know the factorization of N. In addition a public encryption exponent is publicly known and each party holds a share of the private exponent that enables threshold decryption. Our protocols are efficient in computation and communication. All results are presented in the honest but curious settings (passive adversary).
153|Secure Multi-Party Computation|Contents  1 Introduction and Preliminaries 4 1.1 A Tentative Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.1.1 Overview of the Definitions : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.1.2 Overview of the Known Results : : : : : : : : : : : : : : : : : : : : : : : : : : 5 1.1.3 Aims and nature of the current manuscript : : : : : : : : : : : : : : : : : : : 6 1.1.4 Organization of this manuscript : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.2 Preliminaries (also tentative) : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.2.1 Computational complexity : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.2.2 Two-party and multi-party protocols : : : : : : : : : : : : : : : : : : : : : : : 10 1.2.3 Strong Proofs of Knowledge : : : : : : : : : : : : : : : : : : : : : : : : : : : : 10 2 General Two-Party Computation 13 2.1.1 The semi-honest model : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 
154|Two Party RSA Key Generation|. We present a protocol for two parties to generate an RSA  key in a distributed manner. At the end of the protocol the public key: a  modulus N = PQ, and an encryption exponent e are known to both parties.  Individually, neither party obtains information about the decryption  key d and the prime factors of N : P and Q. However, d is shared among  the parties so that threshold decryption is possible.  1 Introduction  We show how two parties can jointly generate RSA public and private keys. Following the execution of our protocol each party learns the public key: N = PQ and e, but does not know the factorization of N or the decryption exponent d. The exponent d is shared among the two players in such a way that joint decryption of cipher-texts is possible.  Generation of RSA keys in a private, distributed manner figures prominently in several cryptographic protocols. An example is threshold cryptography, see [12] for a survey. In a threshold RSA signature scheme there are k parties who ...
155|Oblivious Polynomial Evaluation|Oblivious polynomial evaluation is a protocol involving two parties, a sender whose input is a polynomial P, and a receiver whose input is a value a. At the end of the protocol the receiver learns P (a) and the sender learns nothing. We describe efficient constructions for this protocol, which are based on new intractability assumptions that are closely related to noisy polynomial reconstruction. Oblivious polynomial evaluation can be used as a primitive in many applications. We describe several such applications, including protocols for private comparison of data, for mutually authenticated key exchange based on (possibly weak) passwords, and for anonymous coupons. 1
156|1On Compression of Data Encrypted with Block Ciphers |This paper investigates compression of data encrypted with block ciphers, such as the Advanced Encryption Standard (AES). It is shown that such data can be feasibly compressed without knowledge of the secret key. Block ciphers operating in various chaining modes are considered and it is shown how compression can be achieved without compromising security of the encryption scheme. Further, it is shown that there exists a fundamental limitation to the practical compressibility of block ciphers when no chaining is used between blocks. Some performance results for practical code constructions used to compress binary sources are presented.
157|Combination of Data Masking and Data Encryption for Cloud Database |Keywords:cloud, cloud database, data masking, encryption Abstract.Database as a service in cloud computing enables the user to create, store, modify, and retrieve data over the Internet. The user does not have to install and maintain the database himself. Instead, the database service provider takes responsibility for installing and maintaining the database. Storing data in a cloud database introduces security risks in data confidentiality, data integrity, and privacy. This research aims to design a method for storing data in a cloud database that provides data confidentiality and privacy and allows querying data. The comparison operations and aggregate functions can be performed on the cloud server. The research employs a combination of data masking and data encryption to achieve the objectives.
158|A Symmetric Data Encryption Algorithm for Increasing Security |Abstract — The demand for adequate security to electronic data systems grows high over the decades. As the security issue also impact on the performance analysis of the wireless network, data encryption is necessary for sending and receiving information secretly over the network. Since 1970s, Data Encryption Standard (DES) has received a substantial amount of attention from academic cryptanalysts. However, in 1998 it has been proved insecure and on October 2000, National Institute of Standards and Technology (NIST) announced that the Rijndael algorithm was selected as the Advance Encryption Standard (AES). Our proposed algorithm is based on Rijndael algorithm, which encrypts block of 200 bits using symmetric keys of 200 bits. This algorithm has been implemented on random mobility model and the performance results have been summarized with a conclusion in this paper. (2 8). The basic mathematical concept needed to get the proposed algorithm is given in [3]. III. PROPOSED ALGORITHM As the security is the function of block length and the size of key length, we increase the block length as well as the key length. Our basic block length is 200 bits, which can be shown as a 5 by 5 matrix of byte. We can increase our block by appending a column at a time but here we emphasize on 200 bits.
159|On Applying Molecular Computation To The Data Encryption Standard|this paper we consider the so called plaintext-ciphertext attack. Here the cryptanalyst obtains a plaintext and its corresponding ciphertext and wishes to determine the key used to perform the encryption. The most naive approach to this problem is to try all 2
160|Efficient DES key search|Abstract. Despite recent improvements in analytic techniques for attacking the Data Encryption Standard (DES), exhaustive key search remains the most practical and efficient attack. Key search is becoming alarmingly practical. We show how to build an exhaustive DES key search machine for $1 million that can find a key in 3.5 hours on average. The design for such a machine is described in detail for the purpose of assessing the resistance of DES to an exhaustive attack. This design is based on mature technology to avoid making guesses about future capabilities. With this approach, DES keys can be found one to two orders of magnitude faster than other recently proposed designs. The basic machine design can be adapted to attack the standard DES modes of operation for a small penalty in running time. The issues of development cost and machine reliability are examined as well. In light of this work, it would be prudent in many applications to use DES in a triple-encryption mode. 1.
161|Breaking DES Using a Molecular Computer|Recently Adleman [1] has shown that a small traveling salesman problem can be solved by molecular operations. In this paper we show how the same principles can be applied to breaking the Data Encryption Standard (DES). Our method is based on an encoding technique presented in Lipton [8]. We describe in detail a library of operations which are useful when working with a molecular computer. We estimate that given one arbitrary (plain-text, cipher-text) pair, one can recover the DES key in about 4 months of work. Furthermore, if one is given cipher-text, but the plain text is only known to be one of several candidates then it is still possible to recover the key in about 4 months of work. Finally, under chosen cipher-text attack it is possible to recover the DES key in one day using some preprocessing. 1 Introduction Due to advances in molecular biology it is nowadays possible to create a soup of roughly 10 18  DNA  strands that fits in a small glass of water. Adleman [1] has shown that e...
162|High Speed FPGA Architectures for the Data Encryption Standard|Most modern security standards and security applications are defined to be algorithm independent, that is, they allow a choice from a set of cryptographic algorithms for the same function. Since the Data Encryption Standard (DES) is currently the most widely used private-key encryption algorithm, DES is usually amongst them. Field Programmable Gate Arrays (FPGA) are reconfigurable hardware devices. They can switch algorithms on-thefly. Thus, cryptographic algorithms which are implemented on FPGAs provide an an ideal match for algorithm independent security applications. On FPGAs, cryptographic algorithms can run much faster than on software while preserving the security of traditional hardware solutions. At the same time, FPGAs allow potentially the same flexibility as software does. Although there have been a few previous reports on DES implementations on reconfigurable devices, there has been no systematic treatment of that matter. We designed and implemented various architecture op...
163|Handbook of Applied Cryptography|As we draw near to closing out the twentieth century, we see quite clearly that the information-processing and telecommunications revolutions now underway will continue vigorously into the twenty-first. We interact and transact by directing flocks of digital packets towards each other through cyberspace, carrying love notes, digital cash, and secret corporate documents. Our personal and economic lives rely more and more on our ability to let such ethereal carrier pigeons mediate at a distance what we used to do with face-to-face meetings, paper documents, and a firm handshake. Unfortunately, the technical wizardry enabling remote collaborations is founded on broadcasting everything as sequences of zeros and ones that one&#039;s own dog wouldn&#039;t recognize. What is to distinguish a digital dollar when it is as easily reproducible as the spoken word? How do we converse privately when every syllable is bounced off a satellite and smeared over an entire continent? How should a bank know that it really is Bill Gates requesting from his laptop in Fiji a transfer of $10,000,000,000 to another bank? Fortunately, the magical mathematics of cryptography can help. Cryptography provides techniques for keeping information secret, for determining that information
164|A High-speed DES Implementation for Network Applications|This paper describes a high-speed data encryption chip implementing the Data Encryption Standard (DES). The DES implementation supports Electronic Code Book mode and Cipher Block Chaining mode. The chip is based on a gallium arsenide (GaAs) gate array containing 50K transistors. At a clock frequency of 250 MHz, data can be encrypted or decrypted at a rate of 1 GBit/second, making this the fastest single-chip implementation reported to date. High performance and high density have been achieved by using customdesigned circuits to implement the core of the DES algorithm. These circuits employ precharged logic, a methodology novel to the design of GaAs devices. A pipelined flow-through architecture and an efficient key exchange mechanism make this chip suitable for low-latency network controllers.  iv  Contents  1 Introduction 1 2 DES Algorithm 1 3 GaAs Gate Array 4 4 DES Chip Implementation 6  4.1 Organization : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :...
165|A First Generation DPGA Implementation|Dynamically Programmable Gate Arrays (DPGAs) represent a hybrid architecture lying between traditional FPGAs and SIMD arrays. Notably, these arrays can efficiently support computations where the function of the array elements needs to vary both among array elements during any single cycle and within any single array element over time. We describe our minimal, first generation DPGA. This DPGA uses traditional 4-LUTs for the basic array element, but backs LUT and interconnect programming cells with a 4-context memory implemented using dynamic RAM. Additionally, this DPGA supports non-intrusive background loads of non-active contexts and automatic refresh for the dynamic memory cells. We draw several lessons from this design experience which may be relevant to future DPGA and FPGA designs. 1 Introduction  Traditional Field-Programmable Gate Arrays (FPGAs) have a set of programmable elements which can be configured to personalize the FPGA to imAcknowledgments:  This research is supported b...
166|Exploiting Parallelism in Hardware Implementation of the DES|The Data Encryption Standard algorithm has features which may be used to advantage in parallelizing an implementation. The kernel of the algorithm, a single round, may be decomposed into several parallel computations resulting in a structure with minimal delay. These rounds may also be computed in a pipelined parallel structure for operations modes which do not require cryptext feedback. Finally, system I/O may be performed in parallel with the encryption computation for further gain. Although several of these ideas have been discussed before separately, the composite presentation is novel. 1 Introduction 1  The Data Encryption Standard (DES) is probably the most widely used publicly available secret-key algorithm. Since its introduction by the National Bureau of Standards (NBS) in 1977[FIPS46], DES implementations have improved greatly in encryption rate. Yet, typical computer communication rates have also increased significantly during the same period. Today&#039;s high-performance comput...
167|Security and Performance Optimization of a new DES Data Encryption Chip|Abstract — Cryptograpfdcaf applications demand both high speed and high security. This paper presents the implementation of a new high-per-formance Data Encryption Standard (DES) data encryption chip. It is the result of close cooperation between cryptographers and chip designers. At the system design level, cryptograpfdcal optimization and equivalence transformations lead to a very efficient floor plan with minimal routing, which otherwise wonld present a serious problem for data scrambling algorithms. These optfmizations, which do not compromise the DES algorithm nor the security, are combined with a highly structured design and layout strategy. Novel CAD tools are nsed at different steps in the design process. The resnft is a single chip of 25 mmz in 3- pm double-metal CMOS, Functionality tests show that a clock of 16.7 MHz can be applied, which means that a 32-Mbk/s data rate can be achieved for all eight byte modes. This is the fastest DES chip reported yet, aflowing equally fast execution of all four DES modes of operation due to an originaf pipeline architecture. 1.
168|An ASCII value based text data encryption |Abstract- Encryption is a process of generating secrect text from the input text using a secret key and a encryption algorithm. Input text is refered to as plain text and the secret text generated is known as cipher text. Encryption algorithms are mainly divided into two categories which are symmetric key encryption algorithm and asymmetric key encryption algorithm. In Symmetric key encryption algorithm the same key is used by both sender and receiver but in asymmetric key algorithm sender and receiver both uses the different keys. In this paper, we present a technique based on symmetric key encryption algorithm which uses ASCII vales of input text to encrypt the data. Text data encryption techniques are very useful in data communication where one user want to send some secret messages to another users.
169|Broadcast Encryption|We introduce new theoretical measures for the qualitative and quantitative assessment of encryption schemes designed for broadcast transmissions. The goal is to allow a central broadcast site to broadcast secure transmissions to an arbitrary set of recipients while minimizing key management related transmissions. We present several schemes that allow a center to broadcast a secret to any subset of privileged users out of a universe of size n so that coalitions of k users not in the privileged set cannot learn the secret. The most interesting scheme requires every user to store O(k log k log n) keys and the center to broadcast O(k  2  log  2  k log n) messages regardless of the size of the privileged set. This scheme is resilient to any coalition of k users. We also present a scheme that is resilient with probability p against a random subset of k users. This scheme requires every user to store O(log k log(1=p)) keys and the center to broadcast O(k log  2  k log(1=p)) messages.    Prel...
171|An Effective Approach for Data encryption technique based on |Abstract- The data encryption using digital signature technique is essential for secure transactions over open networks. It is used in a variety of applications to ensure the integrity of data exchanged or stored and to prove to the recipient the originator&#039;s identity. Digital signature schemes are mostly used in cryptographic protocols to provide services like entity authentication, authenticated key transport and authenticated key agreement. This architecture is related with secure Hash Function and cryptographic algorithm. There are many other algorithms which are based on the hybrid combination of prime factorization and discrete logarithms, but different weaknesses and attacks have been developed against those algorithms. This Research paper presents a new variant of digital signature algorithm which is based on linear block cipher or Hill cipher initiate with Asymmetric algorithm using mod 37.
172|Electronic Signature System with Small Number of Private Keys|We propose a simple server-based electronic signature system in which a small number of common private keys are used. The motivation of such a system is to escape the scalability and complexity problems that arise if a large-scale Public Key Infrastructure (PKI) is used. We argue that the assumption of personal private keys is the main reason for those problems and high cost of electronic signature systems. We conclude that elimination of personal private keys is justified and further argue that this does not reduce the security.
173|Encrypted Key Exchange: Password-Based Protocols Secure Against Dictionary Attacks|Classical cryptographic protocols based on user-chosen keys allow an attacker to mount password-guessing attacks. We introduce a novel combination of asymmetric (public-key) and symmetric (secret-key) cryptography that allow two parties sharing a common password to exchange confidential and authenticated information over an insecure network. These protocols are secure against active attacks, and have the property that the password is protected against off-line &#034;dictionary&#034; attacks. There are a number of other useful applications as well, including secure public telephones.
174|Kerberos: An Authentication Service for Open Network Systems|In an open network computing environment, a workstation cannot be trusted to identify its users correctly to network services.  Kerberos provides an alternative approach whereby a trusted third-party authentication service is used to verify users’ identities. This paper gives an overview of the Kerberos authentication model as implemented for MIT’s Project Athena. It describes the protocols used by clients, servers, and Kerberos to achieve authentication. It also describes the management and replication of the database required.   The views of Kerberos as seen by the user, programmer, and administrator are described.  Finally, the role of Kerberos in the larger Athena picture is given, along with a list of applications that presently use Kerberos for user authentication.  We describe the addition of Kerberos authentication to the Sun Network File System as a case study for integrating Kerberos with an existing application.
176|Discrete Logarithms in Finite Fields and Their Cryptographic Significance|Given a primitive element g of a finite field GF(q), the discrete logarithm of a nonzero element  u  GF(q) is that integer k, 1  k  q - 1, for which u = g  k  . The well-known problem of computing discrete logarithms in finite fields has acquired additional importance in recent years due to its applicability in cryptography. Several cryptographic systems would become insecure if an efficient discrete logarithm algorithm were discovered. This paper surveys and analyzes known algorithms in this area, with special attention devoted to algorithms for the fields GF(2  n  ). It appears that in order to be safe from attacks using these algorithms, the value of n for which GF(2  n  ) is used in a cryptosystem has to be very large and carefully chosen. Due in large part to recent discoveries, discrete logarithms in fields GF(2  n  ) are much easier to compute than in fields GF(p) with p prime. Hence the fields GF(2  n  ) ought to be avoided in all cryptographic applications. On the other hand, ...
177|The RC5 Encryption Algorithm|Abstract. This document describes the RC5 encryption algorithm. RC5 is a fast symmetric block cipher suitable for hardware or software implementations. A novel feature of RC5 is the heavy use of data-dependent rotations. RC5 has a variable word size, a variable number of rounds, and a variable-length secret key. 1 AParameterized Family of Encryption Algorithms RC5 is word-oriented: all of the primitive operations work on w-bit words as their basic unit of information. Here we assume w = 32, although the formal speci cation of RC5 admits variants for other word lengths, such asw = 64 bits. RC5 has two-word (64-bit) input (plaintext) and output (ciphertext) block sizes. RC5 uses an \expanded key table, &#034; S, derived from the user&#039;s supplied secret key. The size t of table S depends on the number r of rounds: S has t =2(r +1) words. There are thus several distinct \RC5 &#034; algorithms, depending on the choice of parameters w and r. We summarize these parameters below: w This is the word size, in bits ? each word contains u =(w=8) 8-bit bytes. The standard value of w is 32 bits ? allowable values of w are 16, 32, and 64. RC5 encrypts two-word blocks: plaintext and ciphertext blocks are each 2w bits long. r This is the number of rounds. Also, the expanded key table S contains t =2(r +1)words. Allowable values of r are 0, 1,..., 255. In addition to w and r, RC5 has a variable-length secret cryptographic key, speci ed parameters b and K: b The number of bytes in the secret key K. Allowable values of b are 0, 1,
178|Ant-Crypto, a Cryptographer for Data Encryption Standard |Swarm Intelligence and Evolutionary Techniques are attracting the cryptanalysts in the field of cryptography. This paper presents a novel swarm based attack called Ant-Crypto (Ant-Cryptographer) for the cryptanalysis of Data Encryption Standard (DES). Ant-Crypto is based on Binary Ant Colony Optimization (BACO) i.e. a binary search space based directed graph is modeled for efficiently searching the optimum result (an original encryption key, in our case). The reason that why evolutionary techniques are becoming attractive is because of the inapplicability of traditional techniques and brute force attacks against feistel ciphers due to their inherent structure based on high nonlinearity and low autocorrelation. Ant-Crypto uses a known-plaintext attack to recover the secret key of DES which is required to break / decipher the secret messages. Ant-Crypto iteratively searches for the secret key while generating several candidate optimum keys that are guessed across different runs on the basis of routes completed by ants. These optimum keys are then used to find each individual bit of the 56 bit secret key used during encryption by DES. Ant-Crypto is compared with some other state of the art evolutionary based attacks i.e. Genetic Algorithm and Comprehensive Binary Particle Swarm Optimization. The experimental results show that Ant-Crypto is an effective evolutionary attack against DES and can deduce large number of valuable bits as compared to other evolutionary algorithms; both in terms of time and space complexity.
179|Solving Symmetric and Asymmetric TSPs by Ant Colonies|In this paper we present ACS, a distributed  algorithm for the solution of combinatorial optimization  problems which was inspired by the observation of real  colonies of ants. We apply ACS to both symmetric and  asymmetric traveling salesman problems. Results show  that ACS is able to find good solutions to these problems.  I. Introduction  In this paper we present Ant Colony System (ACS), a novel distributed approach to combinatorial optimization based on the observation of real ant colonies behavior. ACS finds its ground in one of the authors previous work on the so-called Ant System (AS) [1],[2],[5],[7] and in Ant-Q [8] an extension of AS with Q-learning [12], a reinforcement learning technique. In particular, ACS is a revisited version of Ant-Q where a different way to update the experience accumulated by the artificial ants has been introduced [6].  All the mentioned systems belong to the Artificial Ant Colonies (AAC) family of algorithms that has been applied to various combinat...
180|Optimisation Heuristics for Cryptology|ii iii iv
181|Protecting respondents’ identities in microdata release|Today’s globally networked society places great demand on the dissemination and sharing of information. While in the past released information was mostly in tabular and statistical form, many situations call today for the release of specific data (microdata). In order to protect the anonymity of the entities (called respondents) to which information refers, data holders often remove or encrypt explicit identifiers such as names, addresses, and phone numbers. De-identifying data, however, provides no guarantee of anonymity. Released information often contains other data, such as race, birth date, sex, and ZIP code, that can be linked to publicly available information to re-identify respondents and inferring information that was not intended for disclosure. In this paper we address the problem of releasing microdata while safeguarding the anonymity of the respondents to which the data refer. The approach is based on the definition of k-anonymity. A table provides k-anonymity if attempts to link explicitly identifying information to its content map the information to at least k entities. We illustrate how k-anonymity can be provided without compromising the integrity (or truthfulness) of the information released by using generalization and suppression techniques. We introduce the concept of minimal generalization that captures the property of the release process not to distort the data more than needed to achieve k-anonymity, and present an algorithm for the computation of such a generalization. We also discuss possible preference policies to choose among different minimal generalizations. Index terms:
182|Security-control methods for statistical databases: a comparative study|This paper considers the problem of providing security to statistical databases against disclosure of confidential information. Security-control methods suggested in the literature are classified into four general approaches: conceptual, query restriction, data perturbation, and output perturbation. Criteria for evaluating the performance of the various security-control methods are identified. Security-control methods that are based on each of the four approaches are discussed, together with their performance with respect to the identified evaluation criteria. A detailed comparative analysis of the most promising methods for protecting dynamic-online statistical databases is also presented. To date no single security-control method prevents both exact and partial disclosures. There are, however, a few perturbation-based methods that prevent exact disclosure and enable the database administrator to exercise “statistical disclosure control. ” Some of these methods, however introduce bias into query responses or suffer from the O/l query-set-size problem (i.e., partial disclosure is possible in case of null query set or a query set of size 1). We recommend directing future research efforts toward developing new methods that prevent exact disclosure and provide statistical-disclosure control, while at the same time do not suffer from the bias problem and the O/l query-set-size problem. Furthermore, efforts directed toward developing a bias-correction mechanism and solving the general problem of small query-set-size would help salvage a few of the current perturbation-based methods.
183|Protecting Privacy when Disclosing Information: k-Anonymity and Its Enforcement through Generalization and Suppression|Today&#039;s globally networked society places great demand on the dissemination and sharing of person-specific data. Situations where aggregate statistical information was once the reporting norm now rely heavily on the transfer of microscopically detailed transaction and encounter information. This happens at a time when more and more historically public information is also electronically available. When these data are linked together, they provide an electronic shadow of a person or organization that is as identifying and personal as a fingerprint, even when the sources of the information contains no explicit identifiers, such as name and phone number. In order to protect the anonymity of individuals to whom released data refer, data holders often remove or encrypt explicit identifiers such as names, addresses and phone numbers. However, other distinctive data, which we term quasi-identifiers, often combine uniquely and can be linked to publicly available information to re-identify indiv...
184|A Security Policy Model for Clinical Information Systems|The protection of personal health information has become a live issue in a number of countries including the USA, Canada, Britain and Germany. The debate has shown that there is widespread confusion about what should be protected, and why. Designers of military and banking systems can refer to Bell-LaPadula and Clark-Wilson respectively, but there is no comparable security policy model that spells out clear and concise access rules for clinical information systems. In this article, we present just such a model. It was commissioned by doctors and is driven by medical ethics; it is informed by the actual threats to privacy, and reflects current best clinical practice. Its effect is to restrict both the number of users who can access any record and the maximum number of records accessed by any user. This entails controlling information flows across rather than down and enforcing a strong notification property. We discuss its relationship with existing security policy models, and its possi...
185|Aggregation and inference: Facts and fallacies|This paper examines inference and aggregation problems that can ariee in multilevel relational database systems and points out some fallaciee in our thinking about these problems that may hin-der real progrees from being made toward their so-lution. Although others have done come initial re-search toward solving inference problems, aggrega-tion has been treated only superficially in the liter-ature. This paper attempts to lay a firmer found-tion for a theory of these problems. Several types of problem are identified and approaches toward their solution suggested. 1
186|Fuzzy extractors: How to generate strong keys from biometrics and other noisy data|We provide formal definitions and efficient secure techniques for • turning noisy information into keys usable for any cryptographic application, and, in particular, • reliably and securely authenticating biometric data. Our techniques apply not just to biometric information, but to any keying material that, unlike traditional cryptographic keys, is (1) not reproducible precisely and (2) not distributed uniformly. We propose two primitives: a fuzzy extractor reliably extracts nearly uniform randomness R from its input; the extraction is error-tolerant in the sense that R will be the same even if the input changes, as long as it remains reasonably close to the original. Thus, R can be used as a key in a cryptographic application. A secure sketch produces public information about its input w that does not reveal w, and yet allows exact recovery of w given another value that is close to w. Thus, it can be used to reliably reproduce error-prone biometric inputs without incurring the security risk inherent in storing them. We define the primitives to be both formally secure and versatile, generalizing much prior work. In addition, we provide nearly optimal constructions of both primitives for various measures of “closeness” of input data, such as Hamming distance, edit distance, and set difference.
187|Pseudo-Random Generation from One-Way Functions|Pseudorandom generators are fundamental to many theoretical and applied aspects of computing. We show howto construct a pseudorandom generator from any oneway function. Since it is easy to construct a one-way function from a pseudorandom generator, this result shows that there is a pseudorandom generator iff there is a one-way function.
188|On the Resemblance and Containment of Documents|Given two documents A and B we define two mathematical notions: their  resemblance r(A, B)andtheircontainment c(A, B) that seem to capture well  the informal notions of &#034;roughly the same&#034; and &#034;roughly contained.&#034;  The basic idea is to reduce these issues to set intersection problems that can  be easily evaluated by a process of random sampling that can be done independently  for each document. Furthermore, the resemblance can be evaluated  using a fixed size sample for each document.
189|Secret Key Agreement by Public Discussion From Common Information|. The problem of generating a shared secret key S by two parties knowing dependent random variables X and Y , respectively, but not sharing a secret key initially, is considered. An enemy who knows the random variable Z, jointly distributed with X and Y  according to some probability distribution PXY Z , can also receive all messages exchanged by the two parties over a public channel. The goal of a protocol is that the enemy obtains at most a negligible amount of information about S. Upper bounds on H(S) as a function of  PXY Z are presented. Lower bounds on the rate H(S)=N (as N !1) are derived for the case where X = [X 1 ; : : : ; XN ], Y = [Y 1 ; : : : ; YN ] and Z = [Z 1 ; : : : ; ZN ] result from N independent executions of a random experiment generating X i ; Y i and Z i , for i = 1; : : : ; N . In particular it is shown that such secret key agreement is possible for a scenario where all three parties receive the output of a binary symmetric source over independent binary symmetr...
190|Improved Decoding of Reed-Solomon and Algebraic-Geometry Codes|Given an error-correcting code over strings of length n and an arbitrary input string also of length n, the list decoding problem is that of finding all codewords within a specified Hamming distance from the input string. We present an improved list decoding algorithm for decoding Reed-Solomon codes. The list decoding problem for Reed-Solomon codes reduces to the following &#034;curve-fitting&#034; problem over a field F : Given n points f(x i :y i )g i=1 , x i
191|Generalized Privacy Amplification| This paper provides a general treatment of privacy amplification by public discussion, a concept introduced by Bennett, Brassard and Robert for a special scenario. Privacy amplification is a process that allows two parties to distill a secret key from a common random variable about which an eavesdropper has partial information. The two parties generally know nothing about the eavesdropper&#039;s information except that it satisfies a certain constraint. The results have applications to unconditionally-secure secret-key agreement protocols and quantum cryptography, and they yield results on wire-tap and broadcast channels for a considerably strengthened definition of secrecy capacity.  
192|A Fuzzy Commitment Scheme|We combine well-known techniques from the areas of errorcorrecting codes and cryptography to achieve a new type of cryptographic primitive that we refer to as a fuzzy commitment scheme. Like a conventional cryptographic commitment scheme, our fuzzy commitment scheme is both concealing and binding: it is infeasible for an attacker to learn the committed value, and also for the committer to decommit a value in more than one way. In a conventional scheme, a commitment must be opened using a unique witness, which acts, essentially, as a decryption key. By contrast, our scheme is fuzzy in the sense that it accepts a witness that is close to the original encrypting witness in a suitable metric, but not necessarily identical. This characteristic of our fuzzy commitment scheme makes it useful for applications such as biometric authentication systems, in which data is subject to random noise. Because the scheme is tolerant of error, it is capable of protecting biometric data just as conventional cryptographic techniques, like hash functions, are used to protect alphanumeric passwords.  This addresses a major outstanding problem in the theory of biometric authentication.  We prove the security characteristics of our fuzzy commitment scheme relative to the properties of an underlying cryptographic hash function.
193|A fuzzy vault scheme|Abstract. We describe a simple and novel cryptographic construction that we refer to as a fuzzy vault. A player Alice may place a secret value ? in a fuzzy vault and “lock ” it using a set A of elements from some public universe U. If Bob tries to “unlock ” the vault using a set B of similar length, he obtains ? only if B is close to A, i.e., only if A and B overlap substantially. In constrast to previous constructions of this flavor, ours possesses the useful feature of order invariance, meaning that the ordering of A and B is immaterial to the functioning of the vault. As we show, our scheme enjoys provable security against a computationally unbounded attacker.
194|Randomness is Linear in Space|We show that any randomized algorithm that runs in space S and time T and uses poly(S) random bits can be simulated using only O(S) random bits in space S and time T poly(S). A deterministic simulation in space S follows. Of independent interest is our main technical tool: a procedure which extracts randomness from a defective random source using a small additional number of truly random bits. 1
196|Password Hardening Based on Keystroke Dynamics|Abstract. We present a novel approach to improving the security of passwords. In our approach, the legitimate user’s typing patterns (e.g., durations of keystrokes and latencies between keystrokes) are combined with the user’s password to generate a hardened password that is convincingly more secure than conventional passwords alone. In addition, our scheme automatically adapts to gradual changes in a user’s typing patterns while maintaining the same hardened password across multiple logins, for use in file encryption or other applications requiring a long-term secret key. Using empirical data and a prototype implementation of our scheme, we give evidence that our approach is viable in practice, in terms of ease of use, improved security, and performance.
197|A Proposal for an ISO Standard for Public Key Encryption (version 2.0)  (2001) |This document should be viewed less as a first draft of a standard for public-key encryption, and more as a proposal for what such a draft standard should contain. It is hoped that this proposal will serve as a basis for discussion, from which a consensus for a standard may be formed.
199|Reusable cryptographic fuzzy extractors|We show that a number of recent definitions and constructions of fuzzy extractors are not adequate for multiple uses of the same fuzzy secret—a major shortcoming in the case of biometric applications. We propose two particularly stringent security models that specifically address the case of fuzzy secret reuse, respectively from an outsider and an insider perspective, in what we call a chosen perturbation attack. We characterize the conditions that fuzzy extractors need to satisfy to be secure, and present generic constructions from ordinary building blocks. As an illustration, we demonstrate how to use a biometric secret in a remote error tolerant authentication protocol that does not require any storage on the client’s side. 1
200|Extracting Randomness: A Survey and New Constructions|this paper we do two things. First, we survey extractors and dispersers: what they are, how they can be designed, and some of their applications. The work described in the survey is due to a long list of research papers by various authors##most notably by David Zuckerman. Then, we present a new tool for constructing explicit extractors and give two new constructions that greatly improve upon previous results. The new tool we devise, a merger,&#034; is a function that accepts d strings, one of which is uniformly distributed and outputs a single string that is guaranteed to be uniformly distributed. We show how to build good explicit mergers, and how mergers can be used to build better extractors. Using this, we present two new constructions. The first construction succeeds in extracting all of the randomness from any somewhat random source. This improves upon previous extractors that extract only some of the randomness from somewhat random sources with enough&#034; randomness. The amount of truly random bits used by this extractor, however, is not optimal. The second extractor we build extracts only some of the randomness and works only for sources with enough randomness, but uses a nearoptimal amount of truly random bits. Extractors and dispersers have many applications in removing randomness&#034; in various settings and in making randomized constructions explicit. We survey some of these applications and note whenever our new constructions yield better results, e.g., plugging our new extractors into a previous construction we achieve the first explicit N-superconcentrators of linear size and polyloglog(N) depth. ] 1999 Academic Press  CONTENTS  1. 
201|Subquadratic-time factoring of polynomials over finite fields|Abstract. New probabilistic algorithms are presented for factoring univariate polynomials over finite fields. The algorithms factor a polynomial of degree n over a finite field of constant cardinality in time O(n 1.815). Previous algorithms required time T(n 2+o(1)). The new algorithms rely on fast matrix multiplication techniques. More generally, to factor a polynomial of degree n over the finite field Fq with q elements, the algorithms use O(n 1.815 log q) arithmetic operations in Fq. The new “baby step/giant step ” techniques used in our algorithms also yield new fast practical algorithms at super-quadratic asymptotic running time, and subquadratic-time methods for manipulating normal bases of finite fields. 1.
202|Set Reconciliation with Nearly Optimal Communication Complexity|We consider the problem of efficiently reconciling two similar sets held by different hosts while minimizing the communication complexity. This type of problem arises naturally from gossip protocols used for the distribution of information. We describe an approach to set reconciliation based on the encoding of sets as polynomials. The resulting protocols exhibit tractable computational complexity and nearly optimal communication complexity. Also, these protocols can be adapted to work over a broadcast channel, allowing many clients to reconcile with one host based on a single broadcast, even if each client is missing a different subset.
203|Robust fuzzy extractors and authenticated key agreement from close secrets|Consider two parties holding samples from correlated distributions W and W ', respectively, where these samples are within distance t of each other in some metric space. The parties wish to agree on a close-to-uniformly distributed secret key R by sending a single message over an insecure channel controlled by an all-powerful adversary who may read and modify anything sent over the channel. We consider both the keyless case, where the parties share no additional secret information, and the keyed case, where the parties share a long-term secret SKBSM that they can use to generate a sequence of session keys {Rj} using multiple pairs {(Wj, W ' j)}. The former has applications to, e.g., biometric authentication, while the latter arises in, e.g., the bounded-storage model with errors. We show solutions that improve upon previous work in several respects: • The best prior solution for the keyless case with no errors (i.e., t = 0) requires the minentropy of W to exceed 2n/3, where n is the bit-length of W. Our solution applies whenever the min-entropy of W exceeds the minimal threshold n/2, and yields a longer key. • Previous solutions for the keyless case in the presence of errors (i.e., t&gt; 0) required random oracles. We give the first constructions (for certain metrics) in the standard model. • Previous solutions for the keyed case were stateful. We give the first stateless solution. 1
204|Efficient Cryptographic Protocols based on Noisy Channels|The Wire-Tap Channel of Wyner [20] shows that a Binary Symmetric Channel may be used as a basis for exchanging a secret key, in a cryptographic scenario of two honest people facing an eavesdropper. Later Cr&#039;epeau and Kilian [9] showed how a BSC may be used to implement Oblivious Transfer in a cryptographic scenario of two possibly dishonest people facing each other. Unfortunately this result is rather impractical as it requires\Omega\Gamma  n  11  ) bits to be transmitted through the BSC to accomplish a single OT. The current paper provides efficient protocols to achieve the cryptographic primitives of Bit Commitment and Oblivious Transfer based on the existence of a Binary Symmetric Channel. Our protocols respectively require sending O(n) and  O(n  3  ) bits through the BSC. These results are based on a technique known as Generalized Privacy Amplification [1] that allow two people to extract secret information from partially compromised data. 1 Introduction  The cryptographic power of...
205|Correcting errors without leaking partial information|This paper explores what kinds of information two parties must communicate in order to correct errors which occur in a shared secret string W. Any bits they communicate must leak a significant amount of information about W — that is, from the adversary’s point of view, the entropy of W will drop significantly. Nevertheless, we construct schemes with which Alice and Bob can prevent an adversary from learning any useful information about W. Specifically, if the entropy of W is sufficiently high, then there is no function f(W) which the adversary can learn from the error-correction information with significant probability. This leads to several new results: (a) the design of noise-tolerant “perfectly oneway” hash functions in the sense of Canetti et al. [7], which in turn leads to obfuscation of proximity queries for high entropy secrets W; (b) private fuzzy extractors [11], which allow one to extract uniformly random bits from noisy and nonuniform data W, while also insuring that no sensitive information about W is leaked; and (c) noise tolerance and stateless key re-use in the Bounded Storage Model, resolving the main open problem of Ding [10]. The heart of our constructions is the design of strong randomness extractors with the property that the source W can be recovered from the extracted randomness and any string W ' which is close to W.
206|Simple and tight bounds for information reconciliation and privacy amplification|Abstract. Shannon entropy is a useful and important measure in information processing, for instance, data compression or randomness extraction, under the assumption—which can typically safely be made in communication theory—that a certain random experiment is independently repeated many times. In cryptography, however, where a system’s working has to be proven with respect to a malicious adversary, this assumption usually translates to a restriction on the latter’s knowledge or behavior and is generally not satisfied. An example is quantum key agreement, where the adversary can attack each particle sent through the quantum channel differently or even carry out coherent attacks, combining a number of particles together. In information-theoretic key agreement, the central functionalities of information reconciliation and privacy amplification have, therefore, been extensively studied in the scenario of general distributions: Partial solutions have been given, but the obtained bounds are arbitrarily far from tight, and a full analysis appeared
207|List decoding algorithms for certain concatenated codes|We give efficient (polynomial-time) list-decoding algorithms for certain families of error-correcting codes obtained by “concatenation”. Specifically, we give list-decoding algorithms for codes where the “outer code ” is a Reed-Solomon or Algebraic-geometric code and the “inner code ” is a Hadamard code. Codes obtained by such concatenation are the best known constructions of errorcorrecting codes with very large minimum distance. Our decoding algorithms enhance their nice combinatorial properties with algorithmic ones, by decoding these codes up to the currently known bound on their list-decoding “capacity”. In particular, the number of errors that we can correct matches (exactly) the number of errors for which it is known that the list size is bounded by a polynomial in the length of the codewords. 1
208|On the Relation of Error Correction and Cryptography to an Off Line Biometric Based Identification Scheme|An off-line biometric identification protocol based on error correcting  codes was recently developed as an enabling technology for secure  biometric based user authentication. The protocol was designed to  bind a user&#039;s iris biometric template with authorization information  via a magnetic strip in the off-line case while reducing the exposure of  a user&#039;s biometric data. In this paper we give an in depth discussion  of the role of error correcting codes in the cryptographically secure  biometric authentication scheme.  An Iris scan is a biometric technology which uses the human iris to authenticate users [BAW96, HMW90, Dau92, Wil96]. This technology produces a 2048 bit user biometric template such that any future scan of the same user&#039;s iris will generate a &#034;similar&#034; template. By similar, we mean having an Center for Cryptography, Computer, and Network Security, University of WisconsinMilwaukee, USA. E-mail: davida@cs.uwm.edu.  y  CertCo LLC, New York, NY, USA. E-mail: yfrankel@cs.co...
209|Upper Bounds for Constant-Weight Codes|Let A(n; d; w) denote the maximum possible number of codewords in an (n; d; w) constant-weight binary code. We improve upon the best known upper bounds on A(n; d; w) in numerous instances for n 6 24 and d 6 12,  which is the parameter range of existing tables. Most improvements occur for d = 8; 10, where we reduce the upper bounds in more than half of the unresolved cases. We also extend the existing tables up to n 6 28 and d 6 14.  To obtain these results, we develop new techniques and introduce new classes of codes. We derive a number of general bounds on A(n; d; w) by means of mapping constantweight codes into Euclidean space. This approach produces, among other results, a bound on A(n; d; w) that is tighter than the Johnson bound. A similar improvement over the best known bounds for doubly-constant-weight codes, studied by Johnson and Levenshtein, is obtained in the same way. Furthermore, we introduce the concept of doubly-boundedweight codes, which may be thought of as a generaliz...
210|Secure Applications of Low-Entropy Keys|We introduce the notion of key stretching, a mechanism to  convert short s-bit keys into longer keys, such that the complexity required  to brute-force search a s + t-bit keyspace is the same as the time  required to brute-force search a s-bit key stretched by t bits.
211|Reliable Biometric Authentication with Privacy Protection|Abstract. We propose a new scheme for reliable authentication of physical objects. The scheme allows not only the combination of noisy data with cryptographic functions but has the additional property that the stored reference information is non-revealing. By breaking into the database and retrieving the stored data, the attacker will not be able to obtain any realistic approximation of the original physical object. This technique has applications in secure storage of biometric templates in databases and in authentication of PUFs (Physical Uncloneable Functions). 1.
212|Secure sketch for biometric templates|Abstract. There have been active discussions on how to derive a consistent cryptographic key from noisy data such as biometric templates, with the help of some extra information called a sketch. It is desirable that the sketch reveals little information about the biometric templates even in the worst case (i.e., the entropy loss should be low). The main difficulty is that many biometric templates are represented as points in continuous domains with unknown distributions, whereas known results either work only in discrete domains, or lack rigorous analysis on the entropy loss. A general approach to handle points in continuous domains is to quantize (discretize) the points and apply a known sketch scheme in the discrete domain. However, it can be difficult to analyze the entropy loss due to quantization and to find the “optimal ” quantizer. In this paper, instead of trying to solve these problems directly, we propose to examine the relative entropy loss of any given scheme, which bounds the number of additional bits we could have extracted if we used the optimal parameters. We give a general scheme and show that the relative entropy loss due to suboptimal discretization is at most (nlog 3), where n is the number of points, and the bound is tight. We further illustrate how our scheme can be applied to real biometric data by giving a concrete scheme for face biometrics.
213|Optimal error correction against computationally bounded noise|Abstract. For computationally bounded adversarial models of error, we construct appealingly simple, efficient, cryptographic encoding and unique decoding schemes whose error-correction capability is much greater than classically possible. In particular: 1. For binary alphabets, we construct positive-rate coding schemes which are uniquely decodable from a 1/2 - ? error rate for any constant ?&gt; 0. 2. For large alphabets, we construct coding schemes which are uniquely decodable from a 1 -  v R error rate for any information rate R&gt; 0. Our results are qualitatively stronger than related work: the construction works in the public-key model (requiring no shared secret key or joint local state) and allows the channel to know everything that the receiver knows. In addition, our techniques can potentially be used to construct coding schemes that have information rates approaching the Shannon limit. Finally, our construction is qualitatively optimal: we show that unique decoding under high error rates is impossible in several natural relaxations of our model. 1
214|Lower bounds for embedding edit distance into normed spaces|MIT S. Raskhodnikova MIT 1 Introduction The edit distance (also called Levenshtein metric) between two strings is the minimum number of operations (insertions, deletions and character substitutions) needed to transform one string into another. This distance is of key importance in computational biology, as well as text processing and other areas. Algorithms for problems involving this metric have been extensively investigated. In particular, the quadratic-time dynamic programming algorithm for computing the edit distance between two strings is one of the most investigated and used algorithms in computational biology. Recently, a new approach to problems involving edit distance has been proposed. Its basic component is construction of a mapping f (called an embedding), which maps any string s into a vector f (s) 2!
215|Tight Bounds for Depth-Two Superconcentrators|We show that the minimum size of a depth-two N-superconcentrator is \Theta(N log² N= log log N ). Before this work, optimal bounds were known for all depths except two. For the upper bound, we build superconcentrators by putting together a small number of disperser graphs; these disperser graphs are obtained using a probabilistic argument. We present two different methods for showing lower bounds. First, we show that superconcentrators contain several disjoint disperser graphs. When combined with the lower bound for disperser graphs due to Kovari, S&#039;os and Tur&#039;an, this gives an almost optimal lower bound of \Omega\Gamma N(log N= log log N )²) on the size of N - superconcentrators. The second method, based on the work of Hansel, gives the optimal lower bound.  The method of the Kovari, S&#039;os and Tur&#039;an can be extended to give tight lower bounds for extractors, both in terms of the number of truly random bits needed to extract one additional bit and in terms of the unavoidable entr...
216|Practical Set Reconciliation|We consider the problem of efficiently reconciling two similar sets held by different hosts. This problem is motivated by the problem of data exchange in a gossip protocol, but also has other applications including synchronization of PDA databases and maintenance of routing tables in the face of host failures. Previous results have presented algorithms for set reconciliation that are nearly optimal in terms of communication complexity, but have computational complexity that is cubic in the number of differences. We present and analyze a novel algorithm that has expected computational and communication complexity that is linear in the number of differences between the sets being reconciled. We also provide experimental results from an implementation of this algorithm.
217|Using Voice to Generate Cryptographic Keys|In this position paper, we motivate and summarize our work on repeatably generating cryptographic keys from spoken user input. The goal of this work is to enable a device to generate a key (e.g., for encrypting files) upon its user speaking a chosen password (or passphrase) to it. An attacker who captures the device and extracts all information it contains, however, should be unable to determine this key. We outline our approach for achieving this goal and present preliminary empirical results for it. We also describe several directions for future work.
218|Scrambling Adversarial Errors Using Few Random Bits, Optimal Information Reconciliation, and Better Private Codes|When communicating over a noisy channel, it is typically much easier to deal with random,  independent errors with a known distribution than with adversarial errors. This paper looks at  how one can use schemes designed for random errors in an adversarial context, at the cost of  relatively few additional random bits and without using unproven computational assumptions.
219|Error correction in the bounded storage model|Abstract. We initiate a study of Maurer’s bounded storage model (JoC, 1992) in presence of transmission errors and perhaps other types of errors that cause different parties to have inconsistent views of the public random source. Such errors seem inevitable in any implementation of the model. All previous schemes and protocols in the model assume a perfectly consistent view of the public source from all parties, and do not function correctly in presence of errors, while the private-key encryption scheme of Aumann, Ding and Rabin (IEEE IT, 2002) can be extended to tolerate only a O(1 / log (1/e)) fraction of errors, where e is an upper bound on the advantage of an adversary. In this paper, we provide a general paradigm for constructing secure and error-resilient private-key cryptosystems in the bounded storage model that tolerate a constant fraction of errors, and attain the near optimal parameters achieved by Vadhan’s construction (JoC, 2004) in the errorless case. In particular, we show that any local fuzzy extractor yields a secure and error-resilient cryptosystem in the model, in analogy to the result of Lu (JoC, 2004) that any local strong extractor yields a secure cryptosystem in the errorless case, and construct efficient local fuzzy extractors by extending Vadhan’s sample-then-extract paradigm. The main ingredients of our constructions are averaging samplers (Bellare and Rompel, FOCS ’94), randomness extractors (Nisan and Zuckerman, JCSS, 1996), error correcting codes, and fuzzy extractors (Dodis, Reyzin and Smith, EUROCRYPT ’04). 1
220|Externalized Fingerprint Matching|The 9/11 tragedy triggered an increased interest in biometric  passports. According to several sources [2], the electronic ID market is  expected to increase by more than 50% per annum over the three coming  years, excluding China.
221|Reconciliation puzzles|We consider the problem of exchanging two similar strings held by different hosts with a minimum amount of communication. We reduce the problem of string reconciliation to a problem of multi-set reconciliation, for which nearly optimal solutions exist. Our approach involves transforming a string into a multi-set of substrings, which are reconciled efficiently and then put back together on a remote host using recent graph-theoretic results. We present an analysis of our algorithm to show that its communication complexity compares favorably to an existing method for string reconciliation. A version of this article will appear as: ¯ V. Chauhan and A. Trachtenberg, “Reconciliation puzzles”, IEEE Globecom 2004, Dallas, TX. I.
222|Fuzzy extractors|This chapter presents a general approach for handling secret biometric data in cryptographic applications. The generality manifests itself in two ways: we attempt to minimize the assumptions we make about the data, and to present techniques that are broadly applicable wherever biometric inputs are used. Because biometric data comes from a variety of sources that are mostly outside of anyone’s control, it is prudent to assume as little as possible about how they are distributed; in particular, an adversary may know more about a distribution than a system’s designers and users. Of course, one may attempt to measure some properties of a biometric distribution, but relying on such measurements in the security analysis is dangerous, because the adversary may have even more accurate measurements available to it. For instance, even assuming that some property of a biometric behaves according to a binomial distribution (or some similar discretization of the normal distribution), one could determine the mean of this distribution only to within ˜ 1 v after taking n n samples; a well-motivated adversary can take more measurements, and thus determine the mean more accurately.
223|Data Minder using Steganography: Technology of Data Encryption |Abstract- The internet and the World Wide Web have revolutionized the way in which digital data is distributed. The growing possibilities of modern communication need special means of security especially on computer network. In this paper a new randomized secure data hiding algorithm using file hybridization is proposed for strengthening the security of information through a combination of cryptography and steganography with random transformation and file hybridization. Steganography is the technique of hiding confidential information within any media. Steganography is often confused with cryptography because the two are similar in the way that they both are used to protect confidential information. The difference between the two is in the appearance in the processed output; the output of steganography operation is not apparently visible but in cryptography the output is scrambled so that it can draw attention. Steganlysis is process to detect of presence of steganography. Steganography hides the message so it cannot be seen. A message is a cipher text for instance, might arouse suspicious on the part of the recipient while an “invisible ” message created with steganography. This approach includes a high degree of security and data protection implemented by the concept “Message Sending ” and “Secrete Communication ” at large scale. This System includes three main parts Administrator, Encryption and Decryption. The Administrator creates the monitor accounts. Encryption uses the service of the system &amp; authenticates the user. Decryption is used to decrypt data.
224|Information Security through Data Encryption and Data Hiding |This paper introduces a model for information security and message hiding based on image steganography as well as it describes about various steps of that proposed model in brief. Keywords-Image steganography, Data Hiding I.
225|Evaluation of the RC4 Algorithm for Data Encryption |Analysis of the effect of different parameters of the RC4 encryption algorithm where examined. Some experimental work was performed to illustrate the performance of this algorithm based on changing some of these parameters. The execution time as a function of the encryption key length and the file size was examined; this has been stated as complexity and security. Various data types were analyzed and the role of the data type was also emphasized. The results have been analyzed and interpreted as mathematical equations showing the relationship between the examined data and hence can be used to predict any future performance of the algorithm under different conditions. The order of the polynomial to approximate the execution time was justified.
226|Data mules: Modeling a three-tier architecture for sparse sensor networks|Abstract — This paper presents and analyzes an architecture that exploits the serendipitous movement of mobile agents in an environment to collect sensor data in sparse sensor networks. The mobile entities, called MULEs, pick up data from sensors when in close range, buffer it, and drop off the data to wired access points when in proximity. This leads to substantial power savings at the sensors as they only have to transmit over a short range. Detailed performance analysis is presented based on a simple model of the system incorporating key system variables such as number of MULEs, sensors and access points. The performance metrics observed are the data success rate (the fraction of generated data that reaches the access points) and the required buffer capacities on the sensors and the MULEs. The modeling along with simulation results can be used for further analysis and provide certain guidelines for deployment of such systems. I.
227|An Energy-Efficient MAC Protocol for Wireless Sensor Networks|This paper proposes S-MAC, a medium-access control (MAC) protocol designed for wireless sensor networks. Wireless sensor networks use battery-operated computing and sensing devices. A network of these devices will collaborate for a common application such as environmental monitoring. We expect sensor networks to be deployed in an ad hoc fashion, with individual nodes remaining largely inactive for long periods of time, but then becoming suddenly active when something is detected. These characteristics of sensor networks and applications motivate a MAC that is different from traditional wireless MACs such as IEEE 802.11 in almost every way: energy conservation and self-configuration are primary goals, while per-node fairness and latency are less important. S-MAC uses three novel techniques to reduce energy consumption and support self-configuration. To reduce energy consumption in listening to an idle channel, nodes periodically sleep. Neighboring nodes form virtual clusters to auto-synchronize on sleep schedules. Inspired by PAMAS, S-MAC also sets the radio to sleep during transmissions of other nodes. Unlike PAMAS, it only uses in-channel signaling. Finally, S-MAC applies message passing to reduce contention latency for sensor-network applications that require store-andforward processing as data move through the network. We evaluate our implementation of S-MAC over a sample sensor node, the Mote, developed at University of California, Berkeley. The experiment results show that, on a source node, an 802.11-like MAC consumes 2--6 times more energy than S-MAC for traffic load with messages sent every 1-10s.
228|Mobility increases the capacity of ad-hoc wireless networks|The capacity of ad-hoc wireless networks is constrained by the mutual interference of concurrent transmissions between nodes. We study a model of an ad-hoc network where n nodes communicate in random source-destination pairs. These nodes are assumed to be mobile. We examine the per-session throughput for applications with loose delay constraints, such that the topology changes over the time-scale of packet delivery. Under this assumption, the per-user throughput can increase dramatically when nodes are mobile rather than fixed. This improvement can be achieved by exploiting node mobility as a type of multiuser diversity.  
229|A Survey of Mobility Models for Ad Hoc Network Research|In the performance evaluation of a protocol for an ad hoc network, the protocol should be tested under realistic conditions including, but not limited to, a sensible transmission range, limited buffer space for the storage of messages, representative data traffic models, and realistic movements of the mobile users (i.e., a mobility model). This paper is a survey of mobility models that are used in the simulations of ad hoc networks. We describe several mobility models that represent mobile nodes whose movements are independent of each other (i.e., entity mobility models) and several mobility models that represent mobile nodes whose movements are dependent on each other (i.e., group mobility models). The goal of this paper is to present a number of mobility models in order to offer researchers more informed choices when they are deciding upon a mobility model to use in their performance evaluations. Lastly, we present simulation results that illustrate the importance of choosing a mobility model in the simulation of an ad hoc network protocol. Specifically, we illustrate how the performance results of an ad hoc network protocol drastically change as a result of changing the mobility model simulated.
230|Energy-Efficient Computing for Wildlife Tracking: Design Tradeoffs and Early Experiences with ZebraNet|Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The  eld of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scienti  c and commercial purposes. This paper examines the research decisions and design tradeos that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.
231|Next Century Challenges: Mobile Networking for “Smart Dust”|Large-scale networks of wireless sensors are becoming an active topic of research. Advances in hardware technology and engineering design have led to dramatic reductions in size, power consumption and cost for digital circuitry, wire-less communications and Micro ElectroMechanical Systems (MEMS). This has enabled very compact, autonomous and mobile nodes, each containing one or more sensors, computation and communication capabilities, and a power supply. The missing ingredient is the networking and applications layers needed to harness this revolutionary capability into a complete system. We review the key elements of the emergent technology of “Smart Dust ” and outline the research challenges they present to the mobile networking and systems community, which must provide coherent connectivity to large numbers of mobile network nodes co-located within a small volume. 
233|Epidemic routing for partially-connected ad hoc networks|Mobile ad hoc routing protocols allow nodes with wireless adaptors to communicate with one another without any pre-existing network infrastructure. Existing ad hoc routing protocols, while robust to rapidly changing network topology, assume the presence of a connected path from source to destination. Given power limitations, the advent of short-range wireless networks, and the wide physical conditions over which ad hoc networks must be deployed, in some scenarios it is likely that this assumption is invalid. In this work, we develop techniques to deliver messages in the case where there is never a connected path from source to destination or when a network partition exists at the time a message is originated. To this end, we introduce Epidemic Routing, where random pair-wise exchanges of messages among mobile hosts ensure eventual message delivery. The goals of Epidemic Routing are to: i) maximize message delivery rate, ii) minimize message latency, and iii) minimize the total resources consumed in message delivery. Through an implementation in the Monarch simulator, we show that Epidemic Routing achieves eventual delivery of 100 % of messages with reasonable aggregate resource consumption in a number of interesting scenarios. 1
234|Connectivity in Ad-Hoc and Hybrid Networks|We consider a large-scale wireless network, but with a low density of nodes per unit area. Interferences are then less critical, contrary to connectivity. This paper studies the latter property for both a purely ad-hoc network and a hybrid network, where fixed base stations can be reached in multiple hops. We assume here that power constraints are modeled by a maximal distance above which two nodes are not (directly) connected. We find that
235|Smooth is Better than Sharp: A Random Mobility Model for Simulation of Wireless Networks|This paper presents an enhanced random mobility model for simulation-based studies of wireless networks. Our approach makes the movement trace of individual mobile stations more realistic than common approaches for random movement. After giving a survey of mobility models found in the literature, we give a detailed mathematical formulation of our model and outline its advantages. The movement concept is based on random processes for speed and direction control in which the new values are correlated to previous ones. Upon a speed change event, a new target speed is chosen, and an acceleration is set to achieve this target speed. The principles for a direction change are similar. Moreover, we propose two extensions for modeling typical movement patterns of vehicles. Finally, we consider strategies for the nodes&#039; border behavior (i.e., what happens when nodes move out of the simulation area) and point out a pitfall that occurs when using a bounded simulation area.
236|Sending Messages to Mobile Users in Disconnected Ad-Hoc Wireless Networks|An ad-hoc network is formed by a group of mobile hosts upon a wireless network interface. Previous research in this area has concentrated on routing algorithms which are designed for fully connected networks. The usual way to deal with a disconnected ad-hoc network is to let the mobile computer wait for network reconnection passively, which may lead to unacceptable transmission delays. In this paper, we propose an approach that guarantees message transmission in minimal time. In this approach, mobile hosts actively modify their trajectories to transmit messages. We develop algorithms that minimize the trajectory modifications under two different assumptions: (a) the movements of all the nodes in the system are known and (b) the movements of the hosts in the system are not known. 1.
237|Smart-Tag Based Data Dissemination|Monitoring wide, hostile areas requires disseminating data between fixed, disconnected clusters of sensor nodes. It is not always possible to install long-range radios in order to cover the whole area. We propose to leverage the movement of mobile individuals, equipped with smart-tags, to disseminate data across disconnected static nodes spread across a wide area. Static nodes and mobile smart-tags exchange data when they are in the vicinity of each other; smart-tags disseminate data as they move around. In this paper, we propose an algorithm for update propagation and a model for smart-tag based data dissemination. We use simulation to study the characteristics of the model we propose. Finally, we present an implementation based on bluetooth smart-tags.
238|An Efficient Communication Strategy for Ad-hoc Mobile Networks|Abstract. We investigate here the problem of establishing communication in an ad-hoc mobile network, that is, we assume the extreme case of a total absense of any fixed network infrastructure (for example a case of rapid deployment of a set of mobile hosts in an unknown terrain). We propose, in such a case, that a small subset of the deployed hosts (which we call the support) should be used to manage network operations. However, the vast majority of the hosts are moving arbitrarily according to application needs. We then provide a simple, correct and efficient protocol for communication establishment that avoids message flooding. Our protocol manages to establish communication between any pair of mobile hosts in small, apriori guaranteed expected time bounds even in the worst case of arbitrary motions of the hosts that do not belong to the support (provided that they do not deliberately try to avoid the support). These time bounds, interestingly, do not depend, on the number of mobile hosts that do not belong in the support. They depend only on the size of the area of motions. Our protocol can be implemented in very efficient ways by exploiting knowledge of the space of motions or by adding more power to the hosts of the support. Our results exploit and further develop some fundamental properties of random walks in finite graphs. 1
239|Wireless Subscriber Mobility Management using Adaptive Individual Location Areas for PCS Systems|We consider a new mobility management scheme -- the Adaptive Location Area Tracking Scheme, in which each mobile performs a location registration as it crosses the boundary of its current personal location area and is assigned a new location area. The size and shape of the new location area depend on the mobile&#039;s mobility and call characteristics in its previous location area. The objective is to minimize the combined average signaling cost of both paging and registration activities for each individual mobile user.
240|Probability Criterion Based Location Tracking Approach for Mobility Management of Personal Communications Systems|We consider a probability criterion based location tracking scheme for personal communications systems. This scheme seeks to minimize the average signaling cost due to both paging and registration for individual mobile users. A timer-based location update and a single-step dynamic paging procedure are used. Each user-network interaction resets the timer and modifies location record. The user registers when the timer expires. Users are paged over personally constructed  paging areas. The size of these areas is an increasing function of the time since last user-network contact. The shape of the paging area depends on the particular probability distribution of user location. The problem formulation is applicable to arbitrary motion models assuming the associated user location distribution is available or can be estimated. As an illustrative example, we use a Brownian motion process for user motion and assume Poisson call arrivals. We then investigate the influence of user mobility and cal...
241|Freenet: A Distributed Anonymous Information Storage and Retrieval System|We describe Freenet, an adaptive peer-to-peer network application  that permits the publication, replication, and retrieval of data  while protecting the anonymity of both authors and readers. Freenet operates  as a network of identical nodes that collectively pool their storage  space to store data files and cooperate to route requests to the most  likely physical location of data. No broadcast search or centralized location  index is employed. Files are referred to in a location-independent  manner, and are dynamically replicated in locations near requestors and  deleted from locations where there is no interest. It is infeasible to discover  the true origin or destination of a file passing through the network,  and difficult for a node operator to determine or be held responsible for  the actual physical contents of her own node.
243|Error and attack tolerance of complex networks|Many complex systems display a surprising degree of tolerance against errors. For example, relatively simple organisms grow, persist and reproduce despite drastic pharmaceutical or environmental interventions, an error tolerance attributed to the robustness of the underlying metabolic network [1]. Complex communication networks [2] display a surprising degree of robustness: while key components regularly malfunction, local failures rarely lead to the loss of the global information-carrying ability of the network. The stability of these and other complex systems is often attributed to the redundant wiring of the functional web defined by the systems’ components. In this paper we demonstrate that error tolerance is not shared by all redundant systems, but it is displayed only by a class of inhomogeneously wired networks, called scale-free networks. We find that scale-free networks, describing a number of systems, such as the World Wide Web (www) [3–5], Internet [6], social networks [7] or a cell [8], display an unexpected degree of robustness, the ability of their nodes to communicate being unaffected by even unrealistically high failure rates. However,
244|Private Information Retrieval| Publicly accessible databases are an indispensable resource for retrieving up to date information. But they also pose a significant risk to the privacy of the user, since a curious database operator can follow the user&#039;s queries and infer what the user is after. Indeed, in cases where the users &#039; intentions are to be kept secret, users are often cautious about accessing the database. It can be shown that when accessing a single database, to completely guarantee the privacy of the user, the whole database should be downloaded, namely n bits should be communicated (where n is the number of bits in the database). In this work, we investigate whether by replicating the database, more efficient solutions to the private retrieval problem can be obtained. We describe schemes that enable a user to access k replicated copies of a database (k * 2) and privately retrieve information stored in the database. This means that each individual database gets no information on the identity of the item retrieved by the user. Our schemes use the replication to gain substantial saving. In particular, we have ffl A two database scheme with communication complexity of O(n1=3). ffl A scheme for a constant number, k, of databases with communication complexity O(n1=k). ffl A scheme for 13 log2 n databases with polylogarithmic (in n) communication complexity.
245|The Free Haven Project: Distributed Anonymous Storage Service|We present a design for a system of anonymous storage which resists the attempts of powerful adversaries to find or destroy any stored data. We enumerate distinct notions of anonymity for each party in the system, and suggest a way to classify anonymous systems based on the kinds of anonymity provided. Our design ensures the availability of each document for a publisher-specified lifetime. A reputation system provides server accountability by limiting the damage caused from misbehaving servers. We identify attacks and defenses against anonymous storage services, and close with a list of problems which are currently unsolved.
246|Publius: A robust, tamper-evident, censorship-resistant, web publishing system|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
247|Onion Routing for Anonymous and Private Internet Connections|this article&#039;s publication, the prototype network is processing more than 1 million Web connections per month from more than six thousand IP addresses in twenty countries and in all six main top level domains. [7] Onion Routing operates by dynamically building anonymous connections within a network of real-time Chaum Mixes [3]. A Mix is a store and forward device that accepts a number of fixed-length messages from numerous sources, performs cryptographic transformations on the messages, and then forwards the messages to the next destination in a random order. A single Mix makes tracking of a particular message either by specific bit-pattern, size, or ordering with respect to other messages difficult. By routing through numerous Mixes in the network, determining who is talking to whom becomes even more difficult. Onion Routing&#039;s network of core onion-routers (Mixes) is distributed, faulttolerant, and under the control of multiple administrative domains, so no single onion-router can bring down the network or compromise a user&#039;s privacy, and cooperation between compromised onion-routers is thereby confounded.
248|Web MIXes: A system for anonymous and unobservable Internet access|We present the architecture, design issues and functions of a MIX-based system for anonymous and unobservable real-time Internet access. This system prevents trac analysis as well as ooding attacks. The core technologies include an adaptive, anonymous, time/volumesliced channel mechanism and a ticket-based authentication mechanism. The system also provides an interface to inform anonymous users about their level of anonymity and unobservability.
249|The Eternity Service|The Internet was designed to provide a communications channel that is as resistant to denial of service attacks as human ingenuity can make it. In this note, we propose the construction of a storage medium with similar properties. The basic idea is to use redundancy and scattering techniques to replicate data across a large set of machines (such as the Internet), and add anonymity mechanisms to drive up the cost of selective service denial attacks. The detailed design of this service is an interesting scientific problem, and is not merely academic: the service may be vital in safeguarding individual rights against new threats posed by the spread of electronic publishing.
250|Anonymous Web Transactions with Crowds|This article presents a system called Crowds  that enables the retrieval of information over the  Web without revealing so much potentially private  information to several parties. The goal of  Crowds is to make browsing anonymous, so that  information about either the user or what information  he or she retrieves is hidden from Web  servers and other parties. Crowds prevents a  Web server from learning any potentially identifying  information about the user, including even  the user&#039;s IP address or domain name. Crowds  also prevents Web servers from learning a variety  of other information, such as the page that  referred the user to its site or the user&#039;s computing  platform
251|A Prototype Implementation of Archival Intermemory|An Archival Intermemory solves the problem of highly survivable digital data storage in the spirit of the Internet. In this paper we describe a prototype implementation of Intermemory, including an overall system architecture and implementations of key system components. The result is a working Intermemory that tolerates up to 17 simultaneous node failures, and includes a Web gateway for browser-based access to data. Our work demonstrates the basic feasibility of Intermemory and represents significant progress towards a deployable system.
252|Project &#034;Anonymity and Unobservability in the Internet&#034;|. It is a hard problem to achieve anonymity for real-time services in the Internet (e.g. Web access). All existing concepts fail when we assume a very strong attacker model (i.e. an attacker is able to observe all communication links). We also show that these attacks are realworld attacks. This paper outlines alternative models which mostly render these attacks useless. Our present work tries to increase the efficiency of these measures.  1 The perfect system  1.1 Attacks  The perfect anonymous communication system has to prevent the following attacks:  1.  Message coding attack: If messages do not change their coding during transmission they can be linked or traced.  2.  Timing attack: An opponent can observe the duration of a specific communication by linking its possible endpoints and waiting for a correlation between the creation and/or release event at each possible endpoint.  3.  Message volume attack: The amount of transmitted data (i.e. the message length) can be observed. Thus...
253|TAZ Servers and the Rewebber Network: Enabling Anonymous Publishing on the World Wide Web|The World Wide Web has recently matured enough to provide everyday users with an extremely cheap publishing mechanism. However, the current WWW architecture makes it fundamentally difficult to provide content without identifying yourself. We examine the problem of anonymous publication on the WWW, propose a design suitable for practical deployment, and describe our implementation. Some key features of our design include universal accessibility by pre-existing clients, short persistent names, security against social, legal, and political pressure, protection against abuse, and good performance.
254|The Free Haven Project: Design and Deployment of an Anonymous Secure Data Haven|The Free Haven Project aims to deploy a system for distributed data storage which is robust against attempts by powerful adversaries to find and destroy stored data. Free Haven uses a mixnet for communication, and it emphasizes distributed, reliable, and anonymous storage over efficient retrieval. We provide an outline of a formal definition of anonymity, to help characterize the protection that Free Haven provides and to help compare related services. We also provide some background from case law about anonymous speech and anonymous publication, and examine some of the ethical and moral implications of an anonymous publishing service. In addition, we describe a variety of attacks on the system and ways of protecting against these attacks. Some of the problems Free Haven addresses include providing sufficient accountability without sacrificing anonymity, building trust between servers based entirely on their observed behavior, and providing user interfaces that will make the system easy for end-users.
255|Performance Analysis of Data Encryption Standard Algorithm &amp; Proposed Data Encryption Standard Algorithm |Abstract:- The principal goal guiding the design of any encryption algorithm must be security against unauthorized attacks. Within the last decade, there has been a vast increase in the accumulation and communication of digital computer data in both the private and public sectors. Much of this information has a significant value, either directly or indirectly, which requires protection. The algorithms uniquely define the mathematical steps required to transform data into a cryptographic cipher and also to transform the cipher back to the original form. Performance and security level is the main characteristics that differentiate one encryption algorithm from another. Here introduces a new method to enhance the performance of the Data Encryption Standard (DES) algorithm is introduced here. This is done by replacing the predefined XOR operation applied during the 16 round of the standard algorithm by a new operation depends on using two keys, each key consists of a combination of 4 states (0, 1, 2, 3) instead of the ordinary 2 state key (0, 1). This replacement adds a new level of protection strength and more robustness against breaking methods. Keywords:- DES, Encryption,Decryption,SAC I.
256|A Security Architecture for Computational Grids|State-of-the-art and emerging scientific applications require fast access to large quantities of data and commensurately fast computational resources. Both resources and data are often distributed in a wide-area network with components administered locally and independently. Computations may involve hundreds of processes that must be able to acquire resources dynamically and communicate e#ciently. This paper analyzes the unique security requirements of large-scale distributed (grid) computing and develops a security policy and a corresponding security architecture. An implementation of the architecture within the Globus metacomputing toolkit is discussed.  
257|A Resource Management Architecture for Metacomputing Systems|Metacomputing systems are intended to support remote and/or  concurrent use of geographically distributed computational resources.  Resource management in such systems is complicated by five concerns  that do not typically arise in other situations: site autonomy and heterogeneous  substrates at the resources, and application requirements for policy  extensibility, co-allocation, and online control. We describe a resource  management architecture that addresses these concerns. This architecture  distributes the resource management problem among distinct local  manager, resource broker, and resource co-allocator components and defines  an extensible resource specification language to exchange information  about requirements. We describe how these techniques have been  implemented in the context of the Globus metacomputing toolkit and  used to implement a variety of different resource management strategies.  We report on our experiences applying our techniques in a large testbed,  GUSTO, incorporating 15 sites, 330 computers, and 3600 processors.  
258|A Secure Environment for Untrusted Helper Applications -- Confining the Wily Hacker |Many popular programs, such as Netscape, use untrusted helper applications to process data from the network. Unfortunately, the unauthenticated network data they interpret could well have been created by an adversary, and the helper applications are usually too complex to be bug-free. This raises significant security concerns. Therefore, it is desirable to create a secure environment to contain untrusted helper applications. We propose to reduce therisk  of a security breachby restricting the program&#039;s access to the operating system. In particular, we intercept and filter dangerous system calls via the Solaris process tracing facility. This enabled us to build a simple, clean, user-mode implementation of as ecure  environment for untrusted helper applications. Our implementation has negligible performance impact, and can protect pre-existing applications.
259|The Nexus approach to integrating multithreading and communication|Lightweight threads have an important role to play in parallel systems: they can be used to exploit shared-memory parallelism, to mask communication and I/O latencies, to implement remote memory access, and to support task-parallel and irregular applications. In this paper, we address the question of how to integrate threads and communication in high-performance distributed-memory systems. We propose an approach based on global pointer and remote service request mechanisms, and explain how these mechanisms support dynamic communication structures, asynchronous messaging, dynamic thread creation and destruction, and a global memory model via interprocessor references. We also explain how these mechanisms can be implemented in various environments. Our global pointer and remote service request mechanisms have been incorporated in a runtime system called Nexus that is used as a compiler target for parallel languages and as a substrate for higher-level communication libraries. We report the results of performance studies conducted using a Nexus implementation; these results indicate that Nexus mechanisms can be implemented efficiently on commodity hardware and software systems. 1
260|CLIQUES: A New Approach to Group Key Agreement|This paper considers the problem of key agreement in a group setting with highlydynamic  group member population. A protocol suite, called CLIQUES, is developed by  extending the well-known Diffie-Hellman key agreement method to support dynamic  group operations. Constituent protocol are secure, efficient and applicable to any  protocol layer, communication paradigm and network topology.
261|WebOS: Operating System Services for Wide Area Applications |In this paper, we demonstrate the power of providing a common set of Operating System services to wide-area applications, including mechanisms for naming, persistent storage, remote process execution, resource management, authentication, and security. On a single machine, application developers can rely on the local operating system to provide these abstractions. In the wide area, however, application developers are forced to build these abstractions themselves or to do without. This ad-hoc approach often results in individual programmers implementing non-optimal solutions, wasting both programmer effort and system resources. To address these problems, we are building a system, WebOS, that provides basic operating systems services needed to build applications that are geographically distributed, highly available, incrementally scalable, and dynamically reconfigurable. Experience with a number of applications developed under WebOS indicates that it simplifies system development and improves resource utilization. In particular, we use WebOS to implement Rent-A-Server to provide dynamic replication of overloaded Web services across the wide area in response to client demands. 
262|The Core Legion Object Model|This document describes the core Legion object model. The model specifies the composition and functionality of Legion&#039;s core objects---those objects that cooperate to create, locate, manage, and remove objects from the Legion system. The model reflects the underlying philosophy and objectives of the Legion project. In particular, the object model facilitates a flexible extensible implementation, provides a single persistent name space, grants site autonomy to participating organizations, and scales to millions of sites and trillions of objects. Further, it offers a framework that is well suited to providing mechanisms for high performance, security, fault tolerance, and commerce. 1. This is University of Virginia Computer Science Technical Report CS-95-35. It is located on the Web at ftp://ftp.cs.virginia.edu/pub/techreports/CS-95-35.ps.Z.  The Core Legion Object Model August 30, 1995 2  1 Objectives  The Legion project  1  at the University of Virginia is an attempt to design and bui...
263|Legion: The Next Logical Step Toward a Nationwide Virtual Computer|The coming of giga-bit networks makes possible the realization of a single nationwide virtual computer comprised of a variety of geographically distributed high-performance machines and workstations. To realize the potential that the physical infrastructure provides, software must be developed that is easy to use, supports large degrees of parallelism in applications code, and manages the complexity of the underlying physical system for the user. This paper describes our approach to constructing and exploiting such &#034;metasystems&#034;. Our approach inherits features of earlier work on parallel processing systems and heterogeneous distributed computing systems. In particular, we are building on Mentat, an object-oriented parallel processing system developed at the University of Virginia. This report is a preliminary document. We expect changes to occur as the architecture and design of the system mature. 
264|An architecture for practical delegation a distributed system|Delegation is the process whereby a user in a distributed environ-ment authorizes a system to access remote resources on his behalf. In today’s distributed systems where all the resources required to carry out an operation are rarely local to the system to which the user is logged in, delegation is more often the rule than the exception. Yet, even with the use of state-of-the-art authentica-tion techniques, delegation is typically implicit and transparent to the remote system controlling the resources, making it difficult for that system to determine whether delegation was authorized by the user. This paper describes a practical technique for delegation that provides both cryptographic assurance that a delegation was authorized, and authentication of the delegated systems, thereby allowing reliable access control as well as precise auditing of the systems involved in every access. It goes further than other ap-proaches for delegation in that it also provides termination of a delegation on demand (as when the user logs out) with the as-surance that the delegated systems, if subsequently compromised, cannot continue to act on the user’s behalf. Delegation and revo-cation are provided by a simple mechanism that does not rely on online trusted servers. 1
265|The CRISIS Wide Area Security Architecture|This paper presents the design and implementation of a new authentication and access control system, called CRISIS. A goal of CRISIS is to explore the systematic application of a number of design principles to building highly secure systems, including: redundancy to eliminate single points of attack, caching to improve performance and availability over slow and unreliable wide area networks, fine-grained capabilities and roles to enable lightweight control of privilege, and complete local logging of all evidence used to make each access control decision. Measurements of a prototype CRISIS-enabled wide area file system show that in the common case CRISIS adds only marginal overhead relative to unprotected wide area accesses. 1 Introduction One of the promises of the Internet is to enable a new class of distributed applications that benefit from a seamless interface to global data and computational resources. A major obstacle to enabling such applications is the lack of a general, cohere...
266|A scalable, deployable directory service framework for the internet|This paper describes a directory service framework for the Internet that fits within the approach outlined in the IETF’s RFC 1588. This framework consists of a global directory service that enables virtually any local directory service to operate under it. We also include an optimized local directory service, thereby providing a complete solution for Internet directory service. Our approach uses proven Internet technology (e.g., the Domain Name System and Uniform Resource Locators) and successful or promising pieces of other services (e.g., X.500 and WHOIS++). Previous attempts to create a unified Internet directory service, such as X.500, LDAP, WHOIS++, and SOLO, have not been fully accepted because of difficulties in implementation and deployment. Therefore, we designed our approach with ease of implementation and deployment in mind. To that end, our approach attempts to co-opt the installed base making a switch to the new service as seamless as possible.
267|A Use-Condition Centered Approach to Authenticated Global Capabilities: Security Architectures for Large-Scale Distributed Collaboratory Environments|We are developing a security model and architecture that is intended to provide general, scalable, and effective security services in open and highly distributed network environments. Our objective is to provide, especially for on-line scientific instrument systems, the same level of, and expressiveness of, access control that is available to a local human controller of information and facilities, and the same authority, delegation, individual responsibility and accountability, and expressiveness of policy that one sees in specific environments in scientific organizations. Our model is based on a public-key infrastructure and cryptographically signed certificates that encode use-conditions that are defined by those directly responsible for a resource. Certificates that encode user characteristics that satisfy the use-conditions are supplied by those who can attest to the characteristic. The collection of certificates specifying use-conditions and their satisfaction are combined with on...
268|Credential Management and Secure Single Login for SPKM |The GSS-API [20, 21] offers security services independent of underlying mechanisms. A possible GSS-mechanism is the Simple Public Key Mechanism (SPKM) specified in [1]. In this paper we will focus on the credential management for SPKM. If more than one connection is needed, the standard credential management requires either to cache the secret keys in insecure storage or to make the user entering a password to access the long term secret keys for every new GSS-connection. For environments in which neither one is acceptable we propose a Secure Single Login (SSLogin) variant which works with temporary asymmetric keys and combines security and user comfort.
269|AES Data Encryption in a ZigBee network:|All in-text	references	underlined	in	blue	are	linked	to	publications	on	ResearchGate, letting you	access	and	read	them	immediately.
270|State of the Art in Ultra-Low Power Public Key Cryptography for Wireless Sensor Networks|Security in wireless sensor networks is currently provided exclusively through symmetric key cryptography. In this paper we show that special purpose ultra-low power hardware implementations of public key algorithms can be used on sensor nodes. The reduced protocol overhead due to public key cryptography (PKC) translates into less packet transmissions and hence, power savings. We provide an indepth comparison of three popular public key implementations and describe how four fundamental security services benefit from PKC. 1.
271|A highly regular and scalable AES hardware architecture|Abstract—This article presents a highly regular and scalable AES hardware architecture, suited for full-custom as well as for semi-custom design flows. Contrary to other publications, a complete architecture (even including CBC mode) that is scalable in terms of throughput and in terms of the used key size is described. Similarities of encryption and decryption are utilized to provide a high level of performance using only a relatively small area (10,799 gate equivalents for the standard configuration). This performance is reached by balancing the combinational paths of the design. No other published AES hardware architecture provides similar balancing or a comparable regularity. Implementations of the fastest configuration of the architecture provide a throughput of 241 Mbits/sec on a 0.6 m CMOS process using standard cells.
272|Area-throughput trade-offs for fully pipelined 30 to 70 Gbits/s AES processors |Abstract—This paper explores the area-throughput trade-off for an ASIC implementation of the Advanced Encryption Standard (AES). Different pipelined implementations of the AES algorithm as well as the design decisions and the area optimizations that lead to a low area and high throughput AES encryption processor are presented. With loop unrolling and outer-round pipelining techniques, throughputs of 30 Gbits/s to 70 Gbits/s are achievable in a 0.18- m CMOS technology. Moreover, by pipelining the composite field implementation of the byte substitution phase of the AES algorithm (inner-round pipelining), the area consumption is reduced up to 35 percent. By designing an offline key scheduling unit for the AES processor the area cost is further reduced by 28 percent, which results in a total reduction of 48 percent while the same throughput is maintained. Therefore, the over 30 Gbits/s, fully pipelined AES processor operating in the counter mode of operation can be used for the encryption of data on optical links. Index Terms—Advanced Encryption Standard (AES), cryptography, crypto-processor, security, hardware architectures, ASIC, VLSI. 1
273|Analysis of AES Hardware Implementations|Following paper examines hardware implementation methods regarding Advanced Encryption Standard (AES). Compared to software implementation, migrating to hardware provides higher level of security and faster encryption speed. An overview of existing AES hardware implementation techniques are summarized. Then the direction of recongurable coprocessor as a cryptography hardware is proposed.
274|Language-Based Information-Flow Security|Current standard security practices do not provide substantial assurance that the end-to-end behavior of a computing system satisfies important security policies such as confidentiality. An end-to-end confidentiality policy might assert that secret input data cannot be inferred by an attacker through the attacker&#039;s observations of system output; this policy regulates information flow.
275|On the Security of Public Key Protocols|Recently the use of public key encryption to provide secure network communication has received considerable attention. Such public key systems are usually effective against passive eavesdroppers, who merely tap the lines and try to decipher the message. It has been pointed out, however, that an improperly designed protocol could be vulnerable to an active saboteur, one who may impersonate another user or alter the message being transmitted. Several models are formulated in which the security of protocols can be discussed precisely. Algorithms and characteri-zations that can be used to determine protocol security in these models are given. 
276|Proof-Carrying Code|This paper describes proof-carrying code (PCC), a mechanism by which a host system can determine with certainty that it is safe to execute a program supplied (possibly in binary form) by an untrusted source. For this to be possible, the untrusted code producer must supply with the code a safety proof that attests to the code&#039;s adherence to a previously defined safety policy. The host can then easily and quickly validate the proof without using cryptography and without consulting any external agents. In order to gain preliminary experience with PCC, we have performed several case studies. We show in this paper how proof-carrying code might be used to develop safe assembly-language extensions of ML programs. In the context of this case study, we present and prove the adequacy of concrete representations for the safety policy, the safety proofs, and the proof validation. Finally, we briefly discuss how we use proof-carrying code to develop network packet filters that are faster than similar filters developed using other techniques and are formally guaranteed to be safe with respect to a given operating system safety policy. 
277|Efficient Software-Based Fault Isolation|One way to provide fault isolation among cooperating software modules is to place each in its own address space. However, for tightly-coupled modules, this solution incurs prohibitive context switch overhead. In this paper, we present a software approach to implementing fault isolation within a single address space. Our approach has two parts. First, we load the code and data for a distrusted module into its own fault domain, a logically separate portion of the application&#039;s address space. Second, we modify the object code of a distrusted module to prevent it from writing or jumping to an address outside its fault domain. Both these software operations are portable and programming language independent. Our approach poses a tradeo relative to hardware fault isolation: substantially faster communication between fault domains, at a cost of slightly increased execution time for distrusted modules. We demonstrate that for frequently communicating modules, implementing fault isolation in software rather than hardware can substantially improve end-to-end application performance.
279|JFlow: Practical Mostly-Static Information Flow Control|A promising technique for protecting privacy and integrity of sensitive data is to statically check information flow within programs that manipulate the data. While previous work has proposed programming language extensions to allow this static checking, the resulting languages are too restrictive for practical use and have not been implemented. In this paper, we describe the new language JFlow, an extension to the Java language that adds statically-checked information flow annotations. JFlow provides several new features that make information flow checking more flexible and convenient than in previous models: a decentralized label model, label polymorphism, run-time label checking, and automatic label inference. JFlow also supports many language features that have never been integrated successfully with static information flow control, including objects, subclassing, dynamic type tests, access control, and exceptions. This paper defines the JFlow language and presents formal rules tha...
280|A SOUND TYPE SYSTEM FOR SECURE FLOW ANALYSIS|Ensuring secure information ow within programs in the context of multiple sensitivity levels has been widely studied. Especially noteworthy is Denning&#039;s work in secure ow analysis and the lattice model [6][7]. Until now, however, the soundness of Denning&#039;s analysis has not been established satisfactorily. Weformulate Denning&#039;s approach as a type system and present a notion of soundness for the system that can be viewed as a form of noninterference. Soundness is established by proving, with respect to a standard programming language semantics, that all well-typed programs have this noninterference property.
281|Programming Semantics for Multiprogrammed Computations|... between an assembly language and an advanced algebraic language.  
282|Secrecy by Typing in Security Protocols|We develop principles and rules for achieving secrecy properties in security protocols. Our approach is based on traditional classification techniques, and extends those techniques to handle concurrent processes that use shared-key cryptography. The rules have the form of typing rules for a basic concurrent language with cryptographic primitives, the spi calculus. They guarantee that, if a protocol typechecks, then it does not leak its secret inputs.
283|The slam calculus: programming with secrecy and integrity|The SLam calculus is a typed ?-calculus that maintains security information as well as type information. The type system propagates security information for each object in four forms: the object’s creators and readers, and the object’s indirect creators and readers (i.e., those agents who, through flow-of-control or the actions of other agents, can influence or be influenced by the content of the object). We prove that the type system prevents security violations and give some examples of its power. 1
284|A Core Calculus of Dependency|Notions of program dependency arise in many settings: security, partial evaluation, program slicing, and call-tracking. We argue that there is a central notion of dependency common to these settings that can be captured within a single calculus, the Dependency Core Calculus (DCC), a small extension of Moggi&#039;s computational lambda calculus. To establish this thesis, we translate typed calculi for secure information flow, binding-time analysis, slicing, and call-tracking into DCC. The translations help clarify aspects of the source calculi. We also define a semantic model for DCC and use it to give simple proofs of noninterference results for each case. 
285|SASI Enforcement of Security Policies: A Retrospective|  SASI enforces security policies by modifying object code for a target system before that system is executed. The approach has been prototyped for two rather different machine architectures: Intel x86 and Java JVML. Details of these prototypes and some generalizations about the SASI approach are discussed.
286|Secure information flow in a multi-threaded imperative language|Previously, we developed a type system to ensure secure information flow in a sequential, imperative programming language [VSI96]. Program variables are classified as either high or low security; intuitively, we wish to prevent information from flowing from high variables to low variables. Here, we extend the analysis to deal with a multithreaded language. We show that the previous type system is insufficient to ensure a desirable security property called noninterference. Noninterference basically means that the final values of low variables are independent of the initial values of high variables. By modifying the sequential type system, we are able to guarantee noninterference for concurrent programs. Crucial to this result, however, is the use of purely nondeterministic thread scheduling. Since implementing such scheduling is problematic, we also show how a more restrictive type system can guarantee noninterference, given a more deterministic (and easily implementable) scheduling policy, such as round-robin time slicing. Finally, we consider the consequences of adding a clock to the language.  
287|Transforming out Timing Leaks|One aspect of security in mobile code is privacy: private (or secret)  data should not be leaked to unauthorised agents. Most of the work  on secure information flow has until recently only been concerned with  detecting direct and indirect flows. Secret information can however be  leaked to the attacker also through covert channels. It is very reasonable  to assume that the attacker, even as an external observer, can  monitor the timing (including termination) behaviour of the program.  Thus to claim a program secure, the security analysis must take also  these into account.  In this work we present a surprisingly simple solution to the problem  of detecting timing leakages to external observers. Our system  consists of a type system in which well-typed programs do not leak  secret information directly, indirectly or through timing, and a transformation  for removing timing leakages. For any program that is well  typed according to Volpano and Smith [VS97a], our transformation  generates a program that is also free of timing leaks.   
288|A General Theory of Composition for Trace Sets Closed Under Selective Interleaving Functions|This paper presents a general theory of system composition for &#034;possibilistic&#034; security properties. We see that these properties fall outside of the AlpernSchneider safety/liveness domain and hence, are not subject to the Abadi-Lamport Composition Principle. We then introduce a set of trace constructors called selective interleaving functions and show that possibilistic security properties are closure properties with respect to different classes of selective interleaving functions. This provides a uniform framework for analyzing these properties and allows us to construct a partial ordering for them. We present a number of composition constructs, show the extent to which each preserves closure with respect to different classes of selective interleaving functions, and show that they are sufficient for forming the general hook-up construction. We see that although closure under a class of selective interleaving functions is generally preserved by product and cascading, it is not generall...
289|Robust Declassification|Security properties based on information flow, such as noninterference, provide strong guarantees that confidentiality is maintained. However, programs often need to leak some amount of confidential information in order to serve their intended purpose, and thus violate noninterference. Real systems that control information flow often include mechanisms for downgrading or declassifying information; however, declassification can easily result in the unexpected release of confidential information.
290|Flexible Policy-Directed Code Safety|This work introduces a new approach to code safety. We present Naccio, a system architecture that allows a large class of safety policies to be expressed in a general and platform-independent way. Policies are defined in terms of abstract resource manipulations. We describe mechanisms that can be used to efficiently and conveniently enforce these safety policies by transforming programs. We are developing implementations of Naccio that enforce policies on JavaVM classes and Win32 executables. We report on results using the JavaVM prototype. 1
291|A type-based approach to pro-gram security|Abstract. This paper presents a type system which guarantees that well-typed programs in a procedural programming language satisfy a noninterference security property. With all program inputs and outputs classified at various security levels, the property basically states that a program output, classified at some level, can never change as a result of modifying only inputs classified at higher levels. Intuitively, this means the program does not “leak ” sensitive data. The property is similar to a notion introduced years ago by Goguen and Meseguer to model security in multi-level computer systems [7]. We also give an algorithm for inferring and simplifying principal types, which document the security requirements of programs. 1
292|Probabilistic Noninterference for Multi-threaded Programs|We present a probability-sensitive confidentiality specification -- a form of probabilistic noninterference -- for a small multi-threaded programming language with dynamic thread creation. Probabilistic covert channels arise from a scheduler which is probabilistic. Since scheduling policy is typically outside the language specification for multithreaded languages, we describe how to generalise the security condition in order to define robust security with respect to a wide class of schedulers, not excluding the possibility of deterministic (e.g., round-robin) schedulers and program-controlled thread priorities. The formulation is based on an adaptation of Larsen and Skou&#039;s notion of probabilistic bisimulation. We show how the security condition satisfies compositionality properties which facilitate straightforward proofs of correctness for, e.g., security type systems. We illustrate this by defining a security type system which improves on previous multi-threaded systems, and by provin...
293|Secure Information Flow and Pointer Confinement in a Java-like Language|We consider a sequential object-oriented language with pointers and mutable state, private fields and classbased visibility, dynamic binding and inheritance, recursive classes, casts and type tests, and recursive methods. Programs are annotated with security levels, constrained by security typing rules. A noninterference theorem shows how the rules ensure pointer confinement and secure information flow.
294|Probabilistic noninterference in a concurrent language|In [15], we give a type system that guarantees that well-typed multi-threaded programs are possibilistically noninterfering. If thread scheduling is probabilistic, however, then well-typed programs may have probabilistic timing channels. We describe how they can be eliminated without making the type system more restrictive. We show that well-typed concurrent programs are probabilistically noninterfering if every total command with a high guard executes atomically. The proof uses the concept of a probabilistic state of a computation, following the work of Kozen [10]. 
295|CSP and determinism in security modelling|We show how a variety of confidentiality properties can be expressed in terms of the abstraction mechanisms that CSP provides. We argue that determinism of the abstracted low-security viewpoint provides the best type of property. By changing the form of abstraction mechanism we are able to model di#erent assumptions about how systems behave, including handling the distinction between input and output actions. A detailed analysis of the nature of nondeterminism shows why certain security properties have had the paradoxical property of not being preserved by refinement -- a disadvantage not shared by the determinism-based conditions. Finally we give an e#cient algorithm for testing the determinism properties on a model-checker.
296|A Per Model of Secure Information Flow in Sequential Programs|This paper proposes an extensional semantics-based formal specification of secure information-flow properties in sequential programs based on representing degrees of security by partial equivalence relations (pers). The specification clarifies and unifies a number of specific correctness arguments in the literature and connections to other forms of program analysis. The approach is inspired by (and in the deterministic case equivalent to) the use of partial equivalence relations in specifying binding-time analysis, and is thus able to specify security properties of higher-order functions and &#034;partially confidential data&#034;. We also show how the per approach can handle nondeterminism for a first-order language, by using powerdomain semantics and show how probabilistic security properties can be formalised by using probabilistic powerdomain semantics. We illustrate the usefulness of the compositional nature of the security specifications by presenting a straightforward correctness proof for a simple type-based security analysis. 
297|Protection|The following paper by Butler Lampson has been frequently referenced. Because the original is not widely available, we are reprinting it here. If the paper is referenced in published work,
298|Approximate Non-Interference|We address the problem of characterising the security of a program against unauthorised information flows. Classical approaches are based on non-interference models which depend ultimately on the notion of process equivalence. In these models confidentiality is an absolute property stating the absence of any illegal information flow. We present a model in which the notion of non-interference is approximated in the sense that it allows for some exactly quantified leakage of information. This is characterised via a notion of process similarity which replaces the indistinguishability of processes by a quantitative measure of their behavioural difference. Such a quantity is related to the number of statistical tests needed to distinguish two behaviours. We also present two semantics-based analyses of approximate non-interference and we show that one is a correct abstraction of the other.
299|A Language-Based Approach to Security|Language-based security leverages program analysis and program rewriting to enforce security policies. The approach promises efficient enforcement of fine-grained access control policies and depends on a trusted computing base of only modest size. This paper surveys progress and prospects for the area, giving overviews of in-lined reference monitors,  certifying compilers, and advances in type theory.
300|Eliminating Covert Flows with Minimum Typings|A type system is given that eliminates two kinds of covert flows in an imperative programming language. The first kind arises from nontermination and the other from partial operations that can raise exceptions. The key idea is to limit the source of nontermination in the language to constructs with minimum typings, and to evaluate partial operations within expressions of try commands which also have minimum typings. A mutual progress theorem is proved that basically states that no two executions of a well-typed program can be distinguished on the basis of nontermination versus abnormal termination due to a partial operation. The proof uses a new style of programming language semantics which we call a natural transition semantics.
301|A Uniform Type Structure for Secure Information Flow|The \pi-calculus is a formalism of computing in which we can compositionally represent dynamics of major programming constructs by decomposing them into a single communication primitive, the name passing. This work reports our experience in using a linear/affine typed \pi-calculus for the analysis and development of type systems of programming languages, focussing on secure information flow analysis. After presenting a basic typed calculus for secrecy, we demonstrate its usage by a sound embedding of the dependency core calculus (DCC) and by the development of a novel type discipline for imperative programs which extends both a secure multi-threaded imperative language by Smith and Volpano and (a call-by-value version of) DCC. In each case, the embedding gives a simple proof of noninterference.
302|A Semantic Approach to Secure Information Flow|A classic problem in security is the problem of determining whether a given program has  secure information flow. Informally, this problem may be described as follows: Given a program operating on public and private variables, check whether observations of the public variables before and after execution reveal any information about the initial values of the private variables. Although the problem has been studied for several decades, most of the previous approaches have been syntactic in nature, often using type systems and compiler data flow analysis techniques to analyze program texts. This paper presents a considerably different approach to checking secure information flow, based on a semantic characterization of security. A semantic approach has several desirable features. Firstly, it gives a more precise characterization of security than that possible by conservative methods based on type systems. Secondly, it applies to any programming constructs whose semantics are definable; fo...
303|Information Flow Inference For Free|This paper shows how to systematically extend an arbitrary type system with dependency information, and how soundness and non-interference proofs for the new system may rely upon, rather than duplicate, the soundness proof of the original system. This allows enriching virtually any of the type systems known today with information ow analysis, while requiring only a minimal proof eort.
304|A New Type System for Secure Information Flow|With the variables of a program classified as L (low, public) or H (high, private), we wish to prevent the program from leaking information about H variables into L variables. Given a multi-threaded imperative language with probabilistic scheduling, the goal can be formalized as a property called probabilistic noninterference. Previous work identified a type system sufficient to guarantee probabilistic noninterference, but at the cost of severe restrictions: to prevent timing leaks, H variables were disallowed from the guards of while loops. Here we present a new type system that gives each command a type of the form  1 cmd  2 ; this type says that the command assigns only to variables of level  1 (or higher) and has running time that depends only on variables of level  2 (or lower). Also we use types of the form  cmd n for commands that terminate in exactly n steps. With these typings, we can prevent timing leaks by demanding that no assignment to an L variable may sequentially follow a command whose running time depends on H variables. As a result, we can use H variables more flexibly; for example, under the new system a thread that involves only H variables is always well typed. The soundness of the type system is proved using the notion of probabilistic bisimulation.
305|Untrusted Hosts and Confidentiality: Secure Program Partitioning|This paper presents secure program partitioning, a language-based technique for protecting confidential data during computation in distributed systems containing mutually untrusted hosts. Confidentiality and integrity policies can be expressed by annotating programs with security types that constrain information flow; these programs can then be partitioned automatically to run securely on heterogeneously trusted hosts. The resulting communicating subprograms collectively implement the original program, yet the system as a whole satisfies the security requirements of participating principals without requiring a universally trusted host machine. The experience in applying this methodology and the performance of the resulting distributed code suggest that this is a promising way to obtain secure distributed computation. 
306|Secrecy Types for Asymmetric Communication|We develop a typed process calculus for security protocols in which types convey secrecy properties. We focus on asymmetric communication primitives, especially on public-key encryption. These present special difficulties, partly because they rely on related capabilities (e.g., &#034;public&#034; and &#034;private&#034; keys) with different levels of secrecy and scopes.
307|Quantitative Analysis of the Leakage of Confidential Data|Basic information theory is used to analyse the amount of confidential information which may be leaked by programs written in a very simple imperative language. In particular, a detailed analysis is given of the possible leakage due to equality tests and if statements. The analysis is presented as a set of syntax-directed inference rules and can readily be automated.  
308|Secure Information Flow as Typed Process Behaviour|We propose a new type discipline for the -calculus in which secure information  ow is guaranteed by static type checking. Secrecy levels are assigned to channels and are  controlled by subtyping. A behavioural notion of types capturing causality of actions plays an  essential role for ensuring safe information ow in diverse interactive behaviours, making the  calculus powerful enough to embed known calculi for type-based security. The paper introduces  the core part of the calculus, presents its basic syntactic properties, and illustrates its use as  a tool for programming language analysis by a sound embedding of a secure multi-threaded  imperative calculus of Volpano and Smith. The embedding leads to a practically meaningful  extension of their original type discipline.
309|Proving Noninterference and Functional Correctness Using Traces|this paper we advocate showing that an abstract functional specification satisfies Noninterference directly and then showing that the program satisfies the functional specification. By being carried out on as abstract a level as possible, our security proof can survive implementation changes that do not affect the user interface to the system.
310|Verifying secrets and rela-tive secrecy|Systems that authenticate a user based on a shared secret (such as a password or PIN) normally allow anyone to query whether the secret is a given value. For example, an ATM machine allows one to ask whether a string is the secret PIN of a (lost or stolen) ATM card. Yet such queries are prohibited in any model whose programs satisfy an informationflow property like Noninterference. But there is complexitybased justification for allowing these queries. A type system is given that provides the access control needed to prove that no well-typed program can leak secrets in polynomial time, or even leak them with nonnegligible probability if secrets are of sufficient length and randomly chosen. However, there are well-typed deterministic programs in a synchronous concurrent model capable of leaking secrets in linear time. 1
311|Trust in the ?-Calculus|This paper introduces trust analysis for higher-order languages. Trust analysis encourages the programmer to make explicit the trustworthiness of data, and in return it can guarantee that no mistakes with respect to trust will  be made at run-time. We present a confluent ?-calculus with explicit trust operations, and we equip it with a trust-type system which has the subject reduction property. Trust information in presented as two annotations of each function type constructor, and type inference is computable in O(n³) time.
312|Programming Languages for Information Security|Our society’s widespread dependence on networked information systems for everything from personal finance to military communications makes it essential to improve the security of software. Standard security mechanisms such as access control and encryption are essential components for protecting information, but they do not provide end-to-end guarantees. Programming-languages research has demonstrated that security concerns can be addressed by using both program analysis and program rewriting as powerful and flexible enforcement mechanisms. This thesis investigates security-typed programming languages, which use static typing to enforce information-flow security policies. These languages allow the programmer to specify confidentiality and integrity constraints on the data used in a program; the compiler verifies that the program satisfies the constraints. Previous theoretical security-typed languages research has focused on simple models of computation and unrealistically idealized security policies. The existing practical security-typed languages have not been proved to guarantee security. This thesis addresses these limitations in several ways.
313|Static Confidentiality Enforcement for Distributed Programs|Preserving the con  dentiality of data in a distributed system  is an increasingly important problem of current security research.
314|Logical Relations for Encryption|The theory of relational parametricity and its logical relations proof technique are powerful tools for  reasoning about information hiding in the polymorphic -calculus. We investigate the application  of these tools in the security domain by defining a cryptographic -calculus---an extension of the  standard simply typed -calculus with primitives for encryption, decryption, and key generation---  and introducing syntactic logical relations (in the style of Pitts and Birkedal-Harper) for this calculus  that can be used to prove behavioral equivalences between programs that use encryption. We illustrate
315|Safety versus Secrecy|. Safety and secrecy are formulated for a deterministic programming  language. A safety property is defined as a set of program  traces and secrecy is defined as a binary relation on traces, characterizing  a form of Noninterference. Safety properties may have sound and  complete execution monitors whereas secrecy has no such monitor.  1 Introduction  It is often argued that information flow is not safety. One argument is refinement based and originates with Gray and McLean [5]. They observed that for nondeterministic systems, a class of information flow properties, namely the Possibilistic Noninterference properties, are not safety properties. The reason is because they are not preserved under replacement of nondeterminism in a system with determinism. An example is an implementation of nondeterministic scheduling using a round-robin time-sliced scheduler [8]. A possibilistic property basically asserts that certain system inputs do not interfere with the possibility of certain events....
316|Noninterference for Concurrent Programs  |We propose a type system to ensure the property of noninterference in a system of concurrent programs, described in a standard imperative language extended with parallelism. Our proposal is in the line of some recent work by Irvine, Volpano and Smith. Our type system, as well as our semantics for concurrent programs, seem more natural and less restrictive than those originally presented by these authors. Moreover, we show how to extend the language in order to formalise scheduling policies. The type system is extended to the new constructs, and we show that noninterference still holds, while remaining in a nonprobabilistic setting.
317|Secure Information Flow via Linear Continuations|Security-typed languages enforce secrecy or integrity policies by type-checking. This paper investigates continuation-passing style (CPS) as a means of proving that such languages enforce noninterference and as a  rst step towards understanding their compilation. We present a low-level, secure calculus with higher-order, imperative features and linear continuations.
318|Secure Information Flow and CPS|Security-typed languages enforce secrecy or integrity policies  by type checking. This paper investigates continuation-passing style as  a means of proving that such languages enforce non-interference and as  a first step towards understanding their compilation. We present a lowlevel,  secure calculus with higher-order, imperative features. Our type  system makes novel use of ordered linear continuations.
319|Compile-Time Detection of Information Flow in Sequential Programs|We give a formal definition of the notion of information flow for a simple guarded command  language. We propose an axiomatisation of security properties based on this notion  of information flow and we prove its soundness with respect to the operational semantics of the language. We then identify the sources of non determinism in proofs and we derive in successive steps an inference algorithm which is both sound and complete with respect to the inference system. 
320|A Security Flow Control Algorithm and Its Denotational Semantics Correctness Proof|We derive a security flow control algorithm for message-based, modular systems and prove the algorithm correct. The development is noteworthy because it is completely rigorous: the flow control algorithm is derived as an abstract interpretation of the dentotational semantics of the programming language for the modular system, and the correctness proof is a proof by logical relations of the congruence between the denotational semantics and its abstract interpretation. Effectiveness is also addressed: we give conditions under which an abstract interpretation can be computed as a traditional iterative data flow analysis, and we prove that our security flow control algorithm satisfies the conditions. We also show that symbolic expressions (that is, data flow values that contain unknowns) can be used in a convergent, iterative analysis. An important consequence of the latter result is that the security flow control algorithm can analyze individual modules in a system for well formedness and...
322|Confidentiality for Mobile Code: The Case of a Simple Payment Protocol|We propose an approach to support confidentiality for mobile implementations of security-sensitive protocols using Java/JVM. An applet which receives and passes on confidential information onto a public network has a rich set of direct and indirect channels available to it. The problem is to constrain applet behaviour to prevent those leakages that are unintended while preserving those that are specified in the protocol. We use an approach based on the idea of correlating changes in observable behaviour with changes in input. In the special case where no changes in (low) behaviour are possible we retrieve a version of noninterference. Mapping our approach to JVM a number of particular concerns need to be addressed, including the use of object libraries for IO, the use of labelling to track input/output of secrets, and the choice of proof strategy. We use the bisimulation proof technique. To provide user feedback we employ a variant of proof-carrying code to instrument a security assist...
323|A Simple View of Type-Secure Information Flow in the &amp;pi;-Calculus|One way of enforcing a mandatory access control policy is to use a static type system capable of guaranteeing a non-interference property. Non-interference requires that two processes with distinct &#034;high&#034;-level components, but common &#034;low&#034;-level structure, cannot be distinguished by &#034;low&#034;-level observers. We state this property in terms of a rather strict notion of process equivalence, namely weak barbed reduction congruence.
324|Partial Evaluation and Non-Interference for Object Calculi|We prove the correctness of a multi-level partial evaluator and of an information flow analysis for Abadi and Cardelli&#039;s FOb 1: , a simply typed object calculus with function and object types, object subtyping and subsumption.
325|Information Flow Control In A Distributed Object-Oriented System With Statically Bound Object Variables|this paper is on the manner in which information flow policies can be enforced in an object-oriented distributed environment. Since the various objects of program may be geographically distributed or constructed at different points in time, it may not be feasible for one object to access protection information of other objects. Instead, it is desirable to establish the &#034;internal&#034; information flow security of each procedure in an object independent of other procedures and other objects. Since this does not account for inter-object flow of information, the security of a program must be partially certified at run time. To be useful, this certification must also be efficient. The method presented in this paper is a combined approach of compile-time and runtime information flow certification. The compile-time mechanism establishes the internal security of individual procedures and creates the necessary information structures to allow for efficient run-time certification of inter-object communication. The run-time mechanism completes the certification of the entire program at message passing time by verifying the information flow caused by procedure invocations.
326|Can you Trust your Data?|A new program analysis is presented, and two compile time methods for this analysis are given. The analysis attempts to answer the question: &#034;Given some trustworthy and some untrustworthy input, can we trust the value of a given variable after execution of some code&#034;. The analyses are based on an abstract interpretation framework and a constraint generation framework respectively. The analyses are proved safe with respect to an instrumented semantics. We explicitly deal with a language with pointers and possible aliasing problems. The constraint based analysis is related directly to the abstract interpretation and therefore indirectly to the instrumented semantics    1 
327|A Least Fixed Point Approach To Inter-Procedural Information Flow Control|This paper expands earlier work; it presents an information flow certification mechanism which combines compile-time, link-time and run-time algorithms. In this approach, a user can specify the security class of each variable either to remain constant or to change during execution. The compile time algorithm is basically the same as the one presented in [8]
328|Trust and Dependence Analysis|The two pillars of trust analysis and dependence algebra form the foundation of this thesis. Trust analysis is a static analysis of the run-time trustworthiness of data. Dependence algebra is a rich abstract model of data dependences in programming languages, applicable to several kinds of analyses. We have
329|An Approach to Information Security in Distributed Systems|Information flow control mechanisms detect and prevent illegal transfers of information within a computer system. In this paper, we give an overview of a programming language based approach to information flow control in a system of communicating processes. The language chosen to present the approach is CSP. We give the security semantics of CSP and show, with the aid of examples, how the semantics can be used to conduct both manual and automated security proofs of application programs.  1 Introduction  One of the most noteworthy trends in modern computing is the proliferation of automated systems to commercial, administrative and other fields. The amount of sensitive information being stored in computer systems has increased as a consequence and with that, the seriousness of an attack on a system where a user succeeds in illegally gaining access to information in the system. Distributed computing has excacerbated this problem since information can be accessed over wide geographical di...
332|Dynamic Typing in a Statically Typed Language |Statically typed programming languages allow earlier error checking, better enforcement of disciplined programming styles, and generation of more e cient object code than languages where all type consistency checks are performed at run time. However, even in statically typed languages, there is often the need to deal with data whose type cannot be determined at compile time. To handle such situations safely, we propose to add a type Dynamic whose values are pairs of a value v andatype tag T where v has the type denoted by T. Instances of Dynamic are built with an explicit tagging construct and inspected with a type safe typecase construct. This paper explores the syntax, operational semantics, and denotational semantics of a simple language including the type Dynamic. Wegive examples of how dynamically typed values can be used in programming. Then we discuss an operational semantics for our language and obtain a soundness theorem. We present two formulations of the denotational semantics of this language and relate them to the operational semantics. Finally,we consider the implications of polymorphism and some implementation issues.
333|Typeful programming|There exists an identifiable programming style based on the widespread use of type information handled through mechanical typechecking techniques. This typeful programming style is in a sense independent of the language it is embedded in; it adapts equally well to functional, imperative, object-oriented, and algebraic programming, and it is not incompatible with relational and concurrent programming. The main purpose of this paper is to show how typeful programming is best supported by sophisticated type systems, and how these systems can help in clarifying programming issues and in adding power and regularity to languages. We start with an introduction to the notions of types, subtypes and polymorphism. Then we introduce a general framework, derived in part from constructive logic, into which most of the known type systems can be accommodated and extended. The main part of the paper shows how this framework can be adapted systematically to cope with actual programming constructs. For concreteness we describe a particular programming language with advanced features; the emphasis here is on the combination of subtyping and polymorphism. We then discuss how typing concepts apply to large programs, made of collections of modules, and very large programs, made of collections of large programs. We also sketch how typing applies to system programming; an area which by nature escapes rigid typing. In summary, we compare the most common programming styles, suggesting that many of them are compatible with, and benefit from, a typeful discipline.
334|Complete, Safe Information Flow with Decentralized Labels|The growing use of mobile code in downloaded applications and servlets has increased interest in robust mechanisms for ensuring privacy and secrecy. Information flow control is intended to directly address privacy and secrecy concerns, but most information flow models are too restrictive to be widely used. The decentralized label model is a new information flow model that extends traditional models with per-principal information flow policies and also permits a safe form of declassification. This paper extends this new model further, making it more flexible and expressive. We define a new formal semantics for decentralized labels and a corresponding new rule for relabeling data that is both sound and complete. We also show that these extensions preserve the ability to statically check information flow.  1 Introduction  The growing use of mobile code in downloaded applications and servlets has increased interest in robust mechanisms for ensuring privacy and secrecy. A key problem is tha...
335|Mostly-Static Decentralized  Information Flow Control|The growing use of mobile code in downloaded programs such as applets and servlets has increased interest in robust mechanisms for ensuring privacy and secrecy. Common security mechanisms such as sandboxing and access control are either too restrictive or too weak---they prevent applications from sharing data usefully, or allow private information to leak. For example, security mechanisms in Java prevent many useful applications while still permitting Trojan horse applets to leak private information. This thesis describes the decentralized label model, a new model of information flow control that protects private data while allowing applications to share data. Unlike previous approaches to privacy protection based on information flow, this label model is decentralized: it allows cooperative computation by mutually distrusting principals, without mediation by highly trusted agents. Cooperative computation is possible because individual principals can declassify their own data without infringing on other principals&#039; privacy. The decentralized label model permits programs using it to be checked statically, which is important for the precise detection of information leaks. This thesis also
336|Tractable Constraints in Finite Semilattices|. We introduce the notion of definite inequality constraints involving  monotone functions in a finite meet-semilattice, generalizing the  logical notion of Horn-clauses, and we give a linear time algorithm for  deciding satisfiability. We characterize the expressiveness of the framework  of definite constraints and show that the algorithm uniformly solves  exactly the set of all meet-closed relational constraint problems, running  with small linear time constant factors for any fixed problem. We give  an alternative technique which reduces inequalities to satisfiability of  Horn-clauses (hornsat) and study its efficiency. Finally, we show that  the algorithm is complete for a maximal class of tractable constraints,  by proving that any strict extension will lead to NP-hard problems in  any meet-semilattice.  Keywords: Finite semilattices, constraint satisfiability, program analysis, tractability, algorithms.  1 Introduction  Many program analysis problems can be solved by generating a...
337|Wide-area cooperative storage with CFS|The Cooperative File System (CFS) is a new peer-to-peer readonly storage system that provides provable guarantees for the efficiency, robustness, and load-balance of file storage and retrieval. CFS does this with a completely decentralized architecture that can scale to large systems. CFS servers provide a distributed hash table (DHash) for block storage. CFS clients interpret DHash blocks as a file system. DHash distributes and caches blocks at a fine granularity to achieve load balance, uses replication for robustness, and decreases latency with server selection. DHash finds blocks using the Chord location protocol, which operates in time logarithmic in the number of servers. CFS is implemented using the SFS file system toolkit and runs on Linux, OpenBSD, and FreeBSD. Experience on a globally deployed prototype shows that CFS delivers data to clients as fast as FTP. Controlled tests show that CFS is scalable: with 4,096 servers, looking up a block of data involves contacting only seven servers. The tests also demonstrate nearly perfect robustness and unimpaired performance even when as many as half the servers fail.
338|Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications|A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.
339|A Scalable Content-Addressable Network|Hash tables – which map “keys ” onto “values” – are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internet-like scales. The CAN is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation. 
340|Pastry: Scalable, distributed object location and routing for large-scale peer-to-peer systems|This paper presents the design and evaluation of Pastry, a scalable, distributed object location and routing scheme for wide-area peer-to-peer applications. Pastry provides application-level routing and object location in a potentially very large overlay network of nodes connected via the Internet. It can be used to support a wide range of peer-to-peer applications like global data storage, global data sharing, and naming. An insert operation in Pastry stores an object at a user-defined number of diverse nodes within the Pastry network. A lookup operation reliably retrieves a copy of the requested object if one exists. Moreover, a lookup is usually routed to the node nearest the client issuing the lookup (by some measure of proximity), among the nodes storing the requested object. Pastry is completely decentralized, scalable, and self-configuring; it automatically adapts to the arrival, departure and failure of nodes. Experimental results obtained with a prototype implementation on a simulated network of 100,000 nodes confirm Pastry&#039;s scalability, its ability to self-configure and adapt to node failures, and its good network locality properties.
341|Tapestry: An infrastructure for fault-tolerant wide-area location and routing|In today’s chaotic network, data and services are mobile and replicated widely for availability, durability, and locality. Components within this infrastructure interact in rich and complex ways, greatly stressing traditional approaches to name service and routing. This paper explores an alternative to traditional approaches called Tapestry. Tapestry is an overlay location and routing infrastructure that provides location-independent routing of messages directly to the closest copy of an object or service using only point-to-point links and without centralized resources. The routing and directory information within this infrastructure is purely soft state and easily repaired. Tapestry is self-administering, faulttolerant, and resilient under load. This paper presents the architecture and algorithms of Tapestry and explores their advantages through a number of experiments. 1
342|Resilient Overlay Networks|A Resilient Overlay Network (RON) is an architecture that allows distributed Internet applications to detect and recover from path outages and periods of degraded performance within several seconds, improving over today’s wide-area routing protocols that take at least several minutes to recover. A RON is an application-layer overlay on top of the existing Internet routing substrate. The RON nodes monitor the functioning and quality of the Internet paths among themselves, and use this information to decide whether to route packets directly over the Internet or by way of other RON nodes, optimizing application-specific routing metrics. Results from two sets of measurements of a working RON deployed at sites scattered across the Internet demonstrate the benefits of our architecture. For instance, over a 64-hour sampling period in March 2001 across a twelve-node RON, there were 32 significant outages, each lasting over thirty minutes, over the 132 measured paths. RON’s routing mechanism was able to detect, recover, and route around all of them, in less than twenty seconds on average, showing that its methods for fault detection and recovery work well at discovering alternate paths in the Internet. Furthermore, RON was able to improve the loss rate, latency, or throughput perceived by data transfers; for example, about 5 % of the transfers doubled their TCP throughput and 5 % of our transfers saw their loss probability reduced by 0.05. We found that forwarding packets via at most one intermediate RON node is sufficient to overcome faults and improve performance in most cases. These improvements, particularly in the area of fault detection and recovery, demonstrate the benefits of moving some of the control over routing into the hands of end-systems. 
343|Oceanstore: An architecture for global-scale persistent storage|OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development. 1
344|Summary cache: A scalable wide-area web cache sharing protocol|The sharing of caches among Web proxies is an important technique to reduce Web traffic and alleviate network bottlenecks. Nevertheless it is not widely deployed due to the overhead of existing protocols. In this paper we propose a new protocol called &#034;Summary Cache&#034;; each proxy keeps a summary of the URLs of cached documents of each participating proxy and checks these summaries for potential hits before sending any queries. Two factors contribute to the low overhead: the summaries are updated only periodically, and the summary representations are economical -- as low as 8 bits per entry. Using trace-driven simulations and a prototype implementation, we show that compared to the existing Internet Cache Protocol (ICP), Summary Cache reduces the number of inter-cache messages by a factor of 25 to 60, reduces the bandwidth consumption by over 50%, and eliminates between 30 % to 95 % of the CPU overhead, while at the same time maintaining almost the same hit ratio as ICP. Hence Summary Cache enables cache sharing among a large number of proxies.  
345|Storage management and caching in PAST, a large-scale, persistent peer-to-peer storage utility|This paper presents and evaluates the storage management and caching in PAST, a large-scale peer-to-peer persistent storage utility. PAST is based on a self-organizing, Internetbased overlay network of storage nodes that cooperatively route file queries, store multiple replicas of files, and cache additional copies of popular files. In the PAST system, storage nodes and files are each assigned uniformly distributed identifiers, and replicas of a file are stored at nodes whose identifier matches most closely the file’s identifier. This statistical assignment of files to storage nodes approximately balances the number of files stored on each node. However, non-uniform storage node capacities and file sizes require more explicit storage load balancing to permit graceful behavior under high global storage utilization; likewise, non-uniform popularity of files requires caching to minimize fetch distance and to balance the query load. We present and evaluate PAST, with an emphasis on its storage management and caching system. Extensive tracedriven experiments show that the system minimizes fetch distance, that it balances the query load for popular files, and that it displays graceful degradation of performance as the global storage utilization increases beyond 95%.  
346|Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the World Wide Web|We describe a family of caching protocols for distrib-uted networks that can be used to decrease or eliminate the occurrence of hot spots in the network. Our protocols are particularly designed for use with very large networks such as the Internet, where delays caused by hot spots can be severe, and where it is not feasible for every server to have complete information about the current state of the entire network. The protocols are easy to implement using existing network protocols such as TCP/IP, and require very little overhead. The protocols work with local control, make efficient use of existing resources, and scale gracefully as the network grows. Our caching protocols are based on a special kind of hashing that we call consistent hashing. Roughly speaking, a consistent hash function is one which changes minimally as the range of the function changes. Through the development of good consistent hash functions, we are able to develop caching protocols which do not require users to have a current or even consistent view of the network. We believe that consistent hash functions may eventually prove to be useful in other applications such as distributed name servers and/or quorum systems.  
347|Efficient dispersal of information for security, load balancing, and fault tolerance|Abstract. An Information Dispersal Algorithm (IDA) is developed that breaks a file F of length L =  ( F ( into n pieces F,, 1 5 i 5 n, each of length ( F, 1 = L/m, so that every m pieces suffice for reconstructing F. Dispersal and reconstruction are computationally efficient. The sum of the lengths ( F, 1 is (n/m). L. Since n/m can be chosen to be close to I, the IDA is space eflicient. IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communi-cations between processors in parallel computers. For the latter problem provably time-efftcient and highly fault-tolerant routing on the n-cube is achieved, using just constant size buffers. Categories and Subject Descriptors: E.4 [Coding and Information Theory]: nonsecret encoding schemes
348|A Hierarchical Internet Object Cache| This paper discusses the design andperformance  of a hierarchical proxy-cache designed to make Internet information systems scale better. The design was motivated by our earlier trace-driven simulation study of Internet traffic. We believe that the conventional wisdom, that the benefits of hierarchical file caching do not merit the costs, warrants reconsideration in the Internet environment.  The cache implementation supports a highly concurrent stream of requests. We present performance measurements that show that the cache outperforms other popular Internet cache implementations by an order of magnitude under concurrent load. These measurements indicate that hierarchy does not measurably increase access latency. Our software can also be configured as a Web-server accelerator; we present data that our httpd-accelerator is ten times faster than Netscape&#039;s Netsite and NCSA 1.4 servers.  Finally, we relate our experience fitting the cache into the increasingly complex and operational world of Internet information systems, including issues related to security, transparency to cache-unaware clients, and the role of file systems in support of ubiquitous wide-area information systems.  
349|Separating key management from file system security|No secure network file system has ever grown to span the In-ternet. Existing systems all lack adequate key management for security at a global scale. Given the diversity of the In-ternet, any particular mechanism a file system employs to manage keys will fail to support many types of use. We propose separating key management from file system security, letting the world share a single global file system no matter how individuals manage keys. We present SFS, a se-cure file system that avoids internal key management. While other file systems need key management to map file names to encryption keys, SFS file names effectively contain public keys, making them self-certifying pathnames. Key manage-ment in SFS occurs outside of the file system, in whatever procedure users choose to generate file names. Self-certifying pathnames free SFS clients from any notion of administrative realm, making inter-realm file sharing triv-ial. They let users authenticate servers through a number of different techniques. The file namespace doubles as a key certification namespace, so that people can realize many key management schemes using only standard file utilities. Fi-nally, with self-certifying pathnames, people can bootstrap one key management mechanism using another. These prop-erties make SFS more versatile than any file system with built-in key management.  
351|A Toolkit for User-Level File Systems|This paper describes a C toolkit for easily extending the Unix file system. The toolkit exposes the NFS interface, allowing new file systems to be implemented portably at user level. A number of programs have implemented portable, user-level file systems. However, they have been plagued by low-performance, deadlock, restrictions on file system structure, and the need to reboot after software errors. The toolkit makes it easy to avoid the vast majority of these problems. Moreover, the toolkit also supports user-level access to existing file systems through the NFS interface---a heretofore rarely employed technique. NFS gives software an asynchronous, low-level interface to the file system that can greatly benefit the performance, security, and scalability of certain applications. The toolkit uses a new asynchronous I/O library that makes it tractable to build large, event-driven programs that never block.
352|A Taste of Crispy Squid|Distributed proxy caches are in use throughout the world to reduce access latency and bandwidth demands for Internet object transfer. The CRISP project seeks to build more effective distributed Web caches by exploring alternatives to the hierarchical structure and multicast handling of probes common to the most popular distributed Web cache systems. CRISP caches are structured as a collective of autonomous Web proxy servers sharing their cache directories through a common mapping service that can be queried with at most one message exchange. Individual servers may be configured to replicate all or part of the global map in order to balance access cost, overhead and hit ratio, depending on the size and geographic dispersion of the collective cache. We have prototyped several CRISP cache structures in Crispy Squid, an extension to the Squid Internet Object Cache. We are evaluating these cache structures using Proxycizer, a full-featured package for replaying traces of observed request tr...
354|Globus: A Metacomputing Infrastructure Toolkit|Emerging high-performance applications require the ability to exploit diverse, geographically distributed resources. These applications use high-speed networks to integrate supercomputers, large databases, archival storage devices, advanced visualization devices, and/or scientific instruments to form networked virtual supercomputers or metacomputers. While the physical infrastructure to build such systems is becoming widespread, the heterogeneous and dynamic nature of the metacomputing environment poses new challenges for developers of system software, parallel tools, and applications. In this article, we introduce Globus, a system that we are developing to address these challenges. The Globus system is intended to achieve a vertically integrated treatment of application, middleware, and network. A low-level toolkit provides basic mechanisms such as communication, authentication, network information, and data access. These mechanisms are used to construct various higher-level metacomp...
355|A high-performance, portable implementation of the MPI message passing interface standard|MPI (Message Passing Interface) is a specification for a standard library for message passing that was defined by the MPI Forum, a broadly based group of parallel computer vendors, library writers, and applications specialists. Multiple implementations of MPI have been developed. In this paper, we describe MPICH, unique among existing implementations in its design goal of combining portability with high performance. We document its portability and performance and describe the architecture by which these features are simultaneously achieved. We also discuss the set of tools that accompany the free distribution of MPICH, which constitute the beginnings of a portable parallel programming environment. A project of this scope inevitably imparts lessons about parallel computing, the specification being followed, the current hardware and software environment for parallel computing, and project management; we describe those we have learned. Finally, we discuss future developments for MPICH, including those necessary to accommodate extensions to the MPI Standard now being contemplated by the MPI Forum. 1
356|High performance messaging on workstations: Illinois Fast Messages (FM) for Myrinet  (1995) |In most computer systems, software overhead dominates the cost of messaging, reducing delivered performance, especially for short messages. Efficient software messaging layers are needed to deliver the hardware performance to the application level and to support tightly-coupled workstation clusters. Illinois Fast Messages (FM) 1.0 is a high speed messaging layer that delivers low latency and high bandwidth for short messages. For 128-byte packets, FM achieves bandwidths of 16.2 MB/s and one-way latencies 32 s on Myrinet-connected SPARCstations (user-level to user-level). For shorter packets, we have measured one-way latencies of 25 s, and for larger packets, bandwidth as high as to 19.6 MB/s — delivered bandwidth greater than OC-3. FM is also superior to the Myrinet API messaging layer, not just in terms of latency and usable bandwidth, but also in terms of the message half-power point (n 1 2 which is two orders of magnitude smaller (54 vs. 4,409 bytes). We describe the FM messaging primitives and the critical design issues in building a low-latency messaging layers for workstation clusters. Several issues are critical: the division of labor between host and network coprocessor, management of the input/output (I/O) bus, and buffer management. To achieve high performance, messaging layers should assign as much functionality as possible to the host. If the network interface has DMA capability, the I/O bus should be used asymmetrically, with
357|Dynamically Forecasting Network Performance Using the Network Weather Service|this paper, we outline its design and detail the predictive performance of the forecasts it generates. While the forecasting methods are general, we focus on their ability to predict the TCP/IP end-to-end throughput and latency that is attainable by an application using systems located at different sites. Such network forecasts are needed both to support scheduling [5], and by the metacomputing software infrastructure to develop quality-of-service guarantees [10, 17]. Keywords: scheduling, metacomputing, quality-of-service, statistical forecasting, network performance monitoring
358|A Directory Service for Configuring High-Performance Distributed Computations|High-performance execution in distributed computing environments often requires careful selection and configuration not only of computers, networks, and other resources but also of the protocols and algorithms used by applications. Selection and configuration in turn require access to accurate, up-to-date information on the structure and state of available resources. Unfortunately, no standard mechanism exists for organizing or accessing such information. Consequently, different tools and applications adopt ad hoc mechanisms, or they compromise their portability and performance by using default configurations. We propose a solution to this problem: a Metacomputing Directory Service that provides efficient and scalable access to diverse, dynamic, and distributed information about resource structure and state. We define an extensible data model to represent the information required for distributed computing, and we present a scalable, high-performance, distributed implementation. The dat...
359|Legion -- a view from 50,000 feet|The coming of giga-bit networks makes possible the realization of a single nationwide virtual computer com-prised of a variety of geographically distributed high-pe6ormance machines and workstations. To realize the potential that the physical infrastructure provides, soft-ware must be developed that is easy to use, supports large degrees of parallelism in applications code, and manages the complexity of the underlying physical sys-tem for the usel: Legion is a metasystem project at the University of Virginia designed to provide users with a transparent intelface to the available resources, both at the programming interface level as well as at the user level. Legion addresses issues such as parallelism, fault-tolerance, security, autonomy, heterogeneity, resource management, and access transparency in a multi-language environment. In this paper we present a high-level overview of Legion, its vision, objectives, a brief sketch of how some of those objectives will be met, and the current status of the project.2 1 The Opportunity The dramatic increase in ubiquitously available net-work bandwidth will qualitatively change how the world computes, communicates, and collaborates. The rapid expansion of the world-wide web, and the changes that it has wrought are just the beginning. As high band-width connections become available they “shrink ” dis-tance and change our modes of computation, storage and interaction. Inevitably, users will operate in a wide-area environment that transparently consists of worksta-tions, personal computers, graphics rendering engines, supercomputers, and non-traditional devices: e.g., TVs,
360|An Abstract-Device Interface for Implementing  Portable Parallel-I/O Interfaces|In this paper, we propose a strategy for implementing parallel-I/O interfaces portably and efficiently. We have defined an abstract-device interface for parallel I/O, called ADIO. Any parallel-I/O API can be implemented on multiple file systems by implementing the API portably on top of ADIO, and implementing only ADIO on different file systems. This approach simplifies the task of implementing an API and yet exploits the specific high-performance features of individual file systems. We have used ADIO to implement the Intel PFS interface and subsets of MPI-IO and IBM PIOFS interfaces on PFS, PIOFS, Unix, and NFS file systems. Our performance studies indicate that the overhead of using ADIO as an implementation strategy...
361|Managing Multiple Communication Methods in High-Performance Networked Computing Systems|Modern networked computing environments and applications often require---or can benefit from---the use of multiple communication substrates, transport mechanisms, and protocols, chosen according to where communication is directed, what is communicated, or when communication is performed. We propose techniques that allow multiple communication methods to be supported transparently in a single application, with either automatic or user-specified selection criteria guiding the methods used for each communication. We explain how communication link and remote service request mechanisms facilitate the specification and implementation of multimethod communication. These mechanisms have been implemented in the Nexus multithreaded runtime system, and we use this system to illustrate solutions to various problems that arise when implementing multimethod communication. We also illustrate the application of our techniques by describing a multimethod, multithreaded implementation of the Message Pas...
362|Active Message Applications Programming Interface and Communication Subsystem Organization|High-performance network hardware is advancing, with multi-gigabit link bandwidths and sub-microsecond switch latencies. Network-interface hardware also continues to evolve, although the design space remains large and diverse. One critical abstraction, a simple, portable, and general-purpose communications interface, is required to make effective use of these increasingly high-performance networks and their capable interfaces. Without a new interface, the software overheads of existing ones will dominate communication costs, and many applications may not benefit from the advancements in network hardware. This document specifies a new active message communications interface for these networks. Its primitives, in essence an instruction set for communications, map efficiently onto underlying network hardware and compose effectively into higher-level protocols and applications. For high-performance implementations, the interface enables direct application-network interface interactions. In...
363|CC++: A Declarative Concurrent Object Oriented Programming Notation|CC++ is Compositional C++ , a parallel object-oriented notation that consists of C++ with six extensions. The goals of the CC++ project are to provide a theory, notation and tools for developing reliable scalable concurrent program libraries, and to provide a framework for unifying: 1. distributed reactive systems, batch-oriented numeric and symbolic applications, and user-interface systems, 2. declarative programs and object-oriented imperative programs, and 3. deterministic and nondeterministic programs. This paper is a brief description of the motivation for CC++ , the extensions to C++ , a few examples of CC++ programs with reasoning about their correctness, and an evaluation of CC++ in the context of other research on concurrent computation. A short description of C++ is provided.  
364|Software infrastructure for the I-WAY high-performance distributed computing experiment|High-speed wide area networks are expected to enable innovative applications that integrate geographically distributed, high-performance computing, database, graphics, and networking resources. However, there is as yet little understanding of the higher-level services required to support these applications, or of the techniques required to implement these services in a scalable, secure manner. We report on a large-scale prototyping effort that has yielded some insights into these issues. Building on the hardware base provided by the I-WAY, a national-scale Asynchronous Transfer Mode (ATM) network, we developed an integrated management and application programming system, called I-Soft. This system was deployed at most of the 17 I-WAY sites and used by many of the 60 applications demonstrated on the I-WAY network. In this article, we describe the I-Soft design and report on lessons learned from application experiments. 1
365|Sharing Visualization Experiences among Remote Virtual Environments|Virtual reality has become an increasingly familiar part of the science of visualization and communication of information. This, combined with the increase in connectivity of remote sites via high-speed networks, allows for the development of a collaborative distributed virtual environment. Such an environment enables the development of supercomputer simulations with virtual reality visualizations that can be displayed at multiple sites, with each site interacting, viewing, and communicating about the results being discovered. The early results of an experimental collaborative virtual reality environment are discussed in this paper. The issues that need to be addressed in the implementation, as well as preliminary results are covered. Also provided are a discussion of plans and a generalized application programmers interface for CAVE to CAVE will be provided. 1 Introduction  Sharing a visualization experience among remote virtual environments is a new area of research within the field ...
366|Near-real-time Satellite Image Processing: Metacomputing in CC++|Metacomputing entails the combination of diverse, heterogeneous elements to provide a seamless, integrated computing service. We describe one such metacomputing application using Compositional C that integrates specialized resources, high-speed networks, parallel computers, and VR display technology to process satellite imagery in near-real-time. From the virtual environment, the user can query an InputHandler object for the latest available satellite data, select a satellite pass for processing by CloudDetector objects on a parallel supercomputer, and have the results rendered by a VisualizationManager object which could be a simple workstation, an ImmersaDesk or a CAVE. With an ImmersaDesk or CAVE, the user can navigate over a terrain with elevation and through the cloudscape data as it is being pumped in from the supercomputer. We discuss further issues for the development of metacomputing capabilities with regards to the integration of run-time systems, operating systems, high-s...
367|A Secure Communications Infrastructure for High-Performance Distributed Computing|We describe a software infrastructure designed to support the development of applications  that use high-speed networks to connect geographically distributed supercomputers,  databases, and scientific instruments. Such applications may need to operate over open  networks and access valuable resources, and hence can require mechanisms for ensuring  integrity and confidentiality of communications and for authenticating both users and  resources. Yet security solutions developed for traditional client-server applications do  not provide direct support for the distinctive program structures, programming tools,  and performance requirements encountered in these applications. To address these requirements,  we are developing a security-enhanced version of a communication library  called Nexus, which is then used to provide secure versions of various parallel libraries  and languages, including the popular Message Passing Interface. These tools support the  wide range of process creation mechan...
368|Remote Engineering Tools for the Design of Pollution Control Systems for Commercial Boilers|this paper, we discuss a pilot project involving a collaboration between Nalco Fuel Tech (NFT), a small company that has developed state-of-the-art emission reduction systems for commercial boilers, and the computational science group at Argonne National Laboratory (ANL). The key objective of this project is the development of a real-time, interactive capability that allows the user to drive the computational model from within the virtual environment. In this case, the required interaction involves the placement of chemical injection systems in the boiler and a quick evaluation of  their effectiveness in reducing undesirable emissions from the combustion process. The numerical model of the combustion process and its interaction with the chemical reactions used to reduce emissions is complex, and the solution is computationally intensive. The timely solution of these models depends on advanced numerical methods and the effective use of high-performance computing resources. Our approach includes an implementation of an injector model that can be run on a variety of architectures ranging from the IBM SP to networks of workstations. This calculation is separate from the visualization process and numerical results are communicated to the virtual boiler using the CAVEcomm message-passing library developed at Argonne National Laboratory [4]. The virtual boiler environment consists of several components designed to aide in the efficient optimization of the injection system. In this paper we discuss the software that allows rapid virtual boiler construction, various mechanisms for data visualization, and a package for the interactive placement of injector nozzles. Once the computational design process is complete, engineers are sent to the boiler site to install the injection s...
369|MPI on the I-WAY: A Wide-Area, Multimethod Implementation of the Message Passing Interface|High-speed wide-area networks enable innovative ap-plications that integrate geographically distributed com-puting, database, graphics, and networking resources. The Message Passing Interface (MPI) can be used as a portable, high-performance programming model for such systems. However, the wide-area environment in-troduces challenging problems for the MPI implementor, because of the heterogeneity of both the underlying physical infrastructure and the authentication and software environment at different sites. In this article, we describe an MPI implementation that incorporates so-lutions to these problems. This implementation, which was developed for the I-WAY distributed-computing ex-periment, was constructed by layering MPICH on the Nexus multithreaded runtime system. Nexus provides automatic configuration mechanisms that can be used to select and configure authentication, process creation, and communication mechanisms in heterogeneous systems. 
370|Security Architecture for the Internet Protocol|Content-Type: text/plain
372|Privacy Enhancement for Internet Electronic Mail: Part II: Certificate-Based Key Management|this memo is unlimited. Acknowledgements
373|The Architecture and Implementation of Network-Layer Security Under Unix|swIPe is a network-layer security protocol for the IP protocol suite. This paper presents the architecture, design philosophy, and performance of an implementation of swIPe under several variants of Unix. swIPe provides authentication, integrity, and confidentiality of IP datagrams, and is completely compatible with the existing IP infrastructure. To maintain this compatibility, swIPe is implemented using an encapsulation protocol. Mechanism (the details of the protocol) is decoupled from policy (what and when to protect) and key management. swIPe under Unix is implemented using a virtual network interface. The parts of the implementation that process incoming and outgoing packets are entirely in the kernel; parameter setting and exception handling, however, are managed by userlevel processes. The performance of swIPe on modern workstations is primarily limited only by the speed of the underlying authentication and encryption algorithms; the mechanism overhead is negligible in our prototype. 1.
374|On Internet Authentication|Status of this Memo This document provides information for the Internet community. This memo does not specify an Internet standard of any kind. Distribution of this memo is unlimited. 1.
375|HMAC-MD5 IP Authentication with Replay Prevention&#034;, RFC 2085|This document specifies an Internet standards track protocol for the Internet community, and requests discussion and suggestions for improvements. Please refer to the current edition of the &#034;Internet Official Protocol Standards &#034; (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. This document describes a keyed-MD5 transform to be used in conjunction with the IP Authentication Header [RFC-1826]. The particular transform is based on [HMAC-MD5]. An option is also
376|Domain Name System Protocol Security Extensions. draft-ietf-dnssec-secext-04.txt -- work in progress|This draft, file name draft-ietf-dnssec-secext-03.txt, is intended to be become a proposed standard RFC. Distribution of this document is unlimited. Comments should be sent to the DNS Security Working Group mailing list &lt;dns-security@tis.com&gt; or to the authors. This document is an Internet-Draft. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months. Internet-Drafts may be updated, replaced, or obsoleted by other documents at any time. It is not appropriate to use Internet-Drafts as reference material or to cite them other than as a ‘‘working draft’ ’ or ‘‘work in progress.’’ To learn the current status of any Internet-Draft, please check the 1id-abstracts.txt listing contained in the Internet-Drafts Shadow Directories on ds.internic.net, nic.nordu.net, ftp.isi.edu, munnari.oz.au, or ftp.is.co.za. Eastlake, Kaufman [Page 1] INTERNET-DRAFT DNS Protocol Security Extensions January 1995 The Domain Name System (DNS) has become a critical operational part of the Internet infrastructure yet it has no strong security mechanisms to assure data integrity or authentication. Extensions to the DNS are described that provide these services to security aware resolvers or applications through the use of cryptographic digital signatures. These digital signatures are included in secured zones as resource records. Security can still be provided even through non-security aware DNS servers. The extensions also provide for the storage of authenticated public keys in the DNS. This storage of keys can support a general public key distribution service as well as DNS security. The stored keys enable security aware resolvers to learn the authenticating key of zones in addition to keys for which they are initially configured. Keys associated with DNS names can be retrieved to support other protocols. Provision is made for a variety of key types and algorithms. In addition, the security extensions provide for the optional authentication of DNS protocol transactions.
377|Breaking and Fixing the Needham-Schroeder Public-Key Protocol using FDR|In this paper we analyse the well known Needham-Schroeder Public-Key Protocol using FDR, a refinement checker for CSP. We use FDR to discover an attack upon the protocol, which allows an intruder to impersonate another agent. We adapt the protocol, and then use FDR to show that the new protocol is secure, at least for a small system. Finally we prove a result which tells us that if this small system is secure, then so is a system of arbitrary size. 1 Introduction  In a distributed computer system, it is necessary to have some mechanism whereby a pair of agents can be assured of each other&#039;s identity---they should become sure that they really are talking to each other, rather than to an intruder impersonating the other agent. This is the role of an authentication  protocol.  In this paper we use the Failures Divergences Refinement Checker (FDR) [11, 5], a model checker for CSP, to analyse the Needham-Schroeder PublicKey Authentication Protocol [8]. FDR takes as input two CSP processes, ...
378|A theory of communicating sequential processes|  A mathematical model for communicating sequential processes is given, and a number of its interesting and useful properties are stated and proved. The possibilities of nondetermimsm are fully taken into account.
379|A logic of authentication|Questions of belief are essential in analyzing protocols for the authentication of principals in distributed computing systems. In this paper we motivate, set out, and exemplify a logic specifically designed for this analysis; we show how various protocols differ subtly with respect to the required initial assumptions of the participants and their final beliefs. Our formalism has enabled us to isolate and express these differences with a precision that was not previously possible. It has drawn attention to features of protocols of which we and their authors were previously unaware, and allowed us to suggest improvements to the protocols. The reasoning about some protocols has been mechanically verified. This paper starts with an informal account of the problem, goes on to explain the formalism to be used, and gives examples of its application to protocols from the literature, both with shared-key cryptography and with public-key cryptography. Some of the examples are chosen because of their practical importance, while others serve to illustrate subtle points of the logic and to explain how we use it. We discuss extensions of the logic motivated by actual practice -- for example, in order to account for the use of hash functions in signatures. The final sections contain a formal semantics of the logic and some conclusions.  
380|An Attack on the Needham-Schroeder Public-Key Authentication Protocol|In this paper we present an attack upon the Needham-Schroeder publickey authentication protocol. The attack allows an intruder to impersonate another agent.
381|Security properties and CSP|Security properties such as confidentiality and authenticity may be considered in terms of the flow of messages within a network. To the extent that this characterisation is justified, the use of a process algebra such as Communicating Sequential Processes (CSP) seems appropriate to describe and analyse them. This paper explores ways in which security properties may be described as CSP specifications, how security mechanisms may be captured, and how particular protocols designed to provide these properties may be analysed within the CSP framework. The paper is concerned with the theoretical basis for such analysis. A sketch verification of a simple example is carried out as an illustration. 1 Introduction  Security protocols are designed to provide properties such as authentication, key exchanges, key distribution, non-repudiation, proof of origin, integrity, confidentiality and anonymity, for users who wish to exchange messages over a medium over which they have little control. These ...
382|Internet time synchronization: The network time protocol|This memo describes the Network Time Protocol (NTP) designed to distribute time information in a large, diverse internet system operating at speeds from mundane to lightwave. It uses a returnabletime architecture in which a distributed subnet of time servers operating in a self-organizing, hierarchical, master-slave configuration synchronizes local clocks within the subnet and to national time standards via wire or radio. The servers can also redistribute time information within a network via local routing algorithms and time daemons. The architectures, algorithms and protocols which have evolved to NTP over several years of implementation and refinement are described in this paper. The synchronization subnet which has been in regular operation in the Internet for the last several years is described along with performance data which shows that timekeeping accuracy throughout most portions of the Internet can be ordinarily maintained to within a few tens of milliseconds, even in cases of failure or disruption of clocks, time servers or networks. This memo describes the Network Time Protocol, which is specified as an Internet Standard in
383|Time, Clocks, and the Ordering of Events in a Distributed System|The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.  
384|Data networks|a b s t r a c t In this paper we illustrate the core technologies at the basis of the European SPADnet project (www. spadnet.eu), and present the corresponding first results. SPADnet is aimed at a new generation of MRI-compatible, scalable large area image sensors, based on CMOS technology, that are networked to perform gamma-ray detection and coincidence to be used primarily in (Time-of-Flight) Positron Emission Tomography (PET). The project innovates in several areas of PET systems, from optical coupling to single-photon sensor architectures, from intelligent ring networks to reconstruction algorithms. In addition, SPADnet introduced the first computational model enabling study of the full chain from gamma photons to network coincidence detection through scintillation events, optical coupling, etc. &amp; 2013 Elsevier B.V. All rights reserved. 1.
385|Optimal Clock Synchronization|We present a simple, efficient, and unified solution to the problems of synchronizing, initializing, and integrating clocks for systems with different types of failures: crash, omission, and arbitrary failures with and without message authentication. This is the ft known solution that achieves optimal accuracy -- the accuracy of synchronized clocks (with respect to real time) is as good as that specified for the underlying hardware clocks. The solution is also optimal with respect to the number of faulty processes that can be tolerated to achieve this accuracy.
386|Dynamic Fault-Tolerant Clock Synchronization|This paper gives two simple efficient distributed algorithms: one for keeping clocks in a network synchronized and one for allowing new processors to join the network with their clocks synchronized. Assuming a fault tolerant authentication protocol, the algorithms tolerate both link and processor failures of any type. The algorithm for maintaining synchronization works for arbitrary networks (rather than just completely connected networks) and tolerates any number of processor or communication link faults as long as the correct processors remain connected by fault-free paths. It thus represents an improvement over other clock synchronization algorithms such as [LM,WL], although, unlike them, it does require an authentication protocol to handle Byzantine faults. Our algorithm for allowing new processors to join requires that more than half the processors be correct, a requirement that is provably necessary. 1 Introduction  In a distributed system it is often necessary for processors to ...
387|A New Fault-Tolerant Algorithm for Clock Synchronization|We describe a new fault-tolerant algorithm for solving a variant of Lamport&#039;s clock synchronization problem. The algorithm is designed for a system of distributed processes that communicate by sending messages. Each process has its own read-only physical clock whose drift rate from real time is very small. By adding a value to its physical clock time, the process obtains its local time. The algorithm solves the problem of .maintaining closely synchronized local times, assuming that processes&#039; local times are clo. sely synchronized initially. The algorithm is able to tolerate the failure of just under a third of the participating processes. It maintains synchronization to within a small constant, whose magnitude depends upon the rate of clock drift, the message delivery time, and the initial closeness of synchronization. We also give a characterization of how far the clocks drift from real time. Reintegration of a repaired process can be accomplished using a slight modification of the &#039;basic algorithm. A similar style algorithm can also be used to achieve synchronization initially.
388|A Paradigm for Reliable Clock Synchronization|Existing fault-tolerant clock synchronization protocols are shown to result from refining a  single clock synchronization paradigm. In that paradigm, a reliable time source  periodically issues messages that cause processors to resynchronize their clocks. The  reliable time source is approximated by reading all clocks in the system and using a  convergence function to compute a fault-tolerant average of the values read. The  performance of a clock synchronization algorithm based on the paradigm can be quantified  in terms of the two parameters that characterize the behavior of the convergence function  used: accuracy and precision.
389|The Fuzzball|The Fuzzball is an operating system and applications library designed for the PDP11 family of  computers. It was intended as a development platform and research pipewrench for the DARPA/NSF  Internet, but has occasionally escaped to earn revenue in commercial service. It was designed,  implemented and evolved over a seventeen-year era spanning the development of the ARPANET  and TCP/IP protocol suites and can today be found at Internet outposts from Hawaii to Italy standing  watch for adventurous applications and enduring experiments. This paper describes the Fuzzball and  its applications, including a description of its novel congestion avoidance/control and timekeeping  mechanisms.&lt;E-110&gt; Keywords: protocol testing, network testing, performance evaluation, Internet architecture, TCP/IP protocols, congestion control, internetwork time synchronization.&lt;E-132&gt; 1. Introduction&lt;E-128&gt; The Fuzzball is a software package consisting of a fast, compact operating system, support for the DARPA/...
390|Measured performance of the Network Time Protocol in the Internet system. DARPA Network Working Group Report RFC-1128|This paper describes a series of experiments involving over 100,000 hosts of the Internet system and located in the U.S., Europe and the Pacific. The experiments are designed to evaluate the availability, accuracy and reliability of international standard time distribution using the DARPA/NSF Internet and the Network Time Protocol (NTP), which is specified as an Internet Standard in RFC-1119. NTP is designed specifically for use in a large, diverse internet system operating at speeds from mundane to lightwave. In NTP a distributed subnet of time servers operating in a self-organizing, hierarchical, master-slave configuration exchange precision timestamps in order to synchronize subnet clocks to each other and national time standards via wire or radio. The experiments are designed to locate Internet hosts and gateways that provide time by one of three time distribution protocols and evaluate the accuracy of their indications. For those hosts that support NTP, the experiments determine the distribution of errors and other statistics over paths spanning major portions of the globe. Finally, the experiments evaluate the accuracy and reliability of precision timekeeping using NTP and typical Internet paths involving DARPA, NSFNET and other agency networks. The experiments demonstrate that timekeeping accuracy throughout most
391|The National Bureau of Standards atomic time scale: Generation, stability, accuracy and accessibility|Absrruct-The independent atomic time scale at the National Bureau of Standards AT(NBS), is based upon an ensemble of continuously operating cesium clocks calibrated occasionally by an NBS primary frequency standard. The data of frequency calibrations and interclock comparisons are statistically processed to provide nearly optimum time stability and frequency accuracy. The long-term random fluctuation of AT(NBS) due to nondeterministic perturbations is estimated to be a few parts in and the present accuracy is inferred to be 1 part in A small coordinate rate is added to the rate of AT(NBS) to generate UTC(NBS): this small addition is for the purpose of maintaining syn-chronization within a few microseconds of other international timing centers. IJTQNBS) is readily operationally available over a large part of the world via WWV. WWVH, WWVB, and telephone; also via some passwe time transfer systems, e.g., Loran € and the TV line-10 system; and also experimentaliy via satellite and WWVL. The precision and ac-
392|Scale and performance in a distributed file system|The Andrew File System is a location-transparent distributed tile system that will eventually span more than 5000 workstations at Carnegie Mellon University. Large scale affects performance and complicates system operation. In this paper we present observations of a prototype implementation, motivate changes in the areas of cache validation, server process structure, name translation, and low-level storage representation, and quantitatively demonstrate Andrew’s ability to scale gracefully. We establish the importance of whole-file transfer and caching in Andrew by comparing its performance with that of Sun Microsystem’s NFS tile system. We also show how the aggregation of files into volumes improves the operability of the system.
393|The Case for Geographical Push-Caching|Most existing wide-area caching schemes are client initiated. Decisions on when and where to cache information are made without the benefit of the server&#039;s global knowledge of the situation. We believe that the server should play a role in making these caching decisions, and we propose  geographical push-caching as a way of bringing the server back into the loop. The World Wide Web is an excellent example of a wide-area system that will benefit from geographical push-caching, and we present an architecture that allows a Web server to autonomously replicate HTML pages. 1 Introduction  The World-Wide Web [1] operates for the most part as a cache-less distributed system. When two neighboring clients retrieve a document from the same server, the document is sent twice. This is inefficient, especially considering the ease with which Web browsers allow users to transfer large multimedia documents. To combat this problem, some Web browsers have begun to add local client caches. These prevent ...
394|The Harvest Information Discovery and Access System|It is increasingly difficult to make effective use of Internet information, given the rapid growth in data volume, user base, and data diversity. In this paper we introduce Harvest, a system that provides a scalable, customizable architecture for gathering, indexing, caching, replicating, and accessing Internet information.  
395|Alex -- a global filesystem|The Alex filesystem provides users and applications transparent read access to files in Internet anonymous FTP sites. Today there are thousands of anonymous FTP sites with a total of a few million files and roughly a terabyte of data. The standard approach to accessing these files involves logging in to the remote machine. This means that an application can not access remote files and that users do not have any of their aliases or local tools available when connected to a remote site. Users who want to use an application on a remote file must first manually make a local copy of the file. Not only is this inconvenient, it creates two more problems. First, there is no mechanism for automatically updating this local copy when the remote file changes. The users must keep track of where they get their files from and check to see if there are updates, and then fetch these. Second, many different users at the same site may have made copies of the same remote file, thus wasting disk space. Alex addresses the problems with the above approach while maintaining compatibility with the existing FTP protocol so that the large collection of currently available files can be accessed. To get reasonable performance, long term file caching must be used. Thus consistency must be addressed. Traditional solutions to the cache consistency problem do not work in the Internet FTP domain: callbacks are not an
396|Multi-level Caching in Distributed File Systems or Your cache ain’t nuthin’ but trash|We are investigating the potential for intermediate file servers to address scaling problems in increasingly large distributed file systems. To this end, we have run trace-driven simulations based on data from DEC-SRC and our own data collection to determine the potential of caching-only intermediate servers. The degree of sharing among clients is central to the effectiveness of an intermediate server. This turns out to be quite low in the traces available to us. All told, fewer than 10 % of block accesses are to files shared by more than one file system client. Trace-driven simulation shows that even with an infinite cache at the intermediate, cache hit rates are disappointingly low. For client caches as small as 20 MB, we observe hit rates less than 19%. As client cache sizes increase, the hit rate at the intermediate approaches the degree of sharing among all clients. On the other hand, the intermediate does appear to be effective in reducing the peak load presented to upstream file servers.
397|Demand-based Document Dissemination for the World-Wide Web|We analyzed the logs of our departmental HTTP server
398|Berkeley UNIX+ on 1000 Workstations: Athena Changes to 4.3BSD |4.3BSD UNIX as shipped is designed for use on individually-managed, networked  timesharing systems. A large network of individual workstations and server machines,  all managed centrally, has many important differences from such a model. This paper  discusses some of the changes necessary for 4.3 in this new world, including the file system  layout, configuration files, and software. The integration with Athena&#039;s authentication  system, name service, and service management system are also discussed.  1. Overview  &#034;By 1988, create a new educational computing environment environment at MIT built around high-performance graphics workstations, high-speed networking, and servers of various types.&#034; This one-sentence statement is a highlevel description of the technical goals of Project Athena. While the primary goals are to enhance education, attaining them has required a significant effort to engineer a software environment for use in a large network of workstations and servers.  The Athena...
399|TaintDroid: An Information-Flow Tracking System for Realtime Privacy Monitoring on Smartphones|Today’s smartphone operating systems fail to provide users with adequate control and visibility into how third-party applications use their private data. We present TaintDroid, an efficient, system-wide dynamic taint tracking and analysis system for the popular Android platform that can simultaneously track multiple sources of sensitive data. TaintDroid’s efficiency to perform real-time analysis stems from its novel system design that leverages the mobile platform’s virtualized system architecture. TaintDroid incurs only 14 % performance overhead on a CPU-bound micro-benchmark with little, if any, perceivable overhead when running thirdparty applications. We use TaintDroid to study the behavior of 30 popular third-party Android applications and find several instances of misuse of users ’ private information. We believe that TaintDroid is the first working prototype demonstrating that dynamic taint tracking and analysis provides informed use of third-party applications in existing smartphone operating systems. 
400|Dynamic taint analysis for automatic detection, analysis, and signature generation of exploits on commodity software|Software vulnerabilities have had a devastating effect on the Internet. Worms such as CodeRed and Slammer can compromise hundreds of thousands of hosts within hours or even minutes, and cause millions of dollars of damage [32, 51]. To successfully combat these fast automatic Internet attacks, we need fast automatic attack detection and filtering mechanisms. In this paper we propose dynamic taint analysis for automatic detection and analysis of overwrite attacks, which include most types of exploits. This approach does not need source code or special compilation for the monitored program, and hence works on commodity software. To demonstrate this idea, we have implemented TaintCheck, a mechanism that can perform dynamic taint analysis by performing binary rewriting at run time. We show that TaintCheck reliably detects most types of exploits. We found that TaintCheck produced no false positives for any of the many different programs that we tested. Further, we show how we can use a two-tiered approach to build a hybrid exploit detector that enjoys the same accuracy as TaintCheck but have extremely low performance overhead. Finally, we propose a new type of automatic signature generation—semanticanalysis based signature generation. We show that by backtracing the chain of tainted data structure rooted at the detection point, TaintCheck can automatically identify which original flow and which part of the original flow have caused the attack and identify important invariants of the payload that can be used as signatures. Semantic-analysis based signature generation can be more accurate, resilient against polymorphic worms, and robust to attacks exploiting polymorphism than the pattern-extraction based signature generation methods.
401|Protecting privacy using the decentralized label model|Stronger protection is needed for the confidentiality and integrity of data, because programs containing untrusted code are the rule rather than the exception. Information flow control allows the enforcement of end-to-end security policies, but has been difficult to put into practice. This article describes the decentralized label model, a new label model for control of information flow in systems with mutual distrust and decentralized authority. The model improves on existing multilevel security models by allowing users to declassify information in a decentralized way, and by improving support for fine-grained data sharing. It supports static program analysis of information flow, so that programs can be certified to permit only acceptable information flows, while largely avoiding the overhead of run-time checking. The article introduces the language Jif, an extension to Java that provides static checking of information flow using the decentralized label model.
402|Secure Program Execution via Dynamic Information Flow Tracking|Dynamic information flow tracking is a hardware mechanism to protect programs against malicious attacks by identifying spurious information flows and restricting the usage of spurious information. Every security attack to take control of a program needs to transfer the program’s control to malevolent code. In our approach, the operating system identifies a set of input channels as spurious, and the processor tracks all information flows from those inputs. A broad range of attacks are effectively defeated by disallowing the spurious data to be used as instructions or jump target addresses. We describe two different security policies that track differing sets of dependencies. Implementing the first policy only incurs, on average, a memory overhead of 0.26 % and a performance degradation of 0.02%. This policy does not require any modification of executables. The stronger policy incurs, on average, a memory overhead of 4.5 % and a performance degradation of 0.8%, and requires binary annotation. 1
403|Making information flow explicit in HiStar|HiStar is a new operating system designed to minimize the amount of code that must be trusted. HiStar provides strict information flow control, which allows users to specify precise data security policies without unduly limiting the structure of applications. HiStar’s security features make it possible to implement a Unix-like environment with acceptable performance almost entirely in an untrusted user-level library. The system has no notion of superuser and no fully trusted code other than the kernel. HiStar’s features permit several novel applications, including an entirely untrusted login process, separation of data between virtual private networks, and privacypreserving, untrusted virus scanners. 1
404|Minos: Control Data Attack Prevention Orthogonal to Memory Model|We introduce Minos, a microarchitecture that implements Biba&#039;s low-water-mark integrity policy on individual words of data. Minos stops attacks that corrupt control data to hijack program control flow but is orthogonal to the memory model. Control data is any data which is loaded into the program counter on control flow transfer, or any data used to calculate such data. The key is that Minos tracks the integrity of all data, but protects control flow by checking this integrity when a program uses the data for control transfer. Existing policies, in contrast, need to differentiate between control and non-control data a priori, a task made impossible by coercions between pointers and other data types such as integers in the C language.
405|Taint-enhanced policy enforcement: A practical approach to defeat a wide range of attacks|Policy-based confinement, employed in SELinux and specification-based intrusion detection systems, is a popular approach for defending against exploitation of vulnerabilities in benign software. Conventional access control policies employed in these approaches are effective in detecting privilege escalation attacks. However, they are unable to detect attacks that “hijack ” legitimate access privileges granted to a program, e.g., an attack that subverts an FTP server to download the password file. (Note that an FTP server would normally need to access the password file for performing user authentication.) Some of the common attack types reported today, such as SQL injection and cross-site scripting, involve such subversion of legitimate access privileges. In this paper, we present a new approach to strengthen policy enforcement by augmenting security policies with information about the trustworthiness of data used in securitysensitive operations. We evaluated this technique using 9 available exploits involving several popular software packages containing the above types of vulnerabilities. Our technique sucessfully defeated these exploits. 1
406|Understanding data lifetime via whole system simulation|Rights to individual papers remain with the author or the author&#039;s employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein.
407|Information flow control for standard OS abstractions|Decentralized Information Flow Control (DIFC) [24] is an approach to security that allows application writers to control how data flows between the pieces of an application and the outside world. As applied to privacy, DIFC allows untrusted software to compute with private data while trusted security code controls the release of that data. As applied to integrity, DIFC allows trusted code to protect untrusted software from unexpected malicious inputs. In either case, only bugs in the trusted code, which tends to be small and isolated, can lead to security violations. We present Flume, a new DIFC model and system that applies at the granularity of operating system processes and standard OS abstractions (e.g., pipes and file descriptors). Flume eases DIFC’s use in existing applications and allows safe interaction between conventional and DIFC-aware processes. Flume runs as a user-level reference monitor on Linux. A process confined by Flume cannot perform most system calls directly; instead, an interposition layer replaces system calls with IPC to the reference monitor, which enforces data flow policies and performs safe operations on the process’s behalf. We ported a complex Web application (MoinMoin wiki) to Flume, changing only 2 % of the original code. The Flume version is roughly 30–40 % slower due to overheads in our current implementation but supports additional security policies impossible without DIFC. Categories and Subject Descriptors:
408|On Lightweight Mobile Phone Application Certification|Users have begun downloading an increasingly large number of mobile phone applications in response to advancements in handsets and wireless networks. The increased number of applications results in a greater chance of installing Trojans and similar malware. In this paper, we propose the Kirin security service for Android, which performs lightweight certification of applications to mitigate malware at install time. Kirin certification uses security rules, which are templates designed to conservatively match undesirable properties in security configuration bundled with applications. We use a variant of security requirements engineering techniques to perform an in-depth security analysis of Android to produce a set of rules that match malware characteristics. In a sample of 311 of the most popular applications downloaded from the official Android Market, Kirin and our rules found 5 applications that implement dangerous functionality and therefore should be installed with extreme caution. Upon close inspection, another five applications asserted dangerous rights, but were within the scope of reasonable functional needs. These results indicate that security configuration bundled with Android applications provides practical means of detecting malware.
409|Panorama: Capturing system-wide information flow for malware detection and analysis|Malicious programs spy on users ’ behavior and compromise their privacy. Even software from reputable vendors, such as Google Desktop and Sony DRM media player, may perform undesirable actions. Unfortunately, existing techniques for detecting malware and analyzing unknown code samples are insufficient and have significant shortcomings. We observe that malicious information access and processing behavior is the fundamental trait of numerous malware categories breaching users ’ privacy (including keyloggers, password thieves, network sniffers, stealth backdoors, spyware and rootkits), which separates these malicious applications from benign software. We propose a system, Panorama, to detect and analyze malware by capturing this fundamental trait. In our extensive experiments, Panorama successfully detected all the malware samples and had very few false positives. Furthermore, by using Google Desktop as a case study, we show that our system can accurately capture its information access and processing behavior, and we can confirm that it does send back sensitive information to remote servers in certain settings. We believe that a system such as Panorama will offer indispensable assistance to code analysts and malware researchers by enabling them to quickly comprehend the behavior and innerworkings of an unknown sample.
410|Dytan: A Generic Dynamic Taint Analysis Framework|Dynamic taint analysis is gaining momentum. Techniques based on dynamic tainting have been successfully used in the context of application security, and now their use is also being explored in different areas, such as program understanding, software testing, and debugging. Unfortunately, most existing approaches for dynamic tainting are defined in an ad-hoc manner, which makes it difficult to extend them, experiment with them, and adapt them to new contexts. Moreover, most existing approaches are focused on data-flow based tainting only and do not consider tainting due to control flow, which limits their applicability outside the security domain. To address these limitations and foster experimentation with dynamic tainting techniques, we defined and developed a general framework for dynamic tainting that (1) is highly flexible and customizable, (2) allows for performing both data-flow and control-flow based tainting conservatively, and (3) does not rely on any customized runtime system. We also present DYTAN, an implementation of our framework that works on x86 executables, and a set of preliminary studies that show how DYTAN can be used to implement different tainting-based approaches with limited effort. In the studies, we also show that DYTAN can be used on real software, by using FIRE-FOX as one of our subjects, and illustrate how the specific characteristics of the tainting approach used can affect efficiency and accuracy of the taint analysis, which further justifies the use of our framework to experiment with different variants of an approach.
411|Semantically Rich Application-Centric Security in Android|Abstract—Smartphones are now ubiquitous. However, the security requirements of these relatively new systems and the applications they support are still being understood. As a result, the security infrastructure available in current smartphone operating systems is largely underdeveloped. In this paper, we consider the security requirements of smartphone applications and augment the existing Android operating system with a framework to meet them. We present Secure Application INTeraction (Saint), a modified infrastructure that governs install-time permission assignment and their run-time use as dictated by application provider policy. An in-depth description of the semantics of application policy is presented. The architecture and technical detail of Saint is given, and areas for extension, optimization, and improvement explored. As we show through concrete example, Saint provides necessary utility for applications to assert and control the security decisions on the platform. Keywords-mobile phone security; Android; application interactions; mediation; I.
412|Dynamic Spyware Analysis|Spyware is a class of malicious code that is surreptitiously installed on victims ’ machines. Once active, it silently monitors the behavior of users, records their web surfing habits, and steals their passwords. Current anti-spyware tools operate in a way similar to traditional virus scanners. That is, they check unknown programs against signatures associated with known spyware instances. Unfortunately, these techniques cannot identify novel spyware, require frequent updates to signature databases, and are easy to evade by code obfuscation. In this paper, we present a novel dynamic analysis approach that precisely tracks the flow of sensitive information as it is processed by the web browser and any loaded browser helper objects. Using the results of our analysis, we can identify unknown components as spyware and provide comprehensive reports on their behavior. The techniques presented in this paper address limitations of our previous work on spyware detection and significantly improve the quality and richness of our analysis. In particular, our approach allows a human analyst to observe the actual flows of sensitive data in the system. Based on this information, it is possible to precisely determine which sensitive data is accessed and where this data is sent to. To demonstrate the effectiveness of the detection and the comprehensiveness of the generated reports, we evaluated our system on a substantial body of spyware and benign samples. 1
413|All you ever wanted to know about dynamic taint analysis and forward symbolic execution (but might have been afraid to ask  (2010) |Abstract—Dynamic taint analysis and forward symbolic execution are quickly becoming staple techniques in security analyses. Example applications of dynamic taint analysis and forward symbolic execution include malware analysis, input filter generation, test case generation, and vulnerability discovery. Despite the widespread usage of these two techniques, there has been little effort to formally define the algorithms and summarize the critical issues that arise when these techniques are used in typical security contexts. The contributions of this paper are two-fold. First, we precisely describe the algorithms for dynamic taint analysis and forward symbolic execution as extensions to the run-time semantics of a general language. Second, we highlight important implementation choices, common pitfalls, and considerations when using these techniques in a security context. Keywords-taint analysis, symbolic execution, dynamic analysis I.
414|Cross-Site Scripting Prevention with Dynamic Data Tainting and Static Analysis|Cross-site scripting (XSS) is an attack against web applications in which scripting code is injected into the output of an application that is then sent to a user’s web browser. In the browser, this scripting code is executed and used to transfer sensitive data to a third party (i.e., the attacker). Currently, most approaches attempt to prevent XSS on the server side by inspecting and modifying the data that is exchanged between the web application and the user. Unfortunately, it is often the case that vulnerable applications are not fixed for a considerable amount of time, leaving the users vulnerable to attacks. The solution presented in this paper stops XSS attacks on the client side by tracking the flow of sensitive information inside the web browser. If sensitive information is about to be transferred to a third party, the user can decide if this should be permitted or not. As a result, the user has an additional protection layer when surfing the web, without solely depending on the security of the web application. 1
415|Dynamic Taint Propagation for Java|Improperly validated user input is the underlying root cause for a wide variety of attacks on web-based applications. Static approaches for detecting this problem help at the time of development, but require source code and report a number of false positives. Hence, they are of little use for securing fully deployed and rapidly evolving applications. We propose a dynamic solution that tags and tracks user input at runtime and prevents its improper use to maliciously affect the execution of the program. Our implementation can be transparently applied to Java classfiles, and does not require source code. Benchmarks show that the overhead of this runtime enforcement is negligible and can prevent a number of attacks. 1.
416|Practical taint-based protection using demand emulation|Many software attacks are based on injecting malicious code into a target host. This paper demonstrates the use of a wellknown technique, data tainting, to track data received from the network as it propagates through a system and to prevent its execution. Unlike past approaches to taint tracking, which track tainted data by running the system completely in an emulator or simulator, resulting in considerable execution overhead, our work demonstrates the ability to dynamically switch a running system between virtualized and emulated execution. Using this technique, we are able to explore hardware support for taint-based protection that is deployable in real-world situations, as emulation is only used when tainted data is being processed by the CPU. By modifying the CPU, memory, and I/O devices to support taint tracking and protection, we guarantee that data received from the network may not be executed, even if it is written to, and later read from disk. We demonstrate near native speeds for workloads where little taint data is present.
417|RIFLE: An architectural framework for user-centric information-flow security|Even as modern computing systems allow the manipulation and distribution of massive amounts of information, users of these systems are unable to manage the confidentiality of their data in a practical fashion. Conventional access control security mechanisms cannot prevent the illegitimate use of privileged data once access is granted. For example, information provided by a user during an online purchase may be covertly delivered to malicious third parties by an untrustworthy web browser. Existing information-flow security mechanisms do provide this assurance, but only for programmer-specified policies enforced during program development as a static analysis on special-purpose type-safe languages. Not only are these techniques not applicable to many commonly used programs, but they leave the user with no defense against malicious programmers or altered binaries. In this paper, we propose RIFLE, a runtime informationflow security system designed from the user’s perspective. By addressing information-flow security using architectural support, RIFLE gives users a practical way to enforce their own information-flow security policy on all programs. We prove that, contrary to statements in the literature, runtime systems like RIFLE are no less secure than existing language-based techniques. Using a model of the architectural framework and a binary translator, we demonstrate RIFLE’s correctness and illustrate that the performance cost is reasonable. 1.
418|Improving Application Security with Data Flow Assertions|RESIN is a new language runtime that helps prevent security vulnerabilities, by allowing programmers to specify application-level data flow assertions. RESIN provides policy objects, which programmers use to specify assertion code and metadata; data tracking, which allows programmers to associate assertions with application data, and to keep track of assertions as the data flow through the application; and filter objects, which programmers use to define data flow boundaries at which assertions are checked. RESIN’s runtime checks data flow assertions by propagating policy objects along with data, as that data moves through the application, and then invoking filter objects when data crosses a data flow boundary, such as when writing data to the network or a file. Using RESIN, Web application programmers can prevent a range of problems, from SQL injection and cross-site scripting, to inadvertent password disclosure and missing access control checks. Adding a RESIN assertion to an application requires few changes to the existing application code, and an assertion can reuse existing code and data structures. For instance, 23 lines of code detect and prevent three previously-unknown missing access control vulnerabilities in phpBB, a popular Web forum application. Other assertions comprising tens of lines of code prevent a range of vulnerabilities in Python and PHP applications. A prototype of RESIN incurs a 33 % CPU overhead running the HotCRP conference management application.
419|TightLip: Keeping applications from spilling the beans|Access control misconfigurations are widespread and can result in damaging breaches of confidentiality. This paper presents TightLip, a privacy management system that helps users define what data is sensitive and who is trusted to see it rather than forcing them to understand or predict how the interactions of their software packages can leak data. The key mechanism used by TightLip to detect and prevent breaches is the doppelganger process. Doppelgangers are sandboxed copy processes that inherit most, but not all, of the state of an original process. The operating system runs a doppelganger and its original in parallel and uses divergent process outputs to detect potential privacy leaks. Support for doppelgangers is compatible with legacy-code, requires minor modifications to existing operating systems, and imposes negligible overhead for common workloads. SpecWeb99 results show that Apache running on a TightLip prototype exhibits a 5 % slowdown in request rate and response time compared to an unmodified server environment. 1
420|Tainttrace: Efficient flow tracing with dynamic binary rewriting|TaintTrace is a high performance flow tracing tool that protects systems against security exploits. It is based on dynamic execution binary rewriting empowering our tool with fine-grained monitoring of system activities such as the tracking of the usage and propagation of data origi-nated from the network. The challenge lies in minimizing the run-time overhead of the tool. TaintTrace uses a number of techniques such as direct memory mapping to optimize performance. In this paper, we demonstrate that TaintTrace is effective in protecting against various attacks while main-taining a modest slowdown of 5.5 times, offering significant improvements over similar tools. 1
421|Pointless Tainting? Evaluating the Practicality of Pointer Tainting|This paper evaluates pointer tainting, an incarnation of Dynamic Information Flow Tracking (DIFT), which has recently become an important technique in system security. Pointer tainting has been used for two main purposes: detection of privacy-breaching malware (e.g., trojan keyloggers obtaining the characters typed by a user), and detection of memory corruption attacks against non-control data (e.g., a buffer overflow that modifies a user’s privilege level). In both of these cases the attacker does not modify control data such as stored branch targets, so the control flow of the target program does not change. Phrased differently, in terms of instructions executed, the program behaves ‘normally’. As a result, these attacks are exceedingly difficult to detect. Pointer tainting is considered one of the only methods for detecting them in unmodified binaries. Unfortunately, almost all of the incarnations of pointer tainting are flawed. In particular, we demonstrate that the application of pointer tainting to the detection of keyloggers and other privacybreaching malware is problematic. We also discuss whether pointer tainting is able to reliably detect memory corruption attacks against non-control data. We found that pointer tainting generates itself the conditions for false positives. We analyse the problems in detail and investigate various ways to improve the technique. Most have serious drawbacks in that they are either impractical (and incur many false positives still), and/or cripple the technique’s ability to detect attacks. In conclusion, we argue that depending on architecture and operating system, pointer tainting may have some  
422|Laminar: Practical Fine-Grained Decentralized Information Flow Control|Decentralized information flow control (DIFC) is a promising model for writing programs with powerful, end-to-end security guarantees. Current DIFC systems that run on commodity hardware can be broadly categorized into two types: language-level and operating system-level DIFC. Language level solutions provide no guarantees against security violations on system resources, like files and sockets. Operating system solutions can mediate accesses to system resources, but are inefficient at monitoring the flow of information through fine-grained program data structures. This paper describes Laminar, the first system to implement decentralized information flow control using a single set of abstractions for OS resources and heap-allocated objects. Programmers express security policies by labeling data with secrecy and integrity labels, and then access the labeled data in lexically scopedsecurityregions. Laminar enforces the security policies specified by the labels at runtime. Laminar is implemented using a modified Java virtual machine and a new Linux security module. This paper shows that security regions ease incremental deployment and limit dynamic security checks, allowing us to retrofit DIFC policies on four application case studies. Replacing the applications ’ ad-hoc security policies changes less than 10 % of the code, and incurs performance overheads from 1 % to 56%. Whereas prior DIFC systems only support limited types of multithreaded programs, Laminar supports a more general class of multithreaded DIFC programs that can access heterogeneously labeled data.
423|WASP: Protecting Web Applications Using Positive Tainting and Syntax-Aware|Abstract—Many software systems have evolved to include a Web-based component that makes them available to the public via the Internet and can expose them to a variety of Web-based attacks. One of these attacks is SQL injection, which can give attackers unrestricted access to the databases that underlie Web applications and has become increasingly frequent and serious. This paper presents a new highly automated approach for protecting Web applications against SQL injection that has both conceptual and practical advantages over most existing techniques. From a conceptual standpoint, the approach is based on the novel idea of positive tainting and on the concept of syntax-aware evaluation. From a practical standpoint, our technique is precise and efficient, has minimal deployment requirements, and incurs a negligible performance overhead in most cases. We have implemented our techniques in the Web Application SQL-injection Preventer (WASP) tool, which we used to perform an empirical evaluation on a wide range of Web applications that we subjected to a large and varied set of attacks and legitimate accesses. WASP was able to stop all of the otherwise successful attacks and did not generate any false positives. Index Terms—Security, SQL injection, dynamic tainting, runtime monitoring. Ç 1
424|Measuring Channel Capacity to Distinguish Undue Influence |The channel capacity of a program is a quantitative measure of the amount of control that the inputs to a program have over its outputs. Because it corresponds to worst-case assumptions about the probability distribution over those inputs, it is particularly appropriate for security applications where the inputs are under the control of an adversary. We introduce a family of complementary techniques for measuring channel capacity automatically using a decision procedure (SAT or #SAT solver), which give either exact or narrow probabilistic bounds. We then apply these techniques to the problem of analyzing false positives produced by dynamic taint analysis used to detect control-flow hijacking in commodity software. Dynamic taint analysis is based on the principle that an attacker should not be able to control values such as function pointers and return addresses, but it uses a simple binary approximation of control that commonly leads to both false positive and false negative errors. Based on channel capacity, we propose a more refined quantitative measure of influence, which can effectively distinguish between true attacks and false positives. We use a practical implementation of our influence measuring techniques, integrated with a dynamic taint analysis operating on x86 binaries, to classify tainting warnings produced by vulnerable network servers, such as those attacked by the Blaster and SQL Slammer worms. Influence measurement correctly distinguishes real attacks from tainting false positives, a task that would otherwise need to be done manually.
425|Efficient Fine-Grained Binary Instrumentation with Applications to Taint-Tracking|Fine-grained binary instrumentations, such as those for tainttracking, have become very popular in computer security due to their applications in exploit detection, sandboxing, malware analysis, etc. However, practical application of taint-tracking has been limited by high performance overheads. For instance, previous software based techniques for taint-tracking on binary code have typically slowed down programs by a factor of 3 or more. In contrast, source-code based techniques have achieved better performance using high level optimizations. Unfortunately, these optimizations are difficult to perform on binaries since much of the high level program structure required by such static analyses is lost during the compilation process. In this paper, we address this challenge by developing static techniques that can recover some of the higher level structure from x86 binaries. Our new static analysis enables effective optimizations, which are applied in the context of taint tracking. As a result, we achieve a substantial reduction in performance overheads as compared to previous works.
426|Implicit flows: Can’t live with ’em, can’t live without ’em|Abstract. Verifying that programs trusted to enforce security actually do so is a practical concern for programmers and administrators. However, there is a disconnect between the kinds of tools that have been successfully applied to real software systems (such as taint mode in Perl and Ruby), and information-flow compilers that enforce a variant of the stronger security property of noninterference. Tools that have been successfully used to find security violations have focused on explicit flows of information, where high-security information is directly leaked to output. Analysis tools that enforce noninterference also prevent implicit flows of information, where high-security information can be inferred from a program’s flow of control. However, these tools have seen little use in practice, despite the stronger guarantees that they provide. To better understand why, this paper experimentally investigates the explicit and implicit flows identified by the standard algorithm for establishing noninterference. When applied to implementations of authentication and cryptographic functions, the standard algorithm discovers many real implicit flows of information, but also reports an extremely high number of false alarms, most of which are due to conservative handling of unchecked exceptions (e.g., null pointer exceptions). After a careful analysis of all sources of true and false alarms, due to both implicit and explicit flows, the paper concludes with some ideas to improve the false alarm rate, toward making stronger security analysis more practical. 1
427|A Virtual Machine Based Information Flow Control System for Policy Enforcement |The ability to enforce usage policies attached to data in a fine grained manner requires that the system be able to trace and control the flow of information within it. This paper presents the design and implementation of such an information flow control system, named Trishul, as a Java Virtual Machine. In particular we address the hard problem of tracing implicit information flow, which had not been resolved by previous run-time systems and the intricacies added on by the Java architecture. We argue that the security benefits offered by Trishul are substantial enough to counter-weigh the performance overhead of the system as shown by our experiments.
428|Privacy Oracle: A System for Finding Application Leaks with Black Box Differential Testing|We describe the design and implementation of Privacy Oracle, a system that reports on application leaks of user information via the network traffic that they send. Privacy Oracle treats each application as a black box, without access to either its internal structure or communication protocols. This means that it can be used over a broad range of applications and information leaks (i.e., not only Web traffic content or credit card numbers). To accomplish this, we develop a differential testing technique in which perturbations in the application inputs are mapped to perturbations in the application outputs to discover likely leaks; we leverage alignment algorithms from computational biology to find high quality mappings between different byte-sequences efficiently. Privacy Oracle includes this technique and a virtual machine-based testing system. To evaluate it, we tested 26 popular applications, including system and file utilities, media players, and IM clients. We found that Privacy Oracle discovered many small and previously undisclosed information leaks. In several cases, these are leaks of directly identifying information that are regularly sent in the clear (without endto-end encryption) and which could make users vulnerable to tracking by third parties or providers.
429|Using Labeling to Prevent Cross-Service Attacks against Smart Phones|Wireless devices that integrate the functionality of PDAs  and cell phones are becoming commonplace, making di#erent types of  network services available to mobile applications. However, the integration  of di#erent services allows an attacker to cross service boundaries.
430|What You See is What they Get: Protecting users from unwanted use of microphones, camera, and other sensors|Sensors such as cameras and microphones collect privacy-sensitive data streams without the user’s explicit action. Conventional sensor access policies either hassle users to grant applications access to sensors or grant with no approval at all. Once access is granted, an application may collect sensor data even after the application’s interface suggests that the sensor is no longer being accessed. We introduce the sensor-access widget, a graphical user interface element that resides within an application’s display. The widget provides an animated representation of the personal data being collected by its corresponding sensor, calling attention to the application’s attempt to collect the data. The widget indicates whether the sensor data is currently allowed to flow to the application. The widget also acts as a control point through which the user can configure the sensor and grant or deny the application access. By building perpetual disclosure of sensor data collection into the platform, sensor-access widgets enable new access-control policies that relax the tension between the user’s privacy needs and applications ’ ease of access. 1
431|RedFlag: Reducing Inadvertent Leaks by Personal Machines |Reference monitors rely on correct access-control policies to prevent confidential data from leaking. Unfortunately, sensitive data is increasingly stored on personal machines operated by users who are either unwilling or unqualified to properly protect their sensitive files. This often leads to misconfigured applications and damaging leaks. In this paper, we describe RedFlag, a system designed to unobtrusively identify and protect sensitive files on personal machines. Our main insight is that personal machines often receive sensitive data from servers over encrypted network connections. Using this heuristic allows RedFlag to help prevent large classes of leaks, without requiring user-defined policies or changes to existing server-side and client-side applications. 1
432|Precip : Towards practical and retrofittable confidential information protection|A grand challenge in information protection is how to preserve the confidentiality of sensitive information under spyware surveillance. This problem has not been well addressed by the existing access-control mechanisms which cannot prevent the spyware already in a system from monitoring an authorized party’s interactions with sensitive data. Our answer to this challenge is PRECIP, a new security policy model which takes a first step towards practical and retrofittable confidential information protection. This model is designed to offer efficient online protection for commercial applications and operating systems. It intends to be retrofitted to these applications and systems without modifying their code. To this end, PRECIP addresses several practical issues critical to containing spyware surveillance, which however are not well handled by the previous work in access control and information-flow security. Examples include the models for human input devices such as keyboard whose sensitivity level must be dynamically determined, other shared resources such as clipboard and screen which must be accessed by different processes, and the multitasked processes which work on public and sensitive data concurrently. We applied PRECIP to Windows XP to protect the applications for editing or viewing sensitive documents and browsing sensitive websites. We demonstrate that our implementation works effectively against a wide spectrum of spyware, including keyloggers, screen grabbers and file stealers. We also evaluated the overheads of our technique, which are shown to be very small. 1
433|Data Encryption by Excluding Repetitive Character in Cipher Text |Abstract- In modern world, cryptography hackers try to break a cryptographic algorithm or try to retrieve the key, which is needed to encrypt a message (data), by analyzing the insertion or presence of repetitive bits / characters (bytes) in the message and encrypted message to find out the encryption algorithm or the key used for it. So it is must for a good encryption method to exclude the repetitive terms such that no trace of repetitions can be tracked down., Somdip Dey, Joyshree Nath and Ashoke Nath (SJA) is an amalgamation of Modified Caesar cipher(MCC), Bit Rotation Reversal(BRR), Neeraj Khanna, Joel James, Joyshree Nath, Sayantyan Chakraborty, Amlan Chakrabarti, Asoke nath (NJJSAA), Function Encryption (FE) methods.By using MCC, BRR, NJJSAA and FE methods we can reduce time complexity by exluding repetitive characters in the cipher text. The proposed method is robust in terms of efficiency and computational costs
434|Cryptology: From Caesar Ciphers to Public-key Cryptosystems|He received an M.A. and Ph.D. (in commutative algebra under the direction of David
435|Bro: A System for Detecting Network Intruders in Real-Time|We describe Bro, a stand-alone system for detecting network intruders in real-time by passively monitoring a network link over which the intruder&#039;s traffic transits. We give an overview of the system&#039;s design, which emphasizes highspeed (FDDI-rate) monitoring, real-time notification, clear separation between mechanism and policy, and extensibility. To achieve these ends, Bro is divided into an “event engine” that reduces a kernel-filtered network traffic stream into a series of higher-level events, and a “policy script interpreter” that interprets event handlers written in a specialized language used to express a site&#039;s security policy. Event handlers can update state information, synthesize new events, record information to disk, and generate real-time notifications via syslog. We also discuss a number of attacks that attempt to subvert passive monitoring systems and defenses against these, and give particulars of how Bro analyzes the six applications integrated into it so far: Finger, FTP, Portmapper, Ident, Telnet and Rlogin. The system is publicly available in source code form.  
436|StackGuard: Automatic adaptive detection and prevention of buffer-overflow attacks|1
438|Flow-Sensitive Type Qualifiers|We present a system for extending standard type systems with flow-sensitive type qualifiers. Users annotate their programs with type qualifiers, and inference checks that the annotations are correct. In our system only the type qualifiers are modeled flow-sensitively - the underlying standard types are unchanged, which allows us to obtain an efficient constraint-based inference algorithm that integrates flow-insensitive alias analysis, effect inference, and ideas from linear type systems to support strong updates. We demonstrate the usefulness of flow-sensitive type qualifiers by finding a number of new locking bugs in the Linux kernel.
439|Dynamic Program Slicing|The conventional notion of a program slice---the set of all statements that might affect the value of a variable occurrence---is totally independent of the program input values. Program debugging, however, involves analyzing the program behavior under the specific inputs that revealed the bug. In this paper we address the dynamic counterpart of the static slicing problem---finding all statements that really affected the value of a variable occurrence for the given program inputs. Several approaches for computing dynamic slices are examined. The notion of a Dynamic Dependence Graph and its use in computing dynamic slices is discussed. We introduce the concept of a Reduced Dynamic Dependence Graph whose size does not depend on the length of execution history, which is unbounded in general, but whose size is bounded and is proportional to the number of dynamic slices arising during the program execution.
440|CCured: Type-Safe Retrofitting of Legacy Code|In this paper we propose a scheme that combines type inference and run-time checking to make existing C programs type safe. We describe the CCured type system, which extends that of C by separating pointer types according to their usage. This type system allows both pointers whose usage can be verified statically to be type safe, and pointers whose safety must be checked at run time. We prove a type soundness result and then we present a surprisingly simple type inference algorithm that is able to infer the appropriate pointer kinds for existing C programs. Our experience with the CCured system shows that the inference is very effective for many C programs, as it is able to infer that most or all of the pointers are statically verifiable to be type safe. The remaining pointers are instrumented with efficient run-time checks to ensure that they are used safely. The resulting performance loss due to run-time checks is 0–150%, which is several times better than comparable approaches that use only dynamic checking. Using CCured we have discovered programming bugs in established C programs such as several SPECINT95 benchmarks.
441|Autograph: Toward automated, distributed worm signature detection|Today’s Internet intrusion detection systems (IDSes) monitor edge networks ’ DMZs to identify and/or filter malicious flows. While an IDS helps protect the hosts on its local edge network from compromise and denial of service, it cannot alone effectively intervene to halt and reverse the spreading of novel Internet worms. Generation of the worm signatures required by an IDS—the byte patterns sought in monitored traffic to identify worms—today entails non-trivial human labor, and thus significant delay: as network operators detect anomalous behavior, they communicate with one another and manually study packet traces to produce a worm signature. Yet intervention must occur early in an epidemic to halt a worm’s spread. In this paper, we describe Autograph, a system that automatically generates signatures for novel Internet worms that propagate using TCP transport. Autograph generates signatures by analyzing the prevalence of portions of flow payloads, and thus uses no knowledge of protocol semantics above the TCP level. It is designed to produce signatures that exhibit high sensitivity (high true positives) and high specificity (low false positives); our evaluation of the system on real DMZ traces validates that it achieves these goals. We extend Autograph to share port scan reports among distributed monitor instances, and using trace-driven simulation, demonstrate the value of this technique in speeding the generation of signatures for novel worms. Our results elucidate the fundamental trade-off between early generation of signatures for novel worms and the specificity of these generated signatures. 1
442|Improving Host Security with System Call Policies|We introduce a system that eliminates the need to run programs in privileged process contexts. Using our system, programs run unprivileged but may execute certain operations with elevated privileges as determined by a configurable policy eliminating the need for suid or sgid binaries. We present the design and analysis of the &#034;Systrace&#034; facility which supports fine grained process confinement, intrusion detection, auditing and privilege elevation. It also facilitates the often difficult process of policy generation. With Systrace, it is possible to generate policies automatically in a training session or generate them interactively during program execution. The policies describe the desired behavior of services or user applications on a system call level and are enforced to prevent operations that are not explicitly permitted. We show that Systrace is efficient and does not impose significant performance penalties.
443|Automated worm fingerprinting|Network worms are a clear and growing threat to the security of today’s Internet-connected hosts and networks. The combination of the Internet’s unrestricted connectivity and widespread software homogeneity allows network pathogens to exploit tremendous parallelism in their propagation. In fact, modern worms can spread so quickly, and so widely, that no human-mediated reaction can hope to contain an outbreak. In this paper, we propose an automated approach for quickly detecting previously unknown worms and viruses based on two key behavioral characteristics – a common exploit sequence together with a range of unique sources generating infections and destinations being targeted. More importantly, our approach – called “content sifting ”  – automatically generates precise signatures that can then be used to filter or moderate the spread of the worm elsewhere in the network. Using a combination of existing and novel algorithms we have developed a scalable content sifting implementation with low memory and CPU requirements. Over months of active use at UCSD, our Earlybird prototype system has automatically detected and generated signatures for all pathogens known to be active on our network as well as for several new worms and viruses which were unknown at the time our system identified them. Our initial experience suggests that, for a wide range of network pathogens, it may be practical to construct fully automated defenses – even against so-called “zero-day” epidemics. 1
444|Secure Execution Via Program Shepherding|We introduce program shepherding, a method for monitoring control flow transfers during program execution to enforce a security policy. Program shepherding provides three techniques as building blocks for security policies. First, shepherding can restrict execution privileges on the basis of code origins. This distinction can ensure that malicious code masquerading as data is never executed, thwarting a large class of security attacks. Second, shepherding can restrict control transfers based on instruction class, source, and target. For example, shepherding can forbid execution of shared library code except through declared entry points, and can ensure that a return instruction only targets the instruction after a call. Finally, shepherding guarantees that sandboxing checks placed around any type of program operation will never be bypassed. We have implemented these capabilities efficiently in a runtime system with minimal or no performance penalties. This system operates on unmodified native binaries, requires no special hardware or operating system support, and runs on existing IA-32 machines under both Linux and Windows.
445|Address obfuscation: an efficient approach to combat a broad range of memory error exploits|Rights to individual papers remain with the author or the author&#039;s employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein.
446|Anomalous payload-based network intrusion detection|We present a payload-based anomaly detector, we call PAYL, for intrusion detection. PAYL models the normal application payload of network traffic in a fully automatic, unsupervised fashion. The method we choose is very efficient; our goal is to deploy the detector in high bandwidth environments either on a firewall, a network appliance, a proxy server or on the target hosts. We first compute during a training phase a profile byte frequency distribution and their standard deviation of the application payload flowing to a single host and port. We then use Mahalanobis distance during the detection phase to calculate the similarity of new data against the pre-computed profile. The detector compares this measure against a threshold and generates an alert when the distance of the new input exceeds this threshold. The model is host- and port-specific and is conditioned on the payload length. Thus, a set of profiles are computed for every possible length payload. A second phase clusters the profiles to increase accuracy and decrease resource consumption. The method has the advantage of being very fast to compute, is state-less and does not parse the input stream, generates a small model, and can be easily modified to an incremental online learning algorithm so that the model is continuously updated to maintain an accurate normal model in real-time. The modeling method is completely unsupervised, and is tolerant to noise in the training data. Furthermore, the method is also resistant to mimicry-attack; attackers would need to model the site’s normal payload distributions in order to pad their poisoned payload to go unnoticed by the
447|Valgrind: A program supervision framework|a;1
448|Countering Code-Injection Attacks With Instruction-Set Randomization|We describe a new, general approach for safeguarding systems against any type of code-injection attack. We apply Kerckhoff’s principle, by creating process-specific randomized instruction sets (e.g., machine instructions) of the system executing potentially vulnerable software. An attacker who does not know the key to the randomization algorithm will inject code that is invalid for that randomized processor, causing a runtime exception. To determine the difficulty of integrating support for the proposed mechanism in the operating system, we modified the Linux kernel, the GNU binutils tools, and the bochs-x86 emulator. Although the performance penalty is significant, our prototype demonstrates the feasibility of the approach, and should be directly usable on a suitable-modified processor (e.g., the Transmeta Crusoe). Our approach is equally applicable against code-injecting attacks in scripting and interpreted languages, e.g., web-based SQL injection. We demonstrate this by modifying the Perl interpreter to permit randomized script execution. The performance penalty in this case is minimal. Where our proposed approach is feasible (i.e., in an emulated environment, in the presence of programmable or specialized hardware, or in interpreted languages), it can serve as a low-overhead protection mechanism, and can easily complement other mechanisms.
449|Backwards-compatible bounds checking for arrays and pointers in C programs|function-typed variables, virtual functions, and 7/7 call-backs. 8/8 Maintain shadow bitmap:  Maintain a map indicating which storage regions are valid. Update it when stack allocations,  malloc  and  free  occur. Augment each memory access instruction with code to check whether the address is valid [Hastings and Joyce, 1992].   Advantages:  Fairly ecient Doesn&#039;t require access to source code, so can (must) be applied to all constituents of application    False negatives - fails to ag accesses to a valid region using an 9/9 improperly-derived pointer 10/10 Summarise requirements:  Track intended referent for each pointer  It is not good enough just to check that accesses are to valid locations  No change to pointer representation  In order to inter-operate with unchecked code without restriction, no information can be bundled with the pointer. 11/11 How to do it . . . 3: the central idea  Invariant: Assume all stored pointers are properly-derived pointers to their intended referent Im
450|Honeycomb - Creating Intrusion Detection Signatures Using Honeypots|Abstract — This paper describes a system for automated generation of attack signatures for network intrusion detection systems. Our system applies pattern-matching techniques and protocol conformance checks on multiple levels in the protocol hierarchy to network traffic captured a honeypot system. We present results of running the system on an unprotected cable modem connection for 24 hours. The system successfully created precise traffic signatures that otherwise would have required the skills and time of a security officer to inspect the traffic manually. Index Terms — network intrusion detection, traffic signatures, honeypots, pattern detection, protocol analysis, longest-commonsubstring algorithms, suffix trees. I.
451|A Practical Dynamic Buffer Overflow Detector|Despite previous efforts in auditing software manually and automatically, buffer overruns are still being discovered in programs in use. A dynamic bounds checker detects buffer overruns in erroneous software before it occurs and thereby prevents attacks from corrupting the integrity of the system. Dynamic buffer overrun detectors have not been adopted widely because they either (1) cannot guard against all buffer overrun attacks, (2) break existing code, or (3) incur too high an overhead. This paper presents a practical detector called CRED (C Range Error Detector) that avoids each of these deficiencies. CRED finds all buffer overrun attacks as it directly checks for the bounds of memory accesses. Unlike the original referent-object based bounds-checking technique, CRED does not break existing code because it uses a novel solution to support program manipulation of out-of-bounds addresses. Finally, by restricting the bounds checks to strings in a program, CRED’s overhead is greatly reduced without sacrificing protection in the experiments we performed. CRED is implemented as an extension of the GNU C compiler version 3.3.1. The simplicity of our design makes possible a robust implementation that has been tested on over 20 open-source programs, comprising over 1.2 million lines of C code. CRED proved effective in detecting buffer overrun attacks on programs with known vulnerabilities, and is the only tool found to guard against a testbed of 20 different buffer overflow attacks[34]. Finding overruns only on strings impose an overhead of less
452|Shield: Vulnerability-Driven Network Filters for Preventing Known Vulnerability Exploits|Software patching has not been an effective first-line defense preventing large-scale worm attacks, even when patches had long been available for their corresponding vulnerabilities. Generally, people have been reluctant to patch their systems immediately, because patches are perceived to be unreliable and disruptive to apply. To address this problem, we propose a first-line worm defense in the network stack, using shields -- vulnerability-specific, exploit-generic network filters installed in end systems once a vulnerability is discovered and before the patch is applied. These filters examine the incoming or outgoing traffic of vulnerable applications, and drop traffic that exploits vulnerabilities. Shields are less disruptive to install and uninstall, easier to test for bad side effects, and hence more reliable than traditional software patches. In this paper, we show...
453|Hardening COTS Software with Generic Software Wrappers|Numerous techniques exist to augment the security functionality of Commercial Off-The-Shelf (COTS) applications and operating systems, making them more suitable for use in mission-critical systems. Although individually useful, as a group these techniques present difficulties to system developers because they are not based onacommon framework which might simplify integration and promote portability and reuse. This paper presents techniques for developing Generic Software Wrappers -- protected, non-bypassable kernel-resident software extensions for augmenting security without modi cation of COTS source. We describe the key elements of our work: our high-level Wrapper Definition Language (WDL), and our framework for configuring, activating, and managing wrappers. We also discuss code reuse, automatic management of extensions, a framework for system-building through composition, platform-independence, and our experiences with our Solaris and FreeBSD prototypes.  
454|FormatGuard: Automatic Protection From printf Format String Vulnerabilities|Symposium
456|Hunting for metamorphic|As virus writers developed numerous polymorphic engines, virus scanners became stronger in their defense against them. A virus scanner which used a code emulator to detect viruses looked like it was on steroids compared to those without an emulator-based scanning engine. Nowadays, most polymorphic viruses are considered boring. Even though they can be extremely hard to detect, most of today’s products are able to deal with them relatively easily. These are the scanners that survived the DOS polymorphic days. For some of the scanners DOS polymorphic viruses meant the ‘end of days’. Other scanners died with the macro virus problem. For most products the next challenge to take is 32-bit metamorphosis. Metamorphic viruses are nothing new. We have seen them in DOS days, though some of them, like ACG, already used 32-bit instructions. The next step is 32-bit metamorphosis under Windows environments. Virus writers already took the first step in that direction. In this paper the authors will examine metamorphic engines to provide a better general understanding of the problem that we are facing. The authors also provide detection examples of some of the metamorphic viruses. VIRUS BULLETIN CONFERENCE ©2001 Virus Bulletin Ltd, The Pentagon, Abingdon, Oxfordshire, OX14 3YP, England. Tel +44 1235 555139. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form, without the prior written permission of the publishers. 124 • SZÖR &amp; FERRIE, HUNTING FOR METAMORPHIC 1
457|MAPbox: Using Parameterized Behavior Classes to Confine Applications|Designing a suitable mechanism to confine commonly used applications is challenging as such a mechanism needs to satisfy conflicting requirements. The trade-off is between configurability and ease of use. In this paper, we present the design, implementation and evaluation of MAPbox,  a general-purpose confinement mechanism that retains the ease of use of specialized sandboxes such as Janus and SBOX while providing significantly more configurability. The key idea is to group application behaviors into classes based on the expected functionality and the resources required to achieve that functionality. Classification of behaviors provides a set of behavior labels (class names) that can be used to concisely communicate the expected functionality of programs between the provider and the users. This is similar to the MIME-types used to concisely describe the expected format of data files. Classification of application behaviors also allows class-specific sandboxes to be built and instantiat...
458|Using CQUAL for static analysis of authorization hook placement|The Linux Security Modules (LSM) framework is a set of authorization hooks for implementing flexible access control in the Linux kernel. While much effort has been devoted to defining the module interfaces, little attention has been paid to verifying the correctness of hook placement. This paper presents a novel approach to the verification of LSM authorization hook placement using CQUAL, a type-based static analysis tool. With a simple CQUAL lattice configuration and some GCC-based analyses, we are able to verify complete mediation of operations on key kernel data structures. Our results reveal some potential security vulnerabilities of the current LSM framework, one of which we demonstrate to be exploitable. Our experiences demonstrate that combinations of conceptually simple tools can be used to perform fairly complex analyses. 1
459|Mitigating buffer overflows by operating system randomization|Abstract We propose three methods for mitigating buffer overflows by using operating system randomization: randomization of system call mappings, randomization of global library entry points, and randomization of stack placement. These mechanisms are light weight. They increase the heterogeneity of systems and make it more difficult for attackers to exploit buffer overflow vunerabilities. We also present a mechanism, timed capabilities, for more secure resource management.
460|On gray-box program tracking for anomaly detection|Rights to individual papers remain with the author or the author&#039;s employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein.
461|TRON: Process-Specific File Protection for the UNIX Operating System|The file protection mechanism provided in UNIX is insufficient for current computing environments. While the UNIX file protection system attempts to protect users from attacks by other users, it does not directly address the agents of destruction--- executing processes. As computing environments become more interconnected and interdependent, there is increasing pressure and opportunity for users to acquire and test non--secure, and possibly malicious, software. We introduce TRON, a process--level discretionary access control system for UNIX. TRON allows users to specify capabilities for a process&#039; access to individual files, directories, and directory trees. These capabilities are enforced by system call wrappers compiled into the operating system kernel. No privileged system calls, special files, system administrator intervention, or changes to the file system are required. Existing UNIX programs can be run without recompilation under TRON--enhanced UNIX. Thus, TRON improves UNIX secu...
462|A Network Worm Vaccine Architecture|The ability of worms to spread at rates that effectively preclude human-directed reaction has elevated them to a first-class security threat to distributed systems. We present the first reaction mechanism that seeks to automatically patch vulnerable software. Our system employs a collection of sensors that detect and capture potential worm infection vectors. We automatically test the effects of these vectors on appropriately-instrumented sandboxed instances of the targeted application, trying to identify the exploited software weakness. Our heuristics allow us to automatically generate patches that can protect against certain classes of attack, and test the resistance of the patched application against the infection vector. We describe our system architecture, discuss the various components, and propose directions for future research.
463|we contain Internet worms|Worm containment must be automatic because worms can spread too fast for humans to respond. Recent work has proposed a network centric approach to automate worm containment: network traffic is analyzed to derive a packet classifier that blocks (or rate-limits) worm propagation. This approach has fundamental limitations because the analysis has no information about the application vulnerabilities exploited by worms. This paper proposes Vigilante, a new host centric approach for automatic worm containment that addresses these limitations. Vigilante relies on collaborative worm detection at end hosts in the Internet but does not require hosts to trust each other. Hosts detect worms by analysing attempts to infect applications and broadcast self-certifying alerts (SCAs) when they detect a worm. SCAs are automatically generated machine-verifiable proofs of vulnerability; they can be independently and inexpensively verified by any host. Hosts can use SCAs to generate filters or patches that prevent infection. We present preliminary results showing that Vigilante can effectively contain fast spreading worms that exploit unknown vulnerabilities. 1.
465|Buttercup: On network-based detection of polymorphic buffer overflow vulnerabilities|Attack polymorphism is a powerful tool for the attackers in the Internet to evade signature-based intrusion detection/prevention systems. In addition, new and faster Internet worms can be coded and launched easily by even high school students anytime against our critical infrastructures, such as DNS or update servers. We believe that polymorphic Internet worms will be developed in the future such that many of our current solutions might have a very small chance to survive. In this paper, we propose a simple solution called “Buttercup ” to counter against attacks based on buffer-overflow exploits (such as CodeRed, Nimda, Slammer, and Blaster). We have implemented our idea in SNORT, and included 19 return address ranges of buffer-overflow exploits. With a suite of tests against 55 TCPdump traces, the false positive rate for our best algorithm is as low as 0.01%. This indicates that, potentially, Buttercup can drop 100 % worm attack packets on the wire while only 0.01 % of the good packets will be sacrificed.
466|Run-time type checking for binary programs|Abstract. Many important software systems are written in the C programming language. Unfortunately, the C language does not provide strong safety guarantees, and many common programming mistakes introduce type errors that are not caught by the compiler. These errors only manifest themselves at run time through unexpected program behavior, and it is often hard to isolate and identify their causes. This paper presents the Hobbes run-time type checker for compiled C programs. Our tool interprets compiled binaries, tracks type information for all memory and register locations, and reports warnings when a variety of type errors occur. Because the Hobbes type checker does not rely on source code, it is effective in many situations where similar tools are not, such as when full source code is not available or when C source is linked with program fragments written in assembly or other languages. 1
467|Remark on a “Non-Breakable Data Encryption” Scheme by |We break a cryptosystem by Kish and Sethuraman and show that the authentication problem of their protocol can be fixed. We prove that finding an instance of this cryptosystem, which meets the design criteria, would show that P ? = NP. Keywords: Classical information; encryption; public key cryptography; Kish/Sethuraman cipher
468|One-way permutations and self-witnessing languages|A desirable property of one-way functions is that they be total, one-to-one, and onto—in other words, that they be permutations. We prove that one-way permutations exist exactly if PaUP-coUP: This provides the first characterization of the existence of one-way permutations based on a complexity-class separation and shows that their existence is equivalent to a number of previously studied complexitytheoretic hypotheses. We study permutations in the context of witness functions of nondeterministic Turing machines. A language is in PermUP if, relative to some unambiguous, nondeterministic, polynomial-time Turingmachine acceptingthe language, the function mappingeach stringto its unique witness is a permutation of the members of the language. We show that, under standard complexity-theoretic assumptions, PermUP is a strict subset of UP. We study SelfNP, the set of all languages such that, relative to some nondeterministic, polynomial-time Turing machine that accepts the language, the set of all witnesses of strings in the language is identical to the language itself. We show that SATASelfNP; and, under standard complexity-theoretic assumptions, SelfNPaNP:
469|A Survey on Sensor Networks|Recent advancement in wireless communica- tions and electronics has enabled the develop- ment of low-cost sensor networks. The sensor networks can be used for various application areas (e.g., health, military, home). For different application areas, there are different technical issues that researchers are currently resolving. The current state of the art of sensor networks is captured in this article, where solutions are discussed under their related protocol stack layer sections. This article also points out the open research issues and intends to spark new interests and developments in this field.
470|Directed Diffusion: A scalable and robust communication paradigm for sensor networks|Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.
471|Minimum energy mobile wireless networks| We describe a distributed position-based network protocol optimized for minimum energy consumption in mobile wireless networks that support peer-to-peer communications. Given any number of randomly deployed nodes over an area, we illustrate that a simple local optimization scheme executed at each node guarantees strong connectivity of the entire network and attains the global minimum energy solution for stationary networks. Due to its localized nature, this protocol proves to be self-reconfiguring and stays close to the minimum energy solution when applied to mobile networks. Simulation results are used to verify the performance of the protocol. 
472|A Transmission Control Scheme for Media Access in Sensor Networks|We study the problem of media access control in the novel regime of sensor networks, where unique application behavior and tight constraints in computation power, storage, energy resources, and radio technology have shaped this design space to be very different from that found in traditional mobile computing regime. Media access control in sensor networks must not only be energy efficient but should also allow fair bandwidth allocation to the infrastructure for all nodes in a multihop network. We propose an adaptive rate control mechanism aiming to support these two goals and find that such a scheme is most effective in achieving our fairness goal while being energy effcient for both low and high duty cycle of network traffic.
473|Minimum energy mobile wireless networks revisited|Energy conservation is a critical issue in designing wireless ad hoc networks, as the nodes are powered by batteries only. Given a set of wireless network nodes, the directed weighted transmission graph Gt has an edge uv if and only if node v is in the transmission range of node u and the weight of uv is typically defined as II,,vll + c for a constant 2 &lt;_ t ~ &lt; 5 and c&gt; O. The minimum power topology Gm is the smallest subgraph of Gt that contains the shortest paths between all pairs of nodes, i.e., the union of all shortest paths. In this paper, we described a distributed position-based networking protocol to construct an enclosure graph G~, which is an approximation of Gin. The time complexity of each node u is O(min(dG ~ (u)dG ~ (u), dG ~ (u) log dG ~ (u))), where dc(u) is the degree of node u in a graph G. The space required at each node to compute the minimum power topology is O(dG ~ (u)). This improves the previous result that computes Gm in O(dG, (u) a) time using O(dGt(U) 2) spaces. We also show that the average degree dG,(u) is usually a constant, which is at most 6. Our result is first developed for stationary network and then extended to mobile networks. I.
474|Near ground wideband channel measurement|Abstract- Frequency domain channel propagation measurements in the 800-1000 MHz band have been performed with ground-lying antennas. The range of path-loss exponent and shadowing variance for indoor and outdoor environment were determined. The range of these values roughly agree with those measured for higher elevation antennas. Frequency selectivity of the RF channel was also characterized by means of determining average coherence bandwidth (CBW). It was observed that there is a relationship between CBW and distance between transmitting and receiving antennas. I.
475|Low-power directsequence spread-spectrum modem architecture for distributed wireless sensor networks|Emerging CMOS and MEMS technologies enable the implementation of a large number of wireless distributed microsensors that can be easily and rapidly deployed to form highly redundant, self-configuring, and ad hoc sensor networks. To facilitate ease of deployment, these sensors should operate on battery for extended periods of time. A particular challenge in maintaining extended battery lifetime lies in achieving communications with low power. This paper presents a directsequence spread-spectrum modem architecture that provides robust communications for wireless sensor networks while dissipating very low power. The modem architecture has been verified in an FPGA implementation that dissipates only 33 mW for both transmission and reception. The implementation can be easily mapped to an ASIC technology with an estimated power performance of less than 1 mW.
476| A Stream Enabled Routing (SER) Protocol for Sensor Networks   (2002) |As the number of communication components can be integrated into a single chip increases, the possibility of high volume but low cost sensor nodes is realizable in the near future. Each sensor node can be designed to perform a single or multiple sensing operations, e.g., detecting temperature, seismic activity, object movement, and environmental pollution. As a result, a routing protocol must provide the quality of service (QoS) needed by the sensor nodes. A new routing protocol called Stream Enabled Routing (SER) is proposed to allow the sources choose the routes based on the instruction given by the sinks. It also takes into account the available energy of the sensor nodes. Also, SER allows the sink to give new instruction to the sources without setting up another path. Sources are the sensor nodes in the sensor field that are performing the sensing task. As a result, an interactive user-to-sources communication is achieved. In addition, the routing protocol is shown mathematically to perform well in the sensor network environment.
479|Domain names - Implementation and Specification|This RFC describes the details of the domain system and protocol, and assumes that the reader is familiar with the concepts discussed in a companion RFC, &#034;Domain Names- Concepts and Facilities &#034; [RFC-1034]. The domain system is a mixture of functions and data types which are an official protocol and functions and data types which are still experimental. Since the domain system is intentionally extensible, new data types and experimental behavior should always be expected in parts of the system beyond the official protocol. The official protocol parts include standard queries, responses and the Internet class RR data formats (e.g., host addresses). Since the previous RFC set, several definitions have changed, so some previous definitions are obsolete. Experimental or obsolete features are clearly marked in these RFCs, and such information should be used with caution. The reader is especially cautioned not to depend on the values which appear in examples to be current or complete, since their purpose is
480|Assigned Numbers|Status of this Memo
481|Requirements for Internet Hosts -- Application and Support|This RFC is an official specification for the Internet community. It incorporates by reference, amends, corrects, and supplements the primary protocol standards documents relating to hosts. Distribution of this document is unlimited. Summary This RFC is one of a pair that defines and discusses the requirements for Internet host software. This RFC covers the application and support protocols; its companion RFC-1122 covers the communication protocol layers: link layer, IP layer, and transport layer.
482|Scalable Feedback Control for Multicast Video Distribution in the Internet|We describe a mechanism for scalable control of multicast continuous media streams. The mechanism uses a novel probing mechanism to solicit feedback information in a scalable manner and to estimate the number of receivers. In addition, it separates the congestion signal from the congestion control algorithm, so as to cope with heterogeneous networks. This mechanism has been implemented in the IVS videoconference system using options within RTP to elicit information about the quality of the video delivered to the receivers. The H.261 coder of IVS then uses this information to adjust its output rate, the goal being to maximize the perceptual quality of the image received at the destinations while minimizing the bandwidth used by the video transmission. We find that our prototype control mechanism is well suited to the Internet environment. Furthermore, it prevents video sources from creating congestion in the Internet. Experiments are underway to investigate how the scalable probing mech...
483|The synchronization of periodic routing messages|Abstract — The paper considers a network with many apparently-independent periodic processes and discusses one method by which these processes can inadvertent Iy become synchronized. In particular, we study the synchronization of periodic routing messages, and offer guidelines on how to avoid inadvertent synchronization. Using simulations and analysis, we study the process of synchronization and show that the transition from unsynchronized to synchronized traffic is not one of gradual degradation but is instead a very abrupt ‘phase transition’: in general, the addition of a single router will convert a completely unsynchronized traffic stream into a completely synchronized one. We show that synchronization can be avoided by the addition of randomization to the tra~c sources and quantify how much randomization is necessary. In addition, we argue that the inadvertent synchronization of periodic processes is likely to become an increasing problem in computer networks.
484|RTP profile for audio and video conferences with minimal control|This document is an Internet-Draft. Internet-Drafts are working
485|Address Allocation for Private Internets|Status of this Memo This document specifies an Internet Best Current Practices for the Internet Community, and requests discussion and suggestions for improvements. Distribution of this memo is unlimited. 1.
486|Security mechanisms in high-level network protocols|The implications of adding security mechanisms to high-level network protocols operating in an open-system environment are analyzed. First the threats to security that may arise in such an environment are described, and then a set of goals for communications security measures is established. This is followed by a brief description of the two basic
487|Congestion control principles|Status of this Memo This document is an Internet-Draft and is in full conformance with all provisions of Section 10 of RFC2026. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months and may be updated, replaced, or obsoleted by other documents at any time. It is inappropriate to use Internet- Drafts as reference material or to cite them other than as &#034;work in progress.&#034; The list of current Internet-Drafts can be accessed at
488|Dynamic QoS Control of Multimedia Applications based on RTP|We describe a mechanism for dynamic adjustment of the bandwidth requirements of multimedia applications.  The sending application uses RTP receiver reports to compute packet loss and jitter. Based  on these metrics the congestion state seen by the receivers is determined and the bandwidth is adjusted  by a linear regulator with dead zone.  The suggested mechanism has been implemented and controls the bandwidth of the vic video conferencing  system. Currently we are evaluating the proposed algorithms by simulations and experiments on  the Internet and on our local ATM network.  1 Introduction  Multimedia applications pose unique challenges for network control and management: they offer high data rates, stringent real-time contraints, long connection durations and relatively inflexible demands on bandwidth. Due to the long connection duration, the standard cycle of QoS negotiation purely at the beginning of a session leads either to high call rejection probability at busy times or unnece...
489|Issues in Designing a Transport Protocol for Audio and Video Conferences and other. . .|This memorandum is a companion document to the current version of the RTP protocol specification draft-ietf-avt-rtp-*.ftxt,psg. It discusses protocol aspects of transporting real-time services (for example, voice or video) over packet-switched networks such as the Internet. It compares and evaluates design alternatives for a real-time transport protocol, providing rationales for the design decisions made for RTP. Also covered are issues of port assignment and multicast address allocation. An appendix provides a comprehensive glossary of terms related to multimedia conferencing. This document is a product of the Audio-Video Transport working group within the Internet Engineering Task Force. Comments are solicited and should be addressed to the working group&#039;s mailing list at rem-conf@es.net and/or the author(s).  INTERNET-DRAFT draft-ietf-avt-issues-02.ps May 9, 1994 Contents  1 Introduction 4 2 Goals 7 3 Services 9  3.1 Control and Data : : : : : : : : : : : : : : : : : : : : : : : : ...
490|Randomness Recommendations for Security|Security systems today are built on increasingly strong cryptographic algorithms that foil pattern analysis attempts. However, the security of these systems is dependent on generating secret quantities for passwords, cryptographic keys, and similar quantities. The use of pseudo-random processes to generate secret quantities can result in pseudo-security. The sophisticated attacker of these security  systems may find it easier to reproduce the environment that produced the secret quantities, searching the resulting small set of  possibilities, than to locate the quantities in the whole of the number space.
491|Security services for multimedia conferencing|Multimedia conferencing is the use of mixed media such as real-time audio, video, and groupware for group tele-collaboration. Multimedia conferencing may well become as widespread an application on network computers as electronic mail. The demand for security in multimedia conferencing is high because the users ’ expectation of being monitored is high. In this paper we discuss security services for protecting multimedia conferencing, how well these services scale to large conferences, and what
492|Status of this Memo Telnet Data Encryption Option|This document is an Internet-Draft. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months and may be updated, replaced, or obsoleted by other documents at any time. It is inappropriate to use Internet- Drafts as reference material or to cite them other than as &#034;work in progress.&#034; To view the entire list of current Internet-Drafts, please check the &#034;1id-abstracts.txt &#034; listing contained in the Internet-Drafts Shadow Directories on ftp.is.co.za (Africa), ftp.nordu.net (Europe), munnari.oz.au (Pacific Rim), ds.internic.net (US East Coast), or ftp.isi.edu (US West Coast).
493|Telnet Protocol Specification|The Internet Protocol (IP) [1] is used for host-to-host datagram service in a system of interconnected networks called the Catenet [2]. The network connecting devices are called Gateways. These gateways communicate between themselves for control purposes
494|Oracle Advanced Security Transparent Data Encryption Using|The following is intended to outline our general product direction. It is intended for information purposes only, and may not be incorporated into any contract. It is not a commitment to deliver any material, code, or functionality, and should not be relied upon in making purchasing decisions. The development, release, and
495|Stream Control Transmission Protocol|This document is an Internet-Draft and is in full conformance with all provisions of Section 10 of RFC 2026. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months and may be updated, replaced, or obsoleted by other documents at any time. It is inappropriate to use Internet- Drafts as reference material or to cite them other than as ‘‘work in progress.’’ The list of current Internet-Drafts can be accessed at
496|On Estimating End-to-End Network Path Properties|The more information about current network conditions available to a transport protocol, the more efficiently it can use the network to transfer its data. In networks such as the Internet, the transport protocol must often form its own estimates of network properties based on measurements performed by the connection endpoints. We consider two basic transport estimation problems: determining the setting of the retransmission timer (RTO) for a reliable protocol, and estimating the bandwidth available to a connection as it begins. We look at both of these problems in the context of TCP, using a large TCP measurement set [Pax97b] for trace-driven simulations. For RTO estimation, we evaluate a number of different algorithms, finding that the performance of the estimators is dominated by their minimum values, and to a lesser extent, the timer granularity, while being virtually unaffected by how often round-trip time measurements are made or the settings of the parameters in the exponentially-weighted moving average estimators commonly used. For bandwidth estimation, we explore techniques previously sketched in the literature [Hoe96, AD98] and find that in practice they perform less well than anticipated. We then develop a receiver-side algorithm that performs significantly better. 1
498|TCP congestion control with a misbehaving receiver|In this paper, we explore the operation of TCP congestion control when the receiver can misbehave, as might occur with a greedy Web client. We first demonstrate that there are simple attacks that allow a misbehaving receiver to drive a standard TCP sender arbitrarily fast, without losing end-to-end reliability. These attacks are widely applicable because they stem from the sender behavior specified in RFC 2581 rather than implementation bugs. We then show that it is possible to modify TCP to eliminate this undesirable behavior entirely, without requiring assumptions of any kind about receiver behavior. This is a strong result: with our solution a receiver can only reduce the data transfer rate by misbehaving, thereby eliminating the incentive to do so. 1
499|Randomness Requirements for Security|This document is intended to become a Best Current Practice. Comments should be sent to the authors. Distribution is unlimited. This document is an Internet-Draft and is in full conformance with all provisions of Section 10 of RFC 2026. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months and may be updated, replaced, or obsoleted by other documents at any time. It is inappropriate to use Internet-Drafts as reference material or to cite them other than as &#034;work in progress. &#034; The list of current Internet-Drafts can be accessed at http://www.ietf.org/ietf/1id-abstracts.txt The list of Internet-Draft Shadow Directories can be accessed at
500|A Key-Management Scheme for Distributed Sensor Networks|Distributed Sensor Networks (DSNs) are ad-hoc mobile networks that include sensor nodes with limited computation and communication capabilities. DSNs are dynamic in the sense that they allow addition and deletion of sensor nodes after deployment to grow the network or replace failing and unreliable nodes. DSNs may be deployed in hostile areas where communication is monitored and nodes are subject to capture and surreptitious use by an adversary. Hence DSNs require cryptographic protection of communications, sensorcapture detection, key revocation and sensor disabling. In this paper, we present a key-management scheme designed to satisfy both operational and security requirements of DSNs.
501|System architecture directions for networked sensors|Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world or spread throughout our environment. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances. 
502|Parallelizable Encryption Mode with Almost Free Message Integrity|this documentwe propose a new mode of operation for symmetric key block cipher algorithms. The main feature distinguishing the proposed mode from existing modes is that along with providing confidentiality of the message, it also provides message integrity. In other words, the new mode is not just a mode of operation for encryption, but a mode of operation for authenticated encryption. As the title of the document suggests, the new mode achieves the additional property with little extra overhead, as will be explained below. The new mode is also highly parallelizable. In fact, it has critical path of only two block cipher invocations. By one estimate, a hardware implementation of this mode on a single board (housing 1000 block cipher units) achieves terabits/sec (10  12  bits/sec) of authenticated encryption. Moreover, there is no penalty for doing a serial implementation of this mode. The new mode also comes with proofs of security, assuming that the underlying block ciphers are secure. For confidentiality,themode achieves the same provable security bound as CBC. For authentication, the mode achieves the same provable security bound as CBC-MAC. The new parallelizable mode removes chaining from the well known CBC mode, and instead does an input whitening (as well an output whitening) with a pairwise independent sequence. Thus, it becomes similar to the ECB mode. However, with the input whitening with the pairwise independent sequence the new mode has provable security similar to CBC (Note: ECB does not have security guarantees like CBC). Also, the output whitening with the pairwise independent sequence guarantees message integrity. The pairwise independent sequence can be generated with little overhead. In fact, the input and output whitening sequence need only be pairwi...
503|Trade-offs Between Communication and Storage in Unconditionally Secure Schemes for Broadcast Encryption and Interactive Key Distribution|. In 1993, Beimel and Chor presented an unconditionally secure interactive protocol which allows a subset of users in a network to establish a common key. This scheme made use of a key predistribution scheme due to Blom. In this paper, we describe some variations and generalizations of the Beimel-Chor scheme, including broadcast encryption schemes as well as interactive key distribution schemes. Our constructions use the key predistribution scheme of Blundo et al, which is a generalization of the Blom scheme. We obtain families of schemes in which the amount of secret information held by the network users can be traded off against the amount of information that needs to be broadcast. We also discuss lower bounds on the storage and communcation requirements of protocols of these types. Some of our schemes are optimal (or close to optimal) with respect to these bounds. 1 Introduction  When a subset of users in a network wishes to communicate privately in conference, encryption algorithms...
504|A calculus for cryptographic protocols: The spi calculus|We introduce the spi calculus, an extension of the pi calculus designed for the description and analysis of cryptographic protocols. We show how to use the spi calculus, particularly for studying authentication protocols. The pi calculus (without extension) suffices for some abstract protocols; the spi calculus enables us to consider cryptographic issues in more detail. We represent protocols as processes in the spi calculus and state their security properties in terms of coarsegrained notions of protocol equivalence.
507|Testing Equivalences for Processes|Abstract. Given a set of processes and a set of tests on these processes we show how to define in a natural way three different eyuitalences on processes. ThesP equivalences are applied to a particular language CCS. We give associated complete proof systems and fully abstract models. These models have a simple representation in terms of trees.
508|A Calculus for Access Control in Distributed Systems|We study some of the concepts, protocols, and algorithms for access control in distributed systems, from a logical perspective. We account for how a principal may come to believe that another principal is making a request, either on his own or on someone else&#039;s behalf. We also provide a logical language for access control lists, and theories for deciding whether requests should be granted.
509|Prudent Engineering Practice for Cryptographic Protocols|We present principles for the design of cryptographic protocols. The principles are neither necessary nor sufficient for correctness. They are however helpful, in that adherence to them would have avoided a considerable number of published errors. Our principles are informal guidelines. They complement formal methods, but do not assume them. In order to demonstrate the actual applicability of these guidelines, we discuss some instructive examples from the literature. 1
510|Typing and Subtyping for Mobile Processes|The pi-calculus is a process algebra that supports process mobility by focusing on the communication of channels. Milner&#039;s
512|Provably Secure Session Key Distribution --  The Three Party Case|We study session key distribution in the three-party setting of Needham and Schroeder. (This is the trust model assumed by the popular Kerberos authentication system.) Such protocols are basic building blocks for contemporary distributed systems -- yet the underlying problem has, up until now, lacked a definition or provably-good solution. One consequence is that incorrect protocols have proliferated. This paper provides the first treatment of this problem in the complexity-theoretic framework of modern cryptography. We present a definition, protocol, and a proof that the protocol satisfies the definition, assuming the (minimal) assumption of a pseudorandom function. When this assumption is appropriately instantiated, our protocols are simple and efficient. 
513|A Calculus of Mobile Processes, Part I|We present the ß-calculus, a calculus of communicating systems in which one can naturally express processes which have changing structure. Not only may the component agents of a system be arbitrarily linked, but a communication between neighbours may carry information which changes that linkage. The calculus is an extension of the process algebra CCS, following work by Engberg and Nielsen who added mobility to CCS while preserving its algebraic properties. The ß-calculus gains simplicity by removing all distinction between variables and constants; communication links are identified by names, and computation is represented purely as the communication of names across links. After an illustrated description of how the ß-calculus generalises conventional process algebras in treating mobility, several examples exploiting mobility are given in some detail. The important examples are the encoding into the ß-  calculus of higher-order functions (the -calculus and combinatory algebra), the tr...
514|On the Bisimulation Proof Method|The most popular method for establishing bisimilarities among processes is to exhibit bisimulation relations. By definition, R is a bisimulation relation if R progresses to R itself, i.e., pairs of processes in R can match each other&#039;s actions and their derivatives are again in R.  We study generalisations of the method aimed at reducing the size of the relations to exhibit and hence relieving the proof work needed to establish bisimilarity results. We allow a relation R  to progress to a different relation F(R), where F is a function on relations. Functions which can be safely used in this way (i.e., such that if R progresses to F(R), then R only includes pairs of bisimilar processes) are sound. We give a simple condition which ensures soundness. We show that the class of sound functions contains non-trivial functions and we study the closure properties of the class w.r.t. various important function constructors, like composition, union and iteration. These properties allow us to cons...
515|Applying Formal Methods to the Analysis of a Key Management Protocol|In this paper we develop methods for analyzing key management and authentication protocols using techniques developed for the solutions of equations in a term rewriting system. In particular, we describe a model of a class of protocols and possible attacks on those protocols as term rewriting systems, and we also describe a software tool based on a narrowing algorithm that can be used in the analysis of such protocols. We formally model a protocol and describe the results of using these techniques to analyze security properties. We show how a security flaw was found, and we also describe the verification of a corrected scheme using these techniques.  1 Introduction  It is difficult to be certain whether or not a cryptographic protocol satisfies its requirements. In a number of cases subtle security flaws have been found in protocols some time after they were published. These flaws were independent of the strengths or weakness of the cryptographic algorithms used. Examples include the N...
516|Testing equivalence for mobile processes|Abst rac t. The impact of applying the testing approach to a calculus of processes with a dynamically changing structure is investigated. A proof system for the finite fragment of the calculus is introduced which consists of two groups of laws: those for strong observational equivalence and those needed to deal with x actions. Soundness and completeness w.r.t, a testing preorder are shown. A fully abstract denotational model for the language is presented which relies on the existence of normal forms for processes. 1.
517|Using Temporal Logic to Specify and Verify Cryptographic Protocols (Progress Report)  (1995) |We use standard linear-time temporal logic to specify cryptographic protocols, model the system penetrator, and specify correctness requirements. The requirements are specified as standard safety properties, for which standard proof techniques apply. In particular, we are able to prove that the system penetrator cannot obtain a session key by any logical or algebraic techniques. We compare our work to Meadows&#039; method. We argue that using standard temporal logic provides greater flexibility and generality, firmer foundations, easier integration with other formal methods, and greater confidence in the verification results.
518|On two Proposals for On-line Bankcard Payments using Open Networks: Problems and Solutions|Recently, two major bankcard payment instrument operators VISA and MasterCard published specifications for securing bankcard payment transactions on open networks for open scrutiny. (VISA: Secure Transaction Technology, STT; MasterCard: Secure Electronic Payment Protocol, SEPP.) Based on their success in operating the existing on-line payment systems, both proposals use advanced cryptographic technologies to supply some security services that are wellunderstood to be inadequate in open networks, and otherwise specify systems similar to today&#039;s private-network versions. In this paper we reason that when an open network is used for underlying electronic commerce some subtle vulnerabilities will emerge and the two specifications are seen not in anticipation of them. A number of weaknesses are found as a result of missing and misuse of security services. Missing and misused services include: authentication, nonrepudiation, integrity, and timeliness. We identify problems and devise solution...
519|Crowds: Anonymity for Web Transactions|this paper we introduce a system called Crowds for protecting users&#039; anonymity on the worldwide -web. Crowds, named for the notion of &#034;blending into a crowd&#034;, operates by grouping users into a large and geographically diverse group (crowd) that collectively issues requests on behalf of its members. Web servers are unable to learn the true source of a request because it is equally likely to have originated from any member of the crowd, and even collaborating crowd members cannot distinguish the originator of a request from a member who is merely forwarding the request on behalf of another. We describe the design, implementation, security, performance, and scalability of our system. Our security analysis introduces degrees of anonymity as an important tool for describing and proving anonymity properties.
520|Randomized Algorithms|Randomized algorithms, once viewed as a tool in computational number theory, have by now found widespread application. Growth has been fueled by the two major benefits of randomization: simplicity and speed. For many applications a randomized algorithm is the fastest algorithm available, or the simplest, or both. A randomized algorithm is an algorithm that uses random numbers to influence the choices it makes in the course of its computation. Thus its behavior (typically quantified as running time or quality of output) varies from
521|Fail-Stop Processors: An Approach to Designing Fault-Tolerant Computing Systems|This paper was originally submitted to ACM Transactions on Programming Languages and Systems. The responsible editor was Susan L. Graham. The authors and editor kindly agreed to transfer the paper to the ACM Transactions on Computer Systems
522|Anonymous Connections and Onion Routing|Onion Routing provides anonymous connections that are strongly resistant to both eavesdropping and traffic analysis. Unmodified Internet applications can use these anonymous connections by means of proxies. The proxies may also make communication anonymous by removing identifying information from the data stream. Onion routing has been implemented on Sun Solaris 2.X with proxies for Web browsing, remote logins, and e-mail. This paper&#039;s contribution is a detailed specification of the implemented onion routing system, a vulnerability analysis based on this specification, and performance results. 1 Introduction  Private electronic communication is becoming an increasingly important public issue. Encryption can effectively hide the content of a conversation from eavesdroppers, and this protection is being integrated into many systems. But, hiding the identities of communicating parties from eavesdroppers, or from each other, is usually not considered. Who is communicating with whom, howeve...
523|Using Process Groups to Implement Failure Detection in Asynchronous Environments|Agreement on the membership of a group of processes in a distributed system is a basic problem that arises in a wide range of applications. Such groups occur when a set of processes co-operate to perform some task, share memory, monitor one another, subdivide a computation, and so forth. In this paper we discuss the Group Membership Problem as it relates to failure detection in asynchronous, distributed systems. We present a rigorous, formal specification for group membership under this interpretation. We then present a solution for this problem that improves upon previous work.
524|Reaching Agreement on Processor Group Membership in Synchronous Distributed Systems|Reaching agreement on the identity of correctly functioning processors of a distributed  system in the presence of random communication delays, failures and processor  joins is a fundamental problem in fault-tolerant distributed systems. Assuming a  synchronous communication network that is not subject to partition occurrences, we  specify the processor-group membership problem and we propose three simple protocols  for solving it. The protocols provide all correct processors with consistent views  of the processor-group membership and guarantee bounded processor failure detection  and join delays.  Key words: Communication network -- Distributed system -- Failure detection --  Fault tolerance -- Real time system -- Replicated data  1 Introduction  When designing a computing service that must remain available despite component failures, a key idea is to replicate service state information at several servers running on distinct processors. The service state typically consists of the ser...
525|ISDN-MIXes: Untraceable Communication with Very Small Bandwidth Overhead|Untraceable communication for services like telephony is often considered infeasible in the near future because of bandwidth limitations. We present a technique, called ISDN-MIXes, which shows that this is not the case. As little changes as possible are made to the narrowband-ISDN planned by the PTTs. In particular, we assume the same subscriber lines with the same bit rate, and the same long-distance network between local exchanges, and we offer the same services. ISDN-MIXes are a combination of a new variant of CHAUM&#039;s MIXes, dummy traffic on the subscriber lines (where this needs no additional bandwidth), and broadcast of incoming-call messages in the subscriber-area. 1 Introduction  The need to keep communication untraceable, i.e. to keep secret who communicates with whom, has been discussed, e.g., in [Chau_81, Cha8_85, PfWa_86, PfPW_88]. Untraceable communication for services like telephony is often considered infeasible in the near future because of bandwidth limitations. We pres...
526|Intrusion Tolerance in Distributed Computing Systems|An intrusion-tolerant distributed system is a system which is designed so that any intrusion into a part of the system will not endanger confidentiality, integrity and availability. This approach is suitable for distributed systems, because distribution enables isolation of elements so that an intrusion gives physical access to only a part of the system. By intrusion, we mean not only computer break-ins by non-registered people, but also attempts by registered users to exceed or to abuse their privileges. In particular, possible malice of security administrators is taken into account. This paper describes how some functions of distributed systems can be designed to tolerate intrusions, in particular security functions such as user authentication and authorization, and application functions such as file management. Introduction  Most of the currently developed secure systems are based on paradigms such as access control matrix, reference monitor, security kernel or trusted computing bas...
527|How to Make Personalized Web Browsing Simple, Secure, and Anonymous| An increasing number of web-sites require users to establish an account before they can access the information stored on that site (&#034;personalized web browsing&#034;). Typically, the user is required to provide at least a unique username, a secret password and an e-mail address. Establishing accounts at multiple web-sites is a tedious task. A securityand privacy-aware user may have toinvent a distinct username and a secure password, both unrelated to his/her identity, for each web-site. The user may also desire mechanisms for anonymous e-mail. Besides the information that the user supplies voluntarily to the web-site, additional information about the user may flow (involuntarily) from the user&#039;s site to the web-site, due to the nature of the HTTP protocol and the cookie mechanism. This paper describes the Janus Personalized Web Anonymizer, which makes personalized web browsing simple, secure and anonymous by providing convenient solutions to each of the above problems. Janus serves as an intermediary entity between a user and a web-site. Given a user and a web-site, Janus automatically generates an alias -- typically a username, a password and an e-mail address -- that can be used to establish an anonymous account at the web-site. Different aliases are generated for each user, web-site pair ? however the same alias is presented whenever a particular user visits a particular web-site. Janus frees the user from the burden of inventing and memorizing distinct usernames and secure passwords for each web-site, and guarantees that an alias (including an e-mail address) does not reveal the true identity of the user. Janus also provides mechanisms to complete an anonymous e-mail exchange from a web-site to a user, and filters the information-flow of the HTTP protocol to preserve user privacy. Thus Janus provides simultaneous user identification and user privacy, as required for anonymous personalized web browsing. 
528|Increasing Availability and Security of an Authentication Service|. Authentication is a process by which one satisfies another about one&#039;s claim of identity. Typically an authentication server provides the authentication service via an authentication protocol. The authentication service is a security bottleneck in that its compromise can lead to the compromise of the whole system. The service is also a performance bottleneck because many activities cannot proceed unless the identities of concerned parties can be satisfactorily established. Therefore, a desirable authentication service should be both highly secure and highly available. We propose a general solution by replicating the authentication server such that a minority of malicious and colluding servers cannot compromise security or disrupt service. We discuss some unusual features of such a distributed authentication service, including the trade-off between availability and security. A distributed service is also useful when clients cannot identify or agree upon trusted servers prior to authen...
529|How To Break The Direct RSA-Implementation Of Mixes|MIXes are a means of untraceable communication based on a public key cryptosystem, as published by David Chaum in 1981 (CACM 24/2, 84-88) (=[6]). In the case where RSA is used as this cryptosystem directly, i.e. without composition with other functions (e.g. destroying the multiplicative structure), we show how the resulting MIXes can be broken by an active attack which is perfectly feasible in a typical MIX-environment. The attack does not affect the idea of MIXes as a whole: if the security requirements of [6] are concretized suitably and if a cryptosystem fulfils them, one can implement secure MIXes directly. However, it shows that present security notions for public key cryptosystems, which do not allow active attacks, do not suffice for a cryptosystem which is used to implement MIXes directly. We also warn of the same attack and others on further possible implementations of MIXes, and we mention several implementations which are not broken by any attack we know. I. INTRODUCTION: M...
530|A Security Architecture for Fault-Tolerant Systems|Process groups are a common abstraction for fault-tolerant computing in distributed systems. We present a security architecture that extends the process group into a security abstraction. Integral parts of this architecture are services that securely and fault tolerantly support cryptographic key distribution. Using replication only when necessary, and introducing novel replication techniques when it was necessary, we have constructed these services both to be easily defensible against attack and to permit key distribution despite the transient unavailabil-ity ofa substantial number of servers. We detail the design andimplementation of these services and the secure process group abstraction they support. We also give preliminary performance figures for some common group operations.
531|Mixing Email with BABEL|Increasingly large numbers of people communicate today via electronic means such as email or news forums. One of the basic properties of the current electronic communication means is the identification of the end-points. However, at times it is desirable or even critical to hide the identity and/or whereabouts of the end-points (e.g., human users) involved. This paper
532|Telos: enabling ultra-low power wireless research|Abstract — We present Telos, an ultra low power wireless sensor module (“mote”) for research and experimentation. Telos is the latest in a line of motes developed by UC Berkeley to enable wireless sensor network (WSN) research. It is a new mote design built from scratch based on expe-riences with previous mote generations. Telos ’ new design consists of three major goals to enable experimentation: minimal power consumption, easy to use, and increased software and hardware robustness. We discuss how hardware components are selected and integrated in order to achieve these goals. Using a Texas Instruments MSP430 microcontroller, Chipcon IEEE 802.15.4-compliant radio, and USB, Telos ’ power profile is almost one-tenth the consumption of previous mote platforms while providing greater performance and throughput. It eliminates programming and support boards, while enabling experimentation with WSNs in both lab, testbed, and deployment settings. I.
533|The design of an acquisitional query processor for sensor networks|We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. 1.
534|Maté: A Tiny Virtual Machine for Sensor Networks|Composed of tens of thousands of tiny devices with very limited resources (&#034;motes&#034;), sensor networks are subject to novel systems problems and constraints. The large number of motes in a sensor network means that there will often be some failing nodes; networks must be easy to repopu-late. Often there is no feasible method to recharge motes, so energy is a precious resource. Once deployed, a network must be reprogrammable although physically unreachable, and this reprogramming can be a significant energy cost. We present Maté, a tiny communication-centric virtual machine designed for sensor networks. Mat~&#039;s high-level in-terface allows complex programs to be very short (under 100 bytes), reducing the energy cost of transmitting new programs. Code is broken up into small capsules of 24 instructions, which can self-replicate through the network. Packet sending and reception capsules enable the deploy-ment of ad-hoc routing and data aggregation algorithms. Maté&#039;s concise, high-level program representation simplifies programming and allows large networks to be frequently re-programmed in an energy-efficient manner; in addition, its safe execution environment suggests a use of virtual ma-chines to provide the user/kernel boundary on motes that have no hardware protection mechanisms. 
535|The dynamic behavior of a data dissemination protocol for network programming at scale |To support network programming, we present Deluge, a reliable data dissemination protocol for propagating large data objects from one or more source nodes to many other nodes over a multihop, wireless sensor network. Deluge builds from prior work in density-aware, epidemic maintenance protocols. Using both a real-world deployment and simulation, we show that Deluge can reliably disseminate data to all nodes and characterize its overall performance. On Mica2dot nodes, Deluge can push nearly 90 bytes/second, oneninth the maximum transmission rate of the radio supported under TinyOS. Control messages are limited to 18 % of all transmissions. At scale, the protocol exposes interesting propagation dynamics only hinted at by previous dissemination work. A simple model is also derived which describes the limits of data propagation in wireless networks. Finally, we argue that the rates obtained for dissemination are inherently lower than that for single path propagation. It appears very hard to significantly improve upon the rate obtained by Deluge and we identify establishing a tight lower bound as an open problem.
536|An analysis of a large scale habitat monitoring application|Habitat and environmental monitoring is a driving application for wireless sensor networks. We present an analysis of data from a second generation sensor networks deployed during the summer and autumn of 2003. During a 4 month deployment, these networks, consisting of 150 devices, produced unique datasets for both systems and biological analysis. This paper focuses on nodal and network performance, with an emphasis on lifetime, reliability, and the the static and dynamic aspects of single and multi-hop networks. We compare the results collected to expectations set during the design phase: we were able to accurately predict lifetime of the single-hop network, but we underestimated the impact of multihop traffic overhearing and the nuances of power source selection. While initial packet loss data was commensurate with lab experiments, over the duration of the deployment, reliability of the backend infrastructure and the transit network had a dominant impact on overall network performance. Finally, we evaluate the physical design of the sensor node based on deployment experience and a post mortem analysis. The results shed light on a number of design issues from network deployment, through selection of power sources to optimizations of routing decisions.  
537|Simulating the power consumption of large-scale sensor network applications|Developing sensor network applications demands a new set of tools to aid programmers. A number of simulation environments have been developed that provide varying degrees of scalability, realism, and detail for understanding the behavior of sensor networks. To date, however, none of these tools have addressed one of the most important aspects of sensor application design: that of power consumption. While simple approximations of overall power usage can be derived from estimates of node duty cycle and communication rates, these techniques often fail to capture the detailed, low-level energy requirements of the CPU, radio, sensors, and other peripherals. In this paper, we present PowerTOSSIM, a scalable simulation environment for wireless sensor networks that provides an accurate, per-node estimate of power consumption. PowerTOSSIM is an extension to TOSSIM, an event-driven simulation environment for TinyOS applications. In PowerTOSSIM, TinyOS components corresponding to specific hardware peripherals (such as the radio, EEPROM, LEDs, and so forth) are instrumented to obtain a trace of each device’s activity during the simulation run. PowerTOSSIM employs a novel code-transformation technique to estimate the number of CPU cycles executed by each node, eliminating the need for expensive instruction-level simulation of sensor nodes. PowerTOSSIM includes a detailed model of hardware energy consumption based on the Mica2 sensor node platform. Through instrumentation of actual sensor nodes, we demonstrate that PowerTOSSIM provides accurate estimation of power consumption for a range of applications and scales to support very large simulations.
538|Lessons From A Sensor Network Expedition|Habitat monitoring is an important driving application for wireless sensor networks (WSNs). Although researchers anticipate some challenges arising in the real-world deployments of sensor networks, a number of problems can be discovered only through experience. This paper evaluates a sensor network system described in an earlier work and presents a set of experiences from a four month long deployment on a remote island o# the coast of Maine. We present an in-depth analysis of the environmental and node health data. The close integration of WSNs with their environment provides biological data at densities previous impossible; however, we show that the sensor data is also useful for predicting system operation and network failures. Based on over one million data and health readings, we analyze the node and network design and develop network reliability profiles and failure models.
540|COTS Dust|Contents  Preface iv 1.0 Introduction 1  1.1 Smart Dust Scenarios 2  1.1.1 Forest Fire Warning 2 1.1.2 Enemy Troop Monitoring 3  1.2 Smart Dust Capabilities 3  1.2.1 Distributed Sensor Networks and Ad-hoc Networking 4 1.2.2 High Level Interpretation of Spatial Sensor Data 4 1.2.3 Distributed Processing 5 1.2.4 COTS Dust 6  2.0 COTS Dust Architecture 7  2.1 Power 8 2.2 Computation 9  2.2.1 Static vs. Dynamic Current 11 2.2.2 Strong Thumb 11  2.3 Sensors 12  2.3.1 Magnetometer (2/3 Axis) 13 2.3.2 Accelerometers (2/3 Axis) 14 2.3.3 Light Sensor 16 2.3.4 Temperature Sensor 17 2.3.5 Pressure Sensor 17 2.3.6 Humidity Sensor 19  2.4 Communication 19  2.4.1 Acoustic Communication 20 2.4.2 RF Communication 23 2.4.3 Optical Communication 27 2.4.4 Optical Communication vs. RF Communication 32  3.0 COTS Dust Systems 35  3.1 Mouse Collars 35 3.2 Radio Frequency Mote (RF Mote) 39  3.2.1 RF Communica
541|Wireless sensor networks: a survey|This paper describes the concept of sensor networks which has been made viable by the convergence of microelectro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the potential sensor networks applications are explored, and a review of factors influencing the design of sensor networks is provided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are
542|Energy-efficient communication protocol for wireless microsensor networks|Wireless distributed microsensor systems will enable the reliable monitoring of a variety of environments for both civil and military applications. In this paper, we look at communication protocols, which can have significant impact on the overall energy dissipation of these networks. Based on our findings that the conventional protocols of direct transmission, minimum-transmission-energy, multihop routing, and static clustering may not be optimal for sensor networks, we propose LEACH (Low-Energy Adaptive Clustering Hierarchy), a clustering-based protocol that utilizes randomized rotation of local cluster base stations (cluster-heads) to evenly distribute the energy load among the sensors in the network. LEACH uses localized coordination to enable scalability and robustness for dynamic networks, and incorporates data fusion into the routing protocol to reduce the amount of information that must be transmitted to the base station. Simulations show that LEACH can achieve as much as a factor of 8 reduction in energy dissipation compared with conventional routing protocols. In addition, LEACH is able to distribute energy dissipation evenly throughout the sensors, doubling the useful system lifetime for the networks we simulated. 
543|Next century challenges: Scalable coordination in sensor networks|Networked sensors-those that coordinate amongst them-selves to achieve a larger sensing task-will revolutionize information gathering and processing both in urban envi-ronments and in inhospitable terrain. The sheer numbers of these sensors and the expected dynamics in these environ-ments present unique challenges in the design of unattended autonomous sensor networks. These challenges lead us to hypothesize that sensor network coordination applications may need to be structured differently from traditional net-work applications. In particular, we believe that localized algorithms (in which simple local node behavior achieves a desired global objective) may be necessary for sensor net-work coordination. In this paper, we describe localized al-gorithms, and then discuss directed diffusion, a simple com-munication model for describing localized algorithms. 1
544|SPINS: Security Protocols for Sensor Networks|As sensor networks edge closer towards wide-spread deployment, security issues become a central concern. So far, the main research focus has been on making sensor networks feasible and useful, and less emphasis was placed on security. We design a suite of security building blocks that are optimized for resource-constrained environments and wireless communication. SPINS has two secure building blocks: SNEP and TESLA. SNEP provides the following important baseline security primitives: Data con£dentiality, two-party data authentication, and data freshness. A particularly hard problem is to provide efficient broad-cast authentication, which is an important mechanism for sensor networks. TESLA is a new protocol which provides authenticated broadcast for severely resource-constrained environments. We implemented the above protocols, and show that they are practical even on minimalistic hardware: The performance of the protocol suite easily matches the data rate of our network. Additionally, we demonstrate that the suite can be used for building higher level protocols. 
545|Geography-informed Energy Conservation for Ad Hoc Routing|We introduce a geographical adaptive fidelity (GAF) algorithm that reduces energy consumption in ad hoc wireless networks. GAF conserves energy by identifying nodes that are equivalent from a routing perspective and then turning off unnecessary nodes, keeping a constant level of routing fidelity. GAF moderates this policy using application- and system-level information; nodes that source or sink data remain on and intermediate nodes monitor and balance energy use. GAF is independent of the underlying ad hoc routing protocol; we simulate GAF over unmodified AODV and DSR. Analysis and simulation studies of GAF show that it can consume 40% to 60% less energy than an unmodified ad hoc routing protocol. Moreover, simulations of GAF suggest that network lifetime increases proportionally to node density; in one example, a four-fold increase in node density leads to network lifetime increase for 3 to 6 times (depending on the mobility pattern). More generally, GAF is an example of adaptive fidelity, a technique proposed for extending the lifetime of self-configuring systems by exploiting redundancy to conserve energy while maintaining application fidelity.   
546|The Cricket Location-Support System|This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, locationdependent applications. It allows applications running on mobile and static nodes to learn their physical location by using listeners that hear and analyze information from beacons spread throughout the building. Cricket is the result of several design goals, including user privacy, decentralized administration, network heterogeneity, and low cost. Rather than explicitly tracking user location, Cricket helps devices learn where they are and lets them decide whom to advertise this information to; it does not rely on any centralized management or control and there is no explicit coordination between beacons; it provides information to devices regardless of their type of network connectivity; and each Cricket device is made from off-the-shelf components and costs less than U.S. $10. We describe the randomized algorithm used by beacons to transmit information, the use of concurrent radio and ultrasonic signals to infer distance, the listener inference algorithms to overcome multipath and interference, and practical beacon configuration and positioning techniques that improve accuracy. Our experience with Cricket shows that several location-dependent applications such as in-building active maps and device control can be developed with little effort or manual configuration.  1 
547|Power-Aware Routing in Mobile Ad Hoc Networks|In this paper we present a case for using new power-aware metrics for determining routes in wireless  ad hoc networks. We present five different metrics based on battery power consumption at nodes. We  show that using these metrics in a shortest-cost routing algorithm reduces the cost/packet of routing  packets by 5-30% over shortest-hop routing (this cost reduction is on top of a 40-70% reduction in energy  consumption obtained by using PAMAS, our MAC layer protocol). Furthermore, using these new metrics  ensures that the mean time to node failure is increased significantly. An interesting property of using  shortest-cost routing is that packet delays do not increase. Finally, we note that our new metrics can  be used in most traditional routing protocols for ad hoc networks.  1 
548|I-TCP: Indirect TCP for mobile hosts|Abstract — IP-based solutions to accommodate mobile hosts within existing internetworks do not address the distinctive features of wireless mobile computing. IP-based transport protocols thus suffer from poor performance when a mobile host communicates with a host on the fixed network. This is caused by frequent disruptions in network layer connectivity due to — i) mobility and ii) unreliable nature of the wireless link. We describe the design and implementation of I-TCP, which is an indirect transport layer protocol for mobile hosts. I-TCP utilizes the resources of Mobility Support Routers (MSRs) to provide transport layer communication between mobile hosts and hosts on the fixed network. With I-TCP, the problems related to mobility and the unreliability of wireless link are handled entirely within the wireless link; the TCP/IP software on the fixed hosts is not modified. Using I-TCP on our testbed, the throughput between a fixed host and a mobile host improved substantially in comparison to regular TCP. 1
549|Protocols for self-organization of a wireless sensor network|We present a suite of algorithms for self-organization of wireless sensor networks, in which there is a scalably large number of mainly static nodes with highly constrained energy resources. The protocols further support slow mobility by a subset of the nodes, energy-efficient routing, and formation of ad hoc subnetworks for carrying out cooperative signal processing functions among a set of the nodes.
550|ASCENT: Adaptive self-configuring sensor networks topologies| Advances in microsensor and radio technology will enable small but smart sensors to be deployed for a wide range of environmental monitoring applications. The low per-node cost will allow these wireless networks of sensors and actuators to be densely distributed. The nodes in these dense networks will coordinate to perform the distributed sensing and actuation tasks. Moreover, as described in this paper, the nodes can also coordinate to exploit the redundancy provided by high density so as to extend overall system lifetime. The large number of nodes deployed in these systems will preclude manual configuration, and the environmental dynamics will preclude design-time preconfiguration. Therefore, nodes will have to self-configure to establish a topology that provides communication under stringent energy constraints. ASCENT builds on the notion that, as density increases, only a subset of the nodes are necessary to establish a routing forwarding backbone. In ASCENT, each node assesses its connectivity and adapts its participation in the multihop network topology based on the measured operating region. This paper motivates and describes the ASCENT algorithm and presents analysis, simulation, and experimental measurements. We show that the system achieves linear increase in energy savings as a function of the density and the convergence time required in case of node failures while still providing adequate connectivity. 
551|Instrumenting the world with wireless sensor networks|Pervasive micro-sensing and actuation may revolutionize the way in which we understand and manage complex physical systems: from airplane wings to complex ecosystems. The capabilities for detailed physical monitoring and manipulation offer enormous opportunities for almost every scientific discipline, and it will alter the feasible granularity of engineering. We identify opportunities and challenges for distributed signal processing in networks of these sensing elements and investigate some of the architectural challenges posed by systems that are massively distributed, physically-coupled, wirelessly networked, and energy limited. 
552|Smart Dust: Communicating with a Cubic-Millimeter Computer|building virtual keyboards;  . managing inventory control;  . monitoring product quality;  . constructing smart office spaces; and  . providing interfaces for the disabled.  SMART DUST REQUIREMENTS  Smart Dust requires both evolutionary and revolutionary  advances in miniaturization, integration, and  energy management. Designers can use microelectromechanical  systems (MEMS) to build small sensors,  optical communication components, and power supplies,  whereas microelectronics provides increasing  functionality in smaller areas, with lower energy consumption.  Figure 1 shows the conceptual diagram of  a Smart Dust mote. The power system consists of a  thick-film battery, a solar cell with a charge-integrating  capacitor for periods of darkness, or both.  Depending on its objective, the design integrates various  sensors, including light, temperature, vibration,  magnetic field, acoustic, and wind shear, onto the  mote. An integrated circuit provides sensor-signal pr
553|The Simulation and Evaluation of Dynamic Voltage Scaling Algorithms|The reduction of energy consumption in microprocessors can be accomplished without impacting the peak performance through the use of dynamic voltage scaling (DVS). This approach varies the processor voltage under software control to meet dynamically varying performance requirements. This paper presents a foundation for the simulation and analysis of DVS algorithms. These algorithms are applied to a benchmark suite specifically targeted for PDA devices. 2.
554|Comparing Algorithms for Dynamic Speed-Setting of a Low-Power CPU|To take advantage of the full potential of ubiquitous computing, we will need systems which minimize powerconsumption. Weiser et al. and others have suggested that this may be accomplished by a CPU which dynamically changes speed and voltage, thereby saving energy by spreading run cycles into idle time. Here we continue this research, using a simulation to compare a number of policies for dynamic speed-setting. Our work clarifies a fundamental power vs. delay tradeoff, as well as the role of prediction and of smoothing in dynamic speed-setting policies. We conclude that success seemingly depends more on simple smoothing algorithms than on sophisticated prediction techniques, but defer to the replication of these results on future variable-speed systems. 1 Introduction  Recent developments in ubiquitous computing make it likely that the future will see a proliferation of cordless computing devices. Clearly it will be advantageous for such devices to minimize power-consumption. The top p...
555|Building Efficient Wireless Sensor Networks with Low-Level Naming|In most distributed systems, naming of nodes for low-level communication leverages topological location (such as node addresses)  and is independent of any application. In this paper, we investigate an emerging class of distributed systems where low-level communication does not rely on network topological location. Rather, low-level communication is based on attributes that are external to the network topology and relevant to the application. When combined with dense deployment of nodes, this kind of named data enables in-network processing for data aggregation, collaborative signal processing, and similar problems. These approaches are essential for emerging applications such as sensor networks where resources such as bandwidth and energy are limited. This paper is the first description of the software architecture that supports  named data and in-network processing in an operational, multi-application sensor-network. We show that approaches such as in-network aggregation and nested queries can significantly affect network traffic. In one experiment aggregation reduces traffic by up to 42% and nested queries reduce loss rates by 30%. Although aggregation has been previously studied in simulation, this paper demonstrates nested queries as another form of in-network processing, and it presents the first evaluation of these approaches over an operational testbed.  
556|Power Efficient Organization of Wireless Sensor Networks|Abstract-- Wireless sensor networks have emerged recently as an effective way of monitoring remote or inhospitable physical environments. One of the major challenges in devising such networks lies in the constrained energy and computational resources available to sensor nodes. These constraints must be taken into account at all levels of system hierarchy. The deployment of sensor nodes is the first step in establishing a sensor network. Since sensor networks contain a large number of sensor nodes, the nodes must be deployed in clusters, where the location of each particular node cannot be fully guaranteed a priori. Therefore, the number of nodes that must be deployed in order to completely cover the whole monitored area is often higher than if a deterministic procedure were used. In networks with stochastically placed nodes, activating only the necessary number of sensor nodes at any particular moment can save energy. We introduce a heuristic that selects mutually exclusive sets of sensor nodes, where the members of each of those sets together completely cover the monitored area. The intervals of activity are the same for all sets, and only one of the sets is active at any time. The experimental results demonstrate that by using only a subset of sensor nodes at each moment, we achieve a significant energy savings while fully preserving coverage. I.
557|Achieving MAC Layer Fairness in Wireless Packet Networks|Link-layer fairness models that have been proposed for wireline and packet cellular networks cannot be generalized for shared channel wireless networks because of the unique characteristics of the wireless channel, such as location-dependent contention, inherent conflict between optimizing channel utilization and achieving fairness, and the absence of any centralized control. In this paper, we propose a general analytical framework that captures the unique characteristics of shared wireless channels and allows the modeling of a large class of systemwide fairness models via the specification of per-flow utility functions. We show that system-wide fairness can be achieved without explicit global coordination so long as each node executes a contention resolution algorithm that is designed to optimize its local utility function. We present a general mechanism for translating a given fairness model in our framework into a corresponding contention resolution algorithm. Using this translation...
558|Physical Layer Driven Protocol and Algorithm Design for Energy-Efficient Wireless Sensor Networks|The potential for collaborative, robust networks of microsensors has attracted a great deal of research attention. For the most part, this is due to the compelling applications that will be enabled once wireless microsensor networks are in place
560|Upper Bounds on the Lifetime of Sensor Networks|In this paper, we ask a fundamental question concerning the limits of energy e#ciency of sensor networks - What is the upper bound on the lifetime of a sensor network that collects data from a specified region using a certain number of energy-constrained nodes? The answer to this question is valuable for two main reasons. First, it allows calibration of real world data-gathering protocols and an understanding of factors that prevent these protocols from approaching fundamental limits. Secondly, the dependence of lifetime on factors like the region of observation, the source behavior within that region, basestation location, number of nodes, radio path loss characteristics, e#ciency of node electronics and the energy available on a node, is exposed. This allows architects of sensor networks to focus on factors that have the greatest potential impact on network lifetime. By employing a combination of theory and extensive simulations of constructed networks, we show that in all data gathe...
561|Robust Range Estimation Using Acoustic and Multimodal Sensing|Many applications of robotics and embedded sensor technology can benet from ne-grained localization. Fine-grained localization can simplify multi-robot collaboration, enable energy ecient multi-hop routing for low-power radio networks, and enable automatic calibration of distributed sensing systems. In this work we focus on range estimation, a critical prerequisite for ne-grained localization. While many mechanisms for range estimation exist, any individual mode of sensing can be blocked or confused by the environment. We present and analyze an acoustic ranging system that performs well in the presence of many types of interference, but can return incorrect measurements in non-line-of-sight conditions. We then suggest how evidence from an orthogonal sensory channel might be used to detect and eliminate these measurements. This work illustrates the more general research theme of combining multiple modalities to obtain robust results. 1 
562|Exposure In Wireless Ad-Hoc Sensor Networks|Wireless ad-hoc sensor networks will provide one of the missing connections between the Internet and the physical world. One of the fundamental problems in sensor networks is the calculation of coverage. Exposure is directly related to coverage in that it is a measure of how well an object, moving on an arbitrary path, can be observed by the sensor network over a period of time.  In addition to the informal definition, we formally define exposure and study its properties. We have developed an efficient and effective algorithm for exposure calculation in sensor networks, specifically for finding minimal exposure paths. The minimal exposure path provides valuable information about the worst case exposure-based coverage in sensor networks. The algorithm works for any given distribution of sensors, sensor and intensity models, and characteristics of the network. It provides an unbounded level of accuracy as a function of run time and storage. We provide an extensive collection of experimental results and study the scaling behavior of exposure and the proposed algorithm for its calculation.  I. 
563|Sensor Information Networking Architecture and Applications|This article introduces a sensor information networking architecture, called SINA, that facilitates querying, monitoring, and tasking of sensor networks. SINA plays the role of a middleware that abstracts a network of sensor nodes as a collection of massively distributed objects. The SINA&#039;s execution environment provides a set of configuration and communication primitives that enable scalable and energy-efficient organization of and interactions among sensor objects. On top the execution environment is a programmable substrate that provides mechanisms to create associations and coordinate activities among sensor nodes. Users then access information within a sensor network using declarative queries, or perform tasks using programming scripts.
564|Adaptive Frame Length Control for Improving Wireless Link Throughput, Range, and Energy Efficiency|Wireless network links are characterized by rapidly  time varying channel conditions and battery energy limitations at  the wireless mobile user nodes. Therefore static link control techniques  that make sense in comparatively well behaved wired links  do not necessarily apply to wireless links. New adaptive link layer  control techniques are needed to provide robust and energy efficient  operation even in the presence of orders of magnitude variations  in bit error rates and other radio channel conditions. For  example, recent research has advocated adaptive link layer techniques  such as adaptive error control [Lettieri97], channel state  dependent protocols [Bhagwat96, Fragouli97], and variable  spreading gain [Chien97]. In this paper we explore one such  adaptive technique: dynamic sizing of the MAC layer frame, the  atomic unit that is sent through the radio channel. A trade-off  exists between the desire to reduce header and physical layer  overhead by making frames large, and th...
565|Scalable coordination for wireless sensor networks: self-configuring localization systems|Pervasive networks of micro-sensors and actuators offer to revolutionize the ways in which we understand and construct complex physical systems. Sensor networks must be scalable, long-lived and robust systems, overcoming energy limitations and a lack of pre-installed infrastructure. We explore three themes in the design of self-configuring sensor networks: tuning density to trade operational quality against lifetime; using multiple sensor modalities to obtain robust measurements; and exploiting fixed environmental characteristics. We illustrate these themes through the problem of localization, which is a key building block for sensor systems that itself requires coordination.
566|Error Control and Energy Consumption in Communications for Nomadic Computing|We consider the problem of communications over a wireless channel in support of data transmissions from the  perspective of small portable devices that must rely on limited battery energy. We model the channel outages as statistically  correlated errors. Classic ARQ strategies are found to lead to a considerable waste of energy, due to the large number of  transmissions. The use of finite energy sources in the face of dependent channel errors leads to new protocol design criteria. As an  example, a simple probing scheme, which slows down the transmission rate when the channel is impaired, is shown to be more  energy efficient, with a slight loss in throughput. A modified scheme that yields slightly better performance but requires some  additional complexity is also studied. Some references on the modeling of battery cells are discussed to highlight the fact that  battery charge capacity is strongly influenced by the available &#034;relaxation time&#034; between current pulses. A formal approach ...
567|Intelligent Medium Access for Mobile Ad Hoc Networks with Busy Tones and Power Control|In a mobile ad-hoc networks (MANET), one essential issue is how to increase channel utilization while avoiding the hidden-terminal and the exposed terminal problems. Several MAC protocols, such as RTS/CTS-based and busytone-based schemes, have been proposed to alleviate these problems. In this paper, we explore the possibility of combining the concept of power control with the RTS/CTS-based and busy-tone-based protocols to further increase channel utilization. A sender will use an appropriate power level to transmit its packets so as to increase the possibility of channel reuse. The possibility of using discrete, instead of continuous, power levels is also discussed. Through analyses and simulations, we demonstrate the advantage of our new MAC protocol. This, together with the extra bene ts such as saving battery energy and reducing cochannel interference, does show a promising direction to enhance the performance of MANETs.
568|What is complexity|SFI Working Papers contain accounts of scientific work of the author(s) and do not necessarily represent the views of the Santa Fe Institute. We accept papers intended for publication in peer-reviewed journals or proceedings volumes, but not papers that have already appeared in print. Except for papers by our external faculty, papers must be based on work done at SFI, inspired by an invited visit to or collaboration at SFI, or funded by an SFI grant. ©NOTICE: This working paper is included by permission of the contributing author(s) as a means to ensure timely distribution of the scholarly and technical work on a non-commercial basis. Copyright and all rights therein are maintained by the author(s). It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author&#039;s copyright. These works may be reposted only with the explicit permission of the copyright holder. www.santafe.edu
569|Design Considerations for Distributed Microsensor Systems|Wireless distributed microsensor systems will enable the reliable monitoring and control of a variety of applications that range from medical and home security to machine diagnosis, chemical/biological detection and other military applications. The sensors have to be designed in a highly integrated fashion, optimizing across all levels of system abstraction, with the goal of minimizing energy dissipation. This paper addresses some of the key design considerations for future microsensor systems including the network protocols required for collaborative sensing and information distribution, system partitioning considering computation and communication costs, low energy electronics, power system design and energy harvesting techniques.  1. Introduction  Over the last few years, the design of micropower wireless sensor systems has gained increasing importance for a variety of civil and military applications. The Low Power Wireless Integrated Microsensors (LWIM) project has made major advan...
570|DataSpace: Querying and Monitoring Deeply Networked Collections in Physical Space|In this article we introduce a new conception of three-dimensional DataSpace, which is physical space enhanced by connectivity to the network.
571|Dynamic Voltage Scaling Techniques for Distributed Microsensor Networks|Distributed microsensor networks promise a versatile and robust platform for remote environment monitoring. Crucial to long system lifetimes for these microsensors are algorithms and protocols that provide the option of trading quality for energy savings. Dynamic voltage scaling on the sensor node&#039;s processor enables energy savings from these scalable algorithms. We demonstrate dynamic voltage scaling on the beginnings of a sensor node prototype, which currently consists of a commercial processor, a digitally adjustable DC-DC regulator, and a power-aware operating system.  1. Introduction  Distributed microsensor networks are emerging as a compelling new hardware platform for remote environment monitoring [1]. Researchers are considering a range of applications including remote climate monitoring, battlefield surveillance, and intra-machine monitoring [2]. A distributed microsensor network consists of many small, expendable, battery-powered wireless nodes. Once the nodes are deployed t...
572|Power-Aware Communication for Mobile Computers|Recently, the mobile community has focused on techniques for reducing energy consumption  for mobile hosts. These power management techniques typically target communication  devices such as wireless network interfaces, aiming to reduce usage, and thus energy consumption,  of the particular device itself. We observe that optimization of a single device&#039;s energy  consumption, without considering the effect of the strategy on the rest of the machine, can  have negative consequences. We propose power management techniques addressing mobile host  communications that encompass all components of a mobile host in an effort to optimize total  energy consumption. Specifically, we propose runtime adaptation of communication parameters  in order to minimize the energy consumed during active data transfer. Information about the  network environment is used to drive such adaptations in an effort to compensate for the effect  of dynamic service from wireless communication device on the energy consume...
573|A versatile architecture for the distributed sensor integration problem|Abstract-The computational issues related to information in-tegration in multisensor systems and distributed sensor networks has become an active area of research. From a computational viewpoint, the efficient extraction of information from noisy and faulty signals emanating from many sensors requires the solution of problems related a) to the architecture and fault tolerance of the distributed sensor network, b) to the proper synchronization of sensor signals, and c) to the integration of information to keep the communication and the centralized processing require-ments small. In this paper, we propose a versatile architecture for a distributed sensor network which consists of a multilevel network with the nodes (processing elementlsensor pairs) at each level interconnected as a deBruijn network. We show that this multilevel network has reasonable fault tolerance, admits simple and decentralized routing, and offers easy extensibility. We model information from sensors as real valued intervals and derive an interesting property related to information integration in the presence of faults. Using this property, the search for a fault is narrowed down to two potentially faulty sensors or communication links. In a distributed environment, information has to be integrated from “temporally close ” signals in the presence of imperfect clocks in a distributed environment. We apply the results of past research in this area to state various relationships between the clocks of the processing elements in the network for proper information integration. Index Terms- Abstract estimate, clock synchronization, dis-tributed sensor networks, deBruijn networks, fault tolerance, information integration. I.
574|The Mobile Patient: Wireless Distributed Sensor Networks for Patient Monitoring and Care|In this paper, the concept of a 3 layer distributed sensor network for patient monitoring and care is introduced. The envisioned network has a leaf node layer (consisting of patient sensors), a intermediate node layer (consisting of the supervisory processor residing with each patient) and the root node processor (residing at a central monitoring facility). The introduced paradigm has the capability of dealing with the bandwidth bottleneck at the wireless patient - root node link and the processing bottleneck at the central processor or root node of the network.
575|Energy-efficient link layer for wireless microsensor network |Wireless microsensors are being used to form large, dense networks for the purposes of long-term environmental sensing and data collection. Unfortunately, these networks are typically deployed in remote environments where energy sources are limited. Thus, designing fault-tolerant wire-less microsensor networks with long system lifetimes can be challenging. By applying energy-efficient techniques at all levels of the system hierarchy, system lifetime can be ex-tended. In this paper, energy-efficient techniques that adapt underlying communication parameters will be presented in the context of wireless microsensor networks. In particular, the effect of adapting link and physical layer parameters, such as output transmit power and error control coding, on system energy consumption will be examined. 1.
576|Diagnosis of Sensor Networks|As sensor nodes are embedded into physical environments and becoming integral parts of our daily lives, sensor networks will become the important nerve systems that monitor and actuate our physical environments. We define the process of monitoring the status of a sensor network and figuring out the problematic sensor nodes sensor network diagnosis. However, the high sensor node-to-manager ratio makes it extremely difficult to pay special attention to any individual node. In addition, the response implosion problem, which occurs when a high volume of incoming replies triggered by diagnosis queries cause the central diagnosing node to become a bottleneck, is one major obstacle to be overcome. In this paper, we describe approaches to addressing the response implosion problem in sensor network diagnosis. We will also present simulation experiments on the performance of these approaches, and discuss presentation schemes for diagnostic results.
