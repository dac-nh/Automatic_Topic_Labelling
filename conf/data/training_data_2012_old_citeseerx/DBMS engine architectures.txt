ID|Title|Summary
1|The Architecture of Cognition|Spanning seven orders of magnitude: a challenge for
2|Toward an instance theory of automatization|This article presents a theory in which automatization is construed as the acquisition of a domain-specific knowledge base, formed of separate representations, instances, of each exposure to the task. Processing is considered automatic if it relies on retrieval of stored instances, which will occur only after practice in a consistent environment. Practice is important because it increases the amount retrieved and the speed of retrieval; consistency is important because it ensures that the retrieved instances will be useful. The theory accounts quantitatively for the power-function speed-up and predicts a power-function reduction in the standard deviation that is constrained to have the same exponent as the power function for the speed-up. The theory accounts for qualitative properties as well, explaining how some may disappear and others appear with practice. More generally, it provides an alternative to the modal view of automaticity, arguing that novice performance is limited by a lack of knowledge rather than a scarcity of resources. The focus on learning avoids many problems with the modal view that stem from its focus on resource limitations. Automaticity is an important phenomenon in everyday men-tal life. Most of us recognize that we perform routine activities quickly and effortlessly, with little thought and conscious aware-ness--in short, automatically (James, 1890). As a result, we of-ten perform those activities on &amp;quot;automatic pilot &amp;quot; and turn our minds to other things. For example, we can drive to dinner while conversing in depth with a visiting scholar, or we can make coffee while planning dessert. However, these benefits may be offset by costs. The automatic pilot can lead us astray, caus-ing errors and sometimes catastrophes (Reason &amp; Myceilska, 1982). If the conversation is deep enough, we may find ourselves and the scholar arriving at the office rather than the restaurant, or we may discover that we aren&#039;t sure whether we put two or three scoops of coffee into the pot. Automaticity is also an important phenomenon in skill acqui-sition (e.g., Bryan &amp; Harter, 1899). Skills are thought to consist largely of collections of automatic processes and procedures
4|Intelligent Tutoring Goes to School in the Big City|Abstract. This paper reports on a large-scale experiment introducing and evaluating intelligent tutoring in an urban High School setting. Critical to the success of this project has been a client-centered design approach that has matched our client&#039;s expertise in curricular objectives and classroom teaching with our expertise in artificial intelligence and cognitive psychology. The Pittsburgh Urban Mathematics Project (PUMP) has produced an algebra curriculum that is centrally focused on mathematical analysis of real world situations and the use of computational tools. We have built an intelligent tutor, called PAT, that supports this curriculum and has been made a regular part of 9th grade Algebra in 3 Pittsburgh schools. In the 1993-94 school year, we evaluated the effect of the PUMP curriculum and PAT tutor use. On average the 470 students in experimental classes outperformed students in comparison classes by 15% on standardized tests and 100 % on tests targeting the PUMP objectives. This study provides further evidence that laboratory tutoring systems can be scaled up and made to work, both technically and pedagogically, in real and unforgiving settings like urban high schools. 1.
5|Cognitive tutors: Lessons learned|This article reviews the 10-year history of tutor development based on the
6|Judgments of frequency and recognition memory in a multiple-trace memory model (Tech  (1986) |The multiple-trace simulation model, MINERVA 2, was applied to a number of phenomena found in experiments on relative and absolute judgments of frequency, and forced-choice and yes-no recognition memory. How the basic model deals with effects of repetition, forgetting, list length, orientation task, selective retrieval, and similarity and how a slightly modified version accounts for effects of contextual variability on frequency judgments were shown. Two new experiments on similarity and recognition memory were presented, together with appropriate simulations; attempts to modify the model to deal with additional phenomena were also described. Questions related to the representation of frequency are addressed, and the model is evaluated and compared with related models of frequency judgments and recognition memory. Although memory for specific events (episodic memory) and memory for abstract concepts (generic memory) seem quite different intuitively, experimental evidence for different underlying systems is sparse (see McKoon, Ratcliff,  &amp; Dell, 1986; Ratcliff &amp; McKoon, 1986; Tulving, 1986). One suggestion has been that the two systems are affected differently by repetition,
7|Automated Intelligent Pilots for Combat Flight Simulation|TacAir-Soar is an intelligent, rule-based system that generates believable &#034;human-like&#034; behavior for large scale, distributed military simulations. The innovation of the application is primarily a matter of scale and integration. The system is capable of executing most of the airborne missions that the United States military flies in fixed-wing aircraft. It accomplishes this by integrating a wide variety of intelligent capabilities, including real-time hierarchical execution of complex goals and plans, communication and coordination with humans and simulated entities, maintenance of situational awareness, and the ability to accept and respond to new orders while in flight. The system is currently deployed at the Oceana Naval Air Station WISSARD Lab and the Air Force Research Laboratory in Mesa, AZ. Its most dramatic use was in the Synthetic Theater of War 1997, which was an operational training exercise that ran for 48 continuous hours during which TacAir-Soar flew all U.S. fixed-wing aircraft.
8|Retrieval of propositional information from long-term memory|Three experiments are reported in which subjects learn propositions like A hippie is in the park. The experiments manipulate the number of such propositions involving a particular person (e.g., hippie) or a par-ticular location (e.g., park). After learning the material, subjects are asked to judge whether particular probe propositions are from the study set. Times to make these judgments about probe propositions increase with the number of study propositions involving the person or location used in the probe proposition. A model is presented which assumes a subject simul-taneously accesses memory from all concepts in a probe proposition and serially searches through all study propositions involving each concept. Search of memory terminates as soon as one search process from a concept finds the probe proposition or exhausts the study propositions attached to that concept. This paper is concerned with how propositional information is re-trieved from long-term memory. Three variations will be reported on a
9|The Fan Effect: New Results and New Theories|The fan effect (Anderson, 1974) has been attributed to interference among competing associations to a concept. Recently, it has been suggested that such effects might be due to multiple mental models (Radvansky, Spieler,  &amp; Zacks, 1993) or suppression of concepts (Anderson &amp; Spellman, 1995; Conway &amp; Engle, 1994). We show that the ACT-R (Adaptive Control of Thought-Rational) theory, which embodies associative interference, is consistent with the Radvansky et al results and we fail to find any evidence for concept suppression in a new fan experiment. The ACT-R model provides good quantitative fits to the results from a variety of experiments. The three key concepts in these fits are (a) the associative strength between two concepts reflect the degree to which one concept predicts the other; (b) foils are rejected by retrieving mismatching facts; and (c) subjects can adjust the relative weights they give to various cues in retrieval. 
10|Locus of feedback control in computer-based tutoring: Impact on learning rate, achievement and attitudes|The advent of second-generation intelligent computer tutors raises an important instructional design question:  when should tutorial advice be presented in problem solving? This paper examines four feedback conditions in the ACT Programming Tutor. Three versions offer the student different levels of control over error feedback and correction: (a) immediate feedback and immediate error correction; (b) immediate error flagging and student control of error correction; (c) feedback on demand and student control of error correction. A fourth, No-tutor condition offers no step-by-step problem solving support. The immediate feedback group with greatest tutor control of problem solving yielded the most efficient learning. These students completed the tutor problems fastest, and the three tutor-
11|Does learning a complex task have to be complex? A study in learning decomposition|Many theories of skill acquisition have had considerable success in addressing the fine details of learning in relatively simple tasks, but can they scale up to complex tasks that are more typical of human learning in the real world? Some theories argue for scalability by making the implicit assumption that complex tasks consist of many smaller parts, which are learned according to basic learning principles. Surprisingly, there has been rather sparse empirical testing of this crucial assumption. In this article, we examine this assumption directly by decomposing the learning in the Kanfer–Ackerman Air-Traffic Controller Task (Ackerman, 1988) from the learning at the global level all the way down to the learning at the keystroke level. First, we reanalyze the data from Ackerman (1988) and show that the learning in this complex task does indeed reflect the learning of smaller parts at the keystroke level. Second, in a follow-up eye-tracking experiment, we show that a large portion of the learning at the keystroke level reflects the learning even at a lower, i.e., attentional level. © 2001 Academic Press Over the past 2 decades there have appeared a number of theories of skill
13|The dynamics of cognition: An ACT-R model of cognitive arithmetic|not be interpreted as representing the official policies, either expressed or implied, of the ONR or the U.S. government. Keywords: ACT-R, cognitive arithmetic, Bayesian learning, activation spreading, dynamical systems, parameter analysis, power law, machine learning, hybrid systems. Cognitive arithmetic, the study of the mental representation of numbers and arithmetic facts and the processes that create, access and manipulate them, offers a unique window into human cognition. Unlike traditional Artificial Intelligence (AI) tasks, cognitive arithmetic is trivial for computers but requires years of formal training for humans to master. Understanding the basic assumptions of the human cognitive system which make such a simple and well-understood task so challenging might in turn help us understand how humans perform other, more complex tasks and engineer systems to emulate them. The wealth of psychological data on every aspect of human performance of arithmetic makes precise computational modeling of the detailed error
14|Student modeling from conventional test data: A bayesian approach without priors|Abstract. Although conventional tests are often used for determining a student’s overall competence, they are seldom used for determining a finegrained model. However, this problem does arise occasionally, such as when a conventional test is used to initialize the student model of an ITS. Existing psychometric techniques for solving this problem are intractable. Straightforward Bayesian techniques are also inapplicable because they depend too strongly on the priors, which are often not available. Our solution is to base the assessment on the difference between the prior and posterior probabilities. If the test data raise the posterior probability of mastery of a piece of knowledge even slightly above its prior probability, then that is interpreted as evidence that the student has mastered that piece of knowledge. Evaluation of this technique with artificial students indicates that it can deliver highly accurate assessments.
15|Instructional Interventions in Computer-Based Tutoring: Differential Impact on Learning Time and Accuracy|We can reliably build “second generation ” intelligent computer tutors that are approximately half as effective as human tutors. This paper evaluates two interface enhancements designed to improve the effectiveness of one successful second generation tutor, the ACT Programming Tutor. One enhancement employs animated feedback to make key data structure relationships salient. The second enhancement employs subgoal scaffolding to support students in developing simple programming plans. Both interventions were successful, but had very different impacts on student effort required to achieve mastery in the tutor environment and on subsequent posttest accuracy. These results represent a step forward in closing the gap between computer tutors and human tutors.
16|Implicit strategies and errors in an improved model of early algebra problem solving|We have been refining a cognitive model, written in ACT-R, of student performance in early algebra problem solving. &#034;Early algebra &#034; refers to a class of problems and competencies at the boundary between arithmetic and algebra. Our empirical studies in this domain establish a striking contrast between students &#039; difficulties with symbolic algebra and their relative success with certain kinds of &#034;intuitive &#034; algebraic reasoning. To better understand this contrast, we analyzed student solutions to identify the strategies and errors exhibited and then set out to account for this detailed process data with the utility-based choice mechanism of ACT-R. Our first model contained production rules for explicitly selecting strategies and for making certain systematic errors or bugs. It provided a good quantitative fit to student performance data (R 2 =.90), however, it had two qualitative shortcomings: 1) the productions for strategy selection appeared to serve no computational purpose and 2) the model systematically underpredicted the frequency of non-trivial errors on more complex problems. We created a new model in which explicit strategy selection was eliminated (strategic behavior is emergent) and in which failure to fire a production (an implicit, non-buggy error) is an option at every model choice point. Compared to the first model, this model achieved an equivalent quantitative fit with fewer productions and without the systematic deviations from the error data. We consider the implications of implicit strategies and errors for instruction.
17|Levels indeed! A response to Broadbent|Although Broadbent concedes that we are probably correct in supposing that memory representations are distributed, he argues that psychological evidence is irrelevant to our argument because our point is relevant only at what Marr (1982) has called the implementation ^ level of description and that psychological theory is only properly concerned with what Marr calls the computational level. We believe that Broadbent is wrong on both counts. First, our model is stated at a third level between the other two, Marr&#039;s representational and algorithmic level. Second, we believe that psychology is properly concerned with all three of these levels and that the information processing approach to psychology has been primarily concerned with the same level that we are, namely, the algorithmic level. Thus, our model is a competitor of the logogen model and other models of human information processing. We discuss these and other aspects of the question of levels, concluding that distributed models may ultimately provide more compelling accounts of a number of aspects of cognitive processes than other, competing algorithmic accounts.
18|An Advanced Embedded Training System (AETS) for Tactical Team Training  (1998) |. The Advanced Embedded Training System (AETS) applies intelligent  tutoring systems technology to improve tactical training quality and reduce  manpower needs in simulation-based shipboard team training. AETS provides  layers of performance assessment, cognitive diagnosis, and instructor-support  on top of the existing embedded mission simulation capability. Detailed cognitive  models of trainee task performance are used to drive the assessment, diagnosis  and instructional functions of the system.  1 Introduction and System Rationale  The development of automated instruction has proceeded through several stages, beginning with simple computer-aided instruction (CAI) systems in the late 1950s which provided strictly didactic material in rigid instructional sequences. Dominant themes in the last two decades have been the emphasis on providing dynamic environments for applying and practicing problem-solving knowledge (problem-based learning), focusing diagnosis on the underlying knowled...
19|Architectural Styles and the Design of Network-based Software Architectures|
The World Wide Web has succeeded in large part because its software architecture has been designed to meet the needs of an Internet-scale distributed hypermedia system. The Web has been iteratively developed over the past ten years through a series of modifications to the standards that define its architecture. In order to identify those aspects of the Web that needed improvement and avoid undesirable modifications, a model for the modern Web architecture was needed to guide its design, definition, and deployment.

Software architecture research investigates methods for determining how best to partition a system, how components identify and communicate with each other, how information is communicated, how elements of a system can evolve independently, and how all of the above can be described using formal and informal notations. My work is motivated by the desire to understand and evaluate the architectural design of network-based application software through principled use of architectural constraints, thereby obtaining the functional, performance, and social properties desired of an architecture. An architectural style is a named, coordinated set of architectural constraints.

This dissertation defines a framework for understanding software architecture via architectural styles and demonstrates how styles can be used to guide the architectural design of network-based application software. A survey of architectural styles for network-based applications is used to classify styles according to the architectural properties they induce on an architecture for distributed hypermedia. I then introduce the Representational State Transfer (REST) architectural style and describe how REST has been used to guide the design and development of the architecture for the modern Web.

REST emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. I describe the software engineering principles guiding REST and the interaction constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. Finally, I describe the lessons learned from applying REST to the design of the Hypertext Transfer Protocol and Uniform Resource Identifier standards, and from their subsequent deployment in Web client and server software.
20|On the Criteria To Be Used in Decomposing Systems into Modules|This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a “modularization ” is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.
21|Implementing remote procedure calls|Remote procedure calls (RPC) appear to be a useful paradig m for providing communication across a network between programs written in a high-level language. This paper describes a package providing a remote procedure call facility, the options that face the designer of such a package, and the decisions ~we made. We describe the overall structure of our RPC mechanism, our facilities for binding RPC clients, the transport level communication protocol, and some performance measurements. We include descriptioro ~ of some optimizations used to achieve high performance and to minimize the load on server machines that have many clients.
23|A case study of open source software development: the Apache server|According to its proponents, open source style software development has the capacity to compete successfully, and perhaps in many cases displace, traditional commercial development methods. In order to begin investigating such claims, we examine the development process of a major open source application, the Apache web server. By using email archives of source code change history and problem reports we quantify aspects of developer participation, core team size, code ownership, productivity, defect density, and problem resolution interval for this OSS project. This analysis reveals a unique process, which performs well on important measures. We conclude that hybrid forms of development that borrow the most effective techniques from both the OSS and commercial worlds may lead to high performance software processes.
24|Foundations for the Study of Software Architecture|The purpose of this paper is to build the foundation for software architecture. We first develop an intuition for software architecture by appealing to several well-established architectural disciplines. On the basis of this intuition, we present a model of software architec-ture that consists of three components: elements, form, and rationale. Elements are either processing, data, or connecting elements. Form is defined in terms of the properties of, and the relationships among, the elements-- that is, the constraints on the elements. The ratio-nale provides the underlying basis for the architecture in terms of the system constraints, which most often derive from the system:requirements. We discuss the compo-nents of the model in the context of both architectures and architectural styles and present an extended exam-ple to illustrate some important architecture and style considerations. We conclude by presenting some of the benefits of our approach to software architecture, sum-marizing our contributions, and relating our approach to other current work.  
25|The 4+1 view model of architecture|The 4+1 View Model organizes a description of a software architecture using five concurrent views, each of which
addresses a specific set of concerns. Architects capture their design decisions in four views and use the fifth view to illustrate and validate them.
26|Understanding Code Mobility|The technologies, architectures, and methodologies traditionally used to develop distributed applications exhibit a variety of limitations and drawbacks when applied to large scale distributed settings (e.g., the Internet). In particular, they fail in providing the desired degree of configurability, scalability, and customizability. To address these issues, researchers are investigating a variety of innovative approaches. The most promising and intriguing ones are those based on the ability of moving code across the nodes of a network, exploiting the notion of mobile code. As an emerging research field, code mobility is generating a growing body of scientific literature and industrial developments. Nevertheless, the field is still characterized by the lack of a sound and comprehensive body of concepts and terms. As a consequence, it is rather difficult to understand, assess, and compare the existing approaches. In turn, this limits our ability to fully exploit them in practice, and to further promote the research work on mobile code. Indeed, a significant symptom of this situation is the lack of a commonly accepted and sound definition of the term &#034;mobile code&#034; itself. This paper presents a conceptual framework for understanding code mobility. The framework is centered around a classification that introduces three dimensions: technologies, design paradigms, and applications. The contribution of the paper is twofold. First, it provides a set of terms and concepts to understand and compare the approaches based on the notion of mobile code. Second, it introduces criteria and guidelines that support the developer in the identification of the classes of applications that can leverage off of mobile code, in the design of these applications, and, finally, in the selection of the most appropriate implementation technologies. The presentation of the classification is intertwined with a review of the state of the art in the field. Finally, the use of the classification is exemplified in a case study.
27|Principled design of the modern web architecture |The World Wide Web has succeeded in large part because its software architecture has been designed to meet the needs of an Internet-scale distributed hypermedia system. The modern Web architecture emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. In this paper, we introduce the Representational State Transfer (REST) architectural style, developed as an abstract model of the Web architecture to guide our redesign and definition of the Hypertext Transfer Protocol and Uniform Resource Identifiers. We describe the software engineering principles guiding REST and the interaction constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. We then compare the abstract model to the currently deployed Web architecture in order to elicit mismatches between the existing protocols and the applications they are intended to support.
28|Design and Implementation or the Sun Network Filesystem|this paper we discuss the design and implementation of the/&#039;fiesystem interface in the kernel and the NF$ virtual/&#039;fiesystem. We describe some interesting design issues and how they were resolved, and point out some of the shortcomings of the current implementation. We conclude with some ideas for future enhancements
29|Specifying Distributed Software Architectures|There is a real need for clear and sound design specifications of distributed systems at the architectural level. This is the level of the design which deals with the high-level organisation of computational elements and the interactions between those elements. The paper presents the Darwin notation for specifying this high-level organisation. Darwin is in essence a declarative binding language which can be used to define hierarchic compositions of interconnected components. Distribution is dealt with orthogonally to system structuring. The language supports the specification of both static structures and dynamic structures which may evolve during execution. The central abstractions managed by Darwin are components and services. Services are the means by which components interact. In addition to its use in specifying the architecture of a distributed system, Darwin has an operational semantics for the elaboration of specifications such that they may be used at runtime to di...
30|Specification and analysis of system architecture using Rapide|  Rapide is an event-based concurrent, object-oriented language specifically designed for prototyping system architectures. Two principle design goals are (1) to provide constructs for defining executable prototypes of architectures, and (2) to adopt an execution model in which the concurrency, synchronization, dataflow, and timing properties of a prototype are explicitly represented. This paper describes the partially ordered event set (poset) execution model and outlines with examples some of the event-based features for defining communication architectures and relationships between architectures. Various features of Rapide are illustrated by excerpts from a prototype of the X/Open distributed transaction processing reference architecture.
31|Architectural Mismatch or Why it&#039;s hard to build systems out of existing parts|Many would argue that future breakthroughs in software productivity will depend on our ability to combine existing pieces of software to produce new applications. An important step towards this goal is the development of new techniques to detect and cope with mismatches in the assembled parts. Some problems of composition are due to low-level issues of interoperability, such as mismatches in programming languages or database schemas. However, in this paper we highlight a different, and in manywaysmore pervasive, class of problem: architectural mismatch. Specifically, we use our experience in building a family of software design environments from existing parts to illustrate a variety of types of mismatch that center around the assumptions a reusable part makes about the structure of the application in which is to appear. Based on this experience we show how an architectural view of the mismatch problem exposes some fundamental, thorny problems for software composition and suggests poss...
32|Abstractions for Software Architecture and Tools to Support Them|Architectures for software use rich abstractions and idioms to describe system components, the nature of interactions among the components, and the patterns that guide the composition  of components into systems. These abstractions are higher-level than the elements usually  supported by programming languages and tools. They capture packaging and interaction issues  as well as computational functionality. Well-established (if informal) patterns guide architectural design of systems. We sketch a model for defining architectures and present an implementation of the basic level of that model. Our purpose is to support the abstractions  used in practice by software designers. The implementation provides a testbed for experiments  with a variety of system construction mechanisms. It distinguishes among different  types of components and different ways these components can interact. It supports abstract  interactions such as data flow and scheduling on the same footing as simple procedure...
33|An Event-Based Architecture Definition Language|This paper discusses general requirements for architecture definition languages, and describes the syntax and semantics of the subset of the Rapide language that is designed to satisfy these requirements. Rapide is a concurrent event-based simulation language for defining and simulating the behavior of system architectures. Rapide is intended for modelling the architectures of concurrent and distributed systems, both hardware and software. In order to represent the behavior of distributed systems in as much detail as possible, Rapide is designed to make the greatest posible use of event-based modelling by producing causal event simulations. When a Rapide model is executed it produces a simulation that shows not only the events that make up the model&#039;s behavior, and their timestamps, but also which events caused other events, and which events happened independently.  The architecture definition features of Rapide are described here: event patterns, interfaces, architectures and event pa...
34|Architecture-based runtime software evolution|Continuous availability is a critical requirement for an important class of software systems. For these systems, runtime system evolution can mitigate the costs and risks associated with shutting down and restarting the system for an update. We present an architecture-based approach to runtime software evolution and highlight the role of software connectors in supporting runtime change. An initial implementation of a tool suite for supporting the runtime modification of software architectures, called ArchStudio, is presented. 1
35|Dynamic structure in software architectures|Much of the recent work on Architecture Description Languages (ADL) has concentrated on specifying organisations of components and connectors which are static. When the ADL specification is used to drive system construction, then the structure of the resulting system in terms of its component instances and their interconnection is fixed. This paper examines ADL features which permit the description of dynamic software architectures in which the organisation of components and connectors may change during system execution. The paper outlines examples of language features which support dynamic structure. These examples are taken from Darwin, a language used to describe distributed system structure. An operational semantics for these features is presented in the n-calculus, together with a discussion of their advantages and limitations. The paper discusses some general approaches to dynamic architecture description suggested by these examples. 1
36|Structure and encapsulation in distributed systems: the proxy principle|We present a novel view of the structuring of distributed systems, and a few examples of its utilization in an object-oriented context. In a distributed system, the structure of a service or subsystem may be complex, being implemented as a set of communicating server objects; however, this complexity of structure should not be apparent to the client. In our proposal, a client must first acquire a local object, called a proxy, in order to use such a service. The proxy represents the whole set of servers. The client directs all its communication to the proxy. The proxy, and all the objects it represents, collectively form one distributed object, which is not decomposable by the client. Any higher-level communication protocols are internal to this distributed object. Such a view provides a powerful structuring framework for distributed systems; it can be implemented cheaply without sacrificing much flexibility. It subsumes may previous proposals, but encourages better information-hiding and encapsulation. 1
37|World-wide web: The information universe|The World-Wide Web (W 3) initiative is a practical project to bring a global information universe into existence using available technology. This article describes the aims, data model, and protocols needed to implement the “web”, and compares them with various contemporary systems. The Dream Pick up your pen, mouse or favorite pointing device and press it on a reference in this document- perhaps to the author’s name, or organization, or some related work. Suppose you are directly presented with the background material- other papers, the author’s coordinates, the organization’s address and its entire telephone directory. Suppose each of these documents has the same property of being linked to other original documents all over the world. You would have at your fingertips all you need to know about electronic publishing, high-energy physics or for that matter Asian culture. If you are reading this article on paper, you can only dream, but read on.
38|Correct Architecture Refinement|A method is presented for the stepwise refinement of an abstract architecture into a relatively correct lower-level architecture that is intended to implement it. A refinement step involves the application of a predefined refinement pattern that provides a routine solution to a standard architectural design problem. A pattern contains an abstract architecture schema and a more detailed schema intended to implement it. The two schemas usually contain very different architectural concepts (from different architectural styles). Once a refinement pattern is proven correct, instances of it can be used without proof in developing specific architectures. Individual refinements are compositional, permitting incremental development and local reasoning. A special correctness criterion is defined for the domain of software architecture, as well as an accompanying proof technique. A useful syntactic form of correct composition is defined. The main points are illustrated by means of familiar archit...
39|Exploiting Style in Architectural Design Environments|As the design of software architectures emerges as a discipline within software engineering, it will become increasingly important to support architectural description and analysis with tools and environments. In this paper we describe a system for developing architectural design environments that exploit architectural styles to guide software architects in producing specific systems. The primary contributions of this research are: (a) a generic object model for representing architectural designs; (b) the characterization of architectural styles as specializations of this object model; and (c) a toolkit for creating an open architectural design environment from a description of a specific architectural style. We use our experience in implementing these concepts to illustrate how style-oriented architectural design raises new challenges for software support environments. 
40|The Polylith Software Bus|We describe a system called Polylith that helps programmers prepare and interconnect mixed-language software components for execution in heterogeneous environments. Polylith&#039;s principal benefit is that programmers are free to implement functional requirements separately from their treatment of interfacing requirements; this means that once an application has been developed for use in one execution environment (such as a distributed network) it can be adapted for reuse in other environments (such as a shared-memory multiprocessor) by automatic techniques. This flexibility is provided without loss of performance. We accomplish this by creating a new run-time organization for software. An abstract decoupling agent, called the software bus, is introduced between the system components. Heterogeneity in language and architecture is accommodated since program units are prepared to interface directly to the bus, not to other program units. Programmers specify application structure in terms of ...
41|A Caching Relay for the World Wide Web|We describe the design and performance of a caching relay for the World Wide Web. We model the distribution of requests for pages from the web and see how this distribution affects the performance of a cache. We use the data gathered from the relay to make some general characterizations about the web. (A version of this paper is available at http://www.research.digital.com/- SRC/personal/Steve Glassman/-  CachingTheWeb.html or .../CachingTheWeb.ps)  1 Overview  In January 1994, we set up a caching World Wide Web [10] relay for Digital Equipment Corporation &#039;s facilities in Palo Alto, California. We use a relay to reach the Web because Digital has a security firewall that restricts direct interaction between Digital internal computers and machines outside of Digital. We added caching to the relay because we wanted to improve the relay&#039;s performance and reduce its external network traffic. Clients use the relay for accessing the Web outside of Digital; requests for internal Digital pages...
42|SAAM: A Method for Analyzing the Properties of Software Architectures|While software architecture has become an increasingly important research topic in recent years, insufficient atten-tion has been paid to methods for evaluation of these archi-tectures. Evaluating architectures is dijjicultfor two main reasons. First, there is no common language used 10 de-scribe different architectures. Second, there is no clear way of understanding an architecture with respect to an organi-zation’s ll~e cycle concerns—software quality concerns such as maintainability, portability, modularity, reusability, and so forth. This paper addresses these shortcomings by describing three perspectives by which we can understand the description of a soflware architecture and then propos-ing ajve-step method for analyzing software architectures called SAAM (Software Architecture Analysis Method). We illustrate the method by analyzing three separate user in-terface architectures with respect to the qualiiy of modifi-ability. 1
43|A Design Framework for Internet-Scale Event Observation and Notification|There is increasing interest in having software systems execute and interoperate over the Internet. Execution and interoperation at this scale imply a degree of loose coupling and heterogeneity among the components from which such systems will be built. One common architectural style for distributed; loosely-coupled, heterogeneous software systems is a structure based on event generation, observation and notification. The technology to support this approach is well-developed for local area networks, but it is illsuited to networks on the scale of the Internet. Hence, new technologies are needed to support the construction of large-scale, event-based software systems for the Internet. We have begun to design a new facility for event observation and notification that better serves the needs of Internet-scale applications. In this paper we present results from our first step in this design process, in which we defined a framework that captures many of the relevant design dimensions. Our framework comprises seven models-an object model, an event model, a naming model, an observation model, a time model, a notification model, and a resource model. The paper discusses each of these models in detail and illustrates them using an example involving an update to a Web page. The paper also evaluates three existing technologies with respect to the seven models.
44|A Language and Environment for Architecture-Based Software Development and Evolution|Software architectures have the potential to substantially improve the development and evolution of large, complex, multi-lingual, multi-platform, long-running systems. However, in order to achieve this potential, specific techniques for architecture-based modeling, analysis, and evolution must be provided. Furthermore, one cannot fully benefit from such techniques unless support for mapping an architecture to an implementation also exists. This paper motivates and presents one such approach, which is an outgrowth of our experience with systems developed and evolved according to the C2 architectural style. We describe an architecture description language (ADL) specifically designed to support architecturebased evolution and discuss the kinds of evolution the language supports. We then describe a component-based environment that enables modeling, analysis, and evolution of architectures expressed in the ADL, as well as mapping of architectural models to an implementation infrastructure. The architecture of the environment itself can be evolved easily to support multiple ADLs, kinds of analyses, architectural styles, and implementation platforms. Our approach is fully reflexive: the environment can be used to describe, analyze, evolve, and (partially) implement itself, using the very ADL it supports. An existing architecture is used throughout the paper to provide illustrations and examples. Keywords Software architecture, architecture description language,
45|A Field Guide to Boxology: Preliminary Classification of Architectural Styles for Software Systems|Software architects use a number of commonly-recognized “styles” to guide their design of system structures. Each of these is appropriate for some classes of problems, but none is suitable for all problems. How, then, does a software designer choose an architecture suitable for the problem at hand? Two kinds of information are required: (1) careful discrimination among the candidate architectures and (2) design guidance on how to make appropriate choices. Here we support careful discrimination with a preliminary classification of styles. We use a two-dimensional classification strategy with control and data issues as the dominant organizing axes. We position the major styles within this space and use finer-grained discriminations to elaborate variations on the styles. This provides a framework for organizing design guidance, which we partially flesh out with rules of thumb.
47|A Framework for Classifying and Comparing Architecture Description Languages|Abstract. Software architectures shift developers ’ focus from lines-of-code to coarser-grained architectural elements and their interconnection structure. Architec-ture description languages (ADLs) have been proposed as modeling notations to support architecture-based development. There is, however, little consensus in the research community on what is an ADL, what aspects of an architecture should be modeled in an ADL, and which ADL is best suited for a particular problem. Fur-thermore, the distinction is rarely made between ADLs on one hand and formal specification, module interconnection, simulation, and programming languages on the other. This paper attempts to provide an answer to these questions. It motivates and presents a definition and a classification framework for ADLs. The utility of the definition is demonstrated by using it to differentiate ADLs from other modcling notations. The framework is used to classify and compare several existing ADLs.’
48|Formal Specification and Analysis of Software Architectures Using the Chemical Abstract Machine Model|We are exploring an approach to formally specifying and analyzing software architectures that is based on viewing software systems as chemicals whose reactions are controlled by explicitly stated rules. This powerful metaphor was devised in the domain of theoretical computer science by Banatre and Le M&#039;etayer and then reformulated as the Chemical Abstract Machine, or CHAM, by Berry and Boudol. The CHAM formalism provides a framework for developing operational specifications that does not bias the described system toward any particular computational model. It also encourages the construction and use of modular specifications at different levels of detail. We illustrate the use of the CHAM for architectural description and analysis by applying it to two different architectures for a simple, but familiar, software system, the multiphase compiler. 
49|World-Wide Web Proxies|A WWW proxy server, proxy for short, provides access to the Web for people on closed subnets who can only access the Internet through a firewall machine. The hypertext server developed at CERN, cern_httpd, is capable of running as a proxy, providing seamless external access to HTTP, Gopher, WAIS and FTP. cern_httpd has had gateway features for a long time, but only this spring they were extended to support all the methods in the HTTP protocol used by WWW clients. Clients don&#039;t lose any functionality by going through a proxy, except special processing they may have done for nonnative Web protocols such as Gopher and FTP. A brand new feature is caching performed by the proxy, resulting in shorter response times after the first document fetch. This makes proxies useful even to the people who do have full Internet access and don&#039;t really need the proxy just to get out of their local subnet. This paper gives an overview of proxies and reports their current status.  1.0 Introduction  The pri...
50|Paradigms for process interaction in distributed programs|Distributed computations are concurrent programs in which processes communicate by message passing. Such programs typically execute on network architectures such as networks of workstations ordistributed memory parallel machines (i. e, multicomputers such ashypercubes). Several paradigms—examples or models—for process interaction
51|Modeling the Performance of HTTP over Several Transport Protocols|This paper is a draft that will appear in IEEE/ACM Transactions on Networking. Final editing is still expected. Please replace it with the final version when published.
52|Experience with Performing Architecture Tradeoff Analysis|Software architectures, like complex designs in any field, embody tradeoffs made by the designers. However, these tradeoffs are not always made explicitly by the designers and they may not understand the impacts of their decisions. This paper describes the use of a scenario-based and model-based analysis technique for software architectures—called ATAM—that not only analyzes a software architecture with respect to multiple quality attributes, but explicitly considers the tradeoffs inherent in the design. This is a method aimed at illuminating risks in the architecture through the identification of attribute trends, rather than at precise characterizations of measurable quality attribute values. The ATAM is illustrated in this paper via an example where we analyzed a U.S. Army system for battlefield management.
55|Exploiting ADLs to Specify Architectural Styles Induced by Middleware Infrastructures|Architecture Dejnition Languages (ADLs) enable the for-malization of the architecture of software systems and the execution of preliminary analyses on them. These analyses aim at supporting the identification and solution of design problems in the early stages of software development. We have used ADLs to describe middleware-induced architec-tural styles. These styles describe the assumptions and con-straints that middleware infrastructures impose on the archi-tecture of systems. Our work originates from the belief that the explicit representation of these styles at the architectural level can guide designers in the definition of an architecture compliant with a pre-selected middleware infrastructure, or, conversely can support designers in the identification of the most suitable middleware infrastructure for a specific archi-tecture. In this paper we provide an evaluation of ADLs as to their suitability for defining middleware-induced architec-tural styles. We identify new requirements for ADLs, and we highlight the importance of existing capabilities. Although our experimentation starts from an attempt to solve a spe-cific problem, the results we have obtained provide general lessons about ADLs, learned from defining the architecture of existing, complex, distributed, running systems.
56|Using Off-the-Shelf Middleware to Implement Connectors in Distributed Software Architectures|Software architectures promote development focused on modular building blocks and their interconnections. Since architecture-level components often contain complex functionality, it is reasonable to expect that their interactions will also be complex. Modeling and implementing software connectors thus becomes a key aspect of architecture-based development. Software interconnection and middleware technologies such as RMI, CORBA, ILU, and ActiveX provide a valuable service in building applications from components. The relation of such services to software connectors in the context of software architectures, however, is not well understood. To understand the tradeoffs among these technologies with respect to architectures, we have evaluated several off-the-shelf middleware technologies and identified key techniques for utilizing them in implementing software connectors. Our platform for investigation was C2, a component- and message-based architectural style. By encapsulating middleware functionality within software connectors, we have coupled C2’s existing benefits such as component interchangeability, substrate independence and structural guidance with new capabilities of multi-lingual, multi-process and distributed application development in a manner that is transparent to architects.
57|Maintaining Distributed Hypertext Infostructures: Welcome to MOMspider&#039;s Web|Most documents made available on the World-Wide Web can be considered part of an infostructure --- an information resource database with a specifically designed structure. Infostructures often contain a wide variety of information sources, in the form of interlinked documents at distributed sites, which are maintained by a number of different document owners (usually, but not necessarily, the original document authors). Individual documents may also be shared by multiple infostructures. Since it is rarely static, the content of an infostructure is likely to change over time and may vary from the intended structure. Documents may be moved or deleted, referenced information may change, and hypertext links may be broken.  As it grows, an infostructure becomes complex and difficult to maintain. Such maintenance currently relies upon the error logs of each server (often never relayed to the document owners), the complaints of users (often not seen by the actual document maintainers), and pe...
58|Multilanguage Interoperability in Distributed Systems: EXPERIENCE REPORT|The Q system provides interoperability support for multilingual, heterogeneous component-based software systems. Initial development of Q began in 1988, and was driven by the very pragmatic need for a communication mechanism between a client program written in Ada and a server written in C. The initial design was driven by language features present in C, but not in Ada, or vice-versa. In time our needs and aspirations grew and Q evolved to support other languages, such as C++, Lisp, and Prolog. As a result of pervasive usage by the Arcadia SDE research project, usage levels and modes of the Q system grew and so more emphasis was placed upon portability, reliability, and performance. In that context we identified specific ways in which programming language support systems can directly impede effective interoperability. This necessitated extensive changes to both our conceptual model, and our implementation, of the Q system. We also discovered the need to support modes of interoperabilit...
59|Architecture-Based Specification-Time Software Evolution|OF THE DISSERTATION  Specification-Time Software Evolution by Nenad Medvidovic Doctor of Philosophy in Information and Computer Science University of California, Irvine, 1999 Professor Richard N. Taylor, Chair Software architectures shift the focus of developers from lines-of-code to coarser-grained architectural elements and their overall interconnection structure. Architectures have the potential to substantially improve the development and evolution of large, complex, multi-lingual, multiplatform, long-running systems. In order to achieve this potential, specific architecture-based modeling, analysis, and evolution techniques must be provided. To date, software architecture research has produced an abundance of techniques for architecture modeling and analysis, while largely neglecting architecture-based evolution. This dissertation motivates, presents, and validates a methodology for software evolution at architecture specification-time. The methodology consists of a collection of techniques that, individually and in concert, support flexible, systematic evolution of software architectures in a manner that preserves the desired architectural relationships and properties. The methodology is comprehensive in its scope: it addresses the evolution of individual architectural building blocks---components and connectors---as well as entire architectures; it also supports the transfer of (evolved) architecture-level decisions into implemented systems. The unique aspects of the methodology are: component evolution via heterogeneous subtyping, well suited to a wide range of design and reuse circumstances; connector evolution, facilitated by evolvable interfaces and heterogeneous communication protocols; architecture evolution, facilitated by minimal component interdependenci...
60|The specification of process synchronization by path expressions|A new method of expressing synchronization is presented and the motivations and considerations which led to this method are explained. Synchronization rules, given by &#039;path expressions&#039;, are incorporated into the type definitions which are used to introduce data objects shared by several asynchronous processes. It is shown that the method&#039;s ability to express synchronization rules is equivalent to that of P and V operations, and a means of automatically translating path expressions to existing primitive synchronization operations is given.
61|The Inscape Environment|The Inscape Environment is an integrated software development enviroment for building large software systems by large groups of developers. It provides tools that are knowledgeable about the process of system construction and evolution and that work in symbiosis with the system builders and evolvers. These tools are integrated around the constructive use of formal module interface specifications. We first discuss the problems that Inscape addresses, outline our research strategies and approaches to solving these problems, and summarize the contributions of the Inscape Environment. We then discuss the major aspects
62|Sesame: A Generic Architecture for Storing and Querying RDF and RDF Schema|RDF and RDF Schema are two W3C standards aimed at  enriching the Web with machine-processable semantic data.
63|Resource Description Framework (RDF) Model and Syntax Specification  (1998) |This document is a revision of the public working draft dated 1998-08-19 incorporating suggestions received in review comments and further deliberations of the W3C RDF Model and Syntax Working Group. With the publication of this draft, the RDF Model and Syntax Specification enters &#034;last call.&#034; The last call period will end on October 23, 1998. Comments on this specification may be sent to www-rdf-comments@w3.org. The archive of public comments is available at http://www.w3.org/Archives/Public/www-rdf-comments. Significant changes from the previous draft are highlighted in Appendix E. While we do not anticipate substantial changes, we still caution that further changes are possible. Therefore while we encourage active implementation to test this specification we also recommend that only software that can be easily field-upgraded be implemented to this specification at this time. This is a W3C Working Draft for review by W3C members and other interested parties. Publication as a working draft does not imply endorsement by the W3C membership. The RDF Model and Syntax  Working Group will not allow early implementation to constrain their ability to make changes to this specification prior to final release. This is a draft document and may be updated, replaced or obsoleted by other documents at any time. It is inappropriate to cite W3C Working Drafts as other than &#034;work in progress&#034;. This work is part of the W3C Metadata Activity.
64|The RDFSuite: Managing Voluminous RDF Description Bases|Metadata are widely used in order to fully exploit information resources available  on corporate intranets or the Internet. The Resource Description Framework (RDF)  aims at facilitating the creation and exchange of metadata as any other Web data. The  growing number of available information resources and the proliferation of description  services in various user communities, lead nowadays to large volumes of RDF metadata.  Managing such RDF resource descriptions and schemas with existing low-level APIs and  file-based implementations does not ensure fast deployment and easy maintenance of realscale  RDF applications. In this paper, we advocate the use of database technology to  support declarative access, as well as, logical and physical independence for voluminous  RDF description bases.  We present RDFSuite, a suite of tools for RDF validation, storage and querying.  Specifically, weintroduce a formal data model for RDF description bases created using  multiple schemas. Compared to ...
65|Querying Community Web Portals|Anewgeneration of information systems suchasorganizational memories, vertical aggregators,  infomediaries, etc. is emerging nowadays. Such systems, termed CommunityWeb  Portals, intend to support specific communities of interest (e.g., enterprise, professional, trading)  on corporate intranets or the Web. More precisely, Portal Catalogs, organize and describe  various information resources (e.g., sites, documents, data) for diverse target audiences  (corporate, inter-enterprise, e-marketplace, etc.), in a multitude of ways, which are far more  flexible and complex than those provided by standard (relational or object) databases. Yet, in  commercial software for deploying CommunityPortals, querying is still limited to full-text (or  attribute-value) retrieval and more advanced information-seeking needs implies navigational  access. Furthermore, recentWeb standards for describing resources are completely ignored.
67|Statecharts: A Visual Formalism For Complex Systems|We present a broad extension of the conventional formalism of state machines and state diagrams, that is relevant to the specification and design of complex discrete-event systems, such as multi-computer real-time systems, communication protocols and digital control units. Our diagrams, which we call statecharts, extend conventional state-transition diagrams with essentially three olements, dealing, respectively, with the notions of hierarchy, concurrency and communication. These transform the language of state diagrams into a highly structured&#039; and economical description language. Statecharts are thus compact and expressive--small diagrams can express complex behavior--as well as compositional and modular. When coupled with the capabilities of computerized graphics, statecharts enable viewing the description at different levels of detail, and make even very large specifications manageable and comprehensible. In fact, we intend to demonstrate here that statecharts counter many of the objections raised against conventional state diagrams, and thus appear to render specification by diagrams an attractive and plausible approach. Statecharts can be used either as a stand-alone behavioral description or as part of a more general design methodology that deals also with the system&#039;s other aspects, such as functional decomposition and data-flow specification. We also discuss some practical experience that was gained over the last three years in applying the statechart formalism to the specification of a particularly complex system.
68|The design and implementation of hierarchical software systems with reusable components|We present a domain-independent model of hierarchical software system design and construction that is based on interchangeable software components and largescale reuse. The model unifies the conceptualizations of two independent projects, Genesis and Avoca, that are successful examples of software component/building-block technologies and domain modeling. Building-block technologies exploit large-scale reuse, rely on open architecture software, and elevate the granularity of programming to the subsystem level. Domain modeling formalizes the similarities and differences among systems of a domain. We believe our model is a blue-print for achieving software component technologies in many domains.
69|The X Window System|The X Window System, Version 11, is the standard window system on Linux and UNIX systems. X11, designed in 1987, was “state of the art ” at that time. From its inception, X has been a network transparent window system in which X client applications can run on any machine in a network using an X server running on any display. While there have been some significant extensions to X over its history (e.g. OpenGL support), X’s design lay fallow over much of the 1990’s. With the increasing interest in open source systems, it was no longer sufficient for modern applications and a significant overhaul is now well underway. This paper describes revisions to the architecture of the window system used in a growing fraction of desktops and embedded systems 1
70|A Formal Approach to Software Architecture|As software systems become more complex, the overall system structure---or software architecture---becomes a central design problem. A system&#039;s architecture provides a model of the system that suppresses implementation detail, allowing the architect to concentrate on the analyses and decisions that are most crucial to structuring the system to satisfy its requirements.  Unfortunately, current representations of software architecture are informal and ad hoc. While architectural concepts are often embodied in infrastructure to support specific architectural styles and in the initial conceptualization of a system configuration, the lack of an explicit, independently-characterized architecture or architectural style significantly limits the benefits of software architectural design in current practice.  In this dissertation, I show that an Architecture Description Language based on a formal, abstract model of system behavior can provide a practical means of describing and analyzing softwar...
71|Using Style to Understand Descriptions of Software Architecture|The software architecture of most systems is described informally and diagrammatically. In order for these descriptions to be meaningful at all, figures are understood by interpreting the boxes and lines in specific, conventionalized ways [5]. The imprecision of these interpretations has a number of limitations. In this paper we consider these conventionalized interpretations as architectural styles and provide a formal framework for their uniform definition. In addition to providing a template for precisely defining new architectural styles, this framework allows for the proof that the notational constraints on a style are sufficient to guarantee the meanings of all described systems and provides a unified semantic base through which different stylistic interpretations can be compared.
72|Formalizing design spaces: Implicit invocation mechanisms |An important goal of software engineering is to exploit commonalities in system design in order to reduce the complexity of building new systems, support largescale reuse, and provide automated assistance for system development. A significant roadblock to accomplishing this goal is that common properties of systems are poorly understood. In this paper we argue that formal specification can help solve this problem. A formal definition of a design framework can identify the common properties of a family of systems and make clear the dimensions of specialization. New designs can then be built out of old ones in a principled way, at reduced cost to designers and implementors. To illustrate these points, we present a formalization of a system integration technique called implicit invocation. We show how many previously unrelated systems can be viewed as instances of the same underlying framework. Then we briefly indicate how the formalization allows us to reason about certain properties of those systems as well as the relationships between different systems. 1
73|Software Requirements Negotiation and Renegotiation Aids: A Theory-W Based Spiral Approach|A major problem in requirements engineering is obtaining requirements that address the concerns of multiple stakeholders. An approach to such a problem is the Theory-W based Spiral Model. One key element of this model is stakeholder collaboration and negotiation to obtain win-win requirements. This paper focuses on the problem of developing a support system for such a model. In particular it identifies needs and capabilities required to address the problem of negotiation and renegotiation that arises when the model is applied to incremental requirements engineering. The paper formulates elements of the support system, called WinWin, for providing such capabilities. These elements were determined by experimenting with versions of WinWin and understanding their merits and deficiencies. The key elements of WinWin are described and their use in incremental requirements engineering are demonstrated, using an example renegotiation scenario from the domain of software engineering environments...
74|Reconciling the Needs of Architectural Description with Object-Modelling Notations|Abstract. Complex software systems require expressive notations for representing their software architectures. Two competing paths have emerged. One is to use a specialized notation for architecture – or architecture description language (ADL). The other is to adapt a general-purpose modeling notation, such as UML. The latter has a number of benefits, including familiarity to developers, close mapping to implementations, and commercial tool support. However, it remains an open question as to how best to use object-oriented notations for architectural description, and, indeed, whether they are sufficiently expressive, as currently defined. In this paper we take a systematic look at these questions, examining the space of possible mappings from ADLs into object notations. Specifically, we describe (a) the principle strategies for representing architectural structure in UML; (b) the benefits and limitations of each strategy; and (c) aspects of architectural description that are intrinsically difficult to model in UML using the strategies. 1
75|Using object-oriented typing to support architectural design in the C2 style|Abstract-- Software architectures enable large-scale software development. Component reuse and substitutability, two key aspects of large-scale development, must be planned for during software design. Object-oriented (OO) type theory supports reuse by structuring inter-component relationships and verifying those relationships through type checking in an architecture definition language (ADL). In this paper, we identify the issues and discuss the ramifications of applying OO type theory to the C2 architectural style. This work stems from a series of experiments that were conducted to investigate component reuse and substitutability in C2. We also discuss the limits of applicability of OO typing to C2 and how we addressed them in the C2 ADL. 1
76|Describing Software Architecture with UML|: This paper describes our experience using UML, the Unified Modeling Language, to describe the software architecture of a system. We found that it works well for communicating the static structure of the architecture: the elements of the architecture, their relations, and the variability of a structure. These static properties are much more readily described with it than the dynamic properties. We could easily describe a particular sequence of activities, but not a general sequence. In addition, the ability to show peer-to-peer communication is missing from UML.  Keywords: software architecture, UML, architecture descriptions, multiple views  1. INTRODUCTION  UML, the Unified Modeling Language, is a standard that has wide acceptance and will likely become even more widely used. Although its original purpose was for detailed design, its ability to describe elements and the relations between them makes it potentially applicable much more broadly. This paper describes our experience usin...
77|Software Interconnection Models|We present a formulation of interconnection models and present the unit and syntactic models --- the primary models used for managing the evolution of large software systems. We discuss various tools that use these models and evaluate how well these models support the management of system evolution. We then introduce the semantic interconnection model. The semantic interconnection model incorporates the advantages of the unit and syntactic interconnection models and provides extremely useful extensions to them. By refining the grain of interconnections to the level of semantics (that is, to the predicates that define aspects of behavior) we provide tools that are better suited to manage the details of evolution in software systems and that provide a better understanding of the implications of changes. We do this by using the semantic interconnection model to formalize the semantics of program construction, the semantics of changes, and the semantics of version equivalence and compatibi...
78|Using tool abstraction to compose systems|paradigms support the evolution of large-scale software systems. Data abstraction eases design changes in the representation of data structures, while tool abstraction does the same with system functions. M anaging complexity and supporting evolution are two fundamental “i, problems with large-scale software systems. ’ Although modularization,. has long been accepted as the basic approach to managing complexity, as David Parnas observed nearly 20 years ago, not all modularizations are equally good at handling evolution.’ Data abstraction is a popular, important style of modularization. In this style, an abstract data type is defined by an explicit interface that specifies operations on
79|Assessing the Suitability of a Standard Design Method for Modeling Software Architectures| Software architecture descriptions are high-level models of software systems.  Most existing special-purpose architectural notations have a great deal of  expressive power but are not well integrated with common development  methods. Conversely, mainstream development methods are accessible to  developers, but lack the semantics needed for extensive analysis. In our  previous work, we described an approach to combining the advantages of  these two ways of modeling architectures. While this approach suggested a  practical strategy for bringing architectural modeling into wider use, it  introduced specialized extensions to a standard modeling notation, which  could also hamper wide adoption of the approach. This paper attempts to  assess the suitability of a standard design method &#034;as is&#034; for modeling  software architectures.  
80|Experience with a course on architectures for software systems|Abstract. As software systems grow in size and complexity their design problem extends beyond algorithms and data structures to issues of system design. This area receives little or no treatment in existing computer science curricula. Although courses about speci c systems are usually available, there is no systematic treatment of the organizations used to assemble components into systems. These issues { the software architecture level of software design { are the subject of a new course that we taught for the rst time in Spring 1992. This paper describes the motivation for the course, the content and structure of the current version, and our plans for improving the next version. 1
81|Aladdin: A Tool for Architecture-Level Dependence Analysis of Software Systems|The emergence of formal architecture description languages provides an opportunity to perform analyses at high levels of abstraction, as well as early in the development process. Previous research has primarily focused on developing techniques such as algebraic and transition-system analysis to detect component mismatches or global behavioral incorrectness. In this paper, we present Aladdin, a tool that implements chaining, a static dependence analysis technique for use with architectural descriptions. Dependence analysis has been used widely at the implementation level to aid program optimization, anomaly checking, program understanding, testing, and debugging. We investigate the definition and application of dependence analysis at the architectural level. We illustrate the utility of chaining, through the use of Aladdin, by showing how the technique can be used to answer various questions one might pose of a Rapide architecture specification. 
82|The impact of Mesa on system design|The Mesa programming language supports program modularity in ways that permit subsystems to be developed separately but to be bound together with complete type safety. Separate and explicit interface definitions provide an effective means of communication, both between programs and between programmers. A configuration language describes the organization of a system and controls the scopes of interfaces. These facilities have had a profound impact on the way we design systems and Organize development projects. This paper reports our recent experience with Mesa, particularly its use in the development of an operating system. It illustrates techniques for designing interfaces, for using the interface language as a specification language, and for organizing a ~ystem to achieve the practical benefits of program modularity without sacrificing strict type-checking.
83|Security Architecture for the Internet Protocol|Content-Type: text/plain
84|New Directions in Cryptography|Two kinds of contemporary developments in cryptography are examined. Widening applications of teleprocessing have given rise to a need for new types of cryptographic systems, which minimize the need for secure key distribution channels and supply the equivalent of a written signature. This paper suggests ways to solve these currently open problems. It also discusses how the theories of communication and computation are beginning to provide the tools to solve cryptographic problems of long standing.
86|Privacy Enhancement for Internet Electronic Mail: Part II: Certificate-Based Key Management|this memo is unlimited. Acknowledgements
87|The Architecture and Implementation of Network-Layer Security Under Unix|swIPe is a network-layer security protocol for the IP protocol suite. This paper presents the architecture, design philosophy, and performance of an implementation of swIPe under several variants of Unix. swIPe provides authentication, integrity, and confidentiality of IP datagrams, and is completely compatible with the existing IP infrastructure. To maintain this compatibility, swIPe is implemented using an encapsulation protocol. Mechanism (the details of the protocol) is decoupled from policy (what and when to protect) and key management. swIPe under Unix is implemented using a virtual network interface. The parts of the implementation that process incoming and outgoing packets are entirely in the kernel; parameter setting and exception handling, however, are managed by userlevel processes. The performance of swIPe on modern workstations is primarily limited only by the speed of the underlying authentication and encryption algorithms; the mechanism overhead is negligible in our prototype. 1.
88|On Internet Authentication|Status of this Memo This document provides information for the Internet community. This memo does not specify an Internet standard of any kind. Distribution of this memo is unlimited. 1.
89|HMAC-MD5 IP Authentication with Replay Prevention&#034;, RFC 2085|This document specifies an Internet standards track protocol for the Internet community, and requests discussion and suggestions for improvements. Please refer to the current edition of the &#034;Internet Official Protocol Standards &#034; (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. This document describes a keyed-MD5 transform to be used in conjunction with the IP Authentication Header [RFC-1826]. The particular transform is based on [HMAC-MD5]. An option is also
90|Domain Name System Protocol Security Extensions. draft-ietf-dnssec-secext-04.txt -- work in progress|This draft, file name draft-ietf-dnssec-secext-03.txt, is intended to be become a proposed standard RFC. Distribution of this document is unlimited. Comments should be sent to the DNS Security Working Group mailing list &lt;dns-security@tis.com&gt; or to the authors. This document is an Internet-Draft. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months. Internet-Drafts may be updated, replaced, or obsoleted by other documents at any time. It is not appropriate to use Internet-Drafts as reference material or to cite them other than as a ‘‘working draft’ ’ or ‘‘work in progress.’’ To learn the current status of any Internet-Draft, please check the 1id-abstracts.txt listing contained in the Internet-Drafts Shadow Directories on ds.internic.net, nic.nordu.net, ftp.isi.edu, munnari.oz.au, or ftp.is.co.za. Eastlake, Kaufman [Page 1] INTERNET-DRAFT DNS Protocol Security Extensions January 1995 The Domain Name System (DNS) has become a critical operational part of the Internet infrastructure yet it has no strong security mechanisms to assure data integrity or authentication. Extensions to the DNS are described that provide these services to security aware resolvers or applications through the use of cryptographic digital signatures. These digital signatures are included in secured zones as resource records. Security can still be provided even through non-security aware DNS servers. The extensions also provide for the storage of authenticated public keys in the DNS. This storage of keys can support a general public key distribution service as well as DNS security. The stored keys enable security aware resolvers to learn the authenticating key of zones in addition to keys for which they are initially configured. Keys associated with DNS names can be retrieved to support other protocols. Provision is made for a variety of key types and algorithms. In addition, the security extensions provide for the optional authentication of DNS protocol transactions.
91|A Note on Distributed Computing|We argue that objects that interact in a distributed system need to be dealt with in ways that are intrinsically different from objects that interact in a single address space. These differences are required because distributed systems require that the programmer be aware of latency, have a different model of memory access, and take into account issues of concurrency and partial failure. We look at a number of distributed systems that have attempted to paper over the distinction between local and remote objects, and show that such systems fail to support basic requirements of robustness and reliability. These failures have been masked in the past by the small size of the distributed systems that have been built. In the enterprise-wide distributed systems foreseen in the near future, however, such a masking will be impossible. We conclude by discussing what is required of both systems-level and application-level programmers and designers if one is to take distribution seriously.
92|A Component- and Message-Based Architectural Style for GUI Software|While a large fraction of application code is devoted to graphical user interface (GUI) functions, support for reuse in this domain has largely been confined to the creation of GUI toolkits (&#034;widgets&#034;). We present a novel architectural style directed at supporting larger grain reuse and flexible system composition. Moreover, the style supports design of distributed, concurrent applications. Asynchronous notification messages and asynchronous request messages are the sole basis for inter-component communication. A key aspect of the style is that components are not built with any dependencies on what typically would be considered lower-level components, such as user interface toolkits. Indeed, all components are oblivious to the existence of any components to which notification messages are sent. While our focus has been on applications involving graphical user interfaces, the style has the potential for broader applicability. Several trial applications using the style are described.
93|Organization-Based Analysis of Web-Object Sharing and Caching|Performance-enhancing mechanisms in the World Wide Web primarily exploit repeated requests to Web documents by multiple clients. However, little is known about patterns of shared document access, particularly from diverse client populations. The principal goal of this paper is to examine the sharing of Web documents from an organizational point of view. An organizational analysis of sharing is important, because caching is often performed on an organizational basis; i.e., proxies are typically placed in front of large and small companies, universities, departments, and so on. Unfortunately, simultaneous multi-organizational traces do not currently exist and are difficult to obtain in practice.
94|Uniform Resource Locators|Many protocols and systems for document search and retrieval are currently in use, and many more protocols or refinements of existing protocols are to be expected in a field whose expansion is explosive. These systems are aiming to achieve global search and readership of documents across differing computing platforms, and despite a plethora of protocols and data formats. As protocols evolve, gateways can allow global access to remain possible. As data formats evolve, format conversion programs can preserve global access. There is one area, however, in which it is impractical to make conversions, and that is in the names and addresses used to identify objects. This is because names and addresses of objects are passed on in so many ways, from the backs of envelopes to hypertext objects, and may have a long life. This paper discusses the requirements on a universal syntax which can be used to refer to objects available using existing protocols, and may be extended with technology. It make...
95|HTTP State Management Mechanism|This document specifies a way to create a stateful session with HTTP requests and responses. It describes two new headers, Cookie and Set-Cookie2, which carry state information between participating origin servers and user agents. The method described here differs from Netscape&#039;s Cookie proposal [Netscape],   but it can interoperate with HTTP/1.0 user agents that use Netscape&#039;s method. (See the HISTORICAL section.)  This document reflects implementation experience with RFC 2109 [RFC2109] and obsoletes it.   2. TERMINOLOGY  The terms user agent , client , server, proxy, and origin server have the same meaning as in the HTTP/1.1   specification [RFC2068].  Host name (HN) means either the host domain name (HDN) or the numeric Internet Protocol (IP) address of a host. The fully qualified domain name is preferred; use of numeric IP addresses is strongly discouraged.  The terms request-host and request-URI refer to the values the client would send to the server as, respectively, the host (bu...
97|The magical number seven, plus or minus two: Some limits on our capacity for processing information|z Information measurement z Absolute judgments of unidimensional stimuli z Absolute judgments of multidimensional stimuli z Subitizing
98|Explanation-Based Learning: An Alternative View|Key words: machine learning, concept acquisition, explanation-based learning Abstract. In the last issue of this journal Mitchell, Keller, and Kedar-Cabelli presented a unifying framework for the explanation-based approach to machine learning. While it works well for a number of systems, the framework does not adequately capture certain aspects of the systems under development by the explanation-based learning group at Illinois. The primary inadequacies arise in the treatment of concept operationality, organization of knowledge into schemata, and learning from observation. This paper outlines six specific problems with the previously proposed framework and presents an alternative generalization method to perform explanation-based learning of new concepts.
99|Repair Theory: A generative theory of bugs in procedural skills |This paper describes a generative theory of bugs. It claims that all bugs of D procedural skill con be derived by a highly constrained form of problem solving acting on incomplete procedures. These procedures are characterized by formal deletion operations that model incomplete learning and forgetting. The problem solver and the deletion operator have been constrained to make it impossible to derive “star-bugs”--xJgorithms that are so absurd that expert diagnosticians agree that the alogorithm will never be observed as o bug. Hence, the theory not only generates the observed bugs, it fails to generate star-bugs. The theory has been tested on on extensive doto base of bugs for multidigit subtraction that was collected with the aid of the diagnostic systems BUGGY and DEBUGGY. In addition to predicting bug occurrence, by adoption of additional hypotheses, the theory also makes predictions about the frequency and stability of bugs, as well as the occurrence of certain lotencies in processing time during testing. Arguments are given that the theory can be applied to domains other than subtraction and that it con be extended to provide a theory of procedural learning that accounts for bug acquisition. Lastly, particular care has been taken to make the theory principled so that it can not be tailored to fit ony possible data. 1.
101|Mapping explanation-based generalization onto Soar|Explanation-based generalization (EBG) is a powerful approach to concept formation in which a justifiable concept definition is acquired from a single training example and an underlying theory of how the example is an instance of the concept. Soar is an attempt to build a general cognitive architecture combining general learning, problem solving, and memory capabilities. It includes an independently developed learning mechanism, called chunking, that is similar to but not the same as explanation-based generalization. In this article we clarify the relationship between the explanation-based generalization framework and the Soar/chunking combination by showing how the EBG framework maps onto Soar, how several EBG conceptformation tasks are implemented in Soar, and how the Soar approach suggests answers to some of the outstanding issues in explanation-based generalization. I
102|FFTW: An Adaptive Software Architecture For The FFT|FFT literature has been mostly concerned with minimizing the number of floating-point operations performed by an algorithm. Unfortunately, on present-day microprocessors this measure is far less important than it used to be, and interactions with the processor pipeline and the memory hierarchy have a larger impact on performance. Consequently, one must know the details of a computer architecture in order to design a fast algorithm. In this paper, we propose an adaptive FFT program that tunes the computation automatically for any particular hardware. We compared our program, called FFTW, with over 40 implementations of the FFT on 7 machines. Our tests show that FFTW&#039;s self-optimizing approach usually yields significantly better performance than all other publicly available software. FFTW also compares favorably with machine-specific, vendor-optimized libraries. 1. INTRODUCTION The discrete Fourier transform (DFT) is an important tool in many branches of science and engineering [1] and...
103|Cilk: An Efficient Multithreaded Runtime System|Cilk (pronounced &#034;silk&#034;) is a C-based runtime system for multithreaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the &#034;work&#034; and &#034;critical-path length&#034; of a Cilk computation can be used to model performance accurately. Consequently, a Cilk programmer can focus on reducing the computation&#039;s work and critical-path length, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of &#034;fully strict&#034; (well-structured) programs, the Cilk scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The Cilk
104|FAST FOURIER TRANSFORMS: A TUTORIAL REVIEW AND A STATE OF THE ART|The publication of the Cooley-Tukey fast Fourier transform (FIT) algorithm in 1965 has opened a new area in digital signal processing by reducing the order of complexity of some crucial computational tasks like Fourier transform and convolution from N 2 to N log2 N, where N is the problem size. The development of the major algorithms (Cooley-Tukey and split-radix FFT, prime factor algorithm and Winograd fast Fourier transform) is reviewed. Then, an attempt is made to indicate the state of the art on the subject, showing the standing of research, open problems and implementations. 
105|An analysis of dag-consistent distributed shared-memory algorithms|In this paper, we analyze the performance of parallel multi-threaded algorithms that use dag-consistent distributed shared memory. Specifically, we analyze execution time, page faults, and space requirements for multithreaded algorithms executed by a FP(C) workstealing thread scheduler and the BACKER algorithm for maintaining dag consistency. We prove that if the accesses to the backing store are random and independent (the BACKER algorithm actually uses hashing), the expected execution time TP(C)of a “fully strict” multithreaded computation on P processors, each with a LRU cache of C pages, is O(T1(C)=P+mCT8), where T1(C)is the total work of the computation including page faults, T 8 is its critical-path length excluding page faults, and m is the minimum page transfer time. As
106|An algorithm for computing the mixed radix fast Fourier transform|This paper presents an algorithm for computing the fast Fourier transform, based on a method proposed by Cooley and Tukey. As in their algorithm, the dimension n of the transform is factored (if possi-ble), and n/p elementary transforms of dimension p are computed for each factor p of n. An improved method of computing a transform step corresponding to an odd factor of n is given; with this method, the number of complex multiplicatiops for an elementary transform of di-mension p is reduced from (p-1)2 to (p-1)2/4 for odd p. The fast Fourier transform, when computed in place, requires a final permuta-tion step to arrange the results in normal order. This algorithm in-cludes an efficient method for permuting the results in place. The al-gorithm is described mathematically and illustrated by a FORTRAN subroutine.
107|A Framework for Generating Distributed-Memory Parallel Programs for Block Recursive Algorithms|A framework for synthesizing communication-efficient distributed-memory parallel programs for block recursive algorithms such as the fast Fourier transform (FFT) and Strassen’s matrix multiplication is presented. This framework is based on an algebraic representation of the algorithms, which involves the tensor (Kronecker) product and other matrix operations. This representation is useful in analyzing the communication implications of computation partitioning and data distributions. The programs are synthesized under two different target program models. These two models are based on different ways of managing the distribution of data for optimizing communication. The first model uses point-to-point interprocessor communication primitives, whereas the second model uses data redistribution primitives involving collective all-to-many communication. These two program models are shown to be suitable for different ranges of problem size. The methodology is illustrated by synthesizing communication-efficient programs for the FFT. This framework has been incorporated into the EX-TENT system for automatic generation of parallel/vector programs for block recursive algorithms. © 1996 Academic Press, Inc. 1.
108|Automatic Generation of Prime Length FFT Programs|We describe a set of programs for circular convolution and prime length FFTs that are relatively short, possess great structure, share many computational procedures, and cover a large variety of lengths. The programs make clear the structure of the algorithms and clearly enumerate independent computational branches that can be performed in parallel. Moreover, each of these independent operations is made up of a sequence of sub-operations which can be implemented as vector/parallel operations. This is in contrast with previously existing programs for prime length FFTs: they consist of straight line code, no code is shared between them, and they can not be easily adapted for vector/parallel implementations. We have also developed a program that automatically generates these programs for prime length FFTs. This code generating program requires information only about a set of modules for computing cyclotomic convolutions. Contact Address:  Ivan W. Selesnick Electrical and Computer Engineer...
109|The Quick Discrete Fourier Transform|This paper will look at an approach that uses symmetric properties of the basis function to remove redundancies in the calculation of discrete Fourier transform (DFT). We will develop an algorithm, called the quick Fourier transform (QFT), that will reduce the number of floating point operations necessary to compute the DFT by a factor of two or four over direct methods or Goertzel&#039;s method for prime lengths. Further apply the idea to the calculation of a DFT of length-2  M  , we construct a new O(N log N) algorithm. The algorithm can be easily modified to compute the DFT with only a subset of input points, and it will significantly reduce the number of operations when the data are real. The simple structure of the algorithm and the fact that it is well suited for DFTs on real data should lead to efficient implementations and to a wide range of applications. 1. INTRODUCTION  In the field of digital signal processing, the discrete Fourier transform (DFT) is an interesting, important, an...
110|A Study|on the rubrene emission sensitized by a phosphorescent Ir compound in the host of CBP
111|From Domain Model to Architectures|A software system can be evaluated against criteria in two broad categories: • functional and performance attributes: how well does the system, during execution, satisfy its behavioral, functional, and performance requirements? Does it provide the required results? Does it provide them in a timely enough manner? Are the results correct, or within specified accuracy and stability tolerances? • non-functional attributes: how easy is the system to integrate, test, and modify? How expensive was it to develop? These two categories are orthogonal; systems that unfailingly meet all of their requirements may or may not have been prohibitively expensive to develop, and may or may not be impossible to modify. Highly modifiable systems may or may not produce correct results. Given a set of requirements for a system, the developer must choose an architecture that will allow the implementation of the system to proceed in a straightforward manner, producing a product that meets its functional and non-functional requirements. How is that done? 1.1 Producing architectures to meet functional requirements There is, unfortunately, no reliable automatic or semi-automatic technology that will produce
112| An Architecture for Wide-Area Multicast Routing |Existing multicast routing mechanisms were intended for use within regions where a group is widely represented or bandwidth is universally plentiful. When group members, and senders to those group members, are distributed sparsely across a wide area, these schemes are not efficient; data packets or membership report information are occasionally sent over many links that do not lead to receivers or senders, respectively. Wehave developed a multicast routing architecture that efficiently establishes distribution trees across wide area internets, where many groups will be sparsely represented. Efficiency is measured in terms of the state, control message processing, and data packet processing, required across the entire network in order to deliver data packets to the members of the group. Our Protocol Independent Multicast (PIM) architecture: (a) maintains the traditional IP multicast service model of receiver-initiated membership; (b) can be configured to adapt to different multicast group and network characteristics; (c) is not dependent on a specific unicast routing protocol; and (d) uses soft-state mechanisms to adapt to underlying network conditions and group dynamics. The robustness, flexibility, and scaling properties of this architecture make it well suited to large heterogeneous inter-networks.
113|A Reliable Multicast Framework for Light-weight Sessions and Application Level Framing|This paper... reliable multicast framework for application level framing and light-weight sessions. The algorithms of this framework are efficient, robust, and scale well to both very large networks and very large sessions. The framework has been prototype in wb, a distributed whiteboard application, and has been extensively tested on a global scale with sessions ranging from a few to more than 1000 participants. The paper describes the principles that have guided our design, including the 1P multicast group delivery model, an end-to-end, receiver-based model of reliability, and the application level framing protocol model. As with unicast communications, the performance of a reliable multicast delivery algorithm depends on the underlying topology and operational environment. We investigate that dependence via analysis and simulation, and demonstrate an adaptive algorithm that uses the results of previous loss recovery events to adapt the control parameters used for future loss recovery. Whh the adaptive algorithm, our reliable multicast delivery algorithm provides good performance over a wide range of underlying topologies. 
114|Multicast Routing in Datagram Internetworks and Extended LANs|Multicasting, the transmission of a packet to a group of hosts, is an important service for improving the efficiency and robustness of distributed systems and applications. Although multicast capability is available and widely used in local area networks, when those LANs are interconnected by store-and-forward routers, the multicast service is usually not offered across the resulting internetwork. To address this limitation, we specify extensions to two common internetwork routing algorithms-distance-vector routing and link-state routing-to support low-delay datagram multicasting beyond a single LAN. We also describe modifications to the single-spanning-tree routing algorithm commonly used by link-layer bridges, to reduce the costs of multicasting in large extended LANs. Finally, we discuss how the use of multicast scope control and hierarchical multicast routing allows the multicast service to scale up to large internetworks.
116|Distance Vector Multicast Routing Protocol|This RFC describes a distance-vector-style routing protocol for routing multicast datagrams through an internet. It is derived from the Routing Information Protocol (RIP) [1], and implements multicasting as described in RFC-1054. This is an experimental protocol, and its implementation is not recommended at this time. Distribution of this memo is unlimited. 2.
117|Multicast Extensions to OSPF|This memo documents enhancements to the OSPF protocol enabling the routing of IP multicast datagrams. In this proposal, an IP multicast packet is routed based both on the packet&#039;s source and its multicast destination (commonly referred to as source/destination routing). As it is routed, the multicast packet follows a shortest path to each multicast destination. During packet forwarding, any commonality of paths is exploited; when multiple hosts belong to a single multicast group, a multicast packet will be replicated only when the paths to the separate hosts diverge. OSPF, a link-state routing protocol, provides a database describing the Autonomous System&#039;s topology. A new OSPF link state advertisement is added describing the location of multicast destinations. A multicast packet&#039;s path is then calculated by building a pruned shortest-path tree rooted at the packet&#039;s IP source. These trees are built on demand, and the results of the calculation are cached for use by subsequent packets....
118|The Trade-offs of Multicast Trees and Algorithms|Multicast trees can be shared across sources (shared trees) or may be source-specific (shortest path trees). Inspired by recent interests in using shared trees for interdomain multicasting, we investigate the trade-offs among shared tree types and source specific shortest path trees, by comparing performance over both individual multicast group and the whole network. The performance is evaluated in terms of path length, link cost, and traffic concentration. We present simulation results over a real network as well as random networks under different circumstances. One practically significant conclusion is that member- or sendercentered trees have good delay and cost properties on average, but they exhibit heavier traffic concentration which makes them inappropriate as the universal form of trees for all types of applications.  Keywords: Multicast, Routing, Scalability, Center Placement Strategy  1 Introduction  Multimedia communication is often multi-point and has contributed to the dem...
119|Internet group management protocol, version 3,&amp;quot; Internet draft|This document is an Internet-Draft and is in full conformance with all provisions of Section 10 of RFC2026. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months and may be updated, replaced, or obsoleted by other documents at any time. It is inappropriate to use Internet- Drafts as reference material or to cite them other than as work in progress. The list of current Internet-Drafts can be accessed at
120|A Comparison of Multicast Trees and Algorithms|. Multicast trees can be shared across  sources or may be source-specific. Inspired by recent  interests in using shared trees for interdomain multicasting  [BFC93] [WLE  +  92], this paper investigates  the trade-offs among different algorithms and tree  types. Because of the dynamic nature of graphs, only  worst case delay bounds can be calculated using analytical  methods. We present simulation results over  random graphs that demonstrate the performance of  these trees, under different circumstances. We evaluate  the performance in terms of path length, link cost,  and traffic concentrations.  Draft submitted to INFOCOM&#039;94  1 Introduction  Point-to-multipoint communications will play a critical role in future computer networks. The problem of computing the optimal multicast path, in the shape of a tree or a group of trees, has many potential solutions; however, to date there have not been systematic comparisons among the different solutions. Today&#039;s multicast applications are prima...
121|Protocol Independent Multicast (PIM): Motivation and Architecture  (1995) |Existing multicast routing mechanisms were intended for use within regions where a group is widely represented or bandwidth is universally plentiful. When group members, and senders to those group members, are distributed sparsely across a wide area, these schemes are not efficient; data packets or membership report information are occasionally sent over many links that do not lead to receivers or senders, respectively. We have developed a multicast routing architecture that efficiently establishes distribution trees across wide area internets, where many groups will be sparsely represented. Efficiency is measured in terms of the state, control message processing, and data packet processing, required across the entire network in order to deliver data packets to the members of the group. Our Protocol Independent Multicast (PIM) architecture: a) maintains the traditional IP multicast service model of receiver-initiated membership; b) can be configured to adapt to different multicast grou...
122|Analysis of a Resequencer Model for Multicast over ATM Networks|Multicast delivery saves bandwidth and offers logical addressing capabilities to the applications. The receivers of a multicast group need to differentiate cells sent by different sources. This demultiplexing requirement can be satisfied in an ATM environment using multiple dedicated point-to-multipoint virtual channel connections (VCs), but with certain shortcomings. This paper discusses an alternative resequencing model to solve this problem. It scales well in large networks. Three resequencing methods are are developed and simulation results reported. The strategy is useful for applications spanning large regions where it is desirable to mix streams of cells from different bursty sources onto the same virtual channel.
123|Towards an Active Network Architecture|Active networks allow their users to inject customized programs into the nodes of the network. An extreme case, in which we are most interested, replaces packets with &#034;capsules&#034; -- program fragments that are executed at each network router/switch they traverse. Active architectures permit a massive increase in the sophistication of the computation that is performed within the network. They will enable new applications, especially those based on application-specific multicast, information fusion, and other services that leverage network-based computation and storage. Furthermore, they will accelerate the pace of innovation by decoupling network services from the underlying hardware and allowing new services to be loaded into the infrastructure on demand. In this paper, we describe our vision of an active network architecture, outline our approach to its design, and survey the technologies that can be brought to bear on its implementation. We propose that the research community mount a j...
124|A logic of authentication|Questions of belief are essential in analyzing protocols for the authentication of principals in distributed computing systems. In this paper we motivate, set out, and exemplify a logic specifically designed for this analysis; we show how various protocols differ subtly with respect to the required initial assumptions of the participants and their final beliefs. Our formalism has enabled us to isolate and express these differences with a precision that was not previously possible. It has drawn attention to features of protocols of which we and their authors were previously unaware, and allowed us to suggest improvements to the protocols. The reasoning about some protocols has been mechanically verified. This paper starts with an informal account of the problem, goes on to explain the formalism to be used, and gives examples of its application to protocols from the literature, both with shared-key cryptography and with public-key cryptography. Some of the examples are chosen because of their practical importance, while others serve to illustrate subtle points of the logic and to explain how we use it. We discuss extensions of the logic motivated by actual practice -- for example, in order to account for the use of hash functions in signatures. The final sections contain a formal semantics of the logic and some conclusions.  
125|Active Messages: a Mechanism for Integrated Communication and Computation|The design challenge for large-scale multiprocessors is (1) to minimize communication overhead, (2) allow communication to overlap computation, and (3) coordinate the two without sacrificing processor cost/performance. We show that existing message passing multiprocessors have unnecessarily high communication costs. Research prototypes of message driven machines demonstrate low communication overhead, but poor processor cost/performance. We introduce a simple communication mechanism, Active Messages, show that it is intrinsic to both architectures, allows cost effective use of the hardware, and offers tremendous flexibility. Implementations on nCUBE/2 and CM-5 are described and evaluated using a split-phase shared-memory extension to C, Split-C. We further show that active messages are sufficient to implement the dynamically scheduled languages for which message driven machines were designed. With this mechanism, latency tolerance becomes a programming/compiling concern. Hardware support for active messages is desirable and we outline a range of enhancements to mainstream processors.  
126|Efficient Software-Based Fault Isolation|One way to provide fault isolation among cooperating software modules is to place each in its own address space. However, for tightly-coupled modules, this solution incurs prohibitive context switch overhead. In this paper, we present a software approach to implementing fault isolation within a single address space. Our approach has two parts. First, we load the code and data for a distrusted module into its own fault domain, a logically separate portion of the application&#039;s address space. Second, we modify the object code of a distrusted module to prevent it from writing or jumping to an address outside its fault domain. Both these software operations are portable and programming language independent. Our approach poses a tradeo relative to hardware fault isolation: substantially faster communication between fault domains, at a cost of slightly increased execution time for distrusted modules. We demonstrate that for frequently communicating modules, implementing fault isolation in software rather than hardware can substantially improve end-to-end application performance.
127|Exokernel: An Operating System Architecture for Application-Level Resource Management|We describe an operating system architecture that securely multiplexes machine resources while permitting an unprecedented degree of application-specific customization of traditional operating system abstractions. By abstracting physical hardware resources, traditional operating systems have significantly limited the performance, flexibility, and functionality of applications. The exokernel architecture removes these limitations by allowing untrusted software to implement traditional operating system abstractions entirely at application-level. We have implemented a prototype exokernel-based system that includes Aegis, an exokernel, and ExOS, an untrusted application-level operating system. Aegis defines the low-level interface to machine resources. Applications can allocate and use machine resources, efficiently handle events, and participate in resource revocation. Measurements show that most primitive Aegis operations are 10–100 times faster than Ultrix,a mature monolithic UNIX operating system. ExOS implements processes, virtual memory, and inter-process communication abstractions entirely within a library. Measurements show that ExOS’s application-level virtual memory and IPC primitives are 5–50 times faster than Ultrix’s primitives. These results demonstrate that the exokernel operating system design is practical and offers an excellent combination of performance and flexibility. 1
128|A Hierarchical Internet Object Cache| This paper discusses the design andperformance  of a hierarchical proxy-cache designed to make Internet information systems scale better. The design was motivated by our earlier trace-driven simulation study of Internet traffic. We believe that the conventional wisdom, that the benefits of hierarchical file caching do not merit the costs, warrants reconsideration in the Internet environment.  The cache implementation supports a highly concurrent stream of requests. We present performance measurements that show that the cache outperforms other popular Internet cache implementations by an order of magnitude under concurrent load. These measurements indicate that hierarchy does not measurably increase access latency. Our software can also be configured as a Web-server accelerator; we present data that our httpd-accelerator is ten times faster than Netscape&#039;s Netsite and NCSA 1.4 servers.  Finally, we relate our experience fitting the cache into the increasingly complex and operational world of Internet information systems, including issues related to security, transparency to cache-unaware clients, and the role of file systems in support of ubiquitous wide-area information systems.  
129|Improving TCP/IP performance over wireless networks|TCP is a reliable transport protocol tuned to perform well in traditional networks made up of links with low bit-error rates. Networks with higher bit-error rates, such as those with wireless links and mobile hosts, violate many of the assumptions made by TCP, causing degraded end-to-end performance. In tbis paper, we describe the design and implementation of a simple protocol, called the snoop protocol, that improves TCP performance in wireless networks. The protocol modifies network-layer software mainly at a base station and preserves end-to-end TCP semantics. The main idea of the protocol is to cache packets at the base station and perform local retransmissions across the wireless link. We have implemented the snoop protocol on a wireless testbed consisting of IBM ThinkPad laptops and i486 base
130|The Case for Geographical Push-Caching|Most existing wide-area caching schemes are client initiated. Decisions on when and where to cache information are made without the benefit of the server&#039;s global knowledge of the situation. We believe that the server should play a role in making these caching decisions, and we propose  geographical push-caching as a way of bringing the server back into the loop. The World Wide Web is an excellent example of a wide-area system that will benefit from geographical push-caching, and we present an architecture that allows a Web server to autonomously replicate HTML pages. 1 Introduction  The World-Wide Web [1] operates for the most part as a cache-less distributed system. When two neighboring clients retrieve a document from the same server, the document is sent twice. This is inefficient, especially considering the ease with which Web browsers allow users to transfer large multimedia documents. To combat this problem, some Web browsers have begun to add local client caches. These prevent ...
131|A dynamic network architecture|Network software is a critical component of any distributed system. Because of its complexity, network software is commonly layered into a hierarchy of protocols, or more generally, into a protocol graph Typical protocol graphs-including those standardized in the IS0 and TCP/IP network architectures-share three important properties: the protocol graph is simple, the nodes of the graph (protocols) encapsulate complex functionality, and the topology of the graph is relatively static. This paper describes a new way to organize network software that differs from conventional architectures in all three of these properties In our approach, the protocol graph is complex, individual protocols encapsulate a single function. and the topology of the graph is dynamic. The main contribution of this paper is to describe the ideas behind our new architec-ture, illustrate the advantages of using the architecture, and demonstrate that the architecture results in efficient network software.
132|Interposition Agents: Transparently Interposing User Code at the System Interface|1.1. Terminology Many contemporary operating systems utilize a system Many contemporary operating systems provide an call interface between the operating system and its clients. interface between user code and the operating system Increasing numbers of systems are providing low-level services based on special &#034;system calls&#034;. One can view mechanisms for intercepting and handling system calls in the system interface as simply a special form of structured user code. Nonetheless, they typically provide no higher- communication channel on which messages are sent, level tools or abstractions for effectively utilizing these allowing such operations as interposing programs that mechanisms. Using them has typically required record or modify the communications that take place on reimplementation of a substantial portion of the system this channel. In this paper, such a program that both uses interface from scratch, making the use of such facilities and provides the system interface will be refe...
133|An Application Level Video Gateway|The current model for multicast transmission of video over the Internet assumes that a fixed average bandwidth is uniformly present throughout the network. Consequently, sources limit their transmission rates to accommodate the lowest bandwidth links, even though high-bandwidth connectivity might be available to many of the participants. We propose an architecture where a video transmission can be decomposed into multiple sessions with different bandwidth requirements using an application-level gateway. Our video gateway transparently connects pairs of sessions into a single logical conference by manipulating the data and control information of the video streams. In particular, the gateway performs bandwidth adaptation through transcoding and rate-control. We describe an efficient algorithm for transcoding Motion-JPEG to H.261 that runs in real-time on standard workstations. By making the Real-time Transport Protocol (RTP) an integral component of our architecture, the video gateway in...
134|Scout: A Communications-Oriented Operating System|This white paper describes Scout, a new operating system being designed for systems connected  to the National Information Infrastructure (NII). Scout provides a communication-oriented  software architecture for building operating system code that is specialized for the different  systems that we expect to be available on the NII. It includes an explicit path abstraction that  both facilitates effective resource management and permits optimizations of the critical path  that I/O data follows. These path-enabled optimizations, along with the application of advanced  compiler techniques, result in a system that has both predictable and scalable performance. 
135|`C: A Language for High-Level, Efficient, and Machine-independent Dynamic Code Generation|Dynamic code generation allows specialized code sequences to be crafted using runtime information. Since this  information is by definition not available statically, the use of dynamic code generation can achieve performance  inherently beyond that of static code generation. Previous attempts to support dynamic code generation have been  low-level, expensive, or machine-dependent. Despite the growing use of dynamic code generation, no mainstream  language provides flexible, portable, and efficient support for it. We describe

136|Distributed Object Management in Thor|Thor is a new object-oriented database management system (OODBMS), intended to be used in heterogeneous distributed systems to allow programs written in different programming languages to share objects in a convenient manner. Thor objects are persistent in spite of failures, are highly likely to be accessible whenever they are needed, and can be structured to reflect the kinds of information of interest to users. Thor combines the advantages of the object-oriented approach with those of database systems: users can store and manipulate objects that capture the semantics of their applications, and can also access objects via queries. Thor is an ongoing project, and this paper is a snapshot: we describe our first design and a partial implementation of that design. This design is primarily concerned with issues related to the implementation of an OODBMS as a distributed system. 1 INTRODUCTION Distributed systems contain different kinds of computers, and users write programs in different...
137|Filter Propagation in Dissemination Trees: Trading off Bandwidth for Processing|We describe the concept of the relocatable continuous media filter. The novelty of these filters is how they can propagate over a dissemination tree in a network. We describe the filter propagation protocol to achieve this. Execution of filters inside a network allows the network to be viewed in a novel way, as a &#039;+processor &#034; with its &#034;&#039;instruction set &#034; being the various types of available filters. Since filters generally modify the data rate of the continuous media stream, usually (but not necessarily) reducing it, filters allow the trading off of bandwidth and processing in a network. 1.
138|Omniware: A Universal Substrate for Mobile Code|In this paper we describe Omniware, a system for producing and executing mobile code. Mobile code will be used in next generation Web applications to specify dynamic behavior in Web pages, implement new Web protocols and data formats, and dynamically distribute computation between servers and browsers. Like all mobile code systems, Omniware provides portability and safety. The same compiled Omniware module can be executed transparently on different machines, and a module&#039;s access to host resources can be precisely controlled. In addition to portability and safety, Omniware has two unique features. First, Omniware is open. The Omniware virtual machine, OmniVM, supports standard programming languages, enabling Web developers to leverage the vast store of existing software and programming expertise. OmniVM was designed to be a straightforward compilation target for a large variety of source languages. Second, Omniware is fast. We evaluate Omniware under the Solaris 2.4 operating system on...
139|Mediators in the architecture of future information systems|The installation of high-speed networks using optical fiber and high bandwidth messsage forwarding gateways is changing the physical capabilities of information systems. These capabilities must be complemented with corresponding software systems advances to obtain a real benefit. Without smart software we will gain access to more data, but not improve access to the type and quality of information needed for decision making. To develop the concepts needed for future information systems we model information processing as an interaction of data and knowledge. This model provides criteria for a high-level functional partitioning. These partitions are mapped into information process-ing modules. The modules are assigned to nodes of the distributed information systems. A central role is assigned to modules that mediate between the users &#039; workstations and data re-sources. Mediators contain the administrative and technical knowledge to create information needed for decision-making. Software which mediates is common today, but the structure, the interfaces, and implementations vary greatly, so that automation of integration is awkward. By formalizing and implementing mediation we establish a partitioned information sys-
140|A theory of communicating sequential processes|  A mathematical model for communicating sequential processes is given, and a number of its interesting and useful properties are stated and proved. The possibilities of nondetermimsm are fully taken into account.
141|A calculus of mobile processes, I|We present the a-calculus, a calculus of communicating systems in which one can naturally express processes which have changing structure. Not only may the component agents of a system be arbitrarily linked, but a communication between neighbours may carry information which changes that linkage. The calculus is an extension of the process algebra CCS, following work by Engberg and Nielsen, who added mobility to CCS while preserving its algebraic properties. The rr-calculus gains simplicity by removing all distinction between variables and constants; communication links are identified by names, and computation is represented purely as the communication of names across links. After an illustrated description of how the n-calculus generalises conventional process algebras in treating mobility, several examples exploiting mobility are given in some detail. The important examples are the encoding into the n-calculus of higher-order functions (the I-calculus and com-binatory algebra), the transmission of processes as values, and the representation of data structures as processes. The paper continues by presenting the algebraic theory of strong bisimilarity and strong equivalence, including a new notion of equivalence indexed by distinctions-i.e., assumptions of inequality among names. These theories are based upon a semantics in terms of a labeled transition system and a notion of strong bisimulation, both of which are expounded in detail in a companion paper. We also report briefly on work-in-progress based upon the corresponding notion of weak bisimulation, in which internal actions cannot be observed.  
142|Symbolic Model Checking: 10^20 States and Beyond|Many different methods have been devised for automatically verifying finite state systems by examining state-graph models of system behavior. These methods all depend on decision procedures that explicitly represent the state space using a list or a table that grows in proportion to the number of states. We describe a general method that represents the state space symbolical/y instead of explicitly. The generality of our method comes from using a dialect of the Mu-Calculus as the primary specification language. We describe a model checking algorithm for Mu-Calculus formulas that uses Bryant’s Binary Decision Diagrams (Bryant, R. E., 1986, IEEE Trans. Comput. C-35) to represent relations and formulas. We then show how our new Mu-Calculus model checking algorithm can be used to derive efficient decision procedures for CTL model checking, satistiability of linear-time temporal logic formulas, strong and weak observational equivalence of finite transition systems, and language containment for finite w-automata. The fixed point computations for each decision procedure are sometimes complex. but can be concisely expressed in the Mu-Calculus. We illustrate the practicality of our approach to symbolic model checking by discussing how it can be used to verify a simple synchronous pipeline circuit.  
144|Petri Nets|Over the last decade, the Petri net has gamed increased usage and acceptance as a basic model of systems of asynchronous concurrent computation. This paper surveys the basic concepts and uses of Petm nets. The structure of Petri nets, their markings and execution, several examples of Petm net models of computer hardware and software, and
145|Regular Types for Active Objects|Previous work on type-theoretic foundations for object-oriented programming languages has mostly focused on applying or extending functional type theory to functional &#034;objects.&#034; This approach, while benefiting from a vast body of existing literature, has the disadvantage of dealing with state change either in a roundabout way or not at all, and completely sidestepping issues of concurrency. In particular, dynamic issues of non-uniform service availability and conformance to protocols are not addressed by functional types. We propose a new type framework that characterizes objects as regular (finite state) processes that provide guarantees of service along public channels. We also propose a new notion of subtyping for active objects, based on Brinksma&#039;s notion of extension, that extends Wegner and Zdonik&#039;s &#034;principle of substitutability&#034; to non-uniform service availability. Finally, we formalize what it means to &#034;satisfy a client&#039;s expectations,&#034; and we show how regular types canbe used...
146|Formulations and Formalisms in  Software Architecture|  Software architecture is the level of software design that addresses the overall structure and properties of software systems. It provides a focus for certain aspects of design and development that are not appropriately addressed within the constituent modules. Architectural design depends heavily on accurate specifications of subsystems and their interactions. These specifications must cover a wide variety of properties, so the specification notations and associated methods must be selected or developed to match the properties of interest. Unfortunately, the available formal methods are only a partial match for architectural needs, which entail description of structure, packaging, environmental assumptions, representation, and performance as well as functionality. A prerequisite for devising or selecting a formal method is sound understanding of what needs to be formalized. For software architecture, much of this understanding is arising through progressive codification, which begins with real-world examples and creates progressively more precise models that eventually support formalization. This paper explores the progressive
147|Partial Orderings of Event Sets and Their Application to Prototyping Concurrent, Timed Systems|Rapide is a concurrent, object-oriented language specifically designed for prototyping large concurrent systems. One of the principle design goals has been to adopt a computation model in which the synchronization, concurrency, dataflow, and timing aspects of a prototype are explicitly represented and easily accessible both to the prototype itself and to the prototyper. This paper describes the partially ordered event set (poset) computation model, and the features of Rapide for using posets in reactive prototypes and for automatically checking posets. An example prototyping scenario illustrates uses of the poset computation model, with and without timing.  keywords: Rapide, partial orders, prototyping, concurrency, real-time, architecture, programming languages. Principle contact: Larry M. Augustin ERL 414, M/C 4055 Computer Systems Laboratory Stanford University Stanford, CA 94305 Tel: (415) 723--9285 Fax: (415) 725--6949 Email: lma@dayton.Stanford.EDU 1  This research was supported ...
148|On agent-based software engineering|Agent-oriented techniques represent an exciting new means of analysing, designing and building complex software systems. They have the potential to significantly improve current practice in software engineering and to extend the range of applications that can feasibly be tackled. Yet, to date, there have been few serious attempts to cast agent systems as a software engineering paradigm. This paper seeks to rectify this omission. Specifically, it will be argued that: (i) the conceptual apparatus of agent-oriented systems is well-suited to building software solutions for complex systems and (ii) agent-oriented approaches represent a genuine advance over the current state of the art for engineering complex systems. Following on from this view, the major issues raised by adopting an agent-oriented approach to software engineering are highlighted and discussed.
149|An axiomatic basis for computer programming|In this paper an attempt is made to explore the logical founda-tions of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This in-volves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantages, both theoretical and prac-tical, may follow from a pursuance of these topics.
150|Intelligent agents: Theory and practice|The concept of an agent has become important in both Artificial Intelligence (AI) and mainstream computer science. Our aim in this paper is to point the reader at what we perceive to be the most important theoretical and practical issues associated with the design and construction of intelligent agents. For convenience, we divide these issues into three areas (though as the reader will see, the divisions are at times somewhat arbitrary). Agent theory is concerned with the question of what an agent is, and the use of mathematical formalisms for representing and reasoning about the properties of agents. Agent architectures can be thought of as software engineering models of agents; researchers in this area are primarily concerned with the problem of designing software or hardware systems that will satisfy the prop-erties specified by agent theorists. Finally, agent languages are software systems for programming and experimenting with agents; these languages may embody principles proposed by theorists. The paper is not intended to serve as a tutorial introduction to all the issues mentioned; we hope instead simply to identify the most important issues, and point to work that elaborates on them. The article includes a short review of current and potential applications of agent technology.
151|Object-oriented Analysis and Design with Applications|My friend, my lover, my wife
152|Design and Synthesis of Synchronization Skeletons Using Branching Time Temporal Logic|We propose a method of constructing concurrent programs in which the synchroni-zation skeleton of the program ~s automatically synthesized from a high-level (branching time) Temporal Logic specification. The synchronization skeleton is an abstraction of the actual program where detail irrelevant to synchronization is
153|BDI Agents: From Theory to Practice|The study of computational agents capable of  rational behaviour has received a great deal of  attention in recent years. Theoretical formalizations  of such agents and their implementations  have proceeded in parallel with little or  no connection between them. This paper explores  a particular type of rational agent, a BeliefDesire  -Intention (BDI) agent. The primary aim  of this paper is to integrate (a) the theoretical  foundations of BDI agents from both a quantitative  decision-theoretic perspective and a symbolic  reasoning perspective; (b) the implementations  of BDI agents from an ideal theoretical perspective  and a more practical perspective; and (c) the  building of large-scale applications based on BDI  agents. In particular, an air-traffic management  application will be described from both a theoretical  and an implementation perspective.  
154|Agent-Oriented Software Engineering|Software and knowledge... In this article, we argue that intelligent agents and agent-based systems offer novel opportunities for developing effective tools and techniques. Following a discussion on the classic subject of what makes software complex, we introduce intelligent agents as software structures capable of making &#034;rational decisions&#034;. Such rational decision-makers are well-suited to the construction of certain types of software, which mainstream software engineering has had little success with. We then go on to examine a number of prototype techniques proposed for engineering agent systems, including formal specification and verification methods for agent systems, and techniques for implementing agent specifications
156|Pitfalls of Agent-Oriented Development|While the theoretical and experimental foundations of agent-based systems are becoming increasingly well understood, comparatively little effort has been devoted to understanding the pragmatics of (multi-)agent systems development --- the everyday reality of carrying out an agent-based development project. As a result, agent system developers are needlessly repeating the same mistakes, with the result that, at best, resources are wasted --- at worst, projects fail. This paper identifies the main pitfalls that await the agent system developer, and where possible, makes tentative recommendations for how these pitfalls can be avoided or rectified. 1 Introduction  It is now more than two decades since Frederick Brooks wrote The Mythical Man-Month --- arguably the best-known and most influential work on software engineering and software project management yet published. In a series of memorable essays, Brooks highlighted some of the most common mistakes made in the software development proc...
157|Modelling and Design of Multi-Agent Systems|Agent technologies are now being applied to the development  of large-scale commercial and industrial software systems. Such systems  are complex, involving hundreds, perhaps thousands of agents, and there  is a pressing need for system modelling techniques that permit their complexity  to be effectively managed, and principled methodologies to guide  the process of system design. Without adequate techniques to support the  design process, such systems will not be sufficiently reliable, maintainable  or extensible, will be difficult to comprehend, and their elements will not  be re-usable. In this paper
158|METATEM: A Framework for Programming in Temporal Logic|In this paper we further develop the methodology of temporal logic as an executable imperative language, presented by Moszkowski [Mos86] and Gabbay [Gab87, Gab89] and present a concrete framework, called METATEM for executing (modal and) temporal logics. Our approach is illustrated by the development of an execution mechanism for a propositional temporal logic and for a restricted first order temporal logic.
159|The Logical Modelling of Computational Multi-Agent Systems|THE aim of this thesis is to investigate logical formalisms for describing, reasoning about, specifying, and perhaps ultimately verifying the properties of systems composed of multiple intelligent computational agents. There are two obvious resources available for this task. The first is the (largely AI) tradition of reasoning about the intentional notions (belief, desire, etc.). The second is the (mainstream computer science) tradition of temporal logics for reasoning about reactive systems. Unfortunately, neither resource is ideally suited to the task: most intentional logics have little to say on the subject of agent architecture, and tend to assume that agents are perfect reasoners, whereas models of concurrent systems from mainstream computer science typically deal with the execution of individual program instructions. This thesis proposes a solution which draws upon both resources. It defines a model of agents and multi-agent systems, and then defines two execution models, which ...
160|Towards a Social Level Characterisation of Socially Responsible Agents|This paper presents a high-level framework for analysing and designing intelligent agents. The framework&#039;s key abstraction mechanism is a new computer level called the  Social Level. The Social Level sits immediately above the Knowledge Level, as defined by Allen Newell, and is concerned with the inherently social aspects of multiple agent systems. To illustrate the working of this framework, an important new class of agent is identified and then specified. Socially responsible agents retain their local autonomy but still draw from, and provide resources to, the larger community. Through empirical evaluation, it is shown that such agents produce both good system-wide performance and good individual performance. 1. INTRODUCTION The number of multi-agent systems being designed and built is rapidly increasing as software agents gain acceptance as a powerful and useful technology for solving complex problems (Chaib-draa, 1995; Jennings, 1994; PAAM, 1996). As applications become more comple...
161|Industrial Applications of Distributed AI|This article argues that a DAI approach can be used to cope with the complexity of industrial applications. DAI techniques are beginning to have a broad impact; the current introduction of these techniques by an ESPRIT project, a Palo Alto consortium, ARPA, Carnegie Mellon University, MCC, and others are good examples. In the near future, other industrial products will emerge from the application of DAI techniques to other domains, including distributed databases, computer-supported cooperative work, and air traffic control. An important advantage of a DAI approach is the ability to integrate existing standalone knowledge-based systems. This factor is important because software for industrial applications is often developed in an ad hoc fashion. Thus, organizations possess a large number of standalone systems developed at different times by different people using different techniques. These systems all operate in the same physical environment, all have expertise that is related but distinct, and all could benefit from cooperation with other such standalone systems
162|An Open Approach to Concurrent Theorem-Proving|The purpose of this paper is twofold. Its main aim is to present an alternative mechanism for representing concurrent theorem-proving activity which primarily relies upon massive parallelism and efficient broadcast communication. A secondary aim is to further motivate this model of concurrent theorem-proving by outlining how the basic model might be utilised in order to provide more dynamic, flexible and adaptable systems. The approach to concurrent theorem-proving we propose is based upon the use of asynchronously executing concurrent objects. Each object contains a particular set of formulae and broadcasts messages corresponding to specific information about those formulae. Based upon the messages that an object receives, it can make simple inferences, transforming the formulae it contains and sending out further messages as a result. Communication can be organised so that, if a formula, distributed across a set of objects, is unsatisfiable then at least one of the objects will event...
163|SEDA: An Architecture for Well-Conditioned, Scalable Internet Services|  We propose a new design for highly concurrent Internet services, whichwe call the staged event-driven architecture (SEDA). SEDA is intended
164|Random Early Detection Gateways for Congestion Avoidance|This paper presents Random Early Detection (RED) gate-ways for congestion avoidance in packet-switched networks. The gateway detects incipient congestion by com-puting the average queue size. The gateway could notify connections of congestion either by dropping packets ar-riving at the gateway or by setting a bit in packet headers. When the average queue size exceeds a preset threshold,the gateway drops or marks each arriving packet with a certain probability, where the exact probability is a func-tion of the average queue size. RED gateways keep the average queue size low while allowing occasional bursts of packets in the queue. During congestion, the probability that the gateway notifies a particular connection to reduce its window is roughly proportional to that connection&#039;s share of the bandwidth throughthe gateway. RED gateways are designed to accompany a transport-layer congestion control protocol such as TCP.The RED gateway has no bias against bursty traffic and avoids the global synchronization of many connectionsdecreasing their window at the same time. Simulations of a TCP/IP network are used to illustrate the performance of RED gateways. 
165|The click modular router| Click is a new software architecture for building flexible and configurable routers. A Click router is assembled from packet processing modules called elements. Individual elements implement simple router functions like packet classification, queueing, scheduling, and interfacing with network devices. A router configuration is a directed graph with elements at the vertices; packets flow along the edges of the graph. Configurations are written in a declarative language that supports user-defined abstractions. This language is both readable by humans and easily manipulated by tools. We present language tools that optimize router configurations and ensure they satisfy simple invariants. Due to Click’s architecture and language, Click router configurations are modular and easy to extend. A standards-compliant Click IP router has sixteen elements on its forwarding path. We present extensions to this router that support dropping policies, fairness among flows, quality-of-service, and
166|Oceanstore: An architecture for global-scale persistent storage|OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development. 1
167|Freenet: A Distributed Anonymous Information Storage and Retrieval System|We describe Freenet, an adaptive peer-to-peer network application  that permits the publication, replication, and retrieval of data  while protecting the anonymity of both authors and readers. Freenet operates  as a network of identical nodes that collectively pool their storage  space to store data files and cooperate to route requests to the most  likely physical location of data. No broadcast search or centralized location  index is employed. Files are referred to in a location-independent  manner, and are dynamically replicated in locations near requestors and  deleted from locations where there is no interest. It is infeasible to discover  the true origin or destination of a file passing through the network,  and difficult for a node operator to determine or be held responsible for  the actual physical contents of her own node.
168|Resource Containers: A New Facility for Resource Management in Server Systems|General-purpose operating systems provide inadequate support for resource management in large-scale servers. Applications lack sufficient control over scheduling and management of machine resources, which makes it difficult to enforce priority policies, and to provide robust and controlled service. There is a fundamental mismatch between the original design assumptions underlying the resource management mechanisms of current general-purpose operating systems, and the behavior of modern server applications. In particular, the operating system’s notions of protection domain and resource principal coincide in the process abstraction. This coincidence prevents a process that manages large numbers of network connections, for example, from properly allocating system resources among those connections. We propose and evaluate a new operating system abstraction called a resource container, which separates the notion of a protection domain from that of a resource principal. Resource containers enable fine-grained resource management in server systems and allow the development of robust servers, with simple and firm control over priority policies. 1
169|Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism|Threads are the vehicle,for concurrency in many approaches to parallel programming. Threads separate the notion of a sequential execution stream from the other aspects of traditional UNIX-like processes, such as address spaces and I/O descriptors. The objective of this separation is to make the expression and control of parallelism sufficiently cheap that the programmer or compiler can exploit even fine-grained parallelism with acceptable overhead. Threads can be supported either by the operating system kernel or by user-level library code in the application address space, but neither approach has been fully satisfactory. This paper addresses this dilemma. First, we argue that the performance of kernel threads is inherently worse than that of user-level threads, rather than this being an artifact of existing implementations; we thus argue that managing par- allelism at the user level is essential to high-performance parallel computing. Next, we argue that the lack of system integration exhibited by user-level threads is a consequence of the lack of kernel support for user-level threads provided by contemporary multiprocessor operating systems; we thus argue that kernel threads or processes, as currently conceived, are the wrong abstraction on which to support user- level management of parallelism. Finally, we describe the design, implementation, and performance of a new kernel interface and user-level thread package that together provide the same functionality as kernel threads without compromis- ing the performance and flexibility advantages of user-level management of parallelism.
170|Extensibility, safety and performance in the SPIN operating system|This paper describes the motivation, architecture and performance of SPIN, an extensible operating system. SPIN provides an extension infrastructure, together with a core set of extensible services, that allow applications to safely change the operating system&#039;s interface and implementation. Extensions allow an application to specialize the underlying operating system in order to achieve a particular level of performance and functionality. SPIN uses language and link-time mechanisms to inexpensively export ne-grained interfaces to operating system services. Extensions are written in a type safe language, and are dynamically linked into the operating system kernel. This approach o ers extensions rapid access to system services, while protecting the operating system code executing within the kernel address space. SPIN and its extensions are written in Modula-3 and run on DEC Alpha workstations. 1
171|Cluster-Based Scalable Network Services|This paper has benefited from the detailed and perceptive comments of our reviewers, especially our shepherd Hank Levy. We thank Randy Katz and Eric Anderson for their detailed readings of early drafts of this paper, and David Culler for his ideas on TACC&#039;s potential as a model for cluster programming. Ken Lutz and Eric Fraser configured and administered the test network on which the TranSend scaling experiments were performed. Cliff Frost of the UC Berkeley Data Communications and Networks Services group allowed us to collect traces on the Berkeley dialup IP network and has worked with us to deploy and promote TranSend within Berkeley. Undergraduate researchers Anthony Polito, Benjamin Ling, and Andrew Huang implemented various parts of TranSend&#039;s user profile database and user interface. Ian Goldberg and David Wagner helped us debug TranSend, especially through their implementation of the rewebber
172|Flash: An efficient and portable Web server|This paper presents the design of a new Web server architecture called the asymmetric multiprocess event-driven (AMPED) architecture, and evaluates the performance of an implementation of this architecture, the Flash Web server. The Flash Web server combines the high performance of single-process event-driven servers on cached workloads with the performance of multi-process and multithreaded servers on disk-bound workloads. Furthermore, the Flash Web server is easily portable since it achieves these results using facilities available in all modern operating systems. The performance of different Web server architectures is evaluated in the context of a single implementation in order to quantify the impact of a server&#039;s concurrency architecture on its performance. Furthermore, the performance of Flash is compared with two widely-used Web servers, Apache and Zeus. Results indicate that Flash can match or exceed the performance of existing Web servers by up to 50 % across a wide range of real workloads. We also present results that show the contribution of various optimizations embedded in Flash.  
173|The Design and Implementation of an Operating System to Support Distributed Multimedia Applications|Support for multimedia applications by general purpose computing platforms has been the subject of considerable research. Much of this work is based on an evolutionary strategy in which small changes to existing systems are made. The approach adopted here is to start ab initio with no backward compatibility constraints. This leads to a novel structure for an operating system. The structure aims to decouple applications from one another and to provide multiplexing of all resources, not just the CPU, at a low level. The motivation for this structure, a design based on the structure, and its implementation on a number of hardware platforms is described. I. Introduction G ENERAL purpose multimedia computing platforms should endow text, images, audio and video with equal status: interpreting an audio or video stream should not be a privileged task of special functions provided by the operating system, but one of ordinary user programs. Support for such processing on a platform on which ot...
174|Making Paths Explicit in the Scout Operating System|This paper makes a case for paths as an explicit abstraction in operating system design. Paths provide a unifying infrastructure for several OS mechanisms that have been introduced in the last several years, including fbufs, integrated layer processing, packet classifiers, code specialization, and migrating threads. This paper articulates the potential advantages of a path-based OS structure, describes the specific path architecture implemented in the Scout OS, and demonstrates the advantages in a particular application domain---receiving, decoding, and displaying MPEG-compressed video. 1 Introduction  Layering is a fundamental structuring technique with a long history in system design. From early work on layered operating systems and network architectures [12, 32], to more recent advances in stackable systems [27, 15, 14, 26], layering has played a central role in managing complexity, isolating failure, and enhancing configurability. This paper describes a complementary, but equally f...
175|A Feedback-driven Proportion Allocator for Real-Rate Scheduling|In this paper we propose changing the decades-old practice of allocating CPU to threads based on priority to a scheme based on proportion and period. Our scheme allocates to each thread a percentage of CPU cycles over a period of time, and uses a feedback-based adaptive scheduler to assign automatically both proportion and period. Applications with known requirements, such as isochronous software devices, can bypass the adaptive scheduler by specifying their desired proportion and/or period. As a result, our scheme provides reservations to applications that need them, and the benefits of proportion and period to those that do not. Adaptive scheduling using proportion and period has several distinct benefits over either fixed or adaptive priority based schemes: finer grain control of allocation, lower variance in the amount of cycles allocated to a thread, and avoidance of accidental priority inversion and starvation, including defense against denial-of-service attacks. This paper descr...
176|Application performance and flexibility on Exokernel systems|The exokernel operating system architecture safely gives untrusted software efficient control over hardware and software resources by separating management from protection. This paper describes an exokernel system that allows specialized applications to achieve high performance without sacrificing the performance of unmodified UNIX programs. It evaluates the exokernel architecture by measuring end-to-end application performance on Xok, an exokernel for Intel x86-based computers, and by comparing Xok’s performance to the performance of two widely-used 4.4BSD UNIX systems (Free-BSD and OpenBSD). The results show that common unmodified UNIX applications can enjoy the benefits of exokernels: applications either perform comparably on Xok/ExOS and the BSD UNIXes, or perform significantly better. In addition, the results show that customized applications can benefit substantially from control over their resources (e.g., a factor of eight for a Web server). This paper also describes insights about the exokernel approach gained through building three different exokernel systems, and presents novel approaches to resource multiplexing. 1
179|On the Duality of Operating System Structures|Because the original of the following paper by Lauer and Needham is not
180|Scalable, Distributed Data Structures for Internet Service Construction|This paper presents a new persistent data management layer designed to simplify cluster-based Internet service construction. This self-managing layer, called a distributed data structure (DDS), presents a conventional single-site data structure interface to service authors, but partitions and replicates the data across a cluster. We have designed and implemented a distributed hash table DDS that has properties necessary for Internet services (incremental scaling of throughput and data capacity, fault tolerance and high availability, high concurrency, consistency, and durability). The hash table uses two-phase commits to present a coherent view of its data across all cluster nodes, allowing any node to service any task. We show that the distributed hash table simplies Internet service construction by decoupling service-specic logic from the complexities of persistent, consistent state management, and by allowing services to inherit the necessary service properties from the DDS rather ...
181|Using Control Theory to Achieve Service Level Objectives In Performance Management|A widely used approach to achieving service level objectives for a target  system (e.g., an email server) is to add a controller that manipulates  the target system&#039;s tuning parameters. We describe a methodology for  designing such controllers for software systems that builds on classical  control theory. The classical approach proceeds in two steps: system  identification and controller design. In system identification, we construct  mathematical models of the target system. Traditionally, this has been  based on a first-principles approach, using detailed knowledge of the target  system. Such models can be di#cult to build, and too complex to validate,  use, and maintain. In our methodology, a statistical (ARMA) model is fit  to historical measurements of the target being controlled. These models  are easier to obtain and use and allow us to apply control-theoretic design  techniques to a larger class of systems. When applied to a Lotus Notes  groupware server, we obtain model fits with R    no lower than 75% and  as high as 98%.
182|Connection Scheduling in Web Servers|Under high loads, a Web server may be servicing manyhundreds of connections concurrently. In traditional
183|Defending against Denial of Service Attacks in Scout|We describe a two-dimensional architecture for defending against denial of service attacks. In one dimension, the architecture accounts for all resources consumed by each I/O path in the system; this accounting mechanism is implemented as an extension to the path object in the Scout operating system. In the second dimension, the various modules that define each path can be configured in separate protection domains; we implement hardware enforced protection domains, although other implementations are possible. The resulting system---which we call Escort---is the first example of a system that simultaneously does end-to-end resource accounting (thereby protecting against resource based denial of service attacks where principals can be identified) and supports multiple protection domains (thereby allowing untrusted modules to be isolated from each other). The paper describes the Escort architecture and its implementation in Scout, and reports a collection of experiments that measure the c...
184|A Prototype Implementation of Archival Intermemory|An Archival Intermemory solves the problem of highly survivable digital data storage in the spirit of the Internet. In this paper we describe a prototype implementation of Intermemory, including an overall system architecture and implementations of key system components. The result is a working Intermemory that tolerates up to 17 simultaneous node failures, and includes a Web gateway for browser-based access to data. Our work demonstrates the basic feasibility of Intermemory and represents significant progress towards a deployable system.
185|Scalable kernel performance for Internet servers under realistic loads|UNIX Internet servers with an event-driven architecture often perform poorly under real workloads, even if they perform well under laboratory benchmarking conditions. We investigated the poor performance of event-driven servers. We found that the delays typical in wide-area networks cause busy servers to manage a large number of simultaneous connections. We also observed that the select system call implementation in most UNIX kernels scales poorly with the number of connections being managed by a process. The UNIX algorithm for allocating file descriptors also scales poorly. These algorithmic problems lead directly to the poor performance of event-driven servers.  We implemented scalable versions of the select system call and the descriptor allocation algorithm. This led to an improvement of up to 58% in Web proxy and Web server throughput, and dramatically improved the scalability of the system.  
186|Using Cohort Scheduling to Enhance Server Performance|A server application is commonly organized as a collection of concurrent threads, each of which executes the code necessary to process a request. This software architecture, which causes frequent control transfers between unrelated pieces of code, decreases instruction and data locality, and consequently reduces the effec- tiveness of hardware mechanisms such as caches, TLBs, and branch predictors. Numerous measurements demonstrate this effect in server applications, which often utilize only a fraction of a modern processor&#039;s computational throughput.
187|Kernel mechanisms for service differentiation in overloaded Web servers|We have implemented these mechanisms in AIX 5.0. Through numerous experiments we demonstrate their effectiveness in achieving the desired degree of service differentiation during overload. We also show that the kernel mechanisms are more efficient and scalable than application level controls implemented in the Web server.
188|ASHs: Application-Specific Handlers for High-Performance Messaging|Application-specific safe message handlers (ASHs) are designed to provide applications with hardware-level network performance. ASHs are user-written code fragments that safely and efficiently execute in the kernel in response to message arrival. ASHs can direct message transfers (thereby eliminating copies) and send messages (thereby reducing send-response latency). In addition, the ASH system provides support for dynamic integrated layer processing (thereby eliminating duplicate message traversals) and dynamic protocol composition (thereby supporting modularity). ASHs provide this high degree of flexibility while still providing network performance as good as, or (if they exploit application-specific knowledge) even better than, hard-wired in-kernel implementations. A combination of user-level microbenchmarks and end-to-end system measurements using TCP demonstrate the benefits of the ASH system.
189|Techniques for Developing and Measuring High-Performance Web Servers over ATM Networks|High-performance Web servers are essential to meet the growing demands of the Internet and large-scale intranets. Satisfying these demands requires a thorough understanding of key factors affecting Web server performance. This paper presents empirical analysis illustrating how dynamic and static adaptivity can enhance Web server performance. Two research contributions support this conclusion.  First, the paper presents results from a comprehensive empirical study of Web servers (such as Apache, Netscape Enterprise, PHTTPD, Zeus, and JAWS) over high-speed ATM networks. This study illustrates their relative performance and precisely pinpoints the server design choices that cause performance bottlenecks. We found that once network and disk I/O overheads are reduced to negligible constant factors, the main determinants of Web server performance are its protocol processing path and concurrency strategy. Moreover, no single strategy performs optimally for all load conditions and traffic type...
190|Scheduling computations on a software-based router|ABSTRACT Recent efforts to add new services to the Internet have increased the interest in software-based routers that are easy to extend and evolve. This paper describes our experiences implementing a software-based router, with a particular focus on the main difficulty we encountered: how to schedule the router&#039;s CPU cycles. The scheduling decision is complicated by the desire to differentiate the level of service for different packet flows, which leads to two fundamental conflicts: (1) assigning processor shares in a way that keeps the processes along the forwarding path in balance while meeting QoS promises, and (2) adjusting the level of batching in a way that minimizes overhead while meeting QoS promises. 1.
191|Virtualization Considered Harmful: OS Design Directions for Well-Conditioned Services|We argue that existing OS designs are ill-suited for the needs of Internet service applications. These applications demand massive concurrency (supporting a large number of requests per second) and must be well-conditioned to load (avoiding degradation of performance and predictability when demand exceeds capacity). The transparency and virtualization provided by existing operating systems leads to limited concurrency and lack of control over resource usage. We claim that Internet services would be far better supported by operating systems by reconsidering the role of resource virtualization. We propose a new design for server applications, the staged event-driven architecture (SEDA). In SEDA, applications are constructed as a set of eventdriven  stages separated by queues. We present the SEDA architecture and its consequences for operating system design. 1. 
192|The 1999 Southern California seismic network bulletin|together with the Caltech Seismological Laboratory, perates a network of more than 350 remote seismometers in southern California called the Southern California Seismic Network
193|Distributed Database Systems |this article, we discuss the fundamentals of distributed DBMS technology. We address the data distribution and architectural design issues as well as the algorithms that need to be implemented to provide the basic DBMS functions such as query processing, concurrency control, reliability, and replication control.
194|A Delay-Tolerant Network Architecture for Challenged Internets|The highly successful architecture and protocols of today’s Internet may operate poorly in environments characterized by very long delay paths and frequent network partitions. These problems are exacerbated by end nodes with limited power or memory resources. Often deployed in mobile and extreme environments lacking continuous connectivity, many such networks have their own specialized protocols, and do not utilize IP. To achieve interoperability between them, we propose a network architecture and application interface structured around optionally-reliable asynchronous message forwarding, with limited expectations of end-to-end connectivity and node resources. The architecture operates as an overlay above the transport layers of the networks it interconnects, and provides key services such as in-network data storage and retransmission, interoperable naming, authenticated forwarding and a coarse-grained class of service.
195|System architecture directions for networked sensors|Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world or spread throughout our environment. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances. 
196|Energy-Efficient Computing for Wildlife Tracking: Design Tradeoffs and Early Experiences with ZebraNet|Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The  eld of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scienti  c and commercial purposes. This paper examines the research decisions and design tradeos that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.
197|The design and implementation of an intentional naming system|This paper presents the design and implementation of the Intentional Naming System (INS), a resource discovery and service location system for dynamic and mobile networks of devices and computers. Such environments require a naming system that is (i) expressive, to describe and make requests based on specific properties of services, (ii) responsive, to track changes due to mobility and performance, (iii) robust, to handle failures, and (iv) easily configurable. INS uses a simple language based on attributes and values for its names. Applications use the language to describe what they are looking for (i.e., their intent), not where to find things (i.e., not hostnames). INS implements a late binding mechanism that integrates name resolution and message routing, enabling clients to continue communicating with end-nodes even if the name-to-address mappings change while a session is in progress. INS resolvers self-configure to form an application-level overlay network, which they use to discover new services, perform late binding, and maintain weak consistency of names using soft-state name exchanges and updates. We analyze the performance of the INS algorithms and protocols, present measurements of a Java-based implementation, and describe three applications we have implemented that demonstrate the feasibility and utility of INS.
198|Data mules: Modeling a three-tier architecture for sparse sensor networks|Abstract — This paper presents and analyzes an architecture that exploits the serendipitous movement of mobile agents in an environment to collect sensor data in sparse sensor networks. The mobile entities, called MULEs, pick up data from sensors when in close range, buffer it, and drop off the data to wired access points when in proximity. This leads to substantial power savings at the sensors as they only have to transmit over a short range. Detailed performance analysis is presented based on a simple model of the system incorporating key system variables such as number of MULEs, sensors and access points. The performance metrics observed are the data success rate (the fraction of generated data that reaches the access points) and the required buffer capacities on the sensors and the MULEs. The modeling along with simulation results can be used for further analysis and provide certain guidelines for deployment of such systems. I.
199|Epidemic routing for partially-connected ad hoc networks|Mobile ad hoc routing protocols allow nodes with wireless adaptors to communicate with one another without any pre-existing network infrastructure. Existing ad hoc routing protocols, while robust to rapidly changing network topology, assume the presence of a connected path from source to destination. Given power limitations, the advent of short-range wireless networks, and the wide physical conditions over which ad hoc networks must be deployed, in some scenarios it is likely that this assumption is invalid. In this work, we develop techniques to deliver messages in the case where there is never a connected path from source to destination or when a network partition exists at the time a message is originated. To this end, we introduce Epidemic Routing, where random pair-wise exchanges of messages among mobile hosts ensure eventual message delivery. The goals of Epidemic Routing are to: i) maximize message delivery rate, ii) minimize message latency, and iii) minimize the total resources consumed in message delivery. Through an implementation in the Monarch simulator, we show that Epidemic Routing achieves eventual delivery of 100 % of messages with reasonable aggregate resource consumption in a number of interesting scenarios. 1
200|Building Efficient Wireless Sensor Networks with Low-Level Naming|In most distributed systems, naming of nodes for low-level communication leverages topological location (such as node addresses)  and is independent of any application. In this paper, we investigate an emerging class of distributed systems where low-level communication does not rely on network topological location. Rather, low-level communication is based on attributes that are external to the network topology and relevant to the application. When combined with dense deployment of nodes, this kind of named data enables in-network processing for data aggregation, collaborative signal processing, and similar problems. These approaches are essential for emerging applications such as sensor networks where resources such as bandwidth and energy are limited. This paper is the first description of the software architecture that supports  named data and in-network processing in an operational, multi-application sensor-network. We show that approaches such as in-network aggregation and nested queries can significantly affect network traffic. In one experiment aggregation reduces traffic by up to 42% and nested queries reduce loss rates by 30%. Although aggregation has been previously studied in simulation, this paper demonstrates nested queries as another form of in-network processing, and it presents the first evaluation of these approaches over an operational testbed.  
201|A Protocol for Packet Network Intercommunication|A protocol that supports the sharing of resources that exists in different packet switching networks is presented. The protocol provides for variation in individual network packet sizes, transmission failures, sequencing, flow control, end-to-end error checking, and the creation and destruction of logical process-to-process connections. Some implementation issues are considered, and problems such as internetwork routing, accounting, and timeouts are exposed.
202|Low power distributed MAC for ad hoc sensor radio networks|Abstract- Targeting at multi-hop wireless sensor networks, a set of low power MAC design principles have been proposed, and a novel ultra-low power MAC is designed to be distributed in nature to support scalability, survivability and adaptability requirements. Simple CSMA and spread spectrum technique are combined to trade off bandwidth and power efficiency. A distributed algorithm is used to do dynamic channel assignment. A novel wake-up radio scheme is incorporated to take advantage of new radio technologies. The notion of mobility awareness is introduced into an adaptive protocol to reduce network maintenance overhead. The resulted protocol shows much higher power efficiency for typical sensor network applications.
203|A Linear Programming Formulation of Flows over Time with Piecewise Constant Capacity and Transit Times| We present an algorithm to solve a deterministic form of a routing problem in delay tolerant networking, in which contact possibilities are known in advance. The algorithm starts with a finite set of contacts with time-varying capacities and transit delays. The output is an optimal schedule assigning messages to edges and times, that respects message priority and minimizes the overall delivery delay. The algorithm consists of two main ingredients: a discretization step in which the raw data provided by the contacts is used to obtain appropriate subdivisions of the relevant time intervals, and a linear program, a dynamic version of the classical multicommodity flow problem, in which transit times are piecewise constant, and where both edges and nodes are capacitated (in the case of edges, with piecewise constant capacities). In fact, we present two equivalent LP formulations, of which one is smaller and runs faster in CPLEX, a general purpose linear solver.  
205|Latency-Aware Information Access with User-Directed Fetch Behaviour for Weakly-Connected Mobile Wireless Clients”, BBN|Abstract – Mobile wireless clients have highly variable network connectivity and available bandwidth. There are times when they may be completely disconnected from the larger Internet. This dynamism of connectivity poses unique constraints for the problem of information access in general, and Web access in particular. These and other factors (such as loaded servers and congested networks) contribute to unpredictably high response times in content retrieval. We examine the problem of improving the utility of information access applications for these imperfectly connected devices. In particular, we are concerned with three related problems: 1. Providing information to the user on the estimated response time to retrieve content, the freshness of cached content, and the status on the strength of connection to the
207|Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers  (1994) |An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize adhoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.  
208|A Performance Comparison of Multi-Hop Wireless Ad Hoc Network Routing Protocols|An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Due to the limited transmission range of wireless network interfaces, multiple network &amp;quot;hops &amp;quot; may be needed for one node to exchange data with another across the network. In recent years, a variety of new routing protocols targeted specifically at this environment have been developed, but little performance information on each protocol and no realistic performance comparison between them is available. This paper presents the results of a detailed packet-level simulation comparing four multi-hop wireless ad hoc network routing protocols that cover a range of design choices: DSDV, TORA, DSR, and AODV. We have extended the ns-2 network simulator to accurately model the MAC and physical-layer behavior of the IEEE 802.11 wireless LAN standard, including a realistic wireless transmission channel model, and present the results of simulations of networks of 50 mobile nodes. 1
209|Next century challenges: Scalable coordination in sensor networks|Networked sensors-those that coordinate amongst them-selves to achieve a larger sensing task-will revolutionize information gathering and processing both in urban envi-ronments and in inhospitable terrain. The sheer numbers of these sensors and the expected dynamics in these environ-ments present unique challenges in the design of unattended autonomous sensor networks. These challenges lead us to hypothesize that sensor network coordination applications may need to be structured differently from traditional net-work applications. In particular, we believe that localized algorithms (in which simple local node behavior achieves a desired global objective) may be necessary for sensor net-work coordination. In this paper, we describe localized al-gorithms, and then discuss directed diffusion, a simple com-munication model for describing localized algorithms. 1
210|A comparison of mechanisms for improving TCP performance over wireless links|Reliable transport protocols such as TCP are tuned to perform well in traditional networks where packet losses occur mostly because of congestion. However, networks with wireless and other lossy links also suffer from significant losses due to bit errors and handoffs. TCP responds to all losses by invoking congestion control and avoidance algorithms, resulting in degraded end-to-end performance in wireless and lossy systems. In this paper, we compare several schemes designed to improve the performance of TCP in such networks. We classify these schemes into three broad categories: end-to-end protocols, where loss recovery is performed by the sender; link-layer protocols, that provide local reliability; and split-connection protocols, that break the end-to-end connection into two parts at the base station. We present the results of several experiments performed in both LAN and WAN environments, using throughput and goodput as the metrics for comparison. Our results show that a reliable link-layer protocol that is TCP-aware provides very good performance. Furthermore, it is possible to achieve good performance without splitting the end-to-end connection at the base station. We also demonstrate that selective acknowledgments and explicit loss notifications result in significant performance improvements.
211|Power-Aware Routing in Mobile Ad Hoc Networks|In this paper we present a case for using new power-aware metrics for determining routes in wireless  ad hoc networks. We present five different metrics based on battery power consumption at nodes. We  show that using these metrics in a shortest-cost routing algorithm reduces the cost/packet of routing  packets by 5-30% over shortest-hop routing (this cost reduction is on top of a 40-70% reduction in energy  consumption obtained by using PAMAS, our MAC layer protocol). Furthermore, using these new metrics  ensures that the mean time to node failure is increased significantly. An interesting property of using  shortest-cost routing is that packet delays do not increase. Finally, we note that our new metrics can  be used in most traditional routing protocols for ad hoc networks.  1 
212|Adaptive clustering for mobile wireless networks|This paper describes a self-organizing, multihop, mobile radio network, which relies on a code division access scheme for multimedia support. In the proposed network architecture, nodes are organized into nonoverlapping clusters. The clusters are independently controlled and are dynamically reconfigured as nodes move. This network architecture has three main advantages. First, it provides spatial reuse of the bandwidth due to node clustering. Secondly, bandwidth can be shared or reserved in a controlled fashion in each cluster. Finally, the cluster algorithm is robust in the face of topological changes caused by node motion, node failure and node insertion/removal. Simulation shows that this architecture provides an efficient, stable infrastructure for the integration of different types of traffic in a dynamic radio network. 1.
213|Videoconferencing on the Internet|This paper describes the INRIA Videoconferencing System (IVS), a low bandwidth tool for real-time video between workstations on the Internet using UDP datagrams and the IP multicast extension. The video coder-decoder (codec) is a software implementation of the UIT-T recommendation H.261 originally developed for the Integrated Services Digital Network (ISDN). Our focus in this paper is on adapting this codec for the Internet environment. We propose a packetization scheme, an error control scheme and an output rate control scheme that adapts the image coding process based on network conditions. This work shows that it is possible to maintain videoconferences with reasonable quality across packet-switched networks without requiring special support from the network such as resource reservation or admission control.
214|A mobility-based framework for adaptive clustering in wireless ad hoc networks|Abstract—This paper presents a novel framework for dynamically organizing mobile nodes in wireless ad hoc networks into clusters in which the probability of path availability can be bounded. The purpose of the ( ; t) cluster is to help minimize the far-reaching effects of topological changes while balancing the need to support more optimal routing. A mobility model for ad hoc networks is developed and is used to derive expressions for the probability of path availability as a function of time. It is shown how this model provides the basis for dynamically grouping nodes into clusters using an efficient distributed clustering algorithm. Since the criteria for cluster organization depends directly upon path availability, the structure of the cluster topology is adaptive with respect to node mobility. Consequently, this framework supports an adaptive hybrid routing architecture that can be more responsive and effective when mobility rates are low and more efficient when mobility rates are high. Index Terms—Ad hoc networks, dynamic clustering, hierarchical routing, mobile computing, mobility models, routing algorithms, wireless networks. I.
216|Increasing Network Throughput by Integrating Protocol Layers|Integrating protocol data manipulations is a strategy for increasing the throughput of network protocols. The idea is to combine a series of protocol layers into a pipeline so as to access message data more efficiently. This paper introduces a widely-applicable technique for integrating protocols. This technique not only improves performance, but also preserves the modularity of protocol layers by automatically integrating independently expressed protocols. The paper also describes a prototype integration tool, and studies the performance limits and scalability of protocol integration. Department of Computer Science The University of Arizona Tucson, AZ 85721 1 This research was sponsored in part by DARPA Contract DABT63-91-C-0030. 1 Introduction Data manipulation---e.g., encryption, presentation formatting, compression, computing checksums---is one of the costliest aspects of data transfer [3, 4, 6]. This is because reading, and possibly writing, each byte of data in a message invo...
217|Exact and Approximation Algorithms for Clustering|In this paper we present a n O(k1?1=d) time algorithm for solving the k-center problem in R d, under L1 and L2 metrics. The algorithm extends to other metrics, and can be used to solve the discrete k-center problem, as well. We also describe a simple (1 +)-approximation algorithm for the k-center problem, with running time O(n log k) + (k = ) O(k1?1=d). Finally, we present a n O(k1?1=d) time algorithm for solving the L-capacitated k-center problem, provided that L = (n=k 1?1=d) or L = O(1). We conclude with a simple approximation algorithm for the L-capacitated k-center problem.
218|Design Considerations for Distributed Microsensor Systems|Wireless distributed microsensor systems will enable the reliable monitoring and control of a variety of applications that range from medical and home security to machine diagnosis, chemical/biological detection and other military applications. The sensors have to be designed in a highly integrated fashion, optimizing across all levels of system abstraction, with the goal of minimizing energy dissipation. This paper addresses some of the key design considerations for future microsensor systems including the network protocols required for collaborative sensing and information distribution, system partitioning considering computation and communication costs, low energy electronics, power system design and energy harvesting techniques.  1. Introduction  Over the last few years, the design of micropower wireless sensor systems has gained increasing importance for a variety of civil and military applications. The Low Power Wireless Integrated Microsensors (LWIM) project has made major advan...
219|Clustering with power control|Abstract – This paper proposes a stable, dynamic, distributed clustering for energy efficient networking. Via simulation, we evaluate the impacts of mobility and transmission power variation on network stability. 1.
220|A comparison of MAC protocols for wireless local networks based on battery power consumption|Abstract- Energy efficiency is an important issue in mobile wireless networks since the battery life of mobile terminals is limited. Conservation of battery power has been addressed using many techniques. This paper addresses energy efficiency in medium access control (MAC) protocols for wireless networks. The paper develops a framework to study the energy consumption of a MAC protocol from the transceiver usage perspective. This framework is then applied to compare the performance of a set of protocols that includes IEEE 802.11, EC-MAC, PRMA, MDR-TDMA, and DQRUMA a. The performance metrics considered are transmitter and receiver usage times for packet transmission and reception. The analysis here shows that protocols that aim to reduce the number of contentions perform better from a energy consumption perspective. The receiver usage time, however, tends to be higher for protocols that require the mobile to sense the medium before attempting transmission. 1
221|Embedded Computation Meets the World-Wide-Web|Two important trends are converging to bring about a radical transformation in the operation of our world. First, the computer industry&#039;s remarkable ability to squeeze more and more transistors into a smaller and smaller area of silicon is increasing the computational abilities of our devices, while simultaneously decreasing their cost and power consumption. Second, the proliferation of wired and wireless networking spurred by the development of the world-wide web and demands for mobile access are enabling low-cost connectivity among computing devices. It is now possible to connect not only our desktop machines, but every computing device into a true worldwide web that connects the physical world of sensors and actuators to the virtual world of our information utilities and services. What amazing new applications and services will result? How will ubiquitous computation affect our everyday lives? Will the long envisioned invisible computing paradigm finally be possible? This paper expl...
222|Energy-Scalable algorithms and Protocols for wireless Microsensor Networks|Wireless microsensor networks lend themselves to trade-offs in energy and quality. In these networks, the individual sensor data per se is not necessarily important to the end user. Rather, it is the combined knowledge of all the sensors that describes what is occurring in the environment. By allowing the algorithms and protocols to adapt the quality of this description, with a corresponding change in energy dissipation, sensor networks can be flexible to the end-user’s requirements. In this paper, we provide models for predicting quality and energy and show the advantages of trading off these two parameters. By ensuring that the system operates at a minimum energy for each quality point, the system can achieve both flexibility and energy efficiency, allowing the end-user to maximize system lifetime. 1.
223|Dynamic Voltage Scaling Techniques for Distributed Microsensor Networks|Distributed microsensor networks promise a versatile and robust platform for remote environment monitoring. Crucial to long system lifetimes for these microsensors are algorithms and protocols that provide the option of trading quality for energy savings. Dynamic voltage scaling on the sensor node&#039;s processor enables energy savings from these scalable algorithms. We demonstrate dynamic voltage scaling on the beginnings of a sensor node prototype, which currently consists of a commercial processor, a digitally adjustable DC-DC regulator, and a power-aware operating system.  1. Introduction  Distributed microsensor networks are emerging as a compelling new hardware platform for remote environment monitoring [1]. Researchers are considering a range of applications including remote climate monitoring, battlefield surveillance, and intra-machine monitoring [2]. A distributed microsensor network consists of many small, expendable, battery-powered wireless nodes. Once the nodes are deployed t...
224|Energy-Scalable Protocols for Battery-Operated MicroSensor Networks|To maximize battery lifetimes of distributed wireless sensors,  network protocols and data fusion algorithms should be designed with low  power techniques. Network protocols minimize energy by using localized communication  and control and by exploiting computation/communication tradeoffs.  In addition, data fusion algorithms such as beamforming aggregate data  from multiple sources to reduce data redundancy and enhance signal-to-noise  ratios, thus further reducing the required communications. We have developed  a sensor network system that uses a localized clustering protocol and beamforming  data fusion to enable energy-efficient collaboration. We have implemented  two beamforming algorithms, the Maximum Power and the Least  Mean Squares (LMS) beamforming algorithms, on the StrongARM (SA-1100)  processor. Results from our experiments show that the LMS algorithm  requires less than one-fifth the energy required by the Maximum Power beamforming  algorithm with onlya3dBloss in performa...
225|Bluetooth - A New Low-Power Radio Interface Providing Short-Range Connectivity|this paper, we review the Bluetooth technology, a new universal radio interface enabling electronic devices to connect and communicate wirelessly via short-range connections. Motivations for the air interface design and radio requirement decisions are discussed. Frequency hopping, interference resistance, and the concepts of ad hoc connectivity and scatternets are explained in detail. Furthermore, Bluetooth characteristics enabling low-cost single-chip implementations and supporting low power consumption are discussed
226|Software radio architecture with smart antennas: a tutorial on algorithms and complexity|Abstract — Recently, there has been considerable interest in using antenna arrays in wireless communication networks to increase the capacity and decrease the cochannel interference. Adaptive beamforming with smart antennas at the receiver increases the carrier-to-interference ratio (CIR) in a wireless link. This paper considers a wireless network with beamforming capabilities at the receiver which allows two or more transmitters to share the same channel to communicate with the base station. The concrete computational complexity and algorithm structure of a base station are considered in terms of a software radio system model, initially with an omnidirectional antenna. The software radio computational model is then expanded to characterize a network with smart antennas. The application of the software radio smart antenna is demonstrated through two examples. First, traffic improvement in a network with a smart antenna is considered, and the implementation of a hand-off algorithm in the software radio is presented. The blocking probabilities of the calls and total carried traffic in the system under different traffic policies are derived. The analytical and numerical results show that adaptive beamforming at the receiver reduces the probability of blocking and forced termination of the calls and increases the total carried traffic in the system. Then, a joint beamforming and power control algorithm is implemented in a software radio smart antenna in a CDMA network. This shows that, by using smart antennas, each user can transmit with much lower power, and therefore the system capacity increases significantly. Index Terms—Adaptive beamforming, handoff, power control, smart antennas, software radio. I.
227|Low power systems for wireless microsensors|Abstract-- Low power wireless sensor networks provide a new monitoring and control capability for civil and military applications in transportation, manufacturing, biomedical, environmental management, and safety and security systems. Wireless microsensor network nodes, operating at average and peak power levels constrained by compact power sources, offer a range of important challenges for low power methods. This paper reports advances in low power systems spanning network design, through power management, low power mixed signal circuits, and highly integrated RF network interfaces. Particular attention is focused on methods for low power RF receiver systems. I.
228|The design and implementation of HomeRF: A radio frequency wireless networking standard for the connected home|of over 100 companies from the computer, telecommunications, and consumer electronics industries. This group has developed an open specification called the Shared Wireless Access Protocol- Cordless Access (SWAP-CA) that enables radio frequency (RF) wireless connectivity between a diverse set of devices and computing resources in and around a typical home. Built around an RF spectrum with worldwide availability, SWAP-CA includes operational support for both managed and ad hoc network of devices. It combines and extends wireless networking and cordless telephony into a single unified protocol allowing mobile devices to communicate via both voice and data traffic simultaneously over the Internet and/or over the Public Switched Telephone Network (PSTN). For battery-operated devices it includes a power management mechanism that ensures connection longevity. The technology has been specifically optimized for consumer applications and price points and consequently the HomeRF WG has the broad backing of the major corporate stakeholders interested in enabling tether less networking within the home.
229|Design and implementation of a scalable encryption processor with embedded variable dc/dc converter|This work describes the design and implementation of an energy-efficient, scalable encryption processor that utilizes variable voltage supply techniques and a highefficiency embedded variable output DC/DC converter. The resulting implementation dissipates 134nJ/bit @ V DD = 2.5V, when encrypting at its maximum rate of 1Mb/s using a maximum datapath width of 512 bits. The embedded converter achieves an efficiency of 96 % at this peak load. The processor is 2-3 orders of magnitude more energy efficient than optimized assembly code running on a low-power processor such as the StrongARM. 2.
230|The H.263+ Video Coding Standard: Complexity and Performance|The ITU-T H.263+ low bit-rate video coding standard is Version 2 of the draft international standard ITU-T H.263. Currently, we are a contributing party in the H.263+ standardization effort. In this paper, we discuss this emerging video coding standard and present compression performance results based on our public domain implementation of H.263+.  
231|The Structure-Mapping Engine: Algorithm and Examples|This paper describes the Structure-Mapping Engine (SME), a program for studying analogical processing. SME has been built to explore Gentner&#039;s Structure-mapping theory of analogy, and provides a &#034;tool kit&#034; for constructing matching algorithms consistent with this theory. Its flexibility enhances cognitive simulation studies by simplifying experimentation. Furthermore, SME is very efficient, making it a useful component in machine learning systems as well. We review the Structure-mapping theory and describe the design of the engine. We analyze the complexity of the algorithm, and demonstrate that most of the steps are polynomial, typically bounded by O (N 2 ). Next we demonstrate some examples of its operation taken from our cognitive simulation studies and work in machine learning. Finally, we compare SME to other analogy programs and discuss several areas for future work. This paper appeared in Artificial Intelligence, 41, 1989, pp 1-63. For more information, please contact forbu...
232|Transactional Memory: Architectural Support for Lock-Free Data Structures |A shared data structure is lock-free if its operations do not require mutual exclusion. If one process is interrupted in the middle of an operation, other processes will not be prevented from operating on that object. In highly concurrent systems, lock-free data structures avoid common problems associated with conventional locking techniques, including priority inversion, convoying, and difficulty of avoiding deadlock. This paper introduces transactional memory, a new multiprocessor architecture intended to make lock-free synchronization as efficient (and easy to use) as conventional techniques based on mutual exclusion. Transactional memory allows programmers to define customized read-modify-write operations that apply to multiple, independently-chosen words of memory. It is implemented by straightforward extensions to any multiprocessor cache-coherence protocol. Simulation results show that transactional memory matches or outperforms the best known locking techniques for simple benchmarks, even in the absence of priority inversion, convoying, and deadlock.  
234|Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors|Scalable shared-memory multiprocessors distribute memory among the processors and use scalable interconnection networks to provide high bandwidth and low latency communication. In addition, memory accesses are cached, buffered, and pipelined to bridge the gap between the slow shared memory and the fast processors. Unless carefully controlled, such architectural optimizations can cause memory accesses to be executed in an order different from what the programmer expects. The set of allowable memory access orderings forms the memory consistency model or event ordering model for an architecture.
235|Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors|Busy-wait techniques are heavily used for mutual exclusion and barrier synchronization in shared-memory parallel programs. Unfortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnect contention, introducing performance bottlenecks that become markedly more pronounced as applications scale. We argue that this problem is not fundamental, and that one can in fact construct busy-wait synchronization algorithms that induce no memory or interconnect contention. The key to these algorithms is for every processor to spin on separate locally-accessible ag variables, and for some other processor to terminate the spin with a single remote write operation at an appropriate time. Flag variables may be locally-accessible as a result of coherent caching, or by virtue of allocation in the local portion of physically distributed shared memory. We present a new scalable algorithm for spin locks that generates O(1) remote references per lock acquisition, independent of the number of processors attempting to acquire the lock. Our algorithm provides reasonable latency in the absence of contention, requires only a constant amount of space per lock, and requires no hardware support other than
236|Distributed packet switching for local computer networks|Abstract: Ethernet is a branching broadcast communication system for carrying digital data packets among locally distributed computing stations. The packet transport mechanism provided by Ethernet has been used to build systems which can be viewed as either local computer networks or loosely coupled multiprocessors. An Ethernet&#039;s shared communication facility, its Ether, is a passive broadcast medium with no central control. Coordination of access to the Ether for packet broadcasts is distributed among the contending transmitting stations using controlled statistical arbitration. Switching of packets to their destinations on the Ether is distributed among the receiving stations using packet address recognition. Design principles and implementation are described based on experience.with an operating Ethernet of1 00 nodes along a kilometer of coaxial cable. A model for estimating performance under heavy loads and a packet protocol for error-controlled communication are included for completeness.
237|A Methodology for Implementing Highly Concurrent Data Objects| A concurrent object is a data structure shared by concurrent processes. Conventional techniques for implementing concurrent objects typically rely on critical sections: ensuring that only one process at a time can operate on the object. Nevertheless, critical sections are poorly suited for asynchronous systems: if one process is halted or delayed in a critical section, other, nonfaulty processes will be unable to progress. By contrast, a concurrent object implementation is lock free if it always guarantees that some process will complete an operation in a finite number of steps, and it is wait free if it guarantees that each process will complete an operation in a finite number of steps. This paper proposes a new methodology for constructing lock-free and wait-free implementations of concurrent objects. The object’s representation and operations are written as stylized sequential programs, with no explicit synchronization. Each sequential operation is automatically transformed into a lock-free or wait-free operation using novel synchronization and memory management algorithms. These algorithms are presented for a multiple instruction/multiple data (MIMD) architecture in which n processes communicate by applying atomic read, wrzte, load_linked, and store_conditional operations to a shared memory.
238|Memory access buffering in multiprocessors|In highly-pipelined machines, instructions and data are prefetched and buffered in both the processor and the cache. This is done to reduce the average memory access la-tency and to take advantage of memory interleaving. Lock-up free caches are designed to avoid processor blocking on a cache miss. Write buffers are often included in a pipelined machine to avoid processor waiting on writes. In a shared memory multiprocessor, there are more advantages in buffering memory requests, since each memory access has to traverse the memory- processor interconnection and has to compete with memory requests issued by different processors. Buffering, however, can cause logical problems in multipro-cessors. These problems are aggravated if each processor has a private memory in which shared writable data may be present, such as in a cache-based system or in a system with a distributed global memory. In this paper, we analyze the benefits and problems associated with the buffering of memory requests in shared memory multiprocessors. We show that the logical problem of buffering is directly related to the problem of synchronization. A simple model is presented to evaluate the performance improvement result-ing from buffering. 1.
239|LimitLESS Directories: A Scalable Cache Coherence Scheme|Caches enhance the performance of multiprocessors by reducing network tra#c and average memory access latency. However, cache-based systems must address the problem of cache coherence. We propose the LimitLESS directory protocol to solve this problem. The LimitLESS scheme uses a combination of hardware and software techniques to realize the performance of a full-map directory with the memory overhead of a limited directory. This protocol is supported by Alewife, a large-scale multiprocessor. We describe the architectural interfaces needed to implement the LimitLESS directory, and evaluate its performance through simulations of the Alewife machine.
240|Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors |The memory consistency model supported by a multiprocessor architecture determines the amount of buffering and pipelining that may be used to hide or reduce the latency of memory accesses. Several different consistency models have been proposed. These range from sequential consistency on one end, allowing very limited buffering, to release consistency on the other end, allowing extensive buffering and pipelining. The processor consistency and weak consistency models fall in between. The advantage of the less strict models is increased performance potential. The disadvantage is increased hardware complexity and a more complex programming model. To make an informed decision on the above tradeoff requires performance data for the various models. This paper addresses the issue of performance benefits from the above four consistency models. Our results are based on simulation studies done for three applications. The results show that in an environment where processor reads are blocking and writes are buffered, a significant performance increase is achieved from allowing reads to bypass previous writes. Pipelining of writes, which determines the rate at which writes are retired from the write buffer, is of secondary importance. As a result, we show that the sequential consistency model performs poorly relative to all other models, while the processor consistency model provides most of the benefits of the weak and release consistency models. 
241|The Expandable Split Window Paradigm for Exploiting Fine-Grain Parallelism|We propose a new processing paradigm, called the Expandable Split Window (ESW) paradigm, for exploiting finegrain parallelism. This paradigm considers a window of instructions (possibly having dependencies) as a single unit, and exploits fine-grain parallelism by overlapping the execution of multiple windows. The basic idea is to connect multiple sequential processors, in a decoupled and decentralized manner, to achieve overall multiple issue. This processing paradigm shares a number of properties of the restricted dataflow machines, but was derived from the sequential von Neumann architecture. We also present an implementation of the Expandable Split Window execution model, and preliminary performance results. 1.
242|A Lock-Free Multiprocessor OS Kernel|Typical shared-memory multiprocessor OS kernels use interlocking, implemented as spinlocks or waiting semaphores. We have implemented a complete multiprocessor OS kernel (including threads, virtual memory, and I/O including a window system and a file system) using only lock-free synchronization methods based on Compare-and-Swap. Lock-free synchronization avoids many serious problems caused by locks: considerable overhead, concurrency bottlenecks, deadlocks, and priority inversion in real-time scheduling. Measured numbers show the low overhead of our implementation, competitive with user-level thread management systems.  Contents  1 Introduction 1 2 Synchronization in OS Kernels 1  2.1 Disabling Interrupts : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 1 2.2 Locking Synchronization Methods : : : : : : : : : : : : : : : : : : : : : : : : : : 2 2.3 Lock-Free Synchronization Methods : : : : : : : : : : : : : : : : : : : : : : : : : 2  3 Lock-Free Quajects 3  3.1 LIFO ...
243|Detecting Violations of Sequential Consistency|The performance of a multiprocessor is directly affected by the choice of the memory consistency model supported. Several different consistency models have been proposed in the literature. These range from sequential consistency on one end, allowing limited buffering of memory accesses, to release consistency on the other end, allowing extensive buffering and pipelining. While the relaxed models such as release consistency provide potential for higher performance, they present a more complex programming model than sequential consistency. Previous research has addressed this tradeoff by showing that a release consistent architecture provides sequentially consistent executions for programs that are free of data races. However, the burden of guaranteeing that the program is free of data races remains with the programmer or compiler.
244|Wattch: A Framework for Architectural-Level Power Analysis and Optimizations|Power dissipation and thermal issues are increasingly significant  in modern processors. As a result, it is crucial that  power/performance tradeoffs be made more visible to chip  architects and even compiler writers, in addition to circuit  designers. Most existing power analysis tools achieve high  accuracy by calculating power estimates for designs only after  layout or floorplanning are complete In addition to being  available only late in the design process, such tools are  often quite slow, which compounds the difficulty of running  them for a large space of design possibilities.
245|Complexity-effective superscalar processors|The performance tradeoff between hardware complexity and clock speed is studied. First, a generic superscalar pipeline is defined. Then the specific areas of register renaming, instruction window wakeup and selection logic, and operand bypassing are analyzed. Each is modeled and Spice simulated for feature sizes of 0:8 m, 0:35 m, and0:18 m. Performance results and trends are expressed in terms of issue width and window size. Our analysis indicates that window wakeup and selection logic as well as operand bypass logic are likely to be the most critical in the future. A microarchitecture that simplifies wakeup and selection logic is proposed and discussed. This implementation puts chains of dependent instructions into queues, and issues instructions from multiple queues in parallel. Simulation shows little slowdown as compared with a completely flexible issue window when performance is measured in clock cycles. Furthermore, because only instructions at queue heads need to be awakened and selected, issue logic is simplified and the clock cycle is faster – consequently overall performance is improved. By grouping dependent instructions together, the proposed microarchitecture will help minimize performance degradation due to slow bypasses in future wide-issue machines. 1
246|Pipeline gating: speculation control for energy reduction|Branch prediction has enabled microprocessors to increase instruction level parallelism (ILP) by allowing programs to speculatively execute beyond control boundaries. Although speculative execution is essential for increasing the instructions per cycle (IPC), it does come at a cost. A large amount of unnecessary work results from wrong-path instructions entering the pipeline due to branch misprediction. Results generated with the SimpleScalar tool set using a 4-way issue pipeline and various branch predictors show an instruction overhead of 16 % to 105 % for every instruction committed. The instruction overhead will increase in the future as processors use more aggressive speculation and wider issue widths [9]. In this paper, we present an innovative method for power reduction which, unlike previous work that sacrificed flexibility or performance, reduces power in high-performance microprocessors without impacting performance. In particular, we introduce a hardware mechanism called pipeline gating to control rampant speculation in the pipeline. We present inexpensive mechanisms for determining when a branch is likely to mispredict, and for stopping wrong-path instructions from entering the pipeline. Results show up to a 38 % reduction in wrong-path instructions with a negligible performance loss (   ¢¡¤ £). Best of all, even in programs with a high branch prediction accuracy, performance does not noticeably degrade. Our analysis indicates that there is little risk in implementing this method in existing processors since it does not impact performance and can benefit energy reduction. 1
247|An enhanced access and cycle time model for on-chip caches|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
248|Energy Dissipation In General Purpose Microprocessors|In this paper we investigate possible ways to improve the energy efficiency of a general purpose microprocessor. We show that the energy of a processor depends on its performance, so we chose the energy-delay product to compare different processors. To improve the energy-delay product we explore methods of reducing energy consumption that do not lead to performance loss (i.e., wasted energy), and explore methods to reduce delay by exploiting instruction level parallelism. We found that careful design reduced the energy dissipation by almost 25%. Pipelining can give approximately a 22 improvement in energydelay product. Superscalar issue, however, does not improve the energy-delay product any further since the overhead required offsets the gains in performance. Further improvements will be hard to come by since a large fraction of the energy (50--80%) is dissipated in the clock network and the on-chip memories. Thus, the efficiency of processors will depend more on the technology being ...
249|The filter cache: An energy efficient memory structure|Most modern microprocessors employ one or two levels of on-chip caches in order to improve performance. These caches are typically implemented with static RAM cells and often occupy a large portion of the chip area. Not surprisingly, these caches often consume a significant amount of power. In many applications, such as portable devices, low power is more important than performance. We propose to trade performance for power consumption by filtering cache references through an unusually small L1 cache. An L2 cache, which is similar in size and structure to a typical L1 cache, is positioned behind the filter cache and serves to reduce the performance loss. Experimental results across a wide range of embedded applications show that the filter cache results in improved memory system energy efficiency. For example, a direct mapped 256-byte filter cache achieves a 58 % power reduction while reducing performance by 21%, corresponding to a 51% reduction in the energy-delay product over a conventional design. 1
250|Dynamic Instruction Reuse|This paper introduces the concept of dynamic instruction reuse. Empirical observations suggest that many instructions, and groups of instructions, having the same inputs, are executed dynamically. Such instructions do not have to be executed repeatedly --- their results can be obtained from a buffer where they were saved previously. This paper presents three hardware schemes for exploiting the phenomenon of dynamic instruction reuse, and evaluates their effectiveness using execution-driven simulation. We find that in some cases over 50% of the instructions can be reused. The speedups so obtained, though less striking than the percentage of instructions reused, are still quite significant.  1 Introduction  There are three parameters that influence the execution time of a program. Microarchitecture has concentrated on two of them: (i) the number of instructions executed per clock cycle, i.e., the IPC, and (ii) the clock cycle time. The third parameter: (iii) the total number of instructi...
251|Dynamically Exploiting Narrow Width Operands to Improve Processor Power and Performance|In general-purpose microprocessors, recent trends have pushed towards 64-bit word widths, primarily to accommodate the large addressing needs of some programs. Many integer problems, however, rarely need the full 64-bit dynamic range these CPUs provide. In fact, another recent instruction set trend has been increased support for sub-word operations (that is, manipulating data in quantities less than the full word size). In particular, most major processor families have introduced &#034;multimedia&#034; instruction set extensions that operate in parallel on several sub-word quantities in the same ALU.  This paper notes that across the SPECint95 benchmarks, over half of the integer operation executions require 16 bits or less. With this as motivation, our work proposes hardware mechanisms that dynamically recognize and capitalize on these &#034;narrow-bitwidth&#034; instances. Both optimizations require little additional hardware, and neither requires compiler support.  The first, power-oriented, optimizati...
252|Analytical Energy Dissipation Models For Low Power Caches|We present detailed analytical models for estimating the energy dissipation in conventional caches as well as low energy cache architectures. The analytical models use the run time statistics such as hit/miss counts, fraction of read/write requests and assume stochastical distributions for signal values. These models are validated by comparing the power estimated using these models against the power estimated using a detailed simulator called CAPE (CAache Power Estimator). The analytical models for conventional caches are found to be accurate to within 2% error. However, these analytical models over--predict the dissipations of low--power caches by as much as 30%. The inaccuracies can be attributed to correlated signal values and locality of reference, both of which are exploited in making some cache organizations energy efficient.
253|Low-Power Logic Styles: CMOS Versus Pass-Transistor Logic|Recently reported logic style comparisons based on full-adder circuits claimed complementary passtransistor logic (CPL) to be much more power-efficient than complementary CMOS. However, new comparisons performed on more efficient CMOS circuit realizations and a wider range of different logic cells, as well as the use of realistic circuit arrangementsdemonstrateCMOS to be superior to CPL in most cases with respect to speed, area, power dissipation, and power-delay products. An implemented 32-bit adder using complementary CMOS has a power-delay product of less than half that of the CPL version. Robustness with respect to voltage scaling and transistor sizing, as well as generality and ease-of-use, are additional advantages of CMOS logic gates, especially when cell-based design and logic synthesis are targeted. This paper shows that complementary CMOS is the logic style of choice for the implementation of arbitrary combinational circuits, if low voltage, low power, and small power-delay p...
254|Power considerations in the design of the Alpha 21264 microprocessor|Power dissipation is rapidly becoming a limiting factor in high performance microprocessor design due to ever increasing device counts and clock rates. The 21264 is a third generation Alpha microprocessor implementation, containing 15.2 million transistors and operating at 600 MHz. This paper describes some of the techniques the Alpha design team utilized to help manage power dissipation. In addition, the electrical design of the power, ground, and clock networks is presented. 2.
255|Quantifying the Complexity of Superscalar Processors|The delay of pipeline structures in superscalar processors are studied to determine their potential for  limiting clock cycle times in future designs. First, a generic superscalar pipeline is defined. Then the specific  areas of register renaming, instruction window wakeup and selection logic, and operand bypassing  are analyzed. Each is modeled and Spice simulated for feature sizes of 0:8 m, 0:35 m, and 0:18 m.
256|The energy complexity of register files|Register files (RF) represent a substantial portion of the energy budget in modern processors, and are growing rapidly with the trend towards wider instruction issue. The actual access energy costs depend greatly on the register file circuitry used. This paper compares various RF circuitry techniques for their energy efficiencies, as a function of architectural parameters such as the number of registers and the number of ports. The Port Priority Selection technique was found to be the most energy efficient. The dependence of register file access energy upon technology scaling is also studied. However, as this paper shows, it appears that none of these will be enough to prevent centralized register files from becoming the dominant power component of next-generation superscalar computers, and alternative methods for inter-instruction communication need to be developed. Split register file architecture is analyzed as a possible alternative.
257|Branch prediction, instruction-window size, and cache size: Performance tradeoffs and simulation techniques|Design parameters interact in complex ways in modern processors, especially because out-of-order issue and decoupling buffers allow latencies to be overlapped. Tradeoffs among instruction-window size, branch-prediction accuracy, and instruction- and datacache size can change as these parameters move through different domains. For example, modeling unrealistic caches can under- or over-state the benefits of better prediction or a larger instruction window. Avoiding such pitfalls requires understanding how all these parameters interact. Because such methodological mistakes are common, this paper provides a comprehensive set of SimpleScalar simulation results from SPECint95 programs, showing the interactions among these major structures. In addition to presenting this database of simulation results, major mechanisms driving the observed tradeoffs are described. The paper also considers appropriate simulation techniques when sampling full-length runs with the SPEC reference inputs. In particular, the results show that branch mispredictions limit the benefits of larger instruction windows, that better branch prediction and better instruction cache behavior have synergistic effects, and that the benefits of larger instruction windows and larger data caches trade off and have overlapping effects. In addition, simulations of only 50 million instructions can yield representative results if these short windows are carefully selected.
258|Instruction Pre-Processing in Trace Processors|In trace processors, a sequential program is partitioned at run time into &#034;traces.&#034; A trace is an encapsulation of a dynamic sequence of instructions. A processor that uses traces as the unit of sequencing and execution achieves high instruction fetch rates and can support very wide-issue execution engines. We propose a new class of hardware optimizations that transform the instructions within traces to increase the performance of trace processors. Traces are &#034;pre-processed&#034; to optimize the instructions for execution together. We propose three specific optimizations: instruction scheduling, constant propagation, and instruction collapsing. Together, these optimizations offer substantial performance benefit, increasing performance by up to 24%.
259|Using Value Prediction to Increase the Power of Speculative Execution Hardware|This paper presents an experimental and analytical study of value prediction and its impact on speculative execution in superscalar microprocessors. Value prediction is a new paradigm that suggests predicting outcome values of operations (at run-time) and using these predicted values to trigger the execution of true-data dependent operations speculatively. As a result, stalls to memory locations can be reduced and the amount of instruction-level parallelism can be extended beyond the limits of the program&#039;s dataflow graph. This paper examines the characteristics of the value prediction concept from two perspectives: 1. the related phenomena that are reflected in the nature of computer programs, and 2. the significance of these phenomena to boosting instruction-level parallelism of super-scalar microprocessors that support speculative execution. In order to better understand these characteristics, our work combines both analytical and experimental studies.   
260|Power and Performance Tradeoffs using Various Cache Configurations|In this paper, we will propose several different data and instruction cache configurations and analyze their power as well as performance implications on the processor. Unlike most existing work in low power microprocessor design, we explore a high performance processor with the latest innovations for performance. Using a detailed, architectural-level simulator, we evaluate full system performance using several different power/performance sensitive cache configurations. We then use the information obtained from the simulator to calculate the energy consumption of the memory hierarchy of the system. Based on the results obtained from these simulations, we will determine the general characteristics of each cache configuration. We will also make recommendations on how best to balance power and performance tradeoffs in memory hierarchy design.  1 Introduction  In this paper we will concentrate on reducing the energy demands of an ultra high-performance processor, such as the Pentium Pro or...
261|The Design of a Register Renaming Unit|Register renaming is often used to improve performance in many high-ILP processors. However, there is a lack of publications regarding register renaming hardware design. This paper presents a detailed look at one possible implementation of a register renaming unit, as well as some possible optimizations.  1. 
262|Supporting Real-Time Applications in an Integrated Services Packet Network: Architecture and Mechanism|This paper considers the support of real-time applications in an
263|Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment|The problem of multiprogram scheduling on a single processor is studied from the viewpoint...
264|A generalized processor sharing approach to flow control in integrated services networks: The single-node case|Abstruet-The problem of allocating network resources to the users of an integrated services network is investigated in the context of rate-based flow control. The network is assumed to be a virtual circuiq comection-based packet network. We show that the use of Generalized processor Sharing (GPS), when combined with Leaky Bucket admission control, allows the network to make a wide range of worst-case performance guarantees on throughput and delay. The scheme is flexible in that d~erent users may be given widely different performance guarantees, and is efilcient in that each of the servers is work conserving. We present a practicat packet-by-packet service discipline, PGPS (first proposed by Deme5 Shenker, and Keshav [7] under the name of Weighted Fair Queueing), that closely approximates GPS. This altows us to relate ressdta for GPS to the packet-bypacket scheme in a precise manner. In this paper, the performance of a single-server GPS system is analyzed exactty from the standpoint of worst-case packet delay and burstiness when the sources are constrained by leaky buckets. The worst-case sewdon backlogs are also determined. In the sequel to this paper, these results are extended to arbitrary topology networks with multiple nodes. I.
265|A Scheme for Real-Time Channel Establishment in Wide-Area Networks|Multimedia communication involving digital audio and/or digital video has rather strict delay requirements. A real-time channel is defined in this paper as a simplex connection between a source and a destination characterized by parameters representing the performance requirements of the client. A real-time service is capable of creating realtime channels on demand and guaranteeing their performance. These guarantees often take the form of lower bounds on the bandwidth allocated to a channel and upper bounds on the delays to be experienced by a packet on the channel. In this paper
266|Rate Controlled Servers for Very High-Speed Networks|Future high-speed networks are expected to carry traffic with a wide range of performance requirements. We describe two queue service disciplines, rate-based scheduling and hierarchical round robin scheduling, that allow some connections to receive guaranteed rate and jitter performance, while others receive best effort service. Rate-based scheduling is designed for fast packet networks, while hierarchical round robin is an extension of round robin scheduling suitable for use in networks based on the Asynchronous Transfer Mode (ATM) being defined in CCITT. Both schemes are feasible at rates of one Gigabit/sec. The schemes allow strict bounds on the buffer space required for rate controlled connections and can provide efficient utilization of transmission bandwidth.  Introduction  Future high-speed networks are expected to carry traffic with a wide range of performance requirements. A classic tradeoff in network design is between providing quality of service guarantees on one hand, and ...
267|Distributed Scheduling Based On Due Dates And Buffer Priorities|We are motivated by the problem of scheduling a large semiconductor manufacturing  facility, where jobs of wafers, each with a desired due date, follow essentially  the same route through the manufacturing system, returning several times to many  of the service centers for the processing of successive layers. Neglecting the randomness  introduced by yield, such a system can be modeled as a non-acyclic ow line.
268|Real-Time Scheduling with Quality of Service Constraints|Can the introduction of traffic classes improve upon the performance of ATM networks? We investigate this issue within the framework provided by a class of networks that guarantees quality of service. To provide a meaningful comparison we define the concept of schedulable region, a region in the space of loads for which the quality of service is guaranteed. We show the dependence of the schedulable region on the scheduling algorithm employed, the quality of service parameters and the traffic statistics. An efficient real-time scheduling algorithm is introduced that substantially increases the schedulable region without incurring prohibitive complexity costs. The schedulable region associated with this algorithm is compared with the ones generated by the static priority scheduling algorithm and a variant of the minimum laxity threshold algorithm. The size and shape of the schedulable region is explored by means of simulations. 
269|Cognitive architecture and instructional design|Cognitive load theory has been designed to provide guidelines intended to assist in the presentation of information in a manner that encourages learner activities that optimize intellectual performance. The theory assumes a limited capacity working memory that includes partially independent subcomponents to deal with auditory/verbal material and visual/2- or 3-dimensional information as well as an effectively unlimited long-term memory, holding schemas that vary in their degree of automation. These structures and functions of human cognitive architecture have been used to design a variety of novel instructional procedures based on the assumption that working memory load should be reduced and schema construction encouraged. This paper reviews the theory and the instructional designs generated by it. KEY WORDS: cognition; instructional design; learning; problem solving.
270|Transfer of Cognitive Skill|A framework for skill acquisition is proposed that includes two major stages in the development of a cognitive skill: a declarative stage in which facts about the skill domain are interpreted and a procedural stage in which the domain knowledge is directly embodied in procedures for performing the skill. This general framework has been instantiated in the ACT system in which facts are encoded in a propositional network and procedures are encoded as productions. Knowledge compilation is the process by which the skill transits from the declarative stage to the procedural stage. It consists of the subprocesses of composition, which collapses sequences of productions into single productions, and proceduralization, which embeds factual knowledge into productions. Once proceduralized, further learning processes operate on the skill to make the productions more selective in their range of applications. These processes include generalization, discrimination, and strengthening of productions. Comparisons are made to similar concepts from past learning theories. How these learning mechanisms apply to produce the power law speedup in processing time with practice is discussed. It requires at least 100 hours of learning and practice to acquire any significant cognitive skill to a reasonable degree of proficiency. For instance, after 100 hours a student learning to program a computer has achieved only a very modest facility in the skill. Learning one&#039;s primary language takes tens of thousands of hours. The psychology of human learning has been very thin in ideas about what happens to skills under the impact of this amount of learning—and for obvious reasons. This article presents a theory about the changes in the nature of a skill over such large time scales and about the basic learning processes that are responsible.
271|Controlled and automatic human information processing|A two-process theory of human information processing is proposed and applied to detection, search, and attention phenomena. Automatic processing is activa-tion of a learned sequence of elements in long-term memory that is initiated by appropriate inputs and then proceeds automatically—without subject control, without stressing the capacity limitations of the system, and without necessarily demanding attention. Controlled processing is a temporary activation of a se-quence of elements that can be set up quickly and easily but requires attention, is capacity-limited (usually serial in nature), and is controlled by the subject. A series of studies using both reaction time and accuracy measures is presented, which traces these concepts in the form of automatic detection and controlled, search through the areas of detection, search, and attention. Results in these areas are shown to arise from common mechanisms. Automatic detection is shown to develop following consistent mapping of stimuli to responses over trials. Controlled search is utilized in varied-mapping paradigms, and in our studies, it takes the form of serial, terminating search. The approach resolves a number of apparent conflicts in the literature.
272|Controlled and automatic human information processing: II. Perceptual learning, automatic attending and a general theory|The two-process theory of detection, search, and attention presented by Schneider and Shiffrin is tested and extended in a series of experiments. The studies demonstrate the qualitative difference between two modes of information processing: automatic detection and controlled search. They trace the course of the learning of automatic detection, of categories, and of automaticattention responses. They show the dependence of automatic detection on attending responses and demonstrate how such responses interrupt controlled processing and interfere with the focusing of attention. The learning of categories is shown to improve controlled search performance. A general framework for human information processing is proposed; the framework emphasizes the roles of automatic and controlled processing. The theory is compared to and contrasted with extant models of search and attention.
273|Cognitive load during problem solving: effects on learning|Considerable evidence indicates that domain specific knowledge in the form of schemes is the primary factor distinguishing experts from novices in problem-solving skill. Evidence that conventional problem-solving activity is not effective in schema acquisition is also accumulating. It is suggested that a major reason for the ineffectiveness of problem solving as a learning device, is that the cognitive processes required by the two activities overlap insufficiently, and that conventional problem solving in the form of means-ends analysis requires a relatively large amount of cognitive processing capacity which is consequently unavailable for schema acquisition. A computational model and experimental evidence provide support for this contention. Theoretical and practical implications are discussed. 
274|Animations need narrations: an experimental test of a dualcoding hypothesis|In 2 experiments, mechanically naive college students viewed an animation depicting the operation of a bicycle tire pump that included a verbal description given before (words-before-pictures) or during (words-with-pictures) the animation. The words-with-pictures group outper-formed the words-before-pictures group on tests of creative problem solving that involved reasoning about how the pump works. In a follow-up experiment, students in the words-with-pictures group performed better on the problem-solving test than students who saw the animation without words (pictures only), heard the words without the animation (words only), or received no training (control). Results support a dual-coding hypothesis (Paivio, 1990) that posits two kinds of connections: representational connections between verbal stimuli and verbal represen-tations and between visual stimuli and visual representations and referential connections between visual and verbal representations. A major goal of science is to provide explanations for how various physical, biological, and social systems work. It fol-lows that a major goal of science education is to help students understand scientific explanations. What constitutes an un-
275|The instructive animation: Helping students build connections between words and pictures in multimedia learning|In 2 experiments, students studied an animation depicting the operation of a bicycle tire pump or an automobile braking system, along with concurrent oral narration of the steps in the process (concurrent group), successive presentation of animation and narration (by 4 different methods), animation alone, narration alone, or no instruction (control group). On retention tests, the control group performed more poorly than each of the other groups, which did not differ from one another. On problem-solving tests, the concurrent group performed better than each of the other groups, which did not differ from one another. These results are consistent with a dual-coding model in which retention requires the construction of representational connections and problem solving requires the construction of representational and referential connections. An instructional implication is that pictures and words are most effective when they occur contig-uously in time or space. Imagine an electronic encyclopedia in which a user sits in front of a screen and keyboard. The user simply types in a term (or selects it from a list), such as pump. Then a multi-media presentation begins, involving stereo sound and high-resolution color graphics, with which the user interacts. In this context, words are presented orally and animations are presented visually. The technology for implementing this scenario exists today
276|Reducing cognitive load by mixing auditory and visual presentation modes|This article reports findings on the use of a partly auditory and partly visual mode of presentation for geometry worked examples. The logic was based on the split-attention effect and the effect of presentation modality on working memory. The split-attention effect occurs when students must split their attention between multiple sources of information, which results in a heavy cognitive load. Presentation-modality effects suggest that working memory has partially independent processors for handling visual and auditory material. Effective working memory may be increased by presenting material in a mixed rather than a unitary mode. If so, the negative consequences of split attention in geometry might be ameliorated by presenting geometry statements in auditory, rather than visual, form. The results of 6 experiments supported this hypothesis. In recent years, working memory limitations have been identified as a major factor that needs to be considered when instruction is designed. Researchers have used cognitive load theory (e.g., Sweller, 1988, 1989, 1993, 1994) to sug-gest that many commonly used instructional procedures are
277|Are good texts always better? Interactions of text coherence, background knowledge, and levels of understanding in learning from tex|Two experiments, theoretically motivated by the construction-integration model of
text comprehension ( W. Kintsch, 1988), investigated the role of text coherence in
the comprehension of science texts. In Experiment 1, junior high school students&#039;
comprehension of one of three versions of a biology text was examined via free
recall, written questions, and a key-word sorting task. This study demonstrates
advantages for globally coherent text and for more explanatory text. In Experiment
2, interactions among local and global text coherence, readers&#039; background
knowledge, and levels of understanding were examined. Using the same methods
as in Experiment 1, we examined students&#039; comprehension of one of four versions
of a text, orthogonally varying local and global coherence. We found that readers
who know little about the domain of the text benefit from a coherent text, whereas
high-knowledge readers benefit from a minimally coherent text. We argue that the
poorly written text forces the knowledgeable readers to engage in compensatory
processing to infer unstated relations in the text. These findings, however, depended
on the level of understanding, text base or situational, being measured by the three
comprehension tasks. Whereas the free-recall measure and text-based questions
primarily tapped readers&#039; superficial understanding of the text, the inference
questions, problem-solving questions, and sorting task relied on a situational
understanding of the text. This study provides evidence that the rewards to be
gained from active processing are primarily at the level of the situation model
rather than at the superficial level of text-base understanding.
278|Variability of worked examples and transfer of geometrical problem-solving skills: A cognitive-load approach|Four computer-based training strategies for geometrical problem solving in the domain of computer numerically controlled machinery programming were studied with regard to their effects on training performance, transfer performance, and cognitive load. A low- and a high-variability conventional condition, in which conventional practice problems had to be solved (followed by worked examples), were compared with a low- and a high-variability worked condition, in which worked examples had to be studied. Results showed that students who studied worked examples gained most from high-variability examples, invested less time and mental effort in practice, and attained better and less effort-demanding transfer performance than students who first attempted to solve conventional problems and then studied work examples. In complex cognitive domains such as mathematics, phys-ics, or computer programming, problem solutions can often be characterized by a hierarchical goal structure. The goal of these solutions can be attained only by successfully attaining all subgoals. Learning and performance of complex cogni-tive tasks are typically constrained by limited processing ca-
279|When less is more: Meaningful learning from visual and verbal summaries of textbook lessons|In a series of 3 experiments, college students who read a summary that contained a sequence of short captions with simple illustrations depicting the main steps in the process of lightning recalled these steps and solved transfer problems as well as or better than students who received the full text along with the summary or the full text alone. In Experiment 2, taking away the illustrations or the captions eliminated the effectiveness of the summary. In Experiment 3, adding text to the summary reduced its effectiveness. Implications for a cognitive theory of multimedia learning are discussed; implications for instructional design pertain to the need for conciseness, coherence, and coordination in presenting scientific explanations. Consider the following scenario. A student who is inex-perienced in meteorology reads a textbook lesson explain-ing the cause-and-effect chain of events involved in how lightning storms develop. The explanation is clearly con-tained within the 600 words and five illustrations of the lesson. A few minutes later, we ask the student to write
280|Structuring effective worked examples|you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at
281|The impact of goal specificity on strategy use and the acquisition of problem structure|Theories of skill acquisition have made radically different predictions about the role of general problem-solving methods in acquiring rules that promote effec-tive transfer to new problems. Under one view, methods that focus on reaching specific goals, such as means-ends analysis, are assumed to provide the basis for efficient knowledge compilation (Anderson, 1987). whereas under an alternative view such methods are believed to disrupt rule induction (Sweller, 1988). We sug-gest that the role of general methods in learning varies with both the specificity of the problem solver’s goal and the systematicity of the strategies used for testing hypotheses about rules. In the absence of a specific goal people are more likely to use a rule-induction learning strategy, whereas provision of a specific goal fosters use of difference reduction, which tends to be a non-rule-induction strategy. We performed two experiments to investigate the impact of goal specif-icity and systematicity of rule-induction strategies in learning and transfer within a complex dynamic system. The results of Experiment 1 indicated that during free exploration of a problem space, greater learning occurred if participants adopted
282|Parallel database systems: the future of high performance database systems|Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1.
283|Access path selection in a relational database management system|ABSTRACT: In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research&#039;Laboratory. 1.
284|The Gamma database machine project|This paper describes the design of the Gamma database machine and the techniques employed in its implementation. Gamma is a relational database machine currently operating on an Intel iPSC/2 hypercube with 32 processors and 32 disk drives. Gamma employs three key technical ideas which enable the architecture to be scaled to 100s of processors. First, all relations are horizontally partitioned across multiple disk drives enabling relations to be scanned in parallel. Second, novel parallel algorithms based on hashing are used to implement the complex relational operators such as join and aggregate functions. Third, dataflow scheduling techniques are used to coordinate multioperator queries. By using these techniques it is possible to control the execution of very complex queries with minimal coordination- a necessity for configurations involving a very large number of processors. In addition to describing the design of the Gamma software, a thorough performance evaluation of the iPSC/2 hypercube version of Gamma is also presented. In addition to measuring the effect of relation size and indices on the response time for selection, join, aggregation, and update queries, we also analyze the performance of Gamma relative to the number of processors employed when the sizes of the input relations are kept constant (speedup) and when the sizes of the input relations are increased proportionally to the number of processors (scaleup). The speedup results obtained for both selection and join queries are linear; thus, doubling the number of processors
285|A Performance Evaluation of Four Parallel Join Algorithms in a Shared-Nothing Multiprocessor Environment|The join operator has been a cornerstone of relational database systems since their inception. As such, much time and effort has gone into making joins efficient. With the obvious trend towards multiprocessors, attention has focused on efficiently parallelizing the join operation. In this paper we analyze and compare four parallel join algorithms. Grace and Hybrid hash represent the class of hash-based join methods, Simple hash represents a looping algorithm with hashing, and our last algorithm is the more traditional sort-merge. The Gamma database machine serves as the host for the performance comparison. Gamma’s shared-nothing architecture with commercially available components is becoming increasingly common, both in research and in industry. 1.
286|Dynamic Query Evaluation Plans|Traditional query optimizers assume accurate knowledge of run-time parameters such as selectivities and resource availability during plan optimization, i.e., at compile-time. In reality, however, this assumption is often not justified. Therefore, the “static ” plans produced by traditional optimizers may not be optimal for many of their actual run-time invocations. Instead, we propose a novel optimization model that assigns the bulk of the optimization effort to compile-time and delays carefully selected optimization decisions until run-time. Our previous work defined the run-time primitives, “dynamic plans ” using “choose-plan” operators, for executing such delayed decisions, but did not solve the problem of constructing dynamic plans at compile-time. The present paper introduces techniques that solve this problem. Experience with a working prototype optimizer demonstrates (i) that the additional optimization and start-up overhead of dynamic plans compared to static plans is dominated by their advantage at run-time, (ii) that dynamic plans are as robust as the “brute-force” remedy of run-time optimization, i.e., dynamic plans maintain their optimality even if parameters change between compile-time and run-time, and (iii) that the start-up overhead of dynamic plans is signifmantly less than the time required for complete optimization at run-time. In other words, our proposed techniques are superior to both techniques considered to-date, namely compile-time optimization into a single static plan as well as run-time optimization. Finally, we believe that the concepts and technology described can be transferred to commercial query optimizers in order to improve the performance of embedded queries with host variables in the query predicate and to adapt to run-time system loads unpredictable at compile-time. 1.
287|Data Placement in Bubba|Thus paper examines the problem of data placement in Bubba, a highly-parallel system for data-intensive applications bemg developed at MCC “Highly-parallel” lmplres that load balancmng IS a cntlcal performance issue “Data-mtenave” means data IS so large that operatrons should be executed where the data resides As a result, data placement becomes a cntlcal performance issue In general, determmmg the optimal placement of data across processmg nodes for performance IS a difficult problem We describe our heuristic approach to solvmg the data placement problem w Bubba We then present expenmental results using a specific workload to provide msrght into the problem Several researchers have argued the benefits of deelustering (1 e, spreading each base relation over many nodes) We show that as declustermg IS increased. load balancing contmues to improve However, for transactions mvolvmg complex Joins, further declusterrng reduces throughput because of communications, startup and termmatron overhead We argue that data placement, especially declustermg, m a highly-parallel system must be considered early in the design, so that mechanrsms can be included for supportmg variable declustermg, for mmtmlzmg the most significant overheads associated with large-scale declustenng, and for gathering the required statistics.
288|The Design Of Xprs|This paper presents an overview of the techniques we are using to build a DBMS at Berkeley that will simultaneously provide high performance and high availability in transaction processing environments, in applications with complex ad-hoc queries and in applications with large objects such as images or CAD layouts. We plan to achieve these goals using a general purpose DBMS and operating system and a shared memory multiprocessor. The hardware and software tactics which we are using to accomplish these goals are described in this paper and include a novel &#034;fast path&#034; feature, a special purpose concurrency control scheme, a two-dimensional file system, exploitation of parallelism and a novel method to efficiently mirror disks. 
289|A benchmark of NonStop SQL release 2 demonstrating near-linear speedup and scaleup on large databases|its second release, NonStop SQL transparently and automatically implements parallelism within an SQL statement. This parallelism allows query execution speed to increase almost linearly as processors and discs are added to the system-- speedup. In addition, this parallelism can help jobs restricted to a fIxed &amp;quot;batch window&amp;quot;. When the job doubles in size, its elapsed processing time will not change ifproportionately more equipment is available to process the job-- scaleup. This paper describes the parallelism features of NonStop SQL and an audited benchmark that demonstrates these speedup and scaleup claims.
290|Formal Ontology and Information Systems|Research on ontology is becoming increasingly widespread in the computer science community, and its importance is being recognized in a multiplicity of research fields and application areas, including knowledge engineering, database design and integration, information retrieval and extraction. We shall use the generic term information systems, in its broadest sense, to collectively refer to these application perspectives. We argue in this paper that so-called ontologies present their own methodological and architectural peculiarities: on the methodological side, their main peculiarity is the adoption of a highly interdisciplinary approach, while on the architectural side the most interesting aspect is the centrality of the role they can play in an information system, leading to the perspective of ontology-driven information systems.
291|A translation approach to portable ontology specifications|To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse — definitions of classes, relations, functions, and other objects — is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms. 
292|WordNet: A Lexical Database for English|Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet 1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].
293|Toward Principles for the Design of Ontologies Used for Knowledge Sharing|Recent work in Artificial Intelligence is exploring the use of formal ontologies as a way of specifying content-specific agreements for the sharing and reuse of knowledge among software entities. We take an engineering perspective on the development of such ontologies. Formal ontologies are viewed as designed artifacts, formulated for specific purposes and evaluated against objective design criteria. We describe the role of ontologies in supporting knowledge sharing activities, and then present a set of criteria to guide the development of ontologies for these purposes. We show how these criteria are applied in case studies from the design of ontologies for engineering mathematics and bibliographic data. Selected design decisions are discussed, and alternative representation choices and evaluated against the design criteria.
294|Ontologies: Principles, methods and applications|This paper is intended to serve as a comprehensive introduction to the emerging field concerned with the design and use of ontologies. We observe that disparate backgrounds, languages, tools, and techniques are a major barrier to effective communication among people, organisations, and/or software systems. We show how the development and implementation of an explicit account of a shared understanding (i.e. an `ontology&#039;) in a given subject area, can improve such communication, which in turn, can give rise to greater reuse and sharing, inter-operability, and more reliable software. After motivating their need, we clarify just what ontologies are and what purposes they serve. We outline a methodology for developing and evaluating ontologies, first discussing informal techniques, concerning such issues as scoping, handling ambiguity, reaching agreement and producing de nitions. We then consider the bene ts of and describe, a more formal approach. We re-visit the scoping phase, and discuss the role of formal languages and techniques in the specification, implementation and evaluation of ontologies. Finally, we review the state of the art and practice in this emerging field,
295|Ontologies and knowledge bases: Towards a terminological clarification|The word “ontology ” has recently gained a good popularity within the knowledge engineering community. However, its meaning tends to remain a bit vague, as the term is used in very different ways. Limiting our attention to the various proposals made in the current debate in AI, we isolate a number of interpretations, which in our opinion deserve a suitable clarification. We elucidate the implications of such various interpretations, arguing for the need of clear terminological choices regarding the technical use of terms like “ontology”, “conceptualization ” and “ontological commitment”. After some comments on the use “Ontology ” (with the capital “o”) as a term which denotes a philosophical discipline, we analyse the possible confusion between an ontology intended as a particular conceptual framework at the knowledge level and an ontology intended as a concrete artifact at the symbol level, to be used for a given purpose. A crucial point in this clarification effort is the careful analysis of Gruber’ s definition of an ontology as a specification of a conceptualization. 1
296|Formal Ontology, Conceptual Analysis and Knowledge Representation|The purpose of this paper is to defend the systematic introduction of formal ontological principles in the current practice of knowledge engineering, to explore the various relationships between ontology and knowledge representation, and to present the recent trends in this promising research area. According to the &#034;modelling view&#034; of knowledge acquisition proposed by Clancey, the modeling activity must establish a correspondence between a knowledge base and two separate subsystems: the agent&#039;s behavior (i.e. the problem-solving expertize) and its own environment (the problem domain). Current knowledge modelling methodologies tend to focus on the former subsystem only, viewing domain knowledge as strongly dependent on the particular task at hand: in fact, AI researchers seem to have been much more interested in the nature of reasoning rather than in the nature of the real world. Recently, however, the potential value of task-independent knowlege bases (or &#034;ontologies&#034;) suitable to large scale integration has been underlined in many ways.  In this paper, we compare the dichotomy between reasoning and representation to the philosophical distinction between epistemology and ontology. We introduce the notion of the ontological level, intermediate between the epistemological and the conceptual level discussed by Brachman, as a way to characterize a knowledge representation formalism taking into account the intended meaning of its primitives. We then discuss some formal ontological distinctions which may play an important role for such purpose.   
297|Towards distributed use of large-scale ontologies|Large scale knowledge bases systems are difficult and expensive to construct. If we could share knowledge across systems, costs would be reduced. However, because knowledge bases are typically constructed from scratch, each with their own idiosyncratic structure, sharing is difficult. Recent research has focused on the use of ontologies to promote sharing. An ontology is a hierarchically structured set of terms for describing a domain that can be used as a skeletal foundation for a knowledge base. If two knowledge bases are built on a common ontology, knowledge can be more readily shared, since they share a common underlying structure. This paper outlines a set of desiderata for ontologies, and then describes how we have used a large-scale (50,000+ concept) ontology develop a specialized, domain-specific ontology semiautomatically. We then discuss the relation between ontologies and the process of developing a system, arguing that to be useful, an ontology needs to be created as a &#034;living document&#034;, whose development is tightly integrated with the system’s. We conclude with a discussion of Web-based ontology tools we are developing to support this approach.
298|Enterprise modeling|... This article motivates the need for enterprise models and introduces the concepts of generic and deductive enterprise models. It  reviews research to date on enterprise modeling and considers in detail the Toronto virtual enterprise effort at the University of Toronto.
299|Part-Whole Relations in Object-Centered Systems: An Overview|Knowledge bases, data bases and object-oriented systems (referred to in the paper as Object-Centered systems) all rely on attributes as the main construct used  to associate properties to objects; among these, a fundamental role is played by  the so-called part-whole relation. The representation of such a structural information usually requires a particular semantics together with specialized inference and  update mechanisms, but rarely do current modeling formalisms and methodologies  give it a specific  &#034;first-class&#034; dignity.  The main thesis of this paper is that the part-whole relation cannot simply be  considered as an ordinary attribute, its specific ontological nature requires to be  understood and integrated within data modeling formalisms and methodologies.  On the basis of such an ontological perspective, we survey the conceptual modeling  issues involving part-whole relations, and the various modeling frameworks provided  by knowledge representation and object-oriented formalisms.   
300|Semantic Matching: Formal Ontological Distinctions for Information Organization, Extraction, and Integration|The task of information extraction can be seen as a problem of semantic  matching between a user-defined template and a piece of information written  in natural language. To this purpose, the ontological assumptions of the  template need to be suitably specified, and compared with the ontological implications  of the text. So-called &#034;ontologies&#034;, consisting of theories of various  kinds expressing the meaning of shared vocabularies, begin to be used for this  task. This paper addresses the theoretical issues related to the design and use of  such ontologies for purposes of information retrieval and extraction. After a discussion  on the nature of semantic matching within a model-theoretical framework,  we introduce the subject of Formal Ontology, showing how the notions of  parthood, integrity, identity, and dependence can be of help in understanding,  organizing and formalizing fundamental ontological distinctions. We present  then some basic principles for ontology design, and we illustrate a preliminary  proposal for a top-level ontology develped according to such principles. As a  concrete example of ontology-based information retrieval, we finally report an  ongoing experience of use of a large linguistic ontology for the retrieval of object-oriented software components. 
301|The MOMIS approach to Information Integration|Introduction The web explosion, both at internet and intranet level, has transformed the electronic information system from single isolated node to an entry points into a worldwide network of information exchange and business transactions. Business and commerce has taken the opportunity of the new technologies to define the e-commerce activity. An electronic marketplace represents a virtual place where buyers and sellers meet to exchange goods and services, by sharing information that is often obtained as hypertext catalogs from different companies. Companies have equipped themselves with data storing systems building up informative systems containing data which are related one another, but which are often redundant, heterogeneous and not always substantial. The problems that have to be faced in this field are mainly due to both structural and application heterogeneity, as well as to the lack of a common ontology, causing semantic differences between information sources. Moreo
303|Ontology Reuse and Application|In this paper, we describe an investigation into the reuse and application  of an existing ontology for the purpose of specifying and formally  developing software for aircraft design. Our goals were to clearly identify  the processes involved in the task, and assess the cost-effectiveness  of reuse. Our conclusions are that (re)using an ontology is far from  an automated process, and instead requires significant effort from the  knowledge engineer. We describe and illustrate some intrinsic properties  of the ontology translation problem and argue that fully automatic  translators are unlikely to be forthcoming in the foreseeable future. Despite  the effort involved, our subjective conclusions are that in this case  knowledge reuse was cost-effective, and that it would have taken significantly  longer to design the knowledge content of this ontology from  scratch in our application. Our preliminary results are promising for  achieving larger-scale knowledge reuse in the future.
304|Domain Specific Ontologies for Semantic Information Brokering on the Global Information Infrastructure|Recent emerging technologies such as internetworking and the World Wide Web (WWW) have significantly expanded the types, availability, and volume of data accessible to an information management system. In this new environment it is imperative to view an information source at the level of its relevant semantic concepts. We propose that these semantic concepts be chosen from pre-existing domain specific ontologies. Domain specific ontologies are used as tools/mechanisms for specifying the ontological commitments or agreements between information users and providers on the information infrastructure. We use domain specific ontologies to tackle the information explosion by the: (a) Re-use and organization of knowledge in pre-existing real world ontologies, achieved by mapping semantic concepts in the ontologies to data structures in the underlying repositories; and (b) Knowledge integration and development of mechanisms to translate information requests across ontologies. We thus provide s...
305|A Connection Based Approach to Commonsense Topological Description and Reasoning|The standard mathematical approaches to topology, point-set topology and algebraic  topology, treat points as the fundamental, undefined entities, and construct extended  spaces as sets of points with additional structure imposed on them. Point-set topology  in particular generalises the concept of a `space&#039; far beyond its intuitive meaning. Even  algebraic topology, which concentrates on spaces built out of `cells&#039; topologically equivalent  to n-dimensional discs, concerns itself chiefly with rather abstract reasoning concerning  the association of algebraic structures with particular spaces, rather than the kind of  topological reasoning which is required in everyday life, or which might illuminate the  metaphorical use of topological concepts such as `connection&#039; and `boundary&#039;.  This paper explores an alternative to these approaches, RCC theory, which takes  extended spaces (`regions&#039;) rather than points as fundamental. A single relation, C (x; y)  (read `Region x connects with reg...
306|Ontological Tools for Geographic Representation|Abstract. This paper is concerned with certain ontological issues in the foundations of geographic representation. It sets out what these basic issues are, describes the tools needed to deal with them, and draws some implications for a general theory of spatial representation. Our approach has ramifications in the domains of mereology, topology, and the theory of location, and the question of the interaction of these three domains within a unified spatial representation theory is addressed. In the final part we also consider the idea of nonstandard geographies, which may be associated with geography under a classical conception in the same sense in which non-standard logics are associated with classical logic. 1.
307|The Basic Tools of Formal Ontology|The term ‘formal ontology ’ was first used by the philosopher Edmund Husserl in his Logical Investigations to signify the study of those formal structures and relations – above all relations of part and whole – which are exemplified in the subject-matters of the different material sciences. We follow Husserl in presenting the basic concepts of formal ontology as falling into three groups: the theory of part and whole, the theory of dependence, and the theory of boundary, continuity and contact. These basic concepts are presented in relation to the problem of providing an account of the formal ontology of the mesoscopic realm of everyday experience, and specifically of providing an account of the concept of individual substance.
308|An Ontological Theory of Physical Objects|We discuss an approach to a theory of physical objects and present a logical theory based on a fundamental distinction between objects and their substrates, i.e. chunks of matter and regions of space. The purpose is to establish the basis of a general ontology of space, matter and physical objects for the domain of mechanical artifacts. An extensional mereological framework is assumed for substrates, whereas physical objects are allowed to change their spatial and material substrate while keeping their identity. Besides the parthood relation, simple self-connected region and congruence (or sphere) are adopted as primitives for the description of space. Only threedimensional regions are assumed in the domain. This paper is a revision and slight modification of [Borgo et al. 1996]. 1.
309|Spatial Entities|this paper. However one basic motivation seems easily available. Without going into much detail (see Varzi [1994]), the point is simply that mereological reasoning by itself cannot do justice to the notion of a whole---a self-connected whole, such as a stone or a rope, as opposed to a scattered entity made up of several disconnected parts, such as a broken glass or an archipelago. Parthood is a relational concept, wholeness a global property. And in spite of a widespread tendency to present mereology as a theory of parts and wholes, the latter notion (in its ordinary understanding) cannot be explained in terms of the former. For every whole there is a set of (possibly potential) parts; for every set of parts (i.e., arbitrary objects) there is in principle a complete whole, viz. its mereological sum, or fusion. But there is no way, mereologically, to draw a distinction between &#034;good&#034; and &#034;bad&#034; wholes; there is no way one can rule out wholes consisting of widely scattered or ill assorted entities (the sum consisting of our four eyes and Caesar&#039;s left foot) by reasoning exclusively in terms of parthood. If we allow for the possibility of scattered entities, then we lose the possibility of discriminating them from integral, connected wholes. On the other hand, we cannot just keep the latter without some means of discriminating them from the former.
310|The Ontological Nature of Subject Taxonomies|. Subject based classification is an important part of information retrieval, and has a long history in libraries, where a subject taxonomy was used to determine the location of books on the shelves. We have been studying the notion of subject itself, in order to determine a formal ontology of subject for a large scale digital library card catalog system. Deep analysis reveals a lot of ambiguity regarding the usage of subjects in existing systems and terminology, and we attempt to formalize these notions into a single framework for representing it. 1 Introduction Until recently, library card catalog systems have worked successfully because the amount of material referenced by the system was fairly small. Digital libraries, both formal as in the United States National Digital Library, or informal as in the World Wide Web, promise the potential of billions of electronic documents, and will render the existing card catalog paradigm useless. It has begun already, as web users find themsel...
311|Logical Modelling of Product Knowledge: Towards a Well-Founded Semantics for STEP|The main purpose of the STEP standard is to make possible the integration of product knowledge within the whole enterprise. Under this perspective, the mere exchange of geometric data is not enough, and qualitative knowledge of different kinds needs to be acquired and represented. Here, however, serious semantic problems arise, since the interpretation of the modelling primitives proposed by the standard heavily relies on implicit background knowledge. This problem has been recently underlined in [Metzger 1996], where it is argued that this background knowledge is stable enough and well agreed-upon only in the case of low-level geometric concepts. In the case of more abstract geometric concepts like design features, or non-geometric concepts like part or action, their meaning is not clear enough to be effectively shared by different application protocols. As a result, different interpretations are assumed for the same term in differ
312|Basic Problems of Mereotopology|Mereotopology is today regarded as a major tool for ontological analysis,  and for many good reasons. There are, however, a number of open questions that call  for an answer. Some of them are philosophical, others have direct import for applications,  but all are crucial for a proper assessment of the strengths and limits of  mereotopology. This paper is an attempt to put some order into this still untamed area  of research. I will not attempt any answers. But I shall try to give an idea of the problems,  and of their relevance for the systematic development of formal ontological  theories.
313|The Neutral Representation Project|The evolving complexity of many modern artifacts,  such as aircraft, has led to a serious fragmentation of  knowledge among software systems required for their  design and manufacture. In the case of aircraft design,  views of the same generic design knowledge are redundantly  encoded in multiple software systems, each  system using its own idiosyncractic ontology, and each  system containing that knowledge in an implicit, taskand  vendor-specific form. This situation is expensive,  due to high cost of developing from scratch, maintaining  and keeping synchronized the many systems used  in design.  Boeing&#039;s &#034;Neutral Representation&#034; project aims to address  these concerns by prototyping languages and  methods for making these underlying ontologies and  knowledge explicit, and hence more sharable and  maintainable. We are approaching this goal through  three tasks: Building explicit, neutral, machinesensible  representations of design knowledge; structuring  that knowledge into reusable components, indexed  by the ontologies which they use; and linking those  representations with existing design systems. In this  paper we present the work done this year, and discuss  issues related to ontological engineering and knowledge  sharing which have arisen.  
314|A Two-bit Differentiated Services Architecture for the Internet|This document presents a differentiated services architecture for the internet. Dave Clark and Van Jacobson each presented work on differentiated services at the Munich IETF meeting [2,3]. Each explained how to use one bit of the IP header to deliver a new kind of service to packets in the internet. These were two very different kinds of service with quite different policy assumptions. Ensuing discussion has convinced us that both service types have merit and that both service types can be implemented with a set of very similar mechanisms.
315|Grounding in communication|We give a general analysis of a class of pairs of positive self-adjoint operators A and B for which A + XB has a limit (in strong resolvent sense) as h-10 which is an operator A,  # A! Recently, Klauder [4] has discussed the following example: Let A be the operator-(d2/A2) + x2 on L2(R, dx) and let B = 1 x 1-s. The eigenvectors and eigenvalues of A are, of course, well known to be the Hermite functions, H,(x), n = 0, l,... and E,  = 2n + 1. Klauder then considers the eigenvectors of A + XB (A&gt; 0) by manipulations with the ordinary differential equation (we consider the domain questions, which Klauder ignores, below). He finds that the eigenvalues E,(X) and eigenvectors &amp;(A) do not converge to 8, and H, but rather AO) + (en 4 Ho+, J%(X)-+ gn+1 I n = 0, 2,..., We wish to discuss in detail the general phenomena which Klauder has uncovered. We freely use the techniques of quadratic forms and strong resolvent convergence; see e.g. [3], [5]. Once one decides to analyze Klauder’s phenomenon in the language of quadratic forms, the phenomenon is quite easy to understand and control. In fact, the theory is implicit in Kato’s book [3, VIII.31.
316|Link-Sharing and Resource Management Models for Packet Networks| This paper discusses the use of link-sharing mechanisms in packet networks and presents algorithms for hierarchical link-sharing. Hierarchical link-sharing allows multiple agencies, protocol families, or traflic types to share the bandwidth on a tink in a controlled fashion. Link-sharing and real-time services both require resource management mechanisms at the gateway. Rather than requiring a gateway to implement separate mechanisms for link-sharing and real-time services, the approach in this paper is to view link-sharing and real-time service requirements as simultaneous, and in some respect complementary, constraints at a gateway that can be implemented with a unified set of mechanisms. White it is not possible to completely predict the requirements that might evolve in the Internet over the next decade, we argue that controlled link-sharing is an essential component that can provide gateways with the flexibility to
317|Explicit Allocation of Best-Effort Packet Delivery Service|This paper presents the “allocated-capacity” framework for providing different levels of best-effort service in times of network congestion. The “allocatedcapacity” framework—extensions to the Internet protocols and algorithms—can allocate bandwidth to different users in a controlled and predictable way during network congestion. The framework supports two complementary ways of controlling the bandwidth allocation: sender-based and receiver-based. In today’s heterogeneous and commercial Internet the framework can serve as a basis for charging for usage and for more efficiently utilizing the network resources. We focus on algorithms for essential components of the framework: a differential dropping algorithm for network routers and a tagging algorithm for profile meters at the edge of the network for bulk-data transfers. We present simulation results to illustrate the effectiveness of the combined algorithms in controlling transmission control protocol (TCP) traffic to achieve certain targeted sending rates.
318|Specification of the controlled-load network element service|This document specifies an Internet standards track protocol for the Internet community, and requests discussion and suggestions for improvements. Please refer to the current edition of the &#034;Internet Official Protocol Standards &#034; (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. This memo specifies the network element behavior required to deliver Controlled-Load service in the Internet. Controlled-load service provides the client data flow with a quality of service closely approximating the QoS that same flow would receive from an unloaded network element, but uses capacity (admission) control to assure that this service is received even when the network element is overloaded. 1.
319|Adding Service Discrimination to the Internet|This paper explores extensions to the Internet that can provide discrimination in the service offered to different users in times of network congestion. It proposed a scheme which allows different users to adjust their sending rates to different values during overload. This scheme is contrasted with a number of resource allocation schemes under consideration.
320|Pin: building customized program analysis tools with dynamic instrumentation|Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin’s rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application’s original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin’s versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium R ? , and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website. Categories and Subject Descriptors D.2.5 [Software Engineering]: Testing and Debugging-code inspections and walk-throughs,
321|Atom: A system for building customized program analysis tools|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
322|Automatically characterizing large scale program behavior|Understanding program behavior is at the foundation of computer architecture and program optimization. Many pro-grams have wildly different behavior on even the very largest of scales (over the complete execution of the program). This realization has ramifications for many architectural and com-piler techniques, from thread scheduling, to feedback directed optimizations, to the way programs are simulated. However, in order to take advantage of time-varying behavior, we.must first develop the analytical tools necessary to automatically and efficiently analyze program behavior over large sections of execution. Our goal is to develop automatic techniques that are ca-pable of finding and exploiting the Large Scale Behavior of programs (behavior seen over billions of instructions). The first step towards this goal is the development of a hardware independent metric that can concisely summarize the behav-ior of an arbitrary section of execution in a program. To this end we examine the use of Basic Block Vectors. We quantify the effectiveness of Basic Block Vectors in capturing program behavior across several different architectural met-rics, explore the large scale behavior of several programs, and develop a set of algorithms based on clustering capable of an-alyzing this behavior. We then demonstrate an application of this technology to automatically determine where to simulate for a program to help guide computer architecture research. 1.
323|Eel: Machine-independent executable editing|EEL (Executable Editing Library) is a library for building tools to analyze and modify an executable (compiled) program. The systems and languages communities have built many tools for error detection, fault isolation, architecture translation, performance measurement, simulation, and optimization using this approach of modifying executables. Currently, however, tools of this sort are difficult and timeconsuming to write and are usually closely tied to a particular machine and operating system. EEL supports a machine- and system-independent editing model that enables tool builders to modify an executable without being aware of the details of the underlying architecture or operating system or being concerned with the consequences of deleting instructions or adding foreign code. 1
324|An API for Runtime Code Patching|We present a post-compiler program manipulation tool called Dyninst which provides a C++ class library for program instrumentation. Using this library, it is possible to instrument and modify application programs during execution. A unique feature of this library is that it permits machine-independent binary instrumentation programs to be written. We describe the interface that a tool sees when using this library. We also discuss three simple tools built using this interface: a utility to count the number of times a function is called, a program to capture the output of an already running program to a file, and an implementation of conditional break points. For the conditional breakpoint example, we show that by using our interface compared with gdb we are able to execute a program with conditional breakpoints up to 900 times faster.  1. Introduction  The normal cycle of developing a program is to edit source code, compile it, and then execute the resulting binary. However, sometimes t...
325|Valgrind: A program supervision framework|a;1
326|Dynamic instrumentation of production systems|Rights to individual papers remain with the author or the author&#039;s employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein.
327|The Jalapeño Dynamic Optimizing Compiler for Java|The JalapeÃño Dynamic Optimizing Compiler is a key component of the JalapeÃño Virtual Machine, a new Java Virtual Machine (JVM) designed to support efficient and scalable execution of Java applications on SMP server machines. This paper describes the design of the JalapeÃño Optimizing Compiler, and the implementation results that we have obtained thus far. To the best of our knowledge, this is the first dynamic optimizing compiler for Java that is being used in a JVM with a compile-only approach to program execution.
328|The Multiflow Trace Scheduling Compiler|The Multiflow compiler uses the trace scheduling algorithm to find and exploit instruction-level parallelism beyond basic blocks. The compiler generates code for VLIW computers that issue up to 28 operations each cycle and maintain more than 50 operations in flight. At Multiflow the compiler generated code for eight different target machine architectures and compiled over 50 million lines of FORTRAN and C applications and systems code. The requirement of finding large amounts of parallelism in ordinary programs, the trace scheduling algorithm, and the many unique features of the Multiflow hardware placed novel demands on the compiler. New techniques in instruction scheduling, register allocation, memory-bank management, and intermediate-code optimizations were developed, as were refinements to reduce the overhead of trace scheduling. This paper describes the Multiflow compiler and reports on the Multiflow practice and experience with compiling for instruction-level parallelism beyond basic blocks.
330|Vulcan Binary transformation in a distributed environment|Distributed computing on the Internet presents new challenges and opportunities for tools that inspect and modify program binaries. The dynamic and heterogeneous nature of the Internet environment extends the traditional product development process by requiring program development tools like these, which were once used only internally, to work in live environments too. The concept of compilation process must be expanded along with the capabilities of the binary tools. This paper presents Vulcan, a second-generation technology that addresses many of these challenges. Vulcan provides both static and dynamic code modification and provides a framework for cross-component analysis and optimization. It provides system-level analysis for heterogeneous binaries across instruction sets. Vulcan works in the Win32 environment and can process x86, IA64, and MSIL binaries. Vulcan scales to large commercial applications and has been used to improve performance and reliability of Microsoft products in a production environment. 1.
331|Instrumentation and Optimization of Win32/Intel Executables Using Etch|Etch is a general-purpose tool for rewriting arbitrary Win32/x86 binaries without requiring source code. Etch provides a framework for modifying executables for both measurement and optimization. Etch handles the complexities of the Win32 executable file format and the x86 instruction set, allowing tool builders to focus on specifying transformations. Etch also handles the complexities of the Win32 execution environment, allowing tool users to focus on performing experiments. This paper describes Etch and some of the tools that we have built using Etch, including a hierarchical call graph profiler and an instruction layout optimization tool. 1 Introduction During the last decade, the Intel x86 instruction set has become a mainstay of the computing industry. Arguably, Intel processors have executed more instructions than all other computers ever built. Despite the widespread use of Intel processors and applications, however, few tools are available to assist the programmer and user in...
332|Pinpointing representative portions of large Intel Itanium programs with dynamic instrumentation|Detailed modeling of the performance of commercial applications is difficult. The applications can take a very long time to run on real hardware and it is impractical to simulate them to completion on performance models. Furthermore, these applications have complex execution environments that cannot easily be reproduced on a simulator, making porting the applications to simulators difficult. We attack these problems using the well-known SimPoint methodology to find representative portions of an application to simulate, and a dynamic instrumentation framework called Pin to avoid porting altogether. Our system uses dynamic instrumentation instead of simulation to find representative portions — called Pin-Points — for simulation. We have developed a toolkit that automatically detects PinPoints, validates whether they are representative using hardware performance counters, and generates traces for large Itanium ® programs. We compared SimPoint-based selection to random selection of simulation points. We found for 95 % of the SPEC2000 programs we tested, the PinPoints prediction was within 8 % of the actual whole-program CPI, as opposed to 18% for random selection. We measure the end-to-end error, comparing real hardware to a performance model, and have a simple and efficient methodology to determine the step that introduced the error. Finally, we evaluate the system in the context of multiple configurations of real hardware, commercial applications, and industrialstrength performance models to understand the behavior of a complete and practical workload collection system. We have successfully used our system with many commercial Itanium ® programs, some running for trillions of instructions, and have used the resulting traces for predicting performance of those applications on future Itanium processors. 1.
333| System Support for Automatic Profiling and Optimization |The Morph system provides a framework for automatic collection and management of profile information and application of profile-driven optimizations. In this paper, we focus on the operating system support that is required to collect and manage profile information on an end-user’s workstation in an automatic, continuous, and transparent manner. Our implementation for a Digital Alpha machine running Digital UNIX 4.0 achieves run-time overheads of less than 0.3 % during profile collection. Through the application of three code layout optimizations, we further show that Morph can use statistical profiles to improve application performance. With appropriate system support, automatic profiling and optimization is both possible and effective. 
334|The StarJIT Compiler: A Dynamic Compiler for Managed Runtime Environments|Dynamic compilers (or Just-in-Time [JIT] compilers) are a key component of managed runtime environments. This paper describes the design and implementation of the StarJIT compiler, a dynamic compiler for Java Virtual Machines and Common Language Runtime platforms. The goal of the StarJIT compiler is to build an infrastructure to research the influence of managed runtime environments on Intel architectures. The StarJIT compiler can compile both Java    Infrastructure (CLI) bytecodes, and it uses a single intermediate representation and global optimization framework for both Java and CLI. The StarJIT compiler is designed to generate optimized code for the major Intel architectures and currently targets two Intel architectures: IA-32 and the Itanium    Processor Family.
335|Ispike: A post-link optimizer for the intel itanium architecture|Ispike is a post-link optimizer developed for the Intel R Itanium Processor Family (IPF) processors. The IPF architecture poses both opportunities and challenges to post-link optimizations. IPF offers a rich set of performance counters to collect detailed profile information at a low cost, which is essential to post-link optimization being practical. At the same time, the predication and bundling features on IPF make post-link code transformation more challenging than on other architectures. In Ispike, we have implemented optimizations like code layout, instruction prefetching, data layout, and data prefetching that exploit the IPF advantages, and strategies that cope with the IPF-specific challenges. Using SPEC CINT2000 as benchmarks, we show that Ispike improves performance by as much as 40 % on the Itanium R 2 processor, with average improvement of 8.5% and 9.9 % over executables generated by the Intel R Electron compiler and by the Gcc compiler, respectively. We also demonstrate that statistical profiles collected via IPF performance counters and complete profiles collected via instrumentation produce equal performance benefit, but the profiling overhead is significantly lower for performance counters.
336|Hardware Support for Control Transfers in Code Caches|Many dynamic optimization and/or binary translation systems hold optimized/translated superblocks in a code cache. Conventional code caching systems suffer from overheads when control is transferred from one cached superblock to another, especially via register-indirect jumps. The basic problem is that instruction addresses in the code cache are different from those in the original program binary. Therefore, performance for register-indirect jumps depends on the ability to translate efficiently from source binary PC values to code cache PC values. We analyze
337|Low-Power CMOS Digital Design| Motivated by emerging battery-operated applications that demand intensive computation in portable environments, techniques are investigated which reduce power consumption in CMOS digital circuits while maintaining computational throughput. Techniques for low-power operation are shown which use the lowest possible supply voltage coupled with architectural, logic style, circuit, and technology optimizations. An architectural-based scaling strategy is presented which indicates that the optimum voltage is much lower than that determined by other scaling considerations. This optimum is achieved by trading increased silicon area for reduced power consumption. 
338|Content-based image retrieval at the end of the early years|The paper presents a review of 200 references in content-based image retrieval. The paper starts with discussing the working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap. Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrieval sorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by: accumulative and global features, salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures is reviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable of giving by interaction. We briefly discuss aspects of system engineering: databases, system architecture, and evaluation. In the concluding section, we present our view on: the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap.
339|A Model of Saliency-based Visual Attention for Rapid Scene Analysis|A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail. Index terms: Visual attention, scene analysis, feature extraction, target detection, visual search. \Pi I. Introduction Primates have a remarkable ability to interpret complex scenes in real time, despite the limited speed of the neuronal hardware available for such tasks. Intermediate and higher visual processes appear to select a subset of the available sensory information before further processing [1], most likely to reduce the complexity of scene analysis [2]. This selection appears to be implemented in the ...
340|An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions|Consider a set S of n data points in real d-dimensional space, R d , where distances are measured using any Minkowski metric. In nearest neighbor searching we preprocess S into a data structure, so that given any query point q 2 R d , the closest point of S to q can be reported quickly. Given any positive real ffl, a data point p is a (1 + ffl)-approximate nearest neighbor of q if its distance from q is within a factor of (1 + ffl) of the distance to the true nearest neighbor. We show that it is possible to preprocess a set of n points in R d in O(dn log n) time and O(dn) space, so that given a query point q 2 R d , and ffl ? 0, a (1 + ffl)-approximate nearest neighbor of q can be computed in O(c d;ffl log n) time, where c d;ffl d d1 + 6d=ffle d is a factor depending only on dimension and ffl. In general, we show that given an integer k 1, (1 + ffl)-approximations to the k nearest neighbors of q can be computed in additional O(kd log n) time.
341|FastMap: A Fast Algorithm for Indexing, Data-Mining and Visualization of Traditional and Multimedia Datasets|A very promising idea for fast searching in traditional and multimedia databases is to map objects into points in k-d  space, using k feature-extraction functions, provided by a domain expert [25]. Thus, we can subsequently use highly fine-tuned spatial access methods (SAMs), to answer several types of queries, including the `Query By Example&#039; type (which translates to a range query); the `all pairs&#039; query (which translates to a spatial join [8]); the nearest-neighbor or best-match query, etc. However, designing feature extraction functions can be hard. It is relatively easier for a domain expert to assess the similarity/distance of two objects. Given only the distance information though, it is not obvious how to map objects into points. This is exactly the topic of this paper. We describe a fast algorithm to map objects into points in some k-dimensional  space (k is user-defined), such that the dis-similarities are preserved. There are two benefits from this mapping: (a) efficient ret...
342|The SR-tree: An Index Structure for High-Dimensional Nearest Neighbor Queries|Recently, similarity queries on feature vectors have been widely used to perform content-based retrieval of images. To apply this technique to large databases, it is required to develop multidimensional index structures supporting nearest neighbor queries e ciently. The SS-tree had been proposed for this purpose and is known to outperform other index structures such as the R*-tree and the K-D-B-tree. One of its most important features is that it employs bounding spheres rather than bounding rectangles for the shape of regions. However, we demonstrate in this paper that bounding spheres occupy much larger volume than bounding rectangles with high-dimensional data and that this reduces search efficiency. To overcome this drawback, we propose a new index structure called the SR-tree (Sphere/Rectangle-tree) which integrates bounding spheres and bounding rectangles. A region of the SR-tree is specified by the intersection of a bounding sphere and a bounding rectangle. Incorporating bounding rectangles permits neighborhoods to be partitioned into smaller regions than the SS-tree and improves the disjointness among regions. This enhances the performance on nearest neighbor queries especially for highdimensional and non-uniform data which can be practical in actual image/video similarity indexing. We include the performance test results that verify this advantage of the SR-tree and show that the SR-tree outperforms both the SS-tree and the R*-tree.   
343|The Bayesian image retrieval system, PicHunter: Theory, implementation, and psychophysical experiments| This paper presents the theory, design principles, implementation, and performance results of PicHunter, a prototype content-based image retrieval (CBIR) system that has been developed over the past three years. In addition, this document presents the rationale, design, and results of psychophysical experiments that were conducted to address some key issues that arose during PicHunter’s development. The PicHunter project makes four primary contributions to research on content-based image retrieval. First, PicHunter represents a simple instance of a general Bayesian framework we describe for using relevance feedback to direct a search. With an explicit model of what users would do, given what target image they want, PicHunter uses Bayes’s rule to predict what is the target they want, given their actions. This is done via a probability distribution over possible image targets, rather than by refining a query. Second, an entropy-minimizing display algorithm is described that attempts to maximize the information obtained from a user at each iteration of the search. Third, PicHunter makes use of hidden annotation rather than a possibly inaccurate/inconsistent annotation structure that the user must learn and make queries in. Finally, PicHunter introduces two experimental paradigms to quantitatively evaluate the performance of the system, and psychophysical experiments are presented that support the theoretical claims.  
344|Content-based representation and retrieval of visual media: A state-of-the-art review|This paper reviews a number of recently available techniques in contentanalysis of visual media and their application to the indexing, retrieval,abstracting, relevance assessment, interactive perception, annotation and re-use of visualdocuments. 1. Background A few years ago, the problems of representation and retrieval of visualmedia were confined to specialized image databases (geographical, medical, pilot experimentsin computerized slide libraries), in the professional applications of the audiovisualindustries (production, broadcasting and archives), and in computerized training or education. The presentdevelopment of multimedia technology and information highways has put content processing of visualmedia at the core of key application domains: digital and interactive video, large distributed digital libraries, multimedia publishing. Though the most important investments have been targeted at the information infrastructure (networks, servers, coding and compression, deliverymodels, multimedia systems architecture), a growing number of researchers have realized thatcontent processing will be a key asset in putting together successful applications. The need for contentprocessing techniques has been made evident from a variety of angles, ranging from achievingbetter quality in compression, allowing user choice of programs in video-on-demand, achieving betterproductivity in video production, providing access to large still image databases or integrating still images and video in multimedia publishing and cooperative work. Content-based retrieval of visual media and representation of visualdocuments in human-computer interfaces are based on the availability of content representationdata (time-structure for
345|Pictoseek: combining color and shape invariant features for image retrieval |Abstract—We aim at combining color and shape invariants for indexing and retrieving images. To this end, color models are proposed independent of the object geometry, object pose, and illumination. From these color models, color invariant edges are derived from which shape invariant features are computed. Computational methods are described to combine the color and shape invariants into a unified high-dimensional invariant feature set for discriminatory object retrieval. Experiments have been conducted on a database consisting of 500 images taken from multicolored man-made objects in real world scenes. From the theoretical and experimental results it is concluded that object retrieval based on composite color and shape invariant features provides excellent retrieval accuracy. Object retrieval based on color invariants provides very high retrieval accuracy whereas object retrieval based entirely on shape invariants yields poor discriminative power. Furthermore, the image retrieval scheme is highly robust to partial occlusion, object clutter and a change in the object’s pose. Finally, the image retrieval scheme is integrated into the PicToSeek system on-line at
346|Shape-Based Retrieval: A Case Study with Trademark Image Databases|Retrieval efficiency and accuracy are two important issues in designing a content-based database retrieval system. We propose a method for trademark image database retrieval based on object shape information that would supplement traditional text-based retrieval systems. This system achieves both the desired efficiency and accuracy using a two-stage hierarchy: in the first stage, simple and easily computable shape features are used to quickly browse through the database to generate a moderate number of plausible retrievals when a query is presented; in the second stage, the candidates from the first stage are screened using a deformable template matching process to discard spurious matches. We have tested the algorithm using hand drawn queries on a trademark database containing 1; 100 images. Each retrieval takes a reasonable amount of computation time (¸ 4-5 seconds on a Sun Sparc 20 workstation). The top most image retrieved by the system agrees with that obtained by human subjects, ...
347|Convexity Rule for Shape Decomposition Based on Discrete Contour Evolution|We concentrate here on decomposition of 2D objects into meaningful parts of visual form,orvisual parts. It is a simple observation that convex parts of objects determine visual parts. However, the problem is that many significant visual parts are not convex, since a visual part may have concavities. We solve this problem by identifying convex parts at different stages of a proposed contour evolution method in which significant visual parts will become convex object parts at higher stages of the evolution. We obtain a novel rule for decomposition of 2D objects into visual parts, called the hierarchical convexity rule, which states that visual parts are enclosed by maximal convex (with respect to the object) boundary arcs at different stages of the contour evolution. This rule determines not only parts of boundary curves but directly the visual parts of objects. Moreover, the stages of the evolution hierarchy induce a hierarchical structure of the visual parts. The more advanced the stage of contour evolution, the more significant is the shape contribution of the obtained visual parts. c ? 1999 Academic Press Key Words: visual parts; discrete curve evolution; digital curves; digital straight line segments; total curvature; shape hierarchy; digital geometry. 1.
348|A parallel computing approach to creating engineering concept spaces for semantic retrieval: The Illinois Digital Library Initiative project|Abstract-This research presents preliminary results generated from the semantic retrieval research component of the Illinois Digital Library Initiative (DLI) project. Using a variation of the automatic thesaurus generation techniques, to which we refer as the concept space approach, we aimed to create graphs of domain-specific concepts (terms) and their weighted co-occurrence relationships for all major engineering domains. Merging these concept spaces and providing traversal paths across different concept spaces could potentially help alleviate the vocabulary (difference) problem evident in large-scale information retrieval. We have experimented previously with such a technique for a smaller molecular biology domain (Worm Community System, with IO+ MBs of document collection) with encouraging results. In order to address the scalability issue related to large-scale information retrieval and analysis for the current Illinois DLI project, we recently conducted experiments using the concept space approach on parallel supercomputers. Our test collection included 2+ GBs of computer science and electrical engineering abstracts extracted from the INSPEC database. The concept space approach called for extensive textual and statistical analysis (a form of knowledge discovery) based on automatic indexing and cooccurrence analysis algorithms, both previously tested in the biology domain. Initial testing results using a 512-node CM-5 and a 16processor SGI Power Challenge were promising. Power Challenge was later selected to create a comprehensive computer engineering concept space of about 270,000 terms and 4,000,000+ links using 24.5 hours of CPU time. Our system evaluation involving 12 knowledgeable subjects revealed that the automatically-created computer engineering concept space generated
349|A Knowledge-Based Approach for Retrieving Images by Content|A knowledge-based approach is introduced for retrieving images by content. It supports the answering of conceptual image queries involving similar-to predicates, spatial semantic operators, and references to conceptual terms. Interested objects in the images are represented by contours segmented from images. Image content such as shapes and spatial relationships are derived from object contours according to domain-specific image knowledge. A three-layered model is proposed for integrating image representations, extracted image features, and image semantics. With such a model, images can be retrieved based on the features and content specified in the queries. The knowledge-based query processing is based on a query relaxation technique. The image features are classified by an automatic clustering algorithm and represented by Type Abstraction Hierarchies (TAHs) for knowledge-based query processing. Since the features selected for TAH generation are based on context and user profile, and ...
350|Geometric and Illumination Invariants for Object Recognition|We propose invariant formulations that can potentially be combined into a single system. In particular# we describe a framework for computing invariant features which are insensitiveto rigid motion# a#ne transform# changes of parameterization and scene illumination# perspective transform# and view point change. This is unlike most current research on image invariants which concentrates on either geometric or illumination invariants exclusively. The formulations are widely applicable to many popular basis representations# such as wavelets #3# 4# 24# 25## short#time Fourier analysis #13#35## and splines #2# 5#37#. Exploiting formulations that examine information about shape and color at di#erent resolution levels# the new approachisneither strictly global nor local. It enables a quasi#localized# hierarchical shape analysis which is rarely found in other known invariant techniques# such as global invariants. Furthermore# it does not require estimating high#order derivatives in computing i...
351|Reliable and Efficient Pattern Matching Using an Affine Invariant Metric|In the field of pattern matching, there is a clear trade-off between  effectiveness, accuracy and robustness on one hand and efficiency and  simplicity on the other hand. For example, matching patterns more  effectively by using a more general class of transformations usually  results in a considerable increase of computational complexity. In this  paper, we introduce a general pattern matching approach which will be  applied to a new measure called the absolute difference. This patternsimilarity  measure is affine invariant, which stands out favourably in  practical use. The problem of finding a transformation mapping to the  minimal absolute difference, like many pattern matching problems, has  a high computational complexity. Therefore, we base our algorithm on  a hierarchical subdivision of transformation space. The method applies  to any affine group of transformations, allowing optimisations for rigid  motion. Our implementation of the method performs well in terms of  reliabilit...
352|A novel vector-based approach to color image retrieval using a vector angular-based distance measure|Color is the characteristic which is most used for image indexing and retrieval. Due to its simplicity, the color histogram remains the most commonly used method for this task. However, the lack of good perceptual histogram similarity measures, the global color content of histograms, and the erroneous retrieval results due to gamma nonlinearity, call for improved methods. We present a new scheme which implements a recursive HSV-space segmentation technique to identify perceptually prominent color areas. The average color vector of these extracted areas are then used to build the image indices, requiring very little storage. Our retrieval is performed by implementing a combination distance measure, based on the vector angle between two vectors. Our system provides accurate retrieval results and high retrieval rate. It allows for queries based on single or multiple colors and, in addition, it allows for certain colors to be excluded in the query. This flexibility is due to our distance measure and the multidimensional query space in which the retrieval ranking of the database images is determined. Furthermore, our scheme proves to be very resistant to gamma nonlinearity providing robust retrieval results for a wide range of gamma nonlinearity values, which proves to be of great importance since, in general, the image acquisition source is unknown. c ? 1999 Academic Press I.
353|Multiscale Texture Segmentation using Wavelet-Domain Hidden Markov Models|Wavelet-domain Hidden Markov Tree (HMT) models are powerful tools for modeling the statistical properties of wavelet transforms. By characterizing the joint statistics of the wavelet coefficients, HMTs efficiently capture the characteristics of a large class of real-world signals and images. In this paper, we apply this multiscale statistical description to the texture segmentation problem. Using the inherent tree structure of the HMT, we classify textures at various scales and then fuse these decisions into a reliable pixel-by-pixel segmentation.   1 Introduction  The goal of an image segmentation algorithm is to assign a class label to each pixel of an image based on the properties of the pixels and their relationships with their neighbors. The segmentation process is a joint detection and estimation of the class labels and shapes of regions with homogeneous behavior. For proper segmentation of images, both the large and small scale behaviors should be utilized to segment both large,...
354|Document image database retrieval and browsing using texture analysis|A system is presented that uses texture to retrieve and browse images stored in a large document image database. A method of graphically generating a candidate search image is used that shows the visual layout and content of a target document. All images similar to this candidate are returned for the purpose of browsing orfurther query. The system is accessed using a World wide Web (Web) browser Applications include the retrieval and browsing of document images including newspapers, faxes and business letters. A system is described in this paper that allows for the retrieval of document images based on such non-text features. The system includes a graphical user interface that allows the user to specify the visual characteristics of a query document. From this description, a set of features are generated that are matched against a database of document images. We use texture to describe the types of features in the document. The target document is known only to have a certain typo of layout and content, which corresponds to a texture measure for that document. Texture in effect becomes the search key for a document. 1.
355|Line pattern retrieval using relational histograms| This paper presents a new compact shape representation for retrieving line-patterns from large databases. The basic idea is to exploit both geometric attributes and structural information to construct a shape histogram. We realize this goal by computing the N-nearest neighbor graph for the lines-segments for each pattern. The edges of the neighborhood graphs are used to gate contributions to a two-dimensional pairwise geometric histogram. Shapes are indexed by searching for the line-pattern that maximizes the cross correlation of the normalized histogram bin-contents. We evaluate the new method on a database containing over 2,500 line-patterns each composed of hundreds of lines.  
356|Semiotics and Agents for Integrating and Navigating through Multimedia Representations of Concepts|The purpose of this paper is two-fold. We begin by exploring the emerging trend to view multimedia information in terms of low-level and high-level components; the former being feature-based and the latter the \semantics&#034; intrinsic to what is portrayed by the media object. Traditionally, this has been viewed by employing analogies with generative linguistics (e.g. compositional semantics). Recently, a new perspective based on the semiotic tradition has been alluded to in several papers. We believe this to be a more appropriate approach. From this, we propose an approach for tackling this problem which uses an associative data structure expressing authored information together with intelligent agents acting autonomously over this structure. We then show how neural networks can be used to implement such agents. The agents act as \vehicles&#034; for bridging the gap between multimedia semantics and concrete expressions of high-level knowledge, but we suggest that traditional neural network tec...
357|Algebraic and Geometric Tools to Compute Projective and Permutation Invariants|. This paper studies the computation of projective invariants in pairs of images from uncalibrated cameras, and presents a detailed study of the projective and permutation invariants for configurations of points and/or lines. We give two basic computational approaches, one algebraic and one geometric, and also the relations between the invariants computed by different approaches. In each case, we show how to compute invariants in projective space assuming that the points and lines have already been reconstructed in an arbitrary projective basis, and also, how to compute them directly from image coordinates in a pair of views using only point and line correspondences and the fundamental matrix. Finally, we develop combinations of those projective invariants which are insensitive to permutations of the geometric primitives of each of the basic configurations.  Introduction  Various visual or visually-guided robotics tasks may be carried out using only a projective representation which sh...
358|Intelligence Without Representation|Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporateeverything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environ...
359|Temporal and modal logic|We give a comprehensive and unifying survey of the theoretical aspects of Temporal and  modal logic.
360|Dynamic Logic|ed to be true under the valuation u iff there exists an a 2 N such that the formula x    = y is true under the valuation u[x=a], where u[x=a] agrees with u everywhere except x, on which it takes the value a. This definition involves a metalogical operation that produces u[x=a] from u for all possible values a 2 N.  This operation becomes explicit in DL in the form of the program x := ?, called a nondeterministic or wildcard assignment. This is a rather unconventional program, since it is not effective; however, it is quite useful as a descriptive tool. A more conventional way to obtain a square root of y, if it exists, would be the program  x := 0 ; while x    &lt; y do x := x + 1: (1) In DL, such programs are first-class objects on a par with formulas, complete with a collection of operators for forming compound programs inductively from a basis of primitive programs. To discuss the effect of the execution of a program  on the truth of a formula &#039;, DL uses a modal construct &lt;&gt;&#039;, which 
361|Intelligence without reason|Computers and Thought are the two categories that together define Artificial Intelligence as a discipline. It is generally accepted that work in Artificial Intelligence over the last thirty years has had a strong influence on aspects of computer architectures. In this paper we also make the converse claim; that the state of computer architecture has been a strong influence on our models of thought. The Von Neumann model of computation has lead Artificial Intelligence in particular directions. Intelligence in biological systems is completely different. Recent work in behavior-based Artificial Intelligence has produced new models of intelligence that are much closer in spirit to biological systems. The non-Von Neumann computational models they use share many characteristics with biological computation.
362|The Role of Emotion in Believable Agents|Articial intelligence researchers attempting to create engaging  apparently living creatures may nd important insight in the work of artists who have explored the idea of believable character  In particular  appropriately timed and clearly expressed emotion is a central requirement for believable characters  We discuss these ideas and suggest how they may apply to believable interactive characters  which we call believable agents This work was supported in part by Fujitsu Laboratories and Mitsubishi Electric Research Laborato ries  The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ocial policies  either expressed or implied  of any other parties Keywords  articial intelligence  emotion  believable agents art  animation  believable characters  BELIEVABILITY   Believability There is a notion in the Arts of believable character  It does not mean an honest or reliable character  but one that provides the illusion of life  and thus permits the audience s suspension of disbelief The idea of believability has long been studied and explored in literature  theater lm  radio drama  and other media  Traditional character animators are among those artists who have sought to create believable characters  and the Disney animators of the   	 s made great strides toward this goal  The rst page of the enormous classic reference work on Disney animation Thomas and Johnston     begins with these words Disney animation makes audiences really believe in   characters  whose adventures and misfortunes make people laugh  and even cry  There is a special ingredient in our type of animation that produces drawings that appear to think and make decisions and act of their own volition  it is what creates the illusion of life Many articial intelligence researchers have long wished to build robots  and their cousins called agents  that seem to think  feel  and live  These are creatures with whom you	d want to share some of your life  as with a companion  or a social pet For instance  in his 
363|Plans And Resource-Bounded Practical Reasoning|An architecture for a rational agent must allow for means-end reasoning, for the weighing of competing alternatives, and for interactions between these two forms of reasoning. Such an architecture must also address the problem of resource boundedness. We sketch a solution of the first problem that points the way to a solution of the second. In particular, we present a high-level specification of the practical-reasoning component of an architecture for a resource-bounded rational agent. In this architecture, a major role of the agent&#039;s plans is to constrain the amount of further practical reasoning she must perform.
364|Software Agents|this paper, we discuss these questions and describe some emerging technologies that provide answers. In the final section, we mention some additional issues and summarize the key points of the paper. (For more information on agent-based software engineering, see [Genesereth 1989] and [Genesereth 1992]. See also [Shoham 1993] for a description of a variation of agent-based software engineering known as &#034;agent-oriented programming&#034;.) 2. Agent Communication Language
365|Elephants don&#039;t play chess|Engineering and Computer Science at M.I.T. and a member of the Artificial Intelligence Laboratory where he leads the mobile robot group. He has authored two books, numerous scientific papers, and is the editor of the International Journal of Computer Vision. There is an alternative route to Artificial Intelligence that diverges from the directions pursued under that banner for the last thirty some years. The traditional approach has emphasized the abstract manipulation of symbols, whose grounding, in physical reality has. rarely been achieved. We explore a research methodology which emphasizes ongoing physical interaction with the environment as the primary source of constraint on the design of intelligent systems. We show how this methodology has recently had significant successes on a par with the most successful classical efforts. We outline plausible future work along these lines which can lead to vastly more ambitious systems. 1.
366|Decision-Making in an Embedded Reasoning System |The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate e ectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability toreact rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.  
367|The Role of Common Ontology in Achieving Sharable, Reusable Knowledge Bases|Although AI research and commercial system development depend on bodies  of formally represented knowledge that are expensive and difficult to construct,  current knowledge base design does not support the accumulation or reuse  of such knowledge. This paper presents a strategy for building libraries of  sharable, reusable knowledge in which common ontologies play a central role  as a knowledge coupling construct. Ontologies are defined as coherent sets  of representational terms, together with textual and formal definitions, that  embody a set of representational design choices. Problems in the design of  sharable ontologies are identified.  Research and development in AI is impeded by an inability to share and reuse knowledge bases. The expense of building serious knowledge bases and the lack of means to exchange them with colleagues makes it difficult to generate, evaluate, and build on research results which depend on &#034;domain theories&#034; or &#034;background knowledge.&#034; Similarly, commerci...
368|Belief, awareness, and limited reasoning|Several new logics for belief and knowledge are introduced and studied, all of which have the property that agents are not logically omniscient. In particular, in these logics, the set of beliefs of an agent does not necessarily contain all valid formulas. Thus, these logics are more suitable than traditional logics for modelling beliefs of humans (or machines) with limited reasoning capabilities. Our first logic is essentially an extension of Levesque&#039;s logic of implicit and explicit belief, where we extend to allow multiple agents and higher-level belief (i.e., beliefs about beliefs). Our second logic deals explicitly with &#034;awareness,&#034; where, roughly speaking, it is necessary to be aware of a concept before one can have beliefs about it. Our third logic gives a model of &#034;local reasoning,&#034; where an agent is viewed as a &#034;society of minds,&#034; each with its own cluster of beliefs, which may contradict each other.  
369|A Survey of Concurrent METATEM - The Language and its Applications|. In this paper we present a survey of work relating to the Concurrent  METATEM programming language. In addition to a description of the basic Concurrent  METATEM system, which incorporates the direct execution of temporal formulae, a variety of extensions that have either been implemented or proposed are outlined. Although still in the development stage, there appear to be many areas where such a language could be applied. We present a variety of sample applications, highlighting the particular features of Concurrent METATEM that we believe will make it appropriate for use in these areas. 1 Introduction  Concurrent METATEM is a language based upon the direct execution of temporal formulae [15]. It consists of two distinct aspects: an execution mechanism for temporal formulae in a particular form; and an operational model that treats single executable temporal logic programs as asynchronously executing objects in a concurrent objectbased system. The motivation for the development of t...
370|Integrating Reactivity, Goals, and Emotion in a Broad Agent|Researchers studying autonomous agents are increasingly examining the problem of integrating multiple capabilities into single agents. The Oz project is developing technology for dramatic, interactive, simulated worlds. One requirement of such worlds is the presence of broad, though perhaps shallow, agents. To support our needs, we are developing an agent architecture, called Tok, that displays reactivity, goal-directed behavior, and emotion, along with other capabilities. Integrating the components of Tok into a coherent whole raises issues of how the parts interact, and seems to place constraints on the nature of each component. Here we describe briefly the integration issues we have encountered in building a particular Tok agent (Lyotard the cat), note their impact on the architecture, and suggest that modeling emotion, in particular, may constrain the design of integrated agent architectures.   
371|TouringMachines: An Architecture for Dynamic, Rational, Mobile Agents|ion-Partitioned Evaluator (APE) architecture which has been tested in a simulated, single-agent, indoor navigation domain [SH90].  The APE architecture is composed of a number of concurrent, hierarchically abstract action control layers, each representing and reasoning about some particular aspect of the agent&#039;s task domain. Implemented as a parallel blackboard-based planner, the five layers --- sensor/motor, spatial, temporal, causal, and conventional (general knowledge) --- effectively partition the agent&#039;s data processing duties along a number of dimensions including temporal granularity, information/resource use, and functional abstraction. Perceptual information flows strictly from the agent sensors (connected to the sensor /motor level) toward the higher levels, while command or goal-achievement information flows strictly downward towards the agent&#039;s effectors (also connected to the sensor/motor level).  Besides mechanisms for communicating with other layers, each layer in the AP...
372|Architectural Foundations for Real-Time Performance in Intelligent Agents|Intelligent agents perform multiple concurrent tasks requiring both knowledge-based reasoning and interaction with dynamic entities in the environment, under real-time constraints. Because an agent&#039;s opportunities to perceive, reason about, and act upon the environment typically exceed its computational resources, it must determine which operations to perform and when to perform them so as to achieve its most important objectives in a timely manner. Accordingly, we view the problem of real-time performance as a problem in intelligent real-time control. We propose and define several important control requirements and present an agent architecture that is designed to address those requirements. The proposed architecture is a blackboard architecture, whose key features include: distribution of perception, action, and cognition among parallel processes, limited-capacity I/O buffers with best-first retrieval and worst-first overflow, dynamic control planning, dynamic focus of attention, and...
373|What Can Machines Know? On the Properties of Knowledge in Distributed Systems|It has been argued that knowledge is a useful tool for designing and analyzing  complex systems. The notion of knowledge that seems most relevant in this context  is an external, information-based notion that can be shown to satisfy all the axioms  of the modal logic S5. We carefully examine the properties of this notion of knowledge  and show that they depend crucially, and in subtle ways, on assumptions we  make about the system and about the language used for describing knowledge. We  present a formal model in which we can capture various assumptions frequently  made about systems, such as whether they are deterministic or nondeterministic,  whether knowledge is cumulative (which means that processes never &#034;forget&#034;), and  whether or not the &#034;environment&#034; affects the state transitions of the processes. We  then show that under some assumptions about the system and the language, certain  states of knowledge are not attainable and the axioms of S5 do not completely  characterize the pr...
374|Representing and Executing Agent-Based Systems|In this paper we describe an approach to the representation and implementation  of agent-based systems where the behaviour of an individual agent is  represented by a set of logical rules in a particular form. This not only provides a  logical specification of the agent, but also allows us to directly execute the rules in  order to implement the agent&#039;s behaviour. Agents communicate with each other  through a simple, and logically well-founded, broadcast communication mechanism.
375|On Being Responsible|this paper is to provide a framework in which one particular class of social activity can be formalised and ultimately analysed: namely that in which a group of autonomous agents (at least two) decides they wish to work together as a team to solve a common problem. A comprehensive theory describing this class of social interaction would need to cover at least the following aspects: when to initiate team activity, how to go about assembling the team, how to plan and distribute work within the team, how to behave once team activity has been initiated and how to complete team activity. The framework described herein defines the prerequisites for such action and also prescribes how agents should behave (both in their own problem solving and with respect to other group members) once the problem solving has been established. Typically in a community of autonomous agents, one of the primary motives for joint action is when no individual is capable of achieving a desired objective alone; only by combining and coordinating with others can the target be reached. Joint action is usually a reciprocal process in which participating agents augment their objectives and problem solving to comply with those of others - hence it is a fairly sophisticated form of cooperation. It requires greater knowledge, awareness and reflection by an agent both with respect to its own problem solving objectives and about their compatibility with the objectives of others, than simpler forms of social interaction (such as task and result sharing [19]). Joint action, by definition, requires an objective the group wishes to achieve - it is the glue which binds the team together. As a consequence of the autonomous nature of the agents, team members will only participate if they can derive some benefit from ...
376|A Logic of Relative Desire|: Although many have proposed formal characterizations of belief structures as bases for rational action, the problem of characterizing rational desires has attracted little attention. AI relies heavily on goal conditions interpreted (apparently) as absolute expressions of desirability, but these cannot express varying degrees of goal satisfaction or preferences among alternative goals. Our previous work provided a relative interpretation of goals as qualitative statements about preferability, all else equal. We extend that treatment to the comparison of arbitrary propositions, and develop a propositional logic of relative desire suitable for formalizing properties of planning and problem-solving methods. 1 Introduction Question your desires.---William Shakespeare Standard theories of rational action take decisions of the agent to depend on beliefs about the relative desirability of the results of its available actions [2, 3]. The predominant approach to planning in artificial intelli...
377|Formalizing Properties of Agents|There is a wide gulf between the formal logics used by logicians to describe agents and the informal vocabulary used by people who actually build robotic agents. In an effort to help bridge the gap, this paper applies techniques borrowed from the field of formal software methods to develop a common vocabulary. Terms useful for discussing agents are given formal definitions. A framework for describing agents, tasks and environments is developed using the Z specification language. The terms successful, capable, perceptive, predictive, interpretive, rational and sound are then defined relative to this framework. The aims of this paper are to develop a precise, common vocabulary for discussing agents and to provide a basis for rational design of agents. Motivation  In the artificial intelligence community, it has become common to talk about agents that perform tasks and to describe such agents in terms of characteristics that would allow them to be successful. Common terms include; success...
378|Oz - A Programming Language for Multi-Agent Systems|Oz is an experimental higher-order concurrent  constraint programming system under development  at DFKI. It combines ideas from logic and  concurrent programming in a simple yet expressive  language. From logic programming Oz inherits  logic variables and logic data structures,  which provide for a programming style where  partial information about the values of variables  is imposed concurrently and incrementally.  A novel feature of Oz is that it accommodates  higher-order programming without sacrificing  that denotation and equality of variables  are captured by first-order logic. Another new  feature of Oz is constraint communication, a  new form of asynchronous communication exploiting  logic variables. Constraint communication  avoids the problems of stream communication,  the conventional communication mechanism  employed in concurrent logic programming.  Constraint communication can be seen  as providing a minimal form of state fully compatible  with logic data structures.  Bas...
379|Reasoning about knowledge: An overview|Abstract: In this overview paper, I will attempt to identify and describe some of the common threads that tie together work in reasoning about knowledge in such diverse fields as philosophy, economics, linguistics, artificial intelligence, and theoretical computer sciencce. I will briefly discuss some of the more recent work, particularly in computer science, and suggest some lines for future research.
380|Specifying and Verifying Distributed Intelligent Systems|. This paper describes first steps towards the formal specification and verification of Distributed Artificial Intelligence (DAI) systems, through the use of temporal belief logics. The paper first describes Concurrent  MetateM, a programming language for DAI, and then develops a logic that may be used to reason about Concurrent MetateM systems. The utility of this logic for specifying and verifying Concurrent MetateM systems is demonstrated through a number of examples. The paper concludes with a brief discussion of the wider implications of the work, and in particular on the use of similar logics for reasoning about DAI systems in general. 1 Introduction  In the past decade, the discipline of DAI has moved from being a somewhat obscure relation of mainstream AI to being a major research area in its own right. DAI techniques have been applied to domains as diverse as archaeology and economics, as well as more mundane problems such as distributed sensing and manufacturing control [7]. ...
381|An agent architecture for distributed medical care|Abstract. This paper describes the design and implementation of a layered agent architecture for decision support applications in general and for distributed medical care in particular. Three important characteristics which shaped the agent design are identified: distribution of data and control, information uncertainty, and environment dynamism. To provide appropriate decision support in these circumstances the architecture combines a number of AI and agent techniques: a symbolic decision procedure for decision making with incomplete and contradictory information, a concept of accountability for task allocation, commitments and conventions for managing coherent cooperation, and a set of communication primitives for inter-agent interaction. 1.
383|Changing attitudes|Much work has been done on the formal analysis of attitudes like knowledge, belief and intention; e.g. [Halpern 1986, Cohen and Levesque 1990]. Much work has also been done on the application of non-monotonic logic to the problems of reasoning about change; e.g. [Brown 1987, Shoham 1988]. This paper brings together ideas from both fields to sketch a theory of changing attitudes. It
384|Elements of a Utilitarian Theory of Knowledge and Action|According to the utilitarian paradigm, an autonomous intelligent agent&#039;s interactions with the environment should be guided by the principle of expected utility maximization. We apply this paradigm to reasoning about an agent&#039;s physical actions and exploratory behavior in urgent, time-constrained situations. We model an agent&#039;s knowledge with a temporalized version of Kripke structures—as a set of branching time lines described by fluents, with accessibility relations holding among the states comprising the time lines. We describe how to compute utility based on this model which reflects the urgency that the environment imposes on time. Since the physical and exploratory actions that an agent could undertake transform the model of branching time lines in specific ways, the expected utilities of these actions can be computed, dictating rational tradeoffs among them depending on the agent&#039;s state of knowledge and the urgency of the situation. 1
385|The Role(s) of Logic in Artificial Intelligence  (1993) |this paper has been made possible by a gift from the System Development Foundation and was conducted as part of a coordinated research effort with the Center for the Study of Language and Information, Stanford University  30 References
386|Multiple Cooperating Robots - Combining Planning And Behaviours|Work at Salford into a behavioural approach to multiple cooperating robots has resulted in the Behaviour Synthesis Architecture, in which cooperation emerges from the combining of behaviours at `self&#039;, `environment &#039;, `species&#039; and `universe&#039; levels using a utility function. While this has been successfully applied to low-level cooperation, the need to cooperate at a higher level with a human operator points to the need to integrate a reflective agent able to apply predictive planning into the system. In addition, robots need a means of detecting failure and recovering from it via a meta-behavioural layer acting as a monitor.  1 INTRODUCTION  There are two main architectural approaches to building advanced robots for the carrying out of complex tasks. The more traditional architecture, going back to SHAKEY, [10] is essentially hierarchical and model-based, with action deriving from predictive planning. This style of planning projects actions into the future in which they will be execut...
387|Machine Learning in Automated Text Categorization|The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last ten years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely document representation, classifier construction, and classifier evaluation.
388|Text Categorization with Support Vector Machines: Learning with Many Relevant Features|This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies, why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and they behave robustly over a variety of different learning tasks. Furthermore, they are fully automatic, eliminating the need for manual parameter tuning.
389|Transductive Inference for Text Classification using Support Vector Machines|This paper introduces Transductive Support Vector Machines (TSVMs) for text classification.  While regular Support Vector Machines  (SVMs) try to induce a general decision  function for a learning task, Transductive  Support Vector Machines take into account  a particular test set and try to minimize  misclassifications of just those particular  examples. The paper presents an analysis  of why TSVMs are well suited for text  classification. These theoretical findings are  supported by experiments on three test collections.  The experiments show substantial  improvements over inductive methods, especially  for small training sets, cutting the number  of labeled training examples down to a  twentieth on some tasks. This work also proposes  an algorithm for training TSVMs efficiently,  handling 10,000 examples and more. 
390|On the optimality of the simple Bayesian classifier under zero-one loss|The simple Bayesian classifier is known to be optimal when attributes are independent given the class, but the question of whether other sufficient conditions for its optimality exist has so far not been explored. Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive. This article shows that, although the Bayesian classifier’s probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian classifier has a much greater range of applicability than previously thought. For example, in this article it is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption. Further, studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain. This article’s results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier, and this is also verified empirically.
391|Irrelevant Features and the Subset Selection Problem|We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets.
392|A Sequential Algorithm for Training Text Classifiers|The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness. 1 Introduction Text classification is the automated grouping of textual or partially textual entities. Document retrieval, categorization, routing, filtering, and clustering, as well as natural language processing tasks such as tagging, word sense disambiguation, and some aspects of understanding can be formulated as text classification. As the amount of online text increases, the demand for text classification to aid the analysis and mana...
393|NewsWeeder: Learning to Filter Netnews|A significant problem in many information filtering systems is the dependence on the user for the creation and maintenance of a user profile, which describes the user&#039;s interests. NewsWeeder is a netnews-filtering system that addresses this problem by letting the user rate his or her interest level for each article being read (1-5), and then learning a user profile based on these ratings. This paper describes how NewsWeeder accomplishes this task, and examines the alternative learning methods used. The results show that a learning algorithm based on the Minimum Description Length (MDL) principle was able to raise the percentage of interesting articles to be shown to users from 14% to 52% on average. Further, this performance significantly outperformed (by 21%) one of the most successful techniques in Information Retrieval (IR), termfrequency /inverse-document-frequency (tf-idf) weighting. 1
394|Hierarchically Classifying Documents Using Very Few Words|The proliferation of topic hierarchies for text documents has resulted in a need for tools that automatically classify new documents within such hierarchies. Existing classification schemes which ignore the hierarchical structure and treat the topics as separate classes are often inadequate in text classification where the there is a large number of classes and a huge number of relevant features needed to distinguish between them. We propose an approach that utilizes the hierarchical topic structure to decompose the classification task into a set of simpler problems, one at each node in the classification tree. As we show, each of these smaller problems can be solved accurately by focusing only on a very small set of features, those relevant to the task at hand. This set of relevant features varies widely throughout the hierarchy, so that, while the overall relevant feature set may be large, each classifier only examines a small subset. The use of reduced feature sets allows us to util...
395|Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval  (1998) |The naive Bayes classifier, currently experiencing a renaissance  in machine learning, has long been a core technique in information  retrieval. We review some of the variations of naive Bayes models used for  text retrieval and classification, focusing on the distributional assump-  tions made about word occurrences in documents.
396|Enhanced Hypertext Categorization Using Hyperlinks|A major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured search using topic taxonomies, circumvents keyword ambiguity, and improves the quality of search and profile-based routing and filtering. Therefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain highquality semantic clues that are lost upon a purely termbased classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a  relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics. We experimented ...
397|A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization|The Rocchio relevance feedback algorithm is one of the most popular and widely applied learning methods from information retrieval. Here, a probabilistic analysis of this algorithm is presented in a text categorization framework. The analysis gives theoretical insight into the heuristics used in the Rocchio algorithm, particularly the word weighting scheme and the similarity metric. It also suggests improvements which lead to a probabilistic variant of the Rocchio classifier. The Rocchio classifier, its probabilistic variant, and a naive Bayes classifier are compared on six text categorization tasks. The results show that the probabilistic algorithms are preferable to the heuristic Rocchio classifier not only because they are more well-founded, but also because they achieve better performance.
398|N-grambased text categorization|Text categorization is a fundamental task in document processing, allowing the automated handling of enormous streams of documents in electronic form. One difficulty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through OCR. Text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems. We describe here an N-gram-based approach to text categorization that is tolerant of textual errors. The system is small, fast and robust. This system worked very well for language classification, achieving in one test a 99.8 % correct classification rate on Usenet newsgroup articles written in different languages. The system also worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject, achieving as high as an 80 % correct classification rate. There are also several obvious directions for improving the system’s classification performance in those cases where it did not do as well. The system is based on calculating and comparing profiles of N-gram frequencies. First, we use the system to compute profiles on training set data that represent the various categories, e.g., language samples or newsgroup content samples. Then the system computes a profile for a particular document that is to be classified. Finally, the system computes a distance measure between the document’s profile and each of the
399|Information Filtering and Information Retrieval: Two Sides of the Same Coin|Information filtering systems are designed for unstructured or semistructured data, as opposed to database applications, which use very structured data. The systems also deal primarily with textual information, but they may also entail images, voice, video or other data types that are part of multimedia information systems. Information filtering systems also involve a large amount of data and streams of incoming data, whether broadcast from a remote source or sent directly by other sources. Filtering is based on descriptions of individual or group information preferences, or profiles, that typically represent long-term interests. Filtering also implies removal of data from an incoming stream rather than finding data in the stream; users see only the data that is extracted. Models of information retrieval and filtering, and lessons for filtering from retrieval research are presented.
400|Hierarchical classification of Web content|sdumais @ microsoft.com This paper explores the use of hierarchical structure for classifying a large, heterogeneous collection of web content. The hierarchical structure is initially used to train different second-level classifiers. In the hierarchical case, a model is learned to distinguish a second-level category from other categories within the same top level. In the flat non-hierarchical case, a model distinguishes a second-level category from all other second-level categories. Scoring rules can further take advantage of the hierarchy by considering only second-level categories that exceed a threshold at the top level. We use support vector machine (SVM) classifiers, which have been shown to be efficient and effective for classification, but not previously explored in the context of hierarchical classification. We found small advantages in accuracy for hierarchical models over flat models. For the hierarchical approach, we found the same accuracy using a sequential Boolean decision rule and a multiplicative decision rule. Since the sequential approach is much more efficient, requiring only 14%-16 % of the comparisons used in the other approaches, we find it to be a good choice for classifying text into large hierarchical structures.
401|A Comparison of Two Learning Algorithms for Text Categorization|This paper examines the use of inductive learning to categorize natural language documents into predefined content categories. Categorization of text is of increasing importance in information retrieval and natural language processing systems. Previous research on automated text categorization has mixed machine learning and knowledge engineering methods, making it difficult to draw conclusions about the performance of particular methods. In this paper we present empirical results on the performance of a Bayesian classifier and a decision tree learning algorithm on two text categorization data sets. We find that both algorithms achieve reasonable performance and allow controlled tradeoffs between false positives and false negatives. The stepwise feature selection in the decision tree algorithm is particularly effective in dealing with the large feature sets common in text categorization. However, even this algorithm is aided by an initial prefiltering of features, confirming the results...
402|OHSUMED: An interactive retrieval evaluation and new large test collection for research|A series of information retrieval experiments was earned out with a computer installed in a medical practice setting for relatively inexperienced physician end-users. Using a commercial MEDLINE product based on the vector space model, these physicians searched just as effectively as more experienced searchers using Boolean searching. The results of this experiment were subsequently used to create anew large medical test collection, which was used in experiments with the SMART ~trieval system to obtain baseline performance data as well as compare SMART with the other searchers. 1
403|Employing EM in Pool-Based Active Learning for Text Classification|This paper shows how a text classifier&#039;s need for labeled training data can be reduced by a combination of active learning and Expectation Maximization (EM) on a pool of unlabeled data. Query-by-Committee is used to actively select documents for labeling, then EM with a naive Bayes model further improves classification accuracy by concurrently estimating probabilistic labels for the remaining unlabeled documents and using them to improve the model. We also present a metric for better measuring disagreement among committee members; it accounts for the strength of their disagreement and for the distribution of the documents. Experimental results show that our method of combining EM and active learning requires only half as many labeled training examples to achieve the same accuracy as either EM or active learning alone. Keywords:  text classification active learning unsupervised learning information retrieval  1 Introduction  In many settings for learning text classifiers, obtaining lab...
404|Automated Learning of Decision Rules for Text Categorization|We describe the results of extensive experiments on large document collections using optimized  rule-based induction methods. The goal of these methods is to automatically discover  classification patterns that can be used for general document categorization or personalized filtering  of free text. Previous reports indicate that human-engineered rule-based systems, requiring  manymanyears of developmental efforts, have been successfully built to &#034;read&#034; documents and  assign topics to them. In this paper, weshowthatmachine generated decision rules appear  comparable to human performance, while using the identical rule-based representation. In comparison  with other machine learning techniques, results on a key benchmark from the Reuters  collection show a large gain in performance, from a previously reported 65% recall/precision  breakeven point to 80.5%. In the context of a very high dimensional feature space, several  methodological alternatives are examined, including universal versu...
405|Heterogeneous uncertainty sampling for supervised learning|Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger. 1
406|Distributional Clustering of Words for Text Classification|This paper describes the application of Distributional Clustering [20] to document classification. This approach clusters words into groups based on the distribution of class labels associated with each word. Thus, unlike some other unsupervised dimensionalityreduction techniques, such as Latent Semantic Indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy. Experimental results obtained on three real-world data sets show that we can reduce the feature dimensionality by three orders of magnitude and lose only 2% accuracy---significantly better than Latent Semantic Indexing [6], class-based clustering [1], feature selection by mutual information [23], or Markov-blanket-based feature selection [13]. We also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clustering. 1 Introduction The popularity of the Internet has caused an exponent...
407|Improving Text Classification by Shrinkage in a Hierarchy of Classes|When documents are organized in a large number of topic categories, the categories are often arranged in a hierarchy. The U.S. patent database and Yahoo are two examples.
408|Context-Sensitive Learning Methods for Text Categorization|this article, we will investigate the performance of two recently implemented machine-learning algorithms on a number of large text categorization problems. The two algorithms considered are set-valued RIPPER, a recent rule-learning algorithm [Cohen A earlier version of this article appeared in Proceedings of the 19th Annual International ACM Conference on Research and Development in Information Retrieval (SIGIR) pp. 307--315
409|Training Algorithms for Linear Text Classifiers|Systems for text retrieval, routing, categorization and other IR tasks rely heavily on linear classifiers. We propose that two machine learning algorithms, the Widrow-Hoff and EG algorithms, be used in training linear text classifiers. In contrast to most IR methods, theoretical analysis provides performance guarantees and guidance on parameter settings for these algorithms. Experimental data is presented showing Widrow-Hoff and EG to be more effective than the widely used Rocchio algorithm on several categorization and routing tasks.  1 Introduction  Document retrieval, categorization, routing, and filtering systems often are based on classification. That is, the IR system decides for each document which of two or more classes it belongs to, or how strongly it belongs to a class, in order to accomplish the IR task of interest. For instance, the two classes may be the documents relevant to and not relevant to a particular user, and the system may rank documents based on how likely it i...
410|A method for disambiguating word senses in a large corpus|Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years. Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources, such as semantic networks and annotated corpora. In particular, much of the work on qualitative methods has had to focus on ‘‘toy’’ domains since currently available semantic networks generally lack broad coverage. Similarly, much of the work on quantitative methods has had to depend on small amounts of hand-labeled text for testing and training. We have achieved considerable progress recently by taking advantage of a new source of testing and training materials. Rather than depending on small amounts of hand-labeled text, we have been making use of relatively large amounts of parallel text, text such as the Canadian Hansards, which are available in multiple languages. The translation can often be used in lieu of hand-labeling. For example, consider the polysemous word sentence, which has two major senses: (1) a judicial sentence, and (2), a syntactic sentence. We can collect a number of sense (1) examples by extracting instances that are translated as peine, and we can collect a number of sense (2) examples by extracting instances that are translated as
411|Automatic Detection of Text Genre|As the text databases available to users become larger and more heterogeneous, genre  becomes increasingly important for computational  linguistics as a complement to  topical and structural principles of classification. We propose a th
412|Combining Classifiers in Text Categorization|Three different types of classifiers were investigated in the context of a text categorization problem in the medical domain: the automatic assignment of ICD9 codes to dictated inpatient discharge summaries. K-nearest-neighbor, relevance feedback, and Bayesian independence classifers were applied individually and in combination. A combination of different classifiers produced better results than any single type of classifier. For this specific medical categorization problem, new query formulation and weighting methods used in the k-nearest-neighbor classifier improved performance.   1 Introduction  Past research in information retrieval has shown that one can improve retrieval effectiveness by using multiple representations in indexing and query formulation [27] [19] [3] [11] and by using multiple search strategies [5] [24] [7]. In this work, we investigate whether we can attain similar improvements in the domain of text categorization by combining different representations and classif...
413|Scalable Feature Selection, Classification and Signature Generation for Organizing Large Text Databases Into Hierarchical Topic Taxonomies|We explore how to organize large text databases hierarchically by topic to aid better searching, browsing and filtering. Many corpora, such as internet directories, digital libraries, and patent databases are manually organized into topic hierarchies, also called taxonomies. Similar to indices for relational data, taxonomies make search and access more efficient. However, the exponential growth in the volume of on-line textual information makes it nearly impossible to maintain such taxonomic organization for large, fast-changing corpora by hand. We describe an automatic system that starts with a small sample of the corpus in which topics have been assigned by hand, and then updates the database with new documents as the corpus grows, assigning topics to these new documents with high speed and accuracy. To do this, we use techniques from statistical pattern recognition to efficiently separate the feature words, or...
414|Detecting Concept Drift with Support Vector Machines|For many learning tasks where data is collected over an extended period of time, its underlying distribution is likely to change. A typical example is information filtering, i.e. the adaptive classification of documents with respect to a particular user interest. Both the interest of the user and the document content change over time. A filtering system should be able to adapt to such concept changes. This paper proposes a new method to recognize and handle concept changes with support vector machines. The method maintains a window on the training data. The key idea is to automatically adjust the window size so that the estimated generalization error is minimized. The new approach is both theoretically well-founded as well as effective and efficient in practice. Since it does not require complicated parameterization, it is simpler to use and more robust than comparable heuristics. Experiments with simulated concept drift scenarios based on real-world text data com...
415|Mistake-Driven Learning in Text Categorization|Learning problems in the text processing  domain often map the text to a space  whose dimensions are the measured fea-  tures of the text, e.g., its words. Three  characteristic properties of this domain are  (a) very high dimensionality, (b) both the  learned concepts and the instances reside  very sparsely in the feature space, and (c)  a high variation in the number of active  features in an instance. In this work we  study three mistake-driven learning algo-  rithms for a typical task of this nature -   text categorization. We argue
416|A Probabilistic Learning Approach for Document Indexing|We describe a method for probabilistic document indexing using relevance feedback data  that has been collected from a set of queries. Our approach is based on three new concepts:  (1) Abstraction from specific terms and documents, which overcomes the restriction of limited  relevance information for parameter estimation. (2) Flexibility of the representation, which  allows the integration of new text analysis and knowledge-based methods in our approach as  well as the consideration of document structures or different types of terms. (3) Probabilistic  learning or classification methods for the estimation of the indexing weights making better use  of the available relevance information. Our approach can be applied under restrictions that  hold for real applications. We give experimental results for five test collections which show  improvements over other indexing methods.
417|Tadepalli, Active Learning with Committees for Text Categorization |The availability of vast amounts of information on the World Wide Web has created a big demand for automatic tools to organize and index that information. Unfortunately, the paradigm of supervised machine learning is ill-suited to this task, as it assumes that the training examples are classi-fied by a teacher – usually a human. In this paper, we de-scribe an active learning method based on Query by Com-mittee (QBC) that reduces the number of labeled training examples (text documents) required for learning by 1-2 or-ders of magnitude. 1.
418|Improving text retrieval for the routing problem using latent semantic indexing|Latent Semantic Indexing (LSI) is a novel approach to information retrieval that attempts to model the underlying structure of term associations by transforming the traditional representation of documents as vectors of weighted term frequencies to a new coordinate space where both documents and terms are represented as linear combinations of underlying semantic factors. In previous research, LSI has produced a small improvement in retrieval performance. In this paper, we apply LSI to the routing task, which operates under the assumption that a sample of relevant and non-relevant documents is available to use in constructing the query. Once again, LSI slightly improves performance. However, when LSI is used is conduction with statistical classification, there is a dramatic improvement in performance. 1
419|Models for retrieval with probabilistic indexing|Abstract- in this article three retrieval models for probabilistic indexing are described along with evaluation results for each. First is the binary independence indexing @II) model, which is a generalized version of the Maron and Kuhns indexing model. In this model, the indexing weight of a descriptor in a document is an estimate of the proba-bility of relevance of this document with respect to queries using this descriptor. Sec-ond is the retrieval-with-probabilistic-indexing (RPI) model, which is suited to different kinds of probabilistic indexing. For that we assume that each indexing scheme has its own concept of “correctness ” to which the probabilities relate. In addition to the prob-abilistic indexing weights, the RPI model provides the possibility of reIevance weight-ing of search terms. A third mode1 that is similar was proposed by Croft some years ago as an extension of the binary independence retrieval model but it can be shown that this model is not based on the probabilistic ranking principle. The probabilistic indexing weights required for any of these models can be provided by an application of the Darm-stadt indexing approach (DIA) for indexing with descriptors from a controlled vocabu-Iary. The experimental results show signi~cant improvements over retrieval with binary indexing. Finally, suggestions are made regarding how the DIA can be applied to prob-abilistic indexing with free text terms. 1.
420|Automatic Essay Grading Using Text Categorization Techniques|The commas are the most useful and usable of all the stops. It is highly important to put them in place as you go along. If you try to come back after doing a paragraph and stick them in the various spots that tempt you you will discover that they tend to swarm like minnows into all sorts of crevices whose existence you hadn?t realized and before you know it the whole long sentence becomes immobilized and lashed up squirming in commas. Better to use them sparingly, and with affection precisely when the need for one arises, nicely, by itself.
421|Noun Homograph Disambiguation Using Local Context in Large Text Corpora|This paper describes an accurate, relatively inexpensive method for the disambiguation of noun homographs using large text corpora. The algorithm checks the context surrounding the target noun against that of previously observed instances and chooses the sense for which the most evidence is found, where evidence consists of a set of orthographic, syntactic, and lexical features. Because the sense distinctions made are coarse, the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms. An implementation of the algorithm is described which, starting with a small set of hand-labeled instances, improves its results automatically via unsupervised training. The approach is compared to other attempts at homograph disambiguation using both machine readable dictionaries and unrestricted text and the use of training instances is determined to be a crucial difference. 1 Introduction  Large text corpora and the computational resources to handle them have ...
423|Text categorization of low quality images|Categorization of text images into content-oriented classes would be a useful capability in a variety of document handling systems. Many methods can be usedtocategorize texts once their words are known, but OCR can garble a large proportion of words, particularly when low quality images are used. Despite this, we show for one data set that fax quality images can be categorized with nearly the same accuracy as the original text. Further, the categorization system can be trained on noisy OCR output, without need for the true text of any image, or for editing of OCR output. The useofavector space classi er and training method robust to large feature sets, combined with discarding of low frequency OCR output strings are the key to our approach. 1
424|&#034;Is This Document Relevant? ...Probably&#034;: A Survey of Probabilistic Models in Information Retrieval|This article surveys probabilistic approaches to modeling information retrieval. The  basic concepts of probabilistic approaches to information retrieval are outlined and  the principles and assumptions upon which the approaches are based are  presented. The various models proposed in the development of IR are described,  classified, and compared using a common formalism. New approaches that  constitute the basis of future research are described
425|A Learner-Independent Evaluation of the Usefulness of Statistical Phrases for Automated Text Categorization|In this work we investigate the usefulness of  n-grams for document indexing in text categorization  (TCi  We call-gram a set g k  of n word stems, and we say that g k occurs  in a document d j when a sequence of  words appears in d j that, after stop word removal  and stemming, consists exactly ofthe  n stems in g k , in some order. Previous researches  have investigated the use of n-grams  (or some variant ofthem) in the context of  specific learning algorithms, and thus have  not obtained general answers on their usefulness  for TC In this work we investigate the  usefulness of n-grams  inTC  independently  ofany specific learning algorithm. We do so  by applying feature selection to the pool of  all k-grams (k  #  n), and checking how many  n-grams score high enough to be selected in  the top #k-grams. We report the results of  our experiments, using various feature selection  measures and varying values of #, performed  on  theReuters-21 standardTC  benchmark. We also report resul...
426|A Patent Search and Classification System|We present a system for searching and classifying U.S. patent documents, based on Inquery. Patents are distributed through hundreds of collections, divided up by general area. The system selects the best collections for the query. Users can search for patents or classify patent text. The user interface helps users search in fields without requiring the knowledge of Inquery query operators. The system includes a unique “phrase help ” facility, which helps users find and add phrases and terms related to those in their query.
427|Joins that Generalize: Text Classification Using WHIRL|WHIRL is an extension of relational databases that can perform &#034;soft joins&#034; based on the similarity of textual identifiers; these soft joins extend the traditional operation of joining tables based on the equivalence of atomic values. This paper evaluates WHIRL on a number of inductive classification tasks using data from the World Wide Web. We show that although WHIRL is designed for more general similaritybased reasoning tasks, it is competitive with mature inductive classification systems on these classification tasks. In particular, WHIRL generally achieves lower generalization error than C4.5, RIPPER, and several nearest-neighbor methods. WHIRL is also fast---up to 500 times faster than C4.5 on some benchmark problems. We also show that WHIRL can be efficiently used to select from a large pool of unlabeled items those that can be classified correctly with high confidence. Introduction  Consider the problem of exploratory analysis of data obtained from the Internet. Assuming that o...
428|Text Categorization and Relational Learning|We evaluate the first order learning system FOIL on a series of text categorization problems. It is shown that FOIL usually forms classifiers with lower error rates and higher rates of precision and recall with a relational encoding than with a propositional encoding. We show that FOIL&#039;s performance can be improved by relation selection, a first order analog of feature selection. Relation selection improves FOIL&#039;s performance as measured by any of recall, precision, F-measure, or error rate. With an appropriate level of relation selection, FOIL appears to be competitive with or superior to existing propositional techniques. 1 INTRODUCTION  There is increasing interest in using intelligent systems to perform tasks like e-mail filtering, news filtering, and automatic indexing of documents. Many of these applications require the ability to classify text into one of several predefined categories, and in many of these applications, it would be highly advantageous to automatically learn such...
429|Experiments on the use of feature selection and negative evidence in automated text categorization|In this work we tackle two different problems of text categorization (TC), namely feature selection and classifier induction. Feature selection refers to the activity of selecting, from the set of r distinct features (i.e. words) occurring in the collection, the subset of r '  « r features that are most useful for compactly representing the meaning of the documents. We propose a novel feature selection technique, based on a simplified variant of the ? 2 statistics. Classifier induction refers instead to the problem of automatically building a text classifier by learning from a set of documents pre-classified under the categories of interest. We propose a novel variant, based on the exploitation of negative evidence, of the well-known k-NN method. We report the results of systematic experimentation of these two methods performed on the standard Reuters-21578 benchmark.
430|Method Combination For Document Filtering|There is strong empirical and theoretic evidence that combination of retrieval methods can improve performance. In this paper, we systematically compare combination strategies in the context of document filtering, using queries from the Tipster reference corpus. We find that simple averaging strategies do indeed improve performance, but that direct averaging of probability estimates is not the correct approach. Instead, the probability estimates must be renormalized using logistic regression on the known relevance judgements. We examine more complex combination strategies but find them less successful due to the high correlations among our filtering methods which are optimized over the same training data and employ similar document representations. 1 Introduction A text filtering system monitors an incoming document stream and selects documents identified as relevant to one or more of its query profiles. If profile interactions are ignored, this reduces to a number of independent bina...
431|The TREC-6 Filtering Track: Description and Analysis|This article details the experiments conducted in the TREC-6 filtering track. The filtering track is an extension of the routing track which adds time sequencing of the document stream and set-based evaluation strategies which simulate immediate distribution of the retrieved documents. It also introduces an adaptive filtering subtrack which is designed to simulate on-line or sequential filtering of documents. In addition to motivating the task and describing the practical details of participating in the track, this document includes a detailed graphical presentation of the experimental results and attempts to analyze and explain the observed patterns. The final section suggests some ways to extend the current research in future experiments. 1 Introduction  There is increasing evidence that text filtering will become a critical tool in searching and managing the flow of data in the information age. New companies are appearing daily which offer push services or intelligent agents centere...
432|Automatic Text Categorization and Its Application to Text Retrieval|We develop an automatic text categorization approach and investigate its application to text retrieval. The categorization approach is derived from a combination of a learning paradigm known as instancebased learning and an advanced document retrieval technique known as retrieval feedback. We demonstrate the effectiveness of our categorization approach using two real-world document collections from the MEDLINE database. Next we investigate the application of automatic categorization to text retrieval. Our experiments clearly indicate that automatic categorization improves the retrieval performance compared with no categorization. We also demonstrate that the retrieval performance using automatic categorization achieves the same retrieval quality as the performance using manual categorization. Furthermore, detailed analysis of the retrieval performance on each individual test query is provided. Index Terms: Text Categorization, Automatic Classification, Text Retrieval, Instance-Based Le...
433|Using WordNet to complement training information in text categorization|Automatic Text Categorization (TC) is a complex and useful task for many natural language applications, and is usually performed through the use of a set of manually classified documents, a training collection. We suggest the utilization of additional resources like lexical databases to increase the amount of information that TC systems make use of, and thus, to improve their performance. Our approach integrates WordNet information with two training approaches through the Vector Space Model. The training approaches we test are the Rocchio (relevance feedback) and the Widrow-Hoff (machine learning) algorithms. Results obtained from evaluation show that the integration of WordNet clearly outperforms training approaches, and that an integrated technique can effectively address the classification of low frequency categories. 1
434|A probabilistic description-oriented approach for categorising Web documents|The automatic categorisation of web documents is becoming crucial for organising the huge amount of information available in the Internet. We are facing a new challenge due to the fact that web documents have a rich structure and are highly heterogeneous. Two ways to respond to this challenge are (1) using a representation of the content of web documents that captures these two characteristics and (2) using more eective classiers.  Our categorisation approach is based on a probabilistic description-oriented representation of web documents, and a probabilistic interpretation of the k-nearest neighbour classifier. With the former, we provide an enhanced document representation that incorporates the structural and heterogeneous nature of web documents. With the latter, we provide a theoretical sound justification for the various parameters of the k-nearest neighbour classifier.  Experimental results show that (1) using an enhanced representation of web documents is crucial for an effective categorisation of web documents, and (2) a theoretical interpretation of the k-nearest neighbour classifier gives us improvement over the standard k-nearest neighbour classifier.  
435|Probabilistic Information Retrieval as Combination of Abstraction, Inductive Learning and Probabilistic Assumptions|   We show that former approaches in probabilistic information retrieval are based on one or two of the three concepts abstraction, inductive learning and probabilistic assumptions, and we propose a new approach which combines all three concepts. This approach is illustrated for the case of indexing with a controlled ...
436|Feature Reduction for Neural Network Based Text Categorization|In a text categorization model using an artificial neural network as the text classifier, scalability is poor if the neural network is trained using the raw feature space since textural data has a very high-dimension feature space. We proposed and compared four dimensionality reduction techniques to reduce the feature space into an input space of much lower dimension for the neural network classifier. To test the effectiveness of the proposed model, experiments were conducted using a subset of the Reuters-22173 test collection for text categorization. The results showed that the proposed model was able to achieve high categorization effectiveness as measured by precision and recall. Among the four dimensionality reduction techniques proposed, Principal Component Analysis was found to be the most effective in reducing the dimensionality of the feature space. 1. Introduction  Text categorization is the classification of text documents into a set of one or more categories. In this paper, ...
437|Text Classification Using ESC-based Stochastic Decision Lists|We propose a new method of text classification using stochastic decision lists. A stochastic decision list is an ordered sequence of IF-THEN rules, and our method can be viewed as a rule-based method for text classification having advantages of readability and refinability of acquired knowledge. Our method is unique in that decision lists are automatically constructed on the basis of the principle of minimizing Extended Stochastic Complexity (ESC), and with it we are able to construct decision lists that have fewer errors in classification. The accuracy of classification achieved with our method appears better than or comparable to those of existing rule-based methods. We have empirically demonstrated that rule-based methods like ours result in high classification accuracy when the categories to which texts are to be assigned are relatively specific ones and when the texts tend to be short. We have also empirically verified the advantages of rule-based methods over non-rule-based ones.
439|Probabilistic learning for selective dissemination of information|New methods and new systems are needed to filter or to selectively distribute the increasing volume of electronic information being produced nowadays. An effective information filtering system is one that provides the exact information that fulfills user&#039;s interests with the minimum effort by the user to describe it. Such a system will have to be adaptive to the user changing interest. In this paper we describe and evaluate a learning model for information filtering which is an adaptation of the generalized probabilistic model of Information Retrieval. The model is based on the concept of `uncertainty sampling&#039;, a technique that allows for relevance feedback both on relevant and nonrelevant documents. The proposed learning model is the core of a prototype information filtering system called ProFile.
440|Autonomous Document Classification for Business|With the continuing exponential growth of the Internet  and the more recent growth of business Intranets, the  commercial world is becoming increasingly aware of  the problem of electronic information overload. This  has encouraged interest in developing agents/softbots  that can act as electronic personal assistants and can  develop and adapt representations of users information  needs, commonly known as profiles. As the
441|Exploiting Thesaurus Knowledge in Rule Induction for Text Classification|Systems for learning text classifiers recently gained considerable interest. One technique to implement such systems is rule induction. While most other approaches rely on a relatively simple document representation and do not make use of any background knowledge, rule induction algorithms offer a good potential for improvements in both of these areas. In this paper, we show how an operator-based view of rule induction enables the easy integration of a thesaurus as background knowledge. Results with an algorithm extended by thesaurus knowledge are presented and interpreted. The interpretation shows the strengths and weaknesses of using thesaurus knowledge and gives hints for future research. 1 Introduction Text classification deals with the task of assigning a label out of a set of predefined classes to a given text document. Example applications include classifying technical reports according to their subject research area for archiving, or analyzing incoming newswire articles wrt. ...
442|U-Net: A User-Level Network Interface for Parallel and Distributed Computing|The U-Net communication architecture provides processes with a virtual view of a network interface to enable userlevel access to high-speed communication devices. The architecture, implemented on standard workstations using offthe-shelf ATM communication hardware, removes the kernel from the communication path, while still providing full protection. The model presented by U-Net allows for the construction of protocols at user level whose performance is only limited by the capabilities of network. The architecture is extremely flexible in the sense that traditional protocols like TCP and UDP, as well as novel abstractions like Active Messages can be implemented efficiently. A U-Net prototype on an 8-node ATM cluster of standard workstations offers 65 microseconds round-trip latency and 15 Mbytes/sec bandwidth. It achieves TCP performance at maximum network bandwidth and demonstrates performance equivalent to Meiko CS-2 and TMC CM-5 supercomputers on a set of Split-C benchmarks. 1
443|An Analysis of TCP Processing Overhead|networks, have been getting faster, perceived throughput at the application has not always increased accordingly. Various performance bottlenecks have been encountered, each of which has to be analyzed and corrected. One aspect of networking often suspected of contributing to low throughput is the transport layer of the protocol suite. This layer, especially in connectionless protocols, has considerable functionality, and is typically executed in software by the host processor at the end points of the network. It is thus a likely source of processing overhead. While this theory is appealing, a preliminary examination suggested to us that other aspects of networking may be a more serious source of overhead. To test this proposition, a detailed study was made of a popular transport protocol, Transmission Control Protocol (TCP) [I]. This paper provides results of that
444|Fbufs: A High-Bandwidth Cross-Domain Transfer Facility|We have designed and implemented a new operating system facility for I/O buffer management and data transfer across protection domain boundaries on shared memory machines. This facility, called fast buffers (fbufs), combines virtual page remapping with shared virtual memory, and exploits locality in I/O traffic to achieve high throughput withoutcompromising protection, security, or modularity. Its goal is to help deliver the high bandwidth afforded by emerging high-speed networks to user-level processes, both in monolithic and microkernel-based operating systems.  This paper outlines the requirements for a cross-domain transfer facility, describes the design of the fbuf mechanism that meets these requirements, and experimentally quantifies the impact of fbufs on network performance. 1 Introduction  Optimizing operations that cross protection domain boundaries has received a great deal of attention recently [2, 3]. This is because an efficient cross-domain invocation facility enables a ...
445|Dynamics of TCP Traffic over ATM Networks|We investigate the performance of TCP connections over ATM networks without ATM-level congestion control, and compare it to the performance of TCP over packet-based networks. For simulations of congested networks, the effective throughput of TCP over ATM can be quite low when cells are dropped at the congested ATM switch. The low throughput is due to wasted bandwidth as the congested link transmits cells from `corrupted&#039; packets, i.e., packets in which at least one cell is dropped by the switch. We investigate two packet discard strategies which alleviate the effects of fragmentation. Partial Packet Discard, in which remaining cells are discarded after one cell has been dropped from a packet, somewhat improves throughput. We introduce Early Packet Discard, a strategy in which the switch drops whole packets prior to buffer overflow. This mechanism prevents fragmentation and restores throughput to maximal levels.  
446|Experiences with a High-Speed Network Adaptor: A Software Perspective|This paper describes our experiences, from a software perspective, with the OSIRIS network adaptor. It first identifies the problems we encountered while programming OSIRIS and optimizing network performance, and outlines how we either addressed them in the software, or had to modify the hardware. It then describes the opportunities provided by OSIRIS that we were able to exploit in the host operating system (OS); opportunities that suggested techniques for making the OS more effective in delivering network data to application programs. The most novel of these techniques, called application device channels, gives application programs running in user space direct access to the adaptor. The paper concludes with the lessons drawn from this work, which we believe will benefit the designers of future network adaptors. 1 Introduction  With the emergence of high-speed network facilities, several research efforts are focusing on the design and implementation of network adaptors [5, 2, 3, 16, 2...
447|Protocol Service Decomposition for High-Performance Networking|In this paper we describe a new approach to implementing network protocols that enables them to have high performance and high flexibility, while retaining complete conformity to existing application programming interfaces. The key insight behind our work is that an application&#039;s interface to the network is distinct and separable from its interface to the operating system. We have separated these interfaces for two protocol implementations, TCP/IP and UDP/IP, running on the Mach 3.0 operating system and UNIX server. Specifically, library code in the application&#039;s address space implements the network protocols and transfers data to and from the network, while an operating system server manages the heavyweight abstractions that applications use when manipulating the network through operations other than send and receive. On DECstation 5000/200 This research was sponsored in part by the Advanced Research Projects Agency, Information Science and Technology Office, under the title &#034;Research...
448|The importance of Non-Data Touching Processing Overheads|We present detailed measurements of various processing overheads of the TCP/IP and UDP/IP protocol stacks on a DECstation 5000/200 running the Ultrix 4.2a operating system. These overheads include data-touching operations, such as the checksum computation and data movemen ~ which are well known to be major time consumers. In this stud y, we also considered overheads due to non-data touching operations, such as network buffer manipulation, protocol-specific processing, operating system functions, data structure manipulations (other than network buffers), and error checking. We show that when one considers realistic message size dktributions, where the majority of messages are small, the cumulative time consumed by the nondata touching overheads represents the majority of processing time. We assert that it will be difficult to significantly reduce the cumulative processing time due to non-data touching overheads. The goal of this study is to determine the relative importance of various processing overheads in network software, in particular, the TCP/IP and UDPAP protocol stacks. In the prrsg significant focus has been placed on maximizing throughput noting that “data
449|Distributed Network Computing over Local ATM Networks|Communication between processors has long been the bottleneck of distributed network computing. However, recent progress in switch-based high-speed Local Area Networks (LANs) may be changing this situation. Asynchronous Transfer Mode (ATM) is one of the most widely-accepted and emerging high-speed network standards which can potentially satisfy the communication needs of distributed network computing. In this paper, we investigate distributed network computing over local ATM networks. We first study the performance characteristics involving end-to-end communication in an environment that includes several types of workstations interconnected via a Fore Systems&#039; ASX-100 ATM Switch. We then compare the communication performance of four different Application Programming Interfaces (APIs). The four APIs were Fore Systems ATM API, BSD socket programming interface, Sun&#039;s Remote Procedure Call (RPC), and the Parallel Virtual Machine (PVM) message passing library. Each API represents distribute...
450|Separating Data and Control Transfer in Distributed Operating Systems|Advances in processor architecture and technology have resulted in workstations in the 100+ MIPS range. As well, newer local-area networks such as ATM promise a ten- to hundred-fold increase in throughput, much reduced latency, greater scalability, and greatly increased reliability, when compared to current LANs such as Ethernet. We believe that these new network and processor technologies will permit tighter coupling of distributed systems at the hardware level, and that distributed systems software should be designed to benefit from that tighter coupling. In this paper, we propose an alternative way of structuring distributed systems that takes advantage of a communication model based on remote network access (reads and writes) to protected memory segments. A key feature of the new structure, directly supported by the communication model, is the separation of data transfer and control transfer. This is in contrast to the structure of traditional distributed systems, which are typical...
451|Protocol Implementation Using Integrated Layer Processing|Integrated Layer Processing (ILP) is an implementation concept which &amp;quot;permit[s] the implementor the option of performing all the [data] manipulation steps in one or two integrated processing loops &amp;quot; [1]. To estimate the achievable benefits of ILP, a file transfer application with an encryption function on top of a user-level TCP has been implemented and the performance of the application in terms of throughput and packet processing times has been measured. The results show that it is possible to obtain performance benefits by integrating marshalling, encryption and TCP checksum calculation. They also show that the benefits are smaller than in simple experiments, where ILP effects have not been evaluated in a complete protocol environment. Simulations of memory access and cache hit rate show that the main benefit of ILP is reduced memory accesses rather than an improved cache hit rate. The results further show that data manipulation characteristics may significantly influence the cache behavior and the achievable performance gain of ILP. 1
452|User-space protocols deliver high performance to applications on a low-cost gb/s lan|Two important questions in high-speed networking are firstly, how to provide GbitJs networking at low cost and secondly, how to provide a flexible low-level network inter-face so that applications can control their data from the instant it arrives. We describe some work that addresses both of these ques-tions. The Jetstream Gbit/s LAN is an experimental, low-cost network interface that provides the services required by delay-sensitive traffic as well as meeting the performance needs of current applications. Jetstream is a combination of traditional shared-medium LAN technology and more recent ATM cell- and switch-based technology. Jetstream frames contain a channel identifier so that the net-work driver can immediately associate an incoming frame with its application. We have developed such a driver that enables applications to control how their data should be managed without the need to first move the data into the application’s address space. Consequently, applications can elect to read just a part of a frame and then instruct the driver to move the remainder directly to its destination. Individual channels can elect to receive frames that have failed their CRC, while applications can specify frame-drop policies on a per-channel basis. Measured results show that both kernel- and user-space pro-tocols can achieve very good throughput: applications using both TCP and our own reliable byte-stream protocol have demonstrated throughputs in excess of 200 Mbit/s. The ben-efits of running protocols in user-space are well known- the drawback has often been a severe penalty in the perform-ance achieved. In this paper we show that it is possible to have the best of both worlds. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given
453|Experience with Active Messages on the Meiko CS-2|Active messages provide a low latency communication architecture which on modern parallel machines achieves more than an order of magnitude performance improvement over more traditional communication libraries. This paper discusses the experience we gained while implementing active messages on the Meiko CS-2, and discusses implementations for similar architectures. During our work we have identified that architectures which only support efficient remote write operations (or DMA transfers as in the case of the CS-2) make it difficult to transfer both data and control as required by active messages. Traditional network interfaces avoid this problem because they have a single point of entry which essentially acts as a queue. To efficiently support active messages on modern network communication co-processors, hardware primitives are required which support this queue behavior. We overcame this problem by producing specialized code which runs on the communications co-processor and supports ...
454|Virtual-memorymapped network interfaces|In today’s multicomputers, software overhead dominates the message-passing latency cost. We designed two multicomputer network interfaces that signif~cantiy reduce this overhead. Both support vMual-memory-mapped communication, allowing user processes to communicate without expensive buffer management and without making system calls across the protection boundary separating user processes from the operating system kerneL Here we compare the two interfaces and discuss the performance trade-offs between them.
455|Grid Information Services for Distributed Resource Sharing|Grid technologies enable large-scale sharing of resources within formal or informal consortia of individuals and/or institutions: what are sometimes called virtual organizations. In these settings, the discovery, characterization, and monitoring of resources, services, and computations are challenging problems due to the considerable diversity, large numbers, dynamic behavior, and geographical distribution of the entities in which a user might be interested. Consequently, information services are a vital part of any Grid software infrastructure, providing fundamental mechanisms for discovery and monitoring, and hence for planning and adapting application behavior.  We present here an information services architecture that addresses performance, security, scalability, and robustness requirements. Our architecture defines simple low-level enquiry  and registration protocols that make it easy to incorporate individual entities into various information structures, such as aggregate directories that support a variety of different query languages and discovery strategies. These protocols can also be combined with other Grid protocols to construct additional higher-level services and capabilities such as brokering, monitoring, fault detection, and troubleshooting. Our architecture has been implemented as MDS-2, which forms part of the Globus Grid toolkit and has been widely deployed and applied.  
456|Globus: A Metacomputing Infrastructure Toolkit|Emerging high-performance applications require the ability to exploit diverse, geographically distributed resources. These applications use high-speed networks to integrate supercomputers, large databases, archival storage devices, advanced visualization devices, and/or scientific instruments to form networked virtual supercomputers or metacomputers. While the physical infrastructure to build such systems is becoming widespread, the heterogeneous and dynamic nature of the metacomputing environment poses new challenges for developers of system software, parallel tools, and applications. In this article, we introduce Globus, a system that we are developing to address these challenges. The Globus system is intended to achieve a vertically integrated treatment of application, middleware, and network. A low-level toolkit provides basic mechanisms such as communication, authentication, network information, and data access. These mechanisms are used to construct various higher-level metacomp...
457|Distributed Snapshots: Determining Global States of Distributed Systems|This paper presents an algorithm by which a process in a distributed system determines a global state of the system during a computation. Many problems in distributed systems can be cast in terms of the problem of detecting global states. For instance, the global state detection algorithm helps to solve an important class of problems: stable property detection. A stable property is one that persists: once a stable property becomes true it remains true thereafter. Examples of stable properties are “computation has terminated,”  “the system is deadlocked” and “all tokens in a token ring have disappeared.” The stable property detection problem is that of devising algorithms to detect a given stable property. Global state detection can also be used for checkpointing. 
458|Unreliable Failure Detectors for Reliable Distributed Systems|We introduce the concept of unreliable failure detectors and study how they can be used to solve Consensus in asynchronous systems with crash failures. We characterise unreliable failure detectors in terms of two properties — completeness and accuracy. We show that Consensus can be solved even with unreliable failure detectors that make an infinite number of mistakes, and determine which ones can be used to solve Consensus despite any number of crashes, and which ones require a majority of correct processes. We prove that Consensus and Atomic Broadcast are reducible to each other in asynchronous systems with crash failures; thus the above results also apply to Atomic Broadcast. A companion paper shows that one of the failure detectors introduced here is the weakest failure detector for solving Consensus [Chandra et al. 1992].
459|A Security Architecture for Computational Grids|State-of-the-art and emerging scientific applications require fast access to large quantities of data and commensurately fast computational resources. Both resources and data are often distributed in a wide-area network with components administered locally and independently. Computations may involve hundreds of processes that must be able to acquire resources dynamically and communicate e#ciently. This paper analyzes the unique security requirements of large-scale distributed (grid) computing and develops a security policy and a corresponding security architecture. An implementation of the architecture within the Globus metacomputing toolkit is discussed.  
460|Matchmaking: Distributed Resource Management for High Throughput Computing|Conventional resource management systems use a system model to describe resources and a centralized scheduler to control their allocation. We argue that this paradigm does not adapt well to distributed systems, particularly those built to support high-throughput computing. Obstacles include heterogeneity of resources, which make uniform allocation algorithms difficult to formulate, and distributed ownership, leading to widely varying allocation policies. Faced with these problems, we developed and implemented the classified advertisement (classad) matchmaking framework, a flexible and general approach to resource management in distributed environment with decentralized ownership of resources. Novel aspects of the framework include a semi-structured data model that combines schema, data, and query in a simple but powerful specification language, and a clean separation of the matching and claiming phases of resource allocation. The representation and protocols result in a robust, scalabl...
462|An Architecture for a Secure Service Discovery Service|The widespread deployment of inexpensive communications technology, computational resources in the networking infrastructure, and network-enabled end devices poses an interesting problem for end users: how to locate a particular network service or device out of hundreds of thousands of accessible services and devices. This paper presents the architecture and implementation of a secure Service Discovery Service (SDS). Service providers use the SDS to advertise complex descriptions of available or already running services, while clients use the SDS to compose complex queries for locating these services. Service descriptions and queries use the eXtensible Markup Language (XML) to encode such factors as cost, performance, location, and device- or service-specific capabilities. The SDS provides a highlyavailable, fault-tolerant, incrementally scalable service for locating services in the wide-area. Security is a core component of the SDS and, where necessary, communications are both encrypt...
464|A Directory Service for Configuring High-Performance Distributed Computations|High-performance execution in distributed computing environments often requires careful selection and configuration not only of computers, networks, and other resources but also of the protocols and algorithms used by applications. Selection and configuration in turn require access to accurate, up-to-date information on the structure and state of available resources. Unfortunately, no standard mechanism exists for organizing or accessing such information. Consequently, different tools and applications adopt ad hoc mechanisms, or they compromise their portability and performance by using default configurations. We propose a solution to this problem: a Metacomputing Directory Service that provides efficient and scalable access to diverse, dynamic, and distributed information about resource structure and state. We define an extensible data model to represent the information required for distributed computing, and we present a scalable, high-performance, distributed implementation. The dat...
465|Development of the Domain Name System|(Originally published in the Proceedings of SIGCOMM ‘88,
466|Forecasting Network Performance to Support Dynamic Scheduling Using the Network Weather Service|The Network Weather Service is a generalizable and extensible facility designed to provide dynamic resource performance forecasts in metacomputing environments. In this paper, we outline its design and detail the predictive performance of the forecasts it generates. While the forecasting methods are general, we focus on their ability to predict the TCP/IP end-to-end throughput and latency that is attainable by an application using systems located at different sites. Such network forecasts are needed both to support scheduling [5], and by the metacomputing software infrastructure to develop quality-of-service guarantees [10, 17]. Keywords: scheduling, metacomputing, quality-ofservice, statistical forecasting, network performance monitoring  1. Introduction  As network technology advances, the resulting improvements in interprocess communication speeds make it possible to use interconnected but separate computer systems as a high-performance computational platform or metacomputer. Effect...
467|Characterizing End-to-End Packet Delay and Loss in the Internet|We use the measured round trip delays of small UDP probe packets sent at regular time intervals to characterize the end-to-end packet delay and loss behavior in the Internet. By varying the interval between probe packets, it is possible to study the structure of the Internet load over different time scales. In this paper, the time scales of interest range from a few milliseconds to a few minutes. Our observations agree with results obtained by others using simulation and experimental approaches. For example, our estimates of Internet workload are consistent with the hypothesis of a mix of bulk traffic with larger packet size, and interactive traffic with smaller packet size. The interarrival time distribution for Internet packets is consistent with an exponential distribution. We also observe a phenomenon of compression (or clustering) of the probe packets similar to the acknowledgement compression phenomenon recently observed in TCP. Our results also show interesting and less expected...
468|Autopilot: Adaptive control of distributed applications|With increasing development of applications for heterogeneous, distributed computing grids, the focus of performance analysis has shifted from a posteriori optimization on homogeneous parallel systems to application tuning for heterogeneous resources with time varying availability. This shift has profound implications for performance instrumentation and analysis techniques. Autopilot is a new infrastructure for dynamic performance tuning of heterogeneous computational grids based on closed loop control. This paper describes the Autopilot model of distributed sensors, actuators, and decision procedures, reports preliminary performance benchmarks, and presents a case study in which the Autopilot library is utilized in the development of an adaptive parallel input/output system.  
469|Proxy-Based Authorization and Accounting for Distributed Systems|Despite recent widespread interest in the secure authentication of principals across computer networks there has been considerably less discussion of distributed mechanisms to support authorization and accounting. By generalizing the authentication model to support restricted proxies, both authorization and accounting can be easily supported. This paper presents the proxy model for authorization and shows how the model can be used to support a wide range of authorization and accounting mechanisms. The proxy model strikes a balance between access-control-list and capability-based mechanisms allowing each to be used where appropriate and allowing their use in combination. The paper describes how restricted proxies can be supported using existing authentication methods.   
470|The NetLogger Methodology for High Performance Distributed Systems Performance Analysis|We describe a methodology that enables the real-time diagnosis of performance problems in complex high-performance distributed systems. The methodology includes tools for generating precision event logs that can be used to provide detailed end-to-end application and system level monitoring; a Java agent-based system for managing the large amount of logging data; and tools for visualizing the log data and real-time state of the distributed system. We developed these tools for analyzing a high-performance distributed system centered around the transfer of large amounts of data at high speeds from a distributed storage server to a remote visualization client. However, this methodology should be generally applicable to any distributed system.
471|Locating Objects in Wide-Area Systems|Locating mobile objects in a worldwide system requires a scalable location service. An object can be a telephone or a notebook computer, but also a software or data object, such as a file or an electronic document. Our service strictly separates an object&#039;s name from the addresses where it can be contacted. This is done by introducing a location-independent object handle. An object&#039;s name is bound to its unique object handle, which, in turn, is mapped to the addresses where the object can be contacted. To locate an object, we need only its object handle. We present a scalable location service based on a worldwide distributed search tree that adapts dynamically to an object&#039;s migration pattern to optimize lookups and updates. 
472|A Fault Detection Service for Wide Area Distributed Computations|The potential for faults in distributed computing systems is a significant complicating factor for application developers. While a variety of techniques exist for detecting and correcting faults, the implementation of these techniques in a particular context can be difficult. Hence, we propose a fault detection service designed to be incorporated, in a modular fashion, into distributed computing systems, tools, or applications. This service uses well-known techniques based on unreliable fault detectors to detect and report component failure, while allowing the user to tradeoff timeliness of reporting against false positive rates. We describe the architecture of this service, report on experimental results that quantify its cost and accuracy, and describe its use in two applications, monitoring the status of system components of the GUSTO computational grid testbed and as part of the NetSolve network-enabled numerical solver.
473|Online Prediction of the Running Time of Tasks|We describe and evaluate the Running Time Advisor (RTA), a system that can predict the running time of a compute-bound task on a typical shared, unreserved commodity host. The prediction is computed from linear time series predictions of host load and takes the form of a confidence interval that neatly expresses the error associated with the measurement and prediction processes--- error that must be captured to make statistically valid decisions based on the predictions. Adaptive applications make such decisions in pursuit of consistent high performance, choosing, for example, the host where a task is most likely to meet its deadline. We begin by describing the system and summarizing the results of our previously published work on host load prediction. We then describe our algorithm for computing predictions of running time from host load predictions. Finally, we evaluate the system using over 100,000 randomized testcases run on 39 different hosts.
474| 	 The Architecture of the Remos System       |Remos provides resource information to distributed applications. Its design goals of scalability, flexibility, and portability are achieved through an architecture that allows components to be positioned across the network, each collecting informationabout its local network. To collect information from different types of networks and from hosts on those networks, Remos provides several collectors that use different technologies, such as SNMP or benchmarking. By matching the appropriate collector to each particular network environment and by providing an architecture for distributing the output of these collectors across all querying environments, Remos collects appropriately detailed information at each site and distributes this information where needed in a scalable manner. Prediction services are integrated at the user-level, allowing history-based data collected across the network to be used to generate the predictions needed by a particular user. Remos has been implemented and tested in a variety of networks and is in use in a number of different environments. 
475|A scalable, deployable directory service framework for the internet|This paper describes a directory service framework for the Internet that fits within the approach outlined in the IETF’s RFC 1588. This framework consists of a global directory service that enables virtually any local directory service to operate under it. We also include an optimized local directory service, thereby providing a complete solution for Internet directory service. Our approach uses proven Internet technology (e.g., the Domain Name System and Uniform Resource Locators) and successful or promising pieces of other services (e.g., X.500 and WHOIS++). Previous attempts to create a unified Internet directory service, such as X.500, LDAP, WHOIS++, and SOLO, have not been fully accepted because of difficulties in implementation and deployment. Therefore, we designed our approach with ease of implementation and deployment in mind. To that end, our approach attempts to co-opt the installed base making a switch to the new service as seamless as possible.
476|White Paper: A Grid Monitoring Service Architecture (DRAFT)  (2001) |Large distributed systems such as Computational and Data Grids require a substantial amount of monitoring  data be collected for a variety of tasks such as fault detection, performance analysis, performance  tuning, performance prediction, and scheduling. Some tools are currently available and others  are being developed for collecting and forwarding this data. The goal of this paper is to describe a  common architecture with all the major components and their essential interactions in just enough  detail that Grid Monitoring systems that follow the architecture described can easily devise common  APIs and wire protocols. To aid implementation, we also discuss the performance characteristics of a  Grid Monitoring system and identify areas that are critical to proper functioning of the system.
477|The Protection of Information in Computer Systems|This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures--whether hardware or software--that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysis of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading. Glossary The following glossary provides, for reference, brief definitions for several terms as used in this paper in the context of protecting information in computers. Access The ability to make use of information stored in a computer system. Used frequently as a verb, to the horror of grammarians. Access control list A list of principals that are authorized to have access to some object. Authenticate To verify the identity of a person (or other agent external to the protection system) making a request.
478|The Unix Time-Sharing System|Unix is a general-purpose, multi-user, interactive operating system for the larger Digital Equipment Corporation PDP-11 and the Interdata 8/32 computers. It offers a number of features seldom found even in larger operating systems, including i A hierarchical file system incorporating demountable volumes, ii Compatible file, device, and inter-process I/O, iii The ability to initiate asynchronous processes, iv System command language selectable on a per-user basis, v Over 100 subsystems including a dozen languages, vi High degree of portability. This paper discusses the nature and implementation of the file system and of the user command interface. I.
479|A Note on the Confinement Problem|This not explores the problem of confining a program during its execution so that it cannot transmit information to any other program except its caller. A set of examples attempts to stake out the boundaries of the problem. Necessary conditions for a solution are stated and informally justified.
480|Programming Semantics for Multiprogrammed Computations|... between an assembly language and an advanced algebraic language.  
481|Protection and the control of information sharing in Multics|This document was originally prepared off-line. This file is the result of scan, OCR, and manual touchup, starting
482|Protection|The following paper by Butler Lampson has been frequently referenced. Because the original is not widely available, we are reprinting it here. If the paper is referenced in published work,
483|A hardware architecture for implementing protection rings|Protection of computations and information is an important aspect of a computer utility. In a system which usessegmentation as a memory addressing scheme, protection can be achieved in part by associating concentric rings of decreasing access privilege with a computation. This paper describes hardware processor mechanisms for implementing these rings of protection. The mechanisms allow cross-ring calls and subsequent returns to occur without trapping to the supervisor. Automatic hardware validation of referencesacross ring boundaries is also performed. Thus, a call by a user procedure to a protected subsystem (including the the supervisor) is identical to a call to a companion user procedure. The mechanisms of passing and referencing arguments are the same in both cases as well.
484|The Multics Virtual Memory: Concepts and Design|As experience with use of on-line operating systems has grown, the need to share information among system users has become increasingly apparent. Many contemporary systems permit some degree of sharing. Usually, sharing is accomplished by allowing several users to share data via input and output of information stored in files kept in secondary storage. Through the use of segmentation, however, Multics provides direct hardware addressing by user and system programs of all information, independent of its physical storage location. Information is stored in segments each of which is potentially sharable and carries its own independent attributes of size and access privilege. Here, the design and implementation considerations of segmentation and sharing in Multics are first discussed under the assumption that all information resides in a large, segmented main memory. Since the size of main memory on contemporary systems is rather limited, it is then shown how the Multics software achieves the effect of a large segmented main memory through the use of the Honeywell 645 segmentation and paging hardware.
485|A User Machine in a Time-Sharing System|This paper describes the design of the computer seen by a machine-language programmer in a timesharing system developed at the University of California at Berkeley. Some of the instructions in this machine are executed by the hardware, and some are implemented by software. The user, however, thinks of them all as part of his machine, a machine having extensive and unusual capabilities, many of which might be part of the hardware of a (considerably more expensive) computer. Among the important features of the machine are the arithmetic and string manipulation instructions, the very general memory allocation and configuration mechanism, and the multiple processes which can be created by the program. Facilities are provided for communication among these processes and for the control of exceptional conditions. The input-output system is capable of handling all of the peripheral equipment in a uniform and convenient manner through files having symbolic names. Programs can access files belonging to a number of people, but each person can protect his own files from unauthorized access by others. Some mention is made at various points of the techniques of implementation, but the main emphasis is on the appearance of the user&#039;s machine. 
486|Ongoing research and development on information protection|Many individuals involved in the projects described here spent time patiently explaining their activities to me, putting together written descriptions for me to work from, and reviewing early drafts of my (often muddled) writeups of their research. Thanks are due all of them, but responsibility for mistakes and omissions is my own.
487|N Degrees of Separation: Multi-Dimensional Separation of Concerns|Done well, separation of concerns can provide many software engineering benefits, including reduced complexity, improved reusability, and simpler evolution. The choice of boundaries for separate concerns depends on both requirements on the system and on the kind(s) of decompositionand composition a given formalism supports. The predominant methodologies and formalisms available, however, support only orthogonal separations of concerns, along single dimensions of composition and decomposition. These characteristics lead to a number of well-known and difficult problems. This paper describes a new paradigm for modeling and implementing software artifacts, one that permits separation of overlapping concerns along multiple dimensions of composition and decomposition. This approach addresses numerous problems throughout the software lifecycle in achieving well-engineered, evolvable, flexible software artifacts and traceability across artifacts.
488|A Framework for Expressing the Relationships Between Multiple Views in Requirements Specification|Composite systems are generally comprised of heterogeneous components whose specifications are developed by many development participants. The requirements of such systems are invariably elicited from multiple perspectives which overlap, complement and contradict each other. Furthermore, these requirements are generally developed and specified using multiple methods and notations respectively. It is therefore necessary to express and check the relationships between the resultant specification fragments.  In this paper we deploy multiple &#034;ViewPoints&#034; that hold partial requirements specifications, described and developed using different representation schemes and development strategies. We discuss the notion of interViewPoint communication in the context of this ViewPoints framework, and propose a general model for ViewPoint interaction and integration. We elaborate on some of the requirements for expressing and enacting inter-ViewPoint relationships - the vehicles for consistency che...
489|D: A LANGUAGE FRAMEWORK FOR DISTRIBUTED PROGRAMMING|Two of the most important issues in distributed systems are the synchronization of concurrent threads and the application-level data transfers between execution spaces. At the design level, addressing these issues typically requires analyzing the components under a different perspective than is required to analyze the functionality. Very often, it also involves analyzing several components at the same time, because of the way those two issues cross-cut the units of functionality. At the implementation level, existing programming languages fail to provide adequate support for programming in terms of these different and cross-cutting perspectives. The result is that the programming of synchronization and remote data transfers ends up being tangled throughout the components code in more or less arbitrary ways. This thesis presents a language framework called D that untangles the implementation of synchronization
490|Using role components to implement collaboration-based design|In this paper we present a method of code imple-mentation that works in conjunction with collab-oration and responsibility based analysis model-ing techniques to achieve better code reuse and resilience to change. Our approach maintains a closer mapping from responsibilities in the analy-sis model to entities in the implementation. In so doing, it leverages the features of flexible design and design reuse found in collaboration-based de-sign models to provide similar adaptability and reuse in the implementation. Our approach re-quires no special development tools and uses only standard features available in the C++ language. In an earlier paper we described the basic mech-anisms used by our approach and discussed its advantages in comparison to the framework ap-proach. In this paper we show how our approach combines code and design reuse, describing spe-cific techniques that can be used in the develop-ment of larger applications. 1
491|Adaptive Plug-and-Play Components for Evolutionary Software Development|In several works on design methodologies, design patterns, programming language design  and application frameworks, the need for adequate higher-level program entities that capture  the patterns of collaboration between several classes has been recognized. The idea is that  in general the unit of reuse is not a single class, but a slice of behavior affecting a set of collaborating  classes. The absence of large-scale components for expressing these collaborations  makes object-oriented programs more difficult to maintain and reuse, because functionality  is spread over several methods and it becomes difficult to get the &#034;big picture&#034;. In this paper,  we propose Adaptive Plug and Play Components to serve this need. These components are  designed such that they not only facilitate the construction of complex software by making  the collaborations explicit, but they do so in a manner that supports the evolutionary nature  of both structure and behavior.  1 Introduction  The step from proc...
492|Specifying reusable components using Contracts|Contracts were introduced by Helm et al. as a high level construct for explicitly specifying interactions among groups of objects. This paper describes further developments and application of the Contract construct. We show how Contracts can be used to represent classic algorithms as large grained reusable object oriented abstractions, how these algorithms can be customized through Contract refinement and how they are reused through Contract  conformance. The example algorithm used throughout is the classic graph depth first traversal algorithm. This algorithm is represented as a Contract which is then refined to specify algorithms which number connected regions of graphs and which check graphs for cycles. Changes to the Contract language are introduced and we discuss some new problems resulting from the simultaneous reuse of related contracts. 1 Introduction  Contracts were introduced by Helm et al. [9] as a construct for explicitly specifying interactions among groups of objects. The...
493|Issues Encountered in Building a Flexible Software Development Environment: Lessons Learned From the Arcadia Project|This paper presents some of the more signi cant technical lessons that the Arcadia project has learned about developing e ective software development environments. The principal components of the Arcadia-1 architecture are capabilities for process de nition and execution, object management, user interface development and management, measurement and evaluation, language processing, and analysis and testing. In simultaneously and cooperatively developing solutions in these areas welearned several key lessons. Among them: the need to combine and apply heterogenous componentry, multiple techniques for developing components, the pervasive need for rich type models, the need for supporting dynamism (and at what granularity), the role and value of concurrency, and the role and various forms of event-based control integration mechanisms. These lessons are explored in the paper. 1
494|System Design by Composing Structures of Interacting Objects|This paper describes the outline of an object-oriented design technique denoted role modeling, emphasizing the ability to compose parts of a design. The purpose of role modeling is to achieve separation of concerns, allowing the designer to consider different aspects, or the same aspect at different levels of detail, more or less independent of other aspects of the overall design.  A role model represents the concept of a structure of communicating objects; each object being represented by a role to be &#039;played&#039; in the context of this role model. Each role model is considered a design of a separate aspect of some overall design. Composition of designs is achieved by synthesizing roles in several role models, constructing more aggregated and specialized roles and role models.  Keywords: O-O Design, Interaction-Oriented Design, Role Modeling, Object Composition  1 Introduction  Our basis is best presented by the words of Beck &amp; Cunningham[Helm], &#039;...no object is an island&#039;. We consider re...
495|Operation-Level Composition: A Case in (Join) Point  (1998) |this document.
496|Subject-Oriented Programming: Supporting Decentralized Development|iented programming is an enhancement of object-oriented programming that allows decentralized class definition. An application developer who needs new operations associated with classes can implement them him/herself, not by editing existing code for the classes, but as a separate collection of class definitions called a subject. Multiple subjects can be composed to yield a complete suite of applications; class definitions within the subjects will be combined so as to satisfy the needs of all the applications in the suite. A simple example is shown in Figure 1. Neither source code access nor recompilation are required to perform this composition, allowing extension and composition of object-code-only applications. Without eliminating the advantages of encapsulation, this approach eliminates the need for class ownership, and hence for the second, more serious kind of negotiation noted above. An application developer can write all the code needed for the application, irrespective of whic
498|Software engineering|Abstract — The commonly used carburetted two-stroke engines in developing countries have high exhaust emission and poor fuel efficiency. To meet more rigid emissions requirements, two-stroke vehicles are typically phase out in favour of four-stroke engines. The problems of ubiquitous legacy two-stroke vehicles remain unsolved by these measures and they are likely to be a major source of transport for many years to come. A number of technologies are available for solving the problems associated with two-stroke engines such as catalytic after-treatment and direct fuel injection (DI). However, these solutions are relatively high cost and have shown only slow market acceptance for applications in developing countries. Research in recent years has demonstrated that direct fuel injection is a well developed and readily deployable solution to existing two-stroke engines. Gaseous fuels such as Liquefied Petroleum Gas (LPG) are considered a promising energy source and in many countries provide fuel cost savings. LPG coupled with DI two-stroke technologies, is expected to be clean and cost effective retrofit solution for two-stroke engines. In this research project, direct injection
499|The x-Kernel: An Architecture for Implementing Network Protocols|This paper describes a new operating system kernel, called the x-kernel, that provides an  explicit architecture for constructing and composing network protocols. Our experience  implementing and evaluating several protocols in the x-kernel shows that this architecture  is both general enough to accommodate a wide range of protocols, yet efficient enough to  perform competitively with less structured operating systems.  1 Introduction  Network software is at the heart of any distributed system. It manages the communication hardware that connects the processors in the system and it defines abstractions through which processes running on those processors exchange messages. Network software is extremely complex: it must hide the details of the underlying hardware, recover from transmission failures, ensure that messages are delivered to the application processes in the appropriate order, and manage the encoding and decoding of data. To help manage this complexity, network software is divi...
500|A Survey of active network Research|Active networks are a novel approach to network architecture in which the switches of the network perform customized computations on the messages flowing through them. This approach is motivated by both lead user applications, which perform user-driven computation at nodes within the network today, and the emergence of mobile code technologies that make dynamic network service innovation attainable. In this paper, we discuss two approaches to the realization of active networks and provide a snapshot of the current research issues and activities. Introduction – What Are Active Networks? In an active network, the routers or switches of the network perform customized computations on the messages flowing through them. For example, a user of an active network could send a “trace ” program to each router and arrange for the program to be executed when their packets are processed. Figure 1 illustrates how the routers of an IP
501|Wide-area Internet traffic patterns and characteristics|Abstract – The Internet is rapidly growing in number of users, traffic levels, and topological complexity. At the same time it is increasingly driven by economic competition. These developments render the characterization of network usage and workloads more difficult, and yet more critical. Few recent studies have been published reporting Internet backbone traffic usage and characteristics. At MCI, we have implemented a high-performance, low-cost monitoring system that can capture traffic and perform analyses. We have deployed this monitoring tool on OC-3 trunks within internetMCI’s backbone and also within the NSF-sponsored vBNS. This paper presents observations on the patterns and characteristics of wide-area Internet traffic, as recorded by MCI’s OC-3 traffic monitors. We report on measurements from two OC-3 trunks in MCI’s commercial Internet backbone over two time ranges (24-hour and 7-day) in the presence of up to 240,000 flows. We reveal the characteristics of the traffic in terms of packet sizes, flow duration, volume, and percentage composition by protocol and application, as well as patterns seen over the two time scales. 1
502|Internet Control Message Protocol|The Internet Protocol (IP) [1] is used for host-to-host datagram service in a system of interconnected networks called the Catenet [2]. The network connecting devices are called Gateways. These gateways communicate between themselves for control purposes
503|Eliminating receive livelock in an interrupt-driven kernel|Most operating systems use interface interrupts to schedule network tasks. Interrupt-driven systems can provide low overhead and good latency at low of-fered load, but degrade significantly at higher arrival rates unless care is taken to prevent several pathologies. These are various forms of receive livelock, in which the system spends all its time processing interrupts, to the exclusion of other neces-sary tasks. Under extreme conditions, no packets are delivered to the user application or the output of the system. To avoid livelock and related problems, an operat-ing system must schedule network interrupt handling as carefully as it schedules process execution. We modified an interrupt-driven networking implemen-tation to do so; this eliminates receive livelock without degrading other aspects of system performance. We present measurements demonstrating the success of our approach. 1.
504|A Stream Input-Output System|In a new version of the Unix operating system, a flexible coroutine-based design replaces the traditional rigid connection between processes and terminals or networks. Processing modules may be inserted dynamically into the stream that connects a user&#039;s program to a device. Programs may also connect directly to programs, providing interprocess communication. Introduction  The part of the Unix operating system that deals with terminals and other character devices has always been complicated. In recent versions of the system it has become even more so, for two reasons. 1) Network connections require protocols more ornate than are easily accommodated in the existing structure. A notion of &#034;line disciplines&#034; was only partially successful, mostly because in the traditional system only one line discipline can be active at a time. 2) The fundamental data structure of the traditional character I/O system, a queue of individual characters (the &#034;clist&#034;), is costly because it accepts and dispense...
505|Small forwarding tables for fast routing lookups|For some time, the networking community has assumed that it is impossible to do IP routing lookups in software fast enough to support gigabit speeds. IP routing lookups must ?nd the routing entry with the longest matching pre?x, a task that has been thought to require hardware support at lookup frequencies of millions per second. We present a forwarding table data structure designed for quick routing lookups. Forwarding tables are small enough to ?t in the cache of a conventional general purpose processor. With the table in cache, a 200 MHz Pentium Pro or a 333 MHz Alpha 21164 can perform a few million lookups per second. This means that it is feasible to do a full routing lookup for each IPpacket at gigabit speeds without special hardware. The forwarding tables are very small, a large routing table with 40,000 routing entries can be compacted to a forwarding table of 150?160 Kbytes. A lookup typically requires less than 100 instructions on an Alpha, using eight memory references accessing a total of 14 bytes. 1
506|Stride Scheduling: Deterministic Proportional-Share Resource Management|This paper presents stride scheduling, a deterministic scheduling technique that efficiently supports the same flexible resource management abstractions introduced by lottery scheduling. Compared to lottery scheduling, stride scheduling achieves significantly improved accuracy over relative throughput rates, with significantly lower response time variability. Stride scheduling implements proportional-share control over processor time and other resources by cross-applying elements of rate-based flow control algorithms designed for networks. We introduce new techniques to support dynamic changes and higher-level resource management abstractions. We also introduce a novel hierarchical stride scheduling algorithm that achieves better throughput accuracy and lower response time variability than prior schemes. Stride scheduling is evaluated using both simulations and prototypes implemented for the Linux kernel.
507|Router plugins: A software architecture for next generation routers|Present day routers typically employ monolithic operating systems which are not easily upgradahle and extensible. With the rapid rate of protocol development it is becoming increasingly important to dynamically upgrade router software in an incre-mental fashion. We have designed and implemented a high performance, modular, extended integrated services router software architecture in the NetBSD operating system kernel. This architecture allows code modules, called plugins, to be dynamically added and configured at run time. One of the novel features of our design is the ability to bind different plugins to individual flows; this allows for distinct plugin implementations to seamlessly coexist in the same runtime environment. High performance is achieved through a carefully designed modular architecture; an innovative packet classification algorithm that is both powerful and highly efficient; and by caching that exploits the flow-like character-istics of Internet traffic. Compared to a monolithic best-effort kernel, our implementation requires an average increase in packet processing overhead of only 8 % , or 500 cycles/2.lms per packet when run-ning on a P61233. 1.1 Keywords High performance integrated services routing, modular router architecture, router plugins 2.
508|Definitions of the differentiated service field (DS field  (1998) |This document is an Internet-Draft. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months and may be updated, replaced, or obsoleted by other documents at any time. It is inappropriate to use Internet-Drafts as reference material or to cite them other than as &#034;work in progress.&#034; To view the entire list of current Internet-Drafts, please check the &#034;1id-abstracts.txt &#034; listing contained in the Internet-Drafts Shadow Directories on ftp.is.co.za (Africa), ftp.nordu.net (Northern
509|A framework for alternate queueing: Towards traffic management by PC-UNIX based routers|Queueing is an essential element of traffic management, but the only queueing discipline used in traditional UNIX systems is simple FIFO queueing. This paper describes ALTQ, a queueing framework that allows the use of a set of queueing disciplines. We investigate the issues involved in designing a generic queueing framework as well as the issues involved in implementing various queueing disciplines. ALTQ is implemented as simple extension to the FreeBSD kernel including minor fixes to the device drivers. Several queueing disciplines including CBQ, RED, and WFQ are implemented onto the framework to demonstrate the design of ALTQ. The traffic management performance of a PC is presented to show the feasibility of traffic management by PC-UNIX based routers. 1
510|The Fuzzball|The Fuzzball is an operating system and applications library designed for the PDP11 family of  computers. It was intended as a development platform and research pipewrench for the DARPA/NSF  Internet, but has occasionally escaped to earn revenue in commercial service. It was designed,  implemented and evolved over a seventeen-year era spanning the development of the ARPANET  and TCP/IP protocol suites and can today be found at Internet outposts from Hawaii to Italy standing  watch for adventurous applications and enduring experiments. This paper describes the Fuzzball and  its applications, including a description of its novel congestion avoidance/control and timekeeping  mechanisms.&lt;E-110&gt; Keywords: protocol testing, network testing, performance evaluation, Internet architecture, TCP/IP protocols, congestion control, internetwork time synchronization.&lt;E-132&gt; 1. Introduction&lt;E-128&gt; The Fuzzball is a software package consisting of a fast, compact operating system, support for the DARPA/...
511|OS Support for General-Purpose Routers|This paper argues that there is a need for routers to move from being closed, special-purpose network devices to being open, general-purpose computing/communication systems. The central challenge in making this shift is to simultaneously support increasing complex forwarding logic and high performance, while using commercial hardware components and open operating systems. This paper introduces the hardware and software architecture for such a general-purpose router. The architecture includes two key innovations. First, it better integrates the router&#039;s switching capacity and compute cycles. We expect this to result in significantly better scaling properties, and an order of magnitude improvement in performance for packets that require only minimum processing cycles. Second, the architecture supports a hierarchy of forwarding paths, ranging from fast/fixed paths implemented entirely in hardware to slow/programmable paths implemented entirely in software, but also including intermediate ...
512|Wireless sensor networks: a survey|This paper describes the concept of sensor networks which has been made viable by the convergence of microelectro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the potential sensor networks applications are explored, and a review of factors influencing the design of sensor networks is provided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are
513|Directed Diffusion: A scalable and robust communication paradigm for sensor networks|Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.
514|Energy-efficient communication protocol for wireless microsensor networks|Wireless distributed microsensor systems will enable the reliable monitoring of a variety of environments for both civil and military applications. In this paper, we look at communication protocols, which can have significant impact on the overall energy dissipation of these networks. Based on our findings that the conventional protocols of direct transmission, minimum-transmission-energy, multihop routing, and static clustering may not be optimal for sensor networks, we propose LEACH (Low-Energy Adaptive Clustering Hierarchy), a clustering-based protocol that utilizes randomized rotation of local cluster base stations (cluster-heads) to evenly distribute the energy load among the sensors in the network. LEACH uses localized coordination to enable scalability and robustness for dynamic networks, and incorporates data fusion into the routing protocol to reduce the amount of information that must be transmitted to the base station. Simulations show that LEACH can achieve as much as a factor of 8 reduction in energy dissipation compared with conventional routing protocols. In addition, LEACH is able to distribute energy dissipation evenly throughout the sensors, doubling the useful system lifetime for the networks we simulated. 
515|SPINS: Security Protocols for Sensor Networks|As sensor networks edge closer towards wide-spread deployment, security issues become a central concern. So far, the main research focus has been on making sensor networks feasible and useful, and less emphasis was placed on security. We design a suite of security building blocks that are optimized for resource-constrained environments and wireless communication. SPINS has two secure building blocks: SNEP and TESLA. SNEP provides the following important baseline security primitives: Data con£dentiality, two-party data authentication, and data freshness. A particularly hard problem is to provide efficient broad-cast authentication, which is an important mechanism for sensor networks. TESLA is a new protocol which provides authenticated broadcast for severely resource-constrained environments. We implemented the above protocols, and show that they are practical even on minimalistic hardware: The performance of the protocol suite easily matches the data rate of our network. Additionally, we demonstrate that the suite can be used for building higher level protocols. 
516|The Cricket Location-Support System|This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, locationdependent applications. It allows applications running on mobile and static nodes to learn their physical location by using listeners that hear and analyze information from beacons spread throughout the building. Cricket is the result of several design goals, including user privacy, decentralized administration, network heterogeneity, and low cost. Rather than explicitly tracking user location, Cricket helps devices learn where they are and lets them decide whom to advertise this information to; it does not rely on any centralized management or control and there is no explicit coordination between beacons; it provides information to devices regardless of their type of network connectivity; and each Cricket device is made from off-the-shelf components and costs less than U.S. $10. We describe the randomized algorithm used by beacons to transmit information, the use of concurrent radio and ultrasonic signals to infer distance, the listener inference algorithms to overcome multipath and interference, and practical beacon configuration and positioning techniques that improve accuracy. Our experience with Cricket shows that several location-dependent applications such as in-building active maps and device control can be developed with little effort or manual configuration.  1 
517|Geography-informed Energy Conservation for Ad Hoc Routing|We introduce a geographical adaptive fidelity (GAF) algorithm that reduces energy consumption in ad hoc wireless networks. GAF conserves energy by identifying nodes that are equivalent from a routing perspective and then turning off unnecessary nodes, keeping a constant level of routing fidelity. GAF moderates this policy using application- and system-level information; nodes that source or sink data remain on and intermediate nodes monitor and balance energy use. GAF is independent of the underlying ad hoc routing protocol; we simulate GAF over unmodified AODV and DSR. Analysis and simulation studies of GAF show that it can consume 40% to 60% less energy than an unmodified ad hoc routing protocol. Moreover, simulations of GAF suggest that network lifetime increases proportionally to node density; in one example, a four-fold increase in node density leads to network lifetime increase for 3 to 6 times (depending on the mobility pattern). More generally, GAF is an example of adaptive fidelity, a technique proposed for extending the lifetime of self-configuring systems by exploiting redundancy to conserve energy while maintaining application fidelity.   
518|Minimum energy mobile wireless networks| We describe a distributed position-based network protocol optimized for minimum energy consumption in mobile wireless networks that support peer-to-peer communications. Given any number of randomly deployed nodes over an area, we illustrate that a simple local optimization scheme executed at each node guarantees strong connectivity of the entire network and attains the global minimum energy solution for stationary networks. Due to its localized nature, this protocol proves to be self-reconfiguring and stays close to the minimum energy solution when applied to mobile networks. Simulation results are used to verify the performance of the protocol. 
519|Next Century Challenges: Mobile Networking for “Smart Dust”|Large-scale networks of wireless sensors are becoming an active topic of research. Advances in hardware technology and engineering design have led to dramatic reductions in size, power consumption and cost for digital circuitry, wire-less communications and Micro ElectroMechanical Systems (MEMS). This has enabled very compact, autonomous and mobile nodes, each containing one or more sensors, computation and communication capabilities, and a power supply. The missing ingredient is the networking and applications layers needed to harness this revolutionary capability into a complete system. We review the key elements of the emergent technology of “Smart Dust ” and outline the research challenges they present to the mobile networking and systems community, which must provide coherent connectivity to large numbers of mobile network nodes co-located within a small volume. 
520|I-TCP: Indirect TCP for mobile hosts|Abstract — IP-based solutions to accommodate mobile hosts within existing internetworks do not address the distinctive features of wireless mobile computing. IP-based transport protocols thus suffer from poor performance when a mobile host communicates with a host on the fixed network. This is caused by frequent disruptions in network layer connectivity due to — i) mobility and ii) unreliable nature of the wireless link. We describe the design and implementation of I-TCP, which is an indirect transport layer protocol for mobile hosts. I-TCP utilizes the resources of Mobility Support Routers (MSRs) to provide transport layer communication between mobile hosts and hosts on the fixed network. With I-TCP, the problems related to mobility and the unreliability of wireless link are handled entirely within the wireless link; the TCP/IP software on the fixed hosts is not modified. Using I-TCP on our testbed, the throughput between a fixed host and a mobile host improved substantially in comparison to regular TCP. 1
521|Protocols for self-organization of a wireless sensor network|We present a suite of algorithms for self-organization of wireless sensor networks, in which there is a scalably large number of mainly static nodes with highly constrained energy resources. The protocols further support slow mobility by a subset of the nodes, energy-efficient routing, and formation of ad hoc subnetworks for carrying out cooperative signal processing functions among a set of the nodes.
522|A Transmission Control Scheme for Media Access in Sensor Networks|We study the problem of media access control in the novel regime of sensor networks, where unique application behavior and tight constraints in computation power, storage, energy resources, and radio technology have shaped this design space to be very different from that found in traditional mobile computing regime. Media access control in sensor networks must not only be energy efficient but should also allow fair bandwidth allocation to the infrastructure for all nodes in a multihop network. We propose an adaptive rate control mechanism aiming to support these two goals and find that such a scheme is most effective in achieving our fairness goal while being energy effcient for both low and high duty cycle of network traffic.
523|ASCENT: Adaptive self-configuring sensor networks topologies| Advances in microsensor and radio technology will enable small but smart sensors to be deployed for a wide range of environmental monitoring applications. The low per-node cost will allow these wireless networks of sensors and actuators to be densely distributed. The nodes in these dense networks will coordinate to perform the distributed sensing and actuation tasks. Moreover, as described in this paper, the nodes can also coordinate to exploit the redundancy provided by high density so as to extend overall system lifetime. The large number of nodes deployed in these systems will preclude manual configuration, and the environmental dynamics will preclude design-time preconfiguration. Therefore, nodes will have to self-configure to establish a topology that provides communication under stringent energy constraints. ASCENT builds on the notion that, as density increases, only a subset of the nodes are necessary to establish a routing forwarding backbone. In ASCENT, each node assesses its connectivity and adapts its participation in the multihop network topology based on the measured operating region. This paper motivates and describes the ASCENT algorithm and presents analysis, simulation, and experimental measurements. We show that the system achieves linear increase in energy savings as a function of the density and the convergence time required in case of node failures while still providing adequate connectivity. 
524|Instrumenting the world with wireless sensor networks|Pervasive micro-sensing and actuation may revolutionize the way in which we understand and manage complex physical systems: from airplane wings to complex ecosystems. The capabilities for detailed physical monitoring and manipulation offer enormous opportunities for almost every scientific discipline, and it will alter the feasible granularity of engineering. We identify opportunities and challenges for distributed signal processing in networks of these sensing elements and investigate some of the architectural challenges posed by systems that are massively distributed, physically-coupled, wirelessly networked, and energy limited. 
525|Smart Dust: Communicating with a Cubic-Millimeter Computer|building virtual keyboards;  . managing inventory control;  . monitoring product quality;  . constructing smart office spaces; and  . providing interfaces for the disabled.  SMART DUST REQUIREMENTS  Smart Dust requires both evolutionary and revolutionary  advances in miniaturization, integration, and  energy management. Designers can use microelectromechanical  systems (MEMS) to build small sensors,  optical communication components, and power supplies,  whereas microelectronics provides increasing  functionality in smaller areas, with lower energy consumption.  Figure 1 shows the conceptual diagram of  a Smart Dust mote. The power system consists of a  thick-film battery, a solar cell with a charge-integrating  capacitor for periods of darkness, or both.  Depending on its objective, the design integrates various  sensors, including light, temperature, vibration,  magnetic field, acoustic, and wind shear, onto the  mote. An integrated circuit provides sensor-signal pr
526|The Simulation and Evaluation of Dynamic Voltage Scaling Algorithms|The reduction of energy consumption in microprocessors can be accomplished without impacting the peak performance through the use of dynamic voltage scaling (DVS). This approach varies the processor voltage under software control to meet dynamically varying performance requirements. This paper presents a foundation for the simulation and analysis of DVS algorithms. These algorithms are applied to a benchmark suite specifically targeted for PDA devices. 2.
527|Comparing Algorithms for Dynamic Speed-Setting of a Low-Power CPU|To take advantage of the full potential of ubiquitous computing, we will need systems which minimize powerconsumption. Weiser et al. and others have suggested that this may be accomplished by a CPU which dynamically changes speed and voltage, thereby saving energy by spreading run cycles into idle time. Here we continue this research, using a simulation to compare a number of policies for dynamic speed-setting. Our work clarifies a fundamental power vs. delay tradeoff, as well as the role of prediction and of smoothing in dynamic speed-setting policies. We conclude that success seemingly depends more on simple smoothing algorithms than on sophisticated prediction techniques, but defer to the replication of these results on future variable-speed systems. 1 Introduction  Recent developments in ubiquitous computing make it likely that the future will see a proliferation of cordless computing devices. Clearly it will be advantageous for such devices to minimize power-consumption. The top p...
528|Power Efficient Organization of Wireless Sensor Networks|Abstract-- Wireless sensor networks have emerged recently as an effective way of monitoring remote or inhospitable physical environments. One of the major challenges in devising such networks lies in the constrained energy and computational resources available to sensor nodes. These constraints must be taken into account at all levels of system hierarchy. The deployment of sensor nodes is the first step in establishing a sensor network. Since sensor networks contain a large number of sensor nodes, the nodes must be deployed in clusters, where the location of each particular node cannot be fully guaranteed a priori. Therefore, the number of nodes that must be deployed in order to completely cover the whole monitored area is often higher than if a deterministic procedure were used. In networks with stochastically placed nodes, activating only the necessary number of sensor nodes at any particular moment can save energy. We introduce a heuristic that selects mutually exclusive sets of sensor nodes, where the members of each of those sets together completely cover the monitored area. The intervals of activity are the same for all sets, and only one of the sets is active at any time. The experimental results demonstrate that by using only a subset of sensor nodes at each moment, we achieve a significant energy savings while fully preserving coverage. I.
529|Achieving MAC Layer Fairness in Wireless Packet Networks|Link-layer fairness models that have been proposed for wireline and packet cellular networks cannot be generalized for shared channel wireless networks because of the unique characteristics of the wireless channel, such as location-dependent contention, inherent conflict between optimizing channel utilization and achieving fairness, and the absence of any centralized control. In this paper, we propose a general analytical framework that captures the unique characteristics of shared wireless channels and allows the modeling of a large class of systemwide fairness models via the specification of per-flow utility functions. We show that system-wide fairness can be achieved without explicit global coordination so long as each node executes a contention resolution algorithm that is designed to optimize its local utility function. We present a general mechanism for translating a given fairness model in our framework into a corresponding contention resolution algorithm. Using this translation...
530|Physical Layer Driven Protocol and Algorithm Design for Energy-Efficient Wireless Sensor Networks|The potential for collaborative, robust networks of microsensors has attracted a great deal of research attention. For the most part, this is due to the compelling applications that will be enabled once wireless microsensor networks are in place
532|Upper Bounds on the Lifetime of Sensor Networks|In this paper, we ask a fundamental question concerning the limits of energy e#ciency of sensor networks - What is the upper bound on the lifetime of a sensor network that collects data from a specified region using a certain number of energy-constrained nodes? The answer to this question is valuable for two main reasons. First, it allows calibration of real world data-gathering protocols and an understanding of factors that prevent these protocols from approaching fundamental limits. Secondly, the dependence of lifetime on factors like the region of observation, the source behavior within that region, basestation location, number of nodes, radio path loss characteristics, e#ciency of node electronics and the energy available on a node, is exposed. This allows architects of sensor networks to focus on factors that have the greatest potential impact on network lifetime. By employing a combination of theory and extensive simulations of constructed networks, we show that in all data gathe...
533|Robust Range Estimation Using Acoustic and Multimodal Sensing|Many applications of robotics and embedded sensor technology can benet from ne-grained localization. Fine-grained localization can simplify multi-robot collaboration, enable energy ecient multi-hop routing for low-power radio networks, and enable automatic calibration of distributed sensing systems. In this work we focus on range estimation, a critical prerequisite for ne-grained localization. While many mechanisms for range estimation exist, any individual mode of sensing can be blocked or confused by the environment. We present and analyze an acoustic ranging system that performs well in the presence of many types of interference, but can return incorrect measurements in non-line-of-sight conditions. We then suggest how evidence from an orthogonal sensory channel might be used to detect and eliminate these measurements. This work illustrates the more general research theme of combining multiple modalities to obtain robust results. 1 
534|Minimum energy mobile wireless networks revisited|Energy conservation is a critical issue in designing wireless ad hoc networks, as the nodes are powered by batteries only. Given a set of wireless network nodes, the directed weighted transmission graph Gt has an edge uv if and only if node v is in the transmission range of node u and the weight of uv is typically defined as II,,vll + c for a constant 2 &lt;_ t ~ &lt; 5 and c&gt; O. The minimum power topology Gm is the smallest subgraph of Gt that contains the shortest paths between all pairs of nodes, i.e., the union of all shortest paths. In this paper, we described a distributed position-based networking protocol to construct an enclosure graph G~, which is an approximation of Gin. The time complexity of each node u is O(min(dG ~ (u)dG ~ (u), dG ~ (u) log dG ~ (u))), where dc(u) is the degree of node u in a graph G. The space required at each node to compute the minimum power topology is O(dG ~ (u)). This improves the previous result that computes Gm in O(dG, (u) a) time using O(dGt(U) 2) spaces. We also show that the average degree dG,(u) is usually a constant, which is at most 6. Our result is first developed for stationary network and then extended to mobile networks. I.
535|Exposure In Wireless Ad-Hoc Sensor Networks|Wireless ad-hoc sensor networks will provide one of the missing connections between the Internet and the physical world. One of the fundamental problems in sensor networks is the calculation of coverage. Exposure is directly related to coverage in that it is a measure of how well an object, moving on an arbitrary path, can be observed by the sensor network over a period of time.  In addition to the informal definition, we formally define exposure and study its properties. We have developed an efficient and effective algorithm for exposure calculation in sensor networks, specifically for finding minimal exposure paths. The minimal exposure path provides valuable information about the worst case exposure-based coverage in sensor networks. The algorithm works for any given distribution of sensors, sensor and intensity models, and characteristics of the network. It provides an unbounded level of accuracy as a function of run time and storage. We provide an extensive collection of experimental results and study the scaling behavior of exposure and the proposed algorithm for its calculation.  I. 
536|Sensor Information Networking Architecture and Applications|This article introduces a sensor information networking architecture, called SINA, that facilitates querying, monitoring, and tasking of sensor networks. SINA plays the role of a middleware that abstracts a network of sensor nodes as a collection of massively distributed objects. The SINA&#039;s execution environment provides a set of configuration and communication primitives that enable scalable and energy-efficient organization of and interactions among sensor objects. On top the execution environment is a programmable substrate that provides mechanisms to create associations and coordinate activities among sensor nodes. Users then access information within a sensor network using declarative queries, or perform tasks using programming scripts.
537|Adaptive Frame Length Control for Improving Wireless Link Throughput, Range, and Energy Efficiency|Wireless network links are characterized by rapidly  time varying channel conditions and battery energy limitations at  the wireless mobile user nodes. Therefore static link control techniques  that make sense in comparatively well behaved wired links  do not necessarily apply to wireless links. New adaptive link layer  control techniques are needed to provide robust and energy efficient  operation even in the presence of orders of magnitude variations  in bit error rates and other radio channel conditions. For  example, recent research has advocated adaptive link layer techniques  such as adaptive error control [Lettieri97], channel state  dependent protocols [Bhagwat96, Fragouli97], and variable  spreading gain [Chien97]. In this paper we explore one such  adaptive technique: dynamic sizing of the MAC layer frame, the  atomic unit that is sent through the radio channel. A trade-off  exists between the desire to reduce header and physical layer  overhead by making frames large, and th...
538|Scalable coordination for wireless sensor networks: self-configuring localization systems|Pervasive networks of micro-sensors and actuators offer to revolutionize the ways in which we understand and construct complex physical systems. Sensor networks must be scalable, long-lived and robust systems, overcoming energy limitations and a lack of pre-installed infrastructure. We explore three themes in the design of self-configuring sensor networks: tuning density to trade operational quality against lifetime; using multiple sensor modalities to obtain robust measurements; and exploiting fixed environmental characteristics. We illustrate these themes through the problem of localization, which is a key building block for sensor systems that itself requires coordination.
539|Error Control and Energy Consumption in Communications for Nomadic Computing|We consider the problem of communications over a wireless channel in support of data transmissions from the  perspective of small portable devices that must rely on limited battery energy. We model the channel outages as statistically  correlated errors. Classic ARQ strategies are found to lead to a considerable waste of energy, due to the large number of  transmissions. The use of finite energy sources in the face of dependent channel errors leads to new protocol design criteria. As an  example, a simple probing scheme, which slows down the transmission rate when the channel is impaired, is shown to be more  energy efficient, with a slight loss in throughput. A modified scheme that yields slightly better performance but requires some  additional complexity is also studied. Some references on the modeling of battery cells are discussed to highlight the fact that  battery charge capacity is strongly influenced by the available &#034;relaxation time&#034; between current pulses. A formal approach ...
540|Intelligent Medium Access for Mobile Ad Hoc Networks with Busy Tones and Power Control|In a mobile ad-hoc networks (MANET), one essential issue is how to increase channel utilization while avoiding the hidden-terminal and the exposed terminal problems. Several MAC protocols, such as RTS/CTS-based and busytone-based schemes, have been proposed to alleviate these problems. In this paper, we explore the possibility of combining the concept of power control with the RTS/CTS-based and busy-tone-based protocols to further increase channel utilization. A sender will use an appropriate power level to transmit its packets so as to increase the possibility of channel reuse. The possibility of using discrete, instead of continuous, power levels is also discussed. Through analyses and simulations, we demonstrate the advantage of our new MAC protocol. This, together with the extra bene ts such as saving battery energy and reducing cochannel interference, does show a promising direction to enhance the performance of MANETs.
541|What is complexity|SFI Working Papers contain accounts of scientific work of the author(s) and do not necessarily represent the views of the Santa Fe Institute. We accept papers intended for publication in peer-reviewed journals or proceedings volumes, but not papers that have already appeared in print. Except for papers by our external faculty, papers must be based on work done at SFI, inspired by an invited visit to or collaboration at SFI, or funded by an SFI grant. ©NOTICE: This working paper is included by permission of the contributing author(s) as a means to ensure timely distribution of the scholarly and technical work on a non-commercial basis. Copyright and all rights therein are maintained by the author(s). It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author&#039;s copyright. These works may be reposted only with the explicit permission of the copyright holder. www.santafe.edu
542|DataSpace: Querying and Monitoring Deeply Networked Collections in Physical Space|In this article we introduce a new conception of three-dimensional DataSpace, which is physical space enhanced by connectivity to the network.
543|Near ground wideband channel measurement|Abstract- Frequency domain channel propagation measurements in the 800-1000 MHz band have been performed with ground-lying antennas. The range of path-loss exponent and shadowing variance for indoor and outdoor environment were determined. The range of these values roughly agree with those measured for higher elevation antennas. Frequency selectivity of the RF channel was also characterized by means of determining average coherence bandwidth (CBW). It was observed that there is a relationship between CBW and distance between transmitting and receiving antennas. I.
544|Power-Aware Communication for Mobile Computers|Recently, the mobile community has focused on techniques for reducing energy consumption  for mobile hosts. These power management techniques typically target communication  devices such as wireless network interfaces, aiming to reduce usage, and thus energy consumption,  of the particular device itself. We observe that optimization of a single device&#039;s energy  consumption, without considering the effect of the strategy on the rest of the machine, can  have negative consequences. We propose power management techniques addressing mobile host  communications that encompass all components of a mobile host in an effort to optimize total  energy consumption. Specifically, we propose runtime adaptation of communication parameters  in order to minimize the energy consumed during active data transfer. Information about the  network environment is used to drive such adaptations in an effort to compensate for the effect  of dynamic service from wireless communication device on the energy consume...
545|A versatile architecture for the distributed sensor integration problem|Abstract-The computational issues related to information in-tegration in multisensor systems and distributed sensor networks has become an active area of research. From a computational viewpoint, the efficient extraction of information from noisy and faulty signals emanating from many sensors requires the solution of problems related a) to the architecture and fault tolerance of the distributed sensor network, b) to the proper synchronization of sensor signals, and c) to the integration of information to keep the communication and the centralized processing require-ments small. In this paper, we propose a versatile architecture for a distributed sensor network which consists of a multilevel network with the nodes (processing elementlsensor pairs) at each level interconnected as a deBruijn network. We show that this multilevel network has reasonable fault tolerance, admits simple and decentralized routing, and offers easy extensibility. We model information from sensors as real valued intervals and derive an interesting property related to information integration in the presence of faults. Using this property, the search for a fault is narrowed down to two potentially faulty sensors or communication links. In a distributed environment, information has to be integrated from “temporally close ” signals in the presence of imperfect clocks in a distributed environment. We apply the results of past research in this area to state various relationships between the clocks of the processing elements in the network for proper information integration. Index Terms- Abstract estimate, clock synchronization, dis-tributed sensor networks, deBruijn networks, fault tolerance, information integration. I.
546|The Mobile Patient: Wireless Distributed Sensor Networks for Patient Monitoring and Care|In this paper, the concept of a 3 layer distributed sensor network for patient monitoring and care is introduced. The envisioned network has a leaf node layer (consisting of patient sensors), a intermediate node layer (consisting of the supervisory processor residing with each patient) and the root node processor (residing at a central monitoring facility). The introduced paradigm has the capability of dealing with the bandwidth bottleneck at the wireless patient - root node link and the processing bottleneck at the central processor or root node of the network.
547|Energy-efficient link layer for wireless microsensor network |Wireless microsensors are being used to form large, dense networks for the purposes of long-term environmental sensing and data collection. Unfortunately, these networks are typically deployed in remote environments where energy sources are limited. Thus, designing fault-tolerant wire-less microsensor networks with long system lifetimes can be challenging. By applying energy-efficient techniques at all levels of the system hierarchy, system lifetime can be ex-tended. In this paper, energy-efficient techniques that adapt underlying communication parameters will be presented in the context of wireless microsensor networks. In particular, the effect of adapting link and physical layer parameters, such as output transmit power and error control coding, on system energy consumption will be examined. 1.
548|Diagnosis of Sensor Networks|As sensor nodes are embedded into physical environments and becoming integral parts of our daily lives, sensor networks will become the important nerve systems that monitor and actuate our physical environments. We define the process of monitoring the status of a sensor network and figuring out the problematic sensor nodes sensor network diagnosis. However, the high sensor node-to-manager ratio makes it extremely difficult to pay special attention to any individual node. In addition, the response implosion problem, which occurs when a high volume of incoming replies triggered by diagnosis queries cause the central diagnosing node to become a bottleneck, is one major obstacle to be overcome. In this paper, we describe approaches to addressing the response implosion problem in sensor network diagnosis. We will also present simulation experiments on the performance of these approaches, and discuss presentation schemes for diagnostic results.
549|Low-power directsequence spread-spectrum modem architecture for distributed wireless sensor networks|Emerging CMOS and MEMS technologies enable the implementation of a large number of wireless distributed microsensors that can be easily and rapidly deployed to form highly redundant, self-configuring, and ad hoc sensor networks. To facilitate ease of deployment, these sensors should operate on battery for extended periods of time. A particular challenge in maintaining extended battery lifetime lies in achieving communications with low power. This paper presents a directsequence spread-spectrum modem architecture that provides robust communications for wireless sensor networks while dissipating very low power. The modem architecture has been verified in an FPGA implementation that dissipates only 33 mW for both transmission and reception. The implementation can be easily mapped to an ASIC technology with an estimated power performance of less than 1 mW.
550|A selforganizing approach to data forwarding in largescale sensor networks|Abstracf- The large number of networked sensors, frequent sensor failures and stringent energy constraints pose unique design challenges for data forwarding in wireless sensor networks. In this paper, we present a new approach to data forwarding in sensor networks that effectively addresses these design issues. Our approach organizes sensors into a dynamic, self-optimizing multicast tree-based forwarding hierarchy, which is data centric and robust to node failures. We demonstrate the effectiveness of our design through simulations. I.
551|All-Digital Impulse Radio For MUI/ISI-Resilient Multi-User Communications Over Frequency-Selective Multipath Channels|Impulse radio (IR) is an ultra-wideband system with attractive features for baseband asynchronous multiple access (MA), multimedia services, and tactical wireless communications. Implemented with analog components, the continuoustime IRMA model utilizes pulse-position modulation (PPM) and random time-hopping codes to alleviate multipath effects and suppress multiuser interference (MUI). We introduce a novel continuous-time Multiple Input Multiple Output (MIMO) PPMIRMA scheme, and derive its discrete-time equivalent model. Relying on a time-division-duplex access protocol and orthogonal user codes, we design composite linear and non-linear receivers for the downlink. The linear step eliminates MUI deterministically and accounts for frequency-selective multipath, while a Maximum Likelihood (ML) receiver performs symbol detection.  1. INTRODU7 ION  The idea of transmitting digital information using ultra-short impulses was first presented in [10] and called Impulse Radio.It  relies on PPM...
552|Learning to rank using gradient descent|We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine. 1.
553|An Efficient Boosting Algorithm for Combining Preferences|The problem of combining preferences arises in several applications, such as combining the results of different search engines. This work describes an efficient algorithm for combining multiple preferences. We first give a formal framework for the problem. We then describe and analyze a new boosting algorithm for combining preferences called RankBoost. We also describe an efficient implementation of the algorithm for certain natural cases. We discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different WWW search strategies, each of which is a query expansion for a given domain. For this task, we compare the performance of RankBoost to the individual search strategies. The second experiment is a collaborative-filtering task for making movie recommendations. Here, we present results comparing RankBoost to nearest-neighbor and regression algorithms.
554|IR evaluation methods for retrieving highly relevant documents|This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in moderu large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (In- Query ) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous rele- vance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods. 1. 
555|Classification by pairwise coupling|We discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the nature of the class probability estimates that arise, and examine the performance of the procedure in real and simulated datasets. Classifiers used include linear discriminants, nearest neighbors, and the support vector machine. 
556|Pranking with Ranking|We discuss the problem of ranking instances. In our framework each instance is associated with a rank or a rating, which is an integer from 1 to k. Our goal is to find a rank-prediction rule that assigns each instance a rank which is as close as possible to the instance&#039;s true rank. We describe a simple and efficient online algorithm, analyze its performance in the mistake bound model, and prove its correctness. We describe two sets of experiments, with synthetic data and with the EachMovie dataset for collaborative filtering. In the experiments we performed, our algorithm outperforms online algorithms for regression and classification applied to ranking.
557|Boosting Algorithms as Gradient Descent|Much recent attention, both experimental and theoretical, has been focussed on classification algorithms which produce voted combinations of classifiers. Recent theoretical work has shown that the impressive generalization performance of algorithms like AdaBoost can be attributed to the classifier having large margins on the training data. We present an abstract algorithm for finding linear combinations of functions that minimize arbitrary cost functionals (i.e functionals that do not necessarily depend on the margin). Many existing voting methods can be shown to be special cases of this abstract algorithm. Then, following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on
558|Log-Linear Models for Label Ranking|Label ranking is the task of inferring a total order over a predefined set of  labels for each given instance. We present a general framework for batch  learning of label ranking functions from supervised data. We assume that  each instance in the training data is associated with a list of preferences  over the label-set, however we do not assume that this list is either complete  or consistent. This enables us to accommodate a variety of ranking  problems. In contrast to the general form of the supervision, our goal is  to learn a ranking function that induces a total order over the entire set  of labels. Special cases of our setting are multilabel categorization and  hierarchical classification. We present a general boosting-based learning  algorithm for the label ranking problem and prove a lower bound on the  progress of each boosting iteration. The applicability of our approach is  demonstrated with a set of experiments on a large-scale text corpus.
559|T.: Using the future to sort out the present: Rankprop and multitask learning for medical risk evaluation|A patient visits the doctor; the doctor reviews the patient&#039;s history, asks questions, makes basic measurements (blood pressure,...), and prescribes tests or treatment. The prescribed course of action is based on an assessment of patient risk|patients at higher risk are given more and faster attention. It is also sequential|it is too expensive to immediately order all tests which might later be of value. This paper presents two methods that together improve the accuracy of backprop nets on a pneumonia risk assessment problem by 10-50%. Rankprop improves on backpropagation with sum of squares error in ranking patients by risk. Multitask learning takes advantage of future lab tests available in the training set, but not available in practice when predictions must be made. Both methods are broadly applicable. 1
560|Online ranking/collaborative filtering using the perceptron algorithm|In this paper we present a simple to implement truly online large margin version of the Perceptron ranking (PRank) algorithm, called the OAP-BPM (Online Aggregate Prank-Bayes Point Machine) algorithm, which finds a rule that correctly ranks a given training sequence of instance and target rank pairs. PRank maintains a weight vector and a set of thresholds to define a ranking rule that maps each instance to its respective rank. The OAP-BPM algorithm is an extension of this algorithm by approximating the Bayes point, thus giving a good generalization performance. The Bayes point is approximated by averaging the weights and thresholds associated with several PRank algorithms run in parallel. In order to ensure diversity amongst the solutions of the PRank algorithms we randomly subsample the stream of incoming training examples. We also introduce two new online versions of Bagging and the voted Perceptron using the same randomization trick as OAP-BPM, hence are referred to as OAP with extension-Bagg and-VP respectively. A rank learning experiment was conducted on a synthetic data set and collaborative filtering experiments on a number of real world data sets were conducted, showing that OAP-BPM has a better performance compared to PRank and a pure online regression algorithm, albeit with a higher computational cost, though is not too prohibitive. 1.
561|Internet time synchronization: The network time protocol|This memo describes the Network Time Protocol (NTP) designed to distribute time information in a large, diverse internet system operating at speeds from mundane to lightwave. It uses a returnabletime architecture in which a distributed subnet of time servers operating in a self-organizing, hierarchical, master-slave configuration synchronizes local clocks within the subnet and to national time standards via wire or radio. The servers can also redistribute time information within a network via local routing algorithms and time daemons. The architectures, algorithms and protocols which have evolved to NTP over several years of implementation and refinement are described in this paper. The synchronization subnet which has been in regular operation in the Internet for the last several years is described along with performance data which shows that timekeeping accuracy throughout most portions of the Internet can be ordinarily maintained to within a few tens of milliseconds, even in cases of failure or disruption of clocks, time servers or networks. This memo describes the Network Time Protocol, which is specified as an Internet Standard in
562|Time, Clocks, and the Ordering of Events in a Distributed System|The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.  
563|Data networks|a b s t r a c t In this paper we illustrate the core technologies at the basis of the European SPADnet project (www. spadnet.eu), and present the corresponding first results. SPADnet is aimed at a new generation of MRI-compatible, scalable large area image sensors, based on CMOS technology, that are networked to perform gamma-ray detection and coincidence to be used primarily in (Time-of-Flight) Positron Emission Tomography (PET). The project innovates in several areas of PET systems, from optical coupling to single-photon sensor architectures, from intelligent ring networks to reconstruction algorithms. In addition, SPADnet introduced the first computational model enabling study of the full chain from gamma photons to network coincidence detection through scintillation events, optical coupling, etc. &amp; 2013 Elsevier B.V. All rights reserved. 1.
564|Optimal Clock Synchronization|We present a simple, efficient, and unified solution to the problems of synchronizing, initializing, and integrating clocks for systems with different types of failures: crash, omission, and arbitrary failures with and without message authentication. This is the ft known solution that achieves optimal accuracy -- the accuracy of synchronized clocks (with respect to real time) is as good as that specified for the underlying hardware clocks. The solution is also optimal with respect to the number of faulty processes that can be tolerated to achieve this accuracy.
565|Dynamic Fault-Tolerant Clock Synchronization|This paper gives two simple efficient distributed algorithms: one for keeping clocks in a network synchronized and one for allowing new processors to join the network with their clocks synchronized. Assuming a fault tolerant authentication protocol, the algorithms tolerate both link and processor failures of any type. The algorithm for maintaining synchronization works for arbitrary networks (rather than just completely connected networks) and tolerates any number of processor or communication link faults as long as the correct processors remain connected by fault-free paths. It thus represents an improvement over other clock synchronization algorithms such as [LM,WL], although, unlike them, it does require an authentication protocol to handle Byzantine faults. Our algorithm for allowing new processors to join requires that more than half the processors be correct, a requirement that is provably necessary. 1 Introduction  In a distributed system it is often necessary for processors to ...
566|A New Fault-Tolerant Algorithm for Clock Synchronization|We describe a new fault-tolerant algorithm for solving a variant of Lamport&#039;s clock synchronization problem. The algorithm is designed for a system of distributed processes that communicate by sending messages. Each process has its own read-only physical clock whose drift rate from real time is very small. By adding a value to its physical clock time, the process obtains its local time. The algorithm solves the problem of .maintaining closely synchronized local times, assuming that processes&#039; local times are clo. sely synchronized initially. The algorithm is able to tolerate the failure of just under a third of the participating processes. It maintains synchronization to within a small constant, whose magnitude depends upon the rate of clock drift, the message delivery time, and the initial closeness of synchronization. We also give a characterization of how far the clocks drift from real time. Reintegration of a repaired process can be accomplished using a slight modification of the &#039;basic algorithm. A similar style algorithm can also be used to achieve synchronization initially.
567|A Paradigm for Reliable Clock Synchronization|Existing fault-tolerant clock synchronization protocols are shown to result from refining a  single clock synchronization paradigm. In that paradigm, a reliable time source  periodically issues messages that cause processors to resynchronize their clocks. The  reliable time source is approximated by reading all clocks in the system and using a  convergence function to compute a fault-tolerant average of the values read. The  performance of a clock synchronization algorithm based on the paradigm can be quantified  in terms of the two parameters that characterize the behavior of the convergence function  used: accuracy and precision.
568|Measured performance of the Network Time Protocol in the Internet system. DARPA Network Working Group Report RFC-1128|This paper describes a series of experiments involving over 100,000 hosts of the Internet system and located in the U.S., Europe and the Pacific. The experiments are designed to evaluate the availability, accuracy and reliability of international standard time distribution using the DARPA/NSF Internet and the Network Time Protocol (NTP), which is specified as an Internet Standard in RFC-1119. NTP is designed specifically for use in a large, diverse internet system operating at speeds from mundane to lightwave. In NTP a distributed subnet of time servers operating in a self-organizing, hierarchical, master-slave configuration exchange precision timestamps in order to synchronize subnet clocks to each other and national time standards via wire or radio. The experiments are designed to locate Internet hosts and gateways that provide time by one of three time distribution protocols and evaluate the accuracy of their indications. For those hosts that support NTP, the experiments determine the distribution of errors and other statistics over paths spanning major portions of the globe. Finally, the experiments evaluate the accuracy and reliability of precision timekeeping using NTP and typical Internet paths involving DARPA, NSFNET and other agency networks. The experiments demonstrate that timekeeping accuracy throughout most
569|The National Bureau of Standards atomic time scale: Generation, stability, accuracy and accessibility|Absrruct-The independent atomic time scale at the National Bureau of Standards AT(NBS), is based upon an ensemble of continuously operating cesium clocks calibrated occasionally by an NBS primary frequency standard. The data of frequency calibrations and interclock comparisons are statistically processed to provide nearly optimum time stability and frequency accuracy. The long-term random fluctuation of AT(NBS) due to nondeterministic perturbations is estimated to be a few parts in and the present accuracy is inferred to be 1 part in A small coordinate rate is added to the rate of AT(NBS) to generate UTC(NBS): this small addition is for the purpose of maintaining syn-chronization within a few microseconds of other international timing centers. IJTQNBS) is readily operationally available over a large part of the world via WWV. WWVH, WWVB, and telephone; also via some passwe time transfer systems, e.g., Loran € and the TV line-10 system; and also experimentaliy via satellite and WWVL. The precision and ac-
570|The SPLASH-2 programs: Characterization and methodological considerations|The SPLASH-2 suite of parallel applications has recently been released to facilitate the study of centralized and distributed shared-address-space multiprocessors. In this context, this paper has two goals. One is to quantitatively characterize the SPLASH-2 programs in terms of fundamental properties and architectural interactions that are important to understand them well. The properties we study include the computational load balance, communication to computation ratio and traffic needs, important working set sizes, and issues related to spatial locality, as well as how these properties scale with problem size and the number of processors. The other, related goal is methodological: to assist people who will use the programs in architectural evaluations to prune the space of application and machine parameters in an informed and meaningful way. For example, by characterizing the working sets of the applications, we describe which operating points in terms of cache size and problem size are representative of realistic situations, which are not, and which re redundant. Using SPLASH-2 as an example, we hope to convey the importance of understanding the interplay of problem size, number of processors, and working sets in designing experiments and interpreting their results.
571|A rapid hierarchical radiosity algorithm|This paper presents a rapid hierarchical radiosity algorithm for illuminating scenes containing lar e polygonal patches. The afgorithm constructs a hierarchic“J representation of the form factor matrix by adaptively subdividing patches into su bpatches according to a user-supplied error bound. The algorithm guarantees that all form factors are calculated to the same precision, removing many common image artifacts due to inaccurate form factors. More importantly, the al o-rithm decomposes the form factor matrix into at most O? n) blocks (where n is the number of elements). Previous radiosity algorithms represented the element-to-element transport interactions with n2 form factors. Visibility algorithms are given that work well with this approach. Standard techniques for shooting and gathering can be used with the hierarchical representation to solve for equilibrium radiosities, but we also discuss using a brightness-weighted error criteria, in conjunction with multigrldding, to even more rapidly progressively refine the image.
572|  A Comparison of Sorting Algorithms for the Connection Machine CM-2 |We have implemented three parallel sorting algorithms on the Connection Machine Supercomputer model CM-2: Batcher&#039;s bitonic sort, a parallel radix sort, and a sample sort similar to Reif and Valiant&#039;s flashsort. We have also evaluated the implementation of many other sorting algorithms proposed in the literature. Our computational experiments show that the sample sort algorithm, which is a theoretically efficient &#034;randomized&#034; algorithm, is the fastest of the three algorithms on large data sets. On a 64K-processor CM-2, our sample sort implementation can sort 32 10 6 64-bit keys in 5.1 seconds, which is over 10 times faster than the CM-2 library sort. Our implementation of radix sort, although not as fast on large data sets, is deterministic, much simpler to code, stable, faster with small keys, and faster on small data sets (few elements per processor). Our implementation of bitonic sort, which is pipelined to use all the hypercube wires simultaneously, is the least efficient of the three on large data sets, but is the most efficient on small data sets, and is considerably more space efficient. This paper analyzes the three algorithms in detail and discusses many practical issues that led us to the particular implementations.  
573|A Low Overhead Coherence Solution for Multiprocessors with Private Cache Memories|This paper presents a cache coherence solu-tion for multiprocessors organized around a single time-shared bus. The solution aims at reducing bus traffic and hence bus wait time. This in turn increases the overall processor utilization. Unlike most traditional high-performance coherence solutions, this solution does not use any global tables. Furthermore, this coherence scheme is modular and easily extensible, requiring no modif-ication of cache modules to add more processors to a system. The performance of this scheme is evaluated by using an approximate analysis method. It is shown that the performance of this scheme is closely tied with the miss ratio and the amount of sharing between processors. I.
574|FFTs in external or hierarchical memory|Conventional algorithms for computing large one-dimensional fast Fourier transforms (FFTs), even those algorithms recently developed for vector and parallel computers, are largely unsuitable for systems with external or hierarchical memory. The principal reason for this is the fact that most FFT algorithms require at least m complete passes through the data set to compute a 2 m-point FFT. This paper describes some advanced techniques for computing an ordered FFT on a computer with external or hierarchical memory. These algorithms (1) require as few as two passes through the external data set, (2) employ strictly unit stride, long vector transfers between main memory and external storage, (3) require only a modest amount of scratch space in main memory, and (4) are well suited for vector and parallel computation. Performance gures are included for implementations of some of these algorithms on Cray supercomputers. Of interest is the fact that a main memory version outperforms the current Cray library FFT routines on the Cray-2, the Cray X-MP,andtheCrayY-MP systems. Using all eight processors on the Cray Y-MP, this main memory routine runs at nearly two giga ops.
575|False sharing and spatial locality in multiprocessor caches|Abstract- The performance of the data cache in shared-memory multiprocessors has been shown to be different from that in uniprocessors. In particular, cache miss rates in multiprocessors do not show the sharp drop typical of uniprocessors when the size of the cache block increases. The resulting high cache miss rate is a cause of concern, since it can significantly limit the performance of multiprocessors. Some researchers have speculated that this effect is due to false sharing, the coherence transactions that result when different processors update different words of the same cache block in an interleaved fashion. While the analysis of six applications in this paper confirms that false sharing has a significant impact on the miss rate, the measurements also show that poor spatial locality among accesses to shared data has an even larger impact. To mitigate false sharing and to enhance spatial locality, we optimize the layout of shared data in cache blocks in a programmer-transparent manner. We show that this approach can reduce the number of misses on shared data by about 10 % on average. Index Terms- Multiprocessing, shared-memory multiproces-sor, cache memory, sharing, false sharing, optimizing compiler, placement of data. s I.
576|Working Sets, Cache Sizes, and Node Granularity Issues for Large-Scale Multiprocessors|The distribution of resources among processors, memory and caches is a crucial question faced by designers of large-scale parallel machines. If a machine is to solve problems with a certain data set size, should it be built with a large number of processors each with a small amount of memory, or a smaller number of processors each with a large amount of memory? How much cache memory should be provided per processor for cost-effectiveness? And how do these decisions change as larger problems are run on larger machines?  In this paper, we explore the above questions based on the characteristics of five important classes of large-scale parallel scientific applications. We first show that all the applications have a hierarchy of well-defined per-processor working sets, whose size, performance impact and scaling characteristics can help determine how large diffkrent levels of a multiprocessor &#039;s cache hierarchy should be. Then, we use these working sets together with certain other imporant characteristics of the applications such as communication to computation ratios, concurrency, and load balancing behavioto reflect upon the broader question of the granularity of processing nodes in highperformance multiprocessors.
577|Volume Rendering on Scalable Shared-Memory MIMD Architectures|Volume rendering is a useful visualization technique for understanding the large amounts of data generated in a variety of scientific disciplines. Routine use of this technique is currently limited by its computational expense. We have designed a parallel volume rendering algorithm for MIMD architectures based on ray tracing and a novel task queue image partitioning technique. The combination of ray tracing and MIMD architectures allows us to employ algorithmic optimizations such as hierarchical opacity enumeration, early ray termination, and adaptive image sampling. The use of task queue image partitioning makes these optimizations efficient in a parallel framework. We have implemented our algorithm on the Stanford DASH Multiprocessor, a scalable shared-memory MIMD machine. Its single address-space and coherent caches provide programming ease and good performance for our algorithm. With only a few days of programming effort, we have obtained nearly linear speedups and near real-time frame update rates on a 48 processor machine. Since DASH is constructed from Silicon Graphics multiprocessors, our code runs on any Silicon Graphics workstation without modification.
578|The Detection And Elimination Of Useless Misses In Multiprocessors|In this paper we introduce a classification of misses in shared-memory multiprocessors based on inter processor communication. We identify the set of essential misses, i.e., the smallest set of misses necessary for correct execution. Essential misses include cold misses and true sharing misses. All other misses are useless misses and can be ignored without affecting program execution. Based on the new classification we evaluate miss reduction techniques in hardware, based on delaying and combining invalidations. We compare the effectiveness of five different protocols for combining invalidations leading to useless misses for cachebased multiprocessors and for multiprocessors with virtual shared memory. In cache based systems these techniques are very effective and lead to miss rates which are close to the minimum. In virtual shared memory systems, the techniques are also effective but leave room for additional improvements. Keywords: Shared memory multiprocessor, distributed shared me...
579|The Performance Advantages of Integrating Block Data Transfer in Cache-Coherent Multiprocessors|Integrating 1 support for block data transfer has become an im- portant emphasis in recent cache-coherent shared address space multiprocessors. This paper examines the potential performance benefits of adding this support. A set of ambitious hardware mechanisms is used to study performance gains in five important scientific computations that appear to be good candidates for using block transfer. Our conclusion is that the benefits of block transfer are not substantial for hardware cache- coherent multiprocessors. The main reasons for this are (i) the relatively modest fraction of time applications spend in communication amenable to block transfer, (ii) the difficulty of finding enough independent computation to overlap with the communication latency that remains after block transfer, and (iii) long cache lines often capture many of the benefits of block transfer in efficient cache-coherent machines. In the cases where block transfer improves performance, prefetching can often provide comparable, if not superior, performance benefits. We also examine the impact of varying important communication parameters and processor speed on the effectiveness of block transfer, and comment on useful features that a block transfer facility should support for real applications.
580|Limitation of cache prefetching on a bus-based multiprocessor|Compiler-directed cache prefetching has the poten-tial to hide much of the high memory latency seen by current and future high-performance processors. How-ever, prefet thing is not without costs, particularly on a multiprocessor. Prefetching can negatively affect bus utilization, overall cache miss rates, memory la-tencies and data sharing. We simulated the effects of a particular compiler-directed prefetching algorithm, running on a bus-based multiprocessor. We showed that, despite a high memory latency, this architecture is not very well-suited for prefetching. For several vari-ations on the architecture, speedups for five parallel programs were no greater than 39%, and degradations were as high as 7Y0, when prefet thing was added to the workload. We examined the sources of cache misses, in light of several different prefetching strategies, and pinpointed the causes of the performance changes. In-validation misses pose a particular problem for current compiler-directed prefetchers. We applied two tech-niques that reduced their impact: a special prefetching heuristic tailored to write-shared data, and restructur-ing shared data to reduce false sharing, thus allowing traditional prefetching algorithms to work well. 1
581|Effective Cache Prefetching on Bus-Based Multiprocessors|Compiler-directed cache prefetching has the potential to hide much of the high memory latency seen by current and future high-performance processors. However, prefetching is not without costs, particularly on a multiprocessor. Prefetching can negatively affect bus utilization, overall cache miss rates, memory latencies and data sharing. We simulate the effects of a compiler-directed prefetching algorithm, running on a range of bus-based multiprocessors. We show that, despite a high memory latency, this architecture does not necessarily support prefetching well, in some cases actually causing performance degradations. We pinpoint several problems with prefetching on a shared memory architecture (additional conflict misses, no reduction in the data sharing traffic and associated latencies, a multiprocessor&#039;s greater sensitivity to memory utilization and the sensitivity of the cache hit rate to prefetch distance) and measure their effect on performance. We then solve those problems throug...
582|Simultaneous Multithreading: Maximizing On-Chip Parallelism|This paper examines simultaneous multithreading, a technique permitting several independent threads to issue instructions to a superscalar’s multiple functional units in a single cycle. We present several models of simultaneous multithreading and compare them with alternative organizations: a wide superscalar, a fine-grain multithreaded processor, and single-chip, multiple-issue multiprocessing architectures. Our results show that both (single-threaded) superscalar and fine-grain multithreaded architectures are limited in their ability to utilize the resources of a wide-issue processor. Simultaneous multithreading has the potential to achieve 4 times the throughput of a superscalar, and double that of fine-grain multithreading. We evaluate several cache configurations made possible by this type of organization and evaluate tradeoffs between them. We also show that simultaneous multithreading is an attractive alternative to single-chip multiprocessors; simultaneous multithreaded processors with a variety of organizations outperform corresponding conventional multiprocessors with similar execution resources. While simultaneous multithreading has excellent potential to increase processor utilization, it can add substantial complexity to the design. We examine many of these complexities and evaluate alternative organizations in the design space. 
583|A study of branch prediction strategies |In high-performance computer systems. performance losses due to conditional branch instructrons can be minrmized by predicting a branch outcome and fetching, decoding, and/or issuing subsequent instructions before the actual outcome is known. This paper discusses branch prediction strategies wrth the goal of maxtmizing prediction accuracy. First. currently used techniques are discussed and analyzed using instruction tmce data. Then, new techniques are proposed and are shown to provide greater accuracy and more flexibility at low cost.
584|Limits of instruction-level parallelism|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There two other research laboratories located in Palo Alto, the Network Systems
585|APRIL: A Processor Architecture for Multiprocessing|Processors in large-scale multiprocessors must be able to tolerate large communication latencies and synchronization delays. This paper describes the architecture of a rapid-context-switching processor called APRIL with support for fine-grain threads and synchronization. APRIL achieves high single-thread performance and supports virtual dynamic threads. A commercial RISC-based implementation of APRIL and a run-time software system that can switch contexts in about 10 cycles is described. Measurements taken for several parallel applications on an APRIL simulator show that the overhead for supporting parallel tasks based on futures is reduced by a factor of twoover a corresponding implementation on the Encore Multimax. The scalability of a multiprocessor based on APRIL is explored using a performance model. We show that the SPARC-based implementation of APRIL can achieve close to 80# processor utilization with as few as three resident threads per processor in a large-scale cache-based machine with an average base network latency of 55 cycles.
586|Limits of Control Flow on Parallelism|This paper discusses three techniques useful in relaxing the constraints imposed by control flow on parallelism: control dependence analysis, executing multiple flows of control simultaneously, and speculative execution. We evaluate these techniques by using trace simulations to find the limits of parallelism for machines that employ different combinations of these techniques. We have three major results. First, local regions of code have limited parallelism, and control dependence analysis is useful in extracting global parallelism from different parts of a program. Second, a superscalar processor is fundamentally limited because it cannot execute independent regions of code concurrently. Higher performance can be obtained with machines, such as multiprocessors and dataflow machines, that can simultaneously follow multiple flows of control. Finally, without speculative execution to allow instructions to execute before their control dependences are resolved, only modest amounts of para...
587|Performance Tradeoffs In Multithreaded Processors|... utilization. By maintaining multiple process contexts in hardware and switching  among them in a few cycles, multithreaded processors can overlap computation with memory  accesses and reduce processor idle time. This paper presents an analytical performance  model for multithreaded processors that includes cache interference, network contention,  context-switching overhead, and data-sharing effects. The model is validated through our  own simulations and by comparison with previously published simulation results. Our results  indicate that processors can substantially benefit from multithreading, even in systems  with small caches. Large caches yield close to full processor utilization with as few as two to  four contexts, while small caches may require up to four times as many contexts. Increased  network contention due to multithreading has a major effect on performance. The available  network bandwidth and the context-switching overhead limits the best possible utilization.
588|The Multiscalar Architecture|The centerpiece of this thesis is a new processing paradigm for exploiting instruction level parallelism. This paradigm, called the multiscalar paradigm, splits the program into many smaller tasks, and exploits fine-grain parallelism by executing multiple, possibly (control and/or data) depen-dent tasks in parallel using multiple processing elements. Splitting the instruction stream at statically determined boundaries allows the compiler to pass substantial information about the tasks to the hardware. The processing paradigm can be viewed as extensions of the superscalar and multiprocess-ing paradigms, and shares a number of properties of the sequential processing model and the dataflow processing model. The multiscalar paradigm is easily realizable, and we describe an implementation of the multis-calar paradigm, called the multiscalar processor. The central idea here is to connect multiple sequen-tial processors, in a decoupled and decentralized manner, to achieve overall multiple issue. The mul-tiscalar processor supports speculative execution, allows arbitrary dynamic code motion (facilitated by an efficient hardware memory disambiguation mechanism), exploits communication localities, and does all of these with hardware that is fairly straightforward to build. Other desirable aspects of the
589|Comparative Evaluation of Latency Reducing and Tolerating Techniques|Techniques that can cope with the large latency of memory accesses are essential for achieving high processor utilization in large-scale shared-memory multiprocessors. In this paper, we consider four architectural techniques that address the latency problem: (i) hardware coherent caches, (ii) relaxed memory consistency, (iii) softwarecontrolled prefetching, and (iv) multiple-context support. While some studies of benefits of the individual techniques have been done, no study evaluates all of the techniques within a consistent framework. This paper attempts to remedy this by providing a comprehensive evaluation of the benefits of the four techniques, both individually and in combinations, using a consistent set of architectural assumptions. The results in this paper have been obtained using detailed simulations of a large-scale shared-memory multiprocessor. Our results show that caches and relaxed consistency uniformly improve performance. The improvements due to prefetching and multiple contexts are sizeable, but are much more applicationdependent. Combinations of the various techniques generally attain better performance than each one on its own. Overall, we show that using suitable combinations of the techniques, performance can be improved by 4 to 7 times.
590|Processor coupling: Integrating compile time and runtime scheduling for parallelism|high-performance floating-point ALUs will be available by 1995. This paper presents processor coupling,a mechanism for controlling multiple ALUs to exploit both instruction-level and inter-thread parallelism, by using compile time and runtime scheduling. The compiler statically schedules individual threads to discover available intra-thread instruction-level parallelism. The runtime scheduling mechanism interleaves threads, exploiting inter-thread parallelism to maintain high ALU utilization. ALUs are assigned to threads on a cycle by cycle basis, and several threads can be active concurrently. We provide simulation results demonstrating that, on four simple numerical benchmarks, processor coupling achieves better performance than purely statically scheduled or multi-processor machine organizations. We examine how performance is affected by restricted communication between ALUs and by long memory latencies. We also present an implementation and feasibility study
591|Interleaving: A Multithreading Technique Targeting Multiprocessors and Workstations|There is an increasing trend to use commodity microprocessors as the compute engines in large-scale multiprocessors. However, given that the majority of the microprocessors are sold in the workstation market, not in the multiprocessor market, it is only natural that architectural features that benefit only multiprocessors are less likely to be adopted in commodity microprocessors. In this paper, we explore multiple-context processors, an architectural technique proposed to hide the large memory latency in multiprocessors. We show that while current multiple-context designs work reasonably well for multiprocessors, they are ineffective in hiding the much shorter uniprocessor latencies using the limited parallelism found in workstation environments. We propose an alternative design that combines the best features of two existing approaches, and present simulation results that show it yields better performance for both multiprogrammed workloads on a workstation and parallel applications on a multiprocessor. By addressing the needs of the workstation environment, our proposal makes multiple contexts more attractive for commodity microprocessors.
592|Analysis of Multithreaded Architectures for Parallel Computing |Multithreading has been proposed as an architectural strategy for tolerating latency in multiprocessors and, through limited empirical studies, shown to offer promise. This paper develops an analytical model of multithreaded processor behavior based on a small set of architectural and program parameters. The model gives rise to a large Markov chain, which is solved to obtain a formula for processor efficiency in terms of the number of threads per processor, the remote reference rate, the latency, and the cost of switching between threads. It is shown that a multithreaded processor exhibits three operating regimes: linear (efficiency is proportional to the number of threads), transition, and saturation (efficiency depends only on the remote reference rate and switch cost). Formulae for regime boundaries are derived. The model is embellished to reflect cache degradation due to multithreading, using an analytical model of cache behavior, demonstrating that returns diminish as the number threads becomes large. Predictions from the embellished model correlate well with published empirical measurements. Prescriptive use of the model under various scenarios indicates that multithreading is effective, but the number of useful threads per processor is fairly small. 
593|Some Efficient Architecture Simulation Techniques|An efficient simulator for the Motorola 88000 at the ISA (Instruction Set Architecture) level is described. By translating instructions on the fly to a quick-to-execute form we achieve an average ratio of 20 simulator host instructions executed per simulated instruction. Lazy allocation of memory allows large memories to be modelled with low start-up time. We describe our experience using the simulator to develop workstation software. The simulator&#039;s speed and extensive I/O device modelling made it possible for us to interactively debug and test a UNIX  ®  kernel and diagnostic software well before the hardware was available. Extensions to closely model caches and multiprocessors are sketched. 1. Introduction  We present techniques for building a high speed architecture simulator for the Motorola 88000 CPU [1] and CMMU (Cache and Memory Management Unit) [2]. These methods can be used for simulation of other architectures, including CISCs. This work was done while the author was at Tekt...
594|The Effectiveness of Multiple Hardware Contexts|Multithreaded processors are used to tolerate long memory latencies. By executing threads loaded in multiple hardware contexts, an otherwise idle processor can keep busy, thus increasing its utilization. However, the larger size of a multi-thread working set can have a negative effect on cache conflict misses. In this paper we evaluate the two phenomena together, examining their combined effect on execution time. The usefulness of multiple hardware contexts depends on: program data locality, cache organization and degree of multiprocessing. Multiple hardware contexts are most effective on programs that have been optimized for data locality. For these programs, execution time dropped with increasing contexts, over widely varying architectures. With unoptimized applications, multiple contexts had limited value.The best performance was seen with only two contexts, and only on uniprocessors and small multiprocessors. The behavior of the unoptimized applications changed more noticeably with...
