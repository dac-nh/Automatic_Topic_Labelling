ID|Title|Summary
1|Physical Data Modeling for Multidimensional Access Methods |Introduction  Despite the fact that the database community has proposed a vast number of indexing methods over the years, no standard physical data model has been established like it has been achieved on the conceptual and logical level. How to optimize a given data model by using various indexing methods is still the &#039;trade secret` of the database administrators. Only recently, some approaches have been tried to make this knowledge available to the normal database user by easy to use optimization tools (e.g., AutoAdmin-Tool of MS SQL Server 7.0). In addition, physical data modeling has concentrated on one-dimensional access methods, since these were the only ones available in commercial database management systems. As multidimensional access methods (MDAMs) are making their way from the research labs into commercial products, a general physical data model should also take MDAMs into account, especially since MDAMs have a high potential to improve processing in important applica
2|AutoAdmin ’what-if’ index analysis utility|surajitca microsoftcorn viveknar @ microsoft.com As databases get widely deployed, it becomes increasingly important to reduce the overhead of database administration. An important aspect of data administration that critically influences performance is the ability to select indexes for a database. In order to decide the right indexes for a database, it is crucial for the database administrator (DBA) to be able to perform a quantitative analysis of the existing indexes. Furthermore, the DBA should have the ability to propose hypothetical (“what-if’) indexes and quantitatively analyze their impact on performance of the system. Such impact analysis may consist of analyzing workloads over the database, estimating changes in the cost of a workload, and studying index usage while taking into account projected changes in the sizes of the database tables. In this paper we describe a novel index analysis utility that we have prototyped for Microsoft SQL Server 7.0. We describe the interfaces exposed by this utility that can be leveraged by a variety of front-end tools and sketch important aspects of the user interfaces enabled by the utility. We also discuss the implementation techniques for efficiently supporting “what-if ’ indexes. Our framework can be extended to incorporate analysis of other aspects of physical database design. 1.
3|Improving OLAP Performance by Multidimensional Hierarchical Clustering|Data-warehousing applications cope with enormous data sets in the range of Gigabytes and Terabytes. Queries usually either select a very small set of this data or perform aggregations on a fairly large data set. Materialized views storing pre-computed aggregates are used to efficiently process queries with aggregations. This approach increases resource requirements in disk space and slows down updates because of the view maintenance problem. Multidimensional hierarchical clustering (MHC) of OLAP data overcomes these problems while offering more flexibility for aggregation paths. Clustering is introduced as a way to speed up aggregation queries without additional storage cost for materialization. Performance and storage cost of our access method are investigated and compared to current query processing scenarios. In addition performance measurements on real world data for a typical star schema are presented.  1. Introduction  Data processing in data warehousing (DW) applications uses dr...
4|Models and issues in data stream systems|In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues. 
5|Randomized Algorithms|Randomized algorithms, once viewed as a tool in computational number theory, have by now found widespread application. Growth has been fueled by the two major benefits of randomization: simplicity and speed. For many applications a randomized algorithm is the fastest algorithm available, or the simplest, or both. A randomized algorithm is an algorithm that uses random numbers to influence the choices it makes in the course of its computation. Thus its behavior (typically quantified as running time or quality of output) varies from
6|The space complexity of approximating the frequency moments|The frequency moments of a sequence containing mi elements of type i, for 1 = i = n, are the numbers Fk = ?n i=1 mki. We consider the space complexity of randomized algorithms that approximate the numbers Fk, when the elements of the sequence are given one by one and cannot be stored. Surprisingly, it turns out that the numbers F0, F1 and F2 can be approximated in logarithmic space, whereas the approximation of Fk for k = 6 requires n?(1) space. Applications to data bases are mentioned as well. 
7|Multiparty Communication Complexity|A given Boolean function has its input distributed among many parties. The aim is to determine which parties to tMk to and what information to exchange with each of them in order to evaluate the function while minimizing the total communication. This paper shows that it is possible to obtain the Boolean answer deterministically with only a polynomial increase in communication with respect to the information lower bound given by the nondeterministic communication complexity of the function.
8|NiagaraCQ: A Scalable Continuous Query System for Internet Databases|Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system’s performance and scalability. 1.
9|Fast subsequence matching in time-series databases|We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an ecient and eective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.  
10|Eddies: Continuously Adaptive Query Processing|In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.  In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry  during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates prom...
11|Mining high-speed data streams|Categories and Subject ?????????? ? ?¨?????????????????????????¦???¦??????????¡¤?? ¡  ? ¡????????????????¦¡¤????§?£????
12|Online Aggregation|Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in postgres. 1 Introduction  Aggregation is an incre...
13|Efficient Filtering of XML Documents for Selective Dissemination of Information|Information Dissemination applications are gaining increasing  popularity due to dramatic improvements in  communications bandwidth and ubiquity. The sheer  volume of data available necessitates the use of selective  approaches to dissemination in order to avoid overwhelming  users with unnecessaryinformation. Existing  mechanisms for selective dissemination typically rely  on simple keyword matching or &#034;bag of words&#034; information  retrieval techniques. The advent of XML as a  standard for information exchangeand the development  of query languages for XML data enables the development  of more sophisticated filtering mechanisms that  take structure information into account. We have developed  several index organizations and search algorithms  for performing efficient filtering of XML documents for  large-scale information dissemination systems. In this  paper we describe these techniques and examine their  performance across a range of document, workload, and  scale scenarios.  1 
14|External Memory Algorithms and Data Structures|Data sets in large applications are often too massive to fit completely inside the computer&#039;s internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this paper, we survey the state of the art in the design and analysis of external memory algorithms and data structures (which are sometimes referred to as &#034;EM&#034; or &#034;I/O&#034; or &#034;out-of-core&#034; algorithms and data structures). EM algorithms and data structures are often designed and analyzed using the parallel disk model (PDM). The three machine-independent measures of performance in PDM are the number of I/O operations, the CPU time, and the amount of disk space. PDM allows for multiple disks (or disk arrays) and parallel CPUs, and it can be generalized to handle tertiary storage and hierarchical memory. We discuss several important paradigms for how to solve batched and online problems efficiently in external memory. Programming tools and environments are available for simplifying the programming task. The TPIE system (Transparent Parallel I/O programming Environment) is both easy to use and efficient in terms of execution speed. We report on some experiments using TPIE in the domain of spatial databases. The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.  
15|Random sampling with a reservoir|We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O(n(1 + log(N/n))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.
16|Stable Distributions, Pseudorandom Generators, Embeddings and Data Stream Computation|In this paper we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular: ffl we show how to maintain (using only O(log n=ffl  2  ) words of storage) a sketch C(p) of a point p 2 l  n  1 under dynamic updates of its coordinates, such that given sketches C(p) and C(q) one can estimate jp \Gamma qj 1 up to a factor of (1 + ffl) with large probability. This solves the main open problem of [10].  ffl we obtain another sketch function C  0  which maps l  n  1 into a normed space l  m  1 (as opposed to C), such that m = m(n) is much smaller than n; to our knowledge this is the first dimensionality reduction lemma for l 1 norm  ffl we give an explicit embedding of l  n  2 into l  n  O(log n) 1  with distortion (1 + 1=n  \Theta(1)  ) and a nonconstructive embedding of l  n  2 into l  O(n)  1 with distortion  (1 + ffl) such that the embedding can be represented using only O(n log  2  n) bits (as opposed to at least...
17|Mining time-changing data streams|Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a station-ary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying pro-cesses generating them changed during this time, sometimes radically. Although a number of algorithms have been pro-posed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes ques-tionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.
18|Continuous Queries over Data Streams|In many recent applications, data may take the form of continuous data streams, rather than finite stored data sets. Several aspects of data management need to be re-considered in the presence of data streams, offering a new research direction for the database community. In this pa-per we focus primarily on the problem of query processing, specifically on how to define and evaluate continuous queries over data streams. We address semantic issues as well as efficiency concerns. Our main contributions are threefold. First, we specify a general and flexible architecture for query processing in the presence of data streams. Second, we use our basic architecture as a tool to clarify alternative semantics and processing techniques for continuous queries. The architecture also captures most previous work on continuous queries and data streams, as
20|Fjording the Stream: An Architecture for Queries over Streaming Sensor Data|If industry visionaries are correct, our lives will soon be full of sensors, connected together in loose conglomerations via wireless networks, each monitoring and collecting data about the environment at large. These sensors behave very differently from traditional database sources: they have intermittent connectivity, are limited by severe power constraints, and typically sample periodically and push immediately, keeping no record of historical information. These limitations make traditional database systems inappropriate for queries over sensors. We present the Fjords architecture for managing multiple queries over many sensors, and show how it can be used to limit sensor resource demands while maintaining high query throughput. We evaluate our architecture using traces from a network of traffic sensors deployed on Interstate 80 near Berkeley and present performance results that show how query throughput, communication costs, and power consumption are necessarily coupled in sensor environments.
21|Maintaining Stream Statistics over Sliding Windows (Extended Abstract)  (2002) |Mayur Datar  Aristides Gionis  y  Piotr Indyk  z  Rajeev Motwani  x  Abstract  We consider the problem of maintaining aggregates and statistics over data streams, with respect to the last N data elements seen so far. We refer to this model as the sliding window model. We consider the following basic problem: Given a stream of bits, maintain a count of the number of 1&#039;s in the last N elements seen from the stream. We show that using O(  1  ffl log  2  N) bits of memory, we can estimate the number of 1&#039;s to within a factor of 1 + ffl. We also give a matching lower bound of \Omega\Gamma  1  ffl log  2  N) memory bits for any deterministic or randomized algorithms. We extend our scheme to maintain the sum of the last N positive integers. We provide matching upper and lower bounds for this more general problem as well. We apply our techniques to obtain efficient algorithms for the Lp norms (for p 2 [1; 2]) of vectors under the sliding window model. Using the algorithm for the basic counting problem, one can adapt many other techniques to work for the sliding window model, with a multiplicative overhead of O(  1  ffl log N) in memory and a 1 + ffl factor loss in accuracy. These include maintaining approximate histograms, hash tables, and statistics or aggregates such as sum and averages.
22|Continuously Adaptive Continuous Queries over Streams|We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive crossquery sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators -- both selections and join state -- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.
23|Multiple-Query Optimization|Some recently proposed extensions to relational database systems, as well as to deductive database systems, require support for multiple-query processing. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions may expand to more than one actual query that has to be run over the database. It is an interesting problem then to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of the paper is a systematic look at the problem, along with the presentation and analysis of algorithms that can be used for multiple-query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably.
24|Trajectory Sampling for Direct Traffic Observation|Traffic measurement is a critical component for the control and engineering of communication networks. We argue that traffic measurement should make it possible to obtain the spatial flow of traffic through the domain, i.e., the paths followed by packets between any ingress and egress point of the domain. Most resource allocation and capacity planning tasks can benefit from such information. Also, traffic measurements should be obtained without a routing model and without knowledge of network state. This allows the traffic measurement process to be resilient to network failures and state uncertainty.  We propose a method that allows the direct inference of traffic flows through a domain by observing the trajectories of a subset of all packets traversing the network. The key advantages of the method are that (i) it does not rely on routing state, (ii) its implementation cost is small, and (iii) the measurement reporting traffic is modest and can be controlled precisely. The key idea of the method is to sample packets based on a hash function computed over the packet content. Using the same hash function will yield the same sample set of packets in the entire domain, and enables us to reconstruct packet trajectories.  I. 
25|  Wavelet-Based Histograms for Selectivity Estimation |Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations. In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data values give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using
26|An Adaptive Query Execution System for Data Integration|Query processing in data integration occurs over networkbound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the  Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila  to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that the Tukwila architecture extends previous innovations in adaptive execution (such as...
27|Surfing wavelets on streams: One-pass summaries for approximate aggregate queries|Abstract We present techniques for computing small spacerepresentations of massive data streams. These are inspired by traditional wavelet-based approx-imations that consist of specific linear projections of the underlying data. We present general&amp;quot;sketch &amp;quot; based methods for capturing various linear projections of the data and use them to pro-vide pointwise and rangesum estimation of data streams. These methods use small amounts ofspace and per-item time while streaming through the data, and provide accurate representation asour experiments with real data streams show.
28|Approximate Query Processing Using Wavelets|Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today’s decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing
29|Space-Efficient Online Computation of Quantile Summaries|An &amp;epsilon;-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of &amp;epsilon;N . We present a new online...
30| Approximate Computation of Multidimensional Aggregates of Sparse Data Using Wavelets |Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries. In this paper, we present anovel method that provides approximate answers to high-dimensional OLAP aggregation queries in massive sparse data sets in a time-efficient and space-efficient manner. We construct a compact data cube, which is an approximate and space-efficient representation of the underlying multidimensional array, based upon a multiresolution wavelet decomposition. In the on-line phase, each aggregation query can generally be answered using the compact data cube in one I/O or a small number of I/Os, depending upon the desired accuracy. We present two I/O-efficient algorithms to construct the compact data cube for the important case of sparse high-dimensional arrays, which often arise in practice. The traditional histogram methods are infeasible for the massive high-dimensional data sets in OLAP applications. Previously developed wavelet techniques are efficient only for dense data. Our on-line query processing algorithm is very fast and capable of refining answers as the user demands more accuracy. Experiments on real data show that our method provides significantly more accurate results for typical OLAP aggregation queries than other efficient approximation techniques such as random sampling.
31|Computing on Data Streams|In this paper we study the space requirement of algorithms that make  only one (or a small number of) pass(es) over the input data. We study such  algorithms under a model of data streams that we introduce here. We give  a number of upper and lower bounds for problems stemming from queryprocessing,  invoking in the process tools from the area of communication  complexity.
32|Processing Complex Aggregate Queries over Data Streams|Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (i.e., data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed.
33|Continual Queries for Internet Scale Event-Driven Information Delivery|In this paper we introduce the concept of continual queries, describe the design of a distributed  event-driven continual query system -- OpenCQ, and outline the initial implementation of OpenCQ  on top of the distributed interoperable information mediation system DIOM [21, 19]. Continual  queries are standing queries that monitor update of interest and return results whenever the update  reaches specified thresholds. In OpenCQ, users may specify to the system the information they would  like to monitor (such as the events or the update thresholds they are interested in). Whenever the  information of interest becomes available, the system immediately delivers it to the relevant users;  otherwise, the system continually monitors the arrival of the desired information and pushes it to the  relevant users as it meets the specified update thresholds. In contrast to conventional pull-based data  management systems such as DBMSs and Web search engines, OpenCQ exhibits two important featu...
34|Join synopses for approximate query answering|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses (join samples) as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known. We also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations. One of our key contributions is a detailed analysis of the error bounds obtained for approximate answers that demonstrates the trade-offs in various methods, as well as the advantages in certain scenarios of a new subsampling method we propose. Our extensive set of experiments on the TPC-D benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper. 1
35|An efficient cost-driven index selection tool for Microsoft SQL Server|In this paper we describe novel techniques that make it possible to build an industrial-strength tool for automating the choice of indexes in the physical design of a SQL database. The tool takes as input a workload of SQL queries, and suggests a set of suitable indexes. We ensure that the indexes chosen are effective in reducing the cost of the workload by keeping the index selection tool and the query optimizer &amp;quot;in step&amp;quot;. The number of index sets that must be evaluated to find the optimal configuration is very large. We reduce the complexity of this problem using three techniques. First, we remove a large number of spurious indexes from consideration by taking into account both query syntax and cost information. Second, we introduce optimizations that make it possible to cheaply evaluate the “goodness ” of an index set. Third, we describe an iterative approach to handle the complexity arising from multicolumn indexes. The tool has been implemented on Microsoft SQL Server 7.0. We performed extensive experiments over a range of workloads, including TPC-D. The results indicate that the tool is efficient and its choices are close to optimal. 1.
36|Reductions in Streaming Algorithms, with an Application to Counting Triangles in Graphs |We introduce reductions in the streaming model as a tool in the design of streaming algorithms. We develop
37|Computing iceberg queries efficiently|Many applications compute aggregate functions...
38|Data-Streams and Histograms|Histograms have been used widely to capture data distribution, to represent the data by a small number of step functions. Dynamic programming algorithms which provide optimal construction of these histograms exist, albeit running in quadratic time and linear space. In this paper we provide linear time construction of 1 + epsilon approximation of optimal histograms, running in polylogarithmic space. Our results extend to the context of data-streams, and in fact generalize to give 1 + epsilon approximation of several problems in data-streams which require partitioning the index set into intervals. The only assumptions required are that the cost of an interval is monotonic under inclusion (larger interval has larger cost) and that the cost can be computed or approximated in small space. This exhibits a nice class of problems for which we can have near optimal data-stream algorithms.
39|XJoin: A Reactively-Scheduled Pipelined Join Operator|Wide-area distribution raises significant performance problems for traditional query processing techniques as data access becomes less predictable due to link congestion, load imbalances, and temporary outages. Pipelined query execution is a promising approach to coping with unpredictability in such environments as it allows scheduling to adjust to the arrival properties of the data. We have developed a non-blocking join operator, called XJoin, which has a small memory footprint, allowing many such operators to be active in parallel. XJoin is optimized to produce initial results quickly and can hide intermittent delays in data arrival by reactively scheduling background processing. We show that XJoin is an effective solution for providing fast query responses to users even in the presence of slow and bursty remote sources.  
40|Rate-Based Query Optimization for Streaming Information Sources|Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences. We present a preliminary validation of our rate-based optimization framework on a prototype XML query engine, though it is generic enough to be used in other database contexts. The results show that rate-based optimization is feasible and can indeed yield correct decisions.
41|Making Views Self-Maintainable for Data Warehousing|A data warehouse stores materialized views over data from one or more sources in order to provide fast access to the integrated data, regardless of the availability of the data sources. Warehouse views need to be maintained inresponse to changes to the base data in the sources. Except for very simple views, maintaining a warehouse view requires access to data that is not available in the view itself. Hence, to maintain the view, one either has to query the data sources or store auxiliary data in the warehouse. We show that by using key and referential integrity constraints, we often can maintain a select-project-join view without going to the data sources or replicating the base relations in their entirety in the warehouse. We derive a set of auxiliary views such that the warehouse view and the auxiliary views together are self-maintainable|they can be maintained without going to the data sources or replicating all base data. In addition, our technique can be applied to simplify traditional materialized view maintenance by exploiting key and referential integrity constraints. 1
42|Approximate Medians and other Quantiles in One Pass and with Limited Memory|We present new algorithms for computing approximate quantiles of large datasets in a single pass. The approximation guarantees are explicit, and apply without regard to the value distribution or the arrival distributions of the dataset. The main memory requirements are smaller than those reported earlier by an order of magnitude.  We also discuss methods that couple the approximation algorithms with random sampling to further reduce memory requirements. With sampling, the approximation guarantees are explicit but probabilistic, i.e., they apply with respect to a (user controlled) confidence parameter.  We present the algorithms, their theoretical analysis and simulation results.  1 Introduction  This article studies the problem of computing order statistics  of large sequences of online or disk-resident data using as little main memory as possible. We focus on computing  quantiles, which are elements at specific positions in the sorted order of the input. The OE-quantile, for OE 2 [0; ...
43|Random Sampling for Histogram Construction: How much is enough?|Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining &#034;How much sampling is enough?&#034; We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a  conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose ...
44|Sampling From a Moving Window Over Streaming Data|We introduce the problem of sampling from a moving window of recent items from a data stream and develop the &#034;chain-sample&#034; and &#034;priority-sample&#034; algorithms for this problem.
45|Tracking join and self-join sizes in limited storage|This paper presents algorithms for tracking (approximate) join and self-join sizes in limited storage, in the presence of insertions and deletions to the data set(s). Such algorithms detect changes in join and self-join sizes without an expensive recomputation from the base data, and without the large space overhead required to maintain such sizes exactly. Query optimizers rely on fast, high-quality estimates of join sizes in order to select between various join plans, and estimates of self-join sizes are used to indicate the degree of skew in the data. For self-joins, we considertwo approaches proposed in [Alon, Matias, and Szegedy. The Space Complexity of Approximating the Frequency Moments. JCSS, vol. 58, 1999, p.137-147], which we denote tug-of-war and sample-count. Wepresent fast algorithms for implementing these approaches, and extensions to handle deletions as well as insertions. We also report on the rst experimental study of the two approaches, on a range of synthetic and real-world data sets. Our study shows that tug-of-war provides more accurate estimates for a given storage limit than sample-count, which in turn is far more accurate than a standard sampling-based approach. For example, tug-of-war needed only 4{256 memory words, depending on the data set, in order to estimate the self-join size
46|Data Cube Approximation and Histograms via Wavelets (Extended Abstract)  (1998) |) Jeffrey Scott Vitter   Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  jsv@cs.duke.edu Min Wang  y Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  minw@cs.duke.edu Bala Iyer  Database Technology Institute IBM Santa Teresa Laboratory P.O. Box 49023 San Jose, CA 95161 USA  balaiyer@vnet.ibm.com Abstract  There has recently been an explosion of interest in the analysis of data in data warehouses in the field of On-Line Analytical Processing (OLAP). Data warehouses can be extremely large, yet obtaining quick answers to queries is important. In many situations, obtaining the exact answer to an OLAP query is prohibitively expensive in terms of time and/or storage space. It can be advantageous to have fast, approximate answers to queries. In this paper, we present an I/O-efficient technique based upon a multiresolution wavelet decomposition that yields an approximate a...
47|The design and implementation of a sequence database system|This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. There are three distinct contributions made in this paper. (1) We describe the specification of sequence queries using the SEQUIN query language. (2) We quantitatively demonstrate the importance of various storage and optimization techniques by studying their effect on performance. (3) We present a novel nested design paradigm used in PREDATOR to combine sequence ‘and relational data.
48|Random sampling techniques for space efficient online computation of order statistics of large datasets|In a recent paper [MRL98], we had described a general framework for single pass approximate quantile nding algorithms. This framework included several known algorithms as special cases. We had identi ed a new algorithm, within the framework, which had a signi cantly smaller requirement for main memory than other known algorithms. In this paper, we address two issues left open in our earlier paper. First, all known and space e cient algorithms for approximate quantile nding require advance knowledge of the length of the input sequence. Many important database applications employing quantiles cannot provide this information. In this paper, we present anovel non-uniform random sampling scheme and an extension of our framework. Together, they form the basis of a new algorithm which computes approximate quantiles without knowing the input sequence length. Second, if the desired quantile is an extreme value (e.g., within the top 1 % of the elements), the space requirements of currently known algorithms are overly pessimistic. We provide a simple algorithm which estimates extreme values using less space than required by the earlier more general technique for computing all quantiles. Our principal observation here is that random sampling is quanti ably better when estimating extreme values than is the case with the median.  
49|Characterizing Memory Requirements for Queries over Continuous Data|This paper deals with continuous conjunctive queries with arithmetic comparisons and optional aggregation over multiple data streams. An algorithm is presented for determining whether or not any given query can be evaluated using a bounded amount of memory for all possible instances of the data streams. For queries that can be evaluated using bounded memory, an execution strategy based on constant-sized synopses of the data streams is proposed. For queries that cannot be evaluated using bounded memory, data stream scenarios are identified in which evaluating the queries requires memory linear in the size of the unbounded streams
50|Congressional samples for approximate answering of group-by queries|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex decision support queries using precomputed summary statistics, such as samples. Decision support queries routinely segment the data into groups and then aggregate the information in each group (group-by queries). Depending on the data, there can be a wide disparity between the number of data items in each group. As a result, approximate answers based on uniform random samples of the data can result in poor accuracy for groups with very few data items, since such groups will be represented in the sample by very few (often zero) tuples. In this paper, we propose a general class of techniques for obtaining fast, highly-accurate answers for group-by queries. These techniques rely on precomputed non-uniform (biased) samples of the data. In particular, we proposecongressional samples, ahybrid union of uniform and biased samples. Given a xed amount of space, congressional samples seek to maximize the accuracy for all possible group-by queries on a set of columns. We present a one pass algorithm for constructing a congressional sample and use this technique to also incrementally maintain the sample up-to-date without accessing the base relation. We also evaluate query rewriting strategies for providing approximate answers from congressional samples. Finally, we conduct an extensive set of experiments on the TPC-D database, which demonstrates the e cacy of the techniques proposed. 1
51|Histogram-Based Approximation of Set-Valued Query Answers|Answering queries approximately has recently  been proposed as a way to reduce query response  times in on-line decision support systems,  when the precise answer is not necessary  or early feedback is helpful. Most of the  work in this area uses sampling-based techniques  and handles aggregate queries, ignoring  queries that return relations as answers. In  this paper, we extend the scope of approximate  query answering to general queries. We propose  a novel and intuitive error measure for  quantifying the error in an approximate query  answer, which can be a multiset in general.
52|Data Integration using Self-Maintainable Views|.  In this paper we de#ne the concept of self-maintainable views  # these are views that can be maintained using only the contents of  the view and the database modi#cations, without accessing any of the  underlying databases. We derive tight conditions under which several  types of select-project-join are self-maintainable upon insertions, deletions  and updates. Self-Maintainability is a desirable property for e#-  ciently maintaining large views in applications where fast response and  high availability are important. One example of suchanenvironment  is data warehousing wherein views are used for integrating data from  multiple databases.  1 Introduction  Most large organizations have related data in distinct databases. Many of these databases may be legacy systems, or systems separated for organizational reasons like funding and ownership. Integrating data from such distinct databases is a pressing business need. A common approach for integration is to de#ne an integrated view and...
53|Monitoring XML Data on the Web|We consider the monitoring of a flow of incoming documents. More precisely, we present here the monitoring used in a very large warehouse built from XML documents found on the web. The flow of documents consists in XML pages (that are warehoused) and HTML pages (that are not). Our contributions are the following:  ffl a subscription language which specifies the monitoring of pages when fetched, the periodical evaluation of continuous queries and the production of XML reports. ffl the description of the architecture of the system we implemented that makes it possible to monitor a flow of millions of pages per day with millions of subscriptions on a single PC, and scales up by using more machines. ffl a new algorithm for processing alerts that can be used in a wider context. We support
55|Expiring data in a warehouse|Data warehouses collect data into materi-alized views for analysis. After some time, some of the data may no longer be needed or may not be of interest. In this pa-per, we handle this by expiring or remov-ing unneeded materialized view tuples. A framework supporting such expiration is presented. Within it, a user or adminis-trator can declaratively request expirations and can specify what type of modifications are expected from external sources. The lat-ter can significantly increase the amount of data that can be expired. We present effi-cient algorithms for determining what data can be expired (data not needed for main-tenance of other views), taking into account the types of updates that may occur. 1
57|Sequence Query Processing|Many applications require the ability to manipulate sequences of data. We motivate the importance of sequence query processing, and present a framework for the optimization of sequence queries based on several novel techniques. These include query transformations, optimizations that utilize meta--data, and caching of intermediate results. We present a bottom-up algorithm that generates an efficient query evaluation plan based on cost estimates. This work also identifies a number of directions in which future research can be directed.  1 Introduction  Many real life applications manipulate data that is inherently sequential. Such data is logically viewed and queried in terms of a sequence abstraction and is often physically stored as a sequence. Databases should (a) allow sequences to be queried in a declarative manner, utilizing the ordered semantics of the data, and (b) take advantage of the opportunities available for query optimization. Relational databases are inadequate in this re...
58|SEQ: A Model for Sequence Databases|This paper presents the model which is the basis for a system to manage various kinds of sequence data. The model separates the data from the ordering information, and includes operators based on two distinct abstractions of a sequence. The main contributions of the model are: (a) it can deal with different types of sequence data, (b) it supports an expressive range of sequence queries, (c) it draws from many of the diverse existing approaches to modeling sequence data. 1
59|Sampling Algorithms: Lower Bounds and Applications (Extended Abstract)  (2001) |]  Ziv Bar-Yossef  y  Computer Science Division  U. C. Berkeley  Berkeley, CA 94720  zivi@cs.berkeley.edu  Ravi Kumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  ravi@almaden.ibm.com  D. Sivakumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  siva@almaden.ibm.com  ABSTRACT  We develop a framework to study probabilistic sampling algorithms that approximate general functions of the form f : A  n  ! B, where A and B are arbitrary sets. Our goal is to obtain lower bounds on the query complexity of functions, namely the number of input variables x i that any sampling algorithm needs to query to approximate f(x1 ; : : : ; xn ).  We define two quantitative properties of functions --- the block sensitivity and the minimum Hellinger distance --- that give us techniques to prove lower bounds on the query complexity. These techniques are quite general, easy to use, yet powerful enough to yield tight results. Our applications include the mean and higher statistical moments, the median and other selection functions, and the frequency moments, where we obtain lower bounds that are close to the corresponding upper bounds.  We also point out some connections between sampling and streaming algorithms and lossy compression schemes.  1. 
60|Approximating a Data Stream for Querying and Estimation: Algorithms and Performance Evaluation|Obtaining fast and good quality approximations to data distributions is a problem of central interest to database management. A variety of popular database applications including, approximate querying, similarity searching and data mining in most application domains, rely on such good quality approximations. Histogram based approximation is a very popular method in database theory and practice to succinctly represent a data distribution in a space efficient manner. In this paper, we place the problem of histogram construction into perspective and we generalize it by raising the requirement of a finite data set and/or known data set size. We consider the case of an infinite data set on which data arrive continuously forming an infinite data stream. In this context, we present the first single pass algorithms capable of constructing histograms of provable good quality. We present algorithms for the fixed window variant of the basic histogram construction problem, supporting incremental maintenance of the histograms. The proposed algorithms trade accuracy for speed and allow for a graceful tradeoff between the two, based on application requirements. In the case of approximate queries on infinite data streams, we present a detailed experimental evaluation comparing our algorithms with other applicable techniques using real data sets, demonstrating the superiority of our proposal. 1
61|A robust, optimization-based approach for approximate answering of aggregate queries|The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are “similar ” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work. 1
62|Online Dynamic Reordering for Interactive Data Processing|Abstract We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing.
64|On Sampling and Relational Operators|A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. We highlight the primary difficulties, summarize the results of some recent work in this area, and indicate directions for future work.   
65|Atmospheric Modeling, Data Assimilation and Predictability|Numerical weather prediction (NWP) now provides major guidance in our daily weather forecast. The accuracy of NWP models has improved steadily since the first successful experiment made by Charney, Fj!rtoft and von Neuman (1950). During the past 50 years, a large number of technical papers and reports have been devoted to NWP, but the number of textbooks dealing with the subject has been very small, the latest being the 1980 book by Haltiner &amp; Williams, which was dedicated to descriptions of the atmospheric dynamics and numerical methods for atmospheric modeling. However, in the intervening years much impressive progress has been made in all aspects of NWP, including the success in model initialization and ensemble forecasts. Eugenia Kalnay’s recent book covers for the first time in the long history of NWP, not only methods for numerical modeling, but also the important related areas of data assimilation and predictability. It incorporates all aspects of environmental computer modeling including an historical overview of NWP, equations of motion and their approximations, a modern description of the methods to determine the initial conditions using weather observations and a clear discussion of chaos in dynamic systems and how these concepts can be
66|Simulating Physics with Computers|A digital computer is generally believed to be an efficient universal computing device; that is, it is believed able to simulate any physical computing device with an increase in computation time of at most a polynomial factor. This may not be true when quantum mechanics is taken into consideration. This paper considers factoring integers and finding discrete logarithms, two problems which are generally thought to be hard on a classical computer and have been used as the basis of several proposed cryptosystems. Efficient randomized algorithms are given for these two problems on a hypothetical quantum computer. These algorithms take a number of steps polynomial in the input size, e.g., the number of digits of the integer to be factored.  AMS subject classifications: 82P10, 11Y05, 68Q10. 1 Introduction  One of the first results in the mathematics of computation, which underlies the subsequent development of much of theoretical computer science, was the distinction between computable and ...
67|Algorithms for Quantum Computation: Discrete Logarithms and Factoring|A computer is generally considered to be a universal computational device; i.e., it is believed able to simulate any physical computational device with a cost in com-putation time of at most a polynomial factol: It is not clear whether this is still true when quantum mechanics is taken into consideration. Several researchers, starting with David Deutsch, have developed models for quantum mechanical computers and have investigated their compu-tational properties. This paper gives Las Vegas algorithms for finding discrete logarithms and factoring integers on a quantum computer that take a number of steps which is polynomial in the input size, e.g., the number of digits of the integer to be factored. These two problems are generally considered hard on a classical computer and have been used as the basis of several proposed cryptosystems. (We thus give the first examples of quantum cryptanulysis.)
68|Quantum theory, the Church-Turing principle and the universal quantum computer|computer
69|Quantum complexity theory|Abstract. In this paper we study quantum computation from a complexity theoretic viewpoint. Our first result is the existence of an efficient universal quantum Turing machine in Deutsch’s model of a quantum Turing machine (QTM) [Proc. Roy. Soc. London Ser. A, 400 (1985), pp. 97–117]. This construction is substantially more complicated than the corresponding construction for classical Turing machines (TMs); in fact, even simple primitives such as looping, branching, and composition are not straightforward in the context of quantum Turing machines. We establish how these familiar primitives can be implemented and introduce some new, purely quantum mechanical primitives, such as changing the computational basis and carrying out an arbitrary unitary transformation of polynomially bounded dimension. We also consider the precision to which the transition amplitudes of a quantum Turing machine need to be specified. We prove that O(log T) bits of precision suffice to support a T step computation. This justifies the claim that the quantum Turing machine model should be regarded as a discrete model of computation and not an analog one. We give the first formal evidence that quantum Turing machines violate the modern (complexity theoretic) formulation of the Church–Turing thesis. We show the existence of a problem, relative to an oracle, that can be solved in polynomial time on a quantum Turing machine, but requires superpolynomial time on a bounded-error probabilistic Turing machine, and thus not in the class BPP. The class BQP of languages that are efficiently decidable (with small error-probability) on a quantum Turing machine satisfies BPP ? BQP ? P ?P. Therefore, there is no possibility of giving a mathematical proof that quantum Turing machines are more powerful than classical probabilistic Turing machines (in the unrelativized setting) unless there is a major breakthrough in complexity theory.
70|Rapid solution of problems by quantum computation|A class of problems is described which can be solved more efficiently by quantum computation than by any classical or stochastic method. The quantum computation solves the problem with certainty in exponentially less time than any classical deterministic computation.
72|Strengths and Weaknesses of quantum computing|  Recently a great deal of attention has been focused on quantum computation following a
73|Elementary Gates for Quantum Computation|We show that a set of gates that consists of all one-bit quantum gates (U(2)) and the two-bit exclusive-or gate (that maps Boolean values (x,y) to (x,x?y)) is universal in the sense that all unitary operations on arbitrarily many bits n (U(2 n)) can be expressed as compositions of these gates. We investigate the number of the above gates required to implement other gates, such as generalized Deutsch-Toffoli gates, that apply a specific U(2) transformation to one input bit if and only if the logical AND of all remaining input bits is satisfied. These gates play a central role in many proposed constructions of quantum computational networks. We derive upper and lower bounds on the exact number of elementary gates required to build up a variety of two- and three-bit quantum gates, the asymptotic number required for n-bit Deutsch-Toffoli gates, and make some observations about the number required for arbitrary n-bit unitary operations.
74|Two-Bit Gates Are Universal for Quantum Computation|A proof is given, which relies on the commutator algebra of the unitary Lie groups, that quantum gates operating on just two bits at a time are sufficient to construct a general quantum circuit. The best previous result had shown the universality of three-bit gates, by analogy to the universality of the Toffoli three-bit gate of classical reversible computing. Two-bit quantum gates may be implemented by magnetic resonance operations applied to a pair of electronic or nuclear spins. A &#034;gearbox quantum computer&#034; proposed here, based on the principles of atomic force microscopy, would permit the operation of such two-bit gates in a physical system with very long phase breaking (i.e., quantum phase coherence) times. Simpler versions of the gearbox computer could be used to do experiments on Einstein-Podolsky-Rosen states and related entangled quantum states. 
75|Universality in quantum computation|We show that in quantum computation almost every gate that operates on two or more bits is a universal gate. We discuss various physical considerations bearing on the proper definition of universality for computational components such as logic gates.
76|Discrete logarithms in gf(p) using the number field sieve  (1993) |Recently, several algorithms using number field sieves have been given to factor a number n in heuristic expected time Ln[1/3; c], where Ln[v; c]  = exp{(c + o(1))(log n) v (log log n) 1-v}, for n ? 8. In this paper we present an algorithm to solve the discrete logarithm problem for GF (p) with heuristic expected running time Lp[1/3; 3 2/3]. For numbers of a special form, there is an asymptotically slower but more practical version of the algorithm.
77|The Number Field Sieve|The number field sieve is an algorithm to factor integers of the form r e  s for small positive r and s . This note is intended as a `report on work in progress&#039; on this algorithm. We informally describe the algorithm, discuss several implementation related aspects, and present some of the factorizations obtained so far. We also
78|Quantum cryptanalysis of hidden linear functions|Abstract. Recently there has been a great deal of interest in the power of \Quantum Computers &#034; [4, 15, 18]. The driving force is the recent beautiful result of Shor that shows that discrete log and factoring are solvable in random quantum polynomial time [15]. We use a method similar to Shor&#039;s to obtain a general theorem about quantum polyno-mial time. We show that any cryptosystem based on what we refer to as a `hidden linear form &#039; can be broken in quantum polynomial time. Our results imply that the discrete log problem is doable in quantum poly-nomial time over any group including Galois elds and elliptic curves. Finally, we introduce the notion of `junk bits &#039; which are helpful when performing classical computations that are not injective. 1
79|The Quantum Challenge to Structural Complexity Theory|This is a non-technical survey paper of recent quantum-mechanical discoveries that challenge generally accepted complexity-theoretic versions of the Church--Turing thesis. In particular, building on pionering work of David Deutsch and Richard Jozsa, we construct an oracle relative to which there exists a set that can be recognized in Quantum Polynomial Time (QP), yet any Turing machine that recognizes it would require exponential time even if allowed to be probabilistic, provided that errors are not tolerated. In particular, QP 6` ZPP relative to this oracle. Furthermore, there are cryptographic tasks that are demonstrably impossible to implement with unlimited computing power probabilistic interactive Turing machines, yet they can be implemented even in practice by quantum mechanical apparatus. 1 Deutsch&#039;s Quantum Computer In a bold paper published in the Proceedings of the Royal Society, David Deutsch put forth in 1985 the quantum computer [7] (see also [8]). Even though this may c...
80|Parallel quantum computation|A computer is a physical system which has a very general ability to simulate other physical systems (and in particular, other computers). In this paper we investigate the question of whether microscopic quantum systems can be computers. Using a reversible cellular automaton model of computation we illustrate several approaches to this question. We then attempt to extend Feynman’s construction of a quantum computer in order to arrive at a quantum model of parallel processing. 1
81|Maintaining coherence in Quantum Computers|The effect of the inevitable coupling to external degrees of freedom of a quantum computer are examined. It is found that for quantum calculations (in which the maintenance of coherence over a large number of states is important), not only must the coupling be small but the time taken in the quantum calculation must be less than the thermal time scale, ¯h/kBT. For longer times the condition on the strength of the coupling to the external world becomes much more stringent. I.
82|The complexity of analog computation |We ask if analog computers can solve NP-complete problems efficiently. Regarding this as unlikely, we formulate a strong version of Church’s Thesis: that any analog computer can be simulated efficiently (in polynomial time) by a digital computer. From this assumption and the assumption that P ? NP we can draw conclusions about the operation of physical devices used for computation. An NP-complete problem, 3-SAT, is reduced to the problem of checking whether a feasible point is a local optimum of an optimization problem. A mechanical device is proposed for the solution of this problem. It encodes variables as shaft angles and uses gears and smooth cams. If we grant Strong Church’s Thesis, that P ? NP, and a certain ‘‘Downhill Principle’ ’ governing the physical behavior of the machine, we conclude that it cannot operate successfully while using only polynomial resources. We next prove Strong Church’s Thesis for a class of analog computers described by well-behaved ordinary differential equations, which we can take as representing part of classical mechanics. We conclude with a comment on the recently discovered connection between spin glasses and combinatorial optimization. 1.
83|Precision-Sensitive Euclidean Shortest Path in 3-Space|This paper introduces the concept of precision-sensitive algorithms,  in analogy to the well-known output-sensitive algorithms. We exploit this idea in studying the complexity of the 3-dimensional Euclidean shortest path problem. Specifically, we analyze an incremental approximation approach based on ideas in [CSY], and show that this approach yields an asymptotic improvement of running time. By using an optimization technique to improve paths on fixed edge sequences, we modify this algorithm to guarantee a relative error of O(2  \Gammar  ) in a time polynomial in r and 1=ffi, where ffi denotes the relative difference in path length between the shortest and the second shortest path. Our result is the best possible in some sense: if we have a strongly precision-sensitive algorithm then we can show that USAT (unambiguous SAT) is in polynomial time, which is widely conjectured to be unlikely. Finally, we discuss the practicability of this approach. Experimental results are provided. 
84|Quantum Computers, Factoring and Decoherence|In a quantum computer any superposition of inputs evolves unitarily into the corresponding superposition of outputs. It has been recently demonstrated that such computers can dramatically speed up the task of finding factors of large numbers – a problem of great practical significance because of its cryptographic applications. Instead of the nearly exponential ( ~ exp L 1/3,foranumberwithL digits) time required by the fastest classical algorithm, the quantum algorithm gives factors in a time polynomial in L ( ~ L 2). This enormous speed-up is possible in principle because quantum computation can simultaneously follow all of the paths corresponding to the distinct classical inputs, obtaining the solution as a result of coherent quantum interference between the alternatives. Hence, a quantum computer is sophisticated interference device, and it is essential for its quantum state to remain coherent in the course of the operation. In this report we investigate the effect of decoherence on the quantum factorization algorithm and establish an upper bound on a “quantum factorizable ” L based on the decoherence suffered per operational step.  
85|The stabilisation of quantum computations|We propose a method for the stabilisation of quantum computations (including quantum state storage). The method is based on the operation of projection into SYM, the symmetric subspace of the full state space of R redundant copies of the computer. We describe an efficient algorithm and quantum network effecting SYM–projection and discuss the stabilising effect of the proposed method in the context of unitary errors generated by hardware imprecision, and nonunitary errors arising from external environmental interaction. Finally, limitations of the method are discussed. Any realistic model of computation must conform to certain requirements imposed not by the mathematical properties of the model but by the laws of physics. Computations which require an exponentially increasing precision or exponential amount of time, space, energy or any other physical resource are normally regarded as unrealistic
87|Data Security|The rising abuse of computers and increasing threat to personal privacy through data banks have stimulated much interest m the techmcal safeguards for data. There are four kinds of safeguards, each related to but distract from the others. Access controls regulate which users may enter the system and subsequently whmh data sets an active user may read or wrote. Flow controls regulate the dissemination of values among the data sets accessible to a user. Inference controls protect statistical databases by preventing questioners from deducing confidential information by posing carefully designed sequences of statistical queries and correlating the responses. Statlstmal data banks are much less secure than most people beheve. Data encryption attempts to prevent unauthorized disclosure of confidential information in transit or m storage. This paper describes the general nature of controls of each type, the kinds of problems they can and cannot solve, and their inherent limitations and weaknesses. The paper is intended for a general audience with little background in the area.
89|New Directions in Cryptography|Two kinds of contemporary developments in cryptography are examined. Widening applications of teleprocessing have given rise to a need for new types of cryptographic systems, which minimize the need for secure key distribution channels and supply the equivalent of a written signature. This paper suggests ways to solve these currently open problems. It also discusses how the theories of communication and computation are beginning to provide the tools to solve cryptographic problems of long standing.
90|The Protection of Information in Computer Systems|This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures--whether hardware or software--that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysis of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading. Glossary The following glossary provides, for reference, brief definitions for several terms as used in this paper in the context of protecting information in computers. Access The ability to make use of information stored in a computer system. Used frequently as a verb, to the horror of grammarians. Access control list A list of principals that are authorized to have access to some object. Authenticate To verify the identity of a person (or other agent external to the protection system) making a request.
91|A Lattice Model of Secure Information Flow|This paper investigates mechanisms that guarantee secure information flow in a computer system. These mechanisms are examined within a mathematical framework suitable for formulating the requirements of secure information flow among security classes. The central component of the model is a lattice structure derived from the security classes and justified by the semantics of information flow. The lattice properties permit concise formulations of the security requirements of different existing systems and facilitate the construction of mechanisms that enforce security. The model provides a unifying view of all systems that restrict information flow, enables a classification of them according to security objectives, and suggests some new approaches. It also leads to the construction of automatic program certification mechanisms for verifying the secure flow of information through a program.
92|A Note on the Confinement Problem|This not explores the problem of confining a program during its execution so that it cannot transmit information to any other program except its caller. A set of examples attempts to stake out the boundaries of the problem. Necessary conditions for a solution are stated and informally justified.
93|Certification of Programs for Secure Information Flow|This paper presents a certification mechanism for verifying the secure flow of information through a program. Because it exploits the properties of a lattice structure among security classes, the procedure is sufficiently simple that it can easily be included in the analysis phase of most existing compilers. Appropriate semantics are presented and proved correct. An important application is the confinement problem: The mechanism can prove that a program cannot cause supposedly nonconfidential results to depend on confidential input data.
94|Password Security: A Case History|This paper describes the history of the design of the password security scheme on a  remotely accessed time-sharing system. The present design was the result of countering  observed attempts to penetrate the system. The result is a compromise between extreme  security and ease of use.
95|A hardware architecture for implementing protection rings|Protection of computations and information is an important aspect of a computer utility. In a system which usessegmentation as a memory addressing scheme, protection can be achieved in part by associating concentric rings of decreasing access privilege with a computation. This paper describes hardware processor mechanisms for implementing these rings of protection. The mechanisms allow cross-ring calls and subsequent returns to occur without trapping to the supervisor. Automatic hardware validation of referencesacross ring boundaries is also performed. Thus, a call by a user procedure to a protected subsystem (including the the supervisor) is identical to a call to a companion user procedure. The mechanisms of passing and referencing arguments are the same in both cases as well.
96|The tracker: a threat to statistical database security|The query programs of certain databases report raw statistics for query sets, which are groups of records specified implicitly by a characteristic formula. The raw statistics include query set size and sums of powers of values in the query set. Many users and designers believe that the individual records will remain confidential as long as query programs refuse to report the statistics of query sets which are too small. It is shown that the compromise of small query sets can in fact almost always be accomplished with the help of characteristic formulas called trackers. Schlorer’s individual tracker is reviewed, it is derived from known characteristics of a given individual and permits deducing additional characteristics he may have. The general tracker is introduced: It permits calculating statistics for arbitrary query sets, without requiring preknowledge of anything in the database. General trackers always exist if there are enough distinguishable classes of individuals in the database, in which case the trackers have a simple form. Almost all databases have a general tracker, and general trackers are almost always easy to find. Security is not guaranteed by the lack of a general tracker.
97|Operating Systems|Introduction Early operating systems were control programs a few thousand bytes long that scheduled jobs, drove peripheral devices, and kept track of system usage for billing purposes. Modern operating systems are much larger, ranging from hundreds of thousands of bytes for personal computers (e.g., MS/DOS, Xenix) to tens of millions of bytes for mainframes (e.g., Honeywell&#039;s Multics, IBM&#039;s MVS, AT
98|Third generation computer systems|The common features of third generation operating systems are surveyed from a general view, with emphasis on the common abstractions that constitute at least the basis for a &amp;quot;theory &amp;quot; of operating systems. Properties of specific systems are not discussed except where examples are useful. The technical aspects of issues and concepts are stressed, the nontechnical aspects mentioned only briefly. A perfunctory knowledge of third generation systems is presumed. Key words and phrases: multiprogramming systems, operating systems, supervisory systems, time-sharing systems, programming, storage allocation, memory allocation, processes, concurrency, parallelism, resource allocation, protection CR categories: 1.3, 4.0, 4.30, 6.20 It has been the custom to divide the era of electronic computing into &amp;quot;generations&amp;quot; whose approximate dates are:
99|Bayesian Data Analysis|I actually own a copy of Harold Jeffreys’s Theory of Probability but have only read small bits of it, most recently over a decade ago to confirm that, indeed, Jeffreys was not too proud to use a classical chi-squared p-value when he wanted to check the misfit of a model to data (Gelman, Meng and Stern, 2006). I do, however, feel that it is important to understand where our probability models come from, and I welcome the opportunity to use the present article by Robert, Chopin and Rousseau as a platform for further discussion of foundational issues. 2 In this brief discussion I will argue the following: (1) in thinking about prior distributions, we should go beyond Jeffreys’s principles and move toward weakly informative priors; (2) it is natural for those of us who work in social and computational sciences to favor complex models, contra Jeffreys’s preference for simplicity; and (3) a key generalization of Jeffreys’s ideas is to explicitly include model checking in the process of data analysis.
100|Bayes Factors|In a 1935 paper, and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P -values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this paper we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology and psychology.
101|Discrete Multivariate Analysis: Theory and Practice|the collaboration of Richard J. Light and Frederick Mosteller.
102|Games and decisions|Agency
104|Bayesian Model Selection in Social Research (with Discussion by Andrew Gelman &amp; Donald B. Rubin, and Robert M. Hauser, and a Rejoinder)  (1995) |It is argued that P-values and the tests based upon them give unsatisfactory results, especially in large samples. It is shown that, in regression, when there are many candidate independent variables, standard variable selection procedures can give very misleading results. Also, by selecting a single model, they ignore model uncertainty and so underestimate the uncertainty about quantities of interest. The Bayesian approach to hypothesis testing, model selection and accounting for model uncertainty is presented. Implementing this is straightforward using the simple and accurate BIC approximation, and can be done using the output from standard software. Specific results are presented for most of the types of model commonly used in sociology. It is shown that this approach overcomes the difficulties with P values and standard model selection procedures based on them. It also allows easy comparison of non-nested models, and permits the quantification of the evidence for a null hypothesis...
105|Prior distributions for variance parameters in hierarchical models|Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-t family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider noninformative and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of “noninformative ” prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-t family when the number of groups is small and in other settings where a weakly informative prior is desired.
106|Model selection and accounting for model uncertainty in graphical models using Occam&#039;s window|We consider the problem of model selection and accounting for model uncertainty in high-dimensional contingency tables, motivated by expert system applications. The approach most used currently is a stepwise strategy guided by tests based on approximate asymptotic P-values leading to the selection of a single model; inference is then conditional on the selected model. The sampling properties of such a strategy are complex, and the failure to take account of model uncertainty leads to underestimation of uncertainty about quantities of interest. In principle, a panacea is provided by the standard Bayesian formalism which averages the posterior distributions of the quantity of interest under each of the models, weighted by their posterior model probabilities. Furthermore, this approach is optimal in the sense of maximising predictive ability. However, this has not been used in practice because computing the posterior model probabilities is hard and the number of models is very large (often greater than 1011). We argue that the standard Bayesian formalism is unsatisfactory and we propose an alternative Bayesian approach that, we contend, takes full account of the true model uncertainty byaveraging overamuch smaller set of models. An efficient search algorithm is developed for nding these models. We consider two classes of graphical models that arise in expert systems: the recursive causal models and the decomposable
107|Posterior Predictive Assessment of Model Fitness Via Realized Discrepancies|Abstract: This paper considers Bayesian counterparts of the classical tests for goodness of fit and their use in judging the fit of a single Bayesian model to the observed data. We focus on posterior predictive assessment, in a framework that also includes conditioning on auxiliary statistics. The Bayesian formulation facilitates the construction and calculation of a meaningful reference distribution not only for any (classical) statistic, but also for any parameter-dependent “statistic ” or discrepancy. The latter allows us to propose the realized discrepancy assessment of model fitness, which directly measures the true discrepancy between data and the posited model, for any aspect of the model which we want to explore. The computation required for the realized discrepancy assessment is a straightforward byproduct of the posterior simulation used for the original Bayesian analysis. We illustrate with three applied examples. The first example, which serves mainly to motivate the work, illustrates the difficulty of classical tests in assessing the fitness of a Poisson model to a positron emission tomography image that is constrained to be nonnegative. The second and third examples illustrate the details of the posterior predictive approach in two problems: estimation in a model with inequality constraints on the parameters, and estimation in a mixture model. In all three examples, standard test statistics (either a ? 2 or a likelihood ratio) are not pivotal: the difficulty is not just how to compute the reference distribution for the test, but that in the classical framework no such distribution exists, independent of the unknown model parameters. Key words and phrases: Bayesian p-value, ? 2 test, discrepancy, graphical assessment, mixture model, model criticism, posterior predictive p-value, prior predictive
108|Sampling and Bayes inference in scientific modeling and robustness. (with discussion  (1980) |t-
109|Information-theoretic asymptotics of Bayes methods| In the absence of knowledge of the true density function, Bayesian models take the joint density function for a sequence of n random variables to be an average of densities with respect to a prior. We examine the relative entropy distance D,, between the true density and the Bayesian density and show that the asymptotic distance is (d/2Xlogn)+ c, where d is the dimension of the parameter vector. Therefore, the relative entropy rate D,,/n converges to zero at rate (logn)/n. The constant c, which we explicitly identify, depends only on the prior density function and the Fisher information matrix evaluated at the true parameter value. Consequences are given for density estima-tion, universal data compression, composite hypothesis testing, and stock-market portfolio selection.  
110|Bayes factors and marginal distributions in invariant situations|SUMMARY. In Bayesian analysis with a “minimal ” data set and common noninformative priors, the (formal) marginal density of the data is surprisingly often independent of the error distribution. This results in great simplifications in certain model selection methodologies; for instance, the Intrinsic Bayes Factor for models with this property reduces simply to the Bayes factor with respect to the noninformative priors. The basic result holds for comparison of models which are invariant with respect to the same group structure. Indeed the condi-tion reduces to a condition on the distributions of the common maximal invariant. In these situations, the marginal density of a “minimal ” data set is typically available in closed form, regardless of the error distribution. This provides very useful expressions for computation of Intrinsic Bayes Factors in more general settings. The conditions for the results to hold are explored in some detail for nonnormal linear models and various transformations thereof. 1.
111|Formal Rules for Selecting Prior Distributions: A Review and Annotated Bibliography|Subjectivism has become the dominant philosophical foundation for Bayesian inference. Yet, in practice, most Bayesian analyses are performed with so-called &#034;noninformative&#034; priors, that is, priors constructed by some formal rule. We review the plethora of techniques for constructing such priors, and discuss some of the practical and philosophical issues that arise when they are used. We give special emphasis to Jeffreys&#039;s rules and discuss the evolution of his point of view about the interpretation of priors, away from unique representation of ignorance toward the notion that they should be chosen by convention. We conclude that the problems raised by the research on priors chosen by formal rules are serious and may not be dismissed lightly; when sample sizes are small (relative to the number of parameters being estimated) it is dangerous to put faith in any &#034;default&#034; solution; but when asymptotics take over, Jeffreys&#039;s rules and their variants remain reasonable choices. We also provi...
112|Bernstein Von Mises Theorem for linear functionals of the density|In this paper, we study the asymptotic posterior distribution of linear functionals of the density. In particular, we give general conditions to obtain a semiparametric version of the Bernstein-Von Mises theorem. We then apply this general result to nonparametric priors based on infinite dimensional exponential families. As a byproduct, we also derive adaptive nonparametric rates of concentration of the posterior distributions under these families of priors on the class of Sobolev and Besov spaces.
113|A Bayesian approach to the selection and testing of mixture models|Abstract: An important aspect of mixture modeling is the selection of the number of mixture components. In this paper, we discuss the Bayes factor as a selection tool. The discussion will focus on two aspects: computation of the Bayes factor and prior sensitivity. For the computation, we propose a variant of Chib’s estimator that accounts for the non-identifiability of the mixture components. To reduce the prior sensitivity of the Bayes factor, we propose to extend the model with a hyperprior. We further discuss the use of posterior predictive checks for examining the fit of the model. The ideas are illustrated by means of a psychiatric diagnosis example.
114|Computational methods for Bayesian model choice|In this note, we shortly survey some recent approaches on the approximation of the Bayes factor used in Bayesian hypothesis testing and in Bayesian model choice. In particular, we reassess importance sampling, harmonic mean sampling, and nested sampling from a unified perspective.
115|Bayesian Inference on Mixtures of Distributions|This survey covers state-of-the-art Bayesian techniques for the estimation of mixtures. It complements the earlier Marin et al. (2005) by studying new types of distributions, the multinomial, latent class and t distributions. It also exhibits closed form solutions for Bayesian inference in some discrete setups. At last, it sheds a new light on the computation of Bayes factors via the approximation of Chib (1995). 
116|Nonasymptotic bounds for Bayesian order identification with application to mixtures|The efficiency of two Bayesian order estimators is studied. By using nonparametric techniques, we prove new underestimation and overestimation bounds. The results apply to various models, including mixture models. In this case, the errors are shown to be O(e -an) and O((log n) b /  v n) (a,b&gt; 0), respectively. 1. Introduction. Order
117|The Elimination of Nuisance Parameters|We review the Bayesian approach to the problem of the elimination of nuisance parameters from a statistical model. Many Bayesian statisticians feel that the framework of Bayesian statistics is so clear and simple that the elimination of nuisance parameters should not be considered a problem: one has simply to compute the marginal posterior distribution of the parameter of interest. However we will argue that this exercise need not be so simple from a practical perspective. The paper is divided in two main parts: the first deals with regular parametric models whereas the second will focus on non regular problem, including the so-called Neyman and Scott’s class of models and semiparametric models where the nuisance parameter lies in an infinite dimensional space. Finally we relate the issues of the elimination of nuisance parameters to other, apparently different, problems. Occasionally, we will mention non Bayesian treatment of nuisance parameters, mainly for comparative analyses.
118|Implementing data cubes efficiently|Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensional data cubes. Each cell of the data cube is a view consisting of an aggregation of interest, like total sales. The values of many of these cells are dependent on the values of other cells in the data cube..A common and powerful query optimization technique is to materialize some or all of these cells rather than compute them from raw data each time. Commercial systems differ mainly in their approach to materializing the data cube. In this paper, we investigate the issue of which cells (views) to materialize when it is too expensive to materialize all views. A lattice framework is used to express dependencies among views. We present greedy algorithms that work off this lattice and determine a good set of views to materialize. The greedy algorithm performs within a small constant factor of optimal under a variety of models. We then consider the most common case of the hypercube lattice and examine the choice of materialized views for hypercubes in detail, giving some good tradeoffs between the space used and the average time to answer a query. 1
119|Data cube: A relational aggregation operator generalizing group-by, cross-tab, and sub-totals|Abstract. Data analysis applications typically aggregate data across many dimensions looking for anomalies or unusual patterns. The SQL aggregate functions and the GROUP BY operator produce zero-dimensional or one-dimensional aggregates. Applications need the N-dimensional generalization of these operators. This paper defines that operator, called the data cube or simply cube. The cube operator generalizes the histogram, crosstabulation, roll-up, drill-down, and sub-total constructs found in most report writers. The novelty is that cubes are relations. Consequently, the cube operator can be imbedded in more complex non-procedural data analysis programs. The cube operator treats each of the N aggregation attributes as a dimension of N-space. The aggregate of a particular set of attribute values is a point in this space. The set of points forms an N-dimensional cube. Super-aggregates are computed by aggregating the N-cube to lower dimensional spaces. This paper (1) explains the cube and roll-up operators, (2) shows how they fit in SQL, (3) explains how users can define new aggregate functions for cubes, and (4) discusses efficient techniques to compute the cube. Many of these features are being added to the SQL Standard.
120|Query evaluation techniques for large databases|Database management systems will continue to manage large data volumes. Thus, efficient algorithms for accessing and manipulating large sets and sequences will be required to provide acceptable performance. The advent of object-oriented and extensible database systems will not solve this problem. On the contrary, modern data models exacerbate it: In order to manipulate large sets of complex objects as efficiently as today’s database systems manipulate simple records, query processing algorithms and software will become more complex, and a solid understanding of algorithm and architectural issues is essential for the designer of database management software. This survey provides a foundation for the design and implementation of query execution facilities in new database management systems. It describes a wide array of practical query evaluation techniques for both relational and post-relational database systems, including iterative execution of complex query evaluation plans, the duality of sort- and hash-based set matching algorithms, types of parallel query execution and their implementation, and special operators for emerging database application domains.
121|Multi-table joins through bitmapped join indices|This technical note shows how to combine some well-known techniques to create a method that will efficiently execute common multi-table joins. We concentrate on a commonly occurring type of join known as a star-join, although the method presented will generalize to any type of multi-table join. A star-join consists of a central detail table with large cardinality, such as an orders table (where an order row contains a single purchase) with foreign keys that join to descriptive tables, such as customers, products, and (sales) agents. The method presented in this note uses join indices with compressed bitmap representations, which allow predicates restricting columns of descriptive tables to determine an answer set (or foundsef) in the central detail table; the method uses different predicates on different descriptive tables in combination to restrict the detail table through compressed bitmap representations of join indices, and easily completes the join of the fully restricted detail table rows back to the descriptive tables. We outline realistic examples where the combination of these techniques yields substantial performance improvements over alternative, more traditional query evaluation plans. 1.
122|Aggregate-Query Processing in Data Warehousing Environments|In this paper we introduce generalized projections (GPs), an extension of duplicate eliminating projections, that capture aggregations, groupbys, duplicate-eliminating projections (distinct), and duplicate-preserving projections in a common unified framework. Using GPs we extend well known and simple algorithms for SQL queries that use distinct projections to derive algorithms for queries using aggregations like sum, max, min, count, and avg. We develop powerful query rewrite rules for aggregate queries that unify and extend rewrite rules previously known in the literature. We then illustrate the power of our approach by solving a very practical and important problem in data warehousing: how to answer an aggregate query on base tables using materialized aggregate views (summary tables).
123|Including Group-By in Query Optimization|In existing relational database systems, processing of group-by and computation of aggregate functions are always postponed until all joins are performed. In this paper, we present transformations that make it possible to push group-by operation past one or more joins and can potentially reduce the cost of processing a query significantly. Therefore, the placement of group-by should be decided based on cost estimation. We explain how the traditional System-R style optimizers can be modified by incorporating the greedy conservative heuristic that we developed. We prove that applications of greedy conservative heuristic produce plans that are better (or no worse) than the plans generated by a traditional optimizer. Our experimental study shows that the extent of improvement in the quality of plans is significant with only a modest increase in optimization cost. Our technique also applies to optimization of Select Distinct queries by pushing down duplicate elimination in a cost-based fas...
124|Eager Aggregation and Lazy Aggregation|Efficient processing of aggregation queries is essential for decision support applications. This paper describes a class of query transformations, called eager aggregation and lazy aggregation, that allows a query optimizer to move group-by operations up and down the query tree. Eager aggregation partially pushes a group-by past a join. After a group-by is partially pushed down, we still need to perform the original group-by in the upper query block. Eager aggregation reduces the number of input rows to the join and thus may result in a better overall plan. The reverse transformation, lazy aggregation, pulls a group-by above a join and combines two group-by operations into one. This transformation is typically of interest when an aggregation query references a grouped view (a view containing a group-by). Experimental results show that the technique is very beneficial for queries in the TPC-D benchmark. 1 Introduction Aggregation is widely used in decision support systems. All queries...
125|The Lorel Query Language for Semistructured Data|We present the Lorel language, designed for querying semistructured data. Semistructured data is becoming more and more prevalent, e.g., in structured documents such as HTML and when performing simple integration of data from multiple sources. Traditional data models and query languages are inappropriate, since semistructured data often is irregular, some data is missing, similar concepts are represented using different types, heterogeneous sets are present, or object structure is not fully known. Lorel is a user-friendly language in the SQL/OQL style for querying such data effectively. For wide applicability, the simple object model underlying Lorel can be viewed as an extension of ODMG and the language as an extension of OQL. The main novelties of the Lorel language are: (i) extensive use of coercion to relieve the user from the strict typing of OQL, which is inappropriate for semistructured data
126|Querying Semi-Structured Data|

127|Object exchange across heterogeneous information sources|We address the problem of providing integrated access to diverse and dynamic information sources. We explain how this problem differs from the traditional database integration problem and we focus on one aspect of the information integration problem, namely information exchange. We define an object-based information exchange model and a corresponding query language that we believe are well suited for integration of diverse information sources. We describe how, the model and language have been used to integrate heterogeneous bibliographic information sources. We also describe two general-purpose libraries we have implemented for object exchange between clients and servers.
128|Querying object-oriented databases|We present a novel language for querying object-oriented databases. The language is built around the idea of extended path expressions that substantially generalize [ZAN83], and on an adaptation of the first-order formalization of object-oriented languages from [KW89, KLW90, KW92]. The language incorporates features not found in earlier proposals; it is easier to use and has greater expressive power. Some of the salient features of our language are: ffl Precise model-theoretic semantics. ffl A very expressive form of path expressions that not only can do joins, selections and unnesting, but can also be used to explore the database schema. ffl Views can be defined and manipulated in a much more uniform way than in other proposals. ffl Database schema can be explored in the very same language that is used to retrieve data. Unlike in relational languages, the user needs not know anything about the system tables that store schema information. ffl The notions of a type and type-correctness have precise meaning. It accommodates a wide variety of queries that might be deemed well- or ill-typed under different circumstances. In particular, we show that there is more than one way of settling the issue of type correctness. For expository purposes and due to space limitation, we chose to make a number of simplifying assumptions and left some features out. A more complete account can be found in [KSK92].
129|From Structured Documents to Novel Query Facilities|Structured documents (e.g., SGML) can benefit a lot from database support and more specifically from object-oriented database (OODB) management systems. This paper describes a natural mapping from SGML documents into OODB&#039;s and a formal extension of two OODB query languages (one SQL-like and the other calculus) in order to deal with SGML document retrieval. Although motivated by structured documents, the extensions of query languages that we present are general and useful for a variety of other OODB applications. A key element is the introduction of paths as first class citizens. The new features allow to query data (and to some extent schema) without exact knowledge of the schema in a simple and homogeneous fashion. 1 Introduction  Structured documents are central to a wide class of applications such as software engineering, libraries, technical documentation, etc. They are often stored in file systems and document access tools are somewhat limited. We believe that (object-oriented) d...
130|Object fusion in mediator systems|One of the main tasks of mediators is to fuse information from heterogeneous information sources. This may involve, for example, removing redundancies, and resolving inconsistencies in favor of the most reliable source. The problem becomes harder when the sources are unstructured/semistructured and we do not have complete knowledge of their contents and structure. In this paper we show how many common fusion operations can be specified non-procedurally and succinctly. The key to our approach is to assign semantically meaningful object ids to objects as they are &amp;quot;imported &amp;quot; into the mediator.
131|Finding Regular Simple Paths In Graph Databases|We consider the following problem: given a labelled directed graph G and a regular expression R, find all pairs of nodes connected by a simple path such that the concatenation of the labels along the path satisfies R. The problem is motivated by the observation that many recursive queries in relational databases can be expressed in this form, and by the implementation of a query language, G+ , based on this observation. We show that the problem is in general intractable, but present an algorithm than runs in polynomial time in the size of the graph when the regular expression and the graph are free of conflicts. We also present a class of languages whose expressions can always be evaluated in time polynomial in the size of both the graph and the expression, and characterize syntactically the expressions for such languages. 
132|MedMaker: A Mediation System Based on Declarative Specifications|Mediators are used for integration of heterogeneous information sources. We present a system for declaratively specifying mediators. It is targeted for integration of sources with unstructured or semi-structured data and/or sources with changing schemas. We illustrate the main features of the Mediator Specification Language (MSL), show how they facilitate integration, and describe the implementation of the system that interprets the MSL specifications. 
133|Representative Objects: Concise Representations of Semistructured Hierarchical Data|In this paper we introduce the representative object, which uncovers the inherent schema(s) in semistructured, hierarchical data sources and provides a concise description of the structure of the data. Semistructured data, unlike data stored in typical relational or object-oriented databases, does not have fixed schema that is known in advance and stored separately from the data. Withthe rapid growth of the World Wide Web, semistructured hierarchical data sources are becoming widely available to the casual user. The lack of external schema information currently makes browsing and querying these data sources inefficient at best, and impossible at worst. We show how representative objects make schema discovery efficient and facilitate the generation of meaningful queries over the data. 1.
135|Evaluating Queries with Generalized Path Expressions|In the past few years, query languages featuring generalized path expressions have been proposed. These languages allow the interrogation of both data and structure. They are powerful and essential for a number of applications. However, until now, their evaluation has relied on a rather naive and inefficient algorithm. In this paper, we extend an object algebra with two new operators and present some interesting rewriting techniques for queries featuring generalized path expressions. We also show how a query optimizer can integrate the new techniques.  1 Introduction  In the past few years there has been a growing interest in query languages featuring generalized path expressions (GPE) [BRG88, KKS92, CACS94, QRS  +  95]. With these languages, one may issue queries on data without exact knowledge of its structure. A GPE queries data and structure at the same time. Although very useful for standard database applications, these languages are vital for new applications dedicated, for insta...
136|Integrating a Structured-Text Retrieval System with an Object-Oriented Database System|We describe the integration of a structured-text retrieval system (TextMachine) into an  object-oriented database system (OpenODB). Our approach is a light-weight one, using the external  function capability of the database system to encapsulate the text retrieval system as an  external information source. Yet, we are able to provide a tight integration in the query language  and processing; the user can access the text retrieval system using a standard database query  language. The efficient and effective retrieval of structured text performed by the text retrieval  system is seamlessly combined with the rich modeling and general-purpose querying capabilities  of the database system, resulting in an integrated system with querying power beyond those of  the underlying systems. The integrated system also provides uniform access to textual data in  the text retrieval system and structured data in the database system, thereby achieving information  fusion. We discuss the design and imple...
137|DataGuides: Enabling Query Formulation and Optimization in Semistructured Databases|In semistructured databases there is no schema fixed  in advance. To provide the benefits of a schema in  such environments, we introduce DataGuides:  concise and accurate structural summaries of  semistructured databases. DataGuides serve as  dynamic schemas, generated from the database; they  are useful for browsing database structure,  formulating queries, storing information such as  statistics and sample values, and enabling query  optimization. This paper presents the theoretical  foundations of DataGuides along with an algorithm  for their creation and an overview of incremental  maintenance. We provide performance results based  on our implementation of DataGuides in the Lore  DBMS for semistructured data. We also describe the  use of DataGuides in Lore, both in the user interface  to enable structure browsing and query formulation,  and as a means of guiding the query processor and  optimizing query execution.
138|Answering queries using views|We consider the problem of computing answers to queries by using materialized views. Aside from its potential in optimizing query evaluation, the problem also arises in applications such as Global Information Systems, Mobile Computing and maintaining physical data independence. We consider the problem of nding a rewriting of a query that uses the materialized views, the problem of nding minimal rewritings, and nding complete rewritings (i.e., rewritings that use only the views). We show that all the possible rewritings can be obtained by considering containment mappings from the views to the query, and that the problems we consider are NP-complete when both the query and the views are conjunctive and don&#039;t involve built-in comparison predicates. We show that the problem has two independent sources of complexity (the number of possible containment mappings, and the complexity of deciding which literals from the original query can be deleted). We describe a polynomial time algorithm for nding rewritings, and show that under certain conditions, it will nd the minimal rewriting. Finally, we analyze the complexity of the problems when the queries and views may be disjunctive and involve built-in comparison predicates. 1
139|A Query Language and Optimization Techniques for Unstructured Data|A new kind of data model has recently emerged in which the database is not constrained by a conventional schema. Systems like ACeDB, which has become very popular with biologists, and the recent Tsimmis proposal for data integration organize data in tree-like structures whose components can be used equally well to represent sets and tuples. Such structures allow great flexibility in data representation  What query language is appropriate for such structures? Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or &#034;vertical&#034; dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the &#034;horizontal&#034; dimension of UnQL.  
140|Lore: A database management system for semistructured data|Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources. 
141|Maintenance of Materialized Views: Problems, Techniques, and Applications|In this paper we motivate and describe materialized views, their applications, and the problems  and techniques for their maintenance. We present a taxonomy of view maintenanceproblems  basedupon the class of views considered, upon the resources used to maintain the view, upon the types of modi#cations to the base data that areconsidered during maintenance, and whether the technique works for all instances of databases and modi#cations. We describe some of the view maintenancetechniques proposed in the literature in terms of our taxonomy. Finally, we consider new and promising application domains that are likely to drive work in materialized  views and view maintenance.  1 Introduction  What is a view? A view is a derived relation de#ned in terms of base #stored# relations. A view thus de#nes a function from a set of base tables to a derived table; this function is typically recomputed every time the view is referenced.  What is a materialized view? A view can be materialized by storin...
142|Objects and Views|Object-oriented databases have been introduced primarily to ease the development of database applications. However, the difficulties encountered when, for instance, trying to restructure data or integrate databases demonstrate that the models being used still lack flexibility. We claim that the natural way to overcome these shortcomings is to introduce a sophisticated view mechanism. This paper presents such a mechanism, one which allows a programmer to restructure the class hierarchy and modify the behavior and structure of objects. The mechanism allows a programmer to specify attribute values implicitly, rather than storing them. It also allows him to introduce new classes into the class hierarchy. These virtual classes are populated by selecting existing objects from other classes and by creating new objects. Fixing the identify of new objects during database updates introduces subtle issues into view design. Our presentation, mostly informal, leans on a number of illustrative examples meant to emphasize the simplicity of our mechanism. 1
143|Regular Path Queries with Constraints|The evaluation of path expression queries on semistructured data in a distributed asynchronous environment is considered. The focus is on the use of local information expressed in the form of path constraints in the optimization of path expression queries. In particular, decidability and complexity results on the implication problem for path constraints are established.
144|Supporting Views in Object-Oriented Databases|  Relational database systems provide a powerful abstraction mechanism: any query, since it returns a relation, can be used to define a view, that becomes a derived (or virtual) relation. Views are defined by a statement such as &#034;define view &lt;name&gt; as &lt;query&gt;.&#034; Views can be used to tailor the global database schema.  . Query formulation is simplified, if frequent subexpressions are predefined.  . Application programs are insulated from changes to the underlying schema.  . Information can be restructured to better suit an application programs&#039; requirements.  . Derived information is kept consistent with base data.  . Access restrictions (for authorization) can be enforced by hiding data.  In contrast to base relations, views are typically not stored permanently, but rather computed on demand. Queries to view relations are modified by query substitution so as to operate on the underlying b
145|MultiView: A Methodology for Supporting Multiple Views in Object-Oriented Databases|A view in object-oriented databases (OODB) corresponds to virtual schema graph with possibly restructured generalization and decomposition hierarchies. We propose a methodology, called MultiView, for supporting multiple such view schemata. MultiView represents a simple yet powerful approach achieved by breaking view specification into independent tasks: class derivation, global schema integration, view class selection, and view schema generation. Novel features of MultiView include an object algebra for class customization; an algorithm for the integration of virtual classes into the global schema; a view definition language for view class selection, and the automatic generation of a view class hierarchy. In addition, we present algorithms that verify the closure property of a view and, if found to be incomplete, transform it into a closed, yet minimal, view. Lastly, we introduce the fundamental concept of view independence and show  MultiView to be view independent.  
146|The GMAP: a versatile tool for physical data independence|.&lt;F3.733e+05&gt; Physical data independence is touted as a central feature of modern database systems. It allows users to frame queries in terms of the logical structure of the data, letting a query processor automatically translate them into optimal plans that access physical storage structures. Both relational and object-oriented systems, however, force users to frame their queries in terms of a logical schema that is directly tied to physical structures. We present an approach that eliminates this dependence. All storage structures are defined in a declarative language based on relational algebra as functions of a logical schema. We present an algorithm, integrated with a conventional query optimizer, that translates queries over this logical schema into plans that access the storage structures. We also show how to compile update requests into plans that update all relevant storage structures consistently and optimally. Finally, we report on experiments with a prototype implementation ...
147|Virtual Schemas and Bases|We propose the notions of virtual schemas and virtual bases as a coherent way of integrating various features in OODB views. A virtual schema is defined based on some existing (real) schema. A virtual base is obtained when a (real) base is attached to a virtual schema. We study the consequences of this simple assumption. In particular, we observe the differences between a real schema and a virtual one. We also consider an extension (that we call generic schemas) where it is necessary to specify several real bases to attach data to a virtual schema. We show how the flexibility provided by virtual schemas can be used to cope with various dynamic features of database systems. 1 Introduction Views are intended to increase the flexibility of database systems and their definition in the object-oriented database (OODB) context comes as a natural extension of the original paradigm. The yet relatively young research on this topic has introduced a large variety of indispensable new features. H...
148|Limma: linear models for microarray data|This free open-source software implements academic research by the authors and co-workers. If you use it, please support the project by citing the appropriate journal articles listed in Section 2.1.Contents
150|Normalization for cDNA microarray data: a robust composite method addressing single and multiple slide systematic variation|There are many sources of systematic variation in cDNA microarray experiments which affect the measured gene expression levels (e.g. differences in labeling efficiency between the two fluorescent dyes). The term normalization refers to the process of removing such variation. A constant adjustment is often used to force the distribution of the intensity log ratios to have a median of zero for each slide. However, such global normalization approaches are not adequate in situations where dye biases can depend on spot overall intensity and/or spatial location within the array. This article proposes normalization methods that are based on robust local regression and account for intensity and spatial dependence in dye biases for different types of cDNA microarray experiments. The selection of appropriate controls for normalization is discussed and a novel set of controls (microarray sample pool, MSP) is introduced to aid in intensity-dependent normalization. Lastly, to allow for comparisons of expression levels across slides, a robust method based on maximum likelihood estimation is proposed to adjust for scale differences among slides.
151|Normalization of cDNA microarray data|Normalization means to adjust microarray data for effects which arise from variation in the technology rather than from biological differences between the RNA samples or between the printed probes. This article describes normalization methods based on the fact that dye balance typically varies with spot intensity and with spatial position on the array. Print-tip loess normalization provides a well-tested general purpose normalization method which has given good results on a wide range of arrays. The method may be refined by using quality weights for individual spots. The method is best combined with diagnostic plots of the data which display the spatial and intensity trends. When diagnostic plots show that biases still remain in the data after normalization, further normalization steps such as plate-order normalization or scalenormalization between the arrays may be undertaken. Composite normalization may be used when control spots are available which are known to be not differentially expressed. Variations on loess normalization include global loess normalization and 2D normalization. Detailed commands are given to implement the normalization techniques using freely available software. 1
152|Use of within-array replicate spots for assessing differential expression in microarray experiments|Motivation. Spotted arrays are often printed with probes in duplicate or triplicate, but current methods for assessing differential expression are not able to make full use of the resulting information. Usual practice is to average the duplicate or triplicate results for each probe before assessing differential expression. This loses valuable information about gene-wise variability. Results. A method is proposed for extracting more information from within-array replicate spots in microarray experiments by estimating the strength of the correlation between them. The method involves fitting separate linear models to the expression data for each gene but with a common value for the between-replicate correlation. The method greatly improves the precision with which the genewise variances are estimated and thereby improves inference methods designed to identify differentially expressed genes. The method may be combined with empirical Bayes methods for moderating the genewise variances between genes. The method is validated using data from a microarray experiment involving calibration and ratio control spots in conjunction with spiked-in RNA. Comparing results for calibration and ratio control spots shows that the common correlation method results in substantially better discrimination of differentially expressed genes from those which are not. The spike-in experiment also confirms that the results may be further improved by empirical Bayes smoothing of the variances when the sample size is small. Availability. The methodology is implemented in the limma software package for R, available from the CRAN repository
153|Identifying differentially expressed genes using false discovery rate controlling procedures|Motivation: DNA microarrays have recently been used for the purpose of monitoring expression levels of thousands of genes simultaneously and identifying those genes that are differentially expressed. The probability that a false identification (type I error) is committed can increase sharply when the number of tested genes gets large. Correlation between the test statistics attributed to gene co-regulation and dependency in the measurement errors of the gene expression levels further complicates the problem. In this paper we address this very large multiplicity problem by adopting the false discovery rate (FDR) controlling approach. In order to address the dependency problem, we present three resampling-based FDR controlling procedures, that account for the test statistics distribution, and compare their performance to that of the naïve application of the linear step-up procedure in Benjamini and Hochberg (1995). The procedures are studied using simulated microarray data, and their performance is examined relative to their ease of implementation. Results: Comparative simulation analysis shows that all four FDR controlling procedures control the FDR at the desired level, and retain substantially more power then the family-wise error rate controlling procedures. In terms of power, using resampling of the marginal distribution of each test statistics substantially improves the performance over the naïve one. The highest power is achieved, at the expense of a more sophisticated algorithm, by the resampling-based procedures that resample the joint distribution of the test statistics and estimate the level of FDR control.
154|Assessing Gene Significance from cDNA Microarray Expression Data via Mixed Models|The determination of a list of differentially expressed genes is a basic objective in many cDNA microarray experiments. We present a statistical approach that allows direct control over the percentage of false positives in such a list and, under certain reasonable assumptions, improves on existing methods with respect to the percentage of false negatives. The method accommodates a wide variety of experimental designs and can simultaneously assess significant differences between multiple types of biological samples. Two interconnected mixed linear models are central to the method and provide a flexible means to properly account for variability both across and within genes. The mixed model also provides a convenient framework for evaluating the statistical power of any particular experimental design and thus enables a researcher to a priori select an appropriate number of replicates. We also suggest some basic graphics for visualizing lists of significant genes. Analyses of published experiments studying human cancer and yeast cells illustrate the results.
155|Statistical Issues in cDNA Microarray Data Analysis|This article summarizes some of the issues involved and provides a brief review of the analysis tools which are available to researchers to deal with them. Any microarray experiment involves a number of distinct stages. Firstly there is the design of the experiment. The researchers must decide which genes are to be printed on the arrays, which sources of RNA are to be hybridized to the arrays and on how many arrays the hybridizations will be replicated. Secondly, after hybridization, there follows a number of data-cleaning steps or `low-level analysis&#039; of the microarray data. The microarray images must be processed to acquire red and green foreground and background intensities for each spot. The acquired red/green ratios must be normalized to adjust for dye-bias and for any systematic variation other than that due to the differences between the RNA samples being studied. Thirdly, the normalized ratios are analyzed by various graphical and numerical means to select differentially expressed genes or to find groups of genes whose expression profiles can reliably classify the different RNA sources into meaningful groups. The sections of this article correspond roughly to the various analysis steps. The following notation will be used throughout the article. The foreground red and green intensities will be written Pp and 9p for each spot. The background intensities will be Pf and 9f . The background-corrected intensities will be P and 9 where usually P Pp Pf 0 # and 9 9p 9f 0 # . The log-differential expression ratio will be vyq # E P 9 0 for each spot. Finally, the log-intensity of the spot will be   vyq 3 P9 0 , a measure of the overall brightness of the spot. (The letter E is a mnemonic for minus as vyq vyq E P 9 0 # while 3 is a mnemonic for add as #vyq vyq #...
156|The Subread aligner: fast, accurate and scalable read mapping by seed-and-vote. Nucleic Acids Res 2013;41:e108 |Read alignment is an ongoing challenge for the analysis of data from sequencing technologies. This article proposes an elegantly simple multi-seed strategy, called seed-and-vote, for mapping reads to a reference genome. The new strategy chooses the mapped genomic location for the read directly from the seeds. It uses a relatively large number of short seeds (called subreads) extracted from each read and allows all the seeds to vote on the optimal location. When the read length is &lt;160bp, overlapping subreads are used. More con-ventional alignment algorithms are then used to fill in detailed mismatch and indel information between the subreads that make up the winning voting block. The strategy is fast because the overall genomic location has already been chosen before the detailed alignment is done. It is sensitive because no individual subread is required to map exactly, nor are individual subreads constrained to map close by other subreads. It is accurate because the final location must be supported by several dif-ferent subreads. The strategy extends easily to find exon junctions, by locating reads that contain sets of subreads mapping to different exons of the same gene. It scales up efficiently for longer reads.
157|CARMAweb: comprehensive R- and Bioconductor-based web service for microarray data analysis|web service for microarray data analysis
158|Optimizing the noise versus bias trade-off for Illumina whole genome expression BeadChips. Nucleic acids research. 2010; 38:e204. [PubMed: 20929874 |Five strategies for pre-processing intensities from Illumina expression BeadChips are assessed from the point of view of precision and bias. The strategies include a popular variance stabilizing transformation and model-based background cor-rections that either use or ignore the control probes. Four calibration data sets are used to evaluate precision, bias and false discovery rate (FDR). The original algorithms are shown to have operating characteristics that are not easily com-parable. Some tend to minimize noise while others minimize bias. Each original algorithm is shown to have an innate intensity offset, by which unlogged intensities are bounded away from zero, and the size of this offset determines its position on the noise–bias spectrum. By adding extra offsets, a continuum of related algorithms with different noise–bias trade-offs is generated, allowing direct comparison of the performance of the strategies on equivalent terms. Adding a positive offset is shown to decrease the FDR of each original algo-rithm. The potential of each strategy to generate an algorithm with an optimal noise–bias trade-off is explored by finding the offset that minimizes its FDR. The use of control probes as part of the back-ground correction and normalization strategy is shown to achieve the lowest FDR for a given bias.
159|Estimating the proportion of microarray probes expressed in an RNA sample. Nucleic acids research 38|A fundamental question in microarray analysis is the estimation of the number of expressed probes in different RNA samples. Negative control probes available in the latest microarray platforms, such as Illumina whole genome expression BeadChips, provide a unique opportunity to estimate the number of expressed probes without setting a threshold. A novel algorithm was proposed in this study to estimate the number of expressed probes in an RNA sample by utilizing these negative controls to measure background noise. The perfor-mance of the algorithm was demonstrated by comparing different generations of Illumina BeadChips, comparing the set of probes targeting well-characterized RefSeq NM transcripts with other probes on the array and comparing pure samples with heterogenous samples. Furthermore, hematopoietic stem cells were found to have a larger transcriptome than progenitor cells. Aire knockout medullary thymic epithelial cells were shown to have significantly less expressed probes than matched wild-type cells.
160|arrayMagic: twocolour cDNA microarray quality control and preprocessing|Summary: arrayMagic is a software package for quality control and preprocessing of two-colour cDNA microarray data. The automated analysis pipeline comprises data import, normalization, replica merging, quality diagnostics and data export. The script-based processing combines reproducibility and flexibility at high-throughput and provides quality-assured and preprocessed microarray data to high-level follow-up analysis. Availability: The R package arrayMagic is available with BSD license at
161|doi:10.1093/nar/gkp366 Pomelo II: finding differentially expressed genes|Pomelo II
162|GK: Individual channel analysis of two-colour microarray data|The traditional approach to the analysis of data from two-colour spotted microarrays is to compute the log-ratio of the expression values for each spot (Chen et al, 1997). The log-ratios are then treated as the responses in any statistical analysis of the data (Yang and Speed, 2003; Smyth, 2004). Relatively few papers have analysed spotted microarrays in terms of the separate red and green log-intensities (Kerr et al, 2000; Jin et al, 2001; Wolfinger et al, 2001). The second and third of these papers popularised a mixed model approach in which each spot is treated as a randomised block of size two. A number of papers starting with Yang et al (2001) have summarised red and green channel intensities in terms of M-values (log-ratios) and A-values (spot log-intensities) for the purposes of graphical displays and normalisation. This paper demonstrates that the usefulness of this partition arises in good part from the fact that the M and A-values for a given spot are approximately independent even though the individual intensities are highly correlated. This paper reformulates the mixed model approach in terms of the M and A-values. This approach not only presents an efficient algorithm for estimating the mixed model but also elucidates the difference between the traditional log-ratio based approach and the analysis of
163|Power-law distributions in empirical data|Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the empirical detection and characterization of power laws is made difficult by the large fluctuations that occur in the tail of the distribution. In particular, standard methods such as least-squares fitting are known to produce systematically biased estimates of parameters for power-law distributions and should not be used in most circumstances. Here we describe statistical techniques for making accurate parameter estimates for power-law data, based on maximum likelihood methods and the Kolmogorov-Smirnov statistic. We also show how to tell whether the data follow a power-law distribution at all, defining quantitative measures that indicate when the power law is a reasonable fit to the data and when it is not. We demonstrate these methods by applying them to twentyfour real-world data sets from a range of different disciplines. Each of the data sets has been conjectured previously to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.
164|A Brief History of Generative Models for Power Law and Lognormal Distributions |Recently, I became interested in a current debate over whether file size distributions are best modelled by a power law distribution or a a lognormal distribution. In trying
165|A Random Graph Model for Massive Graphs|  We propose a random graph model which is a special case of sparse random graphs with given degree sequences. This model involves only a small number of parameters, called logsize and log-log growth rate. These parameters capture some universal characteristics of massive graphs. Furthermore, from these parameters, various properties of the graph can be derived. For example, for certain ranges of the parameters, we will compute the expected distribu-tion of the sizes of the connected components which almost surely occur with high probability. We will illustrate the consistency of our model with the behavior of some massive graphs derived from data in telecommunications. We will also discuss the threshold function, the giant component, and the evolution of random graphs in this model.  
166|Collective entity resolution in relational data|Many databases contain uncertain and imprecise references to real-world entities. The absence of identifiers for the underlying entities often results in a database which contains multiple references to the same entity. This can lead not only to data redundancy, but also inaccuracies in query processing and knowledge extraction. These problems can be alleviated through the use of entity resolution. Entity resolution involves discovering the underlying entities and mapping each database reference to these entities. Traditionally, entities are resolved using pairwise similarity over the attributes of references. However, there is often additional relational information in the data. Specifically, references to different entities may cooccur. In these cases, collective entity resolution, in which entities for cooccurring references are determined jointly rather than independently, can improve entity resolution accuracy. We propose a novel relational clustering algorithm that uses both attribute and relational information for determining the underlying domain entities, and we give an efficient implementation. We investigate the impact that different relational similarity measures have on entity resolution quality. We evaluate our collective entity resolution algorithm on multiple real-world databases. We show that it improves entity resolution performance over both attribute-based baselines and over algorithms that consider relational information but do not resolve entities collectively. In addition, we perform detailed experiments on synthetically generated data to identify data characteristics that favor collective relational resolution over purely attribute-based algorithms.
167|Where mathematics meets the Internet|The Internet has experienced a fascinating evolution in the recent past, especially since the early days of the Web, a fact well-documented not only in the trade journals, but also in the popular press. Unprecedented in its growth, unparalleled in its heterogeneity, and unpredictable or even chaotic in the behavior of its tra c, \the Internet is its own revolution&#034;, as Anthony-Michael Rutkowski, former Executive Director of the Internet Society, likes to put it.
168|A Functional Approach to External Graph Algorithms|. We present a new approach for designing external graph algorithms  and use it to design simple external algorithms for computing connected components,  minimum spanning trees, bottleneck minimum spanning trees, and maximal  matchings in undirected graphs and multi-graphs. Our I/O bounds compete  with those of previous approaches. Unlike previous approaches, ours is purely  functional---without side effects---and is thus amenable to standard checkpointing  and programming language optimization techniques. This is an important  practical consideration for applications that may take hours to run.  1 Introduction  We present a divide-and-conquer approach for designing external graph algorithms, i.e., algorithms on graphs that are too large to fit in main memory. Our approach is simple to describe and implement: it builds a succession of graph transformations that reduce to sorting, selection, and a recursive bucketing technique. No sophisticated data structures are needed. We apply our t...
169|Problems with fitting to the powerlaw distribution |Abstract. This short communication uses a simple experiment to show that fitting to a power law distribution by using graphical methods based on linear fit on the log-log scale is biased and inaccurate. It shows that using maximum likelihood estimation (MLE) is far more robust. Finally, it presents a new
170|Functional and topological characterization of protein interaction networks|The elucidation of the cell’s large-scale organization is a primary challenge for post-genomic biology, and understanding the structure of protein interaction networks offers an important starting point for such studies. We compare four available databases that approximate the protein interaction network of the yeast, Saccharomyces cerevisiae, aiming to uncover the network’s generic large-scale properties and the impact of the proteins ’ function and cellular localization on the network topology. We show how each database supports a scale-free, topology with hierarchical modularity, indicating that these features represent a robust and generic property of the protein interactions network. We also find strong correlations between the network’s structure and the functional role and subcellular localization of its protein constituents, concluding that most functional and/or localization classes appear as relatively segregated subnetworks of the full protein interaction network. The uncovered systematic differences between the four protein interaction databases reflect their relative coverage for different functional and localization classes and provide a guide for their utility in various bioinformatics studies. Keywords: Bioinformatics / Protein interaction networks / Scale-free networks 1
171|On the bias of traceroute sampling: or, power-law degree distributions in regular graphs|Understanding the graph structure of the Internet is a crucial step for building accurate network models and designing efficient algorithms for Internet applications. Yet, obtaining this graph structure can be a surprisingly difficult task, as edges cannot be explicitly queried. For instance, empirical studies of the network of Internet Protocol (IP) addresses typically rely on indirect methods like traceroute to build what are approximately single-source, all-destinations, shortest-path trees. These trees only sample a fraction of the network’s edges, and a recent paper by Lakhina et al. found empirically that the resulting sample is intrinsically biased. Further, in simulations, they observed that the degree distribution under traceroute sampling exhibits a power law even when the underlying degree distribution is Poisson. In this paper, we study the bias of traceroute sampling mathematically and, for a very general class of underlying degree distributions, explicitly calculate the distribution that will be observed. As example applications of our machinery, we prove that traceroute sampling finds power-law degree distributions in both d-regular and Poisson-distributed random graphs. Thus, our work puts the observations of Lakhina et al. on a rigorous footing, and extends them to nearly arbitrary degree distributions.
172|Currency and commodity metabolites: Their identification and relation to the modularity of metabolic networks|The large-scale shape and function of metabolic networks are intriguing topics of systems biology. Such networks are on one hand commonly regarded as modular (i.e. built by a number of relatively independent subsystems), but on the other hand they are robust in a way not expected of a purely modular system. To address this question we carefully discuss the partition of metabolic networks into subnetworks. The practice of preprocessing such networks by removing the most abundant substrates, “currency metabolites,” is formalized into a network-based algorithm. We study partitions for metabolic networks of many organisms and find cores of currency metabolites and modular peripheries of what we call “commodity metabolites.” The networks are found to be more modular than random networks but far from perfectly divisible into modules. We argue that cross-modular edges are the key for the robustness of metabolism. 
173|Likelihood-Based Inference for Stochastic Models of Sexual Network Formation|Sexually-Transmitted Diseases (STDs) constitute a major public health concern. Mathematical models for the transmission dynamics of STDs indicate that heterogeneity in sexual activity level allow them to persist even when the typical behavior of the population would not support endemicity. This insight focuses attention on the distribution of sexual activity level in a population. In this paper, we develop several stochastic process models for the f&#039;ormation of sexual partnership networks. Using likelihood-based model selection procedures, we assess the fit of the different models to three large distributions of sexual partner counts: (1) Rakai, Uganda, (2) Sweden, and (3) the USA. Five of&#039; the six single-sex networks were fit best by the negative binomial model. The American women&#039;s network was best fit by a power-law model, the Yule. For most networks, several competing models fit approximately equally well. These results sug- gest three conclusions: (1) no single unitary process clearly underlies the formation of these sexual networks, (2) behavioral heterogeneity plays an essential role in network structure, (3) substantial model uncertainty exists for sexual network degree distributions. Behavioral research focused on the mechanisms of partnership f&#039;ormation will play an essential role in specifying the best model for empirical degree distributions. We discuss the limitations of inferences f&#039;rom such data, and the utility of degree-based epidemiological models more generally.
174|Editorial: The future of power law research |Abstract. I argue that power law research must move from focusing on observation, interpretation, and modeling of power law behavior to instead considering the challenging problems of validation of models and control of systems. 1. The Problem with Power Law Research To begin, I would like to recall a humorous insight from the paper of Fabrikant, Koutsoupias, and Papadimitriou [Fabrikant et al. 01], consisting of this quote and the following footnote. “Power laws... have been termed ‘the signature of human activity’... ” 1 The study of power laws, especially in networks, has clearly exploded over the last decade, with seemingly innumerable papers and even popular books, such as Barabási’s Linked [Barabási 02] and Watts ’ Six Degrees [Watts 03]. Power laws are, indeed, everywhere. Despite this remarkable success, I believe that research into power laws in computer networks (and networks more generally) suffers from glaring deficiencies that need to be addressed by the community. Coping with these deficiencies should lead to another great burst of exciting and compelling research. To explain the problem, I would like to make an analogy to the area of string theory. String theory is incredibly rich and beautiful mathematically, with a simple and compelling basic starting assumption: the universe’s building blocks do not really correspond to (zero-dimensional) points, but to small 1 “They are certainly the product of one particular kind of human activity: looking for power laws... ” [Fabrikant et al. 01]
175|DYNAMICS OF BAYESIAN UPDATING WITH DEPENDENT DATA AND MISSPECIFIED MODELS|Recent work on the convergence of posterior distributions under Bayesian updating has established conditions under which the posterior will concentrate on the truth, if the latter has a perfect representation within the support of the prior, and under various dynamical assumptions, such as the data being independent and identically distributed or Markovian. Here I establish sufficient conditions for the convergence of the posterior distribution in non-parametric problems even when all of the hypotheses are wrong, and the data-generating process has a complicated dependence structure. The main dynamical assumption is the generalized asymptotic equipartition (or “Shannon-McMillan-Breiman”) property of information theory. I derive a kind of large deviations principle for the posterior measure, and discuss the advantages of predicting using a combination of models known to be wrong. An appendix sketches connections between the present results and the “replicator dynamics” of evolutionary theory.  
176|The QQ-Estimator And Heavy Tails|. A common visual technique for assessing goodness of fit and estimating location and scale is the qq--plot. We apply this technique to data from a Pareto distribution and more generally to data generated by a distribution with a heavy tail. A procedure for assessing the presence of heavy tails and for estimating the parameter of regular variation is discussed which can supplement other standard techniques such as the Hill plot. 1. Introduction.  A graphical technique called the qq-plot is a commonly used method of visually assessing goodness of fit and of estimating location and scale parameters. The method is standard and ubiquitious in various forms. See for example Rice (1988) and Castillo (1988). The method is based on the following simple observation: If  U 1;n  U 2;n  : : : U n;n  are the order statistics from n iid observations which are uniformly distributed on [0; 1], then by symmetry E(U i+1;n \Gamma U i;n ) = 1  n + 1 and hence  EU i;n =  i n + 1  :  Thus since U i;n should...
177|On the frequency of severe terrorist events|The online version of this article can be found at:
178|Radial structure of the internet|The structure of the Internet at the autonomous system (AS) level has been studied by the
179|Estimating heavy–tail exponents through max self–similarity|2 Heavy tailed data • A random variable X is said to be heavy–tailed if P{|X |  = x}  ~ L(x)x -a, as x ? 8, for some a&gt; 0 and a slowly varying function L. ? Here we focus on the simpler but important context: X = 0, a.s. and P{X&gt; x}  ~ Cx -a, as x ? 8. ? X (infinite moments) For p&gt; 0, EX p &lt; 8 if and only if p &lt; a. In particular, and 0 &lt; a = 2 ? Var(X)  = 8 0 &lt; a = 1 ? E|X |  = 8. • The estimation of the heavy–tail exponent a is an important problem with rich history. • Why do we need heavy–tail models? Every finite sample X1,..., Xn has finite sample mean, variance and all sample moments! Why consider heavy tailed models in practice?! 3 Why use heavy–tailed models? “All models are wrong, but some are useful.” George Box Let F and G be any two distributions with positive densities on (0, 8). Let ?&gt; 0 and x1,..., xn ? (0, 8) be arbitrary, then both: and PF {Xi ? (xi - ?, xi + ?), i = 1,..., n}&gt; 0 are positive! PG{Xi ? (xi - ?, xi + ?), i = 1,..., n}&gt; 0 • For a given sample, very many models apply. • The ones that continue to work as the sample grows are most suitable. We next present real data sets of Financial, Insurance and Internet data. They can be very heavy tailed. 4 Traded volumes on the Intel stock
180|Empirical distributions of logreturns: Between the stretched exponential and the power law? Quantitative Finance |A large consensus now seems to take for granted that the distributions of empirical returns of financial time series are regularly varying, with a tail exponent b close to 3. First, we show by synthetic tests performed on time series with time dependence in the volatility with both Pareto and Stretched-Exponential distributions that for sample of moderate size, the standard generalized extreme value (GEV) estimator is quite inefficient due to the possibly slow convergence toward the asymptotic theoretical distribution and the existence of biases in presence of dependence between data. Thus it cannot distinguish reliably between rapidly and regularly varying classes of distributions. The Generalized Pareto distribution (GPD) estimator works better, but still lacks power in the presence of strong dependence. Then, we use a parametric representation of the tail of the distributions of returns of 100 years of daily return of the Dow Jones Industrial Average and over 1 years of 5-minutes returns of the Nasdaq Composite index, encompassing both a regularly varying distribution in one limit of the parameters and rapidly varying distributions of the class of the Stretched-Exponential (SE) and Log-Weibull distributions in other limits. Using the method of nested hypothesis testing (Wilks ’ theorem),
181|Verbal reports as data|The central proposal of this article is that verbal reports are data. Accounting for verbal reports, as for other kinds of data, requires explication of the mech-anisms by which the reports are generated, and the ways in which they are sensitive to experimental factors (instructions, tasks, etc.). Within the theoret-ical framework of human information processing, we discuss different types of processes underlying verbalization and present a model of how subjects, in re-sponse to an instruction to think aloud, verbalize information that they are attending to in short-term memory (STM). Verbalizing information is shown to affect cognitive processes only if the instructions require verbalization of information that would not otherwise be attended to. From an analysis of what would be in STM at the time of report, the model predicts what can reliably be reported. The inaccurate reports found by other research are shown to result from requesting information that was never directly heeded, thus forcing subjects to infer rather than remember their mental processes. After a long period of time during which stimulus-response relations were at the focus of attention, research in psychology is now seeking to understand in detail the mecha-nisms and internal structure of cognitive pro-cesses that produce these relations. In the limiting case, we would like to have process models so explicit that they could actually produce the predicted behavior from the in-formation in the stimulus.
182|Controlled and automatic human information processing|A two-process theory of human information processing is proposed and applied to detection, search, and attention phenomena. Automatic processing is activa-tion of a learned sequence of elements in long-term memory that is initiated by appropriate inputs and then proceeds automatically—without subject control, without stressing the capacity limitations of the system, and without necessarily demanding attention. Controlled processing is a temporary activation of a se-quence of elements that can be set up quickly and easily but requires attention, is capacity-limited (usually serial in nature), and is controlled by the subject. A series of studies using both reaction time and accuracy measures is presented, which traces these concepts in the form of automatic detection and controlled, search through the areas of detection, search, and attention. Results in these areas are shown to arise from common mechanisms. Automatic detection is shown to develop following consistent mapping of stimuli to responses over trials. Controlled search is utilized in varied-mapping paradigms, and in our studies, it takes the form of serial, terminating search. The approach resolves a number of apparent conflicts in the literature.
183|Controlled and automatic human information processing: II. Perceptual learning, automatic attending and a general theory|The two-process theory of detection, search, and attention presented by Schneider and Shiffrin is tested and extended in a series of experiments. The studies demonstrate the qualitative difference between two modes of information processing: automatic detection and controlled search. They trace the course of the learning of automatic detection, of categories, and of automaticattention responses. They show the dependence of automatic detection on attending responses and demonstrate how such responses interrupt controlled processing and interfere with the focusing of attention. The learning of categories is shown to improve controlled search performance. A general framework for human information processing is proposed; the framework emphasizes the roles of automatic and controlled processing. The theory is compared to and contrasted with extant models of search and attention.
184|Data networks|a b s t r a c t In this paper we illustrate the core technologies at the basis of the European SPADnet project (www. spadnet.eu), and present the corresponding first results. SPADnet is aimed at a new generation of MRI-compatible, scalable large area image sensors, based on CMOS technology, that are networked to perform gamma-ray detection and coincidence to be used primarily in (Time-of-Flight) Positron Emission Tomography (PET). The project innovates in several areas of PET systems, from optical coupling to single-photon sensor architectures, from intelligent ring networks to reconstruction algorithms. In addition, SPADnet introduced the first computational model enabling study of the full chain from gamma photons to network coincidence detection through scintillation events, optical coupling, etc. &amp; 2013 Elsevier B.V. All rights reserved. 1.
185|Sequential data assimilation with a nonlinear quasi-geostrophic model using Monte Carlo methods to forecast error statistics|. A new sequential data assimilation method is discussed. It is based on forecasting the error statistics using Monte Carlo methods, a better alternative than solving the traditional and computationally extremely demanding approximate error covariance equation used in the extended Kalman filter. The unbounded error growth found in the extended Kalman filter, which is caused by an overly simplified closure in the error covariance equation, is completely eliminated. Open boundaries can be handled as long as the ocean model is well posed. Well-known numerical instabilities associated with the error covariance equation are avoided because storage and evolution of the error covariance matrix itself are not needed. The results are also better than what is provided by the extended Kalman filter since there is no closure problem and the quality of the forecast error statistics therefore improves. The method should be feasible also for more sophisticated primitive equation models. The computati...
186|Using the Extended Kalman Filter with a Multilayer Quasi-Geostrophic Ocean Model|this paper the extended Kalman filter is used with a nonlinear multilayer quasi-geostrophic (QG) model. This provides us with both a realistic ocean model and a very sophisticated error statistics scheme. The extended Kalman filter is an extension of the common Kalman filter and may be used when the model dynamics or the measurement equation is nonlinear. It consists of an approximative equation for the propagation of error covariances, and also approximative filter equations if the measurement equation is nonlinear. When changing from a linear system to nonlinear dynamics the possible existence of a wide variety of phenomena which are nonexistent in the linear theory is introduced. Nonlinear systems may have solutions with multiple equilibria, where the solutions sometimes abruptly undergo transitions from one equilibrium to another as parameters change (bifurcations). Also chaotic behavior occurs in many deterministic systems, where solutions exhibit an apparently random behavior. The Lorenz [1963] model is probably the best known example of chaotic systems. It has solutions which undergo &#034;unpredictable&#034; transitions between two different equilibria (chaos). As discussed by Miller and Ghil
187|Simplification of the Kalman Filter for Meteorological Data Assimilation|We propose a new statistical data assimilation method that is based on a simplification of the Kalman filter equations. The forecast error covariance evolution is approximated by simply advecting the mass error covariance field, by deriving the remaining covariances geostrophically, and by accounting for external model error forcing only at the end of each forecast cycle. This greatly reduces the cost of the forecast error covariance computation, which is the central and most expensive aspect of the Kalman filter algorithm. In simulations with a linear, one-dimensional shallow-water model and artificially generated data, the performance of the simplified filter is compared with that of the Kalman filter and the optimal interpolation (OI) method. These experiments are designed to isolate the effect of simplifying the forecast error covariance evolution. The simplified filter produces analyses that are nearly optimal, and represents a significant improvement over OI. ae  1 Introduction ...
188|Open Boundary Conditions for the Extended Kalman Filter With a Quasi-Geostrophic Ocean Model|this paper the work in Part I is extended to include open boundaries with inflow and outflow. The use of inflow boundaries with the QG model severely complicates the numerical treatment, but it is also of vital importance if mesoscale circulation is to be studied, using an extended Kalman filter to assimilate data in the QG model. Open and closed boundaries have quite different properties and are normally treated differently in a way that leads to a well-posed problem. It should be remembered that an open boundary with inflow or outflow is an artificial boundary. No knowledge is therefore available about how an open boundary shall be updated unless external data or information can be used. The general boundary conditions for the QG model have been discussed in several publications where Charney et al.
189|Bigtable: A distributed storage system for structured data|Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.  
190|MapReduce: Simplified Data Processing on Large Clusters|MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.  
191|Space/Time Trade-offs in Hash Coding with Allowable Errors|this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash- coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency
192|The Google File System |We have designed and implemented the Google File Sys-tem, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous dis-tributed file systems, our design has been driven by obser-vations of our application workloads and technological envi-ronment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore rad-ically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our ser-vice as well as research and development efforts that require large data sets. The largest cluster to date provides hun-dreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use. Categories and Subject Descriptors D [4]: 3—Distributed file systems
193|The ubiquitous B-tree|B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and nnplemented using B-trees This paper reviews B-trees and shows why they have been so successful It discusses the major variations of the B-tree, especially the B+-tree,
194|Parallel database systems: the future of high performance database systems|Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1.
195|Resource Containers: A New Facility for Resource Management in Server Systems|General-purpose operating systems provide inadequate support for resource management in large-scale servers. Applications lack sufficient control over scheduling and management of machine resources, which makes it difficult to enforce priority policies, and to provide robust and controlled service. There is a fundamental mismatch between the original design assumptions underlying the resource management mechanisms of current general-purpose operating systems, and the behavior of modern server applications. In particular, the operating system’s notions of protection domain and resource principal coincide in the process abstraction. This coincidence prevents a process that manages large numbers of network connections, for example, from properly allocating system resources among those connections. We propose and evaluate a new operating system abstraction called a resource container, which separates the notion of a protection domain from that of a resource principal. Resource containers enable fine-grained resource management in server systems and allow the development of robust servers, with simple and firm control over priority policies. 1
196|Operating System Support for Planetary-Scale Network Services|PlanetLab is a geographically distributed overlay network designed to support the deployment and evaluation of planetary-scale network services. Two high-level goals shape its design. First, to enable a large research community to share the infrastructure, PlanetLab provides distributed virtualization, whereby each service runs in an isolated slice of PlanetLab’s global resources. Second, to support competition among multiple network services, PlanetLab decouples the operating system running on each node from the networkwide services that define PlanetLab, a principle referred to as unbundled management. This paper describes how Planet-Lab realizes the goals of distributed virtualization and unbundled management, with a focus on the OS running on each node. 1
197|Paxos made live: an engineering perspective|We describe our experience building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected algorithmic and engineering problems encountered, and the solutions we found for them. Our measurements indicate that we have built a competitive system. 1
198|Integrating compression and execution in column-oriented database systems|Column-oriented database system architectures invite a reevaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads. In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in roworiented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.
199|Weaving Relations for Cache Performance|Relational database systems have traditionally optimzed for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM) (a.k.a., slotted pages). Recent research, however, indicates that cache utilization and performance is becoming increasingly important on modern platforms. In this paper, we first demonstrate that in-page data placement is the key to high cache performance and that NSM exhibits low cache utilization on modern platforms. Next, we propose a new data organization model called PAX (Partition Attributes Across), that significantly improves cache performance by grouping together all values of each attribute within each page. Because PAX only affects layout inside the pages, it incurs no storage penalty and does not affect I/O behavior. According to our experimental results, when compared to NSM (a) PAX exhibits superior cache and memory bandwidth utilization, saving at least 75% of NSM&#039;s stall time due to data cache accesses, (b) range selection queries and updates on memoryresident relations execute 17-25% faster, and (c) TPC-H queries involving I/O execute 11-48% faster.
200|Data Placement in Bubba|Thus paper examines the problem of data placement in Bubba, a highly-parallel system for data-intensive applications bemg developed at MCC “Highly-parallel” lmplres that load balancmng IS a cntlcal performance issue “Data-mtenave” means data IS so large that operatrons should be executed where the data resides As a result, data placement becomes a cntlcal performance issue In general, determmmg the optimal placement of data across processmg nodes for performance IS a difficult problem We describe our heuristic approach to solvmg the data placement problem w Bubba We then present expenmental results using a specific workload to provide msrght into the problem Several researchers have argued the benefits of deelustering (1 e, spreading each base relation over many nodes) We show that as declustermg IS increased. load balancing contmues to improve However, for transactions mvolvmg complex Joins, further declusterrng reduces throughput because of communications, startup and termmatron overhead We argue that data placement, especially declustermg, m a highly-parallel system must be considered early in the design, so that mechanrsms can be included for supportmg variable declustermg, for mmtmlzmg the most significant overheads associated with large-scale declustenng, and for gathering the required statistics.
201|DB2 Parallel Edition|The rate of increase in database size and response time requirements has outpaced  advancements in processor and mass storage technology. One way to satisfy the increasing  demand for processing power and I/O bandwidth in database applications is to have a  number of processors, loosely or tightly coupled, serving database requests concurrently.  Technologies developed during the last decade have made commercial parallel database  systems a reality and these systems have made an inroad into the stronghold of traditionally  mainframe based large database applications. This paper describes the parallel database  project initiated at IBM Research at Hawthorne and the DB2/AIX-PE product based on  it.  1 Introduction  Large scale parallel processing technology has made giant strides in the past decade and there is no doubt that it has established a place for itself. However, almost all of the applications harnessing this technology are scientific or engineering applications. The lack of com...
202|Pig Latin: A Not-So-Foreign Language for Data Processing |There is a growing need for ad-hoc analysis of extremely large data sets, especially at internet companies where innovation critically depends on being able to analyze terabytes of data collected every day. Parallel database products, e.g., Teradata, offer a solution, but are usually prohibitively expensive at this scale. Besides, many of the people who analyze this data are entrenched procedural programmers, who find the declarative, SQL style to be unnatural. The success of the more procedural map-reduce programming model, and its associated scalable implementations on commodity hardware, is evidence of the above. However, the map-reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse. We describe a new language called Pig Latin that we have designed to fit in a sweet spot between the declarative style of SQL, and the low-level, procedural style of map-reduce. The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. We give a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the development and execution of their data analysis tasks, compared to using Hadoop directly. We also report on a novel debugging environment that comes integrated with Pig, that can lead to even higher productivity gains. Pig is an open-source, Apache-incubator project, and available for general use. 1.
203|Interpreting the Data: Parallel Analysis with Sawzall |Very large data sets often have a flat but regular structure and span multiple disks and machines. Examples include telephone call records, network logs, and web document repositories. These large data sets are not amenable to study using traditional database techniques, if only because they can be too large to fit in a single relational database. On the other hand, many of the analyses done on them can be expressed using simple, easily distributed computations: filtering, aggregation, extraction of statistics, and so on. We present a system for automating such analyses. A filtering phase, in which a query is expressed using a new procedural programming language, emits data to an aggregation phase. Both phases are distributed over hundreds or even thousands of computers. The results are then collated and saved to a file. The design—including the separation into two phases, the form of the programming language, and the properties of the aggregators—exploits the parallelism inherent in having data and computation distributed across many machines. 1
204|Programming Parallel Algorithms|In the past 20 years there has been treftlendous progress in developing and analyzing parallel algorithftls. Researchers have developed efficient parallel algorithms to solve most problems for which efficient sequential solutions are known. Although some ofthese algorithms are efficient only in a theoretical framework, many are quite efficient in practice or have key ideas that have been used in efficient implementations. This research on parallel algorithms has not only improved our general understanding ofparallelism but in several cases has led to improvements in sequential algorithms. Unf:ortunately there has been less success in developing good languages f:or prograftlftling parallel algorithftls, particularly languages that are well suited for teaching and prototyping algorithms. There has been a large gap between languages
205|Map-Reduce-Merge: simplified relational data processing on large clusters|than the artifacts. 2. MapReduce 3. Map-Reduce-Merge: extending MapReduce 4. Using Map-Reduce-Merge to implement
206|Missing data: Our view of the state of the art|Statistical procedures for missing data have vastly improved, yet misconception and unsound practice still abound. The authors frame the missing-data problem, review methods, offer advice, and raise issues that remain unresolved. They clear up common misunderstandings regarding the missing at random (MAR) concept. They summarize the evidence against older procedures and, with few exceptions, dis-courage their use. They present, in both technical and practical language, 2 general approaches that come highly recommended: maximum likelihood (ML) and Bayes-ian multiple imputation (MI). Newer developments are discussed, including some for dealing with missing data that are not MAR. Although not yet in the main-stream, these procedures may eventually extend the ML and MI methods that currently represent the state of the art. Why do missing data create such difficulty in sci-entific research? Because most data analysis proce-dures were not designed for them. Missingness is usu-ally a nuisance, not the main focus of inquiry, but
207|Statistical Analysis with Missing Data|Subsample ignorable likelihood for regression
208|Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation|We propose a remedy for the discrepancy between the way political scientists analyze data with missing values and the recommendations of the statistics community. Methodologists and statisticians agree that &#034;multiple imputation&#034; is a superior approach to the problem of missing data scattered through one&#039;s explanatory and dependent variables than the methods currently used in applied data analysis. The reason for this discrepancy lies with the fact that the computational algorithms used to apply the best multiple imputation models have been slow, difficult to implement, impossible to run with existing commercial statistical packages, and demanding of considerable expertise.  In this paper, we adapt an existing algorithm, and use it to implement a generalpurpose, multiple imputation model for missing data. This algorithm is considerably faster and easier to use than the leading method recommended in the statistics literature. We also quantify the risks of current missing data practices, ...
209|Multiple imputation for multivariate missing-data problems: a data analyst&#039;s perspective|Analyses of multivariate data are frequently hampered by missing values. Until re-cently, the only missing-data methods available to most data analysts have been relatively ad hoc practices such as listwise deletion. Recent dramatic advances in theoretical and com-putational statistics, however, have produced a new generation of flexible procedures with a sound statistical basis. These procedures involve multiple imputation (Rubin, 1987), a simu-lation technique that replaces each missing datum with a set of m&gt;1 plausible values. The m versions of the complete data are analyzed by standard complete-data methods, and the results are combined using simple rules to yield estimates, standard errors, and p-values that formally incorporate missing-data uncertainty. New computational algorithms and software described in a recent book (Schafer, 1997) allow us to create proper multiple imputations in complex multivariate settings. This article reviews the key ideas of multiple imputation, discusses the software programs currently available, and demonstrates their use on data from
210|Application of random-effects pattern-mixture models for missing data in longitudinal studies|Random-effects regression models have become increasingly popular for analysis of longitudinal data. A key advantage of the random-effects approach is that it can be applied when subjects are not measured at the same number of timepoints. In this article we describe use of random-effects pattern-mixture models to further handle and describe the influence of missing data in longitudinal studies. For this approach, subjects are first divided into groups depending on their missing-data pattern and then variables based on these groups are used as model covariates. In this way, researchers are able to examine the effect of missing-data patterns on the outcome (or outcomes) of interest. Furthermore, overall estimates can be obtained by averaging over the missing-data patterns. A psychiatric clinical trials data set is used to illustrate the random-effects pattern-mixture approach to longitudinal data analysis with missing data.  
211|Multiple Imputation for Missing Data: A Cautionary Tale|: Two algorithms for producing multiple imputations for missing data are evaluated with simulated data. Software using a propensity score classifier with the approximate Bayesian boostrap produces badly biased estimates of regression coefficients when data on predictor variables are missing at random or missing completely at random. On the other hand, a regression-based method employing the data augmentation algorithm produces estimates with little or no bias.  4 Multiple imputation (MI) appears to be one of the most attractive methods for generalpurpose handling of missing data in multivariate analysis. The basic idea, first proposed by Rubin (1977) and elaborated in his (1987) book, is quite simple: 1. Impute missing values using an appropriate model that incorporates random variation. 2. Do this M times (usually 3-5 times), producing M &#034;complete&#034; data sets. 3. Perform the desired analysis on each data set using standard complete-data methods. 4. Average the values of the parameter ...
212|On Structural Equation Modeling with Data that are not Missing Completely at Random|A general latent variable model is given which includes the specification of a missing data mechanism. This framework allows for an elucidating discussion of existing general multivariate theory bearing on maximum likelihood estimation with missing data. Here, missing completely at random is not a prerequisite for unbiased estimation in large samples, as when using the traditional listwise or pairwise present data approaches. The theory is connected with old and new results in the area of selection and factorial invariance. It is pointed out that in many applications, maximum likelihood estimation with missing data may be carried out by existing structural equation modeling software, such as LISREL and LISCOMP. Several sets of artifical data are generated within the general model framework. The proposed estimator is compared to the two traditional ones and found superior. Key words: maximum likelihood, ignorability, selectivity, factor analysis, factorial invariance,
213|Multiple Imputation in Practice: Comparison of Software Packages for Regression Models With Missing Variables |This article reviews multiple imputation, describes assumptions that it requires, and reviews software packages that implement this procedure. We apply the methods and compare the results using two examples---a child psychopathology dataset with missing outcomes and an artificial dataset with missing covariates. We conclude with some discussion of the strengths and weaknesses of these implementations as well as advantages and limitations of imputation
214|Imputation of the 1989 Survey of Consumer Finances: Stochastic Relaxation and Multiple Imputation” mimeo, Board of Governors of the Federal Reserve System|acknowledges the support for this work by staff in the Division of Research and Statistics including
215|ANALYSIS WITH MISSING DATA IN PREVENTION RESEARCH|Missing data are pervasive in alcohol and drug abuse prevention evaluation efforts: Researchers administer surveys, and some items are left unanswered. Slow readers often leave large portions incomplete at the end of the survey. Researchers administer the surveys at several points in time, and people fail to show up at one or more waves of measurement. Researchers often design their measures to include a certain amount of “missingness”; some measures are so expensive (in money or time) that researchers can afford to administer them only to some respondents. Missing data problems have been around for years. Until recently, researchers have fumbled with partial solutions and put up only the weakest counterarguments to the admonitions of the critics of prevention and applied psychological research. Things have changed, however. Statistically sound solutions are now available for virtually every missing data problem,
216|Inference with Imputed Conditional Means|In this paper, we develop analytic techniques that can be used to produce appropriate inferences from a data set in which imputation for missing values has been carried out using predictive means. Our derivations are based on asymptotic expansions of point estimators and their associated variance estimators, and the resulting formulas can be thought of as first-order approximations to the estimators that would be used with multiple imputation. The procedures developed can be used either for univariate missing data or for multivariate missing data in which the variables are either missing or observed together, and they are designed for situations in which the complete-data estimator is a smooth function of linear statistics. We illustrate properties of our methods in several examples, including abstract problems as well as applications to large data sets from studies carried out by the federal government. Key Words: Linearization; Missing data; Multiple Imputation; Nonresponse; Taylor s...
217|Maximum Likelihood Analysis of Generalized Linear Models with Missing Covariates|Missing data is a common occurrence in most medical research data collection enterprises. There is an extensive literature concerning missing data, much of which has focused on missing outcomes. Covariates in regression models are often missing, particularly if information is being collected from multiple sources. The method of weights is an implementation of the EM algorithm 8 for general maximum-likelihood analysis of regression models, including generalized linear models 32 (GLMs) with incomplete covariates. In this paper, we will describe the method of weights in detail, illustrate its application with several examples, discuss its advantages and limitations, and review extensions and applications of the method.
218|Multiple imputation and posterior simulation for multivariate missing data in longitudinal studies (pp  (1995) |SUMMARY. This paper outlines a multiple imputation method for handling missing data in designed lon-gitudinal studies. A random coefficients model is developed to accommodate incomplete multivariate con-tinuous longitudinal data. Multivariate repeated measures are jointly modeled; specifically, an i.i.d. normal model is assumed for time-independent variables and a hierarchical random coefficients model is assumed for time-dependent variables in a regression model conditional on the time-independent variables and time, with heterogeneous error variances across variables and time points. Gibbs sampling is used to draw model parameters and for imputations of missing observations. An application to data from a study of startle reactions illustrates the model. A simulation study compares the multiple imputation procedure to the weighting approach of Robins, Rotnitzky, and Zhao (1995, Journal of the American Statistical Association 90, 106-121) that can be used to address similar data structures. KEY WORDS: Gibbs sampling; Missing data; Multiple imputation; Multivariate longitudinal data 1. Background In designed longitudinal studies, missing data often occur be-cause subjects miss visits during the study, because some vari-ables may not be measured at particular visits, or because
221|Regularization and variable selection via the Elastic Net|Summary. We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.
222|On Model Selection Consistency of Lasso|Sparsity or parsimony of statistical models is crucial for their proper interpretations, as  in sciences and social sciences. Model selection is a commonly used method to find such  models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani,  1996) is now being used as a computationally feasible alternative to model selection.
223|Leave-One-Out Support Vector Machines|We present a new learning algorithm for pattern recognition inspired by a recent upper bound on leave--one--out error [ Jaakkola and Haussler, 1999 ] proved for Support Vector Machines (SVMs) [ Vapnik, 1995; 1998 ] . The new approach directly minimizes the expression given by the bound in an attempt to minimize leave--one--out error. This gives a convex optimization problem which constructs a sparse linear classifier in feature space using the kernel technique. As such the algorithm possesses many of the same properties as SVMs. The main novelty of the algorithm is that apart from the choice of kernel, it is parameterless -- the selection of the number of training errors is inherent in the algorithm and not chosen by an extra free parameter as in SVMs. First experiments using the method on benchmark datasets from the UCI repository show results similar to SVMs which have been tuned to have the best choice of parameter.  1 Introduction  Support Vector Machines (SVMs), motivated by minim...
224|Sparse Principal Component Analysis|Principal component analysis (PCA) is widely used in data processing and dimensionality  reduction. However, PCA su#ers from the fact that each principal component is a linear combination  of all the original variables, thus it is often di#cult to interpret the results. We introduce  a new method called sparse principal component analysis (SPCA) using the lasso (elastic net)  to produce modified principal components with sparse loadings. We show that PCA can be  formulated as a regression-type optimization problem, then sparse loadings are obtained by imposing  the lasso (elastic net) constraint on the regression coe#cients. E#cient algorithms are  proposed to realize SPCA for both regular multivariate data and gene expression arrays. We  also give a new formula to compute the total variance of modified principal components. As  illustrations, SPCA is applied to real and simulated data, and the results are encouraging.
225|Boosting with early stopping: convergence and consistency|Abstract Boosting is one of the most significant advances in machine learning for classification and regression. In its original and computationally flexible version, boosting seeks to minimize empirically a loss function in a greedy fashion. The resulted estimator takes an additive function form and is built iteratively by applying a base estimator (or learner) to updated samples depending on the previous iterations. An unusual regularization technique, early stopping, is employed based on CV or a test set. This paper studies numerical convergence, consistency, and statistical rates of convergence of boosting with early stopping, when it is carried out over the linear span of a family of basis functions. For general loss functions, we prove the convergence of boosting&#039;s greedy optimization to the infinimum of the loss function over the linear span. Using the numerical convergence result, we find early stopping strategies under which boosting is shown to be consistent based on iid samples, and we obtain bounds on the rates of convergence for boosting estimators. Simulation studies are also presented to illustrate the relevance of our theoretical results for providing insights to practical aspects of boosting. As a side product, these results also reveal the importance of restricting the greedy search step sizes, as known in practice through the works of Friedman and others. Moreover, our results lead to a rigorous proof that for a linearly separable problem, AdaBoost with ffl! 0 stepsize becomes an L1-margin maximizer when left to run to convergence. 1 Introduction In this paper we consider boosting algorithms for classification and regression. These algorithms present one of the major progresses in machine learning. In their original version, the computational aspect is explicitly specified as part of the estimator/algorithm. That is, the empirical minimization of an appropriate loss function is carried out in a greedy fashion, which means that at each step, a basis function that leads to the largest reduction of empirical risk is added into the estimator. This specification distinguishes boosting from other statistical procedures which are defined by an empirical minimization of a loss function without the numerical optimization details.
226|Empty alternation|Abstract. We introduce the notion of empty alternation by investigating alternating automata which are restricted to empty their storage except for a logarithmically space-bounded tape before making an alternating transition. In particular, we consider the cases when the depth of alternation is bounded by a constant or a polylogarithmic function. In this way we get new characterizations of the classes AC k, SAC k and P using a push-down store and new characterizations of the class T P 2 using Turing tapes. 1
227|Privacy-Preserving Data Mining|A fruitful direction for future data mining research will be the development of techniques that incorporate privacy concerns. Specifically, we address the following question. Since the primary task in data mining is the development of models about aggregated data, can we develop accurate models without access to precise information in individual data records? We consider the concrete case of building a decision-tree classifier from tredning data in which the values of individual records have been perturbed. The resulting data records look very different from the original records and the distribution of data values is also very different from the original distribution. While it is not possible to accurately estimate original values in individual data records, we propose a-novel reconstruction procedure to accurately estimate the distribution of original data values. By using these reconstructed distributions, we are able to build classifiers whose accuracy is comparable to the accuracy of classifiers built with the original data. 
228|Mathematical Methods of Statistics|Optimised flows and bottlenecks in granular fabric
229|Security-control methods for statistical databases: a comparative study|This paper considers the problem of providing security to statistical databases against disclosure of confidential information. Security-control methods suggested in the literature are classified into four general approaches: conceptual, query restriction, data perturbation, and output perturbation. Criteria for evaluating the performance of the various security-control methods are identified. Security-control methods that are based on each of the four approaches are discussed, together with their performance with respect to the identified evaluation criteria. A detailed comparative analysis of the most promising methods for protecting dynamic-online statistical databases is also presented. To date no single security-control method prevents both exact and partial disclosures. There are, however, a few perturbation-based methods that prevent exact disclosure and enable the database administrator to exercise “statistical disclosure control. ” Some of these methods, however introduce bias into query responses or suffer from the O/l query-set-size problem (i.e., partial disclosure is possible in case of null query set or a query set of size 1). We recommend directing future research efforts toward developing new methods that prevent exact disclosure and provide statistical-disclosure control, while at the same time do not suffer from the bias problem and the O/l query-set-size problem. Furthermore, efforts directed toward developing a bias-correction mechanism and solving the general problem of small query-set-size would help salvage a few of the current perturbation-based methods.
230|SPRINT: A scalable parallel classifier for data mining|Classification is an important data mining problem. Although classification is a well-studied problem, most of the current classi-fication algorithms require that all or a por-tion of the the entire dataset remain perma-nently in memory. This limits their suitability for mining over large databases. We present a new decision-tree-based classification algo-rithm, called SPRINT that removes all of the memory restrictions, and is fast and scalable. The algorithm has also been designed to be easily parallelized, allowing many processors to work together to build a single consistent model. This parallelization, also presented here, exhibits excellent scalability as well. The combination of these characteristics makes the proposed algorithm an ideal tool for data min-ing. 1
231|SLIQ: A Fast Scalable Classifier for Data Mining|. Classification is an important problem in the emerging field of data mining. Although classification has been studied extensively in the past, most of the classification algorithms are designed only for memory-resident data, thus limiting their suitability for data mining large data sets. This paper discusses issues in building a scalable classifier and presents the design of SLIQ  1  , a new classifier. SLIQ is a decision tree classifier that can handle both numeric and categorical attributes. It uses a novel pre-sorting technique in the tree-growth phase. This sorting procedure is integrated with a breadth-first tree growing strategy to enable classification of disk-resident datasets. SLIQ also uses a new tree-pruning algorithm that is inexpensive, and results in compact and accurate trees. The combination of these techniques enables SLIQ to scale for large data sets and classify data sets irrespective of the number of classes, attributes, and examples (records), thus making it an ...
232|Beyond Concern: Understanding Net Users&#039;Attitudes About Online Privacy|Introduction  Over the past decade, numerous surveys conducted around the world have found consistently high levels of concern about privacy. The more recent studies have found that this concern is as prevalent in the online environment as it is for physical-world interactions. For example, Westin (Harris 1998) found 81% of Net users are concerned about threats to their privacy while online. While many studies have measured the magnitude of privacy concerns, it is still critical to study the concern in detail, especially for the online environment. As Hine and Eve (1998) point out: Despite this wide range of interests in privacy as a topic, we have little idea of the ways in which people in their ordinary lives conceive of privacy and their reactions to the collection and use of personal information (Hine and Eve 1998, 253) With this study, we have tried to better understand the nature of online privacy concerns; we look beyond the fact that people are concerned and attempt to 
233|Privacy-enhancing technologies for the Internet  |The increased use of the Internet for everyday activities is bringing new threats to personal privacy. This paper gives an overview of existing and potential privacyenhancing technologies for the Internet, as well as motivation and challenges for future work in this field. 
234|Secure databases: Protection against user influence|Users may be able to compromise databases by asking a series of questions and then inferring new information from the answers. The complexity of protecting a database against this technique is discussed here.
235|Security and Privacy Implications of Data Mining|Data mining enables us to discover information we do not expect to find in databases. This can be a security/privacy issue: If we make information available, are we perhaps giving out more than we bargained for? This position paper discusses possible problems and solutions, and outlines ideas for further research in this area.  1 Introduction  Database technology provides a number of advantages. Data mining is one of these; using automated tools to analyze corporate data can help find ways to increase efficiency of an organization.  Another advantage of database technology is information sharing (including sharing with other organizations). For example, publicly accessible corporate telephone books can decrease the need for telephone operators (offloading this task to the caller...) Sharing need not be completely public - making inventory information available to suppliers can help a retail operation to avoid shortages, and can lower the supplier&#039;s cost (thus allowing the retailer to n...
236|Secure statistical database with random sample queries|A new inference control, called random sample queries, is proposed for safeguarding confidential data in on-line statistical databases. The random sample queries control deals directly with the basic principle of compromise by making it impossible for a questioner to control precisely the formation of query sets. Queries for relative frequencies and averages are computed using random samples drawn from the query sets. The sampling strategy permits the release of accurate and timely statistics and can be implemented at very low cost. Analysis shows the relative error in the statistics decreases as the query set size increases; in contrast, the effort required to compromise increases with the query set size due to large absolute errors. Experiments performed on a simulated database support the analysis.
237|Privacy Critics: UI Components to Safeguard Users&#039; Privacy|Creating usable systems to protect online privacy is an inherently difficult problem. Privacy critics are semiautonomous agents that help people protect their online privacy by offering suggestions and warnings. Two sample critics are presented. KEYWORDS: privacy, World Wide Web, critics, agent architectures, CSCW, collaboration, P3P.
238|A data distortion by probability distribution|This paper introduces data distortion by probability distribution, a probability distortion that involves three steps. The first step is to identify the underlying density function of the original series and to estimate the parameters of this density function. The second step is to generate a series of data from the estimated density function. And the final step is to map and replace the generated series for the original one. Because it is replaced by the distorted data set, probability distortion guards the privacy of an individual belonging to the original data set. At the same time, the probability distorted series provides asymptotically the same statistical properties as those of the original series, since both are under the same distribution. Unlike conventional point distortion, probability distortion is difficult to compromise by repeated queries, and provides a maximum exposure for statistical analysis.
239|Privacy Interfaces for Information Management|To facilitate the sharing of information using modern communication networks, users must be able to decide on a  privacy policy---what information to conceal, what to reveal, and to whom. We describe the evolution of privacy interfaces---the user interfaces for specifying privacy policies ---in COLLABCLIO, a system for sharing web browsing histories. Our experience has shown us that privacy policies ought to be treated as first-class objects: policy objects should have an intensional representation, and privacy interfaces should support direct manipulation of these objects. We also show how these conclusions apply to a variety of domains such as file systems, email, and telephony.  Keywords  Privacy, user interfaces, direct manipulation, WWW, information retrieval, intensional/extensional set representations.   INTRODUCTION  It is commonplace that modern communication networks should support the sharing of information while protecting people&#039;s privacy. To this end networks provide mech...
240|Recovering Information from Summary Data|Data is often stored in summarized form, as a histogram of aggregates (COUNTs, SUMs, or AVeraGes) over specified ranges. We study how to estimate the original detail data from the stored summary. We formulate this task as an inverse problem,  specifying a well-defined cost function that has to be optimized under constraints. We show that our formulation includes the uniformity and independence assumptions as a special case, and that it can achieve better reconstruction results if we maximize the smoothness as opposed to the uniformity. In our experiments on real and synthetic datasets, the proposed method almost consistently outperforms its competitor, improving the root-mean-square error by up to 20 per cent for stock price data, and up to 90 per cent for smoother data sets. Finally, we show how to apply this theory to a variety of database problems that involve partial information, such as OLAP, data warehousing and histograms in query optimization.
241|An Analytic Approach to Statistical Databases|Abstract. In the commonly adopted data models (as ins entity-relationship data model 111, for example) an attribute is a mapping between an en-tity set or a relationship set and a value set. The intension of a mapping property is given im-plicitly or explicitly in the data models, but the extension can be generally represented by the set I&lt;entity,value&gt;), as in the relational model. We propose an alternative data model for statisti cal databases, in which an attribute is represen-ted by its analytic properties (the distribution function of the values of the attribute). These analytic properties are described by a set of pa-rameters,which we call the canonica2 coefficients of the attribute. The canonical coefficients can be used to solve the usual statistical queries with no access to the data. In particular, we pre sent: 1) the methods for computing and updating the canonical coefficients, 2) the use of the ca-nonical coefficients for solving the main statis-tical queries, also in distributed statistical database environments. Besides, an application of such parameters to the query decomposition in distributed database environments is discussed.
242|Activity recognition from user-annotated acceleration data|  In this work, algorithms are developed and evaluated to detect physical activities from data acquired using five small biaxial accelerometers worn simultaneously on different parts of the body. Acceleration data was collected from 20 subjects without researcher supervision or observation. Subjects were asked to perform a sequence of everyday tasks but not told specifically where or how to do them. Mean, energy, frequency-domain entropy, and correlation of acceleration data was calculated and several classifiers using these features were tested. Decision tree classifiers showed the best performance recognizing everyday activities with an overall accuracy rate of 84%. The results show that although some activities are recognized well with subject-independent training data, others appear to require subject-specific training data. The results suggest that multiple accelerometers aid in recognition because conjunctions in acceleration feature values can effectively discriminate many activities. With just two biaxial accelerometers – thigh and wrist – the recognition performance dropped only slightly. This is the first work to investigate performance of recognition algorithms with multiple, wire-free accelerometers on 20 activities using datasets annotated by the subjects themselves.  
243|Activity and location recognition using wearable sensors|Using measured acceleration and angular velocity data gathered through inexpensive, wearable sensors, this dead-reckoning method can determine a user’s location, detect transitions between preselected locations, and recognize and classify sitting, standing, and walking behaviors. Experiments demonstrate the proposed method’s effectiveness.
244|Multi-Sensor Activity Context Detection for Wearable Computing|For wearable computing applications, human activity is a central part of the user&#039;s context. In order to avoid user annoyance it should be acquired automatically using body-worn sensors. We propose to use multiple acceleration sensors that are distributed over the body, because they are lightweight, small and cheap. Furthermore activity can best be measured where it occurs. We present a hardware platform that we developed for the investigation of this issue and results as to where to place the sensors and how to extract the context information.
246|What Shall We Teach Our Pants?|If a wearable device can register what the wearer is currently doing, it can anticipate and adjust its behavior to avoid redundant interaction with the user. However, the relevance and properties of the activities that should be recognized depend on both the application and the user. This requires an adaptive recognition of the activities where the user, instead of the designer, can teach the device what he/she is doing. As a case study we connected a pair of pants with accelerometers to a laptop to interpret the raw sensor data. Using a combination of machine learning techniques such as Kohonen maps and probabilistic models, we build a system that is able to learn activities while requiring minimal user attention. This approach to context awareness is more universal since it requires no a priori knowledge about the contexts or the user. 1.
247|Unsupervised, dynamic identification of physiological and activity context in wearable computing|Context-aware computing describes the situation where a wearable / mobile computer is aware of its user’s state and surroundings and modifies its behavior based on this information. We designed, implemented and evaluated a wearable system which can determine typical user context and context transition probabilities online and without external supervision. The system relies on techniques from machine learning, statistical analysis and graph algorithms. It can be used for online classification and prediction. Our results indicate the power of our method to determine a meaningful user context model while only requiring data from a comfortable physiological sensor device. 1.
248|Multi-Sensor Context Aware Clothing|Inspired by perception in biological systems, distribution of a massive amount of simple sensing devices is gaining more support in detection applications. A focus on fusion of sensor signals instead of strong analysis algorithms, and a scheme to distribute sensors, results in new issues. Especially in wearable computing, where sensor data continuously changes, and clothing provides an ideal supporting structure for simple sensors, this approach may prove to be favourable. Experiments with a body-distributed sensor system investigate the influence of two factors that affect classification of what has been sensed: an increase in sensors enhances recognition, while adding new classes or contexts depreciates the results. Finally, a wearable computing related scenario is discussed that exploits the presence of many sensors.
249|Hierarchical recognition of intentional human gestures for sports video annotation |We present a novel technique for the recognition of complex human gestures for video annotation using accelerometers and the hidden Markov model. Our extension to the standard hidden Markov model allows us to consider gestures at different levels of abstraction through a hierarchy of hidden states. Accelerometers in the form of wrist bands are attached to humans performing intentional gestures, such as umpires in sports. Video annotation is then performed by populating the video with time stamps indicating significant events, where a particular gesture occurs. The novelty of the technique lies in the development of a probabilistic hierarchical framework for complex gesture recognition and the use of accelerometers to extract gestures and significant events for video annotation. 1.
250|Physical Activity Recognition from Acceleration Data under SemiNaturalistic Conditions|Achieving context-aware computer systems requires that computers can automatically recognize what people are doing. In this work, algorithms are developed and evaluated to detect physical activities from data acquired using five small accelerometers worn simultaneously on different parts of the body. Acceleration data was collected from twenty subjects in both laboratory and semi-naturalistic environments. For semi-naturalistic data, subjects were asked to perform a sequence of everyday tasks outside of the laboratory. Mean, energy, frequency-domain entropy, and correlation of acceleration data was calculated over 6.71 s sliding windows. Decision table, nearest neighbor, decision tree, and Naive Bayesian classifiers were tested on these features. Classification results using individual training and leave-one-subject-out validation were compared. Leave-one-subject-out validation with decision tree classifiers
251|Acquiring In Situ Training Data for Context-Aware Ubiquitous Computing Applications|Ubiquitous, context-aware computer systems may ultimately enable computer applications that naturally and usefully respond to a user&#039;s everyday activity. Although new algorithms that can automatically detect context from wearable and environmental sensor systems show promise, many of the most flexible and robust systems use probabilistic detection algorithms that require extensive libraries of training data with labeled examples. In this paper, we describe the need for such training data and some challenges we have identified when trying to collect it while testing three contextdetection  systems for ubiquitous computing and mobile applications. Author Keywords  Context-aware, ubiquitous, computing, supervised learning, experience sampling, user interface design  ACM Classification Keywords  H5.m Information interfaces and presentation (e.g. HCI): Miscellaneous.
252|The Entity-Relationship Model: Toward a Unified View of Data|A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, infor-mation retrieval, and data manipulation are discussed. The entity-relationship model can be used as a basis for unification of different views of data: t,he network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented. Key Words and Phrases: database design, logical view of data, semantics of data, data models, entity-relationship model, relational model, Data Base Task Group, network model, entity set
253|Data structure diagrams|Successful communication of ideas has been and will continue to be a limiting factor in man&#039;s endeavors to survive and to better his life. The invention of algebra, essentially a graphic technique for communicating truths with respect to classes of arithmetic statements, broke the bond that slowed the development of mathematics. Whereas &#034;12+ 13=25 &#039; &#039; and &#034;3+7 = 10 &#034; and &#034;14+(-2)  = 12&#034; are arithmetic statements, &#034;a+b=c &#039; &#039; is an algebraic statement. In particular, it is an algebraic statement controlling an entire class of arithmetic statements such as those listed. Data Structure Diagrams The Data Structure Diagram is also a graphic technique. It is based on a type of notation dealing with classes--specifically, with classes of entities and the classes of sets that relate them. For example, individual people and automobiles
254|Deformable models in medical image analysis: A survey|This article surveys deformable models, a promising and vigorously researched computer-assisted medical image analysis technique. Among model-based techniques, deformable models offer a unique and powerful approach to image analysis that combines geometry, physics, and approximation theory. They have proven to be effective in segmenting, matching, and tracking anatomic structures by exploiting (bottom-up) constraints derived from the image data together with (top-down) a priori knowledge about the location, size, and shape of these structures. Deformable models are capable of accommodating the significant variability of biological structures over time and across different individuals. Furthermore, they support highly intuitive interaction mechanisms that, when necessary, allow medical scientists and practitioners to bring their expertise to bear on the model-based image interpretation task. This article reviews the rapidly expanding body of work on the development and application of deformable models to problems of fundamental importance in medical image analysis, includingsegmentation, shape representation, matching, and motion tracking.
255|Snakes: Active contour models|A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the cap-ture region surrounding a feature. Snakes provide a unified account of a number of visual problems, in-cluding detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.
256|Geodesic Active Contours|  A novel scheme for the detection of object boundaries is presented. The technique is based on active contours evolving in time according to intrinsic geometric measures of the image. The evolving contours naturally split and merge, allowing the simultaneous detection of several objects and both interior and exterior boundaries. The proposed approach is based on the relation between active contours and the computation of geodesics or minimal distance curves. The minimal distance curve lays in a Riemannian space whose metric is defined by the image content. This geodesic approach for object segmentation allows to connect classical “snakes ” based on energy minimization and geometric active contours based on the theory of curve evolution. Previous models of geometric active contours are improved, allowing stable boundary detection when their gradients suffer from large variations, including gaps. Formal results concerning existence, uniqueness, stability, and correctness of the evolution are presented as well. The scheme was implemented using an efficient algorithm for curve evolution. Experimental results of applying the scheme to real images including objects with holes and medical data imagery demonstrate its power. The results may be extended to 3D object segmentation as well.
257|Computer Vision|Driver inattention is one of the main causes of traffic accidents. Monitoring a driver to detect inattention is a complex problem that involves physiological and behavioral elements. Different approaches have been made, and among them Computer Vision has the potential of monitoring the person behind the wheel without interfering with his driving. In this paper I have developed a system that can monitor the alertness of drivers in order to prevent people from falling asleep at the wheel. The other main aim of this algorithm is to have efficient performance on low quality webcam and without the use of infrared light which is harmful for the human eye. Motor vehicle accidents cause injury and death, and this system will help to decrease the amount of crashes due to fatigued drivers. The proposed algorithm will work in three main stages. In first stage the face of the driver is detected and tracked. In the second stage the facial features are extracted for further processing. In last stage the most crucial parameter is monitored which is eye’s status. In the last stage it is determined that whether the eyes are closed or open. On the basis of this result the warning is issued to the driver to take a break.
258|Shape modeling with front propagation: A level set approach|Abstract- Shape modeling is an important constituent of computer vision as well as computer graphics research. Shape models aid the tasks of object representation and recognition. This paper presents a new approach to shape modeling which re-tains some of the attractive features of existing methods and over-comes some of their limitations. Our techniques can be applied to model arbitrarily complex shapes, which include shapes with significant protrusions, and to situations where no a priori as-sumption about the object’s topology is made. A single instance of our model, when presented with an image having more than one object of interest, has the ability to split freely to represent each object. This method is based on the ideas developed by Osher and Sethian to model propagating solidhiquid interfaces with curva-ture-dependent speeds. The interface (front) is a closed, noninter-secting, hypersurface flowing along its gradient field with con-stant speed or a speed that depends on the curvature. It is moved by solving a “Hamilton-Jacob? ’ type equation written for a func-tion in which the interface is a particular level set. A speed term synthesizpd from the image is used to stop the interface in the vi-cinity of object boundaries. The resulting equation of motion is solved by employing entropy-satisfying upwind finite difference schemes. We present a variety of ways of computing evolving front, including narrow bands, reinitializations, and different stopping criteria. The efficacy of the scheme is demonstrated with numerical experiments on some synthesized images and some low contrast medical images. Index Terms- Shape modeling, shape recovery, interface mo-tion, level sets, hyperbolic conservation laws, Hamilton-Jacobi
259|The Use of Active Shape Models For Locating Structures in Medical Images|This paper describes a technique for building compact models of the shape  and appearance of flexible objects (such as organs) seen in 2-D images. The models  are derived from the statistics of sets of labelled images of examples of the objects. Each model

260|Realistic Modeling for Facial Animation|A major unsolved problem in computer graphics is the construction and animation of realistic human facial models. Traditionally, facial models have been built painstakingly by manual digitization and animated by ad hoc parametrically controlled facial mesh deformations or kinematic approximation of muscle actions. Fortunately, animators are now able to digitize facial geometries through the use of scanning range sensors and animate them through the dynamic simulation of facial tissues and muscles. However, these techniques require considerableuser input to construct facial models of individuals suitable for animation. In this paper, we present a methodology for automating this challenging task. Starting with a structured facial mesh, we develop algorithms that automatically construct functional models of the heads of human subjects from laser-scanned range and reflectance data. These algorithms automatically insert contractile muscles at anatomically correct positions within a dynamic skin model and root them in an estimated skull structure with a hinged jaw. They also synthesize functional eyes, eyelids, teeth, and a neck and fit them to the final model. The constructed face may be animated via muscle actuations. In this way, we create the most authentic and functional facial models of individuals available to date and demonstrate their use in facial animation.  
261|Boundary Finding with Parametrically Deformable Models|Introduction  This work describes an approach to finding objects in images based on deformable shape models. Boundary finding in two and three dimensional images is enhanced both by considering the bounding contour or surface as a whole and by using model-based shape information.  Boundary finding using only local information has often been frustrated by poor-contrast boundary regions due to occluding and occluded objects, adverse viewing conditions and noise. Imperfect image data can be augmented with the extrinsic information that a geometric shape model provides. In order to exploit model-based information to the fullest extent, it should be incorporated explicitly, specifically, and early in the analysis. In addition, the bounding curve or surface can be profitably considered as a whole, rather than as curve or surface segments, because it tends to result in a more consistent solution overall.  These models are best suited for objects whose diversity and irregularity of shape make 
262|Topologically Adaptable Snakes|This paper presents a topologically adaptable snakes model for image segmentation and object representation. The model is embedded in the framework of domain subdivision using simplicial decomposition. This framework extends the geometric and topological adaptability of snakes while retaining all of the features of traditionalsnakes, such as user interaction, and overcoming many of the limitations of traditionalsnakes. By superposing a simplicial grid over the image domain and using this grid to iteratively reparameterize the deforming snakes model, the model is able to flow into complex shapes, even shapes with significant protrusions or branches, and to dynamically change topology as necessitated by the data. Snakes can be created and can split into multiple parts or seamlessly merge into other snakes. The model can also be easily converted to and from the traditional parametric snakes model representation. We apply a 2D model to various synthetic and real images in order to segment ...
263|Finite Element Methods for Active Contour Models and Balloons for 2D and 3D Images|The use of energy-minimizing curves, known as &#034;snakes&#034; to extract features of interest in images has been introduced by Kass, Witkin and Terzopoulos [23]. A balloon model was introduced in [12] as a way to generalize and solve some of the problems encountered with the original method. We present a 3D generalization of the balloon model as a 3D deformable surface, which evolves in 3D images. It is deformed under the action of internal and external forces attracting the surface toward detected edgels by means of an attraction potential. We also show properties of energy-minimizing surfaces concerning their relationship with 3D edge points. To solve the minimization problem for a surface, two simplified approaches are shown first, defining a 3D surface as a series of 2D planar curves. Then, after comparing Finite Element Method and Finite Difference Method in the 2D problem, we solve the 3D model using the Finite Element Method yielding greater stability and faster convergence. We have a...
264|Closed-Form Solutions for Physically Based Shape Modeling and Recognition| We present a closed-form, physically based solution for recovering a 3-D solid model from collections of 3-D surface measurements. Given a sufficient number of independent mea-surements, the solution is overconstrained and unique except for rotational symmetries. We then present a physically based object-recognition method that allows simple, closed-form comparisons of recovered 3-D solid models. The performance of these methods is evaluated using both synthetic range data with various signal-to-noise ratios and using laser rangefinder data. 
265|A Dynamic Finite Element Surface Model for Segmentation and Tracking in Multidimensional Medical Images with Application to Cardiac 4D Image Analysis|This paper presents a physics-based approach to anatomical surface segmentation, reconstruction, and tracking in multidimensional medical images. The approach makes use of a dynamic &#034;balloon&#034; model---a spherical thin-plate under tension surface spline which deforms elastically to fit the image data. The fitting process is mediated by internal forces stemming from the elastic properties of the spline and external forces which are produced from the data. The forces interact in accordance with Lagrangian equations of motion that adjust the model&#039;s deformational degrees of freedom to fit the data. We employ the finite element method to represent the continuous surface in the form of weighted sums of local polynomial basis functions. We use a quintic triangular finite element whose nodal variables include positions as well as the first and second partial derivatives of the surface. We describe a system, implemented on a high performance graphics workstation, which applies the model fitting ...
266|Volumetric deformable models with parameter functions: A new approach to the 3-D motion analysis|endorsement of any of the University of Pennsylvania&#039;s products or services. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to pubs-permissions@ieee.org. By choosing to view this document, you agree to all provisions of the copyright laws protecting it.
267|An Active Contour Model For Mapping The Cortex|A new active contour model for finding and mapping the outer cortex in brain images is developed. A cross-section of the brain cortex is modeled as a ribbon, and a constant speed mapping of its spine is sought. A variational formulation, an associated force balance condition, and a numerical approach are proposed to achieve this goal. The primary difference between this formulation and that of snakes is in the specification of the external force acting on the active contour. A study of the uniqueness and fidelity of solutions is made through convexity and frequency domain analyses, and a criterion for selection of the regularization coefficient is developed. Examples demonstrating the performance of this method on simulated and real data are provided.  
268|Tracking Points on Deformable Objects Using Curvature Information|The objective of this paper is to present a significant improvement to the approach of Duncan et al. [1, 8] to analyze the deformations of curves in sequences of 2D images. This approach is based on the paradigm that high curvature points usually possess an anatomical meaning, and are therefore good landmarks to guide the matching process, especially in the absence of a reliable physical or deformable geometric model of the observed structures. As Duncan&#039;s
269|Image Registration Based on Boundary Mapping|A new two-stage approach for nonlinear brain image registration is proposed. In the first stage, an active contour algorithm is used to establish a homothetic one-to-one map between a set of region boundaries in two images to be registered. This mapping is used in the second step: a two-dimensional transformation which is based on an elastic body deformation. This method is tested by registering magnetic resonance images to atlas images. I. Introduction  Registration of both intra-subject and inter-subject brain images has been the subject of extensive study in the medical imaging literature. The various techniques that have been proposed can be classified into three major categories: polynomial transformations, similarity-based methods, and boundary-based methods. Polynomial transformations [1, 2, 3] apply a polynomial warping and determine the coefficients of the polynomial using linear regression if a sufficient number of landmark points is provided. Numerical instabilities and the ...
270|Curves and Surfaces for CAGD|This article provides a historical account of the major developments in the area of curves and surfaces as they entered the area of CAGD – Computer Aided Geometric Design – until the middle 1980s. We adopt the definition that CAGD deals with the construction and representation of free-form curves, surfaces, or volumes. 1.
271|Analysis of Left Ventricular Wall Motion Based on Volumetric Deformable Models and MRI-SPAMM|We present a new approach for the analysis of the left ventricular shape and motion that is based on the development of a new class of volumetric deformable models. We estimate the deformation and complex motion of the left ventricle (LV) in terms of a few parameters that are functions and whose values vary locally across the LV. These parameters capture the radial and longitudinal contraction, the axial twisting, and the long-axis deformation. Using Lagrangian dynamics and the finite element theory, we convert these volumetric primitives into dynamic models that deform due to forces exerted by the datapoints. We present experiments where we used magnetic tagging (MRI-SPAMM) to acquire datapoints from the LV during systole. By applying our method to MRI-SPAMM datapoints, we were able to characterize both locally and globally the 3D shape and motion of the LV in a clinically useful way. In addition, based on the model parameters we were able to extract quantitative differences between n...
272|A parametric deformable model to fit unstructured 3D data|Recovery of unstructured 3D data with deformable models has been the subject of many studies over the last ten years. In particular, in medical image understanding, deformable models are useful to get a precise representation of anatomical structures. However, general deformable models involve large linear systems to solve when dealing with high resolution 3D images. The advantage of parametric deformable models like superquadrics is their small number of parameters to describe a shape combined with a better robustness in the presence of noise or sparse data. Also, at the expense of a reasonable number of additional parameters, free form deformations provide a much closer fit and a volumetric deformation field. This article introduces such a model to fit unstructured 3D points with a parametric deformable surface based on a superquadric fit followed by a free form deformation to describe the cardiac left ventricle. We present the mathematical and algorithmic details of the method, as wel...
273|Medical computer vision, virtual reality and robotics|The automated analysis of 3D medical images can improve both diagnosis and therapy significantly. This automation raises a number of new fascinating research problems in the fields of computer vision, graphics and robotics. In this paper, I propose a list of such problems after a review of the current major 3D imaging modalities, and a description of the related medical needs. I then present some of the past and current work done in our research group EPIDAURE * at INRIA, on the following topics: segmentation of 3D images; 3D shape modelling; 3D rigid and nonrigid registration; 3D motion analysis; and 3D simulation of therapy. Most topics are discussed in a synthetic manner, and illustrated by results. Rigid matching is treated more thoroughly as an illustration of a transfer from computer vision towards 3D image processing. The later topics are illustrated by preliminary results, and a number of promising research tracks are suggested.
274|Model-Based Interpretation of 3D Medical Images|The automatic segmentation and labelling of anatomical structures in 3D medical  imagesis a challenging task of practical importance. We describe amodel- based approach  which allows robust and accurate interpretation using explicit anatomical  knowledge. Our method is based on the extension to 3D of Point Distribution Models  (PDMs) and associated image search algorithms. Acombination of global, GeneticAlgorithm  (GA), and local, Active Shape Model (ASM), search is used. We have  built a 3D PDM of the human brain describing a number of major structures. Using  this model we have obtained automatic interpretations for 30 3D Magnetic Resonance  head images from different individuals. The results have been evaluated  quantitativelyand support our claim of robust and accurate interpretation.
275|Fast Segmentation, Tracking, and Analysis of Deformable Objects|We present a physically-based deformable model which can be used to track and to analyze non-rigid motion of dynamic structures in time sequences of 2D or 3D medical images. The model considers an object undergoing an elastic deformation as a set of masses linked by springs, where the natural lengths of the springs is set equal to zero, and is replaced by a set of constant equilibrium forces, which characterize the shape of the elastic structure in the absence of external forces. This model has the extremely nice property of yielding dynamic equations which are linear and decoupled for each coordinate, whatever the amplitude of the deformation. It provides a reduced algorithmic complexity, and a sound framework for modal analysis, which allows a compact representation of a general deformation by a reduced number of parameters. The power of the approach to segment, track and analyze 2-D and 3-D images is demonstrated by a set of experimental results on various complex medical images.  1...
276|Brownian Strings: Segmenting Images with Stochastically Deformable Models|Abstract—This paper describes an image segmentation technique in which an arbitrarily shaped contour was deformed stochastically until it fitted around an object of interest. The evolution of the contour was controlled by a simulated annealing process which caused the contour to settle into the global minimum of an image-derived “energy ” function. The nonparametric energy function was derived from the statistical properties of previously segmented images, thereby incorporating prior experience. Since the method was based on a state space search for the contour with the best global properties, it was stable in the presence of image errors which confound segmentation techniques based on local criteria, such as connectivity. Unlike “snakes ” and other active contour approaches, the new method could handle arbitrarily irregular contours in which each interpixel crack represented an independent degree of freedom. Furthermore, since the contour evolved toward the global minimum of the energy, the method was more suitable for fully automatic applications than the snake algorithm, which frequently has to be reinitialized when the contour becomes trapped in local energy minima. High computational complexity was avoided by efficiently introducing a random local perturbation in a time independent of contour length, providing control over the size of the perturbation, and assuring that resulting shape changes were unbiased. The method was illustrated by using it to find the brain surface in magnetic resonance head images and to track blood vessels in angiograms. Additional information is available from
277|Three--dimensional medical imaging: Algorithms and computer systems|This paper presents an introduction to the field of three-dimensional medical imaging It presents medical imaging terms and concepts, summarizes the basic operations performed in three-dimensional medical imaging, and describes sample algorithms for accomplishing these operations. The paper contains a synopsis of the architectures and algorithms used in eight machines to render three-dimensional medical images, with particular emphasis paid to their distinctive contributions. It compares the performance of the machines along several dimensions, including image resolution, elapsed time to form an image, imaging algorithms used in the machine, and the degree of parallehsm used in the architecture. The paper concludes with general trends for future developments in this field and references on three-dimensional medical imaging.
278|Automatic Retrieval of Anatomical Structures in 3D Medical Images|This paper describes a method to automatically generate the mapping between a completely labeled reference image and the 3D medical image of a patient. Toachieve this, we combined three techniques: the extraction of 3D feature lines, their matching using 3D deformable line models, the extension of the deformation to the whole image space using warping techniques. We present experimental results for the segmentation of structures in Magnetic Resonance images of the brain of different patients; the segmentation of the cortical and ventricle structures. We emphasize the advantages of using crest lines deformable models prior to surface based models. This gives a sparser representation of the data, easier to manipulate, and which makes the convergence of the model much less sensitive to initial positionning. In the future, we hope to use this method to generate anatomical atlases, by the automatic interpretation of large sets of 3D medical images.
279|Superquadrics and free-form deformations : a global model to fit and track 3D medical data|Recovery of 3-D data with simple parametric models has been the subject of many studies over the last ten years. Many have used the notion of superquadrics, introduced for graphics in [4]. It appears, however, that although superquadrics can describe a wide variety of forms, they are too simple to recover and describe complex shapes. This paper describes a method to øt to 3-D points and then track a parametric deformable surface. We suppose that a 3-D image has been segmented to get a set of 3-D points. A ørst estimate consists of our version of a superquadric fit with global tapering. We then apply the technique of free-form deformations, as introduced by [9] in computer graphics to refine the estimate. We present experimental results with real 3-D medical images, where the original points are laid on an iso-surface. This is also applied to give efficient tracking of the deformation of the myocardium
280|Integration Of Boundary Finding And Regionbased Segmentation Using Game Theory|. Robust segmentation of structures from an image is essential for a variety of applications in biomedical image analysis. Here we propose a method that integrates region based segmentation and gradient based boundary finding using game theory in an effort to form a unified approach that is robust to noise and poor initialization. The novelty of the method is that this is a bi-directional framework whereby the two seperate modules improve their results through mutual information sharing.  Keywords: game theory, boundary finding, region based segmentation, Maximum A posteriori probability  1. Introduction  Precise segmentation of underlying objects in an image is very important especially for biomedical image analysis where it constitutes an important pre-processing step to such tasks as the registration of images obtained from two modalities, quantitative analysis of anatomical structures, the derivation of priors for image reconstruction in another modality and cardiac motion tracking...
281|Analyzing the Deformation of the Left Ventricle of the Heart with a Parametric Deformable Model|We present a new approach to analyze the deformation of the left ventricle of the heart, based on a parametric model that gives a compact representation of a set of points in a 3D image. We present four different approaches to tracking surfaces in a sequence of 3D cardiac images. Following tracking, we then infer quantitative parameters which are useful for the physician, suchas  the variation of volume and wall thickness during a cardiac cycle, the ejection fraction or the twist component in the deformation of the ventricle. We explicit the computation of these parameters using our model. Experimental results are shown in time sequences of two kinds of medical images, Nuclear Medicine and X-Ray Computed Tomography (CT).
282|Reconstruction of 3D medical images: A nonlinear interpolation technique for reconstruction of 3D medical images|Three-dimensional medical images reconstructed from a series of two-dimensional images produced by computerized tomography, magnetic resonance imaging, etc., present a valuable tool for modem medicine. Usually, the inter-resolution between two cross sections is less than the intraresolution within each cross section. Therefore, interpolations are required to create a 3D visualization. Many techniques, including voxel-based and patch tiling methods, apply linear interpolations between two cross sections. Although those techniques using linear interpolations are economical in computation, they need much cross-sectional data and are unable to enlarge because of aliasmg. Hence, the techniques that apply two-dimensional nonlinear interpolation functions among cross sections were proposed. In this paper, we introduce the curvature sampling of the contour of a medical object in a CT (computerized tomography) image. Those sampled contour points are the candidates for the control points of Hermite surfaces between each pair of cross sections. Then, a nearest-neighbor mapping of control points between every two cross sections is used for surface formation. The time complexity of our mapping algorithm is O(m + n), where m and II are the numbers of control points of two cross sections. It is much faster than Kehtamavaz and De Figueiredo’s merge method, whose time complexity is O(n’m~). 0 1991 Academic Press, Inc. 1.
283|Conditional random fields: Probabilistic models for segmenting and labeling sequence data|We present conditional random fields, a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data. 1.
285|Gradient-based learning applied to document recognition|Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradientbased learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN’s), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day.
286|A Maximum Entropy approach to Natural Language Processing|The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.  
288|Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging|this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part of speech tagging
289|Inducing Features of Random Fields|We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classifica...
290|A Maximum Entropy Model for Part-Of-Speech Tagging|This paper presents a statistical model which trains from a corpus annotated with Part-OfSpeech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%). The model can be classified as a Maximum Entropy model and simultaneously uses many contextual &#034;features&#034; to predict the POS tag. Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.
291|Maximum entropy markov models for information extraction and segmentation|Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ’s.  
292|Finite-State Transducers in Language and Speech Processing|Finite-state machines have been used in various domains of natural language processing. We consider here the use of a type of transducers that supports very efficient programs: sequential transducers. We recall classical theorems and give new ones characterizing sequential string-tostring transducers. Transducers that output weights also play an important role in language and speech processing. We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms. Some applications of these algorithms in speech recognition are described and illustrated. 1.
293|Discriminative Reranking for Natural Language Parsing|This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model’s score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.  
294|Learning to resolve natural language ambiguities: A unified approach|distinct semanticonceptsuch as interest rate and has interest in Math are conflated in ordinary text. We analyze a few of the commonly used statistics based The surrounding context- word associations and syn-and machine learning algorithms for natural language tactic patterns in this case- are sufflcicnt to identify disambiguation tasks and observe tha they can bc recast as learning linear separators in the feature space. the correct form. Each of the methods makes a priori assumptions, which Many of these arc important stand-alone problems it employs, given the data, when searching for its hy- but even more important is thei role in many applicapothesis. Nevertheless, as we show, it searches a space tions including speech recognition, machine translation, that is as rich as the space of all linear separators. information extraction and intelligent human-machine We use this to build an argument for a data driven interaction. Most of the ambiguity resolution problems approach which merely searches for a good linear sepa- are at the lower level of the natural language inferences rator in the feature space, without further assumptions chain; a wide range and a large number of ambigui-
295|Information Extraction with HMM Structures Learned by Stochastic Optimization|Recent research has demonstrated the strong performance of hidden Markov models applied to information extraction -- the task of populating database slots with corresponding phrases from text documents. A remaining problem, however, is the selection of state-transition structure for the model. This paper demonstrates that extraction accuracy strongly depends on the selection of structure, and presents an algorithm for automatically finding good structures by stochastic optimization. Our algorithm begins with a simple model and then performs hill-climbing in the space of possible structures by splitting states and gauging performance on a validation set. Experimental results show that this technique finds HMM models that almost always out-perform a fixed model, and have superior average performance across tasks.
296|Boosting Applied to Tagging and PP Attachment|Boosting is a machine learning algorithm that is not well known in computational linguistics. We apply it to part-of-speech tagging and prepositional phrase attachment. Performance is very encouraging. We also show how to improve data quality by using boosting to identify annotation errors.
297|Minimization Algorithms for Sequential Transducers|We present general algorithms for minimizing sequential finite-state transducers that output strings or numbers. The algorithms are shown to be efficient since in the case of acyclic transducers and for output strings they operate in O(S+|E|+|V|+(|E|-|V|+|F|)x(|Pmax|+1)) steps, where S is the sum of the lengths of all output labels of the resulting transducer, E the set of transitions of the given transducer, V the set of its states, F the set of final states, and Pmax one of the longest of the longest common prefixes of the output paths leaving each state of the transducer. The algorithms apply to a larger class of transducers which includes subsequential transducers.
298|Boltzmann Chains and Hidden Markov Models|We propose a statistical mechanical framework for the modeling of discrete time series. Maximum likelihood estimation is done via Boltzmann learning in one-dimensional networks with tied weights. We call these networks Boltzmann chains and show that they contain hidden Markov models (HMMs) as a special case. Our framework also motivates new architectures that address particular shortcomings of HMMs. We look at two such architectures: parallel chains that model feature sets with disparate time scales, and looped networks that model long-term dependencies between hidden states. For these networks, we show how to implement the Boltzmann learning rule exactly, in polynomial time, without resort to simulated or mean-field annealing. The necessary computations are done by exact decimation procedures from statistical mechanics. 1 INTRODUCTION AND SUMMARY  Statistical models of discrete time series have a wide range of applications, most notably to problems in speech recognition (Juang &amp; Rabin...
299|A Whole Sentence Maximum Entropy Language Model|We introduce a new kind of language model, which models whole sentences or utterances directly using the Maximum Entropy paradigm. The new model is conceptually simpler, and more naturally suited to modeling whole-sentence phenomena, than the conditional ME models proposed to date. By avoiding the chain rule, the model treats each sentence or utterance as a &#034;bag of features&#034;, where features are arbitrary computable properties of the sentence. The model is unnormalizable, but this does not interfere with training (done via sampling) or with use. Using the model is computationally straightforward. The main computational cost of training the model is in generating sample sentences from a Gibbs distribution. Interestingly, this cost has different dependencies, and is potentially lower, than in the comparable conditional ME model.  1 Motivation  Conventional statistical language models estimate the probability of an sentence s by using the chain rule to decompose it into a product of condit...
300|Equivalence of Linear Boltzmann Chains and Hidden Markov Models|Several authors have studied the relationship between hidden Markov models and `Boltzmann chains&#039; with a linear or `time-sliced&#039; architecture. Boltzmann chains model sequences of states by defining statestate transition energies instead of probabilities. In this note I demonstrate that, under the simple condition that the state sequence has a mandatory end state, the probability distribution assigned by a strictly linear Boltzmann chain is identical to that assigned by a hidden Markov model.  Several authors have made a link between hidden Markov models for time series and energy-based models (Luttrell 1989, Williams 1990, Saul and Jordan 1995). Saul and Jordan (1995) discuss a linear Boltzmann chain model with state-state transition energies A ii  0  (going from state i to state i  0  ) and symbol emission energies B ij , under which the probability of an entire state fi l ; j l g  L  1 given the length of the sequence L, is: P (fi l ; j l g  L  1 j\Pi; A; B; L; HBC ) = 1  Z(\Pi; A; B...
301|Initial Conditions and Moment Restrictions in Dynamic Panel Data Models|Estimation of the dynamic error components model is considered using two alternative linear estimators that are designed to improve the properties of the standard firstdifferenced GMM estimator. Both estimators require restrictions on the initial conditions process. Asymptotic efficiency comparisons and Monte Carlo simulations for the simple AR(1) model demonstrate the dramatic improvement in performance of the proposed estimators compared to the usual first-differenced GMM estimator, and compared to non-linear GMM. The importance of these results is illustrated in an application to the estimation of a labour demand model using company panel data.
302|Bootstrap Methods in Econometrics: Theory and Numerical Performance|1.
303|On active contour models and balloons |The use.of energy-minimizing curves, known as “snakes, ” to extract features of interest in images has been introduced by Kass, Witkhr &amp; Terzopoulos (Znt. J. Comput. Vision 1, 1987,321-331). We present a model of deformation which solves some of the problems encountered with the original method. The external forces that push the curve to the edges are modified to give more stable results. The original snake, when it is not close enough to contours, is not attracted by them and straightens to a line. Our model makes the curve behave like a balloon which is inflated by an additional force. The initial curve need no longer be close to the solution to converge. The curve passes over weak edges and is stopped only if the edge is strong. We give examples of extracting a ventricle in medical images. We have also made a first step toward 3D object reconstruction, by tracking the extracted contour on a series of successive cross sections. 0 1991 Academic press, 1~. I.
304|A computational approach to edge detection|Abstract-This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to- a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge. This detection scheme uses several elongated operators at each point, and the directional operator outputs are integrated with the gradient maximum detector. Index Terms-Edge detection, feature extraction, image processing, machine vision, multiscale image analysis. I.
306|Using Canny’s criteria to derive a recursively implemented optimal edge detector|A highly efficient recursive algorithm for edge detection is presented. Using Canny&#039;s design [1], we show that a solution to his precise formulation of detection and localization for an infinite extent filter leads to an optimal operator in one dimension, which can be efficiently implemented by two recursive filters moving in opposite directions. In addition to the noise truncature immunity which results, the recursive nature of the filtering operations leads, with sequential machines, to a substantial saving in computational effort (five multiplications and five additions for one pixel, independent of the size of the neighborhood). The extension to the two-dimensional case is considered and the resulting filtering structures are im-plemented as two-dimensional recursive filters. Hence, the filter size can be varied by simply changing the value of one parameter without affecting the time execution of the algorithm. Performance measures of this new edge detector are given and compared to Canny&#039;s filters. Various experimental results are shown.
307|Model Driven Edge Detection|Standard edge detectors fail to nd most relevant edges, nding either too many or too few, because they lack a geometric model to guide their search. We present a technique that integrates both photometric and geometric models with an initial estimate of the boundary. The strength of this approach lies in the ability of the geometric model to overcome various photometric anomalies, thereby nding boundaries that could not otherwise be found. Furthermore, edges can be scored based on their goodness of t to the model, thus allowing one to use semantic model information to accept or reject the edges. 1
308|Inductive-Data-Type Systems|In a previous work (&#034;Abstract Data Type Systems&#034;, TCS 173(2), 1997), the leI two authors presented a combined lmbined made of a (strongl normal3zG9 alrmal rewrite system and a typed  #-calA#Ik  enriched by pattern-matching definitions folnitio a certain format,calat the &#034;General Schema&#034;, whichgeneral39I theusual recursor definitions fornatural numbers and simil9 &#034;basic inductive types&#034;. This combined lmbined was shown to bestrongl normalIk39f The purpose of this paper is toreformul33 and extend theGeneral Schema in order to make it easil extensibl3 to capture a more general cler of inductive types, cals, &#034;strictly positive&#034;, and to ease the strong normalgAg9Ik proof of theresulGGg system. Thisresul provides a computation model for the combination of anal&#034;DAfGI specification language based on abstract data types and of astrongl typed functional language with strictly positive inductive types.  
310|ORDERINGS FOR TERM-REWRITING SYSTEMS|  Methods of proving that a term-rewriting system terminates are presented. They are based on the intuitive notion of &#039;simplification orderings&#039;. orderings in which any term that is syntactically simpler than another is smaller than the other. M a consequence of Kruskal&#039;s Tree Theorem, any nonterminating system must be self-embedding in the sense that it allows for the derivation of some term from a simpler one; thus termination is guaranteed jf every rule in the system as a reduction in some simplification ordering. Most 01 the orderings that have been used for proving tennination are indeed simplication orderings; using this notion often allows for much easier proofs. A particularly useful class of simplification orderings, the &#039;recursive path orderings&#039;, is defined. Examples of the use of simplification orderings in termination proofs are given.
311|PROVING TERMINATION WITH MULTISET ORDERINGS  |A common tool for proving the termination of programs is the well-founded set, a set ordered in such a way as to admit no infinite descending sequences. The basic approach is to find a termination function that maps the values of the program variables into some well-founded set, such that the value of the termination function is continually reduced throughout the computation. All too often, the termination functions required are difficult to find and are of a complexity out of proportion to the program under consideration. However, by providing more sophisticated well-founded sets, the corresponding termination functions can be simplified. Given a well-founded set S, we consider multisets over S, &#034;sets&#034; that admit multiple occurrences of elements taken from S. We define an ordering on all finite multisets over S that is induced by the given ordering on S. This multiset ordering is shown to be well-founded. The value of the multiset ordering is that it permits the use of relatively simple and intuitive termination functions in otherwise difficult termination proofs. In particular, we apply the multiset ordering to prove the termination of production systems, programs defined in terms of sets of rewriting rules.
312|Natural termination |Abstract. We generalize the various path orderings and the conditions under which they work, and describe an implementation of this general ordering. We look at methods for proving termination of orthogonal systems and give a new solution to a problem of Zantema&#039;s. 1
313|1987], `Termination of rewriting systems by polynomial interpretations and its implementation&#039;, Science of Computer Programming |Abstract. This paper describes the actual implementation in the rewrite rule laboratory REVE of an elementary procedure that checks inequalities between polynomials and is used for proving termination of rewriting systems, especially in the more difficult case of associative-commutative rewriting systems, for which a complete characterization is given. 1. The origin of the problem Termination is central in programming and in particular in term-rewriting systems, the latter being both a theoretical and a practical basis for functional and logic languages. Indeed the problem is not only a key for ensuring that a program and its procedures eventually produce the expected result, it is also important in concur rent programming where liveness results rely on termination of the components. Term-rewriting systems are also used for proving equational theorems and are a basic tool for checking specifications of abstract data types. Again, the termination problem is crucial in the implementation of the Kn~h-Bendix algorithm, which tests the local confluence and needs the termination to be able to infer the total confluence. Termination is also necessary to direct equations properly. Until now, methods based on recursive path ordering were satisfactory [8,19], but when we recently ran experiments on transformation of FP programs [2], we were faced with a problem that the recursive path ordering could not handle. The problem, motivated by a simple example of code optimization, is just Associativity + Endomorphism: (XI * x2)  * X3 = XI * (x2 * x 3), f(x I * x2)  = f(xJ * f(x 2). The variables are functions,  * is the composition and f is a mapcar-like operator. In order to optimize the program, the user wants to decrease the number of uses * This work was supported by the Greco de Programmation.
314|Completion for rewriting modulo a congruence|Abstract. We present completion methods for rewriting modulo a congruence, generalizing previous methods by Peterson and Stickel (1981) and Jouannaud and Kirchner (1986). We formalize our methods as equational inference systems and describe techniques for reasoning about such systems. 1.
315|Solving Goals in Equational Languages|Solving equations in equational Horn-clause theories is a programming paradigm that combines logic programming and functional programming in a clean manner. Languages like EQLOG, SLOG and RITE, express programs as conditional rewrite rules and goals as equations to be solved. Procedures for completion of conditional equational theories, in a manner akin to that of Knuth and Bendix for unconditional theories, also require methods for solving equations appearing in conditions. Rewrite-based logic-programming uses (conditional) narrowing to solve equational goals. Recently a different, topdown equation solving procedure was proposed for unconditional rewrite systems. In this paper, we express equational goal solving using conditional rules. Some refinements are described: the notion of operator derivability is used to prune useless paths in the search tree and our use of oriented goals eliminates some redundant paths leading to non-normalized solutions. Our goal-directed method can also be extended to handle conditional systems. I. Equational Programming Several proposed programming languages use conditional equations as a means of combining the main features of logic programming and functional programming; such languages include RITE [Dershowitz-Plaisted-85], SLOG [Fribourg-85], and EQLOG [Goguen-Meseguer-86]. In this paradigm, a program is a set of rules, that is, directed (conditional) equations, and a goal is the question whether an equation s = t has a solution in the equational theory presented by the program. Computing consists of finding values (substitutions) for the variables in s and t for which the equality holds. Efficient methods of solving equations are therefore very important. So, too, is the ability to detect that equations are unsatisfiable.
316|Canonical Conditional Rewrite Systems |Conditional equations have been studied for their use in the specification of abstract data types and as a computational paradigm that combines logic and function programming in a clean way. In this paper we examine different formulations of conditional equations as rewrite systems, compare their expressive power and give sufficient conditions for rewrite systems to have the &#034;confluence &#034; property. We then examine a restriction of these systems using a &#034;decreasing&#034; ordering. With this restriction, most of the basic notions (like rewriting and computing normal forms) are decidable, the &#034;critical pair&#034; lemma holds, and some formulations preserve canonicity. 
317|A data locality optimizing algorithm|1 Introduction As processor speed continues to increase faster than me-mory speed, optimizations to use the memory hierarchy efficiently become ever more important. Blocking [9] ortiling [18] is a well-known technique that improves the data locality of numerical algorithms [1, 6, 7, 12, 13].Tiling can be used for different levels of memory hierarchy such as physical memory, caches and registers; multi-leveltiling can be used to achieve locality in multiple levels of the memory hierarchy simultaneously.To illustrate the importance of tiling, consider the example of matrix multiplication: for I1: = 1 to nfor
318|The Cache Performance and Optimizations of Blocked Algorithms|Blocking is a well-known optimization technique for improving the effectiveness of memory hierarchies. Instead of operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused. This paper presents cache performance data for blocked programs and evaluates several optimizations to improve this performance. The data is obtained by a theoretical model of data conflicts in the cache, which has been validated by large amounts of simulation. We show that the degree of cache interference is highly sensitive to the stride of data accesses and the size of the blocks, and can cause wide variations in machine performance for different matrix sizes. The conventional wisdom of trying to use the entire cache, or even a fixed fraction of the cache, is incorrect. If a fixed block size is used for a given cache size, the block size that minimizes the expected number of cache misses is very small. Tailoring the block size according to the matrix size and cache parameters can improve the average performance and reduce the variance in performance for different matrix sizes. Finally, whenever possible, it is beneficial to copy non-contiguous reused data into consecutive locations. 1
319|On understanding types, data abstraction, and polymorphism|Our objective is to understand the notion of type in programming languages, present a model of typed, polymorphic programming languages that reflects recent research in type theory, and examine the relevance of recent research to the design of practical programming languages. Object-oriented languages provide both a framework and a motivation for exploring the interaction among the concepts of type, data abstraction, and polymorphism, since they extend the notion of type to data abstraction and since type inheritance is an important form of polymorphism. We develop a ?-calculus-based model for type systems that allows us to explore these interactions in a simple setting, unencumbered by complexities of production programming languages. The evolution of languages from untyped universes to monomorphic and then polymorphic type systems is reviewed. Mechanisms for polymorphism such as overloading, coercion, subtyping, and parameterization are examined. A unifying framework for polymorphic type systems is developed in terms of the typed ?-calculus augmented to include binding of types by quantification as well as binding of values by abstraction. The typed ?-calculus is augmented by universal quantification to model generic functions with type parameters, existential quantification and packaging (information hiding) to model abstract data types, and
320|Principal type-schemes for functional programs|the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of its publication and date appear, and notice is given
321|Galileo: a strongly typed, interactive conceptual language|Galileo, a programming language for database applications, is presented. Galileo is a strongly typed, interactive programming language designed specifically to support Semantic Data Model features (classification, aggregation and specialization) as well as abstraction mechanisms of modern programming languages (types, abstract types and modularization). The main contributions of Galileo are: a) the proposal of a flexible type system to model database structure and semantic integrity constraints; b) the inclusion of type hierarchies to support the specialization abstraction mechanism of Semantic Data Models. c) the proposal of a modularization mechanism to structure data and operations into interrelated units; d) the integration of the abstraction mechanisms into an expression based language that allows an interactive use of the database without resorting to a new stand alone query language. Galileo will be used in the immediate future as a tool for database design and, in the long term, as a high level interface for DBMSs. 
322|Hope: An Experimental Applicative Language|An applicative language called HOPE is described and discussed. The underlying goal of the design and implementation effort was to produce a very simple programming language which encourages the construction of clear and manipulable programs. HOPE does not include an assignment statement; this is felt to be an important simplification. The user may freely define his own data types, without the need to devise a complicated encoding in terms of low-level types. The language is very strongly typed, and as implemented it incorporates a typechecker which handles polymorphic types and overloaded operators. Functions are defined by a set of recursion equations; the left-hand side of each equation includes a pattern used to determine which equation to use for a given argument. The availability of arbitrary higher-order types allows functions to be defined which &#039;package&#039; recursion. Lazily-evaluated lists are provided, allowing the use of infinite lists which could be used to provide interactive input/output and concurrency.
323|Privacy Preserving Data Mining|In this paper we address the issue of privacy preserving data mining. Specifically, we consider a  scenario in which two parties owning confidential databases wish to run a data mining algorithm on  the union of their databases, without revealing any unnecessary information. Our work is motivated  by the need to both protect privileged information and enable its use for research or other purposes. The
324|Induction of Decision Trees| The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.  
325|Public-key cryptosystems based on composite degree residuosity classes| This paper investigates a novel computational problem, namely the Composite Residuosity Class Problem, and its applications to public-key cryptography. We propose a new trapdoor mechanism and derive from this technique three encryption schemes: a trapdoor permutation and two homomorphic probabilistic encryption schemes computationally comparable to RSA. Our cryptosystems, based on usual modular arithmetics, are provably secure under appropriate assumptions in the standard model.  
326|A randomized protocol for signing contracts|Two parties, A and B, want to sign a contract C over a communication network. To do so, they must “simultaneously” exchange their commitments to C. Since simultaneous exchange is usually impossible in practice, protocols are needed to approximate simultaneity by exchanging partial commitments in piece by piece manner. During such a protocol, one party or another may have a slight advantage; a “fair” protocol keeps this advantage within acceptable limits. We present a new protocol that is fair in the sense that, at any stage in its execution, the conditional probability that one party cannot commit both parties to the contract given that the other party can, is close to zero. This is true even if A and B have vastly different computing powers, and is proved under very weak cryptographic assumptions. Our protocol has the following additional properties: 4 during the procedure the parties exchange probadilistic options for committing both parties to the contract; the protocol never terminates in an asymmetric situation where party A knows that party B is committed to the contract while he is not; the protocol makes use of a weak form of a third party (judge). If both A and B are honest, the judge will never be called upon. Otherwise, the judge rules by performing a simple computation. No bookkeeping is required of the judge. 
327|Multiparty unconditionally secure protocols|Under the assumption that each pair of participants em communieatc secretly, we show that any reasonable multiparty protwol can be achieved if at least Q of the Participants am honest. The secrecy achieved is unconditional, It does not rely on any assumption about computational intractability. 1.
328|Security and Composition of Multi-party Cryptographic Protocols|We present general definitions of security for multi-party cryptographic protocols, with focus  on the task of evaluating a probabilistic function of the parties&#039; inputs. We show that, with  respect to these definitions, security is preserved under a natural composition operation.  The definitions follow the general paradigm of known definitions; yet some substantial modifications  and simplifications are introduced. The composition operation is the natural `subroutine  substitution&#039; operation, formalized by Micali and Rogaway.  We consider several standard settings for multi-party protocols, including the cases of eavesdropping,  Byzantine, non-adaptive and adaptive adversaries, as well as the information-theoretic  and the computational models. In particular, in the computational model we provide the first  definition of security of protocols that is shown to be preserved under composition.  
329|Efficient generation of shared RSA keys|We describe efficient techniques for a number of parties to jointly generate an RSA key. At the end of the protocol an RSA modulus N = pq is publicly known. None of the parties know the factorization of N. In addition a public encryption exponent is publicly known and each party holds a share of the private exponent that enables threshold decryption. Our protocols are efficient in computation and communication. All results are presented in the honest but curious settings (passive adversary).
330|Secure Multi-Party Computation|Contents  1 Introduction and Preliminaries 4 1.1 A Tentative Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.1.1 Overview of the Definitions : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.1.2 Overview of the Known Results : : : : : : : : : : : : : : : : : : : : : : : : : : 5 1.1.3 Aims and nature of the current manuscript : : : : : : : : : : : : : : : : : : : 6 1.1.4 Organization of this manuscript : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.2 Preliminaries (also tentative) : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.2.1 Computational complexity : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.2.2 Two-party and multi-party protocols : : : : : : : : : : : : : : : : : : : : : : : 10 1.2.3 Strong Proofs of Knowledge : : : : : : : : : : : : : : : : : : : : : : : : : : : : 10 2 General Two-Party Computation 13 2.1.1 The semi-honest model : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 
331|Two Party RSA Key Generation|. We present a protocol for two parties to generate an RSA  key in a distributed manner. At the end of the protocol the public key: a  modulus N = PQ, and an encryption exponent e are known to both parties.  Individually, neither party obtains information about the decryption  key d and the prime factors of N : P and Q. However, d is shared among  the parties so that threshold decryption is possible.  1 Introduction  We show how two parties can jointly generate RSA public and private keys. Following the execution of our protocol each party learns the public key: N = PQ and e, but does not know the factorization of N or the decryption exponent d. The exponent d is shared among the two players in such a way that joint decryption of cipher-texts is possible.  Generation of RSA keys in a private, distributed manner figures prominently in several cryptographic protocols. An example is threshold cryptography, see [12] for a survey. In a threshold RSA signature scheme there are k parties who ...
332|Oblivious Polynomial Evaluation|Oblivious polynomial evaluation is a protocol involving two parties, a sender whose input is a polynomial P, and a receiver whose input is a value a. At the end of the protocol the receiver learns P (a) and the sender learns nothing. We describe efficient constructions for this protocol, which are based on new intractability assumptions that are closely related to noisy polynomial reconstruction. Oblivious polynomial evaluation can be used as a primitive in many applications. We describe several such applications, including protocols for private comparison of data, for mutually authenticated key exchange based on (possibly weak) passwords, and for anonymous coupons. 1
333|Data Integration: A Theoretical Perspective|Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. The problem of designing data integration systems is important in current real world applications, and is characterized by a number of issues that are interesting from a theoretical point of view. This document presents on overview of the material to be presented in a tutorial on data integration. The tutorial is focused on some of the theoretical issues that are relevant for data integration. Special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and reasoning on queries.
334|A Survey of Approaches to Automatic Schema Matching|Schema matching is a basic problem in many database application domains, such as data integration, E-business, data warehousing, and semantic query processing. In current implementations, schema matching is typically performed manually, which has significant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match operation for specific application domains. We present a taxonomy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distinguish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classification we review some previous match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component.
335|Querying Heterogeneous Information Sources Using Source Descriptions|We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can ask). (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an imp...
336|Answering Queries Using Views: A Survey|The problem of answering queries using views is to find efficient methods of answering a query using a set of previously defined materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems. In query optimization, finding a rewriting of a query using a set of materialized views can yield a more efficient query execution plan. To support the separation of the logical and physical views of data, a storage schema can be described using views over the logical schema. As a result, finding a query execution plan that accesses the storage amounts to solving the problem of answering queries using views. Finally, the problem arises in data integration systems, where data sources can be described as precomputed views over a mediated schema. This article surveys the state of the art on the problem of answering queries using views, and synthesizes the disparate works into a coherent framework. We describe the different applications of the problem, the algorithms proposed to solve it and the relevant theoretical results.
337|Information integration using logical views|A number of ideas concerning information-integration tools can be thought of as constructing answers to queries using views that represent the capabilities of information sources. We review the formal basis of these techniques, which are closely related to containment algorithms for conjunctive queries and/or Datalog programs. Then we compare the approaches taken by AT&amp;T Labs&#039; &#034;Information Manifold&#034; and the Stanford &#034;Tsimmis&#034; project in these terms.
338|The TSIMMIS Approach to Mediation: Data Models and Languages|TSIMMIS -- The Stanford-IBM Manager of Multiple Information Sources -- is a system for integrating information. It o ers a data model and a common query language that are designed to support the combining of information from many different sources. It also o ers tools for generating automatically the components that are needed to build systems for integrating information. In this paper we shall discuss the principal architectural features and their rationale. 
339|Query Answering in Inconsistent Databases|In this chapter, we summarize the research on querying inconsistent databases we have been conducting over the last five years. The formal framework we have used is based on two concepts: repair and consistent query answer. We describe different approaches to the issue of computing consistent query answers: query transformation, logic programming, inference in annotated logics, and specialized algorithms. We also characterize the computational complexity of this problem. Finally, we discuss related research in artificial intelligence, databases, and logic programming.
340|Index Structures for Path Expressions|In recent years there has been an increased interest in managing data which does not conform to traditional data models, like the relational or object oriented model. The reasons for this non-conformance are diverse. One one hand, data may not conform to such models at the physical level: it may be stored in data exchange formats, fetched from the Internet, or stored as structured les. One the other hand, it may not conform at the logical level: data may have missing attributes, some attributes may be of di erent types in di erent data items, there may be heterogeneous collections, or the data may be simply specified by a schema which is too complex or changes too often to be described easily as a traditional schema. The term semistructured data has been used to refer to such data. The data model proposed for this kind of data consists of an edge-labeled graph, in which nodes correspond to objects and edges to attributes or values. Figure 1 illustrates a semistructured database providing information about a city. Relational databases are traditionally queried with associative queries, retrieving tuples based on the value of some attributes. To answer such queries efciently, database management systems support indexes for translating attribute values into tuple ids (e.g. B-trees or hash tables). In object-oriented databases, path queries replace the simpler associative queries. Several data structures have been proposed for answering path queries e ciently: e.g., access support relations 14] and path indexes 4]. In the case of semistructured data, queries are even more complex, because they may contain generalized path expressions 1, 7, 8, 16]. The additional exibility is needed in order to traverse data whose structure is irregular, or partially unknown to the user.
341|Complexity of answering queries using materialized views|WC study the complexity of the problem of answering queries using materinlized views, This problem has attracted a lot of attention re-cently because of its relevance in data integration. Previous work considered only conjunctive view definitions. We examine the con-sequences of allowing more expressive view definition languages. Tl~olanguagcsweconsiderforviewdefinitionsanduserqueriesare: conjunctive qucrics with inequality, positive queries, datalog, and first-order logic. We show that the complexity of the problem de-pcnds on whether views are assumed to store all the tuples that sat-isfy the view definition, or only a subset of it. Finally, we apply the results to the view consistency and view self-maintainability prob-lems which nrise in data warehousing. 1
342|Semistructured data|In semistructured data, the information that is normally as-sociated with a schema is contained within the data, which is sometimes called “self-describing”. In some forms of semi-structured data there is no separate schema, in others it exists but only places loose constraints on the data. Semi-structured data has recently emerged as an important topic of study for a variety of reasons. First, there are data sources such as the Web, which we would like to treat as databases but which cannot be constrained by a schema. Second, it may be desirable to have an extremely flexible format for data exchange between disparate databases. Third, even when dealing with structured data, it may be helpful to view it. as semistructured for the purposes of browsing. This tu-torial will cover a number of issues surrounding such data: finding a concise formulation, building a sufficiently expres-sive language for querying and transformation, and opti-mizat,ion problems. 1 The motivation The topic of semistructured data (also called unstructured data) is relatively recent, and a tutorial on the topic may well be premature. It represents, if anything, the conver-gence of a number of lines of thinking about new ways to represent and query data that do not completely fit with conventional data models. The purpose of this tutorial is to to describe this motivation and to suggest areas in which further research may be fruitful. For a similar exposition, the reader is referred to Serge Abiteboul’s recent survey pa-per PI. The slides for this tutorial will be made available from a section of the Penn database home page
343| On the Decidability of Query Containment under Constraints |Query containment under constraints is the problem of checking whether for every database satisfying a given set of constraints, the result of one query is a subset of the result of another query. Recent research points out that this is a central problem in several database applications, and we address it within a setting where constraints are specified in the form of special inclusion dependencies over complex expressions, built by using intersection and difference of relations, special forms of quantification, regular expressions over binary relations, and cardinality constraints. These types of constraints capture a great variety of data models, including the relational, the entity-relational, and the object-oriented model. We study the problem of checking whether q is contained in q ' with respect to the constraints specified in a schema S, where q and q ' are nonrecursive Datalog programs whose atoms are complex expressions. We present the following results on query containment. For the case where q does not contain regular expressions, we provide a method for deciding query containment, and analyze its computational complexity. We do the same for the case where neither S nor q, q ' contain number restrictions. To the best of our knowledge, this yields the first decidability result on containment of conjunctive queries with regular expressions. Finally, we prove that the problem is undecidable for the case where we admit inequalities in q'.  
344|Optimizing Queries with Materialized Views|While much work has addressed the problem of maintaining materialized views, the important question of optimizing queries in the presence of materialized views has not been resolved. In this paper, we analyze the optimization question and provide a comprehensive and efficient solution. Our solution has the desirable property that it is a simple generalization of the traditional query optimization algorithm. 1 Introduction  The idea of using materialized views for the benefit of improved query processing has been proposed in the literature more than a decade ago. In this context, problems such as definition of views, composition of views, maintenance of views [BC79, KP81, SI84, BLT86, CW91, Rou91, GMS93] have been researched but one topic has been conspicuous by its absence. This concerns the problem of the judicious use of materialized views in answering a query. It may seem that materialized views should be used to evaluate a query whenever they are applicable. In fact, blind applicat...
345|Integration of Heterogeneous Databases Without Common Domains Using Queries Based on Textual Similarity|Most databases contain &#034;name constants&#034; like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. However, in many cases, this assumption does not hold; determining if two name constants should be considered identical can require detailed knowledge of the world, the purpose of the user&#039;s query, or both. In this paper, we reject the assumption that global domains can be easily constructed, and assume instead that the names are given in natural language text. We then propose a logic called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. We describe an efficient implementation of WHIRL and evaluate it experimentally on data extracted from the World Wide Web. We show that WHIR...
346|Context Interchange: New Features and Formalisms for the Intelligent Integration of Information|The Context Interchange strategy presents a novel perspective for mediated data access in which semantic conflicts among heterogeneous systems are not identified a priori, but are detected and reconciled by a context mediator through comparison of contexts axioms corresponding to the systems engaged in data exchange. In this article, we show that queries formulated on shared views, export schema, and shared “ontologies ” can be mediated in the same way using the Context Interchange framework. The proposed framework provides a logic-based object-oriented formalism for representing and reasoning about data semantics in disparate systems, and has been validated in a prototype implementation providing mediated data access to both traditional and web-based information sources. Categories and Subject Descriptors: H.2.4 [Database Management]: Systems—Query processing; H.2.5 [Database Management]: Heterogeneous Databases—Data translation
347|Answering queries using templates with binding patterns|When integrating heterogeneous information re-sources, it is often the case that the source is rather limited in the kinds of queries it can answer. If a query is asked of the entire system, we have a new kind of optimization problem, in which we must try to express the given query in terms of the lim-ited query templates that this source can answer. For the case of conjunctive queries, we show how to decide with a nondeterministic polynomial-time al-gorithm whether the given query can be answered. We then extend our results to allow arithmetic comparisons in the given query and in the tem-plates.
348|Catching the Boat with Strudel: Experiences with a Web-Site Management System|The Strudel system applies concepts from database management systems to the process of building Web sites. Strudel&#039;s key idea is separating the management of the site&#039;s data, the creation and management of the site&#039;s structure, and the visual presentation of the site&#039;s pages. First, the site builder creates a uniform model of all data available at the site. Second, the builder uses this model to declaratively define the Web site&#039;s structure by applying a &#034;site-definition query&#034; to the underlying data. The result of evaluating this query is a &#034;site graph&#034;, which represents both the site&#039;s content and structure. Third, the builder specifies the visual presentation of pages in Strudel&#039;s HTML-template language. The data model underlying Strudel is a semi-structured model of labeled directed graphs. We describe Strudel&#039;s key characteristics, report on our experiences using Strudel, and present the technical problems that arose from our experience. We describe our experience constructing sev...
349|Answering Recursive Queries Using Views|We consider the problem of answering datalog queries using materialized views. The ability to answer queries using views is crucial in the context of information integration. Previous work on answering queries using views restricted queries to being conjunctive. We extend this work to general recursive queries: Given a datalog program P and a set of views, is it possible to find a datalog program that is equivalent to P and only uses views as EDB predicates? In this paper, we show that the problem of whether a datalog program can be rewritten into an equivalent program that only uses views is undecidable. On the other hand, we prove that a datalog program P can be effectively rewritten into a program that only uses views, that is contained in P,  and that contains all programs that only use views and are contained in P. As a consequence, if there exists a program equivalent to P that only uses views, then our construction is guaranteed to yield a program equivalent to P.  1 Introductio...
350|Query Caching and Optimization in Distributed Mediator Systems|Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source&#039;s performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a costbased optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants  mechanism, which s...
351|Managing Semantic Heterogeneity in Databases : A Theoretical Perspective, Tutorial at PODS|A full version of this tutorial appears at
352|Description Logics in Data Management|Description logics and reasoners, which are descendants of the kl-one language, have been studied in depth in Artificial Intelligence. After a brief introduction, we survey in this paper their application to the problems of information management, using the framework of an abstract information server equipped with several operations -- each involving one or more languages. Specifically, we indicate how one can achieve enhanced access to data and knowledge by using descriptions in languages for schema design and integration, queries, answers, updates, rules, and constraints.
353|The Information Manifold|We describe the Information Manifold (IM), a system for browsing and querying of multiple networked information sources. As a first contribution, the system demonstrates the viability of knowledge representation technology for retrieval and organization of information from disparate (structured and unstructured) information sources. Such an organization allows the user to pose high-level queries that use data from multiple information sources. As a second contribution, we describe novel query processing algorithms used to combine information from multiple sources. In particular, our algorithms are guaranteed to find exactly the set of information sources relevant to a query, and to  completely exploit knowledge about local closed world information (Etzioni et al. 1994). Introduction  We are currently witnessing an explosion in the amount of information that is available online. For example, the rapid rise in popularity of the World Wide Web (WWW) has increased the amount of information...
354|Optimizing Regular Path Expressions Using Graph Schemas|Query languages for data with irregular structure use regular path expressions for navigation. This feature is useful for querying data where parts of the structure is either unknown, unavailable to the user, or changes frequently. Naive execution of regular path expressions is inefficient, however, because it ignores any structure in the data. We describe two optimization techniques for queries with regular path expressions. Both rely on graph schemas for specifying partial knowledge about the data&#039;s structure. Query pruning uses this structure to restrict navigation to only a fragment of the data; we give an efficient algorithm for rewriting any regular path expression query into a pruned one. Query rewriting using state extents  can eliminate or reduce navigation altogether; it is reminiscent of optimizing relational queries using indices. There may be several ways to optimize a query using state extents; we give a polynomial-space algorithm that finds all such optimizations. For re...
355|Description logic framework for information integration|Information Integration is one of the core problems in distributed databases, cooperative information systems, and data warehousing, which are key areas in the software development industry. Two critical factors for the design and maintenance of applications requiring Information Integration are conceptual modeling of the domain, and reasoning support over the conceptual representation. We demonstrate that Knowledge Representation and Reasoning techniques can play an important role for both of these factors, by proposing a Description Logic based framework for Information Integration. We show that the development of successful Information Integration solutions requires not only to resort to very expressive Description Logics, but also to significantly extend them. We present a novel approach to conceptual modeling for Information Integration, which allows for suitably modeling the global concepts of the application, the individual information sources, and the constraints among different sources. Moreover, we devise inference procedures for the fundamental reasoning services, namely relation and concept subsumption, and query containment. Finally, we present a methodological framework for Information Integration, which can be applied in several contexts, and highlights the role of reasoning services within the design process. 1
356|Equivalences among relational expressions with the union and difference operators|ABSTRACT Queries in relational databases can be formulated in terms of relational expressions using the relational operations elect, project, join, union, and difference The equivalence problem for these queries is studied with query optimization m mind It ts shown that testmg eqmvalence of relational expressions with the operators elect, project, join, and union is complete m the class FIt of the polynomial-time hierarchy A nonprocedural representation for queries formulated by these expressions i proposed This method of query representation can be viewed as a generahzatlon f tableaux or conjunctive queries (which are used to represent expressions with only select, project, and join) Furthermore, this method is extended to queries formulated by relatmnal expressions that also contain the difference operator, provided that the project operator isnot applied to subexpresstons with the difference operator A procedure for testing eqmvalence of these queries is given It ts shown that testmg containment of tableaux is a necessary step in testing equivalence of queries with union and difference Three important cases m which containment of tableaux can be tested m polynomial time are described, although the containment problem is shown to be NP-complete ven for tableaux that correspond to expressions with only one project and several join operators KEY WORDS AND PHRASES relatmnal database, relational algebra, query optimization, equivalence of queries, conjunctive query, tableau, NP-complete, polynomial-time hierarchy, H 2P-complete CR CATEGORIES 4 33, 5 25
357|Representing and Using Interschema Knowledge in Cooperative Information Systems|Managing interschema knowledge is an essential task when dealing with cooperative information systems. We propose a logical approach to the problem of both expressing interschema knowledge, and reasoning about it. In particular, we set up a structured representation language for expressing semantic interdependencies between classes belonging to different database schemas, and present a method for reasoning over such interdependencies. The language and the associated reasoning technique makes it possible to build a logic-based module that can draw useful inferences whenever the need arises of both comparing and combining the knowledge represented in the various schemas. Notable examples of such inferences include checking the coherence of interschema knowledge, and providing integrated access to a cooperative information system.
358|Navigational Plans For Data Integration|We consider the problem of building data integration systems when the data sources are webs of data, rather than sets of relations. Previous approaches to modeling data sources are inappropriate in this context because they do not capture the relationships between linked data and the need to navigate through paths in the data source in order to obtain the data. We describe a language for modeling data sources in this new context. We show that our language has the required expressive power, and that minor extensions to it would make query answering intractable. We provide a sound and complete algorithm for reformulating a user query into a query over the data sources, and we show how to create query execution plans that both query and navigate the data sources.  Introduction  The purpose of data integration is to provide a uniform  interface to a multitude of data sources. Data integration applications arise frequently as corporations attempt to provide their customers and employees wit...
359|Tableau Techniques For Querying Information Sources Through Global Schemas|. The foundational homomorphism techniques introduced by Chandra and Merlin for testing containment of conjunctive queries have recently attracted renewed interest due to their central role in information integration applications. We show that generalizations of the classical tableau representation of conjunctive queries are useful for computing query answers in information integration systems where information sources are modeled as views defined on a virtual global schema. We consider a general situation where sources may or may not be known to be correct and complete. We characterize the set of answers to a global query and give algorithms to compute a finite representation of this possibly infinite set, as well as its certain and possible approximations. We show how to rewrite a global query in terms of the sources in two special cases, and show that one of these is equivalent to the Information Manifold rewriting of Levy et al. 1 Introduction Information Integration systems [Ull...
360|What Can Databases Do for Peer-to-Peer?|The Internet community has recently been focused on peer-to-peer systems like Napster, Gnutella, and Freenet. The  grand vision --- a decentralized community of machines pooling their resources to benefit everyone --- is compelling for  many reasons: scalability, robustness, lack of need for administration, and even anonymity and resistance to censorship.
361|Data Integration under Integrity Constraints|Data integratio n systemspro vide accessto a seto fhetero - geneo us, auto no mo us data so urces thro ugh a so -called glo bal schema. There are basically two appro aches fo r designing a data integratio n system. In the glo bal-centric appro ach,o ne defines the elementso f the glo bal schema as viewso ver the so urces, whereas in the lo cal-centric appro ach, o e characterizes the so rces as viewso ver theglo al schema. It is well kno wn that pro cessing queries in the latter appro ach is similar to query answering with inc o plete infoC atio , and, therefo9 is a c o plex task. On theo ther hand, it is a co mmo no pinio n that query pro cessing is much easier in the fo rmer appro ach. In this paper we sho w the surprising result that, when theglo al schema is expressed in the relatio al mo del with integrity c o straints, eveno f simple types, the pr o lemo f inco6 plete info rmatio n implicitly arises, making querypro cessing di#cult in the glo al-centric approC h as well. We thenfo cuso n glo al schemas with key andfo eign key co straints, which represents a situat io which is veryco#=W in practice, and we illustrate techniques fo e#ectively answering queries po sed to the data integratio n system in this case. 1 
362|Towards Heterogeneous Multimedia Information Systems: The Garlic Approach|Abstract: We provide an overview of the Garlic project, a new project at the IBM Almaden Research Center. The goal of this project is to develop a system and associated tools for the management of large quantities of heterogeneous multimedia information. Garlic permits traditional and multimedia data to be
363|Rewriting of Regular Expressions and Regular Path Queries|Recent work on semi-structured data has revitalized the interest in path queries, i.e., queries that ask for all pairs of objects in the database that are connected by a path conforming to a certain specification, in particular to a regular expression. Also, in semi-structured data, as well as in data integration, data warehousing, and query optimization, the problem of view-based query rewriting is receiving much attention: Given a query and a collection of views, generate a new query which uses the views and provides the answer to the original one. In this paper we address the problem of view-based query rewriting in the context of semi-structured data. We present a method for computing the rewriting of a regular expression E in terms of other regular expressions. The method computes the exact rewriting (the one that defines the same regular language as E) if it exists, or the rewriting that defines the maximal language contained in the one defined by E, otherwise. We present a complexity analysis of both the problem and the method, showing that the latter is essentially optimal. Finally, we illustrate how to exploit the method for view-based rewriting of regular path queries in semi-structured data. The complexity results established for the rewriting of regular expressions apply also to the case of regular path queries. 
364|CARIN: A Representation Language Combining Horn Rules and Description Logics|.  We describe CARIN, a novel family of representation languages, which integrate the expressive power of Horn rules and of description logics. We address the key issue in designing such a language, namely, providing a sound and complete inference procedure. We identify existential entailment as a core problem in reasoning in  CARIN, and describe an existential entailment algorithm for CARIN  languages whose description logic component is ALCNR. This algorithm entails several important results for reasoning in CARIN, most notably: (1) a sound and complete inference procedure for non recursive  CARIN-ALCNR, and (2) an algorithm for determining rule subsumption over ALCNR. 1 Introduction  Horn rule languages have formed the basis for many Artificial Intelligence application languages because their expressive power is sufficient for many applications, and they have good computational properties. One of the significant limitations of Horn rules is that they are not expressive enough to mod...
365|Query optimization in the presence of limited access patterns|We consider the problem of query optimization in the presence of limitations on access patterns to the data (i.e., when one must provide values for one of the attributes of a relation in order to obtain tuples). We show that in the presence of limited access patterns we must search a space of annotated query plans, where the annotations describe the inputs that must be given to the plan. We describe a theoretical and experimental analysis of the resulting search space and a novel query optimization algorithm that is designed to perform well under the di erent conditions that may arise. The algorithm searches the set of annotated query plans, pruning invalid and non-viable plans as early as possible in the search space, and it also uses a best- rst search strategy in order to produce a rst complete plan early in the search. We describe experiments to illustrate the performance of our algorithm. 1
366|Quality-driven Integration of Heterogeneous Information Systems|Integrated access to information that is spread over multiple, distributed, and heterogeneous sources is an important problem in many scientific and commercial domains. While much work has been done on query processing and choosing plans under cost criteria, very little is known about the important problem of incorporating the information quality aspect into query planning. In this paper we describe a framework for multidatabase query processing that fully includes the quality of information in many facets, such as completeness, timeliness, accuracy, etc. We seamlessly include information quality into a multidatabase query processor based on a view-rewriting mechanism. We model information quality at different levels to ultimately find a set of high-quality query answering plans.
367|Query Containment for Conjunctive Queries With Regular Expressions|The management of semistructured data has recently received significant attention because of the need of several applications to model and query large volumes of irregular data. This paper considers the problem of query containment for a query language over semistructured data, StruQL0 , that contains the essential feature common to all such languages, namely the ability to specify regular path expressions over the data. We show here that containment of StruQL0 queries is decidable. First, we give a semantic criterion for StruQL0 query containment: we show that it suffices to check containment on only finitely many canonical databases. Second, we give a syntactic criteria for query containment, based on a notion of query mappings, which extends containment mappings for conjunctive queries. Third, we consider a certain fragment of StruQL0 , obtained by imposing restrictions on the regular path expressions, and show that query containment for this fragment of  StruQL0 is NP complete.  1 ...
368|Recursive Plans for Information Gathering|Generating query-answering plans for information gathering agents requires to translate a user query, formulated in terms of a set of virtual relations, to a query that uses relations that are actually stored in information sources. Previous solutions to the translation problem produced sets of conjunctive plans, and were therefore limited in their ability to handle information sources with binding-pattern limitations, and to exploit functional dependencies in the domain model. As a result, these plans were incomplete w.r.t. sources encountered in practice (i.e., produced only a subset of the possible answers). We describe the novel class of recursive  information gathering plans, which enables us to settle two open problems. First, we describe an algorithm for finding a query plan that produces the maximal set of answers from the sources in the presence of functional dependencies. Second, we describe an analogous algorithm in the presence of binding-pattern restrictions in the sources...
369|EQUIVALENCES AMONG RELATIONAL EXPRESSIONS|Many database queries can be formulated in terms of expressions whose operands represent tables of information (relations) and whose operators are the relational operations select, project, and join. This paper studies the equivalence problem for these relational expressions, with expression optimization in mind. A matrix, called a tableau, is proposed as a natural representative for the value of an expression. It is shown how tableaux can be made to reflect functional dependencies among attributes. A polynomial time algorithm is presented for the equivalence of tableaux that correspond to an important subset of expressions, although the equivalence problem is shown to be NP-complete under slightly more general circumstances.
370|On the equivalence of recursive and nonrecursive Datalog programs|vardi Abstract: We study the problem of determining whether a given recursive Datalog program is equivalent to a given nonrecursive Datalog program. Since nonrecursive Dat-alog programs are equivalent to unions of conjunctive queries, we study also the problem of determining whether a given recursive Datalog program is contained in a union of con-junctive queries. For this problem, we prove doubly exponential upper and lower time bounds. For the equivalence problem, we prove triply exponential upper and lower time bounds. 1
371|Containment of conjunctive regular path queries with inverse |Reasoning on queries is a basic problem both in knowledge representation and databases. A fundamental form of reasoning on queries is checking containment, i.e., verifying whether one query yields necessarily a subset of the result of another query. Query containment is crucial in several contexts, such as query optimization, knowledge base verification, information integration, database integrity checking, and cooperative answering. In this paper we address the problem of query containment in the context of semistructured knowledge bases, where the basic querying mechanism, namely regular path queries,
372|Rewriting Aggregate Queries Using Views|We investigate the problem of rewriting queries with aggregate operators using views that mayormay not contain aggregate operators. A rewriting of a query is a second query that uses view predicates such that evaluating first the views and then the rewriting yields the same result as evaluating the original query. In this sense, the original query and the rewriting are equivalent modulo the view definitions. The queries and views we consider correspond to unnested SQL queries, possibly with union, that employ the operators min, max, count, and sum. Our approach is based on syntactic characterizations of the equivalence of aggregate queries. One contribution of this paper are characterizations of the equivalence of disjunctive aggregate queries, which generalize our previous results for the conjunctive case. For each operator &amp;alpha;, we introduce several types of queries using views as candidates for rewritings. We unfold such a candidate by replacing each occurrence of a view predicate with ...
373|An Extensible Framework for Data Cleaning|Data integration solutions dealing with large amounts of data have been strongly required in the last few years.  Besides the traditional data integration problems (e.g. schema integration, local to global schema mappings),  three additional data problems have to be dealt with: (1) the absence of universal keys across dierent databases  that is known as the object identity problem, (2) the existence of keyboard errors in the data, and (3) the presence  of inconsistencies in data coming from multiple sources. Dealing with these problems is globally called the data  cleaning process. In this work, we propose a framework which oers the fundamental services required by this  process: data transformation, duplicate elimination and multi-table matching. These services are implemented  using a set of purposely designed macro-operators. Moreover, we propose an SQL extension for specifying  each of the macro-operators. One important feature of the framework is the ability of explicitly includ...
374|Answering regular path queries using views|Query answering using views amounts to computing the answer to a query having information only on the extension of a set of views. This problem is relevant in several fields, such as information integration, data warehousing, query optimization, mobile computing, and maintaining physical data independence. We address query answering using views in a context where queries and views are regular path queries, i.e., regular expressions that denote the pairs of objects in the database connected by a matching path. Regular path queries are the basic query mechanism when the database is conceived as a graph, such as in semistructured data and data on the web. We study algorithms for answering regular path queries using views under different assumptions, namely, closed and open domain, and sound, complete, and exact information on view extensions. We characterize data, expression, and combined complexity of the problem, showing that the proposed algorithms are essentially optimal. Our results are the first to exhibit decidability in cases where the language for expressing the query and the views allows for recursion. 
375|Query Planning and Optimization in Information Integration|Information integration systems, also knows as mediators, information brokers, or information gathering agents, provide uniform user interfaces to varieties of different information sources. With corporate databases getting connected by intranets, and vast amounts of information becoming available over the Internet, the need for information integration systems is increasing steadily. Our work focusses on query planning in such systems...
376|Answering queries using views over description logics knowledge bases|Answering queries using views amounts to computing the answer to a query having information only on the extension of a set of precomputed queries (views). This problem is relevant in several fields, such as information integration, query optimization, and data warehousing, and has been studied recently in different settings. In this paper we address answering queries using views in a setting where intensional knowledge about the domain is represented using a very expressive Description Logic equipped with n-ary relations, and queries are nonrecursive datalog queries whose predicates are the concepts and relations that appear in the Description Logic knowledge base. We study the problem under different assumptions, namely, closed and open domain, and sound, complete, and exact information on view extensions. We show that under the closed domain assumption, in which the set of all objects in the knowledge base coincides with the set of objects stored in the views, answering queries using views is already intractable. We show also that under the open domain assumption the problem is decidable in double exponential time.
377|A Formal Perspective on the View Selection Problem|The view selection problem is to choose a set of views to materialize over a database schema, such that the cost of evaluating a set of workload queries is minimized and such that the views t into a prespeci ed storage constraint. The two main applications of the view selection problem are materializing views in a database to speed up query processing, and selecting views to materialize in a data warehouse to answer decision support queries. We describe several fundamental results concerning the view selection problem. We consider the problem for views and workloads that consist of equalityselection, project and join queries, and show that the complexity of the problem depends crucially on the quality of the estimates that a query optimizer has on the size of the views it is considering to materialize. When a query optimizer has good estimates of the sizes of the views, we show that an optimal choice of views may involve a number of views that is exponential in the size of the database schema. On the other hand, when an optimizer uses standard estimation heuristics, we show that the number of necessary views and the expression size of each view are polynomially bounded. 1
378|Query containment for data integration systems|The problem of query containment is fundamental to many aspects of database systems,including query optimization,determining independence of queries from updates,and rewriting queries using views. In the data-integration framework,however,the standard notion of query containment does not suffice. We define relative containment,which formalizes the notion of query containment relative to the sources available to the data-integration system. First,we provide optimal bounds for relative containment for several important classes of datalog queries,including the common case of conjunctive queries. Next,we provide bounds for the case when sources enforce access restrictions in the form of binding pattern constraints. Surprisingly,we show that relative containment for conjunctive queries is still decidable in this case,even though it is known that finding all answers to such queries may require a recursive datalog program over the sources. Finally,we provide tight bounds for variants of relative containment when the queries and source descriptions may contain comparison predicates.
379|Specifying and querying database repairs using logic programs with exceptions|Abstract Databases may be inconsistent with respect to a given set of integrity constraints. Nevertheless, most of the data may be consistent. In this paper we show how to specify consistent data and how to query a relational database in such a way that only consistent data is retrieved. The specification and queries are based on disjunctive extended logic programs with positive and negative exceptions that generalize those previously introduced by Kowalski and Sadri.
380|View-based query processing and constraint satisfaction|View-based query processing requires to answer a query posed to a database only on the basis of the information on a set of views, which are again queries over the same database. This problem is relevant in many aspects of database management, and has been addressed by means of two basic approaches, namely, query rewriting and query answering. In the former approach, one tries to compute a rewriting of the query in terms of the views, whereas in the latter, one aims at directly answering the query based on the view extensions. We study view-based query processing for the case of regular-path queries, which are the basic querying mechanisms for the emergent field of semistructured data. Based on recent results, we first show that a rewriting is in general a co-NP function wrt to the size of view extensions. Hence, the problem arises of characterizing which instances of the problem admit a rewriting that is PTIME. A second contribution of the work is to establish a tight connection between view-based query answering and constraint-satisfaction problems, which allows us to show that the above characterization is going to be difficult. As a third contribution of our work, we present two methods for computing PTIME rewritings of specific forms. The first method, which is based on the established connection with constraint-satisfaction problems, gives us rewritings expressed in Datalog with a fixed number of variables. The second method, based on automata-theoretic techniques, gives us rewritings that are formulated as unions of conjunctive regular-path queries with a fixed number of variables.  
381|Optimization Properties for Classes of Conjunctive Regular Path Queries |Abstract. We are interested in the theoretical foundations of the optimization of conjunctive regular path queries (CRPQs). The basic problem here is deciding query containment both in the absence and presence of constraints. Containment without constraints for CRPQs is EXPSPACE-complete, as opposed to only NP-complete for relational conjunctive queries. Our past experience with implementing similar algorithms suggests that staying in PSPACE might still be useful. Therefore we investigate the complexity of containment for a hierarchy of fragments of the CRPQ language. The classifying principle of the fragments is the expressivity of the regular path expressions allowed in the query atoms. For most of these fragments, we give matching lower and upper bounds for containment in the absence of constraints. We also introduce for every fragment a naturally corresponding class of constraints in whose presence we show both decidability and undecidability results for containment in various fragments. Finally, we apply our results to give a complete algorithm for rewriting with views in the presence of constraints for a fragment that contains Kleene-star and disjunction. 1
382|Deciding Containment for Queries with Complex Objects and Aggregations|We address the problem of query containment and query equivalence for complex objects. We show that for a certain conjunctive query language for complex objects, query containment and weak query equivalence are decidable. Our results have two consequences. First, when the answers of the two queries are guaranteed not to contain empty sets, then weak equivalence coincides with equivalence, and our result answers partially an open problem about the equivalence of nest; unnest queries for complex objects [GPG90]. Second, we derive an NP-complete algorithm for checking the equivalence of certain conjunctive queries with grouping and aggregates. Our results rely on a translation of the containment and equivalence conditions for complex objects into novel conditions on conjunctive queries, which we call simulation and strong simulation. These conditions are more complex than containment of conjunctive queries, because they involve arbitrary numbers of quantifier alternations. We prove that c...
383|Capability Based Mediation in TSIMMIS|Introduction  The TSIMMIS system [1] integrates data from multiple heterogeneous sources and provides users with seamless integrated views of the data. It translates a user query on the integrated views into a set of source queries and postprocessing steps that compute the answer to the user query from the results of the source queries. TSIMMIS uses a  mediation architecture [11] to accomplish this (Figure 1). User Source 1 Source 2 Source N  Mediator Figure 1: The TSIMMIS Architecture Many other data integration systems like Garlic [2, 9] and Information Manifold [4] employ a similar architecture. One of the distinguishing features of TSIMMIS is its use of a semi-structured data model (called the Object Exchange Model or OEM [7]) for dealing with the heterogeneity of the data sources. In particular, it employs source wrappers [3] that provide a uniform OEM interface to the mediator. In SIGMO
384|Query Answering in Information Systems with Integrity Constraints|The specifications of most of the nowadays ubiquitous informations systems include integrity constraints, i.e. conditions rejecting so-called &#034;invalid&#034; or &#034;inconsistent &#034; data. Information system consistency and query answering have been formalized referring to classical logic implicitly assuming that query answering only makes sense with consistent information systems. In practice, however, inconsistent as well as consistent information systems need to be queried. In this paper, it is first argued that classical logic is inappropriate for a formalization of information systems because of its global notion of inconsistency. It is claimed that information systems inconsistency should be understood as a  local notion. Then, it is shown that minimal logic, a constructivistic weakening of classical logic which precludes refutation proofs, provides for local inconsistencies that conveniently reflect a practitioner&#039;s intuition. Further, minimal logic is shown to be a convenient foundation fo...
385|Query Folding with Inclusion Dependencies|Query folding is a technique for determining how a query may be answered using a given set of resources, which may include materialized views, cached results of previous queries, or queries answerable by other databases. The power of query folding can be considerably enhanced by taking into account integrity constraints that are known to hold on base relations. This paper describes an extension of query folding that utilizes inclusion dependencies to find foldings of queries that would otherwise be overlooked. We describe a complete strategy for finding foldings in the presence of inclusion dependencies and present a basic algorithm that implements that strategy. We also describe extensions to this algorithm when both inclusion and functional dependencies are considered.
386|Information Integration: the MOMIS Project Demonstration|ranted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 26th VLDB Conference,  Cairo, Egypt, 2000.  2  MOMIS is a joint project among the Universit`a di Modena e Reggio Emilia, the Universit`a di Milano, and the Universit`a di Brescia within the national research project INTERDATA, theme n.3 &#034;Integration of Information over the Web&#034;, coordinated by V. De Antonellis, Universit`a di Brescia.  1. a common data model, ODM I 3  , which is defined according to the ODL I 3 language, to describe source schemas for integration purposes. ODM I 3 and ODL I 3&lt;F12.24
387|Query Planning with Limited Source Capabilities|In information-integration systems, sources may have diverse and limited query capabilities. In this paper we show that because sources have restrictions on retrieving their information, sources not mentioned in a query can contribute to the query result by providing useful bindings. In some cases we can access sources repeatedly to retrieve bindings to answer a query, and query planning thus becomes considerably more challenging. We find all the obtainable answers to a query by translating the query and source descriptions to a simple recursive Datalog program, and evaluating the program on the source relations. This program often accesses sources that are not in the query. Some of these accesses are essential, as they provide bindings that let us query sources, which we could not do otherwise. However, some of these accesses can be proven not to add anything to the query&#039;s answer. We show in which cases these off-query accesses are useless, and prove that in these cases we can comput...
388|Querying Aggregate Data|We introduce a first-order language with real polynomial arithmetic and aggregation operators (count, iterated sum and multiply), which is well suited for the definition of aggregate queries involving complex statistical functions. It offers a good trade-off between expressive power and complexity, with a tractable data complexity. Interestingly, some fundamental properties of first-order with real arithmetic are preserved in the presence of aggregates. In particular, there is an effective quantifier elimination for formulae with aggregation. We consider the problem of querying data that has already been aggregated in aggregate views, and focus on queries with an aggregation over a conjunctive query. Our main conceptual contribution is the introduction of a new equivalence relation among conjunctive queries, the isomorphism modulo a product. We prove that the equivalence of aggregate queries such as for instance averages reduces to it. Deciding if two queries are isomorphic modulo a p...
389|Accessing data integration systems through conceptual schemas|Abstract. Data integration systems provide access to a set of heterogeneous, autonomous data sources through a so-called global, or mediated view. There is a general consensus that the best way to describe the global view is through a conceptual data model, and that there are basically two approaches for designing a data integration system. In the global-as-view approach, one defines the concepts in the global schema as views over the sources, whereas in the local-as-view approach, one characterizes the sources as views over the global schema. It is well known that processing queries in the latter approach is similar to query answering with incomplete information, and, therefore, is a complex task. On the other hand, it is a common opinion that query processing is much easier in the former approach. In this paper we show the surprising result that, when the global schema is expressed in terms of a conceptual data model, even a very simple one, query processing becomes difficult in the global-as-view approach also. We demonstrate that the problem of incomplete information arises in this case too, and we illustrate some basic techniques for effectively answering queries posed to the global schema of the data integration system. 1
390|Answering Queries Using Materialized Views With Disjunctions|We consider the problem of answering datalog queries using  materialized views. More  specifi.
391|On Answering Queries in the Presence of Limited Access Patterns|. In information-integration systems, source relations often  have limitations on access patterns to their data; i.e., when one must  provide values for certain attributes of a relation in order to retrieve its  tuples. In this paper we consider the following fundamental problem: can  we compute the complete answer to a query by accessing the relations  with legal patterns? The complete answer to a query is the answer that  we could compute if we could retrieve all the tuples from the relations.  We give algorithms for solving the problem for various classes of queries,  including conjunctive queries, unions of conjunctive queries, and conjunctive  queries with arithmetic comparisons. We prove the problem is  undecidable for datalog queries. If the complete answer to a query cannot  be computed, we often need to compute its maximal answer. The  second problem we study is, given two conjunctive queries on relations  with limited access patterns, how to test whether the maximal answer to...
392|Answering Queries Using Limited External Query Processors|When answering queries using external information sources, their contents can be described by views. To answer a query, we must rewrite it using the set of views presented by the sources. When the external information sources also have the ability to answer some (perhaps limited) sets of queries that require performing operations on their data, the set of views presented by the source may be infinite (albeit encoded in some finite fashion). Previous work on answering queries using views has only considered the case where the set of views is finite. In order to exploit the ability of information sources to answer more complex queries, we consider the problem of answering conjunctive queries using infinite sets of conjunctive views. Our first result is that an infinite set of conjunctive views can be partitioned into a finite number of equivalence classes, such that picking one view from every nonempty class is sufficient to determine whether the query can be answered using the views. Se...
393|On the Content of Materialized Aggregate Views|We consider the problem of answering queries using only materialized views. We first show that if the views subsume the query from the point of view of the information content, then the query can be answered using only the views, but the resulting query might be extremely inefficient. We then focus on aggregate views and queries over a single relation, which are fundamental in many applications such as data warehousing. We show that in this case, it is possible to guarantee that as soon as the views subsume the query, it can be completely rewritten in terms of the views in a simple query language. Our main contribution is the conception of various rewriting algorithms which run in polynomial time, and the proof of their completeness which relies on combinatorial arguments. Finally, we discuss the choice of materializing or not ratio views such as average and percentage, important for the design of materialized views. We show that it has an impact on the information content, which can b...
394|Lossless Regular Views|If the only information we have on a certain database is through a set of views, the question arises of whether this is sucient to answer completely a given query. We say that the set of views is lossless with respect to the query, if, no matter what the database is, we can answer the query by solely relying on the content of the views. The question of losslessness has various applications, for example in query optimization, mobile computing, data warehousing, and data integration. We study this problem in a context where the database is semistructured, and both the query and the views are expressed as regular path queries. The form of recursion present in this class prevents us from applying known results to our case.
395|View-based query answering and query containment over semistructured data|Abstract. The basic querying mechanism over semistructured data, namely regular path queries, asks for all pairs of objects that are connected by a path conforming to a regular expression. We consider conjunctive two-way regular path queries (C2RPQc’s), which extend regular path queries with two features. First, they add the inverse operator, which allows for expressing navigations in the database that traverse the edges both backward and forward. Second, they allow for using conjunctions of atoms, where each atom specifies that a regular path query with inverse holds between two terms, where each term is either a variable or a constant. For such queries we address the problem of view-based query answering, which amounts to computing the result of a query only on the basis of a set of views. More specifically, we present the following results: (1) We exhibit a mutual reduction between query containment and the recognition problem for view-based query answering for C2RPQc’s, i.e., checking whether a given tuple is in the certain answer to a query. Based on such a result, we can show that the problem of view-based query answering for C2RPQc’s is EXPSPACE-complete. (2) By exploiting techniques based on alternating two-way automata we show that for the restricted class of tree two-way regular path queries (in which the links between variables form a tree), query containment and view-based query answering are, rather surprisingly, in PSPACE (and hence, PSPACE-complete). (3) We present a technique to obtain view-based query answering algorithms that compute the whole set of tuples in the certain answer, instead of requiring to check each tuple separately. The technique is parametric wrt the query language, and can be applied both to C2RPQc’s and to tree-queries. 1
396|Models for Information Integration: Turning Local-as-View Into Global-as-View|There are basically two approaches for designing a data integration system. In the global-as-view approach, one defines the concepts in the global schema as views over the sources, whereas in the local-as-view approach, one characterizes the sources as views over the global schema. The goal of this paper is to verify whether we can transform a data integration system built with the local-as-view approach into a system following the global-as-view approach. We study the problem in a setting where the global schema is expressed in the relational model with inclusion dependencies, and the queries used in the integration systems (both the queries on the global schema, and the views in the mapping) are expressed in the language of conjunctive queries. The result we present is that such a transformation exists: we can always transform a local-as-view system into a global-as-view system such that, for each query, the set of answers to the query wrt the former is the same as the set of answers wrt the latter.
397|Query Rewriting using Semistructured Views|We address the problem of query rewriting for MSL, a semistructured language developed at Stanford in the TSIMMIS project for information integration. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting  queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing containment mappings, the chase, and unification -- techniques which were developed for structured, relational data. At the same time we develop an algorithm for equivalence checking of MSL queries. We show that the rewriting algorithm is sound and complete, i.e., it always finds every conjunctive MSL rewriting query of q, and we discuss its complexity. We currently incorporate the algorithm in the TSIMMIS system. 1 Introduction  Recently, many semistructured data models, query and view definition languages have been proposed [GM  +  97, MAG  +  97, BDHS96, AV97a, MM97, KS95, PGMU96,...
398|Intensional Query Answering by Partial Evaluation|. Intensional query answering aims at providing a response to a query addressed to a knowledge base by making use of the intensional knowledge as opposed to extensional. Such a response is an abstract description of the conventional answer that can be of interest in many situations, for example it may increase the cooperativeness of the system, or it may replace the conventional answer in case access to the extensional part of the knowledge base is costly as for Mobile Systems. In this paper we present a general framework to generate intensional answers in knowledge bases adhering to the logic programming paradigm. Such a framework is based on a program transformation technique, namely Partial Evaluation, and allows for generating complete and procedurally complete (wrt SLDNF-resolution) sets of intensional answers, treating both recursion and negation conveniently. Keywords: Knowledge bases, intensional query answering, logic programs, partial evaluation 1. Introduction Intensional an...
399|Biclustering of Expression Data|An efficient node-deletion algorithm is introduced to find submatrices...
400|The NP-completeness column: an ongoing guide|This is the nineteenth edition of a (usually) quarterly column that covers new developments in the theory of NP-completeness. The presentation is modeled on that used by M. R. Garey and myself in our book &#034;Computers and Intractability: A Guide to the Theory of NP-Completeness,&#034; W. H. Freeman &amp; Co., New York, 1979 (hereinafter referred to as &#034;[G&amp;J]&#034;; previous columns will be referred to by their dates). A background equivalent to that provided by [G&amp;J] is assumed, and, when appropriate, cross-references will be given to that book and the list of problems (NP-complete and harder) presented there. Readers who have results they would like mentioned (NP-hardness, PSPACE-hardness, polynomial-time-solvability, etc.) or open problems they would like publicized, should
401|An Algorithm for Clustering cDNAs for Gene Expression Analysis|We have developed a novel algorithm for cluster analysis that is based on graph theoretic techniques. A similarity graph is defined and clusters in that graph correspond to highly connected subgraphs. A polynomial algorithm to compute them efficiently is presented. Our algorithm produces a clustering with some provably good properties.  The application that motivated this study was gene expression analysis, where a collection of cDNAs must be clustered based on their oligonucleotide fingerprints. The algorithm has been tested intensively on simulated libraries and was shown to outperform extant methods. It demonstrated robustness to high noise levels. In a blind test on real cDNA fingerprint data the algorithm obtained very good results. Utilizing the results of the algorithm would have saved over 70% of the cDNA sequencing cost on that data set.  1 Introduction  Cluster analysis seeks grouping of data elements into subsets, so that elements in the same subset are in some sense more cl...
402|Contentment in graph theory: covering graphs with cliques|Fundamental questions posed by Boole in 1868 on the theory of sets have in recent years been translated to problems in graph theory. The major problems that this paper deals with are determining the minimum number of complete subgraphs of graph G which include all of the edges of G, and determining the minimum number of complete bipartite subgraphs which cover G. The two problems are of a very similar nature. Determining whether there is a projective plane of order p is a special case of the former problem. The latter problem has a natural translation into matrix theory which yields tight upper and lower bounds. An elementary proof is given for Graham&#039;s theorem. Two non-obvious classes are given for which the above problems are easily handled; however, this author doubts that these classes can be extended significantly. Two new problems are shown in this
403|Systematic Management and Analysis of Yeast Gene Expression Data|roarrays, Serial Analysis of Gene Expression (SAGE), and other techniques (Velculescu et al. 1995; Lockhart et al. 1996; DeRisi et al. 1997), has led to the rapid accumulation of large expression data sets and the development of the field of functional genomics. Functional genomics has been contrasted as having a systematic, genome-wide approach to the collection and analysis of biological data compared with more traditional methods, which focus in depth on particular genes, proteins, or pathways (Hieter and Boguski 1997). But despite rapid strides, capped by a string of successful studies (see Table 1), functional genomics has yet to develop a highly integrated system of tools and methods such those used in sequence analysis and structural genomics (Hieter and Boguski 1997); for that, we must await development of the three following components: general databases, data standards, and integrated general-purpose analysis tools. A comparison of the status of these fields with respect to t
404|Graphical models, exponential families, and variational inference|The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide varietyof algorithms — among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations — can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.
405|Constructing Free Energy Approximations and Generalized Belief Propagation Algorithms|Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain regionbased free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a “valid ” or “maxent-normal ” approximation. We describe the relationship between four different methods that can be used to generate valid approximations: the “Bethe method, ” the “junction graph method, ” the “cluster variation method, ” and the “region graph method.” Finally, we explain how to tell whether a region-based approximation, and its corresponding GBP algorithm, is likely to be accurate, and describe empirical results showing that GBP can significantly outperform BP.
406|Belief Propagation|When a pair of nuclear-powered Russian submarines was reported patrolling off the eastern seaboard of the U.S. last summer, Pentagon officials expressed wariness over the Kremlin’s motivations. At the same time, these officials emphasized their confidence in the U.S. Navy’s tracking capabilities: “We’ve known where they were,” a senior Defense Department official told the New York Times, “and we’re not concerned about our ability to track the subs.” While the official did not divulge the methods used by the Navy to track submarines, the Times added that such
407|Codes and Decoding on General Graphs|Iterative decoding techniques have become a viable alternative for constructing high performance coding systems. In particular, the recent success of turbo codes indicates that performance close to the Shannon limit may be achieved. In this thesis, it is showed that many iterative decoding algorithms are special cases of two generic algorithms, the min-sum and sum-product algorithms, which also include non-iterative algorithms such as Viterbi decoding. The min-sum and sum-product algorithms are developed and presented as generalized trellis algorithms, where the time axis of the trellis is replaced by an arbitrary graph, the &#034;Tanner graph&#034;. With cycle-free Tanner graphs, the resulting decoding algorithms (e.g., Viterbi decoding) are maximum-likelihood but suffer from an exponentially increasing complexity. Iterative decoding occurs when the Tanner graph has cycles (e.g., turbo codes); the resulting algorithms are in general suboptimal, but significant complexity reductions are possible compared to the cycle-free case. Several performance estimates for iterative decoding are developed, including a generalization of the union bound used with Viterbi decoding and a characterization of errors that are uncorrectable after infinitely many decoding iterations.
408|Correctness of Local Probability Propagation in Graphical Models with Loops|This article analyzes the behavior of local propagation rules in graphical models with a loop.
409|A New Class of Upper Bounds on the Log Partition Function|Bounds on the log partition function are important in a variety of contexts, including approximate inference, model fitting, decision theory, and large deviations analysis [11, 5, 4]. We introduce a new class of upper bounds on the log partition function, based on convex combinations of distributions in the exponential domain, that is applicable to an arbitrary undirected graphical model. In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe free energy, but distinguished by the following desirable properties: (i) they are convex, and have a unique global minimum; and (ii) the global minimum gives an upper bound on the log partition function. The global minimum is defined by stationary conditions very similar to those defining xed points of belief propagation (BP) or tree-based reparameterization [see 13, 14]. As with BP fixed points, the elements of the minimizing argument can be used as approximations to the marginals of the original model. The analysis described here can be extended to structures of higher treewidth (e.g., hypertrees), thereby making connections with more advanced approximations (e.g., Kikuchi and variants [15, 10]).
411|Multiresolution markov models for signal and image processing|This paper reviews a significant component of the rich field of statistical multiresolution (MR) modeling and processing. These MR methods have found application and permeated the literature of a widely scattered set of disciplines, and one of our principal objectives is to present a single, coherent picture of this framework. A second goal is to describe how this topic fits into the even larger field of MR methods and concepts–in particular making ties to topics such as wavelets and multigrid methods. A third is to provide several alternate viewpoints for this body of work, as the methods and concepts we describe intersect with a number of other fields. The principle focus of our presentation is the class of MR Markov processes defined on pyramidally organized trees. The attractiveness of these models stems from both the very efficient algorithms they admit and their expressive power and broad applicability. We show how a variety of methods and models relate to this framework including models for self-similar and 1/f processes. We also illustrate how these methods have been used in practice. We discuss the construction of MR models on trees and show how questions that arise in this context make contact with wavelets, state space modeling of time series, system and parameter identification, and hidden
412|A Double-Loop Algorithm to Minimize the Bethe and Kikuchi Free Energies|Recent work (Yedidia, Freeman, Weiss [22]) has shown that stable points of belief propagation (BP) algorithms [12] for graphs with loops correspond to extrema of the Bethe free energy [3]. These BP algorithms have been used to obtain good solutions to problems for which alternative algorithms fail to work [4], [5], [10] [11]. In this paper we rst obtain the dual energy of the Bethe free energy which throws light on the BP algorithm. Next we introduce a discrete iterative algorithm which we prove is guaranteed to converge to a minimum of the Bethe free energy. We call this the double-loop algorithm because it contains an inner and an outer loop. It extends a class of mean eld theory algorithms developed by [7],[8] and, in particular, [13]. Moreover, the double-loop algorithm is formally very similar to BP which may help understand when BP converges. Finally, we extend all our results to the Kikuchi approximation which includes the Bethe free energy as a special case [3]. (Yedidia et al [22] showed that a \generalized belief propagation&#034; algorithm also has its xed points at extrema of the Kikuchi free energy). We are able both to obtain a dual formulation for Kikuchi but also obtain a double-loop discrete iterative algorithm that is guaranteed to converge to a minimum of the Kikuchi free energy. It is anticipated that these double-loop algorithms will be useful for solving optimization problems in computer vision and other applications.
413|Linear programming relaxations and belief propagation – an empirical study|The problem of finding the most probable (MAP) configuration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for finding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the fields of computer vision and computational biology. We find that TRBP almost always finds the solution significantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can find the MAP configurations in a matter of minutes for a large range of real world problems. 1.
414|MAP Estimation, Linear Programming and Belief Propagation with Convex Free Energies|Finding the most probable assignment (MAP) in a general graphical model is known to be NP hard but good approximations have been attained with max-product belief propagation (BP) and its variants. In particular, it is known that using BP on a single-cycle graph or tree reweighted BP on an arbitrary graph will give the MAP solution if the beliefs have no ties. In this paper we extend the setting under which BP can be used to provably extract the MAP. We define Convex BP as BP algorithms based on a convex free energy approximation and show that this class includes ordinary BP with single-cycle, tree reweighted BP and many other BP variants. We show that when there are no ties, fixed-points of convex max-product BP will provably give the MAP solution. We also show that convex sum-product BP at sufficiently small temperatures can be used to solve linear programs that arise from relaxing the MAP problem. Finally, we derive a novel condition that allows us to derive the MAP solution even if some of the convex BP beliefs have ties. In experiments, we show that our theorems allow us to find the MAP in many real-world instances of graphical models where exact inference using junction-tree is impossible. 
415|Variational Approximations between Mean Field Theory and the Junction Tree Algorithm|Recently, variational approximations such as  the mean field approximation have received  much interest. We extend the standard mean  field method by using an approximating distribution  that factorises into cluster potentials.  This includes undirected graphs, directed  acyclic graphs and junction trees. We  derive generalised mean field equations to optimise  the cluster potentials. We show that  the method bridges the gap between the standard  mean field approximation and the exact  junction tree algorithm. In addition, we address  the problem of how to choose the structure  and the free parameters of the approximating  distribution. From the generalised  mean field equations we derive rules to simplify  the approximation in advance without  affecting the potential accuracy of the model  class. We also show how the method fits into  some other variational approximations that  are currently popular.  1 INTRODUCTION  Graphical models, such as Bayesian networks, Markov fields, and Bolt...
416|Fractional Belief Propagation|We consider approximate inference in probabilistic graphical models with approximate free energy methods. By considering equivalent factor-graph representations of a probabilistic model, we write down a family of different approximate tree-like free energies. We show that this family interpolates between the naive mean-field free energy and the Bethe free energy. We derive fixed-point equations that lead to fractional belief propagation algorithms, which include standard mean-field equations and loopy belief propagation as special cases. Using a cavity-field argument, we compute the fractional algorithm that gives, in lowest order, a correction around the Bethe (loopy belief) approximation for the means. Simulation results illustrate the potential merits of the approach.
417|Minimizing and learning energy functions for side-chain prediction|Side-chain prediction is an important subproblem of the general protein folding problem. Despite much progress in side-chain prediction, performance is far from satisfactory. As an example, the ROSETTA protocol that uses simulated annealing to select the minimum energy conformations, correctly predicts the first two side-chain angles for approximately 72 % of the buried residues in a standard data set. Is further improvement more likely to come from better search methods, or from better energy functions? Given that exact minimization of the energy is NP hard, it is difficult to get a systematic answer to this question. In this paper, we present a novel search method and a novel method for learning energy functions from training data that are both based on Tree Reweighted Belief Propagation (TRBP). We find that TRBP can find the global optimum of the ROSETTA energy function in a few minutes of computation for approximately 85 % of the proteins in a standard benchmark set. TRBP can also effectively bound the partition function which enables using the Conditional Random Fields (CRF) framework for learning. Interestingly, finding the global minimum does not significantly improve side-chain prediction for
418|Log-Determinant Relaxation for Approximate Inference in Discrete Markov Random Fields| Graphical models are well suited to capture the complex and non-Gaussian statistical dependencies that arise in many real-world signals. A fundamental problem common to any signal processing application of a graphical model is that of computing approximate marginal probabilities over subsets of nodes. This paper proposes a novel method, applicable to discrete-valued Markov random fields (MRFs) on arbitrary graphs, for approximately solving this marginalization problem. The foundation of our method is a reformulation of the marginalization problem as the solution of a low-dimensional convex optimization problem over the marginal polytope. Exactly solving this problem for general graphs is intractable; for binary Markov random fields, we describe how to relax it by using a Gaussian bound on the discrete entropy and a semidefinite outer bound on the marginal polytope. This combination leads to a log-determinant maximization problem that can be solved efficiently by interior point methods, thereby providing approximations to the exact marginals. We show how a slightly weakened log-determinant relaxation can be solved even more efficiently by a dual reformulation. When applied to denoising problems in a coupled mixture-of-Gaussian model defined on a binary MRF with cycles, we find that the performance of this log-determinant relaxation is comparable or superior to the widely used sum-product algorithm over a range of experimental conditions. 
419|Structured region graphs: Morphing EP into GBP|GBP and EP are two successful algorithms for approximate probabilistic inference, which are based on different approximation strategies. An open problem in both algorithms has been how to choose an appropriate approximation structure. We introduce “structured region graphs, ” a formalism which marries these two strategies, reveals a deep connection between them, and suggests how to choose good approximation structures. In this formalism, each region has an internal structure which defines an exponential family, whose sufficient statistics must be matched by the parent region. Reduction operators on these structures allow conversion between EP and GBP free energies. Thus it is revealed that all EP approximations on discrete variables are special cases of GBP, and conversely that some wellknown GBP approximations, such as overlapping squares, are special cases of EP. Furthermore, region graphs derived from EP have a number of good structural properties, including maxent-normality and overall counting number of one. The result is a convenient framework for producing high-quality approximations with a user-adjustable level of complexity. 1
420|Treewidth-Based Conditions for Exactness Of The Sherali-Adams and Lasserre    Relaxations|The Sherali-Adams (SA) and Lasserre (LS) approaches are &#034;lift-and-project&#034; methods that  generate nested sequences of linear and/or semidefinite relaxations of an arbitrary 0-1 polytope      . Although both procedures are known to terminate with an exact description of  P after n steps, there are various open questions associated with characterizing, for particular  problem classes, whether exactness is obtained at some step s  n. This paper provides sufficient conditions for exactness of these relaxations based on the hypergraph-theoretic notion of  treewidth. More specifically, we relate the combinatorial structure of a given polynomial system  to an underlying hypergraph. We prove that the complexity of assessing the global validity of  moment sequences, and hence the tightness of the SA and LS relaxations, is determined by the  treewidth of this hypergraph. We provide some examples to illustrate this characterization.
421|Bayesian random fields: The Bethe-Laplace approximation|While learning the maximum likelihood value of parameters of an undirected graphical model is hard, modelling the posterior distribution over parameters given data is harder. Yet, undirected models are ubiquitous in computer vision and text modelling (e.g. conditional random fields). But where Bayesian approaches for directed models have been very successful, a proper Bayesian treatment of undirected models in still in its infant stages. We propose a new method for approximating the posterior of the parameters given data based on the Laplace approximation. This approximation requires the computation of the covariance matrix over features which we compute using the linear response approximation based in turn on loopy belief propagation. We develop the theory for conditional and “unconditional ” random fields with or without hidden variables. In the conditional setting we introduce a new variant of bagging suitable for structured domains. Here we run the loopy max-product algorithm on a “super-graph ” composed of graphs for individual models sampled from the posterior and connected by constraints. Experiments on real world data validate the proposed methods. 1
422|Linear Response for Approximate Inference|Belief propagation on cyclic graphs is an efficient algorithm for computing approximate marginal probability distributions over single nodes and neighboring nodes in the graph. In this paper we propose two new algorithms for approximating joint probabilities of arbitrary pairs of nodes and prove a number of desirable properties that these estimates fulfill. The first algorithm is a propagation...
423|Data Preparation for Mining World Wide Web Browsing Patterns|The World Wide Web (WWW) continues to grow at an astounding  rate in both the sheer volume of tra#c and the size and complexity  of Web sites. The complexity of tasks such as Web site design,  Web server design, and of simply navigating through a Web site have  increased along with this growth. An important input to these design  tasks is the analysis of how a Web site is being used. Usage analysis includes  straightforward statistics, such as page access frequency, as well as  more sophisticated forms of analysis, such as finding the common traversal  paths through a Web site. Web Usage Mining is the application of  data mining techniques to usage logs of large Web data repositories in  order to produce results that can be used in the design tasks mentioned  above. However, there are several preprocessing tasks that must be performed  prior to applying data mining algorithms to the data collected  from server logs. This paper presents several data preparation techniques  in order to identify unique users and user sessions. Also, a method to divide  user sessions into semantically meaningful transactions is defined  and successfully tested against two other methods. Transactions identified  by the proposed methods are used to discover association rules from  real world data using the WEBMINER system [15].
424|Fast Algorithms for Mining Association Rules|We consider the problem of discovering association rules between items in a large database of sales transactions. We present two new algorithms for solving this problem that are fundamentally different from the known algorithms. Empirical evaluation shows that these algorithms outperform the known algorithms by factors ranging from three for small problems to more than an order of magnitude for large problems. We also show how the best features of the two proposed algorithms can be combined into a hybrid algorithm, called AprioriHybrid. Scale-up experiments show that AprioriHybrid scales linearly with the number of transactions. AprioriHybrid also has excellent scale-up properties with respect to the transaction size and the number of items in the database. 
425|Mining Sequential Patterns: Generalizations and Performance Improvements|Abstract. The problem of mining sequential patterns was recently introduced in [3]. We are given a database of sequences, where each sequence is a list of transactions ordered by transaction-time, and each transaction is a set of items. The problem is to discover all sequential patterns with a user-speci ed minimum support, where the support of a pattern is the number of data-sequences that contain the pattern. An example of a sequential pattern is \5 % of customers bought `Foundation&#039; and `Ringworld &#039; in one transaction, followed by `Second Foundation &#039; in a later transaction&#034;. We generalize the problem as follows. First, we add time constraints that specify a minimum and/or maximum time period between adjacent elements in a pattern. Second, we relax the restriction that the items in an element of a sequential pattern must come from the same transaction, instead allowing the items to be present in a set of transactions whose transaction-times are within a user-speci ed time window. Third, given a user-de ned taxonomy (is-a hierarchy) on items, we allow sequential patterns to include items across all levels of the taxonomy. We present GSP, a new algorithm that discovers these generalized sequential patterns. Empirical evaluation using synthetic and real-life data indicates that GSP is much faster than the AprioriAll algorithm presented in [3]. GSP scales linearly with the number of data-sequences, and has very good scale-up properties with respect to the average datasequence size. 1
426|Efficient and Effective Clustering Methods for Spatial Data Mining|Spatial data mining is the discovery of interesting relationships and characteristics that may exist implicitly in spatial databases. In this paper, we explore whether clustering methods have a role to play in spatial data mining. To this end, we develop a new clustering method called CLARANS which is based on randomized search. We also de- velop two spatial data mining algorithms that use CLARANS. Our analysis and experiments show that with the assistance of CLARANS, these two algorithms are very effective and can lead to discoveries that are difficult to find with current spatial data mining algorithms.
428|Web Mining: Information and Pattern Discovery on the World Wide Web|Application of data mining techniques to the World Wide Web, referred to as Web mining, has been the focus of several recent research projects and papers. However, there is no established vocabulary, leading to confusion when comparing research efforts. The term Web mining has been used in two distinct ways. The first, called Web content mining in this paper, is the process of information discovery from sources across the World Wide Web. The second, called Web usage mining, is the process of mining for user browsing and access patterns. In this paper we define Web mining and present an overview of the various research issues, techniques, and development efforts. We briefly describe WEBMINER, a system for Web usage mining, and conclude this paper by listing research issues. 
429|WebWatcher: A Tour Guide for the World Wide Web|We explore the notion of a tour guide software agent for assisting users browsing the World Wide Web. A Web tour guide agent provides assistance similar to that provided by ahuman tour guide in a museum -- it guides the user along an appropriate path through the collection, based on its knowledge of the user&#039;s interests, of the location and relevance of various items in the collection, and of the way in which others have interacted with the collection in the past. This paper describes a simple but operational tour guide, called Web-Watcher, which has given over 5000 tours to people browsing CMU&#039;s School of Computer Science Web pages. WebWatcher accompanies users from page to page, suggests appropriate hyperlinks, and learns from experience to improve its advice-giving skills. We describe the learning algorithms used by WebWatcher, experimental results showing their effectiveness, and lessons learned from this case study in Web tour guide agents.  
430|Knowledge Discovery from Users Web-Page Navigation|We propose to detect users navigationpaths to the advantage of web-site owners. First, we explain the design and implementationof a profiler which captures client’s selected links and pages order, accurate page viewing time and cache references, using a Java based remote agent. The information captured by the profiler is then utilized by a knowledge discovery technique to cluster users with similar interests. We introduce a novel path clustering method based on the similarity of the history of user navigation. This approach is capable of capturing the interests of the user which could persist through several subsequent hypertext link selections. Finally, we evaluate our path clustering technique via a simulation study on a sample WWW-site. We show that depending on the level of inserted noise, we can recover the correct clusters by %10-%27 of average error margin. 1.
431|Discovering Web Access Patterns and Trends by Applying OLAP and Data Mining Technology on Web Logs|As a con#uence of data mining and WWW technologies, it is now possible to perform data mining on web logrecords collectedfrom the Internet web page access history. The behaviour of the web page readers is imprinted in the web server log #les. Analyzing and exploring regularities in this behaviour can improve system performance, enhance the quality and delivery of Internet information services to the end user, and identify population of potential customers for electronic commerce. Thus, by observing people using collections of data, data mining can bring considerable contribution to digital library designers.
432|Learning Information Retrieval Agents: Experiments with Automated Web Browsing|The current exponential growth of the Internet precipitates a need for new tools to help people cope with the volume of information. To complement recent work on creating searchable indexes of the World-Wide Web and systems for filtering incoming e-mail and Usenet news articles, we describe a system which helps users keep abreast of new and interesting information. Every day it presents a selection of interesting web pages. The user evaluates each page, and given this feedback the system adapts and attempts to produce better pages the following day. We present some early results from an AI programming class to whom this was set as a project, and then describe our current implementation. Over the course of 24 days the output of our system was compared to both randomly-selected and human-selected pages. It consistently performed better than the random pages, and was better than the human-selected pages half of the time.  Introduction  In recent years there has been a well-publicized expl...
433|Data Mining for Path Traversal Patterns in a Web Environment|In this paper, we explore a new data mining capability which involves mining path traversal patterns in a distributed information providing environment like world-wide-web. First, we convert the original sequence of log data into a set of maximal forward references and filter out the effect of some backward references which are mainly made for ease of traveling. Second, we derive algorithms to determine the frequent traversal patterns, i.e., large reference sequences, from the maximal forward references obtained. Two algorithms are devised for determining large reference sequences: one is based on some hashing and pruning techniques, and the other is further improved with the option of determining large reference sequences in batch so as to reduce the number of database scans required. Performance of these two methods is comparatively analyzed.
434|Web Mining: Pattern discovery from World Wide Web transactions|Web-based organizations often generate and collect large volumes of data in their daily operations. Analyzing such data can help these organizations to determine the life time value of clients, design cross marketing strategies across products and services, evaluate the e ectiveness of promotional campaigns, and nd the most e ective logical structure for their Web space. This type of analysis involves the discovery of meaningful relationships from a large collection of primarily unstructured data, often stored in Web server access logs. We propose a framework for Web mining, the applications of data mining and knowledge discovery techniques to data collected in World Wide Web transactions. We present data and transaction models for various Web mining tasks such as the discovery of association rules and sequential patterns from the Web data. We also present aWeb mining system, WEBMINER, which has been implemented based upon the proposed framework, and discuss our experimental results on real-world Web data using the WEBMINER.
435|Learning from hotlists and coldlists: Towards a WWW information filtering and seeking agent|We describe a software agent that learns to find information on the World Wide Web (WWW), deciding what new pages might interest a user. The agent maintains a separate hotlist (for links that were interesting) and coldlist (for links that were not interesting) for each topic. By analyzing the information immediately accessible from each link, the agent learns the types of information the user is interested in. This can be used to inform the user when a new interesting page becomes available or to order the user&#039;s exploration of unseen existing links so that the more promising ones are investigated first. We compare four different learning algorithms on this task. We describe an experiment in which a simple Bayesian classifier acquires a user profile that agrees with a user&#039;s judgment over 90% of the time.
436|Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks|Dryad is a general-purpose distributed execution engine for coarse-grain data-parallel applications. A Dryad applica-tion combines computational “vertices ” with communica-tion “channels ” to form a dataflow graph. Dryad runs the application by executing the vertices of this graph on a set of available computers, communicating as appropriate through files, TCP pipes, and shared-memory FIFOs. The vertices provided by the application developer are quite simple and are usually written as sequential programs with no thread creation or locking. Concurrency arises from Dryad scheduling vertices to run simultaneously on multi-ple computers, or on multiple CPU cores within a computer. The application can discover the size and placement of data at run time, and modify the graph as the computation pro-gresses to make efficient use of the available resources. Dryad is designed to scale from powerful multi-core sin-gle computers, through small clusters of computers, to data centers with thousands of computers. The Dryad execution engine handles all the difficult problems of creating a large distributed, concurrent application: scheduling the use of computers and their CPUs, recovering from communication or computer failures, and transporting data between ver-tices.
437|The click modular router| Click is a new software architecture for building flexible and configurable routers. A Click router is assembled from packet processing modules called elements. Individual elements implement simple router functions like packet classification, queueing, scheduling, and interfacing with network devices. A router configuration is a directed graph with elements at the vertices; packets flow along the edges of the graph. Configurations are written in a declarative language that supports user-defined abstractions. This language is both readable by humans and easily manipulated by tools. We present language tools that optimize router configurations and ensure they satisfy simple invariants. Due to Click’s architecture and language, Click router configurations are modular and easy to extend. A standards-compliant Click IP router has sixteen elements on its forwarding path. We present extensions to this router that support dropping policies, fairness among flows, quality-of-service, and
438|PVM: A Framework for Parallel Distributed Computing|The PVM system is a programming environment for the development and execution of large concurrent or parallel applications that consist of many interacting, but relatively independent, components. It is intended to operate on a collection of heterogeneous computing elements interconnected by one or more networks. The participating processors may be scalar machines, multiprocessors, or special-purpose computers, enabling application components to execute on the architecture most appropriate to the algorithm. PVM provides a straightforward and general interface that permits the description of various types of algorithms (and their interactions), while the underlying infrastructure permits the execution of applications on a virtual computing environment that supports multiple parallel computation models. PVM contains facilities for concurrent, sequential, or conditional execution of application components, is portable to a variety of architectures, and supports certain forms of error dete...
439|Cilk: An Efficient Multithreaded Runtime System|Cilk (pronounced &#034;silk&#034;) is a C-based runtime system for multithreaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the &#034;work&#034; and &#034;critical-path length&#034; of a Cilk computation can be used to model performance accurately. Consequently, a Cilk programmer can focus on reducing the computation&#039;s work and critical-path length, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of &#034;fully strict&#034; (well-structured) programs, the Cilk scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The Cilk
440|Distributed Computing in Practice: The Condor Experience|Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational grid. In this chapter, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course traveled by research ideas as they grow into production systems.
441|Cluster-Based Scalable Network Services|This paper has benefited from the detailed and perceptive comments of our reviewers, especially our shepherd Hank Levy. We thank Randy Katz and Eric Anderson for their detailed readings of early drafts of this paper, and David Culler for his ideas on TACC&#039;s potential as a model for cluster programming. Ken Lutz and Eric Fraser configured and administered the test network on which the TranSend scaling experiments were performed. Cliff Frost of the UC Berkeley Data Communications and Networks Services group allowed us to collect traces on the Berkeley dialup IP network and has worked with us to deploy and promote TranSend within Berkeley. Undergraduate researchers Anthony Polito, Benjamin Ling, and Andrew Huang implemented various parts of TranSend&#039;s user profile database and user interface. Ian Goldberg and David Wagner helped us debug TranSend, especially through their implementation of the rewebber
443|The Gamma database machine project|This paper describes the design of the Gamma database machine and the techniques employed in its implementation. Gamma is a relational database machine currently operating on an Intel iPSC/2 hypercube with 32 processors and 32 disk drives. Gamma employs three key technical ideas which enable the architecture to be scaled to 100s of processors. First, all relations are horizontally partitioned across multiple disk drives enabling relations to be scanned in parallel. Second, novel parallel algorithms based on hashing are used to implement the complex relational operators such as join and aggregate functions. Third, dataflow scheduling techniques are used to coordinate multioperator queries. By using these techniques it is possible to control the execution of very complex queries with minimal coordination- a necessity for configurations involving a very large number of processors. In addition to describing the design of the Gamma software, a thorough performance evaluation of the iPSC/2 hypercube version of Gamma is also presented. In addition to measuring the effect of relation size and indices on the response time for selection, join, aggregation, and update queries, we also analyze the performance of Gamma relative to the number of processors employed when the sizes of the input relations are kept constant (speedup) and when the sizes of the input relations are increased proportionally to the number of processors (scaleup). The speedup results obtained for both selection and join queries are linear; thus, doubling the number of processors
444|Programmable stream processors|The Imagine Stream Processor is a single-chip programmable media processor with 48 parallel ALUs. At 400 MHz, this translates to a peak arithmetic rate of 16 GFLOPS on single-precision data and 32 GOPS on 16bit fixed-point data. The scalability of Imagine’s programming model and architecture enable it to achieve such high arithmetic rates. Imagine executes applications that have been mapped to the stream programming model. The stream model decomposes applications into a set of computation kernels that operate on data streams. This mapping exposes the inherent locality and parallelism in the application, and Imagine exploits the locality and parallelism to provide a scalable architecture that supports 48 ALUs on a single chip. This paper presents the Imagine architecture and programming model in the first half, and explores the scalability of the Imagine architecture in the second half. 1.
445|Accelerator: using data parallelism to program GPUs for general-purpose uses|GPUs are difficult to program for general-purpose uses. Programmers can either learn graphics APIs and convert their applications to use graphics pipeline operations or they can use stream programming abstractions of GPUs. We describe Accelerator, a system that uses data parallelism to program GPUs for general-purpose uses instead. Programmers use a conventional imperative programming language and a library that provides only high-level data-parallel operations. No aspects of GPUs are exposed to programmers. The library implementation compiles the data-parallel operations on the fly to optimized GPU pixel shader code and API calls. We describe the compilation techniques used to do this. We evaluate the effectiveness of using data parallelism to program GPUs by providing results for a set of compute-intensive benchmarks. We compare the performance of Accelerator versions of the benchmarks against hand-written pixel shaders. The speeds of the Accelerator versions are typically within 50 % of the speeds of hand-written pixel shader code. Some benchmarks significantly outperform C versions on a CPU: they are up to 18 times faster than C code running on a CPU.
446|Using Cohort Scheduling to Enhance Server Performance|A server application is commonly organized as a collection of concurrent threads, each of which executes the code necessary to process a request. This software architecture, which causes frequent control transfers between unrelated pieces of code, decreases instruction and data locality, and consequently reduces the effec- tiveness of hardware mechanisms such as caches, TLBs, and branch predictors. Numerous measurements demonstrate this effect in server applications, which often utilize only a fraction of a modern processor&#039;s computational throughput.
447|The CODE 2.0 Graphical Parallel Programming Language |CODE 2.0 is a graphical parallel programming system that targets the three goals of ease of use, portability, and production of efficient parallel code. Ease of use is provided by an integrated graphical/textual interface, a powerful dynamic model of parallel computation, and an integrated concept of program component reuse. Portability is approached by the declarative expression of synchronization and communication operators at a high level of abstraction in a manner which cleanly separates overall computation structure from the primitive sequential computations that make up a program. Execution efficiency is approached through a systematic class hierarchy that supports hierarchical translation refinement including special case recognition. This paper reports results obtained through experimental use of a prototype implementation of the CODE 2.0 system. CODE 2.0 represents a major conceptual advance over its predecessor systems (CODE 1.0 and CODE 1.2) in terms of the expressive power ...
448|Stream Computations Organized for Reconfigurable Execution (SCORE): Introduction and Tutorial  (2000) |A primary impediment to wide-spread exploitation of reconfigurable computing is the lack of a unifying computational model which allows application portability and longevity without sacrificing a substantial fraction of the raw capabilities. We introduce SCORE (Stream Computation Organized for Reconfigurable Execution), a streambased compute model which virtualizes reconfigurable computing resources (compute, storage, and communication) by dividing a computation up into fixed-size &#034;pages&#034; and time-multiplexing the virtual pages on available physical hardware. Consequently, SCORE applications can scale up or down automatically to exploit a wide range of hardware sizes. We hypothesize that the SCORE model will ease development and deployment of reconfigurable applications and expand the range of applications which can benefit from reconfigurable execution. Further, we believe that a well engineered SCORE implementation can be efficient, wasting little of the capabilities of the raw hardw...
449|Paralex: An Environment for Parallel Programming in Distributed Systems|Modern distributed systems consisting of powerful workstations and high-speed interconnection networks are an economical alternative to special-purpose super computers. The technical issues that need to be addressed in exploiting the parallelism inherent in a distributed system include heterogeneity, high-latency communication, fault tolerance and dynamic load balancing. Current software systems for parallel programming provide little or no automatic support towards these issues and require users to be experts in fault-tolerant distributed computing. The Paralex system is aimed at exploring the extent to which the parallel application programmer can be liberated from the complexities of distributed systems. Paralex is a complete programming environment and makes extensive use of graphics to define, edit, execute and debug parallel scientific applications. All of the necessary code for distributing the computation across a network and replicating it to achieve fault tolerance and dynamic load balancing is automatically generated by the system. In this paper we give an overview of Paralex and present our experiences with a prototype implementation.
450|Parallel and Distributed Haskells|Parallel and distributed languages specify computations on multiple processors and have a computation language to describe the algorithm, i.e. what to compute, and a coordination language to describe how to organise the computations across the processors. Haskell has been used as the computation language for a wide variety of parallel and distributed languages, and this paper is a comprehensive survey of implemented languages. We outline parallel and distributed language concepts and classify Haskell extensions using them. Similar example programs are used to illustrate and contrast the coordination languages, and the comparison is facilitated by the common computation language. A lazy language is not an obvious choice for parallel or distributed computation, and we address the question of why Haskell is a common functional computation language.
451|Highly Available, Fault-Tolerant, Parallel Dataflows|We present a technique that masks failures in a cluster to provide high availability and fault-tolerance for long-running, parallelized dataflows. We can use these dataflows to implement a variety of continuous query (CQ) applications that require high-throughput, 24x7 operation. Examples include network monitoring, phone call processing, click-stream processing, and online financial analysis. Our main contribution is a scheme that carefully integrates traditional query processing techniques for partitioned parallelism with the process-pairs approach for high availability. This delicate integration allows us to tolerate failures of portions of a parallel dataflow  without sacrificing result quality. Upon failure, our technique provides quick fail-over, and automatically recovers the lost pieces on the fly. This piecemeal recovery provides minimal disruption to the ongoing dataflow computation and improved reliability as compared to the straight-forward application of the process-pairs technique on a per dataflow basis. Thus, our technique provides the high availability necessary for critical CQ applications. Our techniques are encapsulated in a reusable dataflow operator called Flux, an extension of the Exchange that is used to compose parallel dataflows. Encapsulating the fault-tolerance logic into Flux minimizes modifications to existing operator code and relieves the burden on the operator writer of repeatedly implementing and verifying this critical logic. We present experiments illustrating these features with an implementation of Flux in the TelegraphCQ code base [8].
452|A comparison of stream-oriented high availability algorithms|Recently, significant efforts have focused on developing novel data-processing systems to support a new class of applications that commonly require sophisticated and timely processing of high-volume data streams. Early work in stream processing has primarily focused on streamoriented languages and resource-constrained, one-pass query-processing. High availability, an increasingly important goal for virtually all data processing systems, is yet to be addressed. In this paper, we first describe how the standard highavailability approaches used in data management systems can be applied to distributed stream processing. We then propose a novel stream-oriented approach that exploits the unique data-flow nature of streaming systems. Using analysis and a detailed simulation study, we characterize the performance of each approach and demonstrate that the stream-oriented algorithm significantly reduces runtime overhead at the expense of a small increase in recovery time. 1
453|The 4+1 view model of architecture|The 4+1 View Model organizes a description of a software architecture using five concurrent views, each of which
addresses a specific set of concerns. Architects capture their design decisions in four views and use the fifth view to illustrate and validate them.
454|A Study|on the rubrene emission sensitized by a phosphorescent Ir compound in the host of CBP
455|From Domain Model to Architectures|A software system can be evaluated against criteria in two broad categories: • functional and performance attributes: how well does the system, during execution, satisfy its behavioral, functional, and performance requirements? Does it provide the required results? Does it provide them in a timely enough manner? Are the results correct, or within specified accuracy and stability tolerances? • non-functional attributes: how easy is the system to integrate, test, and modify? How expensive was it to develop? These two categories are orthogonal; systems that unfailingly meet all of their requirements may or may not have been prohibitively expensive to develop, and may or may not be impossible to modify. Highly modifiable systems may or may not produce correct results. Given a set of requirements for a system, the developer must choose an architecture that will allow the implementation of the system to proceed in a straightforward manner, producing a product that meets its functional and non-functional requirements. How is that done? 1.1 Producing architectures to meet functional requirements There is, unfortunately, no reliable automatic or semi-automatic technology that will produce
456|Elastically deformable models|The goal of visual modeling research is to develop mathematical models and associated algorithms for the analysis and synthesis of visual information. Image analysis and synthesis characterize the domains of computer vision and computer graphics, respectively. For nearly three decades, the vision and graphics fields have been developing almost entirely independently—this despite the fact that, at least conceptually, the two disciplines are bound in a mutually converse relationship. Graphics, the direct problem, involves the synthesis of images from object models, whereas vision, the inverse problem, involves the analysis of images to infer object models. Visual modeling takes a unified approach to vision and graphics via modeling that exploits computational physics. In addition to geometry, physics-based modeling employs forces, torques, internal strain energies, and other physical quantities to control the creation and evolution of models. Mathematically, the approach prescribes systems of dynamic (ordinary and partial) differential equations to govern model behavior. These equations of motion may be
457|Symmetry-seeking models and 3D object reconstruction|We propose models of 3D shape which may be viewed as deformable bodies composed of simulated elastic material. In contrast to traditional, purely geometric models of shape, deformable models are active--their shapes change in response to externally applied forces. We develop a deformable model for 3D shape which has a preference for axial symmetry. Symmetry is represented even though the model does not belong to a parametric shape family such as (generalized) cylinders. Rather, a symmetry-seeking property is designed into internal forces that constrain the deformations of the model. We develop a framework for 3D object reconstruction based on symmetry-seeking models. Instances of these models are formed from monocular image data through the action of external forces derived from the data. The forces proposed in this paper deform the model in space so that the shape of its projection into the image plane is consistent with the 2D silhouette of an object of interest. The effectiveness of our approach is demonstrated using natural images. 1
458|Signal Matching Through Scale Space|Given a collection of similar signals that have been deformed with respect to each other, the general signal-matching problem is to recover the deformation. We formulate the problem as the minimization of an energy measure that combines a smoothness term and a similarity term. The minimization reduces to a dynamic system governed by a set of coupled, first-order differential equations. The dynamic system finds an optimal solution at a coarse scale and then tracks it continuously to a fine scale. Among the major themes in recent work on visual signal matching have been the notions of matching as constrained opti-mization, of variational surface reconstruction, and of coarse-to-fine matching. Our solution captures these in a precise, succinct, and unified form. Results are presented for one-dimensional signals, a motion sequence, and a stereo pair. 1
459|Sampling and Reconstruction with Adaptive Meshes|This paper introduces an approach to visual sampling and reconstruction motivated by concepts from numerical grid generation. We develop adaptive meshes that can nonuniformly sample and reconstruct intensity and range data. Adaptive meshes are dynamic models which are assembled by interconnecting nodal masses with adjustable springs. Acting as mobile sampling sites, the nodes observe interesting properties of the input data, such as intensities, depths, gradients, and curvatures. Based on these nodal observations, the springs automatically adjust their stiffnesses so as to distribute the available degrees of freedom of the reconstructed model in accordance with the local complexity of the input data. The adaptive mesh algorithm runs at interactive rates with continuous 3D display on a graphics workstation. We apply it to the adaptive sampling and reconstruction of images and surfaces.
460|Eraser: a dynamic data race detector for multithreaded programs|Multi-threaded programming is difficult and error prone. It is easy to make a mistake in synchronization that produces a data race, yet it can be extremely hard to locate this mistake during debugging. This paper describes a new tool, called Eraser, for dynamically detecting data races in lock-based multi-threaded programs. Eraser uses binary rewriting techniques to monitor every shared memory reference and verify that consistent locking behavior is observed. We present several case studies, including undergraduate coursework and a multi-threaded Web search engine, that demonstrate the effectiveness of this approach. 1
461|Time, Clocks, and the Ordering of Events in a Distributed System|The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.  
462|Atom: A system for building customized program analysis tools|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
463|Monitors: An Operating System Structuring Concept|This is a digitized copy derived from an ACM copyrighted work. It is not guaranteed to be an accurate copy of the author&#039;s original work. This paper develops Brinch-Hansen&#039;s concept of a monitor as a method of structuring an operating system. It introduces a form of synchronization, describes a possible rnctltotl of implementation in terms of semaphorcs and gives a suitable proof rule. Illustrative examples include a single rcsourcc scheduler, a bounded buffer, an alarm clock, a buffer pool, a disk head optimizer, and a version of the problem of readers and writers. Key Words and Phrases: monitors, operating systems,schcduling, mutual exclusion, synchronization, system implementation langua yes, structured multiprogramming CR Categories:
464|Extensibility, safety and performance in the SPIN operating system|This paper describes the motivation, architecture and performance of SPIN, an extensible operating system. SPIN provides an extension infrastructure, together with a core set of extensible services, that allow applications to safely change the operating system&#039;s interface and implementation. Extensions allow an application to specialize the underlying operating system in order to achieve a particular level of performance and functionality. SPIN uses language and link-time mechanisms to inexpensively export ne-grained interfaces to operating system services. Extensions are written in a type safe language, and are dynamically linked into the operating system kernel. This approach o ers extensions rapid access to system services, while protecting the operating system code executing within the kernel address space. SPIN and its extensions are written in Modula-3 and run on DEC Alpha workstations. 1
465|Petal: Distributed Virtual Disks|The ideal storage system is globally accessible, always available, provides unlimited performance and capacity for a large number of clients, and requires no management. This paper describes the design, implementation, and performance of Petal, a system that attempts to approximate this ideal in practice through a novel combination of features. Petal consists of a collection of networkconnected servers that cooperatively manage a pool of physical disks. To a Petal client, this collection appears as a highly available block-level storage system that provides large abstract containers called virtual disks. A virtual disk is globally accessible to all Petal clients on the network. A client can create a virtual disk on demand to tap the entire capacity and performance of the underlying physical resources. Furthermore, additional resources, such as servers and disks, can be automatically incorporated into Petal. We have an initial Petal prototype consisting of four 225 MHz DEC 3000/700 work...
467|Shasta: A LowOverhead Software-Only Approach to Fine-Grain Shared Memory|Digital Equipment Corporation This paper describes Shasta, a system that supports a shared address space in software on clusters of computers with physically distributed memory. A unique aspect of Shasta compared to most other software distributed shared memory systems is that shared data can be kept coherent at a fine granularity. In addition, the system allows the coherence granularity to vary across different shared data structures in a single application. Shasta implements the shared address space by transparently rewriting the application executable to intercept loads and stores. For each shared load or store, the inserted code checks to see if the data is available locally and communicates with other processors if necessary. The system uses numerous techniques to reduce the run-time overhead of these checks. Since Shasta is implemented entirely in software,
468|Experience with Processes and Monitors in Mesa|The use of monitors for describing concurrency has been much discussed in the literature. When monitors are used in real systems of any size, however, a number of problems arise which have not been adequately dealt with: the semantics of nested monitor calls; the various ways of defining the meaning of WAIT; priority scheduling; handling of timeouts, aborts and other exceptional conditions; interactions with process creation and destruction; monitoring large numbers of small objects. These problems are addressed by the facilities described here for concurrent programming in Mesa. Experience with several substantial applications gives us some confidence in the validity of our solutions.
469|On-the-fly Detection of Data Races for Programs with Nested Fork-Join Parallelism|Detecting data races in shared-memory parallel programs is an important debugging problem. This paper presents a new protocol for run-time detection of data races in executions of shared-memory programs with nested fork-join parallelism and no other interthread synchronization. This protocol has significantly smaller worst-case run-time overhead than previous techniques. The worst-case space required by our protocol when monitoring an execution of a program P is O(V N), where V is the number of shared variables in P , and N is the maximum dynamic nesting of parallel constructs in P &#039;s execution. The worst-case time required to perform any monitoring operation is O(N). We formally prove that our new protocol always reports a non-empty subset of the data races in a monitored program execution and describe how this property leads to an e#ective debugging strategy.
470|Online Data-Race Detection via Coherency Guarantees|We present the design and evaluation of an on-thefly data-race-detection technique that handles applications written for the lazy release consistent (LRC) shared memory model. We require no explicit association between synchronization and shared memory. Hence, shared accesses have to be tracked and compared at the minimum granularity of data accesses, which is typically a single word.  The novel aspect of this system is that we are able to leverage information used to support the underlying memory abstraction to perform on-the-fly data-race detection, without compiler support. Our system consists of a minimally modified version of the CVM distributed shared memory system, and instrumentation code inserted by the ATOM code re-writer.  We present an experimental evaluation of our technique by using our system to look for data races in four unaltered programs. Our system correctly found read-write data races in a program that allows unsynchronized read access to a global tour bound, and a write-write race in a program from a standard benchmark suite. Overall, our mechanism reduced program performance by approximately a factor of two.   
471|Compile-time Support for Efficient Data Race Detection in Shared-Memory Parallel Programs|Data race detection strategies based on software runtime monitoring are notorious for dramatically inflating execution times of shared-memory parallel programs. Without significant reductions in the execution overhead incurred when using these techniques, there is little hope that they will be widely used. A promising approach to this problem is to apply compile-time analysis to identify variable references that need not be monitored at run time because they will never be involved in a data race. In this paper we describe eraser, a data race instrumentation tool that uses aggressive program analysis to prune the number of references to be monitored. To quantify the effectiveness of our analysis techniques, we compare the overhead of race detection with three levels of compile-time analysis ranging from little analysis to aggressive interprocedural analysis for a suite of test programs. For the programs tested, using interprocedural analysis and dependence analysis dramatically reduced ...
472|The Nash Bargaining Solution in Economic Modeling|This article establishes the relationship between the static axiomatic theory of bargaining and the sequential strategic approach to bargaining. We consider two strategic models of alternating offers. The models differ in the source of the incentive of the bargaining parties to reach agreement: the bargainers &#039; time preference and the risk of breakdown of negotiation. Each of the models has a unique perfect equilibrium. When the motivation to reach agreement is made negligible, in each model the unique perfect equilibrium outcome approaches the Nash bargaining solution, with utilities that reflect the incentive to settle and with the proper disagreement jfoint chosen. The results provide a guide for the application of the Nash bar-gaining solution in economic modelling. 1.
474|Data mules: Modeling a three-tier architecture for sparse sensor networks|Abstract — This paper presents and analyzes an architecture that exploits the serendipitous movement of mobile agents in an environment to collect sensor data in sparse sensor networks. The mobile entities, called MULEs, pick up data from sensors when in close range, buffer it, and drop off the data to wired access points when in proximity. This leads to substantial power savings at the sensors as they only have to transmit over a short range. Detailed performance analysis is presented based on a simple model of the system incorporating key system variables such as number of MULEs, sensors and access points. The performance metrics observed are the data success rate (the fraction of generated data that reaches the access points) and the required buffer capacities on the sensors and the MULEs. The modeling along with simulation results can be used for further analysis and provide certain guidelines for deployment of such systems. I.
475|An Energy-Efficient MAC Protocol for Wireless Sensor Networks|This paper proposes S-MAC, a medium-access control (MAC) protocol designed for wireless sensor networks. Wireless sensor networks use battery-operated computing and sensing devices. A network of these devices will collaborate for a common application such as environmental monitoring. We expect sensor networks to be deployed in an ad hoc fashion, with individual nodes remaining largely inactive for long periods of time, but then becoming suddenly active when something is detected. These characteristics of sensor networks and applications motivate a MAC that is different from traditional wireless MACs such as IEEE 802.11 in almost every way: energy conservation and self-configuration are primary goals, while per-node fairness and latency are less important. S-MAC uses three novel techniques to reduce energy consumption and support self-configuration. To reduce energy consumption in listening to an idle channel, nodes periodically sleep. Neighboring nodes form virtual clusters to auto-synchronize on sleep schedules. Inspired by PAMAS, S-MAC also sets the radio to sleep during transmissions of other nodes. Unlike PAMAS, it only uses in-channel signaling. Finally, S-MAC applies message passing to reduce contention latency for sensor-network applications that require store-andforward processing as data move through the network. We evaluate our implementation of S-MAC over a sample sensor node, the Mote, developed at University of California, Berkeley. The experiment results show that, on a source node, an 802.11-like MAC consumes 2--6 times more energy than S-MAC for traffic load with messages sent every 1-10s.
476|Mobility increases the capacity of ad-hoc wireless networks|The capacity of ad-hoc wireless networks is constrained by the mutual interference of concurrent transmissions between nodes. We study a model of an ad-hoc network where n nodes communicate in random source-destination pairs. These nodes are assumed to be mobile. We examine the per-session throughput for applications with loose delay constraints, such that the topology changes over the time-scale of packet delivery. Under this assumption, the per-user throughput can increase dramatically when nodes are mobile rather than fixed. This improvement can be achieved by exploiting node mobility as a type of multiuser diversity.  
477|A Survey of Mobility Models for Ad Hoc Network Research|In the performance evaluation of a protocol for an ad hoc network, the protocol should be tested under realistic conditions including, but not limited to, a sensible transmission range, limited buffer space for the storage of messages, representative data traffic models, and realistic movements of the mobile users (i.e., a mobility model). This paper is a survey of mobility models that are used in the simulations of ad hoc networks. We describe several mobility models that represent mobile nodes whose movements are independent of each other (i.e., entity mobility models) and several mobility models that represent mobile nodes whose movements are dependent on each other (i.e., group mobility models). The goal of this paper is to present a number of mobility models in order to offer researchers more informed choices when they are deciding upon a mobility model to use in their performance evaluations. Lastly, we present simulation results that illustrate the importance of choosing a mobility model in the simulation of an ad hoc network protocol. Specifically, we illustrate how the performance results of an ad hoc network protocol drastically change as a result of changing the mobility model simulated.
478|Energy-Efficient Computing for Wildlife Tracking: Design Tradeoffs and Early Experiences with ZebraNet|Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The  eld of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scienti  c and commercial purposes. This paper examines the research decisions and design tradeos that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.
479|Next Century Challenges: Mobile Networking for “Smart Dust”|Large-scale networks of wireless sensors are becoming an active topic of research. Advances in hardware technology and engineering design have led to dramatic reductions in size, power consumption and cost for digital circuitry, wire-less communications and Micro ElectroMechanical Systems (MEMS). This has enabled very compact, autonomous and mobile nodes, each containing one or more sensors, computation and communication capabilities, and a power supply. The missing ingredient is the networking and applications layers needed to harness this revolutionary capability into a complete system. We review the key elements of the emergent technology of “Smart Dust ” and outline the research challenges they present to the mobile networking and systems community, which must provide coherent connectivity to large numbers of mobile network nodes co-located within a small volume. 
481|Epidemic routing for partially-connected ad hoc networks|Mobile ad hoc routing protocols allow nodes with wireless adaptors to communicate with one another without any pre-existing network infrastructure. Existing ad hoc routing protocols, while robust to rapidly changing network topology, assume the presence of a connected path from source to destination. Given power limitations, the advent of short-range wireless networks, and the wide physical conditions over which ad hoc networks must be deployed, in some scenarios it is likely that this assumption is invalid. In this work, we develop techniques to deliver messages in the case where there is never a connected path from source to destination or when a network partition exists at the time a message is originated. To this end, we introduce Epidemic Routing, where random pair-wise exchanges of messages among mobile hosts ensure eventual message delivery. The goals of Epidemic Routing are to: i) maximize message delivery rate, ii) minimize message latency, and iii) minimize the total resources consumed in message delivery. Through an implementation in the Monarch simulator, we show that Epidemic Routing achieves eventual delivery of 100 % of messages with reasonable aggregate resource consumption in a number of interesting scenarios. 1
482|Connectivity in Ad-Hoc and Hybrid Networks|We consider a large-scale wireless network, but with a low density of nodes per unit area. Interferences are then less critical, contrary to connectivity. This paper studies the latter property for both a purely ad-hoc network and a hybrid network, where fixed base stations can be reached in multiple hops. We assume here that power constraints are modeled by a maximal distance above which two nodes are not (directly) connected. We find that
483|Smooth is Better than Sharp: A Random Mobility Model for Simulation of Wireless Networks|This paper presents an enhanced random mobility model for simulation-based studies of wireless networks. Our approach makes the movement trace of individual mobile stations more realistic than common approaches for random movement. After giving a survey of mobility models found in the literature, we give a detailed mathematical formulation of our model and outline its advantages. The movement concept is based on random processes for speed and direction control in which the new values are correlated to previous ones. Upon a speed change event, a new target speed is chosen, and an acceleration is set to achieve this target speed. The principles for a direction change are similar. Moreover, we propose two extensions for modeling typical movement patterns of vehicles. Finally, we consider strategies for the nodes&#039; border behavior (i.e., what happens when nodes move out of the simulation area) and point out a pitfall that occurs when using a bounded simulation area.
484|Sending Messages to Mobile Users in Disconnected Ad-Hoc Wireless Networks|An ad-hoc network is formed by a group of mobile hosts upon a wireless network interface. Previous research in this area has concentrated on routing algorithms which are designed for fully connected networks. The usual way to deal with a disconnected ad-hoc network is to let the mobile computer wait for network reconnection passively, which may lead to unacceptable transmission delays. In this paper, we propose an approach that guarantees message transmission in minimal time. In this approach, mobile hosts actively modify their trajectories to transmit messages. We develop algorithms that minimize the trajectory modifications under two different assumptions: (a) the movements of all the nodes in the system are known and (b) the movements of the hosts in the system are not known. 1.
485|Smart-Tag Based Data Dissemination|Monitoring wide, hostile areas requires disseminating data between fixed, disconnected clusters of sensor nodes. It is not always possible to install long-range radios in order to cover the whole area. We propose to leverage the movement of mobile individuals, equipped with smart-tags, to disseminate data across disconnected static nodes spread across a wide area. Static nodes and mobile smart-tags exchange data when they are in the vicinity of each other; smart-tags disseminate data as they move around. In this paper, we propose an algorithm for update propagation and a model for smart-tag based data dissemination. We use simulation to study the characteristics of the model we propose. Finally, we present an implementation based on bluetooth smart-tags.
486|An Efficient Communication Strategy for Ad-hoc Mobile Networks|Abstract. We investigate here the problem of establishing communication in an ad-hoc mobile network, that is, we assume the extreme case of a total absense of any fixed network infrastructure (for example a case of rapid deployment of a set of mobile hosts in an unknown terrain). We propose, in such a case, that a small subset of the deployed hosts (which we call the support) should be used to manage network operations. However, the vast majority of the hosts are moving arbitrarily according to application needs. We then provide a simple, correct and efficient protocol for communication establishment that avoids message flooding. Our protocol manages to establish communication between any pair of mobile hosts in small, apriori guaranteed expected time bounds even in the worst case of arbitrary motions of the hosts that do not belong to the support (provided that they do not deliberately try to avoid the support). These time bounds, interestingly, do not depend, on the number of mobile hosts that do not belong in the support. They depend only on the size of the area of motions. Our protocol can be implemented in very efficient ways by exploiting knowledge of the space of motions or by adding more power to the hosts of the support. Our results exploit and further develop some fundamental properties of random walks in finite graphs. 1
487|Wireless Subscriber Mobility Management using Adaptive Individual Location Areas for PCS Systems|We consider a new mobility management scheme -- the Adaptive Location Area Tracking Scheme, in which each mobile performs a location registration as it crosses the boundary of its current personal location area and is assigned a new location area. The size and shape of the new location area depend on the mobile&#039;s mobility and call characteristics in its previous location area. The objective is to minimize the combined average signaling cost of both paging and registration activities for each individual mobile user.
488|Probability Criterion Based Location Tracking Approach for Mobility Management of Personal Communications Systems|We consider a probability criterion based location tracking scheme for personal communications systems. This scheme seeks to minimize the average signaling cost due to both paging and registration for individual mobile users. A timer-based location update and a single-step dynamic paging procedure are used. Each user-network interaction resets the timer and modifies location record. The user registers when the timer expires. Users are paged over personally constructed  paging areas. The size of these areas is an increasing function of the time since last user-network contact. The shape of the paging area depends on the particular probability distribution of user location. The problem formulation is applicable to arbitrary motion models assuming the associated user location distribution is available or can be estimated. As an illustrative example, we use a Brownian motion process for user motion and assume Poisson call arrivals. We then investigate the influence of user mobility and cal...
489|A Compositional Approach to Performance Modelling|Performance modelling is concerned with the capture and analysis of the dynamic behaviour of computer and communication systems. The size and complexity of many modern systems result in large, complex models. A compositional approach decomposes the system into subsystems that are smaller and more easily modelled. In this thesis a novel compositional approach to performance modelling is presented. This approach is based on a suitably enhanced process algebra, PEPA (Performance Evaluation Process Algebra). The compositional nature of the language provides benefits for model solution as well as model construction. An operational semantics is provided for PEPA and its use to generate an underlying Markov process for any PEPA model is explained and demonstrated. Model simplification and state space aggregation have been proposed as means to tackle the problems of large performance models. These techniques are presented in terms of notions of equivalence between modelling entities. A framewo...
490|The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain|If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the
491|Modeling Strategic Relationships for Process Reengineering|Existing models for describing a process (such as a business process or a software development process) tend to focus on the \what &#034; or the \how &#034; of the process. For example, a health insurance claim process would typically be described in terms of a number of steps for assessing and approving a claim. In trying to improve orredesign a process, however, one also needs to have an understanding of the \why &#034;  { for example, why dophysicians submit treatment plans to insurance companies before giving treatment? and why do claims managers seek medical opinions when assessing treatment plans? An understanding of the motivations and interests of process participants is often crucial to the successful redesign of processes. This thesis proposes a modelling framework i (pronounced i-star) consisting of two modelling components. The Strategic Dependency (SD) model describes a process in terms of intentional dependency relationships among agents. Agents depend on each other for goals to be achieved, tasks to be performed, and resources to be furnished. Agents are intentional in that they have desires and wants, and strategic in that they are concerned about opportunities and vulnerabilities. The Strategic Rationale (SR) model describes the issues and concerns that agents
492|A Field Study of the Software Design Process for Large Systems|The problems of designing large software systems were studied through interviewing personnel from 17 large projects. A layered behavioral model is used to analyze how three lgf these problems-the thin spread of application domain knowledge, fluctuating and conflicting requirements, and communication bottlenecks and breakdowns-affected software productivity and quality through their impact on cognitive, social, and organizational processes.
493|Value-Based Software Engineering|Abstract—This paper provides a definition of the term “software engineering ” and a survey of the current state of the art and likely future trends in the field. The survey covers the technology available in the various phases of the software life cycle—requirements engineering, design, coding, test, and maintenance—and in the overall area of software management and integrated technology-management approaches. It is oriented primarily toward discussing the domain of applicability of techniques (where and when they work), rather than how they work in detail. To cover the latter, an extensive set of 104 references is provided. Index Terms—Computer software, data systems, information systems,
494|Representing and Using Non-Functional Requirements: A Process-Oriented Approach|The paper proposes a comprehensive framework for representing and using non-functional requirements during the development process. The framework consists of five basic components which provide for the representation of non-functional requirements in terms of interrelated goals. Such goals can be refined through refinement methods and can be evaluated in order to determine the degree to which a set of non-functional requirements is supported by a particular design. Evidence for the power of the framework is provided through the study of accuracy and performance requirements for information systems.  1 
495|Telos: Representing Knowledge About Information Systems|This paper describes a language that is intended to support software engineers in the development of information systems throughout the software lifecycle. This language is not a programming language. Following the example of a number of other software engineering projects, our work is based on the premise that information system development is knowledge-intensive and that the primary responsibility of any language intended to support this task is to be able to formally represent the relevant knowledge.
496|Knowledge Representation and Reasoning in the Design of Composite Systems|Abstract- Our interest is in the design process that spans the gap between the requirements acquisition process and the implementation process, in which the basic architecture of a system is defined, and functions are allocated to software, hard-ware, and human agents. We call this process composite system design. Our goal is an interactive model of composite system design incorporating deficiency-driven design, formal analysis, incremental design and rationalization, and design reuse. We discuss knowledge representations and reasoning techniques for the product (composite system) that we are designing, and for the design process, which support these goals. To evaluate our model, we report on its use to rationally reconstruct the design of two existing composite systems. Index Terms-Automated analysis, composite systems, knowl-edge-based design, rational reconstruction, software specification. 0
497|Characterizing and Assessing a Large-Scale Software Maintenance Organization|One important component of a software process is the organizational context in which the process is enacted. This component is often missing or incomplete in current process modeling approaches. One technique for modeling this perspective is the Actor-Dependency (AD) Model. This paper reports on a case study which used this approach to analyze and assess a large software maintenance organization. Our goal was to identify the approach&#039;s strengths and weaknesses while providing practical recommendations for improvement. The AD model was found to be very useful in capturing the important properties of the organizational context of the maintenance process, and aided in the understanding of the flaws found in this process. However, a number of opportunities for extending and improving the AD model were identified. Among others, there is a need to incorporate quantitative information to complement the qualitative model. 1. Introduction It has now been recognized that, in order to improve the...
498|Goal-Based Process Analysis: A Method for Systematic Process Redesign|A method is proposed for systematically analyzing and redesigning processes. The method, Goal-based Process Analysis (GPA), helps its user to systematically identify missing objectives, ensure implementation of all the objectives, identify non-functional parts of a process, and explore alternative processes for achieving a given set of objectives. As such, GPA addresses a critical component in process reengineering, that of identifying which part of a given process needs to be improved and what alternatives could be used instead.  Keywords  Process Redesign, Process Analysis, Goal Analysis, Work Flow Design, Organizational Design  1. INTRODUCTION  Critical in process reengineering is some way of identifying what needs to be redesigned as well as understanding what alternatives we have. This paper proposes a method, Goal-based Process Analysis (GPA), that provide a systematic way to: . identify missing goals . ensure implementation of all the goals . identify non-functional parts of a p...
499|Representation and Utilization of Non-Functional Requirements for Information System Design|The complexity and usefulness of large information systems are determined partly by their functionality, i.e., what they do, and partly by global constraints on their accuracy, security, cost, user-friendliness, performance, and the like. Even with the growing interest in developing higher-level models and design paradigms, current technology is inadequate both representationally for expressing such global constraints as formal non-functional requirements and methodologically for utilizing them in generating designs. We propose both a representational and methodological framework for non-functional requirements, focusing on accuracy requirements. With the premise that accuracy is an inherent semantic attribute of information, we take a first step towards establishing a representational basis for accuracy. To guide the design process and justify design decisions, we propose a goal-oriented methodology. In the methodology, accuracy requirements are treated as (potentially conflicting) go...
500|Using Quality Requirements To Systematically Develop Quality Software|. Although quality issues such as accuracy, security, and performance are often crucial to the success of a software system, there has been no systematic way to achieve quality requirements during system development. We offer a framework and an implemented tool which treat quality requirements as goals to be achieved systematically during the system development process. We illustrate the process that a developer would go through, in building quality into a system. We have tested the framework on a number of studies involving a variety of quality requirements, organisational settings, and system types. Keywords: non-functional requirements, accuracy, security, performance, information systems, process, software quality, defect detection, conflicts. 1 Problem  Software development is traditionally driven by functional requirements, i.e., the desired functionality of the system. For example, a credit card system should debit and credit accounts, check credit limits, charge interest, issue...
501|Markov Random Field Models in Computer Vision|. A variety of computer vision problems can be optimally posed as Bayesian labeling in which the solution of a problem is defined as the maximum a posteriori (MAP) probability estimate of the true labeling. The posterior probability is usually derived from a prior model and a likelihood model. The latter relates to how data is observed and is problem domain dependent. The former depends on how various prior constraints are expressed. Markov Random Field Models (MRF) theory is a tool to encode contextual constraints into the prior probability. This paper presents a unified approach for MRF modeling in low and high level computer vision. The unification is made possible due to a recent advance in MRF modeling for high level object recognition. Such unification provides a systematic approach for vision modeling based on sound mathematical principles. 1 Introduction  Since its beginning in early 1960&#039;s, computer vision research has been evolving from heuristic design of algorithms to syste...
502|Constructing Simple Stable Descriptions for Image Partitioning|A new formulation of the image partitioning problem is presented: construct a complete and stable description of an image, in terms of a specified descriptive language, that is simplest in the sense of being shortest. We show that a descriptive language limited to a low-order polynomial description of the intensity variation within each region and a chain-code-like description of the region boundaries yields intuitively satisfying partitions for a wide class of images. The advantage of this formulation is that it can be extended to deal with subsequent steps of the image-understanding problem (or to deal with other image attributes, such as texture) in a natural way by augmenting the descriptive language. Experiments performed on a variety of both real and synthetic images demonstrate the superior performance of this approach over partitioning techniques based on clustering vectors of local image attributes and standard edge-detection techniques. 1 Introduction  The partitioning proble...
503|A Markov Random Field Model for Object Matching under Contextual Constraints|This paper presents a Markov random field (MRF) model for object recognition in high level vision. The labeling state of a scene in terms of a model object is considered as an MRF or couples MRFs. Within the Bayesian framework, the optimal solution is defined as the maximum a posteriori (MAP) estimate of the MRF. The posterior distribution is derived based on sound mathematical principles from theories of MRF and probability, which is in contrast to heuristic formulations. An experimental result is presented. 1 Introduction  In object recognition, an object is usually represented by a set of primitives or features. These features are attributed by their properties and are constrained to one another by contextual inter-relations. Two issues must be addressed for successful recognition: how to use contextual constraints effectively and how to deal with uncertainties. Markov random field (MRF) theory provides a way of encoding contextual constraints. Since 1980&#039;s, there has been considera...
504|Toward 3D Vision from Range Images: An Optimization Framework and Parallel Networks |We propose a unified approach to solve low, intermediate and high level computer vision problems for 3D object recognition from range images. All three levels of computation are cast in an optimization framework and can be implemented on neural network style architecture. In the low level computation, the tasks are to estimate curvature images from the input range data. Subsequent processing at the intermediate level is concerned with segmenting these curvature images into coherent curvature sign maps. In the high level, image features are matched against model features based on an object description called  attributed relational graph (ARG). We show that the above computational tasks at each of the three different levels can all be formulated as optimizing a two-term energy function. The first term encodes unary constraints while the second term binary ones. These energy functions are minimized using parallel and distributed relaxation-based algorithms which are well suited for neural...
505|Linked Data -- The story so far  |The term Linked Data refers to a set of best practices for publishing and connecting structured data on the Web. These best practices have been adopted by an increasing number of data providers over the last three years, leading to the creation of a global data space containing billions of assertions- the Web of Data. In this article we present the concept and technical principles of Linked Data, and situate these within the broader context of related technological developments. We describe progress to date in publishing Linked Data on the Web, review applications that have been developed to exploit the Web of Data, and map out a research agenda for the Linked Data community as it moves forward.
506|DBpedia: A Nucleus for a Web of Open Data|Abstract DBpedia is a community effort to extract structured informa-tion from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data. 1
507|Duplicate record detection: A survey|Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a dif cult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats or any combination of these factors. In this article, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar eld entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the ef ciency and scalability of approximate duplicate detection algorithms. We conclude with a coverage of existing tools and with a brief discussion of the big open problems in the area.  
508|Sindice.com: A document-oriented lookup index for open linked data |Developers of Semantic Web applications face a challenge with respect to the decentralised publication model: how and where to find statements about encountered resources. The “linked data” approach mandates that resource URIs should be de-referenced to return resource metadata. But for data discovery linkage itself is not enough, and crawling and indexing of data is necessary. Existing Semantic Web search engines are focused on database-like functionality, compromising on index size, query performance and live updates. We present Sindice, a lookup index over resources crawled on the Semantic Web. Our index allows applications to automatically locate documents containing information about a given resource. In addition, we allow resource retrieval through uniquely identifying inverse-functional properties, offer a full-text search and index SPARQL endpoints. Finally we introduce an extension to the sitemap protocol which allows us to efficiently index large Semantic Web datasets with minimal impact on the data providers.
509|Querying Distributed RDF Data Sources with SPARQL|Abstract. Integrated access to multiple distributed and autonomous RDF data sources is a key challenge for many semantic web applications. As a reaction to this challenge, SPARQL, the W3C Recommendation for an RDF query language, supports querying of multiple RDF graphs. However, the current standard does not provide transparent query federation, which makes query formulation hard and lengthy. Furthermore, current implementations of SPARQL load all RDF graphs mentioned in a query to the local machine. This usually incurs a large overhead in network traffic, and sometimes is simply impossible for technical or legal reasons. To overcome these problems we present DARQ, an engine for federated SPARQL queries. DARQ provides transparent query access to multiple SPARQL services, i.e., it gives the user the impression to query one single RDF graph despite the real data being distributed on the web. A service description language enables the query engine to decompose a query into sub-queries, each of which can be answered by an individual service. DARQ also uses query rewriting and cost-based query optimization to speed-up query execution. Experiments show that these optimizations significantly improve query performance even when only a very limited amount of statistical information is available. DARQ is available under GPL License at
510|Principles of dataspace systems|The most acute information management challenges today stem from organizations relying on a large number of diverse, interrelated data sources, but having no means of managing them in a convenient, integrated, or principled fashion. These challenges arise in enterprise and government data management, digital libraries, “smart ” homes and personal information management. We have proposed dataspaces as a data management abstraction for these diverse applications and DataSpace Support Platforms (DSSPs) as systems that should be built to provide the required services over dataspaces. Unlike data integration systems, DSSPs do not require full semantic integration of the sources in order to provide useful services. This paper lays out specific technical challenges to realizing DSSPs and ties them to existing work in our field. We focus on query answering in DSSPs, the DSSP’s ability to introspect on its content, and the use of human attention to enhance the semantic relationships in a dataspace.  
512|Triplify -- Light-Weight Linked Data Publication from Relational Databases|In this paper we present Triplify – a simplistic but effective approach to publish Linked Data from relational databases. Triplify is based on mapping HTTP-URI requests onto relational database queries. Triplify transforms the resulting relations into RDF statements and publishes the data on the Web in various RDF serializations, in particular as Linked Data. The rationale for developing Triplify is that the largest part of information on the Web is already stored in structured form, often as data contained in relational databases, but usually published by Web applications only as HTML mixing structure, layout and content. In order to reveal the pure structured information behind the current Web, we have implemented Triplify as a light-weight software component, which can be easily integrated into and deployed by the numerous, widely installed Web applications. Our approach includes a method for publishing update logs to enable incremental crawling of linked data sources. Triplify is complemented by a library of configurations for common relational schemata and a REST-enabled data source registry. Triplify configurations containing mappings are provided for many popular Web applications, including osCommerce, WordPress, Drupal, Gallery, and phpBB. We will show that despite its light-weight architecture Triplify is usable to publish very large datasets, such as 160GB of geo data from the OpenStreetMap project.
513|Named Graphs|The Semantic Web consists of many RDF graphs nameable by URIs. This paper extends the syntax and semantics of RDF to cover such named graphs. This enables RDF statements that describe graphs, which is beneficial in many Semantic Web application areas. Named graphs are given an abstract syntax, a formal semantics, an XML syntax, and a syntax based on N3. SPARQL is a query language applicable to named graphs. A specific application area discussed in detail is that of describing provenance information. This paper provides a formally defined framework suited to being a foundation for the Semantic Web trust layer.
514|Bootstrapping pay-as-you-go data integration systems|Data integration systems offer a uniform interface to a set of data sources. Despite recent progress, setting up and maintaining a data integration application still requires significant upfront effort of creating a mediated schema and semantic mappings from the data sources to the mediated schema. Many application contexts involving multiple data sources (e.g., the web, personal information management, enterprise intranets) do not require full integration in order to provide useful services, motivating a pay-as-you-go approach to integration. With that approach, a system starts with very few (or inaccurate) semantic mappings and these mappings are improved over time as deemed necessary. This paper describes the first completely self-configuring data integration system. The goal of our work is to investigate how advanced of a starting point we can provide a pay-as-you-go system. Our system is based on the new concept of a probabilistic mediated schema that is automatically created from the data sources. We automatically create probabilistic schema mappings between the sources and the mediated schema. We describe experiments in multiple domains, including 50-800 data sources, and show that our system is able to produce high-quality answers with no human intervention.
515|Provenance Information in the Web of Data|The openness of the Web and the ease to combine linked data from different sources creates new challenges. Systems that consume linked data must evaluate quality and trustworthiness of the data. A common approach for data quality assessment is the analysis of provenance information. For this reason, this paper discusses provenance of data on the Web and proposes a suitable provenance model. While traditional provenance research usually addresses the creation of data, our provenance model also represents data access, a dimension of provenance that is particularly relevant in the context of Web data. Based on our model we identify options to obtain provenance information and we raise open questions concerning the publication of provenance-related metadata for linked data on the Web.
516|Automatic Interlinking of Music Datasets on the Semantic Web |In this paper, we describe current efforts towards interlinking music-related datasets on the Web. We first explain some initial interlinking experiences, and the poor results obtained by taking a naïve approach. We then detail a particular interlinking algorithm, taking into account both the similarities of web resources and of their neighbours. We detail the application of this algorithm in two contexts: to link a Creative Commons music dataset to an editorial one, and to link a personal music collection to corresponding web identifiers. The latter provides a user with personally meaningful entry points for exploring the web of data, and we conclude by describing some concrete tools built to generate and use such links.
517|The Open Provenance Model|The Open Provenance Model (OPM) is a community-driven data model for Provenance that is designed to support inter-operability of provenance technology. Underpinning OPM, is a notion of directed acyclic graph, used to represent data products and processes involved in past computations, and causal dependencies between these. The Open Provenance Model was derived following two “Provenance Challenges”, international, multidisciplinary activities trying to investigate how to exchange information between multiple systems supporting provenance and how to query it. The OPM design was mostly driven by practical and pragmatic considerations, and is being tested in a third Provenance Challenge, which has just started. The purpose of this paper is to investigate the theoretical foundations of this data model. The formalisation consists of a set-theoretic definition of the data model, a definition of the inferences by transitive closure that are permitted, a formal description of how the model can be used to express dependencies in past computations, and finally, a description of the kind of time-based inferences that are supported. A novel element that OPM introduces is the concept of an account, by which multiple descriptions of a same execution are allowed to co-exist in a same graph. Our formalisation gives a precise meaning to such accounts and associated notions of alternate and refinement. Warning It was decided that this paper should be released as early as possible since it brings useful clarifications on the Open Provenance Model, and therefore can benefit the Provenance Challenge 3 community. The reader should recognise that this paper is however an early draft, and several sections are incomplete. Additionally, figures rely on colours but these may be difficult to read when printed in a black and white. It is advisable to print the paper in colour. 1 1
518|Which Semantic Web?|Through scenarios in the popular press and technical papers in the research literature, the promise of the Semantic Web has raised a number of different expectations. These expectations can be traced to three different perspectives on the Semantic Web. The Semantic Web is portrayed as: (1) a universal library, to be readily accessed and used by humans in a variety of information use contexts; (2) the backdrop for the work of computational agents completing sophisticated activities on behalf of their human counterparts; and (3) a method for federating particular knowledge bases and databases to perform anticipated tasks for humans and their agents. Each of these perspectives has both theoretical and pragmatic entailments, and a wealth of past experiences to guide and temper our expectations. In this paper, we examine all three perspectives from rhetorical, theoretical, and pragmatic viewpoints with an eye toward possible outcomes as Semantic Web efforts move forward.
519|M.: Linked movie data base|The Linked Movie Database (LinkedMDB) project provides a demonstration of the first open linked dataset connecting several major existing (and highly popular) movie web resources. The database exposed by LinkedMDB contains millions of RDF triples with hundreds of thousands of RDF links to existing web data sources that are part of the growing Linking Open Data cloud, as well as to popular movierelated web pages such as IMDb. LinkedMDB uses a novel way of creating and maintaining large quantities of high quality links by employing state-of-the-art approximate join techniques for finding links, and providing additional RDF metadata about the quality of the links and the techniques used for deriving them.
520|A Framework for Semantic Link Discovery over Relational Data |In this paper, we present a framework for online discovery of semantic links from relational data. Our framework is based on declarative specification of the linkage requirements by the user, that allows matching data items in many real-world scenarios. These requirements are translated to queries that can run over the relational data source, potentially using the semantic knowledge to enhance the accuracy of link discovery. Our framework lets data publishers to easily find and publish high-quality links to other data sources, and therefore could significantly enhance the value of the data in the next generation of web.
521|How will we interact with the web of data|The Semantic Web is a global information space of linked data, designed not for human use but for consumption by machines. Right? Well, yes and no. It&#039;s true to say that machine-readable data,  given explicit semantics and published online,  coupled with the ability to link data in distributed data sets are the key selling points of the Semantic Web. Together, these features allow aggregation and integration of heterogeneous data on an unprecedented scale,  and machines will do the grunt work for us. However, without a human being somewhere in this process, to reap the rewards of these new capabilities, the endeavour is meaningless. Far from removing human beings from the equation, a Web of machine-readable data creates significant challenges and significant opportunities for human-computer interaction. To date,  the Semantic Web community has mostly been busy developing the technical infrastructure to make the Web of Data feasible in principle and on publishing linked data sets in order to make it a reality. If we are to fully exploit the challenges and opportunities of a Web of Data from a human perspective,  we need to move beyond the initial phase and work to understand how this changes the user interaction paradigm of the Web.
522|What is the Size of the Semantic Web|Abstract: When attempting to build a scaleable Semantic Web application, one has to know about the size of the Semantic Web. In order to be able to understand the characteristics of the Semantic Web, we examined an interlinked dataset acting as a representative proxy for the Semantic Web at large. Our main finding was that regarding the size of the Semantic Web, there is more than the sheer number of triples; the number and type of links is an equally crucial measure.
523|Tabulator Redux: Browsing and Writing Linked Data |second frame shows information within that source expanded, the third frame shows another source within that source expanded, and finally, the last frame shows that the label of that source has been edited from “Music and artist data interlinked ” to “Music and artist data linked on the Semantic Web” A first category of Semantic Web browsers was designed to present a given dataset (an RDF graph) for perusal in various forms. These include mSpace, Exhibit, and to a certain extent
524|Integration of semantically annotated data by the knofuss architecture|Abstract. Most of the existing work on information integration in the Semantic Web concentrates on resolving schema-level problems. Specific issues of data-level integration (instance coreferencing, conflict resolu-tion, handling uncertainty) are usually tackled by applying the same techniques as for ontology schema matching or by reusing the solutions produced in the database domain. However, data structured according to OWL ontologies has its specific features: e.g., the classes are organized into a hierarchy, the properties are inherited, data constraints differ from those defined by database schema. This paper describes how these fea-tures are exploited in our architecture KnoFuss, designed to support data-level integration of semantic annotations. 1
525|DBpedia Mobile - A Location-Aware Semantic Web Client|Abstract. DBpedia Mobile is a location-aware client for the Semantic Web that can be used on an iPhone and other mobile devices. Based on the current GPS position of a mobile device, DBpedia Mobile renders a map indicating nearby locations from the DBpedia dataset. Starting from this map, the user can explore background information about his surroundings by navigating along data links into other Web data sources. DBpedia Mobile has been designed for the use case of a tourist exploring a city. As the application is not restricted to a fixed set of data sources but can retrieve and display data from arbitrary Web data sources, DBpedia Mobile can also be employed within other use cases, including ones un-foreseen by its developers. Besides accessing Web data, DBpedia Mobile also enables users to publish their current location, pictures and reviews to the Semantic Web so that they can be used by other Semantic Web applications. Instead of simply being tagged with geographical coordi-nates, published content is interlinked with a nearby DBpedia resource and thus contributes to the overall richness of the Geospatial Semantic Web.
526|Information-seeking on the Web with Trusted Social Networks – from Theory to Systems|This research investigates how synergies between the Web and social networks can enhance the process of obtaining relevant and trustworthy information. A review of literature on personalised search, social search, recommender systems, social networks and trust propagation reveals limitations of existing technology in areas such as relevance, collaboration, task-adaptivity and trust. In response to these limitations I present a Web-based approach to information-seeking using social networks. This approach takes a source-centric perspective on the information-seeking process, aiming to identify trustworthy sources of relevant information from within the user&#039;s social network. An empirical study of source-selection decisions in information- and recommendationseeking identified five factors that influence the choice of source, and its perceived trustworthiness. The priority given to each of these factors was found to vary according to the criticality and subjectivity of the task. A series of algorithms have been developed that operationalise three of these factors (expertise, experience, affinity) and generate from various data sources a number of trust metrics for use in social network-based information seeking. The most significant of these data sources is Revyu.com, a reviewing and rating Web site implemented as part of this research, that takes input from regular users and makes it available on the Semantic Web for easy re-use by the implemented algorithms. Output of the algorithms is used in Hoonoh.com, a Semantic Web-based system that has been developed to support users in identifying relevant and trustworthy information   sources within their social networks. Evaluation of this system&#039;s ability to predict source selections showed more promising results for the experience factor than for expertise or affinity. This may be attributed to the greater demands these two factors place in terms of input data. Limitations of the work and opportunities for future research are discussed.  
527|A model for technical inefficiency effects in a stochastic frontier production function for panel data|Abstract: A stochastic frontier production function is defined for panel data on firms, in which the non-negative technical inetGciency effects are assumed to be a function of firm-specific variables and time. The inefficiency effects are assumed to be independently distributed as truncations of normal distributions with constant variance, but with means which are a linear function of observable variables. This panel data model is an extension of recently proposed models for inefTiciency effects in stochastic frontiers for cross-sectional data. An empirical application of the model is obtained using up to ten years of data on paddy farmers from an Indian village. The null hypotheses, that the inefficiency effects are not stochastic or do not depend on the farmer-specific variables and time of observation, are rejected for these data.
529|Oligomorphic permutation groups|A permutation group G (acting on a set ?, usually infinite) is said to be oligomorphic if G has only finitely many orbits on ? n (the set of n-tuples of elements of ?). Such groups have traditionally been linked with model theory and combinatorial enumeration; more recently their group-theoretic properties have been studied, and links with graded algebras, Ramsey theory, topological dynamics, and other areas have emerged. This paper is a short summary of the subject, concentrating on the enumerative and algebraic aspects but with an account of grouptheoretic properties. The first section gives an introduction to permutation groups and to some of the more specific topics we require, and the second describes the links to model theory and enumeration. We give a spread of examples, describe results on the growth rate of the counting functions, discuss a graded algebra associated with an oligomorphic group, and finally discuss group-theoretic properties such as simplicity, the small index property, and “almost-freeness”.
530|Structure and Complexity of Relational Queries|This paper is an attempt at laying the foundations for the classification of queries on  relational data bases according to their structure and their computational complexity. Using  the operations of composition and fixpoints, a Z--// hierarchy of height w 2, called the  fixpoint query hierarchy, is defined, and its properties investigated. The hierarchy includes  most of the queries considered in the literathre including those of Codd and Aho and Ullman
531|Toward Logic Tailored for Computational Complexity|Whereas first-order logic was developed to confront the infinite it is often used in computer science in such a way that infinite models are meaningless. We discuss the first-order theory of finite structures and alternatives to first-order logic, especially polynomial time logic. 
532|The Mordell-Lang conjecture for function fields|In [La65], Lang formulated a hypothesis including as special cases the Mordell conjecture concerning rational points on curves, and the Manin-Mumford conjecture on torsion points of Abelian varieties. Sometimes generalized to semi-Abelian varieties, and to positive characteristic, this has been called the Mordell-Lang conjecture;
533|Extending partial automorphisms and the profinite topology on free groups|Abstract. A class of structures C is said to have the extension property for partial automorphisms (EPPA) if, whenever C1 and C2 are structures in C, C1 finite, C1 ? C2, and p1,p2,...,pn are partial automorphisms of C1 extending to automorphisms of C2, then there exist a finite structure C3 in C and automorphisms a1,a2,...,an of C3 extending the pi. We will prove that some classes of structures have the EPPA and show the equivalence of these kinds of results with problems related with the profinite topology on free groups. In particular, we will give a generalisation of the theorem, due to Ribes and Zalesskii stating that a finite product of finitely generated subgroups is closed for this topology. 1.
534|Logarithmic-Exponential Power Series|. We use generalized power series to construct algebraically a nonstandard model of the theory of the real field with exponentiation. This model enables us to show the undefinability of the zeta function and certain non-elementary and improper integrals. We also use this model to answer a question of Hardy by showing that the compositional inverse to the function (log x)(log log x) is not asymptotic as x ! +1 to a composition of semialgebraic functions, log and exp. x1 Introduction and preliminaries  Let RfX 1 ; : : : ; Xm  g denote the ring of all real power series in X 1 ; : : : ; Xm that converge in a neighborhood of I  m  , where I = [\Gamma1; 1]. For f 2 RfX 1 ; : : : ; Xm  g we let  e f : R  m  ! R be given by:  e f(x) =  ae  f(x); for x 2 I  m  , 0; x 62 I  m  . We call the e f &#039;s restricted analytic functions. Let L an be the language of ordered rings  f!; 0; 1; +; \Gamma; \Deltag augmented by a new function symbol for each function e f . We let R an be the reals with its natur...
535|A zero-one law for logic with a fixed-point operator |The logic obtained by adding the least-fixed-point operator to first-order logic was proposed as a query language by Aho and Ullman (in &#034;Proc. 6th ACM Sympos. on Principles of Programming Languages, &#034; 1979, pp. 110-120) and has been studied, particularly in connection with finite models, by numerous authors. We extend to this logic, and to the logic containing the more powerful iterative-fixedpoint operator, the zero-one law proved for first-order logic in (Glebskii, Kogan, Liogonki, and Talanov (1969), Kibernetika 2, 31-42; Fagin (1976), J. Symbolic Logic 41, 50-58). For any sentence q ~ of the extended logic, the proportion of models of q ~ among all structures with universe {1, 2,..., n} approaches 0 or 1 as n tends to infinity. We also show that the problem of deciding, for any cp, whether this proportion approaches 1 is complete for exponential time, if we consider only q)&#039;s with a fixed finite vocabulary (or vocabularies of bounded arity) and complete for double-exponential time if ~0 is unrestricted. In addition, we establish some related results. © 1985 Academic Press, Inc.
536|A universal algorithm for sequential data compression|A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source.
537|Active Learning with Statistical Models|For manytypes of learners one can compute the statistically &#034;optimal&#034; way to select data. We review how  these techniques have been used with feedforward neural networks [MacKay, 1992# Cohn, 1994]. We then  showhow the same principles may be used to select data for two alternative, statistically-based learning  architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural  networks are expensive and approximate, the techniques for mixtures of Gaussians and locally weighted  regression are both efficient and accurate.
538|Learning Bayesian networks: The combination of knowledge and statistical data|We describe scoring metrics for learning Bayesian networks from a combination of user knowledge and statistical data. We identify two important properties of metrics, which we call event equivalence and parameter modularity. These properties have been mostly ignored, but when combined, greatly simplify the encoding of a user’s prior knowledge. In particular, a user can express his knowledge—for the most part—as a single prior Bayesian network for the domain. 1
539|Improving generalization with active learning|Abstract. Active learning differs from &amp;quot;learning from examples &amp;quot; in that the learning algorithm assumes at least some control over what part of the input domain it receives information about. In some situations, active learning is provably more powerful than learning from examples alone, giving better generalization for a fixed number of training examples. In this article, we consider the problem of learning a binary concept in the absence of noise. We describe a formalism for active concept learning called selective sampling and show how it may be approximately implemented by a neural network. In selective sampling, a learner receives distribution information from the environment and queries an oracle on parts of the domain it considers &amp;quot;useful. &amp;quot; We test our implementation, called an SGnetwork, on three domains and observe significant improvement in generalization.
540|Information-Based Objective Functions for Active Data Selection|Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed which measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness. 1 Introduction  Theories for data modelling often assume that the data is provided by a source that we do not control. However, there are two scenarios in which we are able to actively select training data. In the first, data measurements are relatively expensive or slow, and we want to know where to look next so as to learn as much as possible. According to Jaynes (1986), Bayesian reasoning was first applied to this problem two centuries ago by Laplace, who in consequence made more important discoveries...
541|Supervised learning from incomplete data via an EM approach|Real-world learning tasks may involve high-dimensional data sets with arbitrary patterns of missing data. In this paper we present a framework based on maximum likelihood density estimation for learning from such data sets. We use mixture models for the density estimates and make two distinct appeals to the ExpectationMaximization (EM) principle (Dempster et al., 1977) in deriving a learning algorithm---EM is used both for the estimation of mixture components and for coping with missing data. The resulting algorithm is applicable to a wide range of supervised as well as unsupervised learning problems. Results from a classification benchmark---the iris data set---are presented. 1 Introduction  Adaptive systems generally operate in environments that are fraught with imperfections; nonetheless they must cope with these imperfections and learn to extract as much relevant information as needed for their particular goals. One form of imperfection is incompleteness in sensing information. Inc...
542|Neural network exploration using optimal experiment design|We consider the question &#034;How should one act when the only goal is to learn as much as possible?&#034; Building  on the theoretical results of Fedorov [1972] and MacKay [1992], we apply techniques from Optimal  Experiment Design (OED) to guide the query/action selection of a neural network learner. We demonstrate  that these techniques allow the learner to minimize its generalization error by exploring its domain  efficiently and completely.We conclude that, while not a panacea, OED-based query/action has muchto  offer, especially in domains where its high computational costs can be tolerated.
543|Active Exploration in Dynamic Environments|Whenever an agent learns to control an unknown environment, two opposing principles have to be combined, namely: exploration (long-term optimization) and exploitation (short-term optimization). Many real-valued connectionist approaches to learning control realize exploration by randomness in action selection. This might be disadvantageous when costs are assigned to &#034;negative experiences&#034;. The basic idea presented in this paper is to make an agent explore unknown regions in a more directed manner. This is achieved by a so-called competence map, which is trained to predict the controller&#039;s accuracy, and is used for guiding exploration. Based on this, a bistable system enables smoothly switching attention between two behaviors -- exploration and exploitation -- depending on expected costs and knowledge gain. The appropriateness of this method is demonstrated by a simple robot navigation task.   
544|Reinforcement Driven Information Acquisition In Non-Deterministic Environments|For an agent living in a non-deterministic Markov environment (NME), what is, in theory, the fastest way of acquiring information about its statistical properties? The answer is: to design &#034;optimal&#034; sequences of &#034;experiments&#034; by performing action sequences that maximize expected information gain. This notion is implemented by combining concepts from information theory and reinforcement learning. Experiments show that the resulting method, reinforcement driven information acquisition, can explore certain NMEs much faster than conventional random exploration.  
545|Estimation of probabilities from sparse data for the language model component of a speech recognizer|Abstract-The description of a novel type of rn-gram language model is given. The model offers, via a nonlinear recursive procedure, a com-putation and space efficient solution to the problem of estimating prob-abilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and suc-cessfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of es-timating probabilities from sparse data arises. Sparseness of data is an inherent property of any real text, and it is a problem that one always encounters while collecting fre-quency statistics on words and word sequences (m-grams) from a text of finite size. This means that even for a very large data col-lection, the maximum likelihood estimation method does not allow Turing’s estimate PT for a probability of a word (m-gram) which occurred in the sample r times is r* PT = where r We call a procedure of replacing a count r with a modified count r ’ “discounting ” and a ratio rt/r a discount coefficient dr. When r ’  = r*, we have Turing’s discounting. Let us denote the m-gram wl,  *.., w, as wy and the number of times it occurred in the sample text as c(wT). Then the maxi-mum likelihood estimate is
546|An empirical Bayes approach to statistics|Let X be a random variable which for simplicity we shall assume to have discrete values x and which has a probability distribution depending in a known way on an un-known real parameter A, (1) p (xIX) =Pr [X = xIA =X], A-itself being a random variable with a priori distribution function (2) G (X) =Pr [A-&lt; X. The unconditional probability distribution of X is then given by
547|Efficient dispersal of information for security, load balancing, and fault tolerance|Abstract. An Information Dispersal Algorithm (IDA) is developed that breaks a file F of length L =  ( F ( into n pieces F,, 1 5 i 5 n, each of length ( F, 1 = L/m, so that every m pieces suffice for reconstructing F. Dispersal and reconstruction are computationally efficient. The sum of the lengths ( F, 1 is (n/m). L. Since n/m can be chosen to be close to I, the IDA is space eflicient. IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communi-cations between processors in parallel computers. For the latter problem provably time-efftcient and highly fault-tolerant routing on the n-cube is achieved, using just constant size buffers. Categories and Subject Descriptors: E.4 [Coding and Information Theory]: nonsecret encoding schemes
548|Parallel Prefix Computation|ABSTRACT The prefix problem is to compute all the products x t o x2.... o xk for i ~ k. ~ n, where o is an associative operation A recurstve construction IS used to obtain a product circuit for solving the prefix problem which has depth exactly [log:n] and size bounded by 4n An application yields fast, small Boolean ctrcmts to simulate fimte-state transducers. By simulating a sequentml adder, a Boolean clrcmt which has depth 2[Iog2n] + 2 and size bounded by 14n Is obtained for n-bit binary addmon The size can be decreased significantly by permitting the depth to increase by an addmve constant
549|Cluster I/O with River: Making the Fast Case Common|We introduce River, a data-flow programming environment and I/O substrate for clusters of computers. River is designed to provide maximum performance in the common case --- even in the face of nonuniformities in hardware, software, and workload. River is based on two simple design features: a high-performance distributed queue, and a storage redundancy mechanism called graduated declustering. We have implemented a number of data-intensive applications on River, which validate our design with near-ideal performance in a variety of non-uniform performance scenarios. 
550|Diamond: A storage architecture for early discard in interactive search|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
551|Explicit Control in a Batch-Aware Distributed File System |We present the design, implementation, and evaluation of the Batch-Aware Distributed File System (BAD-FS), a system designed to orchestrate large, I/O-intensive batch workloads on remote computing clusters distributed across the wide area. BAD-FS consists of two novel components: a storage layer which exposes control of traditionally fixed policies such as caching, consistency, and replication; and a scheduler that exploits this control as needed for different users and workloads. By extracting these controls from the storage layer and placing them in an external scheduler, BAD-FS manages both storage and computation in a coordinated way while gracefully dealing with cache consistency, fault-tolerance, and space management issues in an application-specific manner. Using both microbenchmarks and real applications, we demonstrate the performance benefits of explicit control, delivering excellent end-to-end performance across the wide-area.  
552|Using Bayesian networks to analyze expression data|DNA hybridization arrays simultaneously measure the expression level for thousands of genes. These measurements provide a “snapshot ” of transcription levels within the cell. A major challenge in computational biology is to uncover, from such measurements, gene/protein interactions and key biological features of cellular systems. In this paper, we propose a new framework for discovering interactions between genes based on multiple expression measurements. This framework builds on the use of Bayesian networks for representing statistical dependencies. A Bayesian network is a graph-based model of joint multivariate probability distributions that captures properties of conditional independence between variables. Such models are attractive for their ability to describe complex stochastic processes and because they provide a clear methodology for learning from (noisy) observations. We start by showing how Bayesian networks can describe interactions between genes. We then describe a method for recovering gene interactions from microarray data using tools for learning Bayesian networks. Finally, we demonstrate this method on the S. cerevisiae cell-cycle measurements of Spellman et al. (1998). Key words: gene expression, microarrays, Bayesian methods. 1.
553|Gapped Blast and PsiBlast: a new generation of protein database search programs|The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSIBLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.
554|A Bayesian method for the induction of probabilistic networks from data|This paper presents a Bayesian method for constructing probabilistic networks from databases. In particular, we focus on constructing Bayesian belief networks. Potential applications include computer-assisted hypothesis testing, automated scientific discovery, and automated construction of probabilistic expert systems. We extend the basic method to handle missing data and hidden (latent) variables. We show how to perform probabilistic inference by averaging over the inferences of multiple belief networks. Results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases. Finally, we relate the methods in this paper to previous work, and we discuss open problems.
556|Bayesian Network Classifiers|Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods for feature selection.
557|Clustering Gene Expression Patterns|Recent advances in biotechnology allow researchers to measure expression levels for thousands of genes simultaneously, across different conditions and over time. Analysis of data produced by such experiments offers potential insight into gene function and regulatory mechanisms. A key step in the analysis of gene expression data is the detection of groups of genes that manifest similar expression patterns. The corresponding algorithmic problem is to cluster multi-condition gene expression patterns. In this paper we describe a novel clustering algorithm that was developed for analysis of gene expression data. We define an appropriate stochastic error model on the input, and prove that under the conditions of the model, the algorithm recovers the cluster structure with high probability. The running time of the algorithm on an n-gene dataset is O(n 2 (log(n)) c ). We also present a practical heuristic based on the same algorithmic ideas. The heuristic was implemented and its p...
559|Being Bayesian about network structure|Abstract. In many multivariate domains, we are interested in analyzing the dependency structure of the underlying distribution, e.g., whether two variables are in direct interaction. We can represent dependency structures using Bayesian network models. To analyze a given data set, Bayesian model selection attempts to find the most likely (MAP) model, and uses its structure to answer these questions. However, when the amount of available data is modest, there might be many models that have non-negligible posterior. Thus, we want compute the Bayesian posterior of a feature, i.e., the total posterior probability of all models that contain it. In this paper, we propose a new approach for this task. We first show how to efficiently compute a sum over the exponential number of networks that are consistent with a fixed order over network variables. This allows us to compute, for a given order, both the marginal probability of the data and the posterior of a feature. We then use this result as the basis for an algorithm that approximates the Bayesian posterior of a feature. Our approach uses a Markov Chain Monte Carlo (MCMC) method, but over orders rather than over network structures. The space of orders is smaller and more regular than the space of structures, and has much a smoother posterior “landscape”. We present empirical results on synthetic and real-life datasets that compare our approach to full model averaging (when possible), to MCMC over network structures, and to a non-Bayesian bootstrap approach.
560|Learning the structure of dynamic probabilistic networks|Dynamic probabilistic networks are a compact representation of complex stochastic processes. In this paper we examine how to learn the structure of a DPN from data. We extend structure scoring rules for standard probabilistic networks to the dynamic case, and show how to search for structure when some of the variables are hidden. Finally, we examine two applications where such a technology might be useful: predicting and classifying dynamic behaviors, and learning causal orderings in biological processes. We provide empirical results that demonstrate the applicability of our methods in both domains. 1
561|A Theory Of Inferred Causation|This paper concerns the empirical basis of causation, and addresses the following issues: 1. the clues that might prompt people to perceive causal relationships in uncontrolled observations. 2. the task of inferring causal models from these clues, and 3. whether the models inferred tell us anything useful about the causal mechanisms that underly the observations. We propose a minimal-model semantics of causation, and show that, contrary to common folklore, genuine causal influences can be distinguished from spurious covariations following standard norms of inductive reasoning. We also establish a sound characterization of the conditions under which such a distinction is possible. We provide an effective algorithm for inferred causation and show that, for a large class of data the algorithm can uncover the direction of causal influences as defined above. Finally, we address the issue of non-temporal causation.  
562|Theory Refinement on Bayesian Networks|Theory refinement is the task of updating a domain theory in the light of new cases, to be done automatically or with some expert assistance. The problem of theory refinement under uncertainty is reviewed here in the context of Bayesian statistics, a theory of belief revision. The problem is reduced to an incremental learning task as follows: the learning system is initially primed with a partial theory supplied by a domain expert, and thereafter maintains its own internal representation of alternative theories which is able to be interrogated by the domain expert and able to be incrementally refined from data. Algorithms for refinement of Bayesian networks are presented to illustrate what is meant by &#034;partial theory&#034;, &#034;alternative theory representation &#034;, etc. The algorithms are an incremental variant of batch learning algorithms from the literature so can work well in batch and incremental mode. 1 Introduction Theory refinement is the task of updating a domain theory in the light of...
563|Minimum complexity density estimation|The minimum complexity or minimum description-length criterion developed by Kolmogorov, Rissanen, Wallace, So&amp;in, and others leads to consistent probability density estimators. These density estimators are defined to achieve the best compromise between likelihood and simplicity. A related issue is the compromise between accuracy of approximations and complexity relative to the sample size. An index of resolvability is studied which is shown to bound the statistical accuracy of the density estimators, as well as the information-theoretic redundancy.  
564|Learning Bayesian network structure from massive datasets: the “sparse candidate” algorithm|Learning Bayesian networks is often cast as an optimization problem, where the computational task is to find a structure that maximizes a sta-tistically motivated score. By and large, existing learning tools address this optimization problem using standard heuristic search techniques. Since the search space is extremely large, such search procedures can spend most of the time examining candidates that are extremely unreasonable. This problem becomes critical when we deal with data sets that are large either in the number of in-stances, or the number of attributes. In this paper, we introduce an algorithm that achieves faster learning by restricting the search space. This iterative algorithm restricts the par-ents of each variable to belong to a small sub-set of candidates. We then search for a network that satisfies these constraints. The learned net-work is then used for selecting better candidates for the next iteration. We evaluate this algorithm both on synthetic and real-life data. Our results show that it is significantly faster than alternative search procedures without loss of quality in the learned structures. 1
565|Learning Bayesian Networks is NP-Complete|Algorithms for learning Bayesian networks from data havetwo components: a scoring metric and a  search procedure. The scoring metric computes a score reflecting the goodness-of-fit of the structure  to the data. The search procedure tries to identify network structures with high scores. Heckerman  et al. (1995) introduce a Bayesian metric, called the BDe metric, that computes the relative posterior  probabilityofanetwork structure given data. In this paper, we show that the search problem of  identifying a Bayesian network---among those where each node has at most K parents---that has a  relative posterior probability greater than a given constant is NP-complete, when the BDe metric  is used.  12.1 
566|Modeling Regulatory Networks With Weight Matrices|Systematic gene expression analyses provide comprehensive information about the  transcriptional response to different environmental and developmental conditions. With  enough gene expression data points, computational biologists may eventually generate  predictive computer models of transcription regulation. Such models will require  computational methodologies consistent with the behavior of known biological systems that  remain tractable. We represent regulatory relationships between genes as linear coefficients or  weights, with the &#034;net&#034; regulation influence on a gene&#039;s expression being the mathematical  summation of the independent regulatory inputs. Test regulatory networks generated with this  approach display stable and cyclically stable gene expression levels, consistent with known  biological systems. We include variables to model the effect of environmental conditions on  transcription regulation and observed various alterations in gene expression patterns in  response to e...
567|On the Logic of Causal Models|This paper explores the role of Directed Acyclic Graphs (DAGs) as a representation of conditional independence relationships. We show that DAGs offer polynomially sound and complete inference mechanisms for inferring conditional independence relationships from a given causal set of such relationships. As a consequence, d-separation, a graphical criterion for identifying independencies in a DAG, is shown to uncover more valid independencies then any other criterion. In addition, we employ the Armstrong property of conditional independence to show that the dependence relationships displayed by a DAG are inherently consistent, i.e. for every DAG D there exists some probability distribution P that embodies all the conditional independencies displayed in D and none other.  
568|The max-min hill-climbing bayesian network structure learning algorithm|Abstract. We present a new algorithm for Bayesian network structure learning, called Max-Min Hill-Climbing (MMHC). The algorithm combines ideas from local learning, constraint-based, and search-and-score techniques in a principled and effective way. It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. In our extensive empirical evaluation MMHC outperforms on average and in terms of various metrics several prototypical and state-of-the-art algorithms, namely the PC, Sparse Candidate, Three Phase Dependency Analysis, Optimal Reinsertion, Greedy Equivalence Search, and Greedy Search. These are the first empirical results simultaneously comparing most of the major Bayesian network algorithms against each other. MMHC offers certain theoretical advantages, specifically over the Sparse Candidate algorithm, corroborated by our experiments. MMHC and detailed results of our study are publicly available at
569|A Transformational Characterization of Equivalent Bayesian Network Structures|We present a simple characterization  of equivalentBayesian network structures  based on local transformations.  The significance of the characterization  is twofold. First, we are able  to easily proveseveral new invariant  properties of theoretical interest  for equivalent structures. Second,  we use the characterization to derive  an efficient algorithm that identifies  all of the compelled edges in a  structure. Compelled edge identification  is of particular importance for  learning Bayesian network structures  from data because these edges indicate  causal relationships when certain  assumptions hold.  1 
570|A Bayesian Approach to Causal Discovery|We examine the Bayesian approach to the discovery of directed acyclic causal models and compare it to the constraint-based approach. Both approaches rely on the Causal Markov assumption, but the two differ significantly in theory and practice. An important difference between the approaches is that the constraint-based approach uses categorical information about conditional-independence constraints in the domain, whereas the Bayesian approach weighs the degree to which such constraints hold. As a result, the Bayesian approach has three distinct advantages over its constraint-based counterpart. One, conclusions derived from the Bayesian approach are not susceptible to incorrect categorical decisions about independence facts that can occur with data sets of finite size. Two, using the Bayesian approach, finer distinctions among model structures---both quantitative and qualitative---can be made. Three, information from several models can be combined to make better inferences and to better ...
571|Causal discovery from a mixture of experimental and observational data|This paper describes a Bayesian method for combining an arbitrary mixture of observational and experimental data in order to learn causal Bayesian networks. Observational data are passively observed. Experimental data, such as that produced by randomized controlled trials, result from the experimenter manipulating one or more variables (typically randomly) and observing the states of other variables. The paper presents a Bayesian method for learning the causal structure and parameters of the underlying causal process that is generating the data, given that (1) the data contains a mixture of observational and experimental case records, and (2) the causal process is modeled as a causal Bayesian network. This learning method was applied using as input various mixtures of experimental and observational data that were generated from the ALARM causal Bayesian network. In these experiments, the absolute and relative quantities of experimental and observational data were varied systematically. For each of these training datasets, the learning method was applied to predict the causal structure and to estimate the causal parameters that exist among randomly selected pairs of nodes in ALARM that are not confounded. The paper reports how these structure predictions and parameter estimates compare with the true causal structures and parameters as given by the ALARM network. 1
573|Data Analysis with Bayesian Networks: A Bootstrap Approach|In recent years there has been significant  progress in algorithms and methods for inducing  Bayesian networks from data. However, in complex  data analysis problems, we need to go beyond  being satisfied with inducing networks with  high scores. We need to provide confidence measures  on features of these networks: Is the existence  of an edge between two nodes warranted?  Is the Markov blanket of a given node robust?  Can we say something about the ordering of the  variables? We should be able to address these  questions, even when the amount of data is not  enough to induce a high scoring network. In this  paper we propose Efron&#039;s Bootstrap as a computationally  efficient approach for answering these  questions. In addition, we propose to use these  confidence measures to induce better structures  from the data, and to detect the presence of latent  variables. 
574|Identifying Gene Regulatory Networks from Experimental Data|this paper, we propose a methodology for making sense of large, multiple time-series data sets arising in expression analysis, and evaluate it both theoretically and through a case study. First, we build a graph representing all putative activation/inhibition relationships by analyzing the expression profiles for all pairs of genes. Second, we prune this graph by solving a combinatorial optimization problem to identify a small set of interesting candidate regulatory elements. We do not assert that we identify &#034;the&#034; regulatory network as a result of this computation. However, we believe that our approach quickly enables biologists to identify and visualize interesting features from raw expression array data sets. We have implemented our methodology and applied it to the analysis of the Saccharomyces cerevisiae data set. In this paper, we report on our implementation and the results of our data analysis. The problem of inducing gene regulation networks has recently come to the computational biology community. Initial attempts at modeling gene expression abd programs to induce regulatory networks from data include [2, 10, 13]. To take fullest advantage of laboratory experiments that can be performed in which a given set of genes can be explicitly expressed or repressed, and the consequences of these genes on expression biologically determined, Akutsu, et.al. [1] considers the problem of designing a minimum-size series of experiments guaranteed to result in the identification of the correct regulatory network. The candidate regulatory network proposed by our system depends upon the specific optimization criteria employed in the second phase of our procedure, although our experiments suggest that the optimal network is surprisingly robust to changes in the objective function...
575|On the Sample Complexity of Learning Bayesian Networks|In recent years there has been an increasing interest in learning Bayesian networks from data. One of the most effective methods for learning such networks is based on the minimum description length (MDL) principle. Previous work has shown that this learning procedure is asymptotically successful: with probability one, it will converge to the target distribution, given a sufficient number of samples. However, the rate of this convergence has been hitherto unknown. In this work we examine the sample complexity of MDL based learning procedures for Bayesian networks. We show that the number of samples needed to learn an ffl-close approximation (in terms of entropy distance) with confidence ffi is O  i  (  1  ffl )  4 3  log  1  ffl log  1  ffi log log  1  ffi  j  . This means that the sample complexity is a low-order polynomial in the error threshold and sublinear in the confidence bound. We also discuss how the constants in this term depend on the complexity of the target distribution. F...
576|Learning Bayesian Networks: A unification for discrete and Gaussian domains|We examine Bayesian methods for learning Bayesian networks from a combination of prior knowledge and statistical data. In particular, we unify the approaches we presented at last year&#039;s conference for discrete and Gaussian domains. We derive a general Bayesian scoring metric, appropriate for both domains. We then use this metric in combination with well-known statistical facts about the Dirichlet and normal{Wishart distributions to derive our metrics for discrete and Gaussian domains.  
577|Causal Inference in the Presence of Latent Variables and Selection Bias|This paper uses Bayesian network models for that investigation. Bayesian networks, or directed acyclic graph (DAG) models have proved very useful in representing both causal and statistical hypotheses. The nodes of the graph represent vertices, directed edges represent direct influences, and the topology of the graph encodes statistical constraints. We will consider features of such models that can be determined from data under assumptions that are related to those routinely applied in experimental situations:
578|Learning Mixtures of Bayesian Networks|We describe a heuristic method for learning mixtures of Bayesian Networks (MBNs) from possibly incomplete data. The considered class of models is mixtures in which each mixture component is a Bayesian network encoding a conditional Gaussian distribution over a fixed set of variables. Some variables may be hidden or otherwise have missing observations. A key idea in our approach is to treat expected data as real data. This allows us to interleave structure and parameter search and to take advantage of closed form approximations for marginal likelihood. In addition, by treating expected data as real data, the search criterion factors by variable, making the search processes more efficient. We evaluate our approach on synthetic and real-world data sets.  Keywords : Mixture models, Bayesian networks, structure learning, parameter learning, hidden variables, EM algorithm. 1 Introduction  There is growing interest in a class of models for density estimation known as Bayesian networks. In the...
580|Modeling and Analysis of Heterogeneous Regulation in Biological Networks|In this study we propose a novel model for the representation of biological networks and provide algorithms for learning model parameters from experimental data. Our approach is to build an initial model based on extant biological knowledge, and refine it to increase the consistency between model predictions and experimental data. Our model encompasses networks which contain heterogeneous biological entities (mRNA, proteins, metabolites) and aims to capture diverse regulatory circuitry on several levels (metabolism, transcription, translation, post-translation and feedback loops among them).
581|Latent classification models|Abstract. One of the simplest, and yet most consistently well-performing set of classifiers is the Naïve Bayes models. These models rely on two assumptions: (i) All the attributes used to describe an instance are conditionally independent given the class of that instance, and (ii) all attributes follow a specific parametric family of distributions. In this paper we propose a new set of models for classification in continuous domains, termed latent classification models. The latent classification model can roughly be seen as combining the Naïve Bayes model with a mixture of factor analyzers, thereby relaxing the assumptions of the Naïve Bayes classifier. In the proposed model the continuous attributes are described by a mixture of multivariate Gaussians, where the conditional dependencies among the attributes are encoded using latent variables. We present algorithms for learning both the parameters and the structure of a latent classification model, and we demonstrate empirically that the accuracy of the proposed model is significantly higher than the accuracy of other probabilistic classifiers. Keywords: classification, probabilistic graphical models, Naïve Bayes, correlation
583|Multi-level Modeling and Inference of Transcription Regulation |The understanding of transcription regulation is a major goal of today&#039;s biology. The challenge is to utilize  diverse high-throughput data in order to infer mechanistic models of transcription control. We propose a new  model which integrates transcription factor-gene anity, protein abundance and gene expression pro  les. The  model provides detailed, yet computationally tractable description of the relations between transcription factors,  their binding sites at genes promoters and the combinatorial regulation of transcription. At the core, our model  manipulates dose-anity-response functions that associate transcription factor concentrations and transcription  factor-DNA anities to determine the rate of transcription factor-DNA reactions. We study computational  problems that arise in optimizing such models and develop polynomial algorithms for certain problems. We  show how to assess missing values (notably protein abundance) and describe a novel framework to infer models  from currently available data. On budding yeast carbohydrate metabolism data, our results demonstrate the  sensitivity and speci  city of the approach. They also suggest new active binding sites and a regulation model for  the transcription program of the galactose system.
584|A Morphable Model For The Synthesis Of 3D Faces|In this paper, a new technique for modeling textured 3D faces is introduced. 3D faces can either be generated automatically from one or more photographs, or modeled directly through an intuitive user interface. Users are assisted in two key problems of computer aided face modeling. First, new face images or new 3D face models can be registered automatically by computing dense one-to-one correspondence to an internal face model. Second, the approach regulates the naturalness of modeled faces avoiding faces with an &#034;unlikely&#034; appearance. Starting from
585|Active Appearance Models|AbstractÐWe describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors. Index TermsÐAppearance models, deformable templates, model matching. 1
586|Synthesizing Realistic Facial Expressions from Photographs |We present new techniques for creating photorealistic textured 3D facial models from photographs of a human subject, and for creating smooth transitions between different facial expressions by morphing between these different models. Starting from several uncalibrated views of a human subject, we employ a user-assisted technique to recover the camera poses corresponding to the views as well as the 3D coordinates of a sparse set of chosen locations on the subject&#039;s face. A scattered data interpolation technique is then used to deform a generic face mesh to fit the particular geometry of the subject&#039;s face. Having recovered the camera poses and the facial geometry, we extract from the input images one or more texture maps for the model. This process is repeated for several facial expressions of a particular subject. To generate transitions between these facial expressions we use 3D shape morphing between the corresponding face models, while at the same time blending the corresponding tex...
587|Linear Object Classes and Image Synthesis From a Single Example Image|Abstract—The need to generate new views of a 3D object from a single real image arises in several fields, including graphics and object recognition. While the traditional approach relies on the use of 3D models, we have recently introduced [1], [2], [3] simpler techniques that are applicable under restricted conditions. The approach exploits image transformations that are specific to the relevant object class, and learnable from example views of other “prototypical ” objects of the same class. In this paper, we introduce such a technique by extending the notion of linear class proposed by Poggio and Vetter. For linear object classes, it is shown that linear transformations can be learned exactly from a basis set of 2D prototypical views. We demonstrate the approach on artificial objects and then show preliminary evidence that the technique can effectively “rotate ” highresolution face images from a single 2D view. Index Terms—3D object recognition, rotation invariance, deformable templates, image synthesis. 1
588|Automatic interpretation and coding of face images using flexible models|Abstract—Face images are difficult to interpret because they are highly variable. Sources of variability include individual appearance, 3D pose, facial expression, and lighting. We describe a compact parametrized model of facial appearance which takes into account all these sources of variability. The model represents both shape and gray-level appearance, and is created by performing a statistical analysis over a training set of face images. A robust multiresolution search algorithm is used to fit the model to faces in new images. This allows the main facial features to be located, and a set of shape, and gray-level appearance parameters to be recovered. A good approximation to a given face can be reconstructed using less than 100 of these parameters. This representation can be used for tasks such as image coding, person identification, 3D pose recovery, gender recognition, and expression recognition. Experimental results are presented for a database of 690 face images obtained under widely varying conditions of 3D pose, lighting, and facial expression. The system performs well on all the tasks listed above.
589|Making Faces|We have created a system for capturing both the three-dimensional geometry and color and shading information for human facial expressions. We use this data to reconstruct photorealistic, 3D animations of the captured expressions. The system uses a large set of sampling points on the face to accurately track the three dimensional deformations of the face. Simultaneously with the tracking of the geometric data, we capture multiple high resolution, registered video images of the face. These images are used to create a texture map sequence for a three dimensional polygonal face model which can then be rendered on standard 3D graphics hardware. The resulting facial animation is surprisingly life-like and looks very much like the original live performance. Separating the capture of the geometry from the texture images eliminates much of the variance in the image data due to motion, which increases compression ratios. Although the primary emphasis of our work is not compression we have investigated the use of a novel method to compress the geometric data based on principal components analysis. The texture sequence is compressed using an MPEG4 video codec. Animations reconstructed from 512x512 pixel textures look good at data rates as low as 240 Kbits per second.
590|Computer generated animation of faces|This paper describes the representation, animation and data collection techniques that have been used to produce &amp;quot;realistic&amp;quot; computer generated half-tone animated se-quences of a human face changing expres-sion. It was determined that approximating the surface of a face with a polygonal skin containing approximately 250 polygons de-fined by about 400 vertices is sufficient to achieve a realistic face. Animation was accomplished using a cosine interpolation scheme to fill in the intermediate frames between expressions. This approach is good enough to produce realistic facial motion. The three-dimensional data used to describe the expressions of the face was obtained photogrammetrically using pairs of photographs. KEY WORDS AND PHRASES: computer graphics, half-tone rendering, smooth shading, com-puter animation, flexible surfaces, poly-gonal surfaces, facial topology, cosine interpolation, £hree-dimensional data acquisition.
591|Dynamic NURBS with Geometric Constraints for Interactive Sculpting|This article develops a dynamic generalization of the nonuniform rational B-spline (NURBS) model. NURBS have become a de facto standard in commercial modeling systems because of their power to represent free-form shapes as well as common analytic shapes. To date, however, they have been viewed as purely geometric primitives that require the user to manually adjust multiple control points and associated weights in order to design shapes. Dynamic NURBS, or D-NURBS, are physics-based models that incorporate mass distributions, inertial deformation energies, and other physical quantities into the popular NURBS geometric substrate. Using D-NURBS, a modeler can interactively sculpt curves and surfaces and design complex shapes to required specifications not only in the traditional indirect fashion, by adjusting control points and weights, but also through direct physical manipulation, by applying simulated forces and local and global shape constraints. D-NURBS move and deform in a physically intuitive manner in response to the user&#039;s direct manipulations. Their dynamic behavior results from the numerical integration of a set of nonlinear differential equations that automatically evolve the control points and weights in response to the applied forces and constraints. To derive these equations, we employ Lagrangian mechanics and finite-element-like discretization. Our approach supports the trimming of D-NURBS surfaces using D-NURBS curves. We demonstrate D-NURBS models and constraints in applications including the rounding of solids, optimal surface fitting to unstructured data, surface design from cross-sections, and free-form deformation. We also introduce a new technique for 2D shape metamorphosis using constrained D-NURBS surfaces.
592|Multidimensional morphable models: A framework for representing and matching object classes|This thesis describes a flexible model for representing images of objects of a certain class, such as faces, and introduces a new algorithm for matching the model to novel images from the class. The model and matching algorithm are very general and can be used for many image analysis tasks. The flexible model, called a multidimensional morphable model, is learned from example images (called prototypes) of objects of a class. In the learning phase, pixelwise correspon-dences between a reference prototype and each of the other prototypes are first computed and then used to obtain shape and texture vectors associated with each prototype. The mor-phable model is then synthesized as a linear combination that spans the linear vector space determined by the prototypical shapes and textures. We next introduce an effective stochastic gradient descent algorithm that automatically matches a model to a novel image by finding the parameters that minimize the error between the image generated by the model and the novel image. Several experiments demonstrate the robustness and the broad range of applicability of the matching algorithm and the underlying
593|A Bootstrapping Algorithm for Learning Linear Models of Object Classes|Flexible models of object classes, based on linear combinations of prototypical images, are capable of matching novel images of the same class and have been shown to be a powerful tool to solve several fundamental vision tasks such as recognition, synthesis and correspondence. The key problem in creating a specific flexible model is the computation of pixelwise correspondence between the prototypes, a task done until now in a semiautomatic way. In this paper we describe an algorithm that automatically bootstraps the correspondence between the prototypes. The algorithm -- which can be used for 2D images as well as for 3D models -- is shown to synthesize successfully a flexible model of frontal face images and a flexible model of handwritten digits. 1 Introduction  In recent papers we have introduced a new type of flexible model for images of objects of a certain class. The idea is to represent images of a certain type -- for instance images of frontal faces -- as the linear combination ...
594|Constructing Physics-Based Facial Models of Individuals|This paper develops a highly automated approach to constructing realistic, working models of human heads for use in animation. These physics-based models are anatomically accurate and may be made to conform closely to specific individuals. We begin by scanning a person with a laser sensor which circles around the head, acquiring detailed range and reflectance information. Next, an automatic conformation algorithm adapts a triangulated face mesh of predetermined topological structure to these data. The generic mesh, which is reusable with different individuals, reduces the range data to an efficient, polygonal approximation of the facial geometry and supports a high-resolution texture mapping of the skin reflectivity. The conformed polygonal mesh forms the epidermal layer of a new, physics-based model of facial tissue. An automatic algorithm constructs the multilayer synthetic skin and estimates an underlying rigid &#034;skull&#034; substructure with a jointed jaw. Finally, the algorithm inserts synthetic muscles into the deepest layer of the facial tissue. These contractile actuators, which emulate the primary muscles of facial expression, generate forces that deform the synthetic tissue into meaningful expressions. To increase realism, we include constraints to emulate tissue incompressibility and to enable the tissue to slide over the skull substructure without penetrating into it. The resulting animate models appear significantly more realistic than our previous physics-based facial models. Keywords: Physics-Based Facial Modeling, Facial Animation, Cylindrical Facial Scanning, Feature-Based Facial Adaptation, Texture Mapping, Discrete Deformable Models. 1 
595|Estimating Coloured 3D Face Models from Single Images: An Example Based Approach|Abstract. In this paper we present a method to derive 3D shape and surface texture of a human face from a single image. The method draws on a general flexible 3D face model which is “learned ” from examples of individual 3D-face data (Cyberware-scans). In an analysis-by-synthesis loop, the flexible model is matched to the novel face image. From the coloured 3D model obtained by this procedure, we can generate new images of the face across changes in viewpoint and illumination. Moreover, nonrigid transformations which are represented within the flexible model can be applied, for example changes in facial expression. The key problem for generating a flexible face model is the computation of dense correspondence between all given 3D example faces. A new correspondence algorithm is described which is a generalization of common algorithms for optic flow computation to 3D-face data. 1
596|Emotion Editing using Finite Elements|This paper describes the prototype of a facial expression editor. In contrast to existing systems the presented  editor takes advantage of both medical data for the simulation and the consideration of facial anatomy during  the definition of muscle groups. The C¹-continuous geometry and the high degree of abstraction for the expression  editing sets this system apart from others. Using finite elements we achieve a better precision in comparison  to particle systems. Furthermore, a precomputing of facial action units enables us to compose facial  expressions by a superposition of facial action geometries in real-time. The presented model is based on a  generic facial model using a thin plate and membrane approach for the surface and elastic springs for facial  tissue modeling. It has been used successfully for performing facial surgery simulation. We illustrate features  of our system with examples from the Visible Human Dataset. 
597|Design, Transformation and Animation of Human Faces|Creation of new human faces for synthetic actors is a tedious and painful task. The situation may be improved by introducing tools for the creation. Two approaches are discussed in this paper: modification and edition of an existing synthetic actor using local transformations; generation of new synthetic actors obtained by interpolation between two existing actors; creation of a synthetic actor by composition of different parts. This paper also describes the methods used in the facial animation of synthetic actors who change their personalities from one person to another. This means that our purpose is to transform one character into another, and also to transform the animation at the same time. The interpolation must be at several levels: the shape level, the parameter level, the expression level and the script level. For the animation, we introduce three levels of inbetweens: inbetween parameters, inbetween expressions and inbetween scripts. The method has been completely implemented and integrated into the Human Factory software.
598|A user-friendly texture-fitting methodolgy for virtual humans|This paper describes a methodology for applying textures interactively to a 3D object, taking into account its topological features. A user-friendly interface based on this allows the user to evaluate the consequences on the 3D object of manipulating the 2D texture in real time. As a case study, we have applied this methodology to re-create the terra-cotta texture of virtual soldiers from Xian.
599|Learning probabilistic relational models|A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with &amp;quot;flat &amp;quot; data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning — the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases. 1
600|Empirical Analysis of Predictive Algorithm for Collaborative Filtering|1
601|Loopy Belief Propagation for Approximate Inference: An Empirical Study|Recently, researchers have demonstrated that  &#034;loopy belief propagation&#034; --- the use of  Pearl&#039;s polytree algorithm in a Bayesian  network with loops --- can perform well  in the context of error-correcting codes.  The most dramatic instance of this is the  near Shannon-limit performance of &#034;Turbo  Codes&#034; --- codes whose decoding algorithm  is equivalent to loopy belief propagation in a  chain-structured Bayesian network.  In this paper we ask: is there something special  about the error-correcting code context,  or does loopy propagation work as an approximate  inference scheme in a more general  setting? We compare the marginals computed  using loopy propagation to the exact  ones in four Bayesian network architectures,  including two real-world networks: ALARM  and QMR. We find that the loopy beliefs often  converge and when they do, they give a  good approximation to the correct marginals.  However, on the QMR network, the loopy beliefs  oscillated and had no obvious relationship  ...
602|Learning to Extract Symbolic Knowledge from the World Wide Web|The World Wide Web is a vast source of information  accessible to computers, but understandable  only to humans. The goal of the research described  here is to automatically create a computer  understandable world wide knowledge base whose  content mirrors that of the World Wide Web. Such a 
603|Context-Specific Independence in Bayesian Networks|Bayesiannetworks provide a languagefor qualitatively  representing the conditional independence properties  of a distribution. This allows a natural and compact  representation of the distribution, eases knowledge acquisition,  and supports effective inference algorithms.
604|Object-Oriented Bayesian Networks|Bayesian networks provide a modeling language and associated inference algorithm for stochastic domains. They have been successfully applied in a variety of medium-scale applications. However, when faced with a large complex domain, the task of modeling using Bayesian networks begins to resemble the task of programming using logical circuits. In this paper, we describe an object-oriented Bayesian network (OOBN) language, which allows complex domains to be described in terms of inter-related objects. We use a Bayesian network fragment to describe the probabilistic relations between the attributes of an object. These attributes can themselves be objects, providing a natural framework for encoding part-of hierarchies. Classes are used to provide a reusable probabilistic model which can be applied to multiple similar objects. Classes also support inheritance of model fragments from a class to a subclass, allowing the common aspects of related classes to be defined only once. Our language h...
605|Probabilistic Frame-Based Systems|Two of the most important threads of work in knowledge representation today are frame-based representation systems (FRS&#039;s) and Bayesian networks (BNs). FRS&#039;s provide an excellent representation for the organizational structure of large complex domains, but their applicability is limited because of their inability to deal with uncertainty and noise. BNs provide an intuitive and coherent probabilistic representation of our uncertainty, but are very limited in their ability to handle complex structured domains. In this paper, we provide a language that cleanly integrates these approaches, preserving the advantages of both. Our approach allows us to provide natural and compact definitions of probability models for a class, in a way that is local to the class frame. These models can be instantiated for any set of interconnected instances, resulting in a coherent probability distribution over the instance properties. Our language also allows us to represent important types of uncertainty tha...
606|A Bayesian approach to learning Bayesian networks with local structure|Recently several researchers have investigated techniques for using data to learn Bayesian networks containing compact representations for the conditional probability distributions (CPDs) stored at each node. The majority of this work has concentrated on using decision-tree representations for the CPDs. In addition, researchers typically apply non-Bayesian (or asymptotically Bayesian) scoring functions such as MDL to evaluate the goodness-of-fit of networks to the data. In this paper we investigate a Bayesian approach to learning Bayesian networks that contain the more general decision-graph representations of the CPDs. First, we describe how to evaluate the posterior probability— that is, the Bayesian score—of such a network, given a database of observed cases. Second, we describe various search spaces that can be used, in conjunction with a scoring function and a search procedure, to identify one or more high-scoring networks. Finally, we present an experimental evaluation of the search spaces, using a greedy algorithm and a Bayesian scoring function. 1
607|Learning Belief Networks in the Presence of Missing Values and Hidden Variables|In recent years there has been a flurry of works on learning probabilistic belief networks. Current state of the art methods have been shown to be successful for two learning scenarios: learning both network structure and parameters from  complete data, and learning parameters for a fixed network from incomplete data---that is, in the presence of missing values or hidden variables. However, no method has yet been demonstrated to effectively learn network structure from incomplete data. In this paper, we propose a new method for learning network structure from incomplete data. This method is based on an extension of the  Expectation-Maximization (EM) algorithm for model selection problems that performs search for the best structure inside the EM procedure. We prove the convergence of this algorithm, and adapt it for learning belief networks. We then describe how to learn networks in two scenarios: when the data contains missing values, and in the presence of hidden variables. We provide...
608|Unsupervised Learning from Dyadic Data|Dyadic data refers to a domain with two finite sets of objects in which observations are made for dyads, i.e., pairs with one element from either set. This includes event co-occurrences, histogram data, and single stimulus preference data as special cases. Dyadic data arises naturally in many applications ranging from computational linguistics and information retrieval to preference analysis and computer vision. In this paper, we present a systematic, domain-independent framework for unsupervised learning from dyadic data by statistical mixture models. Our approach covers different models with flat and hierarchical latent class structures and unifies probabilistic modeling and structure discovery. Mixture models provide both, a parsimonious yet flexible parameterization of probability distributions with good generalization performance on sparse data, as well as structural information about data-inherent grouping structure. We propose an annealed version of the standard Expectation Maximization algorithm for model fitting which is empirically evaluated on a variety of data sets from different domains.
609|Answering Queries from Context-Sensitive Probabilistic Knowledge Bases|We define a language for representing context-sensitive probabilistic knowledge. A knowledge base consists of a set of universally quantified probability sentences that include context constraints, which allow inference to be focused on only the relevant portions of the probabilistic knowledge. We provide a declarative semantics for our language. We present a query answering procedure which takes a query Q and a set of evidence E and constructs a Bayesian network to compute P (QjE). The posterior probability is then computed using any of a number of Bayesian network inference algorithms. We use the declarative semantics to prove the query procedure sound and complete. We use concepts from logic programming to justify our approach. Keywords: reasoning under uncertainty, Bayesian networks, Probability model construction, logic programming Submitted to Theoretical Computer Science special issue on Uncertainty in Databases and Deductive Systems. This work was partially supported by NSF g...
610|Probabilistic Reasoning for Complex Systems|ii
611|Learning Statistical Models from Relational Data|This workshop is the second in a series of workshops held in conjunction with AAAI and IJCAI. The first workshop was held in July, 2000 at AAAI. Notes from that workshop are available at
612|Learning Probabilities for Noisy First-Order Rules|First-order logic is the traditional basis for knowledge representation languages. However, its applicability to many real-world tasks is limited by its inability to represent uncertainty. Bayesian belief networks, on the other hand, are inadequate for complex KR tasks due to the limited expressivity of the underlying (propositional) language. The need to incorporate uncertainty into an expressive language has led to a resurgence of work on first-order probabilistic logic. This paper addresses one of the main objections to the incorporation of probabilities into the language: &#034;Where do the numbers come from?&#034; We present an approach that takes a knowledge base in an expressive rule-based first-order language, and learns the probabilistic parameters associated with those rules from data cases. Our approach, which is based on algorithms for learning in traditional Bayesian networks, can handle data cases where many of the relevant aspects of the situation are unobserved. It is also capabl...
613|Prospective assessment of ai technologies for fraud detection: A case study|In September 1995, the Congressional O ce of Technology Assessment completed a study of the potential for AI technologies to detect money laundering by screening wire transfers. The study, conducted at the request of the Senate Permanent Subcommittee on Investigations, evaluates the technical and public policy implications of widespread use of AI technologies by the Federal government for fraud detection. Its conclusions are relevant tomanyother uses of AI technologies for fraud detection in both the public and private sectors.
614|PRL: A probabilistic relational language|In this paper, we describe the syntax and semantics for a probabilistic relational language (PRL). PRL is a recasting of recent work in Probabilistic Relational Models (PRMs) into a logic programming framework. We show how to represent varying degrees of complexity in the semantics including attribute uncertainty, structural uncertainty and identity uncertainty. Our approach is similar in spirit to the work in Bayesian Logic Programs (BLPs), and Logical Bayesian Networks (LBNs). However, surprisingly, there are still some important differences in the resulting formalism; for example, we introduce a general notion of aggregates based on the PRM approaches. One of our contributions is that we show how to support richer forms of structural uncertainty in a probabilistic logical language than have been previously described. Our goal in this work is to present a unifying framework that supports all of the types of relational uncertainty yet is based on logic programming formalisms. We also believe that it facilitates understanding the relationship between the frame-based approaches and alternate logic programming approaches, and allows greater
615|Learning in graphical models|Statistical applications in fields such as bioinformatics, information retrieval, speech processing, image processing and communications often involve large-scale models in which thousands or millions of random variables are linked in complex ways. Graphical models provide a general methodology for approaching these problems, and indeed many of the models developed by researchers in these applied fields are instances of the general graphical model formalism. We review some of the basic ideas underlying graphical models, including the algorithmic ideas that allow graphical models to be deployed in large-scale data analysis problems. We also present examples of graphical models in bioinformatics, error-control coding and language processing. 
616|Convex Analysis|In this book we aim to present, in a unified framework, a broad spectrum of mathematical theory that has grown in connection with the study of problems of optimization, equilibrium, control, and stability of linear and nonlinear systems. The title Variational Analysis reflects this breadth. For a long time, ‘variational ’ problems have been identified mostly with the ‘calculus of variations’. In that venerable subject, built around the minimization of integral functionals, constraints were relatively simple and much of the focus was on infinite-dimensional function spaces. A major theme was the exploration of variations around a point, within the bounds imposed by the constraints, in order to help characterize solutions and portray them in terms of ‘variational principles’. Notions of perturbation, approximation and even generalized differentiability were extensively investigated. Variational theory progressed also to the study of so-called stationary points, critical points, and other indications of singularity that a point might have relative to its neighbors, especially in association with existence theorems for differential equations.
617|Factor Graphs and the Sum-Product Algorithm|A factor graph is a bipartite graph that expresses how a &#034;global&#034; function of many variables factors into a product of &#034;local&#034; functions. Factor graphs subsume many other graphical models including Bayesian networks, Markov random fields, and Tanner graphs. Following one simple computational rule, the sum-product algorithm operates in factor graphs to compute---either exactly or approximately---various marginal functions by distributed message-passing in the graph. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative &#034;turbo&#034; decoding algorithm, Pearl&#039;s belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform algorithms. 
618|Monte Carlo Statistical Methods|This paper is also the originator of the Markov Chain Monte Carlo methods developed in the following chapters. The potential of these two simultaneous innovations has been discovered much latter by statisticians (Hastings 1970; Geman and Geman 1984) than by of physicists (see also Kirkpatrick et al. 1983).  5.5.5 ] PROBLEMS 211
619|Low-Density Parity-Check Codes|Preface The Noisy Channel Coding Theorem discovered by C. E. Shannon in 1948 offered communication engineers the possibility of reducing error rates on noisy channels to negligible levels without sacrificing data rates. The primary obstacle to the practical use of this theorem has been the equipment complexity and the computation time required to decode the noisy received data.
621|Dynamic Bayesian Networks: Representation, Inference and Learning|Modelling sequential data is important in many areas of science and engineering. Hidden Markov models (HMMs) and Kalman filter models (KFMs) are popular for this because they are simple and flexible. For example, HMMs have been used for speech recognition and bio-sequence analysis, and KFMs have been used for problems ranging from tracking planes and missiles to predicting the economy. However, HMMs
and KFMs are limited in their “expressive power”. Dynamic Bayesian Networks (DBNs) generalize HMMs by allowing the state space to be represented in factored form, instead of as a single discrete random variable. DBNs generalize KFMs by allowing arbitrary probability distributions, not just (unimodal) linear-Gaussian. In this thesis, I will discuss how to represent many different kinds of models as DBNs, how to perform exact and approximate inference in DBNs, and how to learn DBN models from sequential data.
In particular, the main novel technical contributions of this thesis are as follows: a way of representing
Hierarchical HMMs as DBNs, which enables inference to be done in O(T) time instead of O(T 3), where T is the length of the sequence; an exact smoothing algorithm that takes O(log T) space instead of O(T); a simple way of using the junction tree algorithm for online inference in DBNs; new complexity bounds on exact online inference in DBNs; a new deterministic approximate inference algorithm called factored frontier; an analysis of the relationship between the BK algorithm and loopy belief propagation; a way of
applying Rao-Blackwellised particle filtering to DBNs in general, and the SLAM (simultaneous localization
and mapping) problem in particular; a way of extending the structural EM algorithm to DBNs; and a variety of different applications of DBNs. However, perhaps the main value of the thesis is its catholic presentation of the field of sequential data modelling.
622|The Infinite Hidden Markov Model|We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters define a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be infinite---consider, for example, symbols being possible words appearing in English text.
623|Design of capacity-approaching irregular low-density parity-check codes|We design low-density parity-check (LDPC) codes that perform at rates extremely close to the Shannon capacity. The codes are built from highly irregular bipartite graphs with carefully chosen degree patterns on both sides. Our theoretical analysis of the codes is based on [1]. Assuming that the underlying communication channel is symmetric, we prove that the probability densities at the message nodes of the graph possess a certain symmetry. Using this symmetry property we then show that, under the assumption of no cycles, the message densities always converge as the number of iterations tends to infinity. Furthermore, we prove a stability condition which implies an upper bound on the fraction of errors that a belief-propagation decoder can correct when applied to a code induced from a bipartite graph with a given degree distribution. Our codes are found by optimizing the degree structure of the underlying graphs. We develop several strategies to perform this optimization. We also present some simulation results for the codes found which show that the performance of the codes is very close to the asymptotic theoretical bounds. 
624|Complexity of finding embeddings in a k-tree|  A k-tree is a graph that can be reduced to the k-complete graph by a sequence of removals of a degree k vertex with completely connected neighbors. We address the problem of determining whether a graph is a partial graph of a k-tree. This problem is motivated by the existence of polynomial time algorithms for many combinatorial problems on graphs when the graph is constrained to be a partial k-tree for fixed k. These algorithms have practical applications in areas such as reliability, concurrent broadcasting and evaluation of queries in a relational database system. We determine the complexity status of two problems related to finding the smallest number k such that a given graph is a partial k-tree. First, the corresponding decision problem is NP-complete. Second, for a fixed (predetermined) value of k, we present an algorithm with polynomially bounded (but exponential in k) worst case time complexity. Previously, this problem had only been solved for k = 1,2,3.
625|A family of algorithms for approximate Bayesian inference|One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense. This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible. This method, &#034;Expectation Propagation,&#034; unifies and generalizes two previous techniques: assumeddensity filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. The unification shows how both of these algorithms can be viewed as approximating the true posterior distribution with a simpler distribution, which is close in the sense of KL-divergence. Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. Loopy belief propagation, because it propagates exact belief states, is useful for limited types of belief networks, such as purely discrete networks. Expectation Propagati...
626|The generalized distributive law |Abstract—In this semitutorial paper we discuss a general message passing algorithm, which we call the generalized dis-tributive law (GDL). The GDL is a synthesis of the work of many authors in the information theory, digital communications, signal processing, statistics, and artificial intelligence communities. It includes as special cases the Baum–Welch algorithm, the fast Fourier transform (FFT) on any finite Abelian group, the Gal-lager–Tanner–Wiberg decoding algorithm, Viterbi’s algorithm, the BCJR algorithm, Pearl’s “belief propagation ” algorithm, the Shafer–Shenoy probability propagation algorithm, and the turbo decoding algorithm. Although this algorithm is guaranteed to give exact answers only in certain cases (the “junction tree ” condition), unfortunately not including the cases of GTW with cycles or turbo decoding, there is much experimental evidence, and a few theorems, suggesting that it often works approximately even when it is not supposed to. Index Terms—Belief propagation, distributive law, graphical models, junction trees, turbo codes. I.
627|A Variational Bayesian Framework for Graphical Models|This paper presents a novel practical framework for Bayesian model  averaging and model selection in probabilistic graphical models.  Our approach approximates full posterior distributions over model  parameters and structures, as well as latent variables, in an analytical  manner. These posteriors fall out of a free-form optimization  procedure, which naturally incorporates conjugate priors. Unlike  in large sample approximations, the posteriors are generally nonGaussian  and no Hessian needs to be computed. Predictive quantities  are obtained analytically. The resulting algorithm generalizes  the standard Expectation Maximization algorithm, and its convergence  is guaranteed. We demonstrate that this approach can be  applied to a large class of models in several domains, including  mixture models and source separation.  1 Introduction  A standard method to learn a graphical model  1  from data is maximum likelihood (ML). Given a training dataset, ML estimates a single optimal value f...
628|The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length|. We propose and analyze a distribution learning algorithm for variable memory length Markov processes. These processes can be described by a subclass of probabilistic finite automata which we name Probabilistic Suffix Automata (PSA). Though hardness results are known for learning distributions generated by general probabilistic automata, we prove that the algorithm we present can efficiently learn distributions generated by PSAs. In particular, we show that for any target PSA, the KL-divergence between the distribution generated by the target and the distribution generated by the hypothesis the learning algorithm outputs, can be made small with high confidence in polynomial time and sample complexity. The learning algorithm is motivated by applications in human-machine interaction. Here we present two applications of the algorithm. In the first one we apply the algorithm in order to construct a model of the English language, and use this model to correct corrupted text. In the second ...
629|Axioms for probability and belief-function propagation|In this paper, we describe an abstract framework and axioms under which exact local computation of marginals is possible. The primitive objects of the framework are variables and valuations. The primitive operators of the framework are combination and marginalization. These operate on valuations. We state three axioms for these operators and we derive the possibility of local computation from the axioms. Next, we describe a propagation scheme for computing marginals of a valuation when we have a factorization of the valuation on a hypertree. Finally we show how the problem of computing marginals of joint probability distributions and joint belief functions fits the general framework. 1.
630|Propagation Algorithms for Variational Bayesian Learning|Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set.
631|Linear Time Inference in Hierarchical HMMs|The hierarchical hidden Markov model (HHMM) is a generalization of  the hidden Markov model (HMM) that models sequences with structure  at many length/time scales [FST98]. Unfortunately, the original inference  algorithm is rather complicated, and takes O(T    ) time, where T is  the length of the sequence, making it impractical for many domains. In  this paper, we show how HHMMs are a special kind of dynamic Bayesian  network (DBN), and thereby derive a much simpler inference algorithm,  which only takes O(T ) time. Furthermore, by drawing the connection  between HHMMs and DBNs, we enable the application of many standard  approximation techniques to further speed up inference.
632|Loopy Belief Propagation and Gibbs Measures|We address the question of convergence in the  loopy belief propagation (LBP) algorithm.
633|Graphical models and automatic speech recognition|Graphical models provide a promising paradigm to study both existing and novel techniques for automatic speech recognition. This paper first provides a brief overview of graphical models and their uses as statistical models. It is then shown that the statistical assumptions behind many pattern recognition techniques commonly used as part of a speech recognition system can be described by a graph – this includes Gaussian distributions, mixture models, decision trees, factor analysis, principle component analysis, linear discriminant analysis, and hidden Markov models. Moreover, this paper shows that many advanced models for speech recognition and language processing can also be simply described by a graph, including many at the acoustic-, pronunciation-, and language-modeling levels. A number of speech recognition techniques born directly out of the graphical-models paradigm are also surveyed. Additionally, this paper includes a novel graphical analysis regarding why derivative (or delta) features improve hidden Markov model-based speech recognition by improving structural discriminability. It also includes an example where a graph can be used to represent language model smoothing constraints. As will be seen, the space of models describable by a graph is quite large. A thorough exploration of this space should yield techniques that ultimately will supersede the hidden Markov model.
634|Mixed memory Markov models: decomposing complex stochastic processes as mixtures of simpler ones|. We study Markov models whose state spaces arise from the Cartesian product of two or more discrete random variables. We show how to parameterize the transition matrices of these models as a convex combination---or mixture---of simpler dynamical models. The parameters in these models admit a simple probabilistic interpretation and can be fitted iteratively by an Expectation-Maximization (EM) procedure. We derive a set of generalized Baum-Welch updates for factorial hidden Markov models that make use of this parameterization. We also describe a simple iterative procedure for approximately computing the statistics of the hidden states. Throughout, we give examples where mixed memory models provide a useful representation of complex stochastic processes. Keywords: Markov models, mixture models, discrete time series 1. Introduction  The modeling of time series is a fundamental problem in machine learning, with widespread applications. These include speech recognition (Rabiner, 1989), natu...
635|Empirical and Hierarchical Bayesian Estimation of Ancestral States| Several methods have been proposed to infer the states at the ancestral nodes on a phylogeny. These methods assume a specific tree and set of branch lengths when estimating the ancestral character state. Inferences of the ancestral states, then, are conditioned on the tree and branch lengths being true. We develop a hierarchical Bayes method for inferring the ancestral states on a tree. The method integrates over uncertainty in the tree, branch lengths, and substitution model parameters by using Markov chain Monte Carlo. We compare the hierarchical Bayes inferences of ancestral states with inferences of ancestral states made under the assumption that a specific tree is correct. We find that the methods are correlated, but that accommodating uncertainty in parameters of the phylogenetic model can make inferences of ancestral states even more uncertain than they would be in an empirical Bayes analysis.
636|Design of Provably Good Low-Density Parity Check Codes|We design sequences of low-density parity check codes that provably perform at rates extremely close to the Shannon capacity. The codes are built from highly irregular bipartite graphs with carefully chosen degree patterns on both sides. Our theoretical analysis of the codes is based on [1]. Additionally, based on the assumption that the underlying communication channel is symmetric, we prove that the probability densities at the message nodes of the graph satisfy a certain symmetry. This enables us to derive a succinct description of the density evolution for the case of a belief propagation decoder. Furthermore, we prove a stability condition which implies an upper bound on the fraction of errors that a belief propagation decoder can correct when applied to a code induced from a bipartite graph with a given degree distribution. Our codes are found by optimizing the degree structure of the underlying graphs. We develop several strategies to perform this optimization. We also present s...
637|Multilocus linkage analysis by blocked Gibbs sampling|The problem of multilocus linkage analysis is expressed as a graphical model, making explicit a previously implicit connection, and recent developments in the field are described in this context. A novel application of blocked Gibbs sampling for Bayesian networks is developed to generate inheritance matrices from an irreducible Markov chain. This is used as the basis for reconstruction of historical meiotic states and approximate calculation of the likelihood function for the location of an unmapped genetic trait. We believe this to be the only approach that currently makes fully informative multilocus linkage analysis possible on large extended pedigrees.
638|Semidefinite Relaxations for Approximate Inference on Graphs With Cycles|We present a new method for calculating approximate marginals for probability distributions  defined by graphs with cycles, based on a Gaussian entropy bound combined with a semidefinite  outer bound on the marginal polytope. This combination leads to a log-determinant maximization  problem that can be solved by efficient interior point methods [13]. As with the Bethe  approximation and its generalizations [18], the optimizing arguments of this problem can be  taken as approximations to the exact marginals. In contrast to Bethe/Kikuchi approaches,  our variational problem is strictly convex and so has a unique global optimum. An additional  desirable feature is that the value of the optimal solution is guaranteed to provide an upper  bound on the log partition function. Such upper bounds are of interest in their own right (e.g.,  for parameter estimation, large deviations exponents, combinatorial enumeration). Finally, we  show that taking the zero-temperature limit of our log-determinant relaxation recovers a class  of well-known semidefinite relaxations for integer programming [e.g., 6].
639|Mixture models in measurement error problems, with reference to epidemiological studies|This paper focuses on Bayesian measurement error problems and on the question of the specication of the prior distribution of the unknown covariates. It presents a exible semiparametric model for this distribution based on a mixture of normals with an unknown number of components. Implementation of this prior model as part of a full Bayesian analysis of measurement error problems is described in classical set-ups encountered in epidemiological studies: logistic regression between unknown covariates and outcome, with normal or lognormal error model and a validation group. The feasibility of this combined model is tested and its performance is demonstrated in a simulation study that includes an assessment of the inuence of misspecication of the prior distribution of the unknown covariates and a comparison with the semiparametric ML method of Roeder, Carroll and Lindsay (1996). Finally, the methodology is illustrated on a data set on coronary heart disease and blood level of cholester
640|General Lower Bounds based on Computer Generated Higher Order Expansions|In this article we show the rough outline of a computer algorithm to generate lower bounds on the exponential function of (in principle) arbitrary precision. We implemented this to generate all necessary analytic terms for the Boltzmann machine partition function thus leading to lower bounds of any order. It turns out that the extra variational parameters can be optimized analytically. We show that bounds upto nineth order are still reasonably calculable in practical situations. The generated terms can also be used as extra correction terms (beyond tap) in mean field expansions.
641|Resource Description Framework (RDF) Model and Syntax Specification  (1998) |This document is a revision of the public working draft dated 1998-08-19 incorporating suggestions received in review comments and further deliberations of the W3C RDF Model and Syntax Working Group. With the publication of this draft, the RDF Model and Syntax Specification enters &#034;last call.&#034; The last call period will end on October 23, 1998. Comments on this specification may be sent to www-rdf-comments@w3.org. The archive of public comments is available at http://www.w3.org/Archives/Public/www-rdf-comments. Significant changes from the previous draft are highlighted in Appendix E. While we do not anticipate substantial changes, we still caution that further changes are possible. Therefore while we encourage active implementation to test this specification we also recommend that only software that can be easily field-upgraded be implemented to this specification at this time. This is a W3C Working Draft for review by W3C members and other interested parties. Publication as a working draft does not imply endorsement by the W3C membership. The RDF Model and Syntax  Working Group will not allow early implementation to constrain their ability to make changes to this specification prior to final release. This is a draft document and may be updated, replaced or obsoleted by other documents at any time. It is inappropriate to cite W3C Working Drafts as other than &#034;work in progress&#034;. This work is part of the W3C Metadata Activity.
642|The Coordination of Arm Movements: An Experimentally Confirmed Mathematical Model|This paper presents studies of the coordination of volun-tary human arm movements. A mathematical model is for-mulated which is shown to predict both the qualitative fea-tures and the quantitative details observed experimentally in planar, multijoint arm movements. Coordination is modeled mathematically by defining an objective function, a measure of performance for any possi-ble movement. The unique trajectory which yields the best performance is determined using dynamic optimization the-ory. In the work presented here, the objective function is the square of the magnitude of jerk (rate of change of accelera-tion) of the hand integrated over the entire movement. This is equivalent to assuming that a major goal of motor coordi-nation is the production of the smoothest possible movement
643|Posture control and trajectory formation during arm movement|One hypothesis for the generation of spatially oriented arm movements by the central nervous system is that a desired joint position is determined by the ratio of the tensions of agonist and antagonist muscles. According to this hypothesis, the transition between equilibrium states should be solely a function of the contraction time of the motor units and the mechanical properties of the arm. We tested this hypothesis in intact and deafferented rhesus monkeys by holding the forearm and measuring the accelerative transient after release of the forearm and by directly measuring the time course of the increase in torque during the movement. Both methods indicated an average time of 400 msec for attaining peak torque in a movement with a duration of 700 msec. In addition, by displacing the arm from its normal trajectory during the movement., we observed that the arm returned neither to the initial nor to the final equilibrium positions, but to points intermediate between them. We conclude that the nrocesses underlying traiectorv formation must be more complex than a simple switch between one equilibrium position and another. ”-A major problem in the study of motor control has been elucidation of the mechanisms directing the arm to a new spatial location. A distinction is often made (Brooks et al.,
644|Invariant characteristics of a pointing movement in man|Simple arm movements involving forward projection of the hand toward a target were studied by measuring simultaneous wrist position in three-dimensional space and changes in elbow angle. An attempt was made to identify those features of the movement which exhibit invariant characteristics under the hypothesis that such invariances may reflect the operations by which central processes participate in the organization of the movement. The first such invariance to be identified was that the trajectory in space is independent of movement speed. Secondly, the movement can be viewed as consisting of two phases, an acceleratory phase and a deceleratory one, with the movement during the acceleratory phase being so organized as to maintain the ratio of elbow angular velocity to shoulder angular velocity invariant with respect to target location in the deceleratory phase. It is suggested that proprioceptive information is used to control the movement and that the latter invariance may result from a negative feedback of force involving tendon organ afferents. Learned movements have been characterized by aspects such as the reaction time to their initiation, speed and accuracy and the inter-relationships between these factors and target size and location (cf., Fitts, 1954; Glencross, 1977; Keele, 1968; Prablanc et al., 1979; Woodworth, 1899). By contrast, relatively little work has been done on the manner in which even simple skilled movements are executed in three-dimensional space (Beggs
645|Estimating Wealth Effects without Expenditure Data— or Tears|Abstract: We use the National Family Health Survey (NFHS) data collected in Indian states in 1992 and 1993 to estimate the relationship between household wealth and the probability a child (aged 6 to 14) is enrolled in school. A methodological difficulty to overcome is that the NFHS, modeled closely on the Demographic and Health Surveys (DHS), measures neither household income nor consumption expenditures. As a proxy for long-run household wealth we construct a linear index from a set of asset indicators using principal components analysis to derive the weights. This “asset index ” is robust, produces internally coherent results, and provides a close correspondence with State Domestic Product (SDP) and poverty rates data. We validate the asset index using data from Indonesia, Pakistan and Nepal which contain data on both consumption expenditures and asset ownership. The asset index has reasonable coherence with current consumption expenditures and most importantly, works as well, or better, than traditional expenditure based measures in predicting enrollment status. When the asset index is applied to the Indian data the results show large, and variable, wealth gaps in the enrollment of children across states of India. While on average across India a rich (top 20 percent of the asset index) child is 31 percentage points more likely to be enrolled than a poor child (bottom 40 percent), this wealth gap varies from only 4.6 in Kerala, to 38.2 in Uttar Pradesh and 42.6 percentage points in Bihar. The findings, interpretations, and conclusions expressed in this paper are entirely those of the authors. They do not necessarily represent the views of the World Bank, its Executive Directors, or the countries they represent. Estimating Wealth Effects without Expenditure Data-- or Tears: An Application to Educational Enrollments in States of India 1
646|The effect of household wealth on educational attainment: evidence from 35 countries,” Population and Development Review 25(1  (1999) |Abstract. We use household survey data from the Demographic and Health Surveys (DHS) from 44 surveys (in 35 countries) to document different patterns in the enrollment and attainment of children from rich and poor households. We overcome the lack of income or expenditure data in the DHS by constructing a proxy for long-run wealth of the household from the asset information in the survey using the statistical technique of principal components. There are three major findings. First, the enrollment profiles of the poor differ across countries but fall into distinctive regional patterns: in some regions the poor reach nearly universal enrollment in first grade, but then drop out in droves leading to low attainment (typical of South America),while in other regions the poor never enroll in school (typical of South Asia and Western/Central Africa). Second, there are enormous differences across countries in the “ wealth gap, ” the difference in enrollment and educational attainment of the rich and poor. While in some countries the difference in the median years of school completed of the rich and poor is only a year or two, in other countries the wealth gap in attainment is 9 or 10 years. Third, the attainment profiles can be used as diagnostic tools to examine issues in the educational system, such as the extent to which low enrollment is due to physical unavailability of schools. The findings, interpretations, and conclusions expressed in this paper are entirely those of the authors. They do not necessarily represent the views of the World Bank, its Executive Directors, or the countries they represent. The Effect of Household Wealth on Educational Attainment Around the World: Demographic and Health Survey Evidence 1
648|Household Income and Child Schooling in Vietnam.” World Bank Economic Review 13(2  (1999) |The stronger are the associations between household income and child schooling, the lower is intergenerational social mobility and the less equal is opportunity. This study estimates the associations between household income and children&#039;s school success in Vietnam. The estimates indicate that these associations are considerable. For example, the income elasticity of completed grades is five times the median estimate of earlier studies. Moreover, this association is strongest for grades completed per year of school, not for completed grades, on which most of the previous literature has focused. There are some gender differences, the most important being a smaller association between income and grades completed per year of school for boys than for girls. This difference implies that schooling of girls is treated as more of a luxury (less of a necessity) than is schooling of boys. This article also investigates some ways in which policies relate to household in-comes. School fees are progressive, but school fees are only about one-third of what households pay directly to schools and are a much smaller proportion of a household&#039;s total school-related expenditures. Total household expenditures paid directly to schools
649|Generalized Additive Models|Likelihood based regression models, such as the normal linear regression model and the linear logistic model, assume a linear (or some other parametric) form for the covariate effects. We introduce the Local Scotinq procedure which replaces the liner form C Xjpj by a sum of smooth functions C Sj(Xj)a The Sj(.) ‘s are unspecified functions that are estimated using scatterplot smoothers. The technique is applicable to any likelihood-based regression model: the class of Generalized Linear Models contains many of these. In this class, the Locul Scoring procedure replaces the linear predictor VI = C Xj@j by the additive predictor C ai ( hence, the name Generalized Additive Modeb. Local Scoring can also be applied to non-standard models like Cox’s proportional hazards model for survival data. In a number of real data examples, the Local Scoring procedure proves to be useful in uncovering non-linear covariate effects. It has the advantage of being completely automatic, i.e. no “detective work ” is needed on the part of the statistician. In a further generalization, the technique is modified to estimate the form of the link function for generalized linear models. The Local Scoring procedure is shown to be asymptotically equivalent to Local Likelihood estimation, another technique for estimating smooth covariate functions. They are seen to produce very similar results with real data, with Local Scoring being considerably faster. As a theoretical underpinning, we view Local Scoring and Local Likelihood as empirical maximizers of the ezpected log-likelihood, and this makes clear their connection to standard maximum likelihood estimation. A method for estimating the “degrees of freedom” of the procedures is also given.
650|On distortion functionals |The high-pressure phase transitions of silicon and gallium
651|THE ESTIMATION OF SMOOTH CURVES*|Smooth curves a.re often used to illustrate the relationship between two vari-ables. They are also an important building block in many recent statistical models. A procedure to estimate such a curve is called a smoother. This paper discusses currently available smoothers and introduces the class of maximum likelihood smoothers. A variety of other statistical techniques are shown to be applicable to the problem of smoothing, and some idea of the scope of models that can benefit from the use of smoothing is given.
652|A model of growth through creative destruction|  This paper develops a model based on Schumpeter&#039;s process of creative destruction. It departs from existing models of endogeneous growth in emphasizing obsolescence of old technologies induced by the accumulation of knowledge and the resulting process or industrial innovations. This has both positive and normative implications for growth. In positive terms, the prospect of a high level of research in the future can deter research today by threatening the fruits of that research with rapid obsolescence. In normative terms, obsolescence creates a negative externality from innovations, and hence a tendency for laissez-faire economies to generate too many innovations, i.e too much growth. This &#034;business-stealing&#034; effect is partly compensated by the fact that innovations tend to be too small under laissez-faire. The model possesses a unique balanced growth equilibrium in which the log of GNP follows a random walk with drift. The size of the drift is the average growth rate of the economy and it is endogeneous to the model; in particular it depends on the size and likilihood of innovations resulting from research and also on the degree of market power available to an innovator.
654|Self-fulfilling Prophecies|After a quarter century of unbroken development in the theory of allocation under uncertainty, it has become an obvious fact that randomness in endowments, preferences or technology will generally work its way to the prices and allocations which prevail in equilibrium. Is it true, as intuition may suggest in haste, that random prices necessarily reflect some intrinsic uncertainty in the structure of the economy, or can they arise, as some recent literature [3, 12, 14, 151 indicates, merely from extraneous, self-perpetuating beliefs that prices are stochastic? The question is of interest for it raises the possibility that business cycles are set in motion by arbitrary shifts in any factor, however purely subjective, agents happen to deem relevant to economic activity: animal spirits, consumer sentiment or the prophecies of the Sibyl at Cumae may spark fluc-tuations in which prices change simply because they are expected to and price signals convey no structural information. The evidence on the influence of subjective factors is ample and dates
655|Are Output Fluctuations Transitory|(Article begins on next page) The Harvard community has made this article openly available. Please share how this access benefits you. Your story matters.
656|Learning by|According to the authors, this book is recommended for upper--intermediate to advanced ESL (English as a second language) students who are enrolled in advanced writing courses and “who have had exposure to basic rhetorical styles, and have experience writing essays with a thesis statement and supporting points ” (p. xi). It is clear that the book assumes a working knowledge of thesis statements, paragraph development, and other basic writing skills. Sourcework gives limited, if any, explanation to these skills and seeks to build upon them. As explained in the section for teachers, the overarching goal of the book is to create a learning community in which students collaborate in their exploration of and writing about given topics and the introduction to each chapter explicitly states the goals and the content for the chapter. The text is organized into two main sections. Part One, The Writing Process, is divided into six chapters that aim to help students understand how to write a research essay by walking them through the steps. Each chapter follows a similar TESL-EJ 16.4, March 2013 Dolahite &amp; Huan/Ballard 1format providing: an introduction; a writing concept or skill; a comprehension
657|A Contribution to the Empirics of Economic Growth|This paper examines whether the Solow growth model is consistent with the international variation in the standard of living. It shows that an augmented Solow model that includes accumulation of human as well as physical capital provides an excellent description of the cross-country data. The paper also examines the implications of the Solow model for convergence in standards of living, that is, for whether poor countries tend to grow faster than rich countries. The evidence indicates that, holding population growth and capital accumulation constant, countries converge at about the rate the augmented Solow model predicts.
659|The Economics of Inequality|Measures of inequality are used by economists to answer a wide range of questions. Is the distribution of income more equal than it was in the past? Are underdeveloped countries characterised by greater inequality than advanced countries? Do taxes lead to greater equality in the distri-
661|An Empirical Study of Smoothing Techniques for Language Modeling|We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods. 1
662|A Maximum-Entropy-Inspired Parser|We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &#034;stan- dard&#034; sections of the Wall Street Journal tree- bank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innova- tion is the use of a &#034;maximum-entropy-inspired&#034; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head&#039;s pre-terminal before guessing the lexical head.
663|Class-Based n-gram Models of Natural Language|We address the problem of predicting a word from previous words in a sample of text. In particular we discuss n-gram models based on calsses of words. We also discuss several statistical algoirthms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.
664|Generation and Synchronous Tree-Adjoining Grammars|Tree-adjoining grammars (TAG) have been proposed as a formalism for generation based on the intuition that the extended domain of syntactic locality that TAGs provide should aid in localizing semantic dependencies as well, in turn serving as an aid to generation from semantic representations. We demonstrate that this intuition can be made concrete by using the formalism of synchronous tree-adjoining grammars. The use of synchronous TAGs for generation provides solutions to several problems with previous approaches to TAG generation. Furthermore, the semantic monotonicity requirement previously advocated for generation gram- mars as a computational aid is seen to be an inherent property of synchronous TAGs.
665|An Efficient Boosting Algorithm for Combining Preferences|The problem of combining preferences arises in several applications, such as combining the results of different search engines. This work describes an efficient algorithm for combining multiple preferences. We first give a formal framework for the problem. We then describe and analyze a new boosting algorithm for combining preferences called RankBoost. We also describe an efficient implementation of the algorithm for certain natural cases. We discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different WWW search strategies, each of which is a query expansion for a given domain. For this task, we compare the performance of RankBoost to the individual search strategies. The second experiment is a collaborative-filtering task for making movie recommendations. Here, we present results comparing RankBoost to nearest-neighbor and regression algorithms.
666|Lexical-Functional Grammar:  A Formal System for Grammatical Representation|In learning their native language, children develop a remarkable set of capabilities. They acquire knowledge and skills that enable them to produce and comprehend an indefinite number of novel utterances, and to make quite subtle judgments about certain of their properties. The major goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities. In pursuing this goal, we have adopted what we call the Competence Hypothesis as a methodological principle. We assume that an explanatory model of human language performance will incorporate a theoretically justi ed representation of the native speaker&#039;s linguistic knowledge (a grammar) as a component separate both from the computational mechanisms that operate on it (a processor) and from other nongrammatical processing parameters that might influence the processor&#039;s behavior.  To a certain extent the various components that we postulate can be studied independently, guided where appropriate by the well-established methods and evaluation standards of linguistics, computer science, and experimental psychology. However, the requirement that the various components ultimately must fit together in a consistent and coherent model imposes even stronger constraints on their structure and operation.
667|Three Generative, Lexicalised Models for Statistical Parsing|In this paper we first propose a new statistical  parsing model, which is a generative  model of lexicalised context-free gram-  mar. We then extend the model to in-  clude a probabilistic treatment of both subcategorisation  and wh~movement. Results  on Wall Street Journal text show that the  parser performs at 88.1/87.5% constituent  precision/recall, an average improvement  of 2.3% over (Collins 96).
668|A New Statistical Parser Based on Bigram Lexical Dependencies|This paper describes a new statistical  parser which is based on probabilities of  dependencies between head-words in the  parse tree. Standard bigram probability estimation  techniques are extended to calculate  probabilities of dependencies between  pairs of words. Tests using Wall Street  Journal data show that the method per-  forms at least as well as SPATTER (Magerman  95; Jelinek et al. 94), which has  the best published results for a statistical  parser on this task. The simplicity of the  approach means the model trains on 40,000  sentences in under 15 minutes. With a  beam search strategy parsing speed can be  improved to over 200 sentences a minute  with negligible loss in accuracy.
669|Statistical Parsing with a Context-free Grammar and Word Statistics|We describe a parsing system based upon a language  model for English that is, in turn, based upon assigning  probabilities to possible parses for a sentence. This  model is used in a parsing system by finding the parse  for the sentence with the highest probability. This system  outperforms previous schemes. As this is the third  in a series of parsers by different authors that are similar  enough to invite detailed comparisons but different  enough to give rise to different levels of performance,  we also report on some experiments designed to identify  what aspects of these systems best explain their  relative performance.  Introduction  We present a statistical parser that induces its grammar and probabilities from a hand-parsed corpus (a tree-bank). Parsers induced from corpora are of interest both as simply exercises in machine learning and also because they are often the best parsers obtainable by any method. That is, if one desires a parser that produces trees in the tree-bank ...
670|Statistical Decision-Tree Models for Parsing|Syntactic natural language parsers have  shown themselves to be inadequate for processing  highly-ambiguous large-vocabulary  text, as is evidenced by their poor per-  formance on domains like the Wall Street  Journal, and by the movement away  from parsing-based approaches to textprocessing  in general. In this paper, I describe  SPATTER, a statistical parser based  on decision-tree learning techniques which  constructs a complete parse for every sentence  and achieves accuracy rates far better  than any published result. This work  is based on the following premises: (1)  grammars are too complex and detailed to  develop manually for most interesting domains;  (2) parsing models must rely heavily  on lexical and contextual information  to analyze sentences accurately; and (3)  existing n-gram modeling techniques are  inadequate for parsing models. In experiments  comparing SPATTER with IBM&#039;s  computer manuals parser, SPATTER significantly  outperforms the grammar-based  parser. Evaluating SPATTER against the  Penn Treebank Wall Street Journal corpus  using the PARSEVAL measures, SPATTER  achieves 86% precision, 86% recall,  and 1.3 crossing brackets per sentence for  sentences of 40 words or less, and 91% precision,  90% recall, and 0.5 crossing brackets  for sentences between 10 and 20 words in  length.
671|The Penn Treebank: Annotating Predicate Argument Structure|The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as &#034;underlying &#034; position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles. 1. INTRODUCTION During the first phase of the The Penn Treebank project [10], ending in December 1992, 4.5 million words of text were tagged for part-of-speech, with about two-thirds of this material also annotated with a skeletal syntactic bracketing. All of this material has been hand corre...
673|Three New Probabilistic Models for Dependency Parsing: An Exploration|After presenting a novel O(n³) parsing algorithm  for dependency grammar, we develop  three contrasting ways to stochasticize  it. We propose (a) a lexical affinity model  where words struggle to modify each other,  (b) a sense tagging model where words fluctuate  randomly in their selectional preferences,  and (c) a generative model where  the speaker fleshes out each word&#039;s syntactic  and conceptual structure without regard to  the implications for the hearer. We also give  preliminary empirical results from evaluating  the three models&#039; parsing performance  on annotated Wall Street Journal training  text (derived from the Penn Treebank). In  these results, the generative model performs  significantly better than the others, and  does about equally well at assigning part-of-speech tags.  
674|Treebank Grammars|By a “tree-bank grammar ” we mean a context-free grammar created by reading the production rules directly from hand-parsed sentences in a tree bank. Common wisdom has it that such grammars do not perform we &amp; though we know of no published data on the issue. The primary purpose of this paper is to show that the common wisdom is wrong. In particular, we present results on a tree-bank grammar based on the Penn WaII Street Journal tree bank. To the best of our knowledge, this grammar outperforms ah other non-word-based statistical parsers/grammars on this corpus. That is, it outperforms parsers that consider the input as a string of tags and ignore the actual words of the corpus.
675|Gemini: A Natural Language System For Spoken-Language Understanding|This paper describes the details of the system, and includes relevant measurements of size, efficiency, and performance of each of its components
676|A Linear Observed Time Statistical Parser Based on Maximum Entropy Models|This paper presents a statistical parser for  natural language that obtains a parsing  accuracy--roughly 87% precision and 86%  recall--which surpasses the best previously  published results on the Wall St. Journal  domain. The parser itself requires very little  human intervention, since the information  it uses to make parsing decisions is  specified in a concise and simple manner,  and is combined in a fully automatic way  under the maximum entropy framework.
677|A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation|I.n this paper, we describe a new corpus-based ap  proach to prepositional phrase attachment disambiguation,  and 10resent results comparing perlbrmance  of this algorithm with ol,her corpus-based  approaches to this problem.
678|Towards History-based Grammars: Using Richer Models for Probabilistic Parsing|We describe a generarive probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.
681|A Statistical Parser for Czech|This paper considers statistical parsing of Czech, which differs radically from English in at least two  respects: (1) it is a highly infiected language, and (2) it has relatively free word order. These dif- ferences are likely to .pose new problems for tech- niques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results - 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.
682|Automatic Grammar Induction and Parsing Free Text: A Transformation-Based Approach|In this paper we describe a new technique for parsing free text: a transformational grammar  is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.
683|Equations for Part-of-Speech Tagging|We derive from first principles the basic equations for a few of the basic hidden-Markov-model word taggers as well as equations for other models which may be novel (the descriptions in previous papers being too spare to be sure). We give performance results for all of the models. The results from our best model (96.45% on an unused test sample from the Brown corpus with 181 distinct tags) is on the upper edge of reported results. We also hope these results clear up some confusion in the literature about the best equations to use. However, the major purpose of this paper is to show how the equations for a variety of models may be derived and thus encourage future authors to give the equations for their model and the derivations thereof.  Introduction  The last few years have seen a fair number of papers on part-of-speech tagging --- assigning the correct part of speech to each word in a text [1,2,4,5,7,8,9,10]. Most of these systems view the text as having been produced by a hidden Mar...
684|A novel use of statistical parsing to extract information from text|Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.
685|Efficient Parsing for Bilexical Context-Free Grammars and Head Automaton Grammars|Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words. We present O(n^4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of O(n^5). For a common special case that was known to allow O(n³) parsing (Eisner, 1997), we present an O(n³) algorithm with an improved grammar constant.
686|Parsing Inside-Out|Probabilistic Context-Free Grammars (PCFGs) and variations on them have recently become some of the most common formalisms for parsing. It is common with PCFGs to compute the inside and outside probabilities. When these probabilities are multiplied together and normalized, they produce the probability that any given non-terminal covers any piece of the input sentence. The traditional use of these probabilities is to improve the probabilities of grammar rules. In this thesis we show that these values are useful for solving many other problems in Statistical Natural Language Processing. We give a framework for describing parsers. The framework generalizes the inside and outside values to semirings. It makes it easy to describe parsers that compute a wide variety of interesting quantities, including the inside and outside probabilities, as well as related quantities such as Viterbi probabilities and n-best lists. We also present three novel uses for the inside and outside probabilities. T...
687|Learning Parse and Translation Decisions from Examples with Rich Context|We present a knowledge and context-based system for parsing and translating natural language and evaluate it on sentences from the Wall Street Journal. Applying machine learning techniques, the system uses parse action examples acquired under supervision to generate a deterministic shift-reduce parser in the form of a decision structure. It relies heavily on context, as encoded in features which describe the morphological, syntactic, semantic and other aspects of a given parse state.
688|Efficient Algorithms for Parsing the DOP Model|Excellent results have been reported for DataOriented Parsing (DOP) of natural language texts (Bod, 1993c). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo p arsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model toga small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct con- stituents, rather than the probability of a correct parse tree. Using ithe optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is compara- ble to results from a duplication of Pereira and Schabes&#039;s (1992) experiment on the same data. We show that Bod&#039;s results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.
689|Conditional Structure versus Conditional Estimation in NLP Models|This paper separates conditional parameter estimation,  which consistently raises test set accuracy on  statistical NLP tasks, from conditional model structures,  such as the conditional Markov model used  for maximum-entropy tagging, which tend to lower  accuracy. Error analysis on part-of-speech tagging  shows that the actual tagging errors made by the  conditionally structured model derive not only from  label bias, but also from other ways in which the independence  assumptions of the conditional model  structure are unsuited to linguistic sequences. The  paper presents new word-sense disambiguation and  POS tagging experiments, and integrates apparently  conflicting reports from other recent work.
691|Pearl: A Probabilistic Chart Parser|This i)al)cr descrihcs a natural language i)ars - ing algorith,n for unrestricted text whicll uses a i)rol)ability-based scoring fimctiou to select the &#034;}mst&#034; I)arse of a sentence. The parser, earl, is a I. ime-a.synchronous bottom-ul) chart I)arscr with Earicy-type top-down prediction which pursues the highest-scoring theory i} the chart, where the score of a theory represents the cxteut I,o which t. he context of the sentmice predicts that iuterpretation. This parser differs h&#039;om previous attempts at stochastic parsers in thai. it uses a richer form of conditional probalfilitics based on context to l)rcdiet likelihood. Pearl also provides a fralnework for incorporating l.he results of previous work iu Imrt-olLsl)cech assignnmnt, mlknown word models, and ol.her Irol)al)ilistic models of linguistic features iuto one parslug tool, interleaving these techniques instead of using the traditional pipeline archiLecture. In preliminary tests, &#039;Pearl has been successl&#039;ul aL resolving parL-o[-speech and word (in speech processing) ambiguiLy, de[ermiuing categories [or unknown words, and selecLing cotreeL parses first. using a very loosely fiLing covering grammar. 1  
692|Development and Evaluation of a Broad-Coverage Probabilistic Grammar of English-Language Computer Manuals|We present an approach to grammar development where the task is decomposed into two separate subtasks. The first task is linguistic, with the goal of producing a set of rules that have a large coverage (in the sense that the correct parse is among the proposed parses) on a blind test set of sentences. The second task is statistical, with the goal of developing a model of the grammax which assigns maximum probability for the correct parse. We give parsing results on text from computer manuals.
693|Decision tree parsing using a hidden derivation model|Parser development is generally viewed as a primarily linguistic enterprise. A grammarian examines sentences, skillfully extracts the linguistic generalizations evident in the data, and writes grammar rules which cover the language. The grammarian
694|Efficiency, Robustness and Accuracy in Picky Chart Parsing|This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called probabilistic prediction to predict which grammar rules are likely to lead to an acceptable parse of the input. Using a suboptimal search method, Picky significantly reduces the number of edges produced by CKY-like chart parsing algorithms, while maintaining the robustness of pure bottom-up parsers and the accuracy of existing probabilistic parsers. Experiments using Picky demonstrate how probabilistic modelling can impact upon the efficiency, robustness and accuracy of a parser. 
695|Head Automata and Bilingual Tiling: Translation with Minimal Representations|We present a language model consisting of  a collection of costed bidirectional finite  state automata associated with the head  words of phrases. The model is suitable  for incremental application of lexical associations  in a dynamic programming search  for optimal dependency tree derivations. We also
696|Corpus Statistics Meet the Noun Compound: Some Empirical Results|A variety of statistical methods for noun  compound analysis are implemented and  compared. The results support two main  conclusions. First, the use of conceptual  association not only enables a broad coverage,  but also improves the accuracy. Second,  an analysis model based on dependency  grammar is substantially more accurate  than one based on deepest constituents,  even though the latter is more preva-  lent in the literature.
697|Context-Sensitive Statistics for Improved Grammatical Language Models|We develop a language model using probabilistic context-free grammars (PCFGs) that is &#034;pseudo context-sensitive&#034; in that the probability that a non-terminal N expands using a rule r depends on N &#039;s parent. We derive the equations for estimating the necessary probabilities using a variant of the inside-outside algorithm. We give experimental results showing that, beginning with a high-performance PCFG, one can develop a pseudo PCSG that yields significant performance gains. Analysis shows that the benefits from the context-sensitive statistics are localized, suggesting that we can use them to extend the original PCFG. Experimental results confirm that this is both feasible and the resulting grammar retains the performance gains. This implies that our scheme may be useful as a novel method for PCFG induction. 1 Introduction  Like its non-stochastic brethren, probabilistic parsing has been based upon context-free grammars (CFGs), and for similar reasons: CFGs support a simple and efficien...
698|Statistical Parsing of Messages|The recent trend in natural language processing research has been to develop systems that deal with text concerning small, well defined domains. One practical application for such systems is to process messages pertaining to some very specific task or activity [5]. The advantage of dealing with such domains is twofold- firstly, due to the narrowness of the domain, it is possible to encode most of the knowledge related to the domain and to make this knowledge accessible to the natural language processing system, which in turn can use this knowledge to disambiguate the meanings of the messages. Secondly, in such a domain, there is not a great diversity of language constructs and therefore it becomes easier to construct a grammar which will capture all the constructs which exist in this sub-language. However, some practical aspects of such domains tend to make the problem somewhat difficult. Often, the messages tend not to be absolutely grammatically correct. As a result, the grammar designed for such a system needs to be far more forgiving than one designed for the task of parsing edited English. This can result in a proliferation of parses, which in turn makes the disambiguation task more difficult. This problem is further compounded by the telegraphic nature of the discourse, since telegraphic discourse is more prone to be syntactically ambiguous. Statistical Parsing The major objective of the research described in this paper is to use statistical data to evaluate the likelihood of a parse in order to help the parser prune out unlikely parses. Our conjecture- supported by our results and some prior, similar experiments- is that a more probable parse has a greater chance of being the correct one. The related work by the research team at UCREL
699|Global Thresholding and Multiple-Pass Parsing|We present a variation on classic beam  thresholding techniques that is up to an order  of magnitude faster than the traditional  method, at the same performance level. We  also present a new thresholding technique,  global thresholding, which, combined with  the new beam thresholding, gives an additional  factor of two improvement, and a  novel technique, multiple pass parsing, that  can be combined with the others to yield  yet another 50% improvement. We use a  new search algorithm to simultaneously op-  timize the thresholding parameters of the  various algorithms.
700|Probabilistic Feature Grammars|We present a new formalism, probabilistic feature grammar (PFG). PFGs combine most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance. PFGs generate features one at a time, probabilistically, conditioning the probabilities of each feature on other features in a local context. Because the conditioning is local, efficient polynomial time parsing algorithms exist for computing inside, outside, and Viterbi parses. PFGs can produce probabilities of strings, making them potentially useful for language modeling. Precision and recall results are comparable to the state of the art with words, and the best reported without words. 1 Introduction  Recently, many researchers have worked on statistical parsing techniques which try to capture additional context beyond that of simple probabilistic context-free grammars (PCFGs), including Magerman (1995), Charniak (1996), Collins (1996; 1997), ...
701|Stochastic HPSG|In this paper we provide a probabilistic  interpretation for typed feature structures  very similar to those used by Pollard  nd Sag. We begin with a version  of the interpretation which lacks  a treatment of re-entrant feature struc-  tures, then provide an extended interpre-  tation which allows them. We sketch al-  gorithms allowing the numerical parameters  of our probabilistic interpretations  of HPSG to be estimated from corpora.
702|What is the Minimal Set of Fragments that Achieves Maximal Parse Accuracy?|We aim at finding the minimal set of  fragments which achieves maximal parse  accuracy in Data Oriented Parsing. Experiments  with the Penn Wall Street  Journal treebank show that counts of  almost arbitrary fragments within parse  trees are important, leading to improved  parse accuracy over previous models  tested on this treebank (a precision of 90.8% and a recall of 90.6%). We  isolate some dependency relations which  previous models neglect but which  contribute to higher parse accuracy.
703|Automatic Learning for Semantic Collocation|The real difficulty in development of practical NLP systems comes from the fact that we do not have effective means for gathering &#034;knowledge &#034;. In this paper, we propose an algorithm which acquires automatically knowledge of semantic collocations among &#034;words&#034; from sample corpora. The algorithm
704|A Statistical Model for Parsing and Word-Sense Disambiguation|This paper describes a first attempt at a statistical model for simultaneous syntactic parsing and generalized word-sense disambiguation. On a new data set we have constructed for the task, while we were disappointed not to find parsing improvement over a traditional parsing model, our model achieves a recall of 84.0% and a precision of 67.3% of exact synset matches on our test corpus, where the gold standard has a reported inter-annotator agreement of 78.6%.
705|On The Unsupervised Induction Of Phrase-Structure Grammars|This paper examines why some previous approaches have failed to acquire desired grammars without supervision, and proposes that with a different conception of phrase-structure supervision might not be necessary. In particular, it describes in detail some reasons why SCFGs are poor mod-  2 CARL DE MARCKEN els to use for learning human language, especially when combined with the inside-outside algorithm. Following up on these arguments, it proposes that head-driven grammatical formalisms like link grammars (Sleator and Temperley, 1991) are better suited to the task, and introduces a framework for CFG induction that sidesteps many of the search problems that previous schemes have had. In the end, we hope the analysis presented here convinces others to look carefully at their representations and search strategies before blindly applying them to the language learning task. We start the discussion by examining the differences between the linguistic and statistical motivations for phrase structure; this frames our subsequent analysis. Then we introduce a simple extension to stochastic context-free grammars, and use this new class of language models in two experiments that pinpoint specific problems with both SCFGs and the search strategies commonly applied to them. Finally, we explore fixes to these problems.
706|The Effect of Alternative Tree Representations on Tree Bank Grammars|The performance of PCFGs estimated from  tree banks is shown to be sensitive to the particular  way in which linguistic constructions  are represented as trees in the tree bank. This  paper presents a theoretical analysis of the  effect of different tree representations for PP  attachment on PCFG models, and introduces  a new methodology for empirically examining  such effects using tree transformations. It  shows that one transformation, which copies  the label of a parent node onto the labels of  its children, can improve the performance of  a PCFG model in terms of labelled precision  and recall on held out data from 73% (precision)  and 69% (recall) to 80% and 79% respectively.  It also points out that if only maximum  likelihood parses are of interest then  many productions can be ignored, since they  are subsumed by combinations of other productions  in the grammar. In the Penn II tree  bank grammar, almost 9% of productions are  subsumed in this way.  1 
707|A Probabilistic Parser Applied to Software Testing Documents|We describe an approach to training a statistical parser from a bracketed corpus, and demonstrate its use in a software testing application that translates English specifications into an automated testing language. A grammar is not explicitly specified; the rules and contextual probabilities of occurrence are automatically generated from the corpus. The parser is extremely successful at producing and identifying the correct parse, and nearly deterministic in the number of parses that it produces. To compensate for undertraining, the parser also uses general, linguistic subtheories which aid in guessing some types of novel structures.  Introduction  In constrained domains, natural language processing can often provide leverage. In software testing at AT&amp;T, for example, 20,000 English test cases prescribe the behavior of a telephone switching system. A test case consists of about a dozen sentences describing the goal of the test, the actions to perform, and the conditions to verify. Figu...
708|A Probabilistic Parser and Its Application|We describe a general approach to the probabilistic parsing of context-free grammars. The method integrates context-sensitive statistical knowledge of various types (e.g., syntactic and semantic) and can be trained incrementally from a bracketed corpus. We introduce a variant of the GHR contextfree recognition algorithm, and explain how to adapt it for efficient probabilistic parsing. On a real-world corpus of sentences from software testing documents, with 23 possible parses for a sentence of average length, the system accurately finds the correct parse in 99% of cases, while producing only 1.02 parses per sentence. Significantly, the success rate would be only 66% without the semantic statistics.  Introduction  In constrained domains, natural language processing can often provide leverage. At AT&amp;T, for instance, NL technology can potentially help automate many aspects of software development. A typical example occurs in the software testing area. Here 250,000 English sentences specif...
709|Government spending in a simple model of endogenous growth|(Article begins on next page) The Harvard community has made this article openly available. Please share how this access benefits you. Your story matters.
710|Is public expenditure productive|This paper considers the relationship between aggregate productivity and stock and flow government-spending variables. The empirical results indicate that (i) the nonmilitary public capital stock is dramatically more important in determining productivity than is either the flow of nonmilitary or military spending, (ii) military capital bears little relation to productivity, and (iii) a &#039;core &#039; infrastructure of streets, highways, airports, mass transit, sewers, water systems, etc. has most explanatory power for productivity. The paper also suggests an important role for the net public capital stock in the &#039;productivity slowdown &#039; of the last fifteen years. 1.
712|From data mining to knowledge discovery in databases|¦ Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field. Across a wide variety of fields, data are
714|The Merge/Purge Problem for Large Databases|Many commercial organizations routinely gather large numbers of databases for various marketing and business analysis functions. The task is to correlate information from different databases by identifying distinct individuals that appear in a number of different databases typically in an inconsistent and often incorrect fashion. The problem we study here is the task of merging data from multiple sources in as efficient manner as possible, while maximizing the accuracy of the result. We call this the merge/purge problem. In this paper we detail the sorted neighborhood method that is used by some to solve merge/purge and present experimental results that demonstrates this approach may work well in practice but at great expense. An alternative method based upon clustering is also presented with a comparative evaluation to the sorted neighborhood method. We show a means of improving the accuracy of the results based upon a multi-pass approach that succeeds by computing the Transitive Clos...
715|Unexpectedness as a Measure of Interestingness in Knowledge Discovery|Organizations are taking advantage of &#034;data-mining&#034; techniques to leverage the vast amounts of data captured as they process routine transactions. Data-mining is the process of discovering hidden structure or patterns in data. However several of the pattern discovery methods in datamining systems have the drawbacks that they discover too many obvious or irrelevant patterns and that they do not leverage to a full extent valuable prior domain knowledge that managers have. This research addresses these drawbacks by developing ways to generate interesting patterns by incorporating managers&#039; prior knowledge in the process of searching for patterns in data. Specifically we focus on providing methods that generate unexpected patterns with respect to managerial intuition by eliciting managers&#039; beliefs about the domain and using these beliefs to seed the search for unexpected patterns in data. Our approach should lead to the development of decision support systems that provide managers with mor...
716|The interestingness of deviations|gps~gte.com, matheus~gte.com One of the moet promising areas in Knowledge Discovery in Databases is the automatic analysis of changes and deviations. Several systems have recently been developed for this task. Suc ~ of these systems hinges on their ability to identify s few important and relevant deviations among the multitude of potentially interesting events:  ~ In this paper we argue that related deviations should be grouped togetherin a finding and that the interestingness of a finding is the estimated benefit from a poesible ~tion connected to it. We discuss methods for determining the estimated benefit from the impact of the deviations and the success probability of an action. Our analysis is done in the context of the Key Findings Reporter (KEFIIt), a system for discovering and explaining ~key findings &#034; in large relational databases, currently being applied to the analysis of healthcare information.
717|The World Wide Web: quagmire or gold mine?|This article considers the question: is effective Web mining possible?
718|Discovering Informative Patterns and Data Cleaning|We present a method for discovering informative patterns from data. With this method, large databases can be reduced to only a few representative data entries. Our framework also encompasses methods for cleaning databases containing corrupted data. Both on-line and off-line algorithms are proposed and experimentally checked on databases of handwritten images. The generality of the framework makes it an attractive candidate for new applications in knowledge discovery.  Keywords: knowledge discovery, machine learning, informative patterns, data cleaning, information gain.  4.1 
719|Selecting Among Rules Induced from a Hurricane Database|can achieve orders of magnitude reduction in the volume of data For example, we applied a commercial tool (IXLtin) to a 1,819 record tropical storm database, yielding 161 rules. However, the human comprehension goals of Knowledge Discovery in Databases may require still more orders of magnitude. We present a rule refinement strategy, partly implemented in a Prolog program, that operationalizes &#034;interestingness &#034; into performance, simplicity, novelty, and significance. Applying the strategy to the induced rulebase yielded 10 &#034;genuinely interesting &#034; rules. I. PURPOSE OF THE STUDY At The Travelers Insurance Company, we are involved in applying statistics and artificial intelligence techniques to the solution of business problems. This work is part of an investigation into applications for Natural Hazards Research Services. The purpose of this study is not to deyelop a hurricane model or predictor. It is, rather, to assess the utility of rule induction technology and our particular rule refinement strategy. The object task of the study is to develop rules that
720|KDD for Science Data Analysis: Issues and Examples|Tile analysis of tile massive data sets collected by scientific instruments demands automation as a pre-requisite to analysis. There is an urgent need to cre-ate an intermediate level at which scientists can oper-ate effectively; isolating them from the massive sizes and harnessing human analysis capabilities to focus on tasks in which machines do not even renmtely ap-proach humans--namely, creative data analysis, the-ory and hypothesis formation, and drawing insights into underlying phe,mmena. We give an overview of the main issues in the exploitation of scientific data.sets, present five c,~se studies where KDD tools play important and enabling roles, and conclude with fi,ture challenges for data mining and KDD techniques in science data analysis. 1
721| 	 Active Data Mining   |We introduce an active data mining paradigm that combines the recent work in data mining with the rich literature on active database systems. In this paradigm, data is continuously mined at a desired frequency. As rules are discovered, they are added to a rulebase, and if they already exist, the history of the statistical parameters associated with the rules is updated. When the history starts exhibiting certain trends, specified as shape queries in the user-speci ed triggers, the triggers are red and appropriate actions are initiated. To be able to specify shape queries, we describe the constructs for de ning shapes, and discuss how the shape predicates are used in a query construct to retrieve rules whose histories exhibit the desired trends. We describe how this query capability is integrated into a trigger system to realize an active mining system. The system presented here has been validated using two sets of customer data.
722|Fast Spatio-Temporal Data Mining of Large Geophysical Datasets|The important scientific challenge of understanding global climate change is one that clearly requires the application of knowledge discovery and datamining techniques on a massive scale. Advances in parallel supercomputing technology, enabling high-resolution modeling, as well as in sensor technology, allowing data capture on an unprecedented scale, conspire to overwhelm present-day analysis approaches. We present here early experiences with a prototype exploratory data analysis environment, CONQUEST, designed to provide content-based access to such massive scientific datasets. CONQUEST (CONtent-based Querying in Space and Time) employs a combination of workstations and massively parallel processors (MPP&#039;s) to mine geophysical datasets possessing a prominent temporal component. It is designed to enable complex multi-modal interactive querying and knowledge discovery, while simultaneously coping with the extraordinary computational demands posed by the scope of the datasets involved. A...
723|Predicting Equity Returns from Securities Data|Our experiments with capital markets data suggest that the domain can be effectively modeled by classification rules induced from available historical data for the purpose of making gainful predictions for equityinvestments. New classification techniques developed at IBM Research, including minimal rule generation (R-MINI) and contextual feature analysis, seem robust enough for consistently extracting useful information from noisy domains such as financial markets. We will briefly introduce the rationale for our minimal rule generation technique, and the motivation for the use of contextual information in analyzing features. We will then describe our experience from several experiments with the S&amp;P 500 data, illustrating the general methodology, and the results of correlations and simulated managed investment based on classification rules generated by R-MINI. Wewillsketchhow the rules for classifications can be effectively used for numerical prediction, and eventually to an investment ...
724|Graphical Models for Discovering Knowledge|There are many different ways of representing knowledge, and for each of these ways there are many different discovery algorithms. How can we compare different representations? How can we mix, match and merge representations and algorithms on new problems with their own unique requirements? This chapter introduces probabilistic modeling as a philosophy for addressing these questions and presents graphical models for representing probabilistic models. Probabilistic graphical models are a unified qualitative and quantitative framework for representing and reasoning with probabilities and independencies. 4.1 Introduction Perhaps one common element of the discovery systems described in this and previous books on knowledge discovery is that they are all different. Since the class of discovery problems is a challenging one, we cannot write a single program to address all of knowledge discovery. The KEFIR discovery system applied to health care by Matheus, Piatetsky-Shapiro, and McNeill (199...
725|An Overview of Issues in Developing Industrial Data Mining and Knowledge Discovery Applications|This paper surveys the growing number of indu5 trial applications of data mining and knowledge discovery. We look at the existing tools, describe some representative applications, and discuss the major issues and problems for building and deploying successful applications and their adoption by business users. Finally, we examine how to assess the potential of a knowledge discovery application. 1
726|Analyzing the Benefits of Domain Knowledge in Substructure Discovery|Discovering repetitive, interesting, and functional substructures in a structural  database improves the ability to interpret and compress the data. However, scientists  working with a database in their area of expertise often search for a predetermined  type of structure, or for structures exhibiting characteristics specic to the domain.  This paper presents methods for guiding the discovery process with domain-specic  knowledge. In this paper, the Subdue discovery system is used to evaluate the bene-  ts of using domain knowledge. The domain knowledge is incorporated into Subdue  following a single general methodology to guide the discovery process. Results show  using domain-specic knowledge improves the search for substructures which are useful  to the domain, and leads to greater compression of the data. To illustrate these bene-  ts, examples and experiments from the domain of computer programming, computer  aided design circuit, and a series of articially-generated domains...
727|A Simple Model of Capital Market Equilibrium with Incomplete Information|The sphere of modern financial economics encompases finance, micro investment theory and much of the economics of uncertainty. As is evident from its influence on other branches of economics including public finance, industrial organization and monetary theory, the boundaries of this sphere are both permeable and flexible. The complex interactions of time and uncertainty guarantee intellectual challenge and intrinsic excitement to the study of financial economics. Indeed, the mathematics of the subject contain some of the most interesting applications of probability and optimization theory. But for all its mathematical refinement, the research has nevertheless had a direct and significant influence on practice. It was not always thus. Thirty years ago, finance theory was little more than a collection of anecdotes, rules of thumb, and manipulations of accounting data with an almost exclusive focus on corporate financial management. There is no need in this meeting of the guild to recount the subsequent evolution from this conceptual potpourri to a rigorous economic
728|The Equity Premium: A Puzzle|Restrictions that a class of general equilibrium models place upon the average returns of equity and Treasury bills are found to be strongly violated by the U.S. data in the 1889-1978 period. This result is robust to model specification and measurement problems. We conclude that, most likely, an equilibrium model which is not an Arrow-Debreu economy will be the one that Simultaneously rationalizes both historically observed large average equity return and the small average risk-free return. 1.
730|The relationship between return and market value of common stocks|This study examines the empirical relattonship between the return and the total market value of NYSE common stocks. It is found that smaller firms have had htgher risk adjusted returns, on average, than larger lirms. This ‘size effect ’ has been in existence for at least forty years and is evidence that the capital asset pricing model is misspecttied. The size elfect is not linear in the market value; the main effect occurs for very small tirms while there is little difference m return between average sized and large firms. It IS not known whether size per se is responsible for the effect or whether size IS just a proxy for one or more true unknown factors correlated with size. 1.
732|Optimal Stock Trading with Personal Taxes: Implications for Prices and the Abnormal January Returns|wish to thank ny colleagues at the University of Chicago and
733|Taxes, Market Valuation and Corporate Financial Policy, Nat onal Tax|I N A well-known series of papers Franco
734|An introduction to hidden Markov models|The basic theory of Markov chains has been known to
735|A View Of The Em Algorithm That Justifies Incremental, Sparse, And Other Variants|. The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible. 1. Introduction The Expectation-Maximization (EM) algorithm finds maximum likelihood parameter estimates in problems where some variables were unobserved. Special cases of the algorithm date back several dec...
736|Hierarchical mixtures of experts and the EM algorithm|We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM’s). Learning is treated as a max-imum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parame-ters of the architecture. We also develop an on-line learning algorithm in which the pa-rameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain. 
737|Probabilistic Inference Using Markov Chain Monte Carlo Methods|Probabilistic inference is an attractive approach to uncertain reasoning and empirical learning in artificial intelligence. Computational difficulties arise, however, because probabilistic models with the necessary realism and flexibility lead to complex distributions over high-dimensional spaces. Related problems in other fields have been tackled using Monte Carlo methods based on sampling using Markov chains, providing a rich array of techniques that can be applied to problems in artificial intelligence. The &#034;Metropolis algorithm&#034; has been used to solve difficult problems in statistical physics for over forty years, and, in the last few years, the related method of &#034;Gibbs sampling&#034; has been applied to problems of statistical inference. Concurrently, an alternative method for solving problems in statistical physics by means of dynamical simulation has been developed as well, and has recently been unified with the Metropolis algorithm to produce the &#034;hybrid Monte Carlo&#034; method. In computer science, Markov chain sampling is the basis of the heuristic optimization technique of &#034;simulated annealing&#034;, and has recently been used in randomized algorithms for approximate counting of large sets. In this review, I outline the role of probabilistic inference in artificial intelligence, present the theory of Markov chains, and describe various Markov chain Monte Carlo algorithms, along with a number of supporting techniques. I try to present a comprehensive picture of the range of methods that have been developed, including techniques from the varied literature that have not yet seen wide application in artificial intelligence, but which appear relevant. As illustrative examples, I use the problems of probabilistic inference in expert systems, discovery of latent classes from data, and Bayesian learning for neural networks.
738|Hidden Markov models in computational biology: applications to protein modeling|Hidden.Markov Models (HMMs) are applied t.0 the problems of statistical modeling, database searching and multiple sequence alignment of protein families and protein domains. These methods are demonstrated the on globin family, the protein kinase catalytic domain, and the EF-hand calcium binding motif. In each case the parameters of an HMM are estimated from a training set of unaligned sequences. After the HMM is built, it is used to obtain a multiple alignment of all the training sequences. It is also used to search the. SWISS-PROT 22 database for other sequences. that are members of the given protein family, or contain the given domain. The Hi &#034; produces multiple alignments of good quality that agree closely with the alignments produced by programs that incorporate threedimensional structural information. When employed in discrimination tests (by examining how closely the sequences in a database fit the globin, kinase and EF-hand HMMs), the &#039;\ HMM is able to distinguish members of these families from non-members with a high degree of accuracy. Both the HMM and PROFILESEARCH (a technique used to search for relationships between a protein sequence and multiply aligned sequences) perform better in these tests than PROSITE (a dictionary of sites and patterns in proteins). The HMM appecvs to have a slight advantage over PROFILESEARCH in terms of lower rates of false
740|A Tutorial on Learning Bayesian Networks|We examine a graphical representation of uncertain knowledge called a Bayesian network. The representation is easy to construct and interpret, yet has formal probabilistic semantics making it suitable for statistical manipulation. We show how we can use the representation to learn new knowledge by combining domain knowledge with statistical data. 1 Introduction  Many techniques for learning rely heavily on data. In contrast, the knowledge encoded in expert systems usually comes solely from an expert. In this paper, we examine a knowledge representation, called a Bayesian network, that lets us have the best of both worlds. Namely, the representation allows us to learn new knowledge by combining expert domain knowledge and statistical data. A Bayesian network is a graphical representation of uncertain knowledge that most people find easy to construct and interpret. In addition, the representation has formal probabilistic semantics, making it suitable for statistical manipulation (Howard,...
741|The Infinite Gaussian Mixture Model|In a Bayesian mixture model it is not necessary a priori to limit the number of components to be finite. In this paper an infinite Gaussian mixture model is presented which neatly sidesteps the difficult problem of finding the &#034;right&#034; number of mixture components. Inference in the model is done using an efficient parameter-free Markov Chain that relies entirely on Gibbs sampling.
742|Probabilistic independence networks for hidden Markov probability models|Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper contains a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach. 
743|Stochastic simulation algorithms for dynamic probabilistic networks|Stochastic simulation algorithms such as likelihood weighting often give fast, accurate approximations to posterior probabilities in probabilistic networks, and are the methods of choice for very large networks. Unfortunately, the special characteristics of dynamic probabilistic networks (DPNs), which are used to represent stochastic temporal processes, mean that standard simulation algorithms perform very poorly. In essence, the simulation trials diverge further and further from reality as the process is observed over time. In this paper, we present simulation algorithms that use the evidence observed at each time step to push the set of trials back towards reality. The first algorithm, &amp;quot;evidence reversal &amp;quot; (ER) restructures each time slice of the DPN so that the evidence nodes for the slice become ancestors of the state variables. The second algorithm, called &amp;quot;survival of the fittest &amp;quot; sampling (SOF), &amp;quot;repopulates &amp;quot; the set of trials at each time step using a stochastic reproduction rate weighted by the likelihood of the evidence according to each trial. We compare the performance of each algorithm with likelihood weighting on the original network, and also investigate the benefits of combining the ER and SOF methods. The ER/SOF combination appears to maintain bounded error independent of the number of time steps in the simulation.
744|Hidden Markov Model Induction by Bayesian Model Merging|This paper describes a technique for learning both the number of states and the topology of Hidden Markov Models from examples. The induction process starts with the most specific model consistent with the training data and generalizes by successively merging states. Both the choice of states to merge and the stopping criterion are guided by the Bayesian posterior probability. We compare our algorithm with the Baum-Welch method of estimating fixed-size models, and find that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge.  
745|Mean Field Theory for Sigmoid Belief Networks|We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics.
746|Autoencoders, Minimum Description Length and Helmholtz Free Energy|An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights de ne the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and...
747|An Input Output HMM Architecture|We introduce a recurrent architecture having a modular structure  and we formulate a training procedure based on the EM algorithm.  The resulting model has similarities to hidden Markov models, but  supports recurrent networks processing style and allows to exploit  the supervised learning paradigm while using maximum likelihood estimation.
748|Exploiting Tractable Substructures in Intractable Networks|We develop a refined mean field approximation for inference and learning in probabilistic neural networks. Our mean field theory, unlike most, does not assume that the units behave as independent degrees of freedom; instead, it exploits in a principled way the existence of large substructures that are computationally tractable. To illustrate the advantages of this framework, we show how to incorporate weak higher order interactions into a first-order hidden Markov model, treating the corrections (but not the first order structure) within mean field theory. 1 INTRODUCTION  Learning the parameters in a probabilistic neural network may be viewed as a problem in statistical estimation. In networks with sparse connectivity (e.g. trees and chains), there exist efficient algorithms for the exact probabilistic calculations that support inference and learning. In general, however, these calculations are intractable, and approximations are required. Mean field theory provides a framework for app...
749|Multiple Viewpoint Systems for Music Prediction|This paper examines the prediction and generation of music using a multiple viewpoint system, a collection of independent views of the musical surface each of which models a specific type of musical phenomena. Both the general style and a particular piece are modeled using dual short-term and long-term theories, and the model is created using machine learning techniques on a corpus of musical examples. The models are
750|A hierarchical dirichlet language model|We discuss a hierarchical probabilistic model whose predictions are similar to those of the popular language modelling procedure known as &#039;smoothing&#039;. A number of interesting differences from smoothing emerge. The insights gained from a probabilistic view of this problem point towards new directions for language modelling. The ideas of this paper are also applicable to other problems such as the modelling of triphomes in speech, and DNA and protein sequences in molecular biology. The new algorithm is compared with smoothing on a two million word corpus. The methods prove to be about equally accurate, with the hierarchical model using fewer computational resources. 1
751|Ensemble Learning for Hidden Markov Models|The standard method for training Hidden Markov Models optimizes a point estimate of the model parameters. This estimate, which can be viewed as the maximum of a posterior probability density over the model parameters, may be susceptible to overfitting, and contains no indication of parameter uncertainty. Also, this maximummay be unrepresentative of the posterior probability distribution. In this paper we study a method in which we optimize an ensemble which approximates the entire posterior probability distribution. The ensemble learning algorithm requires the same resources as the traditional Baum--Welch algorithm. The traditional training algorithm for hidden Markov models is an expectation-- maximization (EM) algorithm (Dempster et al. 1977) known as the Baum--Welch algorithm. It is a maximum likelihood method, or, with a simple modification, a penalized maximum likelihood method, which can be viewed as maximizing a posterior probability density over the model parameters. Recently, ...
752|Mixtures of Controllers for Jump Linear and Non-linear Plants|We describe an extension to the Mixture of Experts architecture for modelling and controlling dynamical systems which exhibit multiple modes of behavior. This extension is based on a Markov process model, and suggests a recurrent network for gating a set of linear or non-linear controllers. The new architecture is demonstrated to be capable of learning effective control strategies for jump linear and non-linear plants with multiple modes of behavior. 1 Introduction  Many stationary dynamic systems exhibit significantly different behaviors under different operating conditions. To control such complex systems it is computationally more efficient to decompose the problem into smaller subtasks, with different control strategies for different operating points. When detailed information about the plant is available, gain scheduling has proven a successful method for designing a global control (Shamma and Athans, 1992). The system is partitioned by choosing several operating points and a line...
753|Factorial Learning and the EM Algorithm|Many real world learning problems are best characterized by an  interaction of multiple independent causes or factors. Discovering  such causal structure from the data is the focus of this paper. Based on 
754|Hidden Markov decision trees|We study a time series model that can be viewed as a decision tree with Markov temporal structure. The model is intractable for exact calculations, thus we utilize variational approximations. We consider three different distributions for the approximation: one in which the Markov calculations are performed exactly and the layers of the decision tree are decoupled, one in which the decision tree calculations are performed exactly and the time steps of the Markov chain are decoupled, and one in which a Viterbi-like assumption is made to pick out a single most likely state sequence. We present simulation results for artificial data and the Bach chorales. Accepted for oral presentation at NIPS*96. 1 Introduction  Decision trees are regression or classification models that are based on a nested decomposition of the input space. An input vector x is classified recursively by a set of &#034;decisions&#034; at the nonterminal nodes of a tree, resulting in the choice of a terminal node at which an output...
755|Learning fine motion by Markov mixtures of experts|Compliantcontrol is a standard method for performing fine manipulation tasks, like grasping and assembly,  but it requires estimation of the state of contact between the robot arm and the objects involved. Here  we present a method to learn a model of the movement from measured data. The method requires little  or no prior knowledge and the resulting model explicitly estimates the state of contact . The current state  of contact is viewed as the hidden state variable of a discrete HMM. The control dependent transition  probabilities between states are modeled as parametrized functions of the measurement. We showthat  their parameters can be estimated from measurements concurrently with the estimation of the parameters  of the movementineach state of contact . The learning algorithm is a variant of the EM procedure. The  E step is computed exactly# solving the M step exactly would require solving a set of coupled nonlinear  algebraic equations in the parameters. Instead, gradient ascent is used to produce an increase in likelihood.
756|Yago: A Core of Semantic Knowledge|We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains roughly 900,000 entities and 5,000,000 facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as hasWonPrize). The facts have been automatically extracted from the unification of Wikipedia and WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships – and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information
757|Why and Where: A Characterization of Data Provenance|With the proliferation of database views and curated databases,  the issue of data provenance # where a piece of data came from and the  process by which it arrived in the database # is becoming increasingly  important, especially in scienti#c databases where understanding provenance  is crucial to the accuracy and currency of data. In this paper we  describe an approach to computing provenance when the data of interest  has been created by a database query.We adopt a syntactic approach  and present results for a general data model that applies to relational  databases as well as to hierarchical data such as XML. A novel aspect of  our work is a distinction between #why&#034; provenance #refers to the source  data that had some in#uence on the existence of the data# and #where&#034;  provenance #refers to the location#s# in the source databases from which  the data was extracted#.
758|ULDBs: Databases with uncertainty and lineage|This paper introduces ULDBs, an extension of relational databases with simple yet expressive constructs for representing and manipulating both lineage and uncertainty. Uncertain data and data lineage are two important areas of data management that have been considered extensively in isolation, however many applications require the features in tandem. Fundamentally, lineage enables simple and consistent representation of uncertain data, it correlates uncertainty in query results with uncertainty in the input data, and query processing with lineage and uncertainty together presents computational benefits over treating them separately. We show that the ULDB representation is complete, and that it permits straightforward implementation of many relational operations. We define two notions of ULDB minimality—dataminimal and lineage-minimal—and study minimization of ULDB representations under both notions. With lineage, derived relations are no longer self-contained: their uncertainty depends on uncertainty in the base data. We provide an algorithm for the new operation of extracting a database subset in the presence of interconnected uncertainty. Finally, we show how ULDBs enable a new approach to query processing in probabilistic databases. ULDBs form the basis of the Trio system under development at Stanford.
759|Semantic Wikipedia|Wikipedia is the world&#039;s largest collaboratively edited source of encyclopaedic knowledge. But in spite of its utility, its contents are barely machine-interpretable. Structural knowledge, e. g. about how concepts are interrelated, can neither be formally stated nor automatically processed. Also the wealth of numerical data is only available as plain text and thus can not be processed by its actual meaning. We provide
760|Schema mediation in peer data management systems|permission of the IEEE. Such permission of the IEEE does not in any way imply IEEE endorsement of any of the University of Pennsylvania’s products or services. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to
761|The Chatty Web: Emergent Semantics Through Gossiping|This paper describes a novel approach for obtaining semantic interoperability among data sources in a bottom-up, semi-automatic manner without relying on pre-existing, global semantic models. We assume that large amounts of data exist that have been organized and annotated according to local schemas. Seeing semantics as a form of agreement, our approach enables the participating data sources to incrementally develop global agreement in an evolutionary and completely decentralized process that solely relies on pair-wise, local interactions: Participants provide translations between schemas they are interested in and can learn about other translations by routing queries (gossiping). To support the participants in assessing the semantic quality of the achieved agreements we develop a formal framework that takes into account both syntactic and semantic criteria. The assessment process is incremental and the quality ratings are adjusted along with the operation of the system. Ultimately, this process results in global agreement, i.e., the semantics that all participants understand. We discuss strategies to efficiently find translations and provide results from a case study to justify our claims. Our approach applies to any system which provides a communication infrastructure (existing websites or databases, decentralized systems, P2P systems) and offers the opportunity to study semantic interoperability as a global phenomenon in a network of information sharing parties.
762|What have Innsbruck and Leipzig in common? Extracting Semantics from Wiki Content|Abstract Wikis are established means for the collaborative authoring, versioning and publishing of textual articles. The Wikipedia project, for example, succeeded in creating the by far largest encyclopedia just on the basis of a wiki. Recently, several approaches have been proposed on how to extend wikis to allow the creation of structured and semantically enriched content. However, the means for creating semantically enriched structured content are already available and are, although unconsciously, even used by Wikipedia authors. In this article, we present a method for revealing this structured content by extracting information from template instances. We suggest ways to efficiently query the vast amount of extracted information (e.g. more than 8 million RDF statements for the English Wikipedia version alone), leading to astonishing query answering possibilities (such as for the title question). We analyze the quality of the extracted content, and propose strategies for quality improvements with just minor modifications of the wiki systems being currently used. 1
763|Practical Lineage Tracing in Data Warehouses|We consider the view data lineage problem in a warehousing environment: For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formalize the problem and present a lineage tracing algorithm for relational views with aggregation. Based on our tracing algorithm, we propose a number of schemes for storing auxiliary views that enable consistent and efficient lineage tracing in a multisource data warehouse. We report on a performance study of the various schemes, identifying which schemes perform best in which settings. Based on our results, we have implemented a lineage tracing package in the WHIPS data warehousing system prototype at Stanford. With this package, users can select view tuples of interest, then efficiently &#034;drill down&#034; to examine the source data that produced them. 1 Introduction Data warehousing systems collect data from multiple distributed sources, integrate the information as materialized v...
764|Wikipedia and the Semantic Web - The Missing Links|Wikipedia is the biggest collaboratively created source of encyclopaedic  knowledge. Growing beyond the borders of any traditional  encyclopaedia, it is facing new problems of knowledge management: The  current excessive usage of article lists and categories witnesses the fact  that 19th century content organization technologies like inter-article references  and indices are no longer su#cient for today&#039;s needs.
765|Crossing the Structure Chasm|It has frequently been observed that most of the world&#039;s data lies outside  database systems. The reason is that database systems focus on structured data, leaving the unstructured realm to others. The world of unstructured data has several very appealing properties, such as ease of authoring, querying and data sharing. In contrast, authoring, querying and sharing structured data require significant effort, albeit with the benefit of rich query languages and exact answers. We argue
767|Estimation and Inference in Econometrics|The astonishing increase in computer performance over the past two decades has made it possible for economists to base many statistical inferences on simulated, or bootstrap, distributions rather than on distributions obtained from asymptotic theory. In this paper, I review some of the basic ideas of bootstrap inference. The paper discusses Monte Carlo tests, several types of bootstrap test, and bootstrap confidence intervals. Although bootstrapping often works well, it does not do so in every case.
768|Using Geographic Variation in College Proximity to Estimate the Return to Schooling|Although schooling and earnings are highly correlated, social scientists have argued for decades over the causal effect of education. A convincing analysis of the causal link between education and earnings requires an exogenous source of variation in education outcomes. This paper explores the use of college proximity as an exogenous determinant of schooling. An examination of the NLS Young Men Cohort reveals that men who grew up in local labor markets with a nearby college have significantly higher education and significantly higher earnings than other men. The education and earnings gains are concentrated among men with poorlyeducated parents -- men who would otherwise stop schooling at relatively low levels. When college proximity is taken as an exogenous determinant of schooling the implied instrumental variables estimates of the return to schooling are 25-60% higher than conventional ordinary least squares estimates.
769|Learning by Doing and Learning from Others: Human Capital and Technical Change in Agriculture|University of PennsylvaniaHousehold-level panel data from a nationally representative sample of rural Indian households describing the adoption and profitability of high-yielding seed varieties (HYVs) associated with the Green Revolution are used to test the implications of a model incorporating learning-by-doing and learning spillovers. The estimates indicate that: (i) imperfect knowledge about the management of the new seeds was a significant barrier to adoption; (ii) this barrier diminished as farmer experience with the new technologies increased; (iii) own experience and neighbors &#039; experience with HYV significantly increased HYV profitability; (iv) farmers do not fully incorporate the village returns to learning in making adoption decisions 1 I.
770|Finishing High School and Starting College: Do Catholic Schools Make a Difference? Quarterly|In this paper, we consider two measures of the relative effectiveness of public and Catholic schools: finishing high school and starting college. These measures are potentially more important indicators of school quality than standardized test scores in light of the economic consequences of obtaining more education. Single-equation estimates suggest that for the typical student, attending a Catholic high school raises the probability of finishing high school or entering a four-year college by thirteen percentage points. In bivariate probit models we find almost no evidence that our single-equation estimates are subject to selection bias. I.
771|Estimating the Labor Market Impact of Voluntary Military Service Using Social Security Data on Military Applicants,” Econometrica 66:2|you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at
772|Measuring Positive Externalities from Unobservable Victim Precaution: An Empirical Analysis of Lojack.” Quarterly|Lojack is a hidden radio-transmitter device used for retrieving stolen vehicles. Because there is no external indication that Lojack has been installed, it does not directly affect the likelihood that a protected car will be stolen. There may, however, be positive externalities due to general deterrence. We find that the availability of Lojack is associated with a sharp fall in auto theft. Rates of other crime do not change appreciably. At least historically, the marginal social benefit of an additional unit of Lojack has been fifteen times greater than the marginal social cost in high crime areas. Those who install Lojack, however, obtain less than 10 percent of the total social benefits, leading to underprovision by the market. I.
773|Hedging Winner&#039;s Curse with Multiple Bids: Evidence from the Portuguese Treasury Bill Auction|Auctions of government securities typically permit bidders to enter multiple price-quantity bids. Despite the widespread adoption of this institutional feature and its use by bidders, the motivations behind its use and its e ects on auction outcomes are not well understood theoretically and have been little explored empirically. Using bidding data from treasury bill auctions in Portugal, this paper examines how bidders use multiple bids to hedge against winner&#039;s curse. The data show that, ceteris paribus, a bidder submits a greater number of bids and disperses prices on these bids more widely when there is a greater potential for winner&#039;s curse. In particular, both these measures of bid-spreading increase with the volatility of market interest rates and the expected number of participating well-informed bidders.
774|Pensions and Retirement: Evidence from Union Army Veterans. The Quarterly|I investigate the factors that fostered rising retirement rates prior to social security and most private-sector pensions by estimating the income effect of the first major pension program in the United Sates, that covering Union Army veterans. The elasticity of nonparticipation with respect to Union Army pension income was 0.73. The findings suggest that secularly rising income explains a substantial part of increased retirement rates. Comparisons with elasticities of nonparticipation with respect to social security income suggest that the elasticity of labor force nonpartici-pation may have decreased with time, perhaps because of the increasing attractive-ness of leisure. I. RETIREMENT SINCE THE TURN OF THE CENTURY Increasing numbers of men have permanently abandoned the labor force at ever younger ages during the twentieth century. In 1880 78 percent of men 65 years of age or older were in the labor force and in 1900 65 percent, whereas in 1930 the figure had dropped to 58 percent. But by 1980 the figure was less than 25 percent [Moen 1987; cf. Ransom and Sutch 1986]. Among men aged 55-64 and 45-64, labor force participation rates were 86 and
775|What Do We Know about Capital Structure? Some Evidence from International Data|We investigate the determinants of capital structure choice by analyzing the financing decisions of public firms in the major industrialized countries. At an aggregate level, firm leverage is fairly similar across the G-7 countries. We find that factors identified by previous studies as correlated in the cross-section with firm leverage in the U.S., are similarly correlated in other countries as well. However, a deeper examination of the U.S. and foreign evidence suggests that the theoretical underpinnings of the observed correlations are still largely unresolved.
776|Theory of the Firm: Managerial Behavior, Agency Costs and Ownership Structure|This paper integrates elements from the theory of agency, the theory of property rights and the theory of finance to develop a theory of the ownership structure of the firm. We define the concept of agency costs, show its relationship to the ‘separation and control’ issue, investigate the nature of the agency costs generated by the existence of debt and outside equity, demonstrate who bears costs and why, and investigate the Pareto optimality of their existence. We also provide a new definition of the firm, and show how our analysis of the factors influencing the creation and issuance of debt and equity claims is a special case of the supply side of the completeness of markets problem. 
777|Corporate Financing and Investment Decisions when Firms Have Information that Investors Do Not Have|This paper considers a firm that must issue common stock to raise cash to undertake a valuable investment opportunity. Management is assumed to know more about the firm’s value than potential investors. Investors interpret the firm’s actions rationally. An. equilibrium mode1 of the issue-invest decision is developed under these assumptions. The mode1 shows that firms may refuse to issue stock, and therefore may pass up valuable investment opportunities. The model suggests explanations for several aspects of corporate financing behavior, including the tendency to rely on internal sources of funds, and to prefer debt to equity if external financing is required. Extensions and applications of the model are discussed. 
778|Agency costs of free cash flow, corporate finance and takeovers|The interests and incentives of managers and shareholders conflict over such issues as the optimal size of the firm and the payment of cash to shareholders. These conflicts are especially severe in firms with large free cash flows—more cash than profitable investment opportunities. The theory developed here explains 1) the benefits of debt in reducing agency costs of free cash flows, 2) how debt can substitute for dividends, 3) why “diversification ” programs are more likely to generate losses than takeovers or expansion in the same line of business or liquidation-motivated takeovers, 4) why the factors generating takeover activity in such diverse activities as broadcasting and tobacco are similar to those in oil, and 5) why bidders and some targets tend to perform abnormally well prior to takeover.
782|Eclipse of the Public Corporation|The publicly held corporation has outlived its usefulness in many sectors of the economy. New organizations are emerging. Takeovers, leveraged buyouts, and other going-private transactions are manifestations of this change. A central source of waste in the public corporation is the conflict between owners and managers over free cash flow. This conflict helps explain the prominent role of debt in the new organizations. The new organizations ’ resolution of the conflict explains how they can motivate people and manage resources more effectively than public corporations. McKinsey Award Winner.
783|The Taxation of Income from Capital|a book edited by Mervyn King and Don Fullerton, to be submitted to the University of Chicago Press for publication consideration. The other chapters include an introduction, a theoretical framework, chapters on the
784|The relative informativeness of accounting disclosures in different countries|are grateful for that generous assistance.
785|Investment patterns and financial leverage|This study Investigates the influence of the type of Investment opportunities facing a firm on its choice of capital structure. It Is shown that the more discretionary Investment opportunities a firm faces, the lower its financial leverage. Inclusion of other possible determinants of capital structure, such as availability of internal funds, tax effects and risk, while significant, do not affect the importance of discretionary investment. The evidence supports (1) the existence of a moral bazzard problem which inversely relates risky debt and discretionary investment choice, and (2) a desire by most firms to use sources of internal funds prior to entering the capital market.
786|Molecular classification of cancer: class discovery and class prediction by gene expression monitoring|Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge. The challenge of cancer treatment has been to target specific therapies to pathogenetically distinct tumor types, to maximize efficacy
787|Functional discovery via a compendium of expression profiles. Cell 102:109|have been devised to survey gene functions en masse either computationally (Marcotte et al., 1999) or experimentally; among these, highly parallel assays of
788|Ratio-Based Decisions and the Quantitative Analysis of cDNA Microarray Images|Gene expression can be quantitatively analyzed by hybridizing fluor-tagged mRNA to targets on a cDNA microarray. Comparison of gene expression levels arising from cohybridized samples is achieved by taking ratios of average expression levels for individual genes. A novel method of image segmentation is provided to identify cDNA target sites and a hypothesis test and confidence interval is developed to quantify the significance of observed differences in expression ratios. In particular, the probability density of the ratio and the maximum-likelihood estimator for the distribution are derived, and an iterative procedure for signal calibration is developed.  1997 Society of Photo-Optical Instrumentation Engineers. [S1083-3668(97)00504-2]  Keywords cDNA; microarray; gene expression; image segmentation; Mann--Whitney target detection; ratio density, ratio confidence interval.
789|Analysis of variance for gene expression microarray data|Spotted cDNA microarrays are emerging as a powerful and cost-effective tool for largescale analysis of gene expression. Microarrays can be used to measure the relative quantities of speci ? c mRNAs in two or more tissue samples for thousands of genes simultaneously. While the power of this technology has been recognized, many open questions remain about appropriate analysis of microarray data. One question is how to make valid estimates of the relative expression for genes that are not biased by ancillary sources of variation. Recognizing that there is inherent “noise ” in microarray data, how does one estimate the error variation associated with an estimated change in expression, i.e., how does one construct the error bars? We demonstrate that ANOVA methods can be used to normalize microarray data and provide estimates of changes in gene expression that are corrected for potential confounding effects. This approach establishes a framework for the general analysis and interpretation of microarray data. Key words: Gene expression microarray, differential expression, analysis of variance, bootstrap.
790|On Differential Variability  of Expression Ratios: Improving . . .|We consider the problem of inferring fold changes in gene expression from cDNA microarray data. Standard procedures focus on the ratio of measured fluorescent intensities at each spot on the microarray, but to do so is to ignore the fact that the variation of such ratios is not constant. Estimates of gene expression changes are derived within a simple hierarchical model that accounts for measurement error and fluctuations in absolute gene expression levels. Significant gene expression changes are identified by deriving the posterior odds of change within a similar model. The methods are tested via simulation and are applied to a panel of Escherichia coli microarrays.
791|Issues in cDNA microarray analysis: quality filtering, channel normalization, models of variations and assessment of gene effects|We consider the problem of comparing the gene expression levels of cells grown under two different conditions using cDNA microarray data. We use a quality index, computed from duplicate spots on the same slide, to filter out outlying spots, poor quality genes and problematical slides. We also perform calibration experiments to show that normalization between fluorescent labels is needed and that the normalization is slide dependent and non-linear. A rank invariant method is suggested to select nondifferentially expressed genes and to construct normalization curves in comparative experiments. After normalization the residuals from the calibration data are used to provide prior information on variance components in the analysis of comparative experiments. Based on a hierarchical model that incorporates several levels of variations, a method for assessing the significance of gene effects in comparative experiments is presented. The analysis is demonstrated via two groups of experiments with 125 and 4129 genes, respectively, in Escherichia coli  grown in glucose and acetate.
792|Normalization and analysis of DNA microarray data by self-consistency and local regression|With the advent of DNA hybridization microarrays comes the remarkable ability, in principle, to simultaneously monitor the expression levels of large numbers of genes. The quantitative comparison of 2 or more microarrays can reveal, for example, the distinct patterns of gene expression that dene dierent cellular phenotypes or the genes induced in the cellular response to insult or changing environmental conditions. Normalization of the measured intensities is a prerequisite of such comparisons, and indeed of any statistical analysis, yet little attention has been paid to its systematic study. The most straightforward normalization techniques in use rest on the implicit assumption of linear response between true expression level and output intensity. We nd that these assumptions are not generally met and that these simple methods can be improved. We have developed a robust semi-parametric normalization technique based upon the assumption that the large majority of genes will not have...
793|Feature Extraction and Normalization Algorithms for High-Density Oligonucleotide Gene Expression Array Data|Algorithms for performing feature extraction and normalization on high-density oligonucleotide gene expression arrays, have not been fully explored, and the impact these algorithms have on the downstream analysis is not well understood. Advances in such low-level analysis methods are essential to increase the sensitivity and specificity of detecting whether genes are present and/or differentially expressed. We have developed and implemented a number of algorithms for the analysis of expression array data in a software application, the DNA-Chip Analyzer (dChip). In this report, we describe the algorithms for feature extraction and normalization, and present validation data and comparison results with some of the algorithms currently in use.
794|Unsupervised Models for Named Entity Classification|This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple “seed ” rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98). 1
795|A Theory of the Learnable|  Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems.
796|Combining labeled and unlabeled data with co-training|We consider the problem of using a large unlabeled sample to boost performance of a learning algorithm when only a small set of labeled examples is available. In particular, we consider a setting in which the description of each example can be partitioned into two distinct views, motivated by the task of learning to classify web pages. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks that point to that page. We assume that either view of the example would be su cient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment amuch smaller set of labeled examples. Speci cally, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm&#039;s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to signi cant improvement of hypotheses in practice. As part of our analysis, we provide new re-
797|Automatic Acquisition of Hyponyms from Large Text Corpora|We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidante of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also he acquirable iu this way. A subset of the acquisitiou algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.
798|Improved Boosting Algorithms Using Confidence-rated Predictions| We describe several improvements to Freund and Schapire’s AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper.
799|The Strength of Weak Learnability|Abstract. This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a Source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent. A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error e.
800|Extracting patterns and relations from the world wide web|Abstract. The World Wide Web is a vast resource for information. At the same time it is extremely distributed. A particular type of data such as restaurant lists may be scattered across thousands of independent information sources in many di erent formats. In this paper, we consider the problem of extracting a relation for such a data type from all of these sources automatically. We present a technique which exploits the duality between sets of patterns and relations to grow the target relation starting from a small sample. To test our technique we use it to extract a relation of (author,title) pairs from the World Wide Web. 1
801|Finding Parts in Very Large Corpora|We present a method for extracting parts of objects from wholes (e.g. &#034;speedometer&#034; from &#034;car&#034;). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon.
802|Unsupervised Learning of Disambiguation Rules for Part of Speech Tagging|In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus. We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers. Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text.
803|Additive Models, Boosting, and Inference for Generalized Divergences|We present a framework for designing incremental learning algorithms derived from generalized entropy functionals. Our approach is based on the use of Bregman divergences together with the associated class of additive models constructed using the Legendre transform. A particular one-parameter family of Bregman divergences is shown to yield a family of loss functions that includes the log-likelihood criterion of logistic regression as a special case, and that closely approximates the exponential loss criterion used in the AdaBoost algorithms of Schapire et al., as the natural parameter of the family varies. We also show how the quadratic approximation of the gain in Bregman divergence results in a weighted least-squares criterion. This leads to a family of incremental learning algorithms that builds upon and extends the recent interpretation of boosting in terms of additive models proposed by Friedman, Hastie, and Tibshirani. 1 Introduction  Logistic regression is a widely used statisti...
804|Improved Statistical Alignment Models|In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications.
806|Improved Alignment Models for Statistical Machine Translation|In this paper, we describe improved alignment  models for statistical machine translation. The  statistical translation approach uses two types  of information: a translation model and a lan-  guage model. The language model used is a  bigram or general m-gram model. The translation  model is decomposed into a lexical and an  alignment model. We describe two different approaches  for statistical translation and present  experimental results. The first approach is  based on dependencies between single words,  the second approach explicitly takes shallow  phrase structures into account, using two different  alignment levels: a phrase level alignment  between phrases and a word level alignment  between single words. We present results us-  ing the Verbmobil task (German-English, 6000word  vocabulary) which is a limited-domain  spoken-language task. The experimental tests  were performed on both the text transcription  and the speech recognizer output.
807|Manual annotation of translational equivalence: The Blinker project|Bilingual annotators were paid to link roughly sixteen thousand corresponding words between on-line versions of the Bible in modern French and modern English. These annotations are freely available to the researchcommunity from
808|But dictionaries are data too|Although empiricist approaches to machine translation depend vitally on data in the form of large bilingual corpora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iikefihood estimates of the parameters of a probabilistic model from this combined data and we show how these parameters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between rationalist
809|Evaluation of Word Alignment Systems|Recent years have seen a few serious attempts to develop methods and measures for the evaluation of word alignment systems, notably the Blinker project (Melamed, 1998) and the ARCADE project (Vronis and Langlais, forthcoming). In this paper we discuss different approaches to the problem and report on results from a project where two word alignment systems have been evaluated. These results include methods and tools for the generation of reference data and a set of measures for system performance. We note that the selection and sampling of reference data can have a great impact on scoring results.
810|An intrusion-detection model|A model of a real-time intrusion-detection expert system capable of detecting break-ins, penetrations, and other forms of computer abuse is described. The model is based on the hypothesis that security violations can be detected by monitoring a system&#039;s audit records for abnormal patterns of system usage. The model includes profiles for representing the behavior of subjects with respect to objects in terms of metrics and statistical models, and rules for acquiring knowledge about this behavior from audit records and for detecting anomalous behavior. The model is independent of any particular system, application environment, system vulnerability, or type of intrusion, thereby providing a framework for a general-purpose intrusion-detection expert system.
811|Social force model for pedestrian dynamics|It is suggested that the motion of pedestrians can be described as if they would be subject to ‘social forces’. These ‘forces ’ are not directly exerted by the pedestrians ’ personal environment, but they are a measure for the internal motivations of the individuals to perform certain actions (movements). The corresponding force concept is discussed in more detail and can be also applied to the description of other behaviors. In the presented model of pedestrian behavior several force terms are essential: First, a term describing the acceleration towards the desired velocity of motion. Second, terms reflecting that a pedestrian keeps a certain distance to other pedestrians and borders. Third, a term modeling attractive effects. The resulting equations of motion are nonlinearly coupled Langevin equations. Computer simulations of crowds of interacting pedestrians show that the social force model is capable of describing the self-organization of several observed collective effects of pedestrian behavior very realistically. Typeset using REVTEX 1 I.
812|Impulses and Physiological States in Theoretical Models of Nerve Membrane|ABSTRACT Van der Pol&#039;s equation for a relaxation oscillator is generalized by the addition of terms to produce a pair of non-linear differential equations with either a stable singular point or a limit cycle. The resulting &#034;BVP model &#034; has two variables of state, representing excitability and refractoriness, and qualitatively resembles Bonhoeffer&#039;s theoretical model for the iron wire model of nerve. This BVP model serves as a simple representative of a class of excitable-oscillatory systems including the Hodgkin-Huxley (HH) model of the squid giant axon. The BVP phase plane can be divided into regions corresponding to the physio-logical states of nerve fiber (resting, active, refractory, enhanced, depressed, etc.) to form a &#034;physiological state diagram, &#034; with the help of which many physiological phenomena can be summarized. A properly chosen projection from the 4-dimensional HH phase space onto a plane produces a similar diagram which shows the underlying relationship between the two models. Impulse trains occur in the BVP and HH models for a range of constant applied currents which make the singular point representing the resting state unstable.
813|A quantitative description of membrane currents and its application to conduction and excitation in nerve|This article concludes a series of papers concerned with the flow of electric current through the surface membrane of a giant nerve fibre (Hodgkin, Huxley &amp; Katz, 1952; Hodgkin &amp; Huxley, 1952 a-c). Its general object is to discu the results of the preceding papers (Part I), to put them into mathematical form (Part II) and to show that they will account for con-duction and excitation in quantitative terms (Part III). PART I. DISCUSSION OF EXPERIMENTAL RESULTS The results described in the preceding papers suggest that the electrical behaviour of the membrane may be represented by the network shown in Fig. 1. Current can be carried through the membrane either by charging the membrane capacity or by movement of ion-s through the resistances in parallel with the capacity. The ionic current is divided into components carried by sodium and potassium ions (INa and IK), and a small &#039;leakage current &#039; (I,) made up by chloride and other ions. Each component of the ionic current is determined by a driving force which may conveniently be measured as an
814|Activation of passive iron as a model for the excitation of nerve|If a piece of passive iron in concentrated nitric acid is touched momentarily with a zinc rod, the iron may become active and the activation may spread from the point of contact over the whole piece of iron. Whether the iron will become active, and if it becomes active, whether it will return to its former passive state
815|ELECTROKINETIC MEMBRANE PROCESSES IN RELATION TO PROPERTIES OF EXCITABLE TISSUES II. Sou- ~ &amp;quot;I&#039;m~Om~TXCAL CONSIDERATIONS|A quantitative theory is presented /or the behavior of a membrane-electrolyte system subject to an electric current flow (the &amp;quot;membrane oscillator&amp;quot;). If the mem-brane is porous, carries &amp;quot;fixed charges, &amp;quot; and separates electrolyte solutious of differ-ent conductances, it can be the site of repetitive osculatory changes in the mefiabrane potential, the membrane resistance, and the hydrostatic pressure difference across the membrane. These events are accompanied by a pulsating transport of bulk solutions. The theory assumes the superposition of electrochemical and hydrostatic gradients and centers round the kinetics of resistance changes within the membrane, as caused by effects from diffusion and electro-osmotic fluid streaming. The results are laid down in a set of five simple, basic expressions, which can be transformed into a pair of non-llnear differential equations yielding oscillatory solutions. A graphical inte-gration method is also outlined (Appendix II). The agreement between the theory and previous experimental observations is satisfactory. The applied electrokinetic concepts may have importance in relation to
816|Nonparametric model for background subtraction|Abstract. Background subtraction is a method typically used to seg-ment moving regions in image sequences taken from a static camera by comparing each new frame to a model of the scene background. We present a novel non-parametric background model and a background subtraction approach. The model can handle situations where the back-ground of the scene is cluttered and not completely static but contains small motions such as tree branches and bushes. The model estimates the probability of observing pixel intensity values based on a sample of intensity values for each pixel. The model adapts quickly to changes in the scene which enables very sensitive detection of moving targets. We also show how the model can use color information to suppress detec-tion of shadows. The implementation of the model runs in real-time for both gray level and color imagery. Evaluation shows that this approach achieves very sensitive detection with very low false alarm rates. Key words: visual motion, active and real time vision, motion detection, non-parametric estimation, visual surveillance, shadow detection 1
817|Pfinder: Real-time tracking of the human body|Pfinder is a real-time system for tracking people and interpreting their behavior. It runs at 10Hz on a standard SGI Indy computer, and has performed reliably on thousands of people in many different physical locations. The system uses a multiclass statistical model of color and shape to obtain a 2D representation of head and hands in a wide range of viewing conditions. Pfinder has been successfully used in a wide range of applications including wireless interfaces, video databases, and low-bandwidth coding. 
818|Image segmentation in video sequences: A probabilistic approach|&#034;Background subtraction&#034; is an old technique for finding moving objects in a video sequence---for example, cars driving on a freeway. The idea is that subtracting the current image from a time-averaged background image will leave only nonstationary objects. It is, however, a crude approximation to the task of classifying each pixel of the current image
819|Using adaptive tracking to classify and monitor activities in a site|We describe a vision system that monitors activity in a site over extended periods of time. The system uses a distributed set of sensors to cover the site, and an adaptive tracker detects multiple moving objects in the sensors. Our hypothesis is that motion tracking is sufficient to support a range of computations about site activities. We demonstrate using the tracked motion data: to calibrate the distributed sensors, to construct rough site models, to classify detected objects, to learn common patterns of activity for different object classes, and to detect unusual activities. 
820|A Model of Investor Sentiment|Recent empirical research in finance has uncovered two families of pervasive regularities: underreaction of stock prices to news such as earnings announcements, and overreaction of stock prices to a series of good or bad news. In this paper, we present a parsimonious model of investor sentiment, or of how investors form beliefs, which is consistent with the empirical findings. The model is based on psychological evidence and produces both underreaction and overreaction for a wide range of parameter values. ? 1998 Elsevier Science S.A. All rights reserved. JEL classification: G12; G14
821|The cross-section of expected stock returns|Your use of the JSTOR archive indicates your acceptance of JSTOR &#039; s Terms and Conditions of Use, available at
825|Market Efficiency, Long-Term Returns, and Behavioral Finance|Market efficiency survives the challenge from the literature on long-term return anomalies. Consistent with the market efficiency hypothesis that the anomalies are chance results, apparent overreaction to information is about as common as underreaction, and post-event continuation of pre-event abnormal returns is about as frequent as post-event reversal. Most important, consistent with the market efficiency prediction that apparent anomalies can be due to methodology, most long-term return anomalies tend to disappear with reasonable changes in technique.  
827|Market underreaction to open market share repurchases|We examine long-run firm performance following open market share repurchase announcements, 1980-1990. We find that the average abnormal four-year buy-and-hold return measured after the initial announcement is 12.1%. For ‘value ’ stocks, companies more likely to be repurchasing shares because of undervaluation, the average abnormal return is 45.3%. For repurchases announced by ‘glamour ’ stocks, where undervaluation is less likely to be an important motive, no positive drift in abnormal returns is observed. Thus, at least with respect to value stocks, the market errs in its initial response and appears to ignore much of the information conveyed through repurchase announcements.
829|Momentum strategies|We examine whether the predictability of future returns from past returns is due to the market&#039;s underreaction to information, in particular to past earnings news. Past return and past earnings surprise each predict large drifts in future returns after controlling for the other. Market risk, size, and book-to-market effects do not explain the drifts. There is little evidence of subsequent reversals in the returns of stocks with high price and earnings momentum. Security analysts &#039; earnings forecasts also respond sluggishly to past news, especially in the case of stocks with the worst past performance. The results suggest a market that responds only gradually to new information. AN EXTENSIVE BODY OF RECENT finance literature documents that the crosssection of stock returns is predictable based on past returns. For example, DeBondt and Thaler (1985, 1987)report that long-term past losers outperform long-term past winners over the subsequent three to five years. Jegadeesh (1990) and Lehmann (1990) find short-term return reversals. Jegadeesh and
831|Evidence that stock prices do not fully reflect the implications of current earnings for future earnings|Evidence presented here is consistent with a failure of stock prices to reflect fully the implications of current earnings for future earnings. Specifically, the three-day price reactions to announcements of earnings for quarters t + 1 through I + 4 are predictable, based on earnings of quarter r. Even more surprisingly, the signs and magnitudes of the three-day reactions are related to the autocorrelation structure of earnings, as if stock prices fail to reflect the extent to which each firm’s earnings series differs from a seasonal random walk. 1.
832|Value versus growth: The international evidence|Value stocks have higher returns than growth stocks in markets around the world. For the period 1975 through 1995, the difference between the average returns on global portfolios of high and low book-to-market stocks is 7.68 percent per year, and value stocks outperform growth stocks in twelve of thirteen major markets. An international capital asset pricing model cannot explain the value premium, but a two-factor model that includes a risk factor for relative distress captures the value premium in international returns. 
833|The weighting of evidence and the determinants of confidence|The pattern of overconfidence and underconfidence observed in studies of in-tuitive judgment is explained by the hypothesis that people focus on the strength or extremeness of the available evidence (e.g., the warmth of a letter or the size of an effect) with insufficient regard for its weight or credence (e.g., the credibility of the writer or the size of the sample). This mode of judgment yields overconfi-dence when strength is high and weight is low, and underconfidence when strength is low and weight is high. We first demonstrate this phenomenon in a chance setup where strength is defined by sample proportion and weight is defined by sample size, and then extend the analysis to more complex evidential prob-lems, including general knowledge questions and predicting the behavior of self and of others. We propose that people’s confidence is determined by the balance of arguments for and against the competing hypotheses, with insufficient regard for the weight of the evidence. We show that this account can explain the effect of item difficulty on overconfidence, and we relate the observed discrepancy between confidence judgments and frequency estimates to the illusion of validity.
834|Fads, martingales, and market efficiency|for helpful coments. They share no responsibiTfty for any remaining errors.
835|LogP: Towards a Realistic Model of Parallel Computation|A vast body of theoretical research has focused either on overly simplistic models of parallel  computation, notably the PRAM, or overly specific models that have few representatives in  the real world. Both kinds of models encourage exploitation of formal loopholes, rather than  rewarding development of techniques that yield performance across a range of current and  future parallel machines. This paper offers a new parallel machine model, called LogP, that  reflects the critical technology trends underlying parallel computers. It is intended to serve  as a basis for developing fast, portable parallel algorithms and to offer guidelines to machine  designers. Such a model must strike a balance between detail and simplicity in order to reveal  important bottlenecks without making analysis of interesting problems intractable. The model  is based on four parameters that specify abstractly the computing bandwidth, the communication  bandwidth, the communication delay, and the efficiency of coupling communication  and computation. Portable parallel algorithms typically adapt to the machine configuration, in  terms of these parameters. The utility of the model is demonstrated through examples that are  implemented on the CM-5.
836|Active Messages: a Mechanism for Integrated Communication and Computation|The design challenge for large-scale multiprocessors is (1) to minimize communication overhead, (2) allow communication to overlap computation, and (3) coordinate the two without sacrificing processor cost/performance. We show that existing message passing multiprocessors have unnecessarily high communication costs. Research prototypes of message driven machines demonstrate low communication overhead, but poor processor cost/performance. We introduce a simple communication mechanism, Active Messages, show that it is intrinsic to both architectures, allows cost effective use of the hardware, and offers tremendous flexibility. Implementations on nCUBE/2 and CM-5 are described and evaluated using a split-phase shared-memory extension to C, Split-C. We further show that active messages are sufficient to implement the dynamically scheduled languages for which message driven machines were designed. With this mechanism, latency tolerance becomes a programming/compiling concern. Hardware support for active messages is desirable and we outline a range of enhancements to mainstream processors.  
837|Performance Analysis of k-ary n-cube Interconnection Networks|Abstmct- VLSI communication networks are wire-limited. The cost of a network is not a function of the number of switches required, but rather a function of the wiring density required to construct the network. This paper analyzes commu-nication networks of varying dimension under the assumption of constant wire bisection. Expressions for the latency, average case throughput, and hot-spot throughput of k-ary n-cube networks with constant bisection are derived that agree closely with experi-mental measurements. It is shown that low-dimensional networks (e.g., tori) have lower latency and higher hot-spot throughput than high-dimensional networks (e.g., binary n-cubes) with the same bisection width. Index Terms- Communication networks, concurrent comput-ing, interconnection networks, message-passing multiprocessors, parallel processing, VLSI.
838|Monsoon: an explicit token-store architecture|Dataflow architectures tolerate long unpredictable com-munication delays and support generation and coordi-nation of parallel activities directly in hardware, rather than assuming that program mapping will cause these issues to disappear. However, the proposed mecha-nisms are complex and introduce new mapping com-plications. This paper presents a greatly simplified ap-proach to dataflow execution, called the explicit token store (ETS) architecture, and its current realization in Monsoon. The essence of dynamic datallow execution is captured by a simple transition on state bits associ-ated with storage local to a processor. Low-level storage management is performed by the compiler in assigning nodes to slots in an activation frame, rather than dy-namically in hardware. The processor is simple, highly pipelined, and quite general. It may be viewed as a generalization of a fairly primitive von Neumann archi-tecture. Although the addressing capability is restric-tive, there is exactly one instruction executed for each action on the dataflow graph. Thus, the machine ori-ented ETS model provides new understanding of the merits and the real cost of direct execution of dataflow graphs. 1
839|  A Comparison of Sorting Algorithms for the Connection Machine CM-2 |We have implemented three parallel sorting algorithms on the Connection Machine Supercomputer model CM-2: Batcher&#039;s bitonic sort, a parallel radix sort, and a sample sort similar to Reif and Valiant&#039;s flashsort. We have also evaluated the implementation of many other sorting algorithms proposed in the literature. Our computational experiments show that the sample sort algorithm, which is a theoretically efficient &#034;randomized&#034; algorithm, is the fastest of the three algorithms on large data sets. On a 64K-processor CM-2, our sample sort implementation can sort 32 10 6 64-bit keys in 5.1 seconds, which is over 10 times faster than the CM-2 library sort. Our implementation of radix sort, although not as fast on large data sets, is deterministic, much simpler to code, stable, faster with small keys, and faster on small data sets (few elements per processor). Our implementation of bitonic sort, which is pipelined to use all the hypercube wires simultaneously, is the least efficient of the three on large data sets, but is the most efficient on small data sets, and is considerably more space efficient. This paper analyzes the three algorithms in detail and discusses many practical issues that led us to the particular implementations.  
840|Scans as Primitive Parallel Operations|In most parallel random-access machine (P-RAM) models, memory references are assumed to take unit time. In practice, and in theory, certain scan operations, also known as prefix computations, can executed in no more time than these parallel memory references. This paper outline an extensive study of the effect of including in the P-RAM models, such scan operations as unit-time primitives. The study concludes that the primitives improve the asymptotic running time of many algorithms by an O(lg n) factor, greatly simplify the description of many algorithms, and are significantly easier to implement than memory references. We therefore argue that the algorithm designer should feel free to use these operations as if they were as cheap as a memory reference. This paper describes five algorithms that clearly illustrate how the scan primitives can be used in algorithm design: a radix-sort algorithm, a quicksort algorithm, a minimumspanning -tree algorithm, a line-drawing algorithm and a mergi...
841|Designing Broadcasting Algorithms in the Postal Model for Message-Passing Systems|In many distributed-memory parallel computers and high-speed communication networks, the exact structure of the underlying communication network may be ignored. These systems assume that the network creates a complete communication graph between the processors, in which passing messages is associated with communication latencies. In this paper, we explore the impact of communication latencies on the design of broadcasting algorithms for fully-connected message-passing systems. For this purpose, we introduce the postal model that incorporates a communication latency parameter   1. This parameter measures the inverse of the ratio between the time it takes an originator of a message to send the message and the time that passes until the recipient of the message receives it. We present an optimal algorithm for broadcasting one message in systems with n processors and communication latency , the running time of which is \Theta(   log n  log(+1)  ). For broadcasting m  1 messages, we first e...
842|The Uniform Memory Hierarchy Model of Computation|The Uniform Memory Hierarchy (UMH) model introduced in this paper captures performance-relevant aspects of the hierarchical nature of computer memory. It is used to quantify architectural requirements of several algorithms and to ratify the faster speeds achieved by tuned implementations that use improved data-movement strategies. A sequential computer&#039;s memory is modelled as a sequence hM 0 ; M 1 ; :::i of increasingly large memory modules. Computation takes place in M 0 . Thus, M 0 might model a computer&#039;s central processor, while M 1 might be cache memory, M 2 main memory, and so on. For each module M U , a bus B U connects it with the next larger module M U+1 . All buses may be active simultaneously. Data is transferred along a bus in fixed-sized blocks. The size of these blocks, the time required to transfer a block, and the number of blocks that fit in a module are larger for modules farther from the processor. The UMH  model is parameterized by the rate at which the blocksizes i...
843|Hiding Communication Costs in Bandwidth-Limited Parallel FFT Computation|This paper presentsanovel computation schedule for FFT-type computations on a bandwidth-limited parallel computer. Using P processors, we are able to process n log n an n-input FFT graph in the optimal time of by carefully interleaving inter-P processor communication steps with local computation. Our algorithm is suitable for both shared-memory and distributed memory machines and is analyzed in a simpli cation of the LogP model [5] suitable for studying bandwidth-limited parallel machines. Our parallel FFT algorithm incorporates several techniques that have long been used by parallel programmers to reduce communication costs and our analysis provides theoretical justi cation for the success of these techniques in the context of highly structured computations like FFTs. At another level, our algorithm can be viewed as an optimal simulation of large butter y networks on arbitrary machines (as modeled under LogP.) Thus, we argue that computations thought to be inherently suited to buttery
844|Dynamic topic models|Scientists need new tools to explore and browse large collections of scholarly literature. Thanks to organizations such as JSTOR, which scan and index the original bound archives of many journals, modern scientists can search digital libraries spanning hundreds of years. A scientist, suddenly
845|Latent dirichlet allocation|We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model. 1.
846|A bayesian hierarchical model for learning natural scene categories|We propose a novel approach to learn and recognize natural scene categories. Unlike previous work [9, 17], it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a “theme”. In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes. 1.
847|The Author-Topic Model for Authors and Documents |We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, &amp; Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics
that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact
inference is intractable for these datasets and
we use Gibbs sampling to estimate the topic
and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model)
and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications
to computing similarity between authors and
entropy of author output.
848|Sparse Gaussian processes using pseudo-inputs|We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M « N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M 2 N) training cost and O(M 2) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime. 1
849|Discovering object categories in image collections|Given a set of images containing multiple object categories, we seek to discover those categories and their image locations without supervision. We achieve this using generative models from the statistical text literature: probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA). In text analysis these are used to discover topics in a corpus using the bag-of-words document representation. Here we discover topics as object categories, so that an image containing instances of several categories is modelled as a mixture of topics. The models are applied to images by using a visual analogue of a word, formed by vector quantizing SIFT like region descriptors. We investigate a set of increasingly demanding scenarios, starting with image sets containing only two object categories through to sets containing multiple categories (including airplanes, cars, faces, motorbikes, spotted cats) and background clutter. The object categories sample both intra-class and scale variation, and both the categories and their approximate spatial layout are found without supervision. We also demonstrate classification of unseen images and images containing multiple objects. Performance of the proposed unsupervised method is compared to the semi-supervised approach of [7].  
850|Integrating topics and syntax|Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words. We present a generative model that uses both kinds of dependencies, and can be used to simultaneously find syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency. This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively. 1
851|A Generalized Mean Field Algorithm for Variational Inference in Exponential Families|We present a class of generalized mean field (GMF) algorithms for approximate inference in exponential family graphical models which is analogous to the generalized belief propagation (GBP) or cluster variational methods. While those methods are based on...
852|Collaborative Filtering: A Machine Learning Perspective|Collaborative filtering was initially proposed as a framework for filtering information based on the preferences of users, and has since been refined in many different ways. This thesis is a comprehensive study of rating-based, pure, non-sequential collaborative filtering. We analyze existing methods for the task of rating prediction from a machine learning perspective. We show that many existing methods proposed for this task are simple applications or modi  cations of one or more standard machine learning methods for classifi cation, regression, clustering, dimensionality reduction, and density estimation. We introduce new prediction methods in all of these classes. We introduce a new experimental procedure for testing stronger forms of generalization than has been used previously. We implement a total of nine prediction methods, and conduct large scale prediction accuracy experiments. We show interesting new results on the relative performance of these methods.
853|Applying Discrete PCA in Data Analysis|Methods for analysis of principal components in discrete data have existed for some time under various names such as grade of membership modelling, probabilistic latent semantic analysis, and genotype inference with admixture. In this paper we explore a number of extensions to the common theory, and present some application of these methods to some common statistical tasks. We show that these methods can be interpreted as a discrete version of ICA. We develop a hierarchical version yielding components at different levels of detail, and additional techniques for Gibbs sampling. We compare the algorithms on a text prediction task using support vector machines, and to information retrieval.  
854|The authorrecipienttopic model for topic and role discovery in social networks: Experiments with Enron and academic email|Previous work in social network analysis (SNA) has modeled the existence of links from one entity to another, but not the language content or topics on those links. We present the Author-Recipient-Topic (ART) model for social network analysis, which learns topic distributions based on the the directionsensitive messages sent between entities. The model builds on Latent Dirichlet Allocation and the Author-Topic (AT) model, adding the key attribute that distribution over topics is conditioned distinctly on both the sender and recipient—steering the discovery of topics according to the relationships between people. We give results on both the Enron email corpus and a researcher’s email archive, providing evidence not only that clearly relevant topics are discovered, but that the ART model better predicts people’s roles. 1
856|Simplicial mixtures of Markov chains: Distributed modelling of dynamic user profiles|To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the definition of individual specific and global models. This paper proposes a lineartime distributed model for finite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be ‘explained ’ by a relatively small number of structurally simple common behavioral patterns which may interleave randomly in a user-specific proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efficient representation scheme, reflected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations. 1
857|Modeling TCP Throughput: A Simple Model and its Empirical Validation|In this paper we develop a simple analytic characterization of the steady state throughput, as a function of loss rate and round trip time for a bulk transfer TCP flow, i.e., a flow with an unlimited amount of data to send. Unlike the models in [6, 7, 10], our model captures not only the behavior of TCP’s fast retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP’s timeout mechanism on throughput. Our measurements suggest that this latter behavior is important from a modeling perspective, as almost all of our TCP traces contained more timeout events than fast retransmit events. Our measurements demonstrate that our model is able to more accurately predict TCP throughput and is accurate over a wider range of loss rates. This material is based upon work supported by the National Science Foundation under grants NCR-95-08274, NCR-95-23807 and CDA-95-02639. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
858|Random Early Detection Gateways for Congestion Avoidance|This paper presents Random Early Detection (RED) gate-ways for congestion avoidance in packet-switched networks. The gateway detects incipient congestion by com-puting the average queue size. The gateway could notify connections of congestion either by dropping packets ar-riving at the gateway or by setting a bit in packet headers. When the average queue size exceeds a preset threshold,the gateway drops or marks each arriving packet with a certain probability, where the exact probability is a func-tion of the average queue size. RED gateways keep the average queue size low while allowing occasional bursts of packets in the queue. During congestion, the probability that the gateway notifies a particular connection to reduce its window is roughly proportional to that connection&#039;s share of the bandwidth throughthe gateway. RED gateways are designed to accompany a transport-layer congestion control protocol such as TCP.The RED gateway has no bias against bursty traffic and avoids the global synchronization of many connectionsdecreasing their window at the same time. Simulations of a TCP/IP network are used to illustrate the performance of RED gateways. 
859|End-to-End Internet Packet Dynamics|  We discuss findings from a large-scale study of Internet packet dynamics conducted by tracing 20 000 TCP bulk transfers between 35 Internet sites. Because we traced each 100-kbyte transfer at both the sender and the receiver, the measurements allow us to distinguish between the end-toend behaviors due to the different directions of the Internet paths, which often exhibit asymmetries. We: 1) characterize the prevalence of unusual network events such as out-of-order delivery and packet replication; 2) discuss a robust receiver-based algorithm for estimating “bottleneck bandwidth ” that addresses deficiencies discovered in techniques based on “packet pair;” 3) investigate patterns of packet loss, finding that loss events are not well modeled as independent and, furthermore, that the distribution of the duration of loss events exhibits infinite variance; and 4) analyze variations in packet transit delays as indicators of congestion periods, finding that congestion periods also span a wide range of time scales. 
860|The Macroscopic Behavior of the TCP Congestion Avoidance Algorithm|In this paper, we analyze a performance model for the TCP Congestion Avoidance algorithm. The model predicts the bandwidth of a sustained TCP connection subjected to light to moderate packet losses, such as loss caused by network congestion. It assumes that TCP avoids retransmission timeouts and always has sufficient receiver window and sender data. The model predicts the Congestion Avoidance performance of nearly all TCP implementations under restricted conditions and of TCP with SelectiveAcknowledgements over a much wider range of Internet conditions. We verify
861|Wide-area Internet traffic patterns and characteristics|Abstract – The Internet is rapidly growing in number of users, traffic levels, and topological complexity. At the same time it is increasingly driven by economic competition. These developments render the characterization of network usage and workloads more difficult, and yet more critical. Few recent studies have been published reporting Internet backbone traffic usage and characteristics. At MCI, we have implemented a high-performance, low-cost monitoring system that can capture traffic and perform analyses. We have deployed this monitoring tool on OC-3 trunks within internetMCI’s backbone and also within the NSF-sponsored vBNS. This paper presents observations on the patterns and characteristics of wide-area Internet traffic, as recorded by MCI’s OC-3 traffic monitors. We report on measurements from two OC-3 trunks in MCI’s commercial Internet backbone over two time ranges (24-hour and 7-day) in the presence of up to 240,000 flows. We reveal the characteristics of the traffic in terms of packet sizes, flow duration, volume, and percentage composition by protocol and application, as well as patterns seen over the two time scales. 1
862|Tcp-like congestion control for layered multicast data transfer|Abstract—We present a novel congestion control algorithm suitable for use with cumulative, layered data streams in the MBone. Our algorithm behaves similarly to TCP congestion control algorithms, and shares bandwidth fairly with other instances of the protocol and with TCP flows. It is entirely receiver driven and requires no per-receiver status at the sender, in order to scale to large numbers of receivers. It relies on standard functionalities of multicast routers, and is suitable for continuous stream and reliable bulk data transfer. In the paper we illustrate the algorithm, characterize its response to losses both analytically and by simulations, and analyse its behaviour using simulations and experiments in real networks. We also show how error recovery can be dealt with independently from congestion control by using FEC techniques, so as to provide reliable bulk data transfer.
863|Improving round-trip time estimates in reliable transport protocols|As a reliable, end-to-end transport protocol, the Transmission Control Protocol (TCP) uses positive acknowledgements and retransmission to guarantee delivery. TCP implementations are expected to measure and adapt to changing round-trip delay so that their retransmission behavior balances user throughput and network efficiency. However, TCP suffers from a problem we call retransmission ambiguzty: when an acknowledgement arrives for a datagram that has been retransmitted, there is no indication of which transmission is being acknowledged. As a result, an implementation maybe unable to determine if the round-trip time it measures is for an original transmission or a retransmission of a datagram. Many existing TCP implementa-tions do not handle this problem correctly. Furthermore, the problem of retransmission ambigu-ity is also a characteristic of other major transport protocols, including 0S1 TP4 and DECnet NSP This paper reviews the various approaches to retransmission and presents a novel and effective approach to the retransmission ambiguity problem. Categories and Subject Descriptors: C 20 [Computer Communications Networks]: General—open System Interconnection reference model (0S1); C.2. 1 [Computer Communications Networks]: Network Architecture and Design—packet networks, store and forward net-works; D.4.4 [Operating Systems]: Communications Management — message sending, network communication
864|Why We Don&#039;t Know How to Simulate the Internet|Simulating how the global Internet data network behaves is an immensely challenging undertaking because of the network&#039;s great heterogeneity and rapid change. The heterogeneity ranges from the individual links that carry the network&#039;s traffic, to the protocols that interoperate over the links, to the &#034;mix&#034; of different applications used at a site and the levels of congestion (load) seen on different links. We discuss two key strategies for developing meaningful simulations in the face of these difficulties: searching for invariants and judiciously exploring the simulation parameter space. We finish with a look at a collaborative effort to build a common simulation environment for conducting Internet studies.
865|Automated Packet Trace Analysis of TCP Implementations|We describe tcpanaly, a tool for automatically analyzing a TCP implementation&#039;s behavior by inspecting packet traces of the TCP&#039;s activity. Doing so requires surmounting a number of hurdles, including detecting packet filter measurement errors, coping with ambiguities due to the distance between the measurement point and the TCP, and accommodating a surprisingly large range of behavior among different TCP implementations. We discuss why our efforts to develop a fully general tool failed, and detail a number of significant differences among 8 major TCP implementations, some of which, if ubiquitous, would devastate Internet performance. The most problematic TCPs were all independently written, suggesting that correct TCP implementation is fraught with difficulty. Consequently, it behooves the Internet community to develop testing programs and reference implementations. 1 Introduction  There can be a world of difference between the behavior we expect of a transport protocol, and what we g...
866|The stationary behavior of ideal TCP congestion avoidance|This note derives the stationary behavior of idealized TCP congestion avoidance. More specifically, it derives the stationary distribution of the congestion window size if loss of packets are independentevents with equal probability. The mathematical derivation uses a fluid flow, continuous time, approximation to the discrete time process #W n #, where W n is the congestion window after the n-th packet. We derive explicit results for the stationary distribution and all its moments. Congestion avoidance is the algorithm used by TCP to set its window size (and indirectly its data rate) under moderate to light segment (packet) losses. The congestion avoidance mechanism we model is idealized in the sense that loss of multiple packets does not lead to time-out phenomena. Such idealized behavior can be implemented using Selective Acknowledgements (SACKs). As such, our model predicts behavior of TCP with SACKs. It also is an approximate model in other situations. Among the results are that if eve...
867|Control Mechanisms for Packet Audio in the Internet|Current packet-switched networks such as the Internet do not provide guaranteed performance measures such as maximum loss rate or delay jitter. One way to support packet audio in these networks is to use control mechanisms which adapt the audio coding and decoding processes based on the state of the network so as to maximize the quality of the audio delivered to the destinations. This quality essentially depends on the characteristics of the loss and the delay jitter processes in the network. In this paper, we describe and analyze a set of efficient control mechanisms which attempt to minimize the impact of these processes. The mechanisms include a a jitter control mechanism and a combined error and rate control mechanism. These mechanisms have been implemented and evaluated over the Internet and the MBone. Experiments indicate that they make it possible to establish and maintain good quality audioconferences even across fairly congested connections.
868|Experiments with a Layered Transmission Scheme over the Internet|Combining hierarchical coding of data with receiver-driven control appears to be an attractive scheme for the multicast transmission of audio/video flows in a heterogeneous multicast environment such as the Internet. However, little experimental data is available regarding the actual performance of such schemes over the Internet. Previous work such as that on receiver driven layered multicast uses join experiments to choose the best quality signal a receiver can subscribe to. In this paper, we present a receiver-based multicast rate control mechanism based on a recently proposed TCP-friendly unicast mechanism. We have implemented this mechanism and evaluate its performance in conjunction with a simple layered audio coding scheme. We find that it has interesting convergence and performance properties, but also bring out its limitations.
869|Fronts propagating with curvature dependent speed: algorithms based on Hamilton–Jacobi formulations|We devise new numerical algorithms, called PSC algorithms, for following fronts propagating with curvature-dependent speed. The speed may be an arbitrary function of curvature, and the front can also be passively advected by an underlying flow. These algorithms approximate the equations of motion, which resemble Hamilton-Jacobi equations with parabolic right-hand-sides, by using techniques from the hyperbolic conservation laws. Non-oscillatory schemes of various orders of accuracy are used to solve the equations, providing methods that accurately capture the formation of sharp gradients and cusps in the moving fronts. The algorithms handle topological merging and breaking naturally, work in any number of space dimensions, and do not require that the moving surface be written as a function. The methods can be also used for more general Hamilton-Jacobitype problems. We demonstrate our algorithms by computing the solution to a variety of surface motion problems. 1
870|Motion of level sets by mean curvature |We construct a unique weak solution of the nonlinear PDE which asserts each level set evolves in time according to its mean curvature. This weak solution allows us then to define for any compact set G o a unique generalized motion by mean curvature, existing for all time. We investigate the various geometric properties and pathologies of this evolution. 1.
871|A Fast Level Set Method for Propagating Interfaces|A method is introduced to decrease the computational labor of the standard level set method for propagating interfaces. The fast approach uses only points close to the curve at every time step. We describe this new algorithm and compare its efficiency and accuracy with the standard level set approach.  
872|Surface Modeling with Oriented Particle Systems|Splines and deformable surface models are widely used in computer graphics to describe free-form surfaces. These methods require manual preprocessing to discretize the surface into patches and to specify their connectivity. We present a new model of elastic surfaces based on interacting particle systems, which, unlike previous techniques, can be used to split, join, or extend surfaces without the need for manual intervention. The particles we use have long-range attraction forces and short-range repulsion forces and follow Newtonian dynamics, much like recent computational models of fluids and solids. To enable our particles to model surface elements instead of point masses or volume elements, we add an orientation to each particle&#039;s state. We devise new interaction potentials for our oriented particles which favor locally planar or spherical arrangements. We also develop techniques for adding new particles automatically, which enables our surfaces to stretch and grow. We demonstrate t...
873|An Overview of Workflow Management: From Process Modeling to Workflow Automation Infrastructure|  Today’s business enterprises must deal with global competition, reduce the cost of doing business, and rapidly develop new services and products. To address these requirements enterprises must constantly reconsider and optimize the way they do business and change their information systems and applications to support evolving business processes. Workflow technology facilitates these by providing methodologies and software to support (i) business process modeling to capture business processes as workflow specifications, (ii) business process reengineering to optimize specified processes, and (iii) workflow automation to generate workflow implementations from workflow specifications. This paper provides a high-level overview of the current workflow management methodologies and software products. In addition, we discuss the infrastructure technologies that can address the limitations of current commercial workflow technology and extend the scope and mission of workflow management systems to support increased workflow automation in complex real-world environments involving heterogeneous, autonomous, and distributed information systems. In particular, we discuss how distributed object management and customized transaction management can support further advances in the commercial state of the art in this area.
874|The Action Workflow approach to workflow management technology|This paper describes ActionWorkflowTM approach to workflow management technology: a design methodology and associated computer software for the support of work in organizations. The approach is based on theories of communicative activity as language faction and has been developed in a series of systems for coordination among users of networked computers. This paper describes the approach, gives an example of its application, and shows the architecture of a workflow management system based on it.
875|Managing Heterogeneous Multi-system Tasks to Support Enterprise-wide Operations|. The computing environment in most medium-sized and large enterprises involves old main-frame based (legacy) applications and systems as well as new workstation-based distributed computing systems. The objective of the METEOR project is to support multi-system workflow applications that automate enterprise operations. This paper deals with the modeling and specification of workflows in such applications. Tasks in our heterogeneous environment can be submitted through different types of interfaces on different processing entities. We first present a computational model for workflows that captures the behavior of both transactional and nontransactional tasks of different types. We then develop two languages for specifying a workflow at different levels of abstraction: the Workflow Specification Language (WFSL) is a declarative rulebased language used to express the application-level interactions between multiple tasks, while the Task Specification Language (TSL) focuses on the issues re...
876|Specification and Execution of Transactional Workflows|The basic transaction model has evolved over time to incorporate more complex transaction structures and to selectively modify the atomicity and isolation properties. In this chapter we discuss the application of transaction concepts to activities that involve coordinated execution of multiple tasks (possibly of different types) over different processing entities. Such applications are referred to as transactional workflows. In this chapter we discuss the specification of such workflows and the issues involved in their execution.  
877|Merging Application-centric and Data-centric Approaches to Support Transaction-oriented Multi-system Workflows|Workflow management is primarily concerned with dependencies between the tasks of a workflow, to ensure correct control flow and data flow. Transaction management, on the other hand, is concerned with preserving data dependencies by preventing execution of conflicting operations from multiple, concurrently executing tasks or transactions. In this paper we argue that many applications will be served better if the properties of transaction and workflow models are supported by an integrated architecture. We also present preliminary ideas towards such an architecture. 
878|Business process management with FlowMark|From an enterprise point of view the management of business processes is becoming increasingly important: Business processes control which piece of work will be performed by whom and which resources are exploited for this work, i.e. a business process describes how an enterprise will achieve its business goals. In this paper we sketch FlowMark (FlowMark is a trademark of IBM), an IBM program product supporting both, the modeling of business processes and their execution.
879|Using flexible transactions to support multi-system telecommunication applications|Service order provisioning is an important telecommunication application that automates the process of providing telephone services in response to the customer requests. It is an example of a multi-system application that requires access to multiple, independently developed application systems and their databases. In this paper, we describe the design and implementation of a prototype system 1 that supports the execution of the Flexible Transactions and its use to develop the service order provisioning application. We argue that such approach may be used to support the development of multi-system, flow-through processing applications in a systematic and organized manner. Its advantages include fast and easy specification of new services, support for testing of the declaratively specified work-flows, and the specification of potential concurrency among the tasks constituting an application.
880|Distributed Object Management|Future information processing environments will consist of a vast network of heterogeneous, autonomous, and distributed computing resources, including computers (from mainframe to personal), information-intensive applications, and data (files and databases). A key challenge in this environment is providing capabilities for combining this varied collection of resources into an integrated distributed system, allowing resources to be flexibly combined, and their activities coordinated, to address challenging new information processing requirements. In this paper, we describe the concept of distributed object management, and identify its role in the development of these open, interoperable systems. We identify the key aspects of system architectures supporting distributed object management, and describe specific elements of a distributed object management system being developed at GTE Laboratories. 1. Introduction Today, computer usage is expanding into all parts, and all functions, of lar...
881|A Framework For Enforceable Specification Of Extended Transaction Models And Transactional Workflows|A variety of extensions to the traditional (ACID) transaction model have resulted in a plethora of  extended transaction models (ETMs). Many of these ETMs are application-specific, i.e., they are  designed to provide correctness guarantees adequate for a particular application, but not others. Similarly,  an application-specific ETM may impose restrictions that are unacceptable in one application,  yet required in another. To define new ETMs, to determine whether an ETM is appropriate for an  application, and to integrate ETMs to produce new ETMs, we need a framework for ETM specification  and reasoning. In this paper, we describe such a framework. Our framework supports implementation-independent specification of ETMs described in terms of dependencies between transactions.  Dependencies are specified using dependency descriptors. Unlike other transaction specification  frameworks, dependency descriptors use a common set of primitives, and are enforceable, i.e., can be  evaluated at...
882|Chronological Scheduling of Transactions with Temporal Dependencies|Database applications often impose temporal dependencies between transactions that must be satisfied to preserve data consistency. The extant correctness criteria used to schedule the execution of concurrent transactions are either time independent or use strict, difficult to satisfy real-time constraints. On one end of the spectrum, serializability completely ignores time. On the other end, deadline scheduling approaches consider the outcome of each transaction execution correct only if the transaction meets its real-time deadline. In this article, we explore new correctness criteria and scheduling methods that capture temporal transaction dependencies and belong to the broad area between these two extreme approaches. We introduce the concepts of succession dependency and chronological dependency and define correctness criteria under which temporal dependencies between transactions are preserved even if the dependent transactions execute concurrently. We also propose a chronological scheduler that can guarantee that transaction executions satisfy their chronological constraints. The advantages of chronological scheduling over traditional scheduling methods, as well as the main issues in the implementation and performance of the proposed scheduler, are discussed.
883|A message ferrying approach for data delivery in sparse mobile ad hoc networks|Mobile Ad Hoc Networks (MANETs) provide rapidly deployable and self-configuring network capacity required in many critical applications, e.g., battlefields, disaster relief and wide area sensing. In this paper we study the problem of efficient data delivery in sparse MANETs where network partitions can last for a significant period. Previous approaches rely on the use of either long range communication which leads to rapid draining of nodes ’ limited batteries, or existing node mobility which results in low data delivery rates and large delays. In this paper, we describe a Message Ferrying (MF) approach to address the problem. MF is a mobility-assisted approach which utilizes a set of special mobile nodes called message ferries (or ferries for short) to provide communication service for nodes in the deployment area. The main idea behind the MF approach is to introduce non-randomness in the movement of nodes and exploit such non-randomness to help deliver data. We study two variations of MF, depending on whether ferries or nodes initiate proactive movement. The MF design exploits mobility to improve data delivery performance and reduce energy consumption in nodes. We evaluate the performance of MF via extensive ns simulations which confirm the MF approach is efficient in both data delivery and energy consumption under a variety of network conditions.
884|The capacity of wireless networks| When n identical randomly located nodes, each capable of transmitting at bits per second and using a fixed range, form a wireless network, the throughput @ A obtainable by each node for a randomly chosen destination is 2 bits per second under a noninterference protocol. If the nodes are optimally placed in a disk of unit area, traffic patterns are optimally assigned, and each transmission’s range is optimally chosen, the bit–distance product that can be transported by the network per second is 2 @ A bit-meters per second. Thus even under optimal circumstances, the throughput is only 2 bits per second for each node for a destination nonvanishingly far away. Similar results also hold under an alternate physical model where a required signal-to-interference ratio is specified for successful receptions. Fundamentally, it is the need for every node all over the domain to share whatever portion of the channel it is utilizing with nodes in its local neighborhood that is the reason for the constriction in capacity. Splitting the channel into several subchannels does not change any of the results. Some implications may be worth considering by designers. Since the throughput furnished to each user diminishes to zero as the number of users is increased, perhaps networks connecting smaller numbers of users, or featuring connections mostly with nearby neighbors, may be more likely to be find acceptance. 
885|Ad-hoc On-Demand Distance Vector Routing|An ad-hoc network is the cooperative engagement of a collection of mobile nodes without the required intervention of any centralized access point or existing infrastructure. In this paper we present Ad-hoc On Demand Distance Vector Routing (AODV), a novel algorithm for the operation of such ad-hoc networks. Each Mobile Host operates as a specialized router, and routes are obtained as needed (i.e., on-demand) with little or no reliance on periodic advertisements. Our new routing algorithm is quite suitable for a dynamic selfstarting network, as required by users wishing to utilize ad-hoc networks. AODV provides loop-free routes even while repairing broken links. Because the protocol does not require global periodic routing advertisements, the demand on the overall bandwidth available to the mobile nodes is substantially less than in those protocols that do necessitate such advertisements. Nevertheless we can still maintain most of the advantages of basic distance-vector routing mechanisms. We show that our algorithm scales to large populations of mobile nodes wishing to form ad-hoc networks. We also include an evaluation methodology and simulation results to verify the operation of our algorithm.
886|Dynamic source routing in ad hoc wireless networks|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any established infrastructure or centralized administration. In such an environment, it may be necessary for one mobile host to enlist the aid of other hosts in forwarding a packet to its destination, due to the limited range of each mobile host’s wireless transmissions. This paper presents a protocol for routing in ad hoc networks that uses dynamic source routing. The protocol adapts quickly to routing changes when host movement is frequent, yet requires little or no overhead during periods in which hosts move less frequently. Based on results from a packet-level simulation of mobile hosts operating in an ad hoc network, the protocol performs well over a variety of environmental conditions such as host density and movement rates. For all but the highest rates of host movement simulated, the overhead of the protocol is quite low, falling to just 1 % of total data packets transmitted for moderate movement rates in a network of 24 mobile hosts. In all cases, the difference in length between the routes used and the optimal route lengths is negligible, and in most cases, route lengths are on average within a factor of 1.01 of optimal. 1.
887|GPSR: Greedy perimeter stateless routing for wireless networks| We present Greedy Perimeter Stateless Routing (GPSR), a novel routing protocol for wireless datagram networks that uses the positions of touters and a packer&#039;s destination to make packet forwarding decisions. GPSR makes greedy forwarding decisions using only information about a router&#039;s immediate neighbors in the network topology. When a packet reaches a region where greedy forwarding is impossible, the algorithm recovers by routing around the perimeter of the region. By keeping state only about the local topology, GPSR scales better in per-router state than shortest-path and ad-hoc routing protocols as the number of network destinations increases. Under mobility&#039;s frequent topology changes, GPSR can use local topology information to find correct new routes quickly. We describe the GPSR protocol, and use extensive simulation of mobile wireless networks to compare its performance with that of Dynamic Source Routing. Our simulations demonstrate GPSR&#039;s scalability on densely deployed wireless networks.
888|Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers  (1994) |An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize adhoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.  
889|A Delay-Tolerant Network Architecture for Challenged Internets|The highly successful architecture and protocols of today’s Internet may operate poorly in environments characterized by very long delay paths and frequent network partitions. These problems are exacerbated by end nodes with limited power or memory resources. Often deployed in mobile and extreme environments lacking continuous connectivity, many such networks have their own specialized protocols, and do not utilize IP. To achieve interoperability between them, we propose a network architecture and application interface structured around optionally-reliable asynchronous message forwarding, with limited expectations of end-to-end connectivity and node resources. The architecture operates as an overlay above the transport layers of the networks it interconnects, and provides key services such as in-network data storage and retransmission, interoperable naming, authenticated forwarding and a coarse-grained class of service.
890|Location-Aided Routing (LAR) in mobile ad hoc networks  (1998) |A mobile ad hoc network consists of wireless hosts that may move often. Movement of hosts results in a change in routes, requiring some mechanism for determining new routes. Several routing protocols have already been proposed for ad hoc networks. This paper suggests an approach to utilize location information (for instance, obtained using the global positioning system) to improve performance of routing protocols for ad hoc networks. By using location information, the proposed Location-Aided Routing (LAR) protocols limit the search for a new route to a smaller “request zone” of the ad hoc network. This results in a significant reduction in the number of routing messages. We present two algorithms to determine the request zone, and also suggest potential optimizations to our algorithms.
891|Topology Control of Multihop Wireless Networks using Transmit Power Adjustment| We consider the problem of adjusting the transmit powers of nodes in a multihop wireless network (also called an ad hoc network) to create a desired topology. We formulate it as a constrained optimization problem with two constraints- connectivity and biconnectivity, and one optimization objective- maximum power used. We present two centralized algorithms for use in static networks, and prove their optimality. For mobile networks, we present two distributed heuristics that adaptively adjust node transmit powers in response to topological changes and attempt to maintain a connected topology using minimum power. We analyze the throughput, delay, and power consumption of our algorithms using a prototype software implementation, an emulation of a power-controllable radio, and a detailed channel model. Our results show that the performance of multihop wireless networks in practice can be substantially increased with topology control.  
892|CEDAR: a Core-Extraction Distributed Ad hoc Routing Algorithm|Absiract—CEDARis an algorithm for QoS routing in ad hoc network environments. It has three key components: (a) the establishment and main-tenance of a setf-organizing routing infrastructure catted the core for per-forming route computations, (b) the propagation of the link-state of stable high-bandwidth links in the core through increase/decrease waves, and (c)a QoS ra,ute computation algorithm that is exeeuted at the core nodes using onty locally available state. Our preliminary performance evaluation shows that CEDAR is a robust and adaptive QoS routing algorithm that reacts effectively to the dynamics of the network white stitl approximating link-state performance for stable networks. Keywords—Ad hoc routing, QoS routing I.
893|Distributed topology control for power efficient operation in multihop wireless ad hoc networks|Abstract — The topology of wireless multihop ad hoc networks can be controlled by varying the transmission power of each node. We propose a simple distributed algorithm where each node makes local decisions about its transmission power and these local decisions collectively guarantee global connectivity. Specifically, based on the directional information, a node grows it transmission power until it finds a neighbor node in every direction. The resulting network topology increases network lifetime by reducing transmission power and reduces traffic interference by having low node degrees. Moreover, we show that the routes in the multihop network are efficient in power consumption. We give an approximation scheme in which the power consumption of each route can be made arbitrarily close to the optimal by carefully choosing the parameters. Simulation results demonstrate significant performance improvements. I.
894|The Performance of Query Control Schemes for the Zone Routing Protocol|In this paper, we study the performance of route query control mechanisms for the Zone Routing Protocol (ZRP) for ad hoc networks. ZRP proactively maintains routing information for a local neighborhood (routing zone), while reactively acquiring routes to destinations beyond the routing zone. This hybrid routing approach can be more efficient than traditional routing schemes. However, without proper query control techniques, the ZRP cannot provide the expected reduction in the control traffic.
895|The Shared Wireless Infostation Model - A New Ad Hoc Networking Paradigm (or Where there is a Whale, there is a Way)  (2003) |In wireless ad hoc networks, capacity can be traded for delay. This tradeoff has been the subject of a number of studies, mainly concentrating on the two extremes: either minimizing the delay or maximizing the capacity. However, in between these extremes, there are schemes that allow instantiations of various degrees of this tradeoff. Infostations, which offer geographically intermittent coverage at high speeds, are one such an example. Indeed, through the use of the Infostation networking paradigm, the capacity of a mobile network can be increased at the expense of delay. We propose to further extend the Infostation concept by integrating it with the ad hoc networking technology. We refer to this networking model as Wireless Infostation Model (SWIM). SWIM allows additional improvement in the capacity-delay tradeoff through a moderate increase in the storage requirements. To demonstrate how SWIM can be applied to solve a practical problem, we use the example of a biological information acquisition system - radio-tagged whales - as nodes in an ad hoc network. We derive an analytical formula for the distribution of end-to-end delays and calculate the storage requirements. We further extend SWIM by allowing multi-tiered operation; which in our biological information acquisition system could be realized through seabirds acting as mobile data collection nodes.
896|Age Matters: Efficient Route Discovery in Mobile Ad Hoc Networks Using Encounter Ages|We propose FResher Encounter SearcH (FRESH), a simple algorithm for efficient route discovery in mobile ad hoc networks. Nodes keep a record of their most recent encounter times with all other nodes. Instead of searching for the destination, the source node searches for any intermediate node that encountered the destination more recently than did the source node itself. The intermediate node then searches for a node that encountered the destination yet more recently, and the procedure iterates until the destination is reached. Therefore, FRESH replaces the single network-wide search of current proposals with a succession of smaller searches, resulting in a cheaper route discovery. Routes obtained are loop-free. The performance of such...
897|Mobility Prediction and Routing in Ad Hoc Wireless Networks|Wireless networks allow a more flexible model of communication than traditional networks since the user is not limited to a fixed physical location. Unlike cellular wireless networks, an ad hoc wireless network does not have any fixed communication infrastructure. For an active connection, the end host as well as the intermediate nodes can be mobile. Therefore routes are subject to frequent disconnections. In such an environment, it is important to minimize disruptions caused by the changing topology for critical application such as voice and video. This presents a difficult challenge for routing protocols, since rapid reconstruction of routes is crucial in the presence of topology changes. By exploiting non-random behaviors for the mobility patterns that mobile users exhibit, we can predict the future state of network topology and perform route reconstruction proactively in a timely manner. Moreover, by using the predicted information on the network topology, we can eliminate transmis...
898|CarNet: A Scalable Ad Hoc Wireless Network System|CarNet is an application for a large ad hoc mobile network system that scales well without requiring a xed network infrastructure to route messages. CarNet places radio nodes in cars, which communicate using Grid, a novel scalable routing system. Grid uses geographic forwarding and a scalable distributed location service to route packets from car to car without ooding the network. CarNet will support IP connectivity as well as applications such as cooperative highway congestion monitoring, eet tracking, and discovery of nearby points of interest. 1 Introduction The Internet has evolved in a way that sacri ces dynamism in favor of scale: it groups nodes into an addressing and routing hierarchy that inhibits movement, and it depends on xed physical infrastructure that inhibits rapid deployment. We are designing a scalable and dynamic network architecture called Grid, which will enable new kinds of applications and will be easier to deploy than existing technology. We desire several...
899|Capacity, Delay and Mobility in Wireless Ad-Hoc Networks|Network throughput and packet delay are two important parameters in the design and the evaluation of routing protocols for ad-hoc networks. While mobility has been shown to increase the capacity of a network, it is not clear whether the delay can be kept low without trading off the throughput. We consider a theoretical framework and propose a routing algorithm which exploits the patterns in the mobility of nodes to provide guarantees on the delay. Moreover, the throughput achieved by the algorithm is only a poly-logarithmic factor off from the optimal. The algorithm itself is fairly simple. In order to analyze its feasibility and the performance guarantee, we used various techniques of probabilistic analysis of algorithms. The approach taken in this paper could be applied to the analyses of some other routing algorithms for mobile ad hoc networks proposed in the literature.
900|Mobility Helps Security in Ad Hoc Networks|Contrary to the common belief that mobility makes security more difficult to achieve, we show that node mobility can, in fact, be useful to provide security in ad hoc networks. We propose a technique in which security associations between nodes are established, when they are in the vicinity of each other, by exchanging appropriate cryptographic material. We show that this technique is generic, by explaining its application to fully self-organized ad hoc networks and to ad hoc networks placed under an (off-line) authority. We also propose an extension of this basic mechanism, in which a security association can be established with the help of a &#034;friend&#034;. We show that our mechanism can work in any network configuration and that the time necessary to set up the security associations is strongly influenced by several factors, including the size of the deployment area, the mobility patterns, and the number of friends; we provide a detailed investigation of this influence.
901|Wearable Computers as Packet Transport Mechanisms in Highly-Partitioned Ad-Hoc Networks|The decreasing size and cost of wearable computers and mobile sensors is presenting new challenges and opportunities for deploying networks. Existing network routing protocols provide reliable communication between nodes and allow for mobility and even ad-hoc deployment. They rely, however, on the assumption of constant end-to-end connectivity in the network. In this paper, we address routing support for ad-hoc, wireless networks under conditions of sporadic connectivity and ever-present network partitions. This work proposes a general framework of agent movement and communication in which mobile computers physically carry packets across network partitions. We then propose algorithms that exploit the relative position of stationary devices and non-randomness in the movement of mobile agents in the network. The learned structure of the network is used to inform an adaptive routing strategy. With a simulation, we evaluate these algorithms and their ability to route packets efficiently through a highly-partitioned network.
902|The minimum latency problem|We are given a set of points pl,...,p. and a symmetric distance matrix (o!ij) giving the distance between pi and pj. We wish to construct a tour that minimizes ~~=1 1(z), where l(i) is
903|Delay Limited Capacity of Ad hoc Networks: Asymptotically Optimal Transmission and Relaying Strategy|The delay limited capacity of an ad hoc wireless network confined to a finite region is investigated. A transmission and relaying strategy making use of the nodes&#039; motion to maximize the throughput is constructed. An approximate expression for the capacity as a function of the maximum allowable delay is obtained. It is found that there exists a critical value of the delay such that: (1) for values of the delay d below critical, the capacity does not benefit appreciably from the motion, (2) for moderate values of the delay d above critical, the capacity that can be achieved by taking advantage of the motion increases   , (3) the dependence of the critical delay on the number of nodes is a very slowly increasing function (n    ) . Finally, asymptotic optimality of the proposed strategy in a certain class is shown.
904|Integrated routing and storage for messaging applications in mobile ad hoc networks|This paper is motivated by the observation that traditional ad hoc routing protocols are not an adequate solution for messaging applications (e.g., e-mail) in mobile ad hoc networks. Routing in ad hoc mobile networks is challenging mainly because of node mobility—the more rapid the rate of movement, the greater the fraction of bad routes and undelivered messages. For applications that can tolerate delays beyond conventional forwarding delays, we advocate a relay-based approach to be used in conjunction with traditional ad hoc routing protocols. This approach takes advantage of node mobility to disseminate messages to mobile nodes. The result is the Mobile Relay Protocol (MRP), which integrates message routing and storage in the network; the basic idea is that if a route to a destination is unavailable, a node performs a controlled local broadcast (a relay) to its immediate neighbors. In a network with sufficient mobility—precisely the situation when conventional routes are likely to be non-existent or broken—it is quite likely that one of the relay nodes to which the packet has been relayed will encounter a node that has a valid, short (conventional) route to the eventual destination, thereby increasing the likelihood that the message will be successfully delivered. Our simulation results under a variety of node movement models demonstrate that this idea can work well for applications that prefer reliability over latency. 1
905|Modeling and simulation of genetic regulatory systems: A literature review|In order to understand the functioning of organisms on the molecular level, we need to know which genes are expressed, when and where in the organism, and to which extent. The regulation of gene expression is achieved through genetic regulatory systems structured by networks of interactions between DNA, RNA, proteins, and small molecules. As most genetic regulatory networks of interest involve many components connected through interlocking positive and negative feedback loops, an intuitive understanding of their dynamics is hard to obtain. As a consequence, formal methods and computer tools for the modeling and simulation of genetic regulatory networks will be indispensable. This paper reviews formalisms that have been employed in mathematical biology and bioinformatics to describe genetic regulatory systems, in particular directed graphs, Bayesian networks, Boolean networks and their generalizations, ordinary and partial differential equations, qualitative differential equations, stochastic equations, and rule-based formalisms. In addition, the paper discusses how these formalisms have been used in the simulation of the behavior of actual regulatory systems. 
906|Qualitative process theory|Objects move, collide, flow, bend, heat up, cool down, stretch, compress. and boil. These and other things that cause changes in objects over time are intuitively characterized as processes. To understand commonsense physical reasoning and make programs that interact with the physical world as well as people do we must understand qualitative reasoning about processes, when they will occur, their effects, and when they will stop. Qualitative process theory defines a simple notion of physical process that appears useful as a language in which to write dynamical theories. Reasoning about processes also motivates a new qualitative representation for quantity in terms of inequalities, called the quantity space. This paper describes the basic concepts of qualitative process theory, several different kinds of reasoning that can be performed with them, and discusses its implications for causal reasoning. Several extended examples illustrate the utility of the theory, including figuring out that a boiler can blow up, that an oscillator with friction will eventually stop, and how to say that you can pull with a string, but not push with it. 1
907|Knowledge-based Analysis of Microarray Gene Expression Data By Using Support Vector Machines|We introduce a method of functionally classifying genes by using gene expression data from DNA microarray hybridization experiments. The method is based on the theory of support vector machines (SVMs). SVMs are considered a supervised computer learning method because they exploit prior knowledge of gene function to identify unknown genes of similar function from expression data. SVMs avoid several problems associated with unsupervised clustering methods, such as hierarchical clustering and self-organizing maps. SVMs have many mathematical features that make them attractive for gene expression analysis, including their exibility in choosing a similarity function, sparseness of solution when dealing with large data sets, the ability t...
908|Genetic Network Inference: From Co-Expression Clustering To Reverse Engineering|motivation: Advances in molecular biological, analytical and computational technologies are enabling us to systematically investigate the complex molecular processes underlying biological systems. In particular, using highthroughput gene expression assays, we are able to measure the output of the gene regulatory network. We aim here to review datamining and modeling approaches for conceptualizing and unraveling the functional relationships implicit in these datasets. Clustering of co-expression profiles allows us to infer shared regulatory inputs and functional pathways. We discuss various aspects of clustering, ranging from distance measures to clustering algorithms and multiple-cluster memberships. More advanced analysis aims to infer causal connections between genes directly, i.e. who is regulating whom and how. We discuss several approaches to the problem of reverse engineering of genetic networks, from discrete Boolean networks, to continuous linear and non-linear models. We conclude that the combination of predictive modeling with systematic experimental verification will be required to gain a deeper insight into living organisms, therapeutic targeting and bioengineering.
910|Qualitative Simulation of Genetic Regulatory Networks Using Piecewise-Linear Models|In order to cope with the large amounts of data that have become available in genomics,  mathematical tools for the analysis of networks of interactions between  genes, proteins, and other molecules are indispensable. We present a method  for the qualitative simulation of genetic regulatory networks, based on a class of  piecewise-linear (PL) differential equations that has been well-studied in mathematical  biology. The simulation method is well-adapted to state-of-the-art measurement  techniques in genomics, which often provide qualitative and coarsegrained  descriptions of genetic regulatory networks. Given a qualitative model  of a genetic regulatory network, consisting of a system of PL differential equations  and inequality constraints on the parameter values, the method produces a  graph of qualitative states and transitions between qualitative states, summarizing  the qualitative dynamics of the system. The qualitative simulation method  has been implemented in Java in the computer tool Genetic Network Analyzer.
911|A rigorous derivation of the chemical master equation|It is widely believed that the chemical master equation has no rigorous microphysical basis, and hence no a priori claim to validity. This view is challenged here through arguments purporting to show that the chemical master equation is exact for any gas-phase chemical system that is kept well stirred and thermally equilibrated. 
912|The Escherichia coli MG1655 in silico metabolic genotype: Its definition, characteristics, and capabilities|The Escherichia coli MG1655 genome has been completely sequenced. The annotated sequence, biochemical information, and other information were used to reconstruct the E. coli metabolic map. The stoichiometric coefficients for each metabolic enzyme in the E. coli metabolic map were assembled to construct a genomespecific stoichiometric matrix. The E. coli stoichiometric matrix was used to define the system’s characteristics and the capabilities of E. coli metabolism. The effects of gene deletions in the central metabolic pathways on the ability of the in silico metabolic network to support growth were assessed, and the in silico predictions were compared with experimental observations. It was shown that based on stoichiometric and capacity constraints the in silico analysis was able to qualitatively predict the growth potential of mutant strains in 86 % of the cases examined. Herein, it is demonstrated that the synthesis of in silico metabolic genotypes based on
913|A Test Case of Correlation Metric Construction of a Reaction Pathway from Measurements|A method for the prediction of the interactions within complex reaction networks from experimentally measured time series of the concentration of the species composing the system has been tested experimentally on the first few steps of the glycolytic pathway. The reconstituted reaction system, containing eight enzymes and 14 metabolic inter-mediates, was kept away from equilibrium in a continuous-flow, stirred-tank reactor. Input concentrations of adenosine monophosphate and citrate were externally varied over time, and their concentrations in the reactor and the response of eight other species were measured. Multidimensional scaling analysis and heuristic algorithms applied to two-species time-lagged correlation functions derived from the time series yielded a diagram from which the interactions among all of the species could be deduced. The diagram predicts essential features of the known reaction network in regard to chemical reactions and interactions among the measured species. The approach is applicable to many complex reaction systems. Traditionally, chemical kinetics reveals the mechanism of a chemical reaction by
914|Kinetic analysis of a molecular model of the budding yeast cell cycle|The molecular machinery of cell cycle control is known in more detail for budding yeast, Saccharomyces cerevisiae, than for any other eukaryotic organism. In recent years, many elegant experiments on budding yeast have dissected the roles of cyclin molecules (Cln1–3 and Clb1–6) in coordinating the events of DNA synthesis, bud emergence, spindle formation, nuclear division, and cell separation. These experimental clues suggest a mechanism for the principal molecular interactions controlling cyclin synthesis and degradation. Using standard techniques of biochemical kinetics, we convert the mechanism into a set of differential equations, which describe the time courses of three major classes of cyclin-dependent kinase activities. Model in hand, we examine the molecular events controlling “Start ” (the commitment step to a new round of chromosome replication, bud formation, and mitosis) and “Finish ” (the transition from metaphase to anaphase, when sister chromatids are pulled apart and the bud separates from the mother cell) in wild-type cells and 50 mutants. The model accounts for many details of the physiology, biochemistry, and genetics of cell cycle control in budding yeast.
915|Hybrid Modeling and Simulation of Biomolecular Networks|In a biological cell, cellular functions and the genetic regulatory  apparatus are implemented and controlled by a network of chemical  reactions in which regulatory proteins can control genes that produce  other regulators, which in turn control other genes. Further, the feedback  pathways appear to incorporate switches that result in changes in  the dynamic behavior of the cell. This paper describes a hybrid systems  approach to modeling the intra-cellular network using continuous di#erential  equations to model the feedback mechanisms and mode-switching  to describe the changes in the underlying dynamics. We use two case  studies to illustrate a modular approach to modeling such networks and  describe the architectural and behavioral hierarchy in the underlying  models. We describe these models using Charon [2], a language that  allows formal description of hybrid systems. We provide preliminary simulation  results that demonstrate how our approach can help biologists  in their analysis of noisy genetic circuits. Finally we describe our agenda  for future work that includes the development of models and simulation  for stochastic hybrid systems.
916|Positive and Negative Circuits in Dynamical Systems|We state precisely and demonstrate two conjectures of R. Thomas following which a) the existence of a positive circuit in the oriented interaction graph of a dioeerential system is a necessary condition for the existence of several steady states, and b) the existence of a negative non-oriented circuit of length at least two is a necessary condition for the existence of a stable periodic orbit.
917|Lateral Inhibition through Delta-Notch Signaling: A Piecewise Affine Hybrid Model|Biological cell networks exhibit complex combinations of both discrete and continuous behaviors: indeed, the dynamics that govern the spatial and temporal increase or decrease of protein concentration inside a single cell are continuous di#erential equations, while the activation or deactivation of these continuous dynamics are triggered by discrete switches which encode protein concentrations reaching given thresholds. In this paper, we model as a hybrid system a striking example of this behavior in a biological mechanism called Delta-Notch signaling, which is thought to be the primary mechanism of cell di#erentiation in a variety of cell networks. We present results in both simulation and reachability analysis of this hybrid system. We emphasize how the hybrid system model is computationally superior (for both simulation and analysis) to other nonlinear models in the literature, without compromising faithful modeling of the biological phenomena. 1 
918|A Class of Piecewise Linear Differential Equations Arising In Biological Models|We investigate the properties of the solutions of a class of piecewise-linear differential equations. The equations are appropriate to model biological systems (e.g., genetic networks) in which there are switch-like interactions between the elements. The analysis uses the concept of Filippov solutions of differential equations with a discontinuous righthand side. It gives an insight into the so-called singular solutions which lie on the surfaces of discontinuity. We show that this notion clarifies the study of several examples studied in the literature.
919|Qualitative and Quantitative Simulation: Bridging the Gap|Shortcomings of qualitative simulation and of quantitative simulation motivate combining them to do simulations exhibiting strengths of both. The resulting class of techniques is called  semi-quantitative simulation. One approach to semi-quantitative simulation is to use numeric intervals to represent incomplete quantitative information. In this research we demonstrate semiquantitative simulation using intervals in an implemented semi-quantitative simulator called Q3. Q3 progressively refines a qualitative simulation, providing increasingly specific quantitative predictions which can converge to a numerical simulation in the limit while retaining important correctness guarantees from qualitative and interval simulation techniques.  Q3&#039;s simulations are based on a technique we call step size refinement. While a pure qualitative simulation has a very coarse step size, representing the state of a system trajectory at relatively few qualitatively distinct states, Q3 interpolates newly expl...
920|A System for Identifying Genetic Networks from Gene Expression Patterns Produced by Gene Disruptions and Overexpressions|A hot research topic in genomics is to analyze the interactions between genes by systematic  gene disruptions and gene overexpressions. Based on a boolean network model without time delay,  we have been investigating e#cient strategies for identifying a genetic network by multiple gene  disruptions and overexpressions. This paper first shows the relationship between our boolean  network model without time delay and the standard synchronous boolean network model. Then we  present a simulator of boolean networks without time delay for multiple gene disruptions and gene  overexpressions, which includes a genetic network identifier with a graphic interface that generates  instructions for experiments of gene disruptions and overexpressions.
921|Knowledge-Based Simulation of DNA Metabolism: Prediction of Enzyme Action|We have developed a knowledge-based simulation of DNA metabolism that accurately predicts the actions of enzymes on DNA under a large number of environmental conditions. Previous simulations of enzyme systems rely predominantly on mathematical models. We use a frame-based representation to model enzymes, substrates, and conditions. Interactions between these objects are expressed using production rules and an underlying truth maintenance system. The system performs rapid inference and can explain its reasoning. A graphical interface provides access to all elements of the simulation, including object representations and explanation graphs. Predicting enzyme action is the first step in the development of a large knowledge base to envision the metabolic pathways of DNA replication and repair.  1. Introduction  Our understanding of any process can be measured by the extent to which a simulation we create mimics the real behavior of that process. Deviations of a simulation indicate either l...
922|BioSim - A New Qualitative Simulation Environment for Molecular Biology|Traditionally, biochemical systems are modelled using kinetics and differential equations in a quantitative simulator. However, for many biological processes detailed quantitative information is not available, only qualitative or fuzzy statements about the nature of interactions. In a previous paper we have shown the applicability of qualitative reasoning methods for molecular biological regulatory processes. Now, we present a newly developed simulation environment, BioSim, that is written in Prolog using constraint logic programming techniques. The simulator combines the basic ideas of two main approaches to qualitative reasoning and integrates the contents of a molecular biology knowledge base, EcoCyc. We show that qualitative reasoning can be combined with automatic transformation of contents of genomic databases into simulation models to give an interactive modelling system that reasons about the relations and interactions of biological entities. This is demonstrated on the glycoly...
923|Modeling the Activity of Single Genes |Introduction  1.1 Motivation -- the challenge of understanding gene regulation  The central dogma of molecular biology states that information is stored in DNA, transcribed to messenger RNA (mRNA) and then translated into proteins. This picture is significantly augmentated when we consider the action of certain proteins in regulating transcription. These transcription factors provide a feedback pathway by which genes can regulate one another&#039;s expression as mRNA and then as protein. To review: DNA, RNA and proteins have different functions. DNA is the molecular storehouse of genetic information. When cells divide, the DNA is replicated, so that each daughter cell maintains the same genetic information as the mother cell. RNA acts as a go-between from DNA to proteins. Only a single copy of DNA is present, but multiple copies of the same piece of RNA may be present, allowing cells to make huge amounts of protein. In eukaryotes (organisms with a nuc
924|Sensitivity of biological models to errors in parameter estimates|Since A. M. Turing’s paper proposing a mathematical basis for pattern formation in developing organisms many mathematical approaches have been proposed to model biological phenomenon. Continued laboratory study and recent improvements in measurement capabilities have provided an immense quantity of raw gene expression data. The level of data now available demands the development of well-characterized and tested computational tools. Thus, we have examined one mathematical model’s sensitivity to errors in estimating its ’ parameters. Errors in parameter estimation can arise from noise in the laboratory measurements and recasting of laboratory data. We elected to examine the rulebased mathematical model of Mjolsness et al for its ’ sensitivity to errors in estimated parameters. We have used the technique of sensitivity equations as generally applied in nonlinear systems analysis.
925|A Knowledge Base for D. melanogaster gene interactions involved in pattern formation |The understanding of pattern formation in Drosophila  requires the handling of the many genetic and molecular  interactions which occur between developmental genes. For  that purpose, a knowledge base (KNIFE) has been developed  in order to structure and manipulate the interaction data.  KNIFE contains data about interactions published in the  literature and gathered from various databases. These data  are structured in an object knowledge representation system  into various interrelated entities. KNIFE can be browsed  through a WWW interface in order to select, classify and  examine the objects and their references in other bases. It  also provides specialised biological tools such as interaction  network manipulation and diagnosis of missing interactions.  We are interested in the biological process of pattern formation in Drosophila and in understanding the basis of specific identity acquisition by the different body parts [Fasano et al. 1991; Rder, Vola and Kerridge 1992; Alexandre...
927|Bathtub dynamics: initial results of a systems thinking inventory|Fund. Nelson Repenning graciously permitted us to administer the tasks in his introductory system dynamics class. We also thank Jim Doyle, Michael Radzicki, Terry Tivnan the referees for helpful comments. Christopher Hunter assisted with data entry.
928|Building a Large Annotated Corpus of English: The Penn Treebank|There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.

In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989-1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.
929|A statistical approach to machine translation|In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results.
930|A maximum likelihood approach to continuous speech recognition|Abstract-Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of sta-tistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them. Index Terms-Markov models, maximum likelihood, parameter esti-mation, speech recognition, statistical models. I.
931|Statistical Language Modeling Using The Cmu-Cambridge Toolkit|The CMU Statistical Language Modeling toolkit was released in 1994 in order to facilitate the construction and testing of bigram and trigram language models. It is currently in use in over 40 academic, government and industrial laboratories in over 12 countries. This paper presents a new version of the toolkit. We outline the conventional language modeling technology, as implemented in the toolkit, and describe the extra efficiency and functionality that the new toolkit provides as compared to previous software for this task. Finally,we give an example of the use of the toolkit in constructing and testing a simple language model.
932|A Gaussian prior for smoothing maximum entropy models|In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood train-ing for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods com-pare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty [1] performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parame-ters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance.
933|Entropy-based pruning of backoff language models |A criterion for pruning parameters from N-gram backoff language models is developed, based on the relative entropy between the original and the pruned model. It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models. The relative entropy measure can be expressed as a relative change in training set perplexity. This leads to a simple pruning criterion whereby all N-grams that change perplexity by less than a threshold are removed from the model. Experiments show that a production-quality Hub4 LM can be reduced to 26 % its original size without increasing recognition error. We also compare the approach to a heuristic pruning criterion by Seymore and Rosenfeld [9], and show that their approach can be interpreted as an approximation to the relative entropy criterion. Experimentally, both approaches select similar sets of N-grams (about 85% overlap), with the exact relative entropy criterion giving marginally better performance. 1.
935|The CMU Statistical Language Modeling Toolkit and its use in the 1994 ARPA CSR Evaluation|The Carnegie Mellon Statistical Language Modeling (CMU SLM) Toolkit is a set of Unix software tools designed to facilitate language modeling work in the research community. The package, including source code, is freely available for research purposes. As of December 1994, the toolkit is in active use by 23 research groups in 8 countries. It was recently used to process the 2.5 GBNAB corpus for the ARPA CSR community. In this paper, I first discuss the design principles and features of the toolkit. Then, I describe the composition of the NAB corpus, and report on the ngram statistics, standard vocabulary and language models created using the SLM tools. 1. OVERVIEW OF THE CMU SLM TOOLKIT 1.1. Introduction  The Carnegie Mellon University Statistical Language Modeling (CMU SLM) Toolkit is a set of Unix software tools designed to facilitate language modeling work in the research community. Some of the tools are used to process general textual data into: ffl word frequency lists and vocabula...
936|Scalable Backoff Language Models|When a trigram backoff language model is created from a large body of text, trigrams and bigrams that occur few times in the training text are often excluded from the model in order to decrease the model size. Generally, the elimination of n-grams with very low counts is believed to not significantly affect model performance. This project investigates the degradation of a trigram backoff model&#039;s perplexity and word error rates as bigram and trigram cutoffs are increased. The advantage of reduction in model size is compared to the increase in word error rate and perplexity scores. More importantly, this project also investigates alternative ways of excluding bigrams and trigrams from a backoff language model, using criteria other than the number of times an n-gram occurs in the training text. Specifically, a difference method has been investigated where the difference in the logs of the original and backed off trigram and bigram probabilities is used as a basis for n-gram exclusion from...
937|Evaluation Metrics For Language Models|The most widely-used evaluation metric for language models for speech recognition is the perplexity of test data. While perplexities can be calculated efficiently and without access to a speech recognizer, they often do not correlate well with speech recognition word-error rates. In this research, we attempt to find a measure that like perplexity is easily calculated but which better predicts speech recognition performance. We investigate two approaches; first, we attempt to extend perplexity by using similar measures that utilize information about language models that perplexity ignores. Second, we attempt to imitate the word-error calculation without using a speech recognizer by artificially generating speech recognition lattices. To test our new metrics, we have built over thirty varied language models. We find that perplexity correlates with word-error rate remarkably well when only considering n-gram models trained on in-domain data. When considering other types of models, our novel metrics are superior to perplexity for predicting speech recognition performance. However, we conclude that none of these measures predict word-error rate sufficiently accurately to be effective tools for language model evaluation in speech recognition.
939|Hub4 language modeling using domain interpolation and data clustering|In SRI’s language modeling experiments for the Hub4 domain, three basic approaches were pursued: interpolating multiple models estimated from Hub4 and non-Hub4 training data, adapting the language model (LM) to the focus conditions, and adapting the LM to different topic types. In the first approach, we built separate LMs for the closely transcribed Hub4 material (acoustic training transcripts) and the loosely transcribed Hub4 material (LM training data), as well as the North-American Business News (NABN) and Switchboard training data, projected onto the Hub4 vocabulary. By interpolating the probabilities obtained from these models, we obtained a 20 % reduction in perplexity and a 1.8 % reduction in word error rate, compared to a baseline Hub4-only language model. Two adaptation approaches are also described: adapting language models to the speech styles correlated with different focus conditions, and building cluster-specific LM mixtures. These two approaches give some reduction in perplexity, but no significant reduction in word error. Finally, we identify the problems and future directions of our work. 1.
940|The JANUS Speech Recognizer|JANUS [17] was designed for the translation of spontaneous human-to-human speech. Before the 1994 CSR evaluation, JANUS was run with vocabularies of up to 2500 words. JANUS was also tested on the Conference Registration and the Resource Management tasks. The best error rate on the &#039;89 Resource Management evaluation set was 5.9%. At the June 1994 Verbmobil speech component evaluation [1], JANUS scored best among eight participants on the German appointment scheduling task, a task of spontaneous human to human dialogs. In this paper we give a detailed description of the recognition engine of JANUS, focusing on the acoustic modeling and our first run with the WSJ task. 1. ACOUSTIC MODELING IN JANUS 1.1 PREPROCESSING  For the 1994 CSR evaluation we computed 16 mel scale spectral coefficients from an FFT with a window size of 256 sample points and a window shift (frame rate) of 10 ms. 16 mel spectral coefficients, 16 delta coefficients, and 16 delta-delta coefficients were used to build a 4...
941|Flocks, herds, and schools: a distributed behavioral model|The aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. But this type of complex motion is rarely seen in computer animation. This paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. The simulated flock is an elaboration of a particle system, with the simulated birds being the particles. The aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. Each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the &#034;animator. &#034; The aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated birds.
942|DISTRIBUTED SYSTEMS|Growth of distributed systems has attained unstoppable momentum. If we better understood how to think about, analyze, and design distributed systems, we could direct their implementation with more confidence.
943|Dynamic Graphics using Quasi Parallelism|Dynamic computer graphics is best represented as several processes operating in parallel. Full parallel processing, however, entails much complex mechanism making it difficult to write simple, intuitive programs for generating computer animation. What is presented in this paper is a simple means of attaining the appearance of parallelism and the ability to program the .graphics in a conceptually parallel fashion without the complexity of a more general parallel mechanism. Each entity on the display screen can be independently programmed to move, turn, change size, color or shape and to interact with other entities. &#039;  The scheme presented herein begins with the notion of a quantum of time, or tick, within which there are no ordering constraints on events. Each entity or actor decides what it must do upon the next tick. Ticks are a powerful means of controlling parallel processes but are usually at tOO low a conceptual level for user&#039; convenience. HigherLlevel operations built upon the tick mechanism are presented, most notally the ability to instruct any entity or group of entities to gradually change or move at a rate that is itself changeable by the &#039;same operation. To illustrate these ideas a simple celestial &#039;mechanics simulation &#039; is presented. Upon each tick the velocities&#039;and positions of the objects are updated by the gravitational and propulsive forces acting upon them.
944|Wide-Area Traffic: The Failure of Poisson Modeling|Network arrivals are often modeled as Poisson processes for analytic simplicity, even though a number of traffic studies have shown that packet interarrivals are not exponentially distributed. We evaluate 24 wide-area traces, investigating a number of wide-area TCP arrival processes (session and connection arrivals, FTP data connection arrivals within FTP sessions, and TELNET packet arrivals) to determine the error introduced by modeling them using Poisson processes. We find that user-initiated TCP session arrivals, such as remotelogin and file-transfer, are well-modeled as Poisson processes with fixed hourly rates, but that other connection arrivals deviate considerably from Poisson; that modeling TELNET packet interarrivals as exponential grievously underestimates the burstiness of TELNET traffic, but using the empirical Tcplib [Danzig et al, 1992] interarrivals preserves burstiness over many time scales; and that FTP data connection arrivals within FTP sessions come bunched into “connection bursts,” the largest of which are so large that they completely dominate FTP data traffic. Finally, we offer some results regarding how our findings relate to the possible self-similarity of widearea traffic.  
945|On the Self-similar Nature of Ethernet Traffic (Extended Version)  (1994) | We demonstrate that Ethernet LAN traffic is statistically self-similar, that none of the commonly used traffic models is able to capture this fractal-like behavior, that such behavior has serious implications for the design, control, and analysis of high-speed, cell-based networks, and that aggregating streams of such traffic typically intensifies the self-similarity (“burstiness”) instead of smoothing it. Our conclusions are supported by a rigorous statistical analysis of hundreds of millions of high quality Ethernet traffic measurements collected between 1989 and 1992, coupled with a discussion of the underlying mathematical and statistical properties of self-similarity and their relationship with actual network behavior. We also present traffic models based on self-similar stochastic processes that provide simple, accurate, and realistic descriptions of traffic scenarios expected during B-ISDN deployment. 
946|Supporting Real-Time Applications in an Integrated Services Packet Network: Architecture and Mechanism|This paper considers the support of real-time applications in an
947|Analysis, Modeling and Generation of Self-Similar VBR Video Traffic|We present a detailed statistical analysis of a 2-hour long empirical sample of VBR video. The sample was obtained by applying a simple intraframe video compression code to an action movie. The main findings of our analysis are (1) the tail behavior of the marginal bandwidth distribution can be accurately described using &#034;heavy-tailed&#034; distributions (e.g., Pareto); (2) the autocorrelation of the VBR video sequence decays hyperbolically (equivalent to long-range dependence) and can be modeled using self-similar processes. We combine our findings in a new (non-Markovian) source model for VBR video and present an algorithm for generating synthetic traffic. Trace-driven simulations show that statistical multiplexing results in significant bandwidth efficiency even when long-range dependence is present. Simulations of our source model show long-range dependence and heavy-tailed marginals to be important components which are not accounted for in currently used VBR video traffic models. 1 I...
948|The synchronization of periodic routing messages|Abstract — The paper considers a network with many apparently-independent periodic processes and discusses one method by which these processes can inadvertent Iy become synchronized. In particular, we study the synchronization of periodic routing messages, and offer guidelines on how to avoid inadvertent synchronization. Using simulations and analysis, we study the process of synchronization and show that the transition from unsynchronized to synchronized traffic is not one of gradual degradation but is instead a very abrupt ‘phase transition’: in general, the addition of a single router will convert a completely unsynchronized traffic stream into a completely synchronized one. We show that synchronization can be avoided by the addition of randomization to the tra~c sources and quantify how much randomization is necessary. In addition, we argue that the inadvertent synchronization of periodic processes is likely to become an increasing problem in computer networks.
949|On Traffic Phase Effects in Packet-Switched Gateways|this paper we define the notion of traffic phase in a packet-switched network and describe how phase differences between competing traffic streams can be the dominant factor in relative throughput. Drop Tail gateways in a TCP/IP network with strongly periodic traffic can result in systematic discrimination against some connections. We demonstrate this
950|Empirically-Derived Analytic Models of Wide-Area TCP Connections: Extended Report|We analyze 2.5 million TCP connections that occurred during 14 wide-area traffic traces. The traces were gathered at five &#034;stub&#034; networks and two internetwork gateways, providing a diverse look at wide-area traffic. We derive analytic models describing the random variables associated with telnet, nntp, smtp, and ftp connections, and present a methodology for comparing the effectiveness of the analytic models with empirical models such as tcplib [DJ91]. Overall we find that the analytic models provide good descriptions, generally modeling the various distributions as well as empirical models and in some cases better.
951|Packet Trains: Measurements and a New Model for Computer Network Traffic|Traffic measurements on a ring local area computer network
at the Massachusetts Institute of Technology are presented. The analysis of the arrival pattern shows that the arrival processes are neither Poisson nor compound Poisson. An alternative model called “packet train” is proposed.

In the train model, the traffic on the network consists of a number of packet streams between various pairs of nodes tohne network. Each node-pair stream (or node-pair process, as we call them) consists of a number of trains. Each train consists of a numbero f packets (or cars) going in either direction (from node A to B or from node B to A). The
intercar gap is large (compared to packet transmission time) and random. The intertrain time is even larger. The Poisson and the compound Poisson arrivals are shown to sbpee cial cases of the train arrival model. Another important observation is that the packet arrivals exhibit a
“source locality.” If a packet is seen on the network going from A to B, the probability of the next packet going from A to B or from B to A is very high. Implications of the train arrivals and of source locality on the design
of bridges, gateways, and reservation protocols are discussed. A number of open problems requiring development of analysis techniques for systems with train arrival processes are also described.
952|Telnet Protocol Specification|The Internet Protocol (IP) [1] is used for host-to-host datagram service in a system of interconnected networks called the Catenet [2]. The network connecting devices are called Gateways. These gateways communicate between themselves for control purposes
953|Local Area Network Traffic Characteristics, with Implications for Broadband Network Congestion Management |This paper examines the phenomenon of congestion in order to better understand the congestion management techniques that will be needed in high-speed, cell-based networks. The first step of this study is to use high time-resolution local area network (LAN) traffic data to explore the nature of LAN traffic variability. Then, we use the data for a trace-driven simulation of a connectionless service that provides LAN interconnection. The simulation allows us to characterize what congestion might look like in a high-speed, cell-based network. The most
954|An Empirical Workload Model for Driving Wide-Area TCP/IP Network Simulations|We present an artificial workload model of wide-area internetwork traffic. The model can be used to drive simulation experiments of communication protocols and flow and congestion control experiments. The model is based on analysis of wide-area TCP/IP traffic collected from  one industrial and two academic networks. The artificial workload model uses both detailed knowledge and measured characteristics of the user application programs responsible for the traffic. Observations drawn from our measurements contradict some commonly held beliefs regarding wide-area TCP/IP network traffic.
955|Statistical analysis of CCSN/SS7 traffic data from working CCS subnetworks|In this paper we report on an ongoing statistical analysis of actual CCSN traffic data. The data consist of approximately 170 million signaling messages collected from a variety of different working CCS subnetworks. The key findings from our analysis concern: (1) the characteristics of both the telephone call arrival process and the signaling message arrival process, (2) the tail behavior of the call holding time distribution, and (3) the observed performance of the CCSN with respect to a variety of performance and reliability measurements. 1.
956|Tcplib: A Library of TCP Internetwork Traffic Characteristics|This paper describes tcplib, a workload or source library for network simulation. This paper motivates the need for tools like tcplib and discusses how to incorporate it into a network simulator. Tcplib is available by anonymous ftp 1. 1.
957|Growth Trends in Wide-Area TCP Connections|We analyze the growth of a medium-sized research laboratory &#039;s wide-area TCP connections over a period of more than two years. Our data consisted of six month-long traces of all TCP connections made between the site and the rest of the world. We find that smtp, ftp, and X11 traffic all exhibited exponential growth in the number of connections and bytes transferred, at rates significantly greater than that at which the site&#039;s overall computing resources grew; that individual users increasingly affected the site&#039;s traffic profile by making wide-area connections from background scripts; that the proportion of local computers participating in wide-area traffic outpaces the site&#039;s overall growth; that use of the network by individual computers appears to be constant for some protocols  (telnet) and growing exponentially for others (ftp, smtp);  and that wide-area traffic geography is diverse and dynamic. 1 Introduction  To properly design future networks, we need a thorough understanding of...
958|Long-term traffic aspects of the NSFNET|We present the architecture for data collection for the T3 NSFNET backbone service, and difficulties with using the collected statistics for long-term network forecasting of certain traffic aspects. We describe relevant aspects of the T3 backbone architecture, describe the instrumentation for the statistics collection process, and how it differs from that on the T1 backbone. We then present longterm NSFNET data to elucidate long term trends in both the reachability of Internet components via the NSFNET as well as the growing cross-section of traffic. We focus on the difficulties of forecasting and planning for these two traffic aspects in an infrastructure whose protocol architecture and instrumentation for data collection was not designed to support such objectives. I. Introduction  NSFNET, the National Science Foundation Network, is a general purpose packet-switching network supporting access to scientific computing resources, data, and interpersonal electronic communications.  2  Cl...
959|Goodness-of-fit Techniques|Estimating drizzle drop size and precipitation rate using two-colour lidar measurements 	 ABCDEFB
960|Model-Based Clustering, Discriminant Analysis, and Density Estimation|Cluster analysis is the automated search for groups of related observations in a data set. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as \How many clusters are there?&#034;, &#034;Which clustering method should be used?&#034; and \How should outliers be handled?&#034;. We outline a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, mineeld detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology, a...
962|Bayesian Density Estimation and Inference Using Mixtures|We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation, and are exemplified by special cases where data are modelled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models. Keywords: Kernel estimation; Mixtures of Dirichlet processes; Multimodality; Normal mixtures; Posterior sampling; Smoothing parameter estimation * Michael D. Escobar is Assistant Professor, Department of Statistics and Department of Preventive Medicine and Biostatistics, University ...
964|Regularized discriminant analysis|Linear and quadratic discriminant analysis are considered in the small sample high-dimensional setting. Alternatives to the usual maximum likelihood (plug-in) estimates for the covariance matrices are proposed. These alternatives are characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample based estimate of future misclassification risk. Computationally fast implementations are presented, and the efficacy of the approach is examined through simulation studies and application to data. These studies indicate that in many circumstances dramatic gains in classification accuracy can be achieved. Submitted to Journal of the American Statistical Association
966|Discriminant Analysis by Gaussian Mixtures|Fisher-Rao linear discriminant analysis (LDA) is a valuable tool for multigroup classification. LDA is equivalent to maximum likelihood classification assuming Gaussian distributions for each class. In this paper, we fit Gaussian mixtures to each class to facilitate effective classification in non-normal settings, especially when the classes are clustered. Low dimensional views are an important by-product of LDA---our new techniques inherit this feature. We are able to control the within-class spread of the subclass centers relative to the between-class spread. Our technique for fitting these models permits a natural blend with nonparametric versions of LDA.  Keywords: Classification, Pattern Recognition, Clustering, Nonparametric, Penalized. 1 Introduction  In the generic classification or discrimination problem, the outcome of interest  G falls into J unordered classes, which for convenience we denote by the set J =  f1; 2; 3; \Delta \Delta \Delta Jg. We wish to build a rule for pred...
967|Practical Bayesian Density Estimation Using Mixtures Of Normals|this paper, we propose some solutions to these problems. Our goal is to come up with a simple, practical method for estimating the density. This is an interesting problem in its own right, as well as a first step towards solving other inference problems, such as providing more flexible distributions in hierarchical models. To see why the posterior is improper under the usual reference prior, we write the model in the following way. Let Z = (Z 1 ; : : : ; Z n ) and X = (X 1 ; : : : ; X n ). The Z
969|Detecting Features in  Spatial Point Processes with . . .|We consider the problem of detecting features in spatial point processes in the presence of substantial clutter. One example is the detection of mine elds using reconnaissance aircraft images that erroneously identify many objects that are not mines. Another is the detection of seismic faults on the basis of earthquake catalogs: earthquakes tend to be clustered close to the faults, but there are many that are farther away. Our solution uses model-based clustering based on a mixture model for the process, in which features are assumed to generate points according to highly linear multivariate normal densities, and the clutter arises according to a spatial Poisson process. Very nonlinear features are represented by several highly linear multivariate normal densities, giving a piecewise linear representation. The model is estimated in two stages. In the rst stage, hierarchical model-based clustering is used to provide a rst estimate of the features. In the second stage, this clustering is re ned using the EM algorithm. The number of features is found using an approximation to the posterior probability of each number of features. For the minefield
970|MCLUST: Software for Model-based Cluster Analysis|MCLUST is a software package for cluster analysis written in Fortran and interfaced to the S-PLUS commercial software package1. It implements parameterized Gaussian hierarchical clustering algorithms [16, 1, 7] and the EM algorithm for parameterized Gaussian mixture models [5, 13, 3, 14] with the possible addition of a Poisson noise term. MCLUST also includes functions that combine hierarchical clustering, EM and the Bayesian Information Criterion (BIC) in a comprehensive clustering strategy [4, 8]. Methods of this type have shown promise in a number of practical applications, including character recognition [16], tissue segmentation [1], mine eld and seismic fault detection [4], identi cation of textile aws from images [2], and classi cation of astronomical data [3, 15]. Aweb page with related links can be found at
971|Algorithms for model-based Gaussian hierarchical clustering|1 Funded by the O ce of Naval Research under contracts N00014-96-1-0192 and N00014-96-1-
972|Regularized Gaussian Discriminant Analysis Through Eigenvalue Decomposition|Friedman (1989) has proposed a regularization technique (RDA) of discriminant analysis in the Gaussian framework. RDA makes use of two regularization parameters to design an intermediate classi cation rule between linear and quadratic discriminant analysis. In this paper, we propose an alternative approach to design classi cation rules which have also a median position between linear and quadratic discriminant analysis. Our approach is based on the reparametrization of the covariance matrix k of a group Gk in terms of its eigenvalue decomposition, k = kDkAkD 0 k where k speci es the volume of Gk, Ak its shape, and Dk its orientation. Variations on constraints concerning k?Ak and Dk lead to 14 discrimination models of interest. For each model, we derived the maximum likelihood parameter estimates and our approach consists in selecting the model among the 14 possible models by minimizing the sample-based estimate of future misclassi cation risk by cross-validation. Numerical experiments show favorable behavior of this approach as compared to RDA.
973|Scaling EM (Expectation-Maximization) Clustering to Large Databases  (1999) |Practical statistical clustering algorithms typically center upon an iterative refinement optimization procedure to compute a locally optimal clustering solution that maximizes the fit to data. These algorithms typically require many database scans to converge, and within each scan they require the access to every record in the data table. For large databases, the scans become prohibitively expensive. We present a scalable implementation of the Expectation-Maximization (EM) algorithm. The database community has focused on distance-based clustering schemes and methods have been developed to cluster either numerical or categorical data. Unlike distancebased algorithms (such as K-Means), EM constructs proper statistical models of the underlying data source and naturally generalizes to cluster databases containing both discrete-valued and continuous-valued data. The scalable method is based on a decomposition of the basic statistics the algorithm needs: identifying regions of the data that...
974|Averaging, Maximum Penalized Likelihood and Bayesian Estimation for Improving Gaussian Mixture Probability Density Estimates|We apply the idea of averaging ensembles of estimators to probability density estimation.
975|Non Parametric Maximum Likelihood Estimation of Features in . . .|This paper addresses the problem of estimating the support domain of a bounded point process in presence of background noise. This situation occurs for example in the detection of a mine field from aerial observations. A Maximum Likelihood Estimator for a mixture of uniform point processes is derived using a natural partition of the space defined by the data themselves: the Voronoï tessellation. The methodology is tested on simulations and compared to a model-based clustering technique.
976|Inference in model-based cluster analysis|A new approach to cluster analysis has been introduced based on parsimonious geometric modelling of the within-group covariance matrices in a mixture of multivariate normal distributions, using hierarchical agglomeration and iterative relocation. It works well and is widely used via the MCLUST software available in S-PLUS and StatLib. However, it has several limitations: there is no assessment of the uncertainty about the classification, the partition can be suboptimal, parameter estimates are biased, the shape matrix has to be specified by the user, prior group probabilities are assumed to be equal, the method for choosing the number of groups is based on a crude approximation, and no formal way of choosing between the various possible models is included. Here, we propose a new approach which overcomes all these difficulties. It consists of exact Bayesian inference via Gibbs sampling, and the calculation of Bayes factors (for choosing the model and the number of groups) from the output using the Laplace-Metropolis estimator. It works well in several real and simulated examples.  
977|Principal curve clustering with noise|was supported by ONR grants N00014-96-1-0192 and N00014-96-1-0330. The authors are Clustering on principal curves combines parametric modeling of noise with nonparametric modeling of feature shape. This is useful for detecting curvilinear features in spatial point patterns, with or without background noise. Applications of this include the detection of curvilinear mine elds from reconnaissance images, some of the points in which represent false detections, and the detection of seismic faults from earthquake catalogs. Our algorithm for principal curve clustering is in two steps: the rst is hierarchical and agglomerative (HPCC), and the second consists of iterative relocation based on the Classi cation EM algorithm (CEM-PCC). HPCC is used to combine potential feature clusters, while CEM-PCC re nes the results and deals with background noise. It is importanttohave a good starting point for the algorithm: this can be found manually or automatically using, for example, nearest neighbor clutter removal or model-based clustering. We choose the number of features and the amount of smoothing simultaneously using approximate Bayes
978|Fitting mixtures of Kent distributions to aid in joint set identification|When examining a rock mass, joint sets and their orientations can play a significant role with regard to how the rock mass will behave. To identify joint sets present in the rock mass, the orientation of individual fracture planes can be measured on exposed rock faces and the resulting data can be examined for heterogeneity. In this article, the expectation–maximization algorithm is used to fit mixtures of Kent component distributions to the fracture data to aid in the identification of joint sets. An additional uniform component is also included in the model to accommodate the noise present in the data.
979|From Massive Data Sets to Science Catalogs: Applications and Challenges|With hardware advances in scientific instruments and data gathering techniques comes the  inevitable flood of data that can render traditional approaches to science data analysis severely  inadequate. The traditional approach of manual and exhaustive analysis of a data set is no longer  feasible for many tasks ranging from remote sensing, astronomy, and atmospherics to medicine,  molecular biology, and biochemistry. In this paper we present our views as practitioners engaged  in building computational systems to help scientists deal with large data sets. We focus on  what we view as challenges and shortcomings of the current state-of-the-art in data analysis in  view of the massive data sets that are still awaiting analysis. The presentation is grounded in  applications in astronomy, planetary sciences, solar physics, and atmospherics that are currently  driving much of our work at JPL.  keywords: science data analysis, limitations of current methods, challenges for massive data sets, ...
980|Object Detection with Discriminatively Trained Part Based Models |We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL datasets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI-SVM in terms of latent variables. A latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.
981|Distinctive Image Features from Scale-Invariant Keypoints|This paper presents a method for extracting distinctive invariant features from  images, which can be used to perform reliable matching between different images  of an object or scene. The features are invariant to image scale and rotation,  and are shown to provide robust matching across a a substantial range  of affine distortion, addition of noise, change in 3D viewpoint, and change in  illumination. The features are highly distinctive, in the sense that a single feature  can be correctly matched with high probability against a large database of  features from many images. This paper also describes an approach to using  these features for object recognition. The recognition proceeds by matching  individual features to a database of features from known objects using a fast  nearest-neighbor algorithm, followed by a Hough transform to identify clusters  belonging to a single object, and finally performing verification through leastsquares  solution for consistent pose parameters. This approach to recognition  can robustly identify objects among clutter and occlusion while achieving near  real-time performance.
982|Histograms of Oriented Gradients for Human Detection|We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds. 1
983|Making Large-Scale SVM Learning Practical|Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, off-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.  
984|Robust real-time face detection|We have constructed a frontal face detection system which achieves detection and false positive rates which are equivalent to the best published results [7, 5, 6, 4, 1]. This face detection system is most clearly distinguished from previous approaches in its ability to detect faces extremely rapidly. Operating on 384 by 288 pixel images, faces are detected at 15 frames per second on a conventional 700 MHz Intel Pentium III. In other face detection systems, auxiliary information, such as image differences in video sequences, or pixel color in color images, have been used to achieve high frame rates. Our system achieves high frame rates working only with the information present in a single grey scale image. These alternative sources of information can also be integrated with our system to achieve even higher frame rates.
985|Object class recognition by unsupervised scale-invariant learning|We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals). 1.
986|Local features and kernels for classification of texture and object categories: a comprehensive study|Recently, methods based on local image features have shown promise for texture and object recognition tasks. This paper presents a large-scale evaluation of an approach that represents images as distributions (signatures or histograms) of features extracted from a sparse set of keypoint locations and learns a Support Vector Machine classifier with kernels based on two effective measures for comparing distributions, the Earth Mover’s Distance and the ? 2 distance. We first evaluate the performance of our approach with different keypoint detectors and descriptors, as well as different kernels and classifiers. We then conduct a comparative evaluation with several state-of-the-art recognition methods on four texture and five object databases. On most of these databases, our implementation exceeds the best reported results and achieves comparable performance on the rest. Finally, we investigate the influence of background correlations on recognition performance via extensive tests on the PASCAL database, for which ground-truth object localization information is available. Our experiments demonstrate that image representations based on distributions of local features are surprisingly effective for classification of texture and object images under challenging real-world conditions, including significant intra-class variations and substantial background clutter.
987|PCA-SIFT: A more distinctive representation for local image descriptors|Stable local feature detection and representation is a fundamental component of many image registration and object recognition algorithms. Mikolajczyk and Schmid [14] recently evaluated a variety of approaches and identified the SIFT [11] algorithm as being the most resistant to common image deformations. This paper examines (and improves upon) the local image descriptor used by SIFT. Like SIFT, our descriptors encode the salient aspects of the image gradient in the feature point&#039;s neighborhood; however, instead of using SIFT&#039;s smoothed weighted histograms, we apply Principal Components Analysis (PCA) to the normalized gradient patch. Our experiments demonstrate that the PCAbased local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation. We also present results showing that using these descriptors in an image retrieval application results in increased accuracy and faster matching.
988|A discriminatively trained, multiscale, deformable part model|This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. We believe that our training methods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose. 1.
989|Support vector machines for multiple-instance learning|This paper presents two new formulations of multiple-instance learning as a maximum margin problem. The proposed extensions of the Support Vector Machine (SVM) learning approach lead to mixed integer quadratic programs that can be solved heuristically. Our generalization of SVMs makes a state-of-the-art classification technique, including non-linear classification via kernels, available to an area that up to now has been largely dominated by special purpose methods. We present experimental results on a pharmaceutical data set and on applications in automated image indexing and document categorization. 1
990|Putting objects in perspective|Image understanding requires not only individually estimating elements of the visual world but also capturing the interplay among them. In this paper, we provide a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to refine geometry and vice-versa. Our framework allows painless substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our results confirm the benefits of our integrated approach. 1.
991|Contextual Priming for Object Detection|There is general consensus that context can be a rich source of information about an object&#039;s identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.
992|Spatial priors for part-based recognition using statistical models|We present a class of statistical models for part-based object recognition that are explicitly parameterized according to the degree of spatial structure they can represent. These models provide a way of relating different spatial priors that have been used for recognizing generic classes of objects, including joint Gaussian models and tree-structured models. By providing explicit control over the degree of spatial structure, our models make it possible to study the extent to which additional spatial constraints among parts are actually helpful in detection and localization, and to consider the tradeoff in representational power and computational cost. We consider these questions for object classes that have substantial geometric structure, such as airplanes, faces and motorbikes, using datasets employed by other researchers to facilitate evaluation. We find that for these classes of objects, a relatively small amount of spatial structure in the model can provide statistically indistinguishable recognition performance from more powerful models, and at a substantially lower computational cost. 1.
993|Human Face Detection in Visual Scenes|We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image, and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training the networks, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with other state-of-the-art face detection systems are presented; our system has better performance in terms of detection and false-positive rates. This work was partially supported by a grant from Siemens Corporate Research, Inc., by the Department of the Army, Army Research Office under grant number DAAH04-94-G-0006, and by the Office of Naval Research under grant number N00014-95-1-0591. This work was started while Shumeet Balu...
994|Multiple instance boosting for object detection|A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MIL-Boost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classifier. 1
995|Distance transforms of sampled functions|This paper provides linear-time algorithms for solving a class of minimization problems in-volving a cost function with both local and spatial terms. These problems can be viewed as a generalization of classical distance transforms of binary images, where the binary image is replaced by an arbitrary sampled function. Alternatively they can be viewed in terms of the minimum convolution of two functions, which is an important operation in grayscale mor-phology. A useful consequence of our techniques is a simple, fast method for computing the Euclidean distance transform of a binary image. The methods are also applicable to Viterbi decoding, belief propagation and optimal control. 1
996|A probabilistic approach to object recognition using local photometry and global geometry|Abstract. Many object classes, including human faces, can be modeled as a set of characteristic parts arranged in a variable spatial con guration. We introduce a simpli ed model of a deformable object class and derive the optimal detector for this model. However, the optimal detector is not realizable except under special circumstances (independent part positions). A cousin of the optimal detector is developed which uses \soft &#034; part detectors with a probabilistic description of the spatial arrangement of the parts. Spatial arrangements are modeled probabilistically using shape statistics to achieve invariance to translation, rotation, and scaling. Improved recognition performance over methods based on \hard &#034; part detectors is demonstrated for the problem of face detection in cluttered scenes. 1
997|Towards Automatic Discovery of Object Categories|We propose a method to learn heterogeneous models of object classes for visual recognition. The training images contain a preponderance of clutter and learning is unsupervised. Our models represent objects as probabilistic constellations of rigid parts (features). The variability within a class is represented by a joint probability density function on the shape of the constellation and the appearance of the parts. Our method automatically identifies distinctive features in the training set. The set of model parameters is then learned using expectation maximization (see the companion paper [11] for details). When trained on different, unlabeled and unsegmented views of a class of objects, each component of the mixture model can adapt to represent a subset of the views. Similarly, different component
998|Graphical Templates For Model Registration|A new method of model registration is proposed using graphical templates. A graph of landmarks is chosen in the template image. All possible candidates for these landmarks are found in the data image using local operators. A dynamic programming algorithm on decomposable subgraphs of the template graph finds the optimal match to a subset of the candidate points in polynomial time. This combination of local operators to describe points of interest/landmarks and a graph to describe their geometric orientation in the plane, yields fast and precise matches of the model to the data, with no initialization required. Key words: Graphical templates, decomposable graphs, model registration, dynamic programming, image matching.  Research supported in part by The University of Chicago Block Fund, and ARO DAAL0392 -G-0322.  y Research supported in part by National Institutes of Health grant no. R01-GM46800  x1 Introduction In recent years there has been a growing interest in deformable models for ...
999|Pop: Patchwork of parts models for object recognition|We formulate a deformable template model for objects with a clearly defined mechanism for parameter estimation. A separate model is estimated for each class, and classification is likelihood based- no discrmination boundaries are learned. Nonethe-less high classification rates are achieved with small training samples. The data models are defined on binary oriented edge features that are highly robust to photometric vari-ation and small local deformations. The deformation of an object is defined in terms of locations of a moderate number reference points. Each reference point is associated with a part- a probability map assigning a probability for each edge type at each pixel in a window. The likelihood of the edge data on the entire image conditional on the deformation is described as a patchwork of parts (POP) model- the edges are assumed conditionally independent, and the marginal at each pixel is obtained by a patchwork operation: averaging the marginal probabilities contributed by each part covering the pixel. Object classes are modeled as mixtures of POP models that are discovered se-quentially as more class data is observed. Experiments are presented on the MNIST database, hundreds of deformed LATEX shapes, reading zipcodes, and face detection. 1
1000|Efficient Deformable Template Detection and Localization without User Initialization|A novel deformable template is presented which detects the boundary of an open hand in a grayscale image without initialization by the user. A dynamic programming algorithm enhanced by pruning techniques finds the hand contour in the image in as little as 19 seconds on a Pentium 150. The template is translation- and rotation-invariant and accomodates shape deformation, significant occlusion and background clutter, and the presence of multiple hands.  2  Symbols  Boldface letters, e.g. x, denote vectors.  P (xjy) denotes conditional probability of x given y.  p  a denotes the square root of a.  P  denotes summation.  Q  denotes repeated product.  R  denotes integration.  ? denotes &#034;perpendicular to.&#034;  !, ? denote less than and greater than, respectively.  rI(x) denotes the gradient of I with respect to x.  / denotes &#034;proportional to.&#034;   denotes &#034;approximately equal to.&#034;  argmax  x f(x) denotes the value of x that maximizes f(x).  f ? g denotes the convolution of f with g.   denotes the ...
1001|The generalized A* architecture|We consider the problem of computing a lightest derivation of a global structure using a set of weighted rules. A large variety of inference problems in AI can be formulated in this framework. We generalize A * search and heuristics derived from abstractions to a broad class of lightest derivation problems. We also describe a new algorithm that searches for lightest derivations using a hierarchy of abstractions. Our generalization of A * gives a new algorithm for searching AND/OR graphs in a bottom-up fashion. We discuss how the algorithms described here provide a general architecture for addressing the pipeline problem — the problem of passing information back and forth between various stages of processing in a perceptual system. We consider examples in computer vision and natural language processing. We apply the hierarchical search algorithm to the problem of estimating the boundaries of convex objects in grayscale images and compare it to other search methods. A second set of experiments demonstrate the use of a new compositional model for finding salient curves in images. 1.
1002|Training Deformable Models for Localization|We present a new method for training deformable models. Assume that we have training images where part locations have been labeled. Typically, one fits a model by maximizing the likelihood of the part labels. Alternatively, one could fit a model such that, when the model is run on the training images, it finds the parts. We do this by maximizing the conditional likelihood of the training data. We formulate model-learning as parameter estimation in a conditional random field (CRF). Initializing parameters with their maximum likelihood estimates, we reach the global optimum by gradient ascent. We present a learning algorithm that searches exhaustively over all part locations in an image without relying on feature detectors. This provides millions of examples of training data, and seems to avoid over-fitting issues known with CRFs. Results for part localization are relatively scarce in the community. We present results on three established datasets; Caltech motorbikes [8], USC people [19], and Weizmann horses [3]. In the Caltech set we significantly outperform the state-of-the-art [6]. For the challenging people dataset, we present results that are comparable to [19], but are obtained using a significantly more generic model (devoid of a face or skin detector). Our model is general enough to find other articulated objects; we use it to recover poses of horses in the challenging Weizmann database. 1.
1003|Efficient learning of relational object class models|We present an efficient method for learning part-based object class models. The models include part appearance, as well as location and scale relations between parts. The object class is generatively modeled using a simple Bayesian network with a central hidden node containing location and scale information, and nodes describing object parts. The model’s parameters, however, are optimized to reduce a loss function of the training error, as in discriminative methods. We show how boosting techniques can be extended to optimize the relational model proposed, with complexity linear in the number of parts and the number of features per image. This efficiency allows our method to learn relational models with many parts and features. The method has an advantage over purely generative and purely discriminative approaches, since the former are limited to a small number of parts and features, while the latter neglect geometrical relations between parts. Experimental results are described, using some bench-mark data sets and three sets of newly collected data, showing the relative merits of our method in recognition and localization tasks. 1
1004|Part-Based Statistical Models for Object Classification and Detection|We propose using simple mixture models to define a set of mid-level binary local features based on binary oriented edge input. The features capture natural local structures in the data and yield very high classification rates when used with a variety of classifiers trained on small training sets, exhibiting robustness to degradation with clutter. Of particular interest are the use of the features as variables in simple statistical models for the objects thus enabling likelihood based classification. Pre-training decision boundaries between classes, a necessary component of non-parametric techniques, is thus avoided. Class models are trained separately with no need to access data of other classes. Experimental results are presented for handwritten character recognition, classification of deformed L ATEX symbols involving hundreds of classes, and side view car detection. 1.
1005|Model-Driven Data Acquisition in Sensor Networks|Declarative queries are proving to be an attractive paradigm for interacting with networks of wireless sensors. The metaphor that &#034;the sensornet is a database&#034; is problematic, however, because sensors do not exhaustively represent the data in the real world. In order to map the raw sensor readings onto physical reality, a model of that reality is required to complement the readings. In this paper, we enrich interactive sensor querying with statistical modeling techniques. We demonstrate that such models can help provide answers that are both more meaningful, and, by introducing approximations with probabilistic confidences, significantly more efficient to compute in both time and energy. Utilizing the combination of a model and live data acquisition raises the challenging optimization problem of selecting the best sensor readings to acquire, balancing the increase in the confidence of our answer against the communication and data acquisition costs in the network. We describe an exponential time algorithm for finding the optimal solution to this optimization problem, and a polynomial-time heuristic for identifying solutions that perform well in practice. We evaluate our approach on several real-world sensor-network data sets, taking into account the real measured data and communication quality, demonstrating that our model-based approach provides a high-fidelity representation of the real phenomena and leads to significant performance gains versus traditional data acquisition techniques.
1006|Directed Diffusion: A scalable and robust communication paradigm for sensor networks|Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.
1007|The design of an acquisitional query processor for sensor networks|We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. 1.
1008|Query Processing for Sensor Networks|Hardware for sensor nodes that combine physical sensors, actuators, embedded processors, and communication components has advanced significantly over the last decade, and made the large-scale deployment of such sensors a reality. Applications range from monitoring applications such as inventory maintenance over health care to military applications.
1009|Approximate aggregation techniques for sensor databases|In the emerging area of sensor-based systems, a significant challenge is to develop scalable, fault-tolerant methods to extract useful information from the data the sensors collect. An approach to this data management problem is the use of sensor database systems, exemplified by TinyDB and Cougar, which allow users to perform aggregation queries such as MIN, COUNT and AVG on a sensor network. Due to power and range constraints, centralized approaches are generally impractical, so most systems use in-network aggregation to reduce network traffic. Also, aggregation strategies must provide fault-tolerance to address the issues of packet loss and node failures inherent in such a system. An unfortunate consequence of standard methods is that they typically introduce duplicate values, which must be accounted for to compute aggregates correctly. Another consequence of loss in the network is that exact aggregation is not possible in general. With this in mind, we investigate the use of approximate in-network aggregation using small sketches. Our contributions are as follows: 1) we generalize well known duplicateinsensitive sketches for approximating COUNT to handle SUM (and by extension, AVG and other aggregates), 2) we present and analyze methods for using sketches to produce accurate results with low communication and computation overhead (even on low-powered CPUs with little storage and no floating point operations), and 3) we present an extensive experimental validation of our methods. 1
1010|Evaluating Probabilistic Queries over Imprecise Data|Sensors are often employed to monitor continuously changing entities like locations of moving ob-jects and temperature. The sensor readings are reported to a database system, and are subsequently used to answer queries. Due to continuous changes in these values and limited resources (e.g., net-work bandwidth and battery power), the database may not be able to keep track of the actual values of the entities. Queries that use these old values may produce incorrect answers. However, if the degree of uncertainty between the actual data value and the database value is limited, one can place more confidence in the answers to the queries. More generally, query answers can be augmented with probabilistic guarantees of the validity of the answers. In this paper, we study probabilistic query evaluation based on uncertain data. A classification of queries is made based upon the nature of the result set. For each class, we develop algorithms for computing probabilistic answers, and provide efficient indexing and numeric solutions. We address the important issue of measuring the quality of the answers to these queries, and provide algorithms for efficiently pulling data from relevant sensors or moving objects in order to improve the quality of the executing queries. Extensive experiments
1011|Scalable information-driven sensor querying and routing for ad hoc heterogeneous sensor networks|This paper describes two novel techniques, informationdriven sensor querying (IDSQ) and constrained anisotropic diffusion routing (CADR), for energy-efficient data querying and routing in ad hoc sensor networks for a range of collaborative signal processing tasks. The key idea is to introduce an information utility measure to select which sensors to query and to dynamically guide data routing. This allows us to maximize information gain while minimizing detection latency and bandwidth consumption for tasks such as localization and tracking. Our simulation results have demonstrated that the information-driven querying and routing techniques are more energy efficient, have lower detection latency, and provide anytime algorithms to mitigate risks of link/node failures. 1
1012|Distinct sampling for highly-accurate answers to distinct values queries and event reports |Estimating the number of distinct values is a wellstudied problem, due to its frequent occurrence in queries and its importance in selecting good query plans. Previous work has shown powerful negative results on the quality of distinct-values estimates based on sampling (or other techniques that examine only part of the input data). We present an approach, called distinct sampling, that collects a specially tailored sample over the distinct values in the input, in a single scan of the data. In contrast to the previous negative results, our small Distinct Samples are guaranteed to accurately estimate the number of distinct values. The samples can be incrementally maintained up-to-date in the presence of data insertions and deletions, with minimal time and memory overheads, so that the full scan may be performed only once. Moreover, a stored Distinct Sample can be used to accurately estimate the number of distinct values within any range specified by the query, or within any other subset of the data satisfying a query predicate. We present an extensive experimental study of distinct sampling. Using synthetic and real-world data sets, we show that distinct sampling gives distinct-values estimates to within 0%–10 % relative error, whereas previous methods typically incur 50%–250 % relative error. Next, we show how distinct sampling can provide fast, highlyaccurate approximate answers for “report ” queries in high-volume, session-based event recording environments, such as IP networks, customer service call centers, etc. For a commercial call center environment, we show that a 1 % Distinct Sample
1013|Selectivity Estimation using Probabilistic Models|Estimating the result size of complex queries that involve selection on multiple attributes and the join of several relations is a difficult but fundamental task in database query processing. It arises in cost-based query optimization, query profiling, and approximate query answering. In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM from a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations. Furthermore, our approach is not limited to answering a small set of predetermined queries; a single model can be used to effectively estimate the sizes of a wide collection of potential queries across multiple tables. We present results for our approach on several real-world databases. For both single-table multi-attribute queries and a general class of select-join queries, our approach produces more accurate estimates than standard approaches to selectivity estimation, using comparable space and time.
1014|Independence is Good: Dependency-Based Histogram Synopses for High-Dimensional Data|Approximating the joint data distribution of a multi-dimensional data set through a compact and accurate histogram synopsis is a fundamental problem arising in numerous practical scenarios, including query optimization and approximate query answering. Existing solutions either rely on simplistic independence assumptions or try to directly approximate the full joint data distribution over the complete set of attributes. Unfortunately, both approaches are doomed to fail for high-dimensional data sets with complex correlation patterns between attributes. In this paper, we propose a novel approach to histogram-based synopses that employs the solid foundation of statistical interaction models to explicitly identify and exploit the statistical characteristics of the data. Abstractly, our key idea is to break the synopsis into (1) a statistical interaction model that accurately captures significant correlation and independence patterns in data, and (2) a collection of histograms on low-dimensional marginals that, based on the model, can provide accurate approximations of the overall joint data distribution. Extensive experimental results with several real-life data sets verify the effectiveness of our approach. An important aspect of our general, model-based methodology is that it can be used to enhance the performance of other synopsis techniques that are based on data-space partitioning (e.g., wavelets) by providing an effective tool to deal with the “dimensionality curse”. 1.
1016|Monitoring a complex physical system using a hybrid dynamic bayes net|The Reverse Water Gas Shift system (RWGS) is a complex physical system designed to produce oxygen from the carbon dioxide atmosphere on Mars. If sent to Mars, it would operate without human supervision, thus requiring a reliable automated system for monitoring and control. The RWGS presents many challenges typical of real-world systems, including: noisy and biased sensors, nonlinear behavior, effects that are manifested over different time granularities, and unobservability of many important quantities. In this paper we model the RWGS using a hybrid (discrete/continuous) Dynamic Bayesian Network (DBN), where the state at each time slice contains 33 discrete and 184 continuous variables. We show how the system state can be tracked using probabilistic inference over the model. We discuss how to deal with the various challenges presented by the RWGS, providing a suite of techniques that are likely to be useful in a wide range of applications. In particular, we describe a general framework for dealing with nonlinear behavior using numerical integration techniques, extending the successful Unscented Filter. We also show how to use a fixed-point computation to deal with effects that develop at different time scales, specifically rapid changes occurring during slowly changing processes. We test our model using real data collected from the RWGS, demonstrating the feasibility of hybrid DBNs for monitoring complex real-world physical systems. 1
1017|P.: A comparison of bayesian methods for haplotype reconstruction from population genotype data |In this report, we compare and contrast three previously published Bayesian methods for inferring haplotypes from genotype data in a population sample. We review the methods, emphasizing the differences between them in terms of both the models (“priors”) they use and the computational strategies they employ. We introduce a new algorithm that combines the modeling strategy of one method with the computational strategies of another. In comparisons using real and simulated data, this new algorithm outperforms all three existing methods. The new algorithm is included in the software package PHASE, version 2.0, available online
1018|SCOP, Structural Classification of Proteins Database: Applications to Evaluation of the Effectiveness of Sequence Alignment Methods and Statistics of Protein Structural Data|The Structural Classification of Proteins (SCOP) database provides a detailed and comprehensive description of the relationships of all known protein structures. The classification is on hierarchical levels: the first two levels, family and superfamily, describe near and far evolutionary relationships; the third, fold, describes geometrical relationships. The distinction between evolutionary relationships and those that arise from the physics and chemistry of proteins is a feature that is unique to this database, so far. The database can be used as a source of data to calibrate sequence search algorithms and for the generation of population statistics on protein structures. The database and its associated les are freely accessible from a number of WWW sites mirrored from URL http://scop.mrc-lmb. cam.ac.uk/scop/.
1019|Static Scheduling of Synchronous Data Flow Programs for Digital Signal Processing|Large grain data flow (LGDF) programming is natural and convenient for describing digital signal processing (DSP) systems, but its runtime overhead is costly in real time or cost-sensitive applications. In some situations, designers are not willing to squander computing resources for the sake of program-mer convenience. This is particularly true when the target machine is a programmable DSP chip. However, the runtime overhead inherent in most LGDF implementations is not required for most signal processing systems because such systems are mostly synchronous (in the DSP sense). Synchronous data flow (SDF) differs from traditional data flow in that the amount of data produced and consumed by a data flow node is specified a priori for each input and output. This is equivalent to specifying the relative sample rates in signal processing system. This means that the scheduling of SDF nodes need not be done at runtime, but can be done at compile time (statically), so the runtime overhead evaporates. The sample rates can all be different, which is not true of most current data-driven digital signal processing programming methodologies. Synchronous data flow is closely related to computation graphs, a special case of Petri nets. This self-contained paper develops the theory necessary to statically schedule SDF programs on single or multiple proces-sors. A class of static (compile time) scheduling algorithms is proven valid, and specific algorithms are given for scheduling SDF systems onto single or multiple processors. 
1020|Petri Nets|Over the last decade, the Petri net has gamed increased usage and acceptance as a basic model of systems of asynchronous concurrent computation. This paper surveys the basic concepts and uses of Petm nets. The structure of Petri nets, their markings and execution, several examples of Petm net models of computer hardware and software, and
1021|The eyes have it: A task by data type taxonomy for information visualizations| A useful starting point for designing advanced graphical user interjaces is the Visual lnformation-Seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. This paper offers a task by data type taxonomy with seven data types (one-, two-, three-dimensional datu, temporal and multi-dimensional data, and tree and network data) and seven tasks (overview, Zoom, filter, details-on-demand, relate, history, and extracts). 
1022|Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Displays|This paper offers new principles for visual information seeking (VIS). A key concept is to support browsing, which is distinguished from familiar query composition and information retrieval because of its emphasis on rapid filtering to reduce result sets, progressive refinement of search parameters, continuous reformulation of goals, and visual scanning to identify results. VIS principles developed include: dynamic query filters (query parameters are rapidly adjusted with sliders, buttons, maps, etc.), starfield displays (two-dimensional scatterplots to structure result sets and zooming to reduce clutter), and tight coupling (interrelating query components to preserve display invariants and support progressive refinement combined with an emphasis on using search output to foster search input). A FilmFinder prototype using a movie database demonstrates these principles in a VIS environment.
1023|Tree visualization with Tree-maps: A 2-d space-filling approach|this paper deals with a two-dimensional (2-d) space-filling approach in which each node is a rectangle whose area is proportional to some attribute such as node size. Research on relationships between 2-d images and their representation in tree structures has focussed on node and link representations of 2-d images. This work includes quad-trees (Samet, 1989) and their variants which are important in image processing. The goal of quad trees is to provide a tree representation for storage compression and efficient operations on bit-mapped images. XY-trees (Nagy &amp; Seth, 1984) are a traditional tree representation of two-dimensional layouts found in newspaper, magazine, or book pages. Related concepts include k-d trees (Bentley and Freidman, 1979), which are often explained with the help of a
1024|Treemaps: a space-filling approach to the visualization of hierarchical information structures|This paper describes a novel methodfor the visualization of hierarchically structured information. The Tree-Map visualization technique makes 100 % use of the available display space, mapping the full hierarchy onto a rectangular region in a space-filling manner. This efficient use of space allows very large hierarchies to be displayed in their entirety and facilitates the presentation of semantic information. 1
1025|The Table Lens: Merging Graphical and Symbolic Representations in an Interactive Focus+Context Visualization for Tabular Information|We present a new visualization, called the Table Lens, for visualizing and making sense of large tables. The visualization uses a focus context (fisheye) technique that works effectively on tabular information because it allows display of crucial label information and multiple distal focal areas. In addition, a graphical mapping scheme for depicting table contents has been developed for the most widespread kind of tables, the cases-by-variables table. The Table Lens fuses symbolic and graphical representations into a single coherent view that can be fluidly adjusted by the user. This fusion and interactivity enables an extremely rich and natural style of direct manipulation exploratory data analysis.
1026|Dynamic Queries for Information Exploration: An Implementation and Evaluation|We designed, implemented and evaluated a new concept for direct manipulation of databases, called dynamic queries, that allows users to formulate queries with graphical widgets, such as sliders. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. Eighteen undergraduate chemistry students performed statistically significantly faster usingadynamicqueries interface compared to two interfaces both providing form fillin as input method, one with graphical visualization output and one with all-textual output. The interfaces were used to expore the periodic table of elements and search on their properties. 1. INTRODUCTION Mostdatabasesystems require the user to create andformulate a complex query, whichpresumes that the user is familiar with the logical structure of the database [4]. The queries on a database are usually expressed in high level query languages (such as SQL,QUEL). This works well for many applications, but it ...
1027|Visualizing the non-visual: spatial analysis and interaction with information from test documents|This paper describes an approach to IV that involves spatializing text content for enhanced visual browsing and analysis. The application arena is large text document corpora such as digital libraries, regulations andprocedures, archived reports, etc. The basic idea is that text content from these sources may be transformed to a spatial representation that preserves informational characteristics from the documents. The spatial representation may then be visually browsed and analyzed in ways that avoid language processing and that reduce the analysts’ mental workload. The result is an interaction with text that more nearly resembles perception and action with the natural world than with the abstractions of written language. 1
1028|Dynamic Queries for Visual Information Seeking|Dynamic queries are a novel approach to information seeking that may enable users to cope with information overload. They allow users to see an overview of the database, rapidly (100 msec updates) explore and conveniently filter out unwanted information. Users fly through information spaces by incrementally adjusting a query (with sliders, buttons, and other filters) while continuously viewing the changing results. Dynamic queries on the chemical table of elements, computer directories, and a real estate database were built and tested in three separate exploratory experiments. These results show statistically significant performance improvements and user enthusiasm more commonly seen with video games. Widespread application seems possible but research issues remain in database and display algorithms, and user interface design. Challenges include methods for rapidly displaying and changing many points, colors, and areas; multidimensional pointing; incorporation of sound and visual displ...
1029|LifeLines: Visualizing Personal Histories|LifeLines provide a general visualization environment for personal histories that can be applied to medical and court records, professional histories and other types of biographical data. A one screen overview shows multiple facets of the records. Aspects, for example medical conditions or legal cases, are displayed as individual time lines, while icons indicate discrete events, such as physician consultations or legal reviews. Line color and thickness illustrate relationships or significance, rescaling tools and filters allow users to focus on part of the information. LifeLines reduce the chances of missing information, facilitate spotting anomalies and trends, streamline access to details, while remaining tailorable and easily transferable between applications. The paper describes the use of LifeLines for youth records of the Maryland Department of Juvenile Justice and also for medical records. User&#039;s feedback was collected using a Visual Basic prototype for the youth record. Techniq...
1030|Graphical Fisheye Views|A fisheye camera lens is a very wide angle lens that magnifies nearby objects while shrinking distant objects. It is a valuable tool for seeing both &#034;local detail&#034; and &#034;global context&#034; simultaneously. This paper describes a system for viewing and browsing graphs using a software analog of a fisheye lens. We first show how to implement such a view using solely geometric transformations. We then describe a more general transformation that allows global information about the graph to affect the view. Our general transformation is a fundamental extension to previous research in fisheye views.  
1031|The dynamic HomeFinder: evaluating dynamic queries in a real-estate information exploration system|We designed, implemented, and evaluated a new concept for visualizing and searching databases utilizing direct manipulation called dynarruc queries. Dynamic queries allow users to formulate queries by adjusting graphical widgets, such as sliders, and see the results immediately. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. User testing was done with eighteen undergraduate students who performed significantly faster using a dynamic queries interface compared to both a natural language system and paper printouts. The interfaces were used to explore a real-estate database and find homes meeting specific search criteria. 1
1032|InfoCrystal: a visual tool for information retrieval|This paper introduces a novel representation, called the I n f o C r y s t a l T M,  that can be used as a v isual i za t ion tool as well as a visual query lan-g u a g e to help users search for information. The lnfoCrysta1 visualizes all the possible relation-ships among N concepts. Users can assign relevance weights to the concepts and use thresholding to select relationships of interest. The lnfocrystal allows users to specify Boolean as well as v e c t o r-s p a c e queries graphically. Arbitrarily complex queries can be created by using the 1nfoCrystals as building blocks and organizing them in a hierarchical structure. The 1nfoCrystal enables users to explore and filter information in a flexible, dynamic and interactive way.
1033|IVEE: An Information Visualization &amp; Exploration Environment|The Information Visualization and Exploration Environment (IVEE) is a system for automatic creation of dynamic queries applications. IVEE imports database relations and automatically creates environments holding visualizations and query devices. IVEE offers multiple visualizations such as maps and starfields, and multiple query devices, such as sliders, alphasliders, and toggles. Arbitrary graphical objects can be attached to database objects in visualizations. Multiple visualizations may be active simultaneously. Users can interactively lay out and change between types of query devices. Users may retrieve details-on-demand by clicking on visualization objects. An HTML file may be provided along with the database, specifying how details-ondemand information should be presented, allowing for presentation of multimedia information in database objects. Finally, multiple IVEE clients running on separate workstations on a network can communicate by letting one users actions affect the visua...
1034|The Information Mural: A Technique for Displaying and Navigating Large Information Spaces|Abstract—Information visualizations must allow users to browse information spaces and focus quickly on items of interest. Being able to see some representation of the entire information space provides an initial gestalt overview and gives context to support browsing and search tasks. However, the limited number of pixels on the screen constrain the information bandwidth and make it difficult to completely display large information spaces. The Information Mural is a two-dimensional, reduced representation of an entire information space that fits entirely within a display window or screen. The Mural creates a miniature version of the information space using visual attributes, such as gray-scale shading, intensity, color, and pixel size, along with antialiased compression techniques. Information Murals can be used as stand-alone visualizations or in global navigational views. We have built several prototypes to demonstrate the use of Information Murals in visualization applications; subject matter for these views includes computer software, scientific data, text documents, and geographic information. Index Terms—Information visualization, software visualization, data visualization, focus+context, navigation, browsers. 1 INFORMATION MURALS
1035|Visdb: Database exploration using multidimensional visualization|In this paper we describe the VisDB system, which allows an exploration of large databases using visualization techniques. The goal of the system is to support the query specification process by using each pixel of the display to represent one data item of the database. By arranging and coloring the pixels according to the relevance of the data items with respect to the query, the user gets a visual impression of the resulting data set. Using sliders for each condition of the query, the user may change the query dynamically and receives immediate feedback from the visual representation of the resulting data set. Different visualization techniques are available for different stages of exploration. The first technique uses multiple windows for the different query parts, providing visual feedback for each part of the query and helping the user to understand the overall result. The second technique is an extension of the first one, providing additional information by assigning two dimensions to the axes. The third technique uses a grouping of dimensions and is designed to support a focused search on smaller data sets.
1036|The Alphaslider: A Compact and Rapid Selector|Research has suggested that rapid, serial, visual presentation of text (RSVP) may be an effective way to scan and search through lists of text strings in search of words, names, etc. The Alphaslider widget employs RSVP as a method for rapidly scanning and searching lists or menus in a graphical user interface environment. The Alphaslider only uses an area less than 7 cm x 2.5 cm. The tiny size of the Alphaslider allows it to be placed on a credit card, on a control panel for a VCR, or as a widget in a direct manipulation based database interface. An experiment was conducted with four Alphaslider designs which showed that novice Alphaslider users could locate one item in a list of 10,000 film titles in 24 seconds on average, an expert user in about 13 seconds.  KEYWORDS: Alphaslider, widget, selection technology, menus, dynamic queries  INTRODUCTION  Selecting items from lists is a common task in today&#039;s society. New and exciting applications for selection technology are credit card siz...
1037|The Continuous Zoom: A Constrained Fisheye Technique for Viewing and Navigating Large Information Spaces|Navigating and viewing large information spaces, such as hierarchically-organized networks from complex realtime systems, suffer the problems of viewing a large space on a small screen. Distorted-view approaches, such as fisheye techniques, have great potential to reduce these problems by representing detail within its larger context but introduce new issues of focus, transition between views and user disorientation from excessive distortion. We present a fisheyebased method which supports multiple focus points, enhances continuity through smooth transitions between views, and maintains location constraints to reduce the user’s sense of spatial disorientation. These are important requirements for the representation and navigation of networked systems in supervisory control applications. The method consists of two steps: a global allocation of space to rectangular sections of the display, based on scale factors, followed by degree-of-interest adjustments. Previous versions of the algorithm relied solely on relative scale factors to assign size; we present a new version which allocates space more efficiently using a dynamically calculated degree of interest. In addition to the automatic system sizing, manual user control over the amount of space assigned each area is supported. The amount of detail shown in various parts of the network is controlled by pruning the hierarchy and presenting those sections in summary form. KEYWORDS: graphical user interface, supervisory control systems, information space, hierarchical network, information visualization, fisheye view, navigation.
1038|Enhanced Dynamic Queries via Movable Filters|Traditional database query systems allow users to construct complicated database queries from specialized database language primitives. While powerful and expressive, such systems are not easy to use, especially for browsing or exploring the data. Information visualization systems address this problem by providing graphical presentations of the data and direct manipulation tools for exploring the data. Recent work in this area has reported the value of dynamic queries coupled with two-dimensional data representations for progressive refinement of user queries. However, the queries generated by these systems are limited to conjunctions of global ranges of parameter values. In this paper, we extend dynamic queries by encoding each operand of the query as a Magic Lens filter. Compound queries can be constructed by overlapping the lenses. Each lens includes a slider and a set of buttons to control the value of the filter function and to define the compostion operation generated by overlapp...
1039|Navigating Large Networks with Hierarchies|This paper is aimed at the exploratory visualization of networks where there is a strength or weight associated with each link, and makes use of any hierarchy present on the nodes to aid the investigation of large networks. It describes a method of placing nodes on the plane that gives meaning to their relative positions. The paper discusses how linking and interaction principles aid the user in the exploration. Two examples are given; one of electronic mail communication over eight months within a department, another concerned with changes to a large section of a computer program. I. THE PROBLEM It has almost become a clichŽ to start a paper with the observation that the amount of data in the world is growing rapidly, and that current efforts to extract useful information from data lag far behind the ability to create data. However the clichŽ is true, and no less so in the field of network analysis and visualization than in any other. In many areas, scientists are realizing that the tools they have been using are limited in utility when applied to large, information-rich networks. Not only are networks of interest large in terms of size (as measured by number of nodes or links between nodes), but also in terms of the data collected for each node or link. The ability to examine statistics on the nodes and relate them to the network is of crucial importance. Examples of areas in which the analysis of large networks is important include: i. Trade flows. The concern in this area is monitoring imports and exports of various products at several levels; international, interstate and local. Besides examining many types of trade goods, there is also strong interest in spotting temporal patterns. ii. Communication networks. This is an important and wide category, covering not only telecommunication networks, but also electronic mail (email), financial transaction, ATM/bank data transferal and other data distribution networks.
1040|Using Aggregation and Dynamic Queries for Exploring Large Data Sets|When working with large data sets, users perform three primary types of activities: data manipulation, data analysis, and data visualization. The data manipulation process involves the selection and transformation of data prior to viewing. This paper addresses user goals for this process and the interactive interface mechanisms that support them. We consider three classes of data manipulation goals: controlling the scope (selecting the desired portion of the data), selecting the focus of attention (concentrating on the attributes of data that are relevant to current analysis), and choosing the level of detail (creating and decomposing aggregates of data). We use this classification to evaluate the functionality of existing data exploration interface techniques. Based on these results, we have expanded an interface mechanism called the Aggregate Manipulator (AM) and combined it with Dynamic Query (DQ) to provide complete coverage of the data manipulation goals. We use real estate sales data to demonstrate how the AM and DQ synergistically function in our interface.
1041|Interacting with Huge Hierarchies: Beyond Cone Trees|This paper describes an implementation of a tool for visualizing and interacting with huge information hierarchies. Existing systems for visualizing huge hierarchies using cone trees &#034;break down&#034; once the hierarchy to be displayed exceeds roughly 1000 nodes, due to increasing visual clutter. This paper describes a system called fsviz which visualizes arbitrarily large hierarchies while retaining user control. This is accomplished by augmenting cone trees with several graphical and interaction techniques: usage-based filtering, animated zooming, handcoupled rotation, fish-eye zooming, coalescing of nodes, texturing, effective use of colour for depth cueing, and the applications of dynamic queries. The fsviz system also improves upon earlier cone tree visualization systems through a more elaborate node layout algorithm. This algorithm enhances the usefulness of cone tree visualization for large hierarchies by all but eliminating clutter.  Keywords: Information Visualization, Information ...
1042|Using treemaps to visualize the Analytic Hierarchy Process|this article. References
1043|Exploratory Access to Geographic Data Based on the Map-Overlay Metaphor|Many geographic information systems (GISs) attempt to imitate the manual process of laying transparent map layers over one another on a light table and analyzing the resulting configurations. While this map-overlay metaphor, familiar to many geo-scientists, has been used as a design principle for the underlying architecture of GISs, it has not yet been visually manifested at the user interface. To overcome this shortage, a new direct manipulation user interface for overlay-based GISs has been designed and prototyped. It is characterized by the separation of map layers into data cubes and map templates such that different thematic data can be combined and the same kind of data can be displayed in different formats. This paper introduces the conceptual objects that the user manipulates at the screen surface and discusses ways to visualize effectively the objects and operations upon them.
1044|Visualizing Network Data|Networks are critical to modern society, and a thorough understanding of how they behave is crucial to their efficient operation. Fortunately, data on networks is plentiful; by visualizing this data, it is possible to greatly improve our understanding. Our focus is on visualizing the data associated with a network and not on simply visualizing the structure of the network itself. We begin with three static network displays; two of these use geographical relationships, while the third is a matrix arrangement that gives equal emphasis to all network links. Static displays can be swamped with large amounts of data; hence we introduce directmanipulation techniques that permit the graphs to continue to reveal relationships in the context of much more data. In effect, the static displays are parameterized so that interesting views may easily be discovered interactively. The software to carry out this network visualization is called SeeNet.  1. INTRODUCTION  We are currently in the midst of a...
1045|Stochastic volatility: likelihood inference and comparison with ARCH models|In this paper, Markov chain Monte Carlo sampling methods are exploited to provide a unified, practical likelihood-based framework for the analysis of stochastic volatility models. A highly effective method is developed that samples all the unobserved volatilities at once using an approximating offset mixture model, followed by an importance reweighting procedure. This approach is compared with several alternative methods using real data. The paper also develops simulation-based methods for filtering, likelihood evaluation and model failure diagnostics. The issue of model choice using non-nested likelihood ratios and Bayes factors is also investigated. These methods are used to compare the fit of stochastic volatility and GARCH models. All the procedures are illustrated in detail. 1.
1047|Markov chains for exploring posterior distributions|Your use of the JSTOR archive indicates your acceptance of JSTOR&#039;s Terms and Conditions of Use, available at
1050|Contour Tracking By Stochastic Propagation of Conditional Density|.  In Proc. European Conf. Computer Vision, 1996, pp. 343--356, Cambridge, UK  The problem of tracking curves in dense visual clutter is  a challenging one. Trackers based on Kalman filters are of limited use;  because they are based on Gaussian densities which are unimodal, they  cannot represent simultaneous alternative hypotheses. Extensions to the  Kalman filter to handle multiple data associations work satisfactorily in  the simple case of point targets, but do not extend naturally to continuous  curves. A new, stochastic algorithm is proposed here, the Condensation   algorithm --- Conditional Density Propagation over time. It  uses `factored sampling&#039;, a method previously applied to interpretation  of static images, in which the distribution of possible interpretations is  represented by a randomly generated set of representatives. The Condensation   algorithm combines factored sampling with learned dynamical  models to propagate an entire probability distribution for object  pos...
1051|Bayesian Analysis of Stochastic Volatility Models|this article is to develop new methods for inference and prediction in a simple class of stochastic volatility models in which logarithm of conditional volatility follows an autoregressive (AR) times series model. Unlike the autoregressive conditional heteroscedasticity (ARCH) and gener- alized ARCH (GARCH) models [see Bollerslev, Chou, and Kroner (1992) for a survey of ARCH modeling], both the mean and log-volatility equations have separate error terms. The ease of evaluating the ARCH likelihood function and the ability of the ARCH specification to accommodate the timevarying volatility found in many economic time series has fostered an explosion in the use of ARCH models. On the other hand, the likelihood function for stochastic volatility models is difficult to evaluate, and hence these models have had limited empirical application
1054|Simulated Moments Estimator of Markov Models of Asset Prices|This paper provides a simulated moments estimator (SME) of the parameters of dynamic models in which the state vector follows a time-homogeneous Markov process. Conditions are provided for both weak and strong consistency as well as asymptotic normality. Various tradeoffs among the regularity conditions underlying the large sample properties of the SME are discussed in the context of an asset-pricing model.
1055|Stock Prices and Volume|We undertake a comprehensive investigation of price and volume co-movement using daily New York Stock Exchange data from 1928 to 1987. We adjust the data to take into account well-known calendar effects and long-run trends. To describt tbe process, we use a seminonparametric estimate of the joint density of current price change and volume conditional on past price changes and volume. Four empirical regularities are found: 1) positive correlation between conditional volatility and volume, 2) large price movements are followed by high volume, 3) conditioning on lagged volume substantially attenuates the &#034;leverage &#034; effect, and 4) after conditioning on lagged volume, there is a positive risk/return relation.  
1056|An exact likelihood analysis of the multinomial probit model|We develop new methods for conducting a finite sample, likelihood-based analysis of the multinomial probit model. Using a variant of the Gibbs sampler, an algorithm is developed to draw from the exact posterior of the multinomial probit model with correlated errors. This approach avoids direct evaluation of the likelihood and, thus, avoids the problems associated with calculating choice probabilities which affect both the standard likelihood and method of simulated moments approaches. Both simulated and actual consumer panel data are used to fit six-dimensional choice models. We also develop methods for analyzing random coefficient and multiperiod probit models.
1057|Markov Chain Monte Carlo Simulation Methods in Econometrics|We present several Markov chain Monte Carlo simulation methods that have been widely used in recent years in econometrics and statistics. Among these is the Gibbs sampler, which has been of particular interest to econometricians. Although the paper summarizes some of the relevant theoretical literature, its emphasis is on the presentation and explanation of applications to important models that are studied in econometrics. We include a discussion of some implementation issues, the use of the methods in connection with the EM algorithm, and how the methods can be helpful in model specification questions. Many of the applications of these methods are of particular interest to Bayesians, but we also point out ways in which frequentist statisticians may find the techniques useful.
1058|inference via Gibbs sampling of autoregressive time series subject to Markov mean and variance shifts|We examine autoregressive time series models that are subject to regime switching. These shifts are determined by the outcome of an unobserved two-state indicator variable that follows a Markov process with unknown transition probabilities. A Bayesian framework is developed in which the unobserved states, one for each time point, are treated as missing data and then analyzed via the simulation tool of Gibbs sampling. This method is expedient because the conditional posterior distribution f the parameters, given the states, and the conditional posterior distribution of the states, given the parameters, all have a form amenable to Monte Carlo sampling. The approach is straightforward and generates marginal posterior distributions for all parameters of interest. Posterior distributions of the states, future observations, and the residuals, averaged over the parameter space are also obtained. Several examples with real and artificial data sets and weak prior information illustrate the usefulness of the methodology.
1059|Estimation of Stochastic Volatility Models with Diagnostics|Efficient Method of Moments (EMM) is used to fit the standard stochastic volatility model and various extensions to several daily financial time series. EMM matches to the score of a model determined by data analysis called the score generator. Discrepancies reveal characteristics of data that stochastic volatility models cannot approximate. The two score generators employed here are &#034;Semiparametric ARCH&#034; and &#034;Nonlinear Nonparametric&#034;. With the first, the standard model is rejected, although some extensions are accepted. With the second, all versions are rejected. The extensions required for an adequate fit are so elaborate that nonparametric specifications are probably more convenient.   Corresponding author: George Tauchen, Duke University, Department of Economics, Social Science Building, Box 90097, Durham NC 27708-0097 USA, phone 1-919-660-1812, FAX 1-919-684-8974, e-mail get@tauchen.econ.duke.edu. 0  1 Introduction  The stochastic volatility model has been proposed as a descripti...
1060|Bayes inference in the Tobit censored regression model|We consider the Bayes estimation of the Tobit censored regression model with normally distributed errors. A simple condition for the existence of posterior moments is provided. Suitable versions of Monte Carlo procedures based on symmetric multivariate-t distributions, and Laplacian approximations in a certain parametrization, are developed and illustrated. Ideas involving data augmentation and Gibbs sampling [cf. Tanner and Wong (1987) and Gelfand and Smith (1990)] are also developed. The methods are compared in two examples with diffuse priors, and various combinations of sample sizes and degrees of censoring. 
1061|Bayesian comparison of econometric models|This paper integrates and extends some recent computational advances in Bayesian inference with the objective of more fully realizing the Bayesian promise of coherent inference and model comparison in economics. It combines Markov chain Monte Carlo and independence Monte Carlo with importance sampling to provide an efficient and generic method for updating posterior distributions. It exploits the multiplicative decomposition of marginalized likelihood into predictive factors, to compute posterior odds ratios efficiently and with minimal further investment in software. It argues for the use of predictive odds ratios in model comparison in economics. Finally, it suggests procedures for public reporting that will enable remote clients to conveniently modify priors, form posterior expectations of their own functions of interest, and update the posterior distribution with new observations. A series of examples explores the practicality and efficiency of these methods. 
1062|Bayes Regression with Autoregressive Errors: A Gibbs Sampling Approach|This paper develops a practical framework for the Bayesian analysis of Gaussian and Student-c regression models with autocorrelated errors. As is customary in classical estimation procedures, the posteriors are conditioned on the initial observations. Recourse is taken to the method of Gibbs sampling, an iterative Markovian sampling method, and it is shown that the proposed approach can readily deal with high-order autoregressive processes without requiring an importance sampling function or other tuning constants. Several examples, including one with AR(4) errors, are used to illustrate the ideas. 1.
1063|Dynamic Itemset Counting and Implication Rules for Market Basket Data|We consider the problem of analyzing market-basket data and present several important contributions. First, we present a new algorithm for finding large itemsets which uses fewer passes over the data than classic algorithms, and yet uses fewer candidate itemsets than methods based on sampling. We investigate the idea of item reordering, which can improve the low-level efficiency of the algorithm. Second, we present a new way of generating &#034;implication rules,&#034; which are normalized based on both the antecedent and the consequent and are truly implications (not simply a measure of co-occurrence), and we show how they produce more intuitive results than other methods. Finally, we show how different characteristics of real data, as opposed to synthetic data, can dramatically affect the performance of the system and the form of the results.  1 Introduction  Within the area of data mining, the problem of deriving associations from data has recently received a great deal of attention. The prob...
1064|Mining Association Rules between Sets of Items in Large Databases|We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm. 
1065|Mining Sequential Patterns|We are given a large database of customer transactions, where each transaction consists of customer-id, transaction time, and the items bought in the transaction. We introduce the problem of mining sequential patterns over such databases. We present three algorithms to solve this problem, and empirically evaluate their performance using synthetic data. Two of the proposed algorithms, AprioriSome and AprioriAll, have comparable performance, albeit AprioriSome performs a little better when the minimum number of customers that must support a sequential pattern is low. Scale-up experiments show that both AprioriSome and AprioriAll scale linearly with the number of customer transactions. They also have excellent scale-up properties with respect to the number of transactions per customer and the number of items in a transaction.  
1066|Sampling Large Databases for Association Rules|Discovery of association rules is an important database mining problem. Current algorithms for nding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very signi cant for very large databases. We present new algorithms that reduce the database activity considerably. Theidea is to pick a random sample, to ndusingthis sample all association rules that probably hold in the whole database, and then to verify the results with the restofthe database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and inthose rare cases where our sampling method does not produce all association rules, the missing rules can be found inasecond pass. Our experiments show that the proposed algorithms can nd association rules very e ciently in only onedatabase pass. 1
1067|Database Mining: A Performance Perspective|We present our perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology. We describe three classes of database mining problems involving classification, associations, and sequences, and argue that these problems can be uniformly viewed as requiring discovery of rules embedded in massive data. We describe a model and some basic operations for the process of rule discovery. We show how the database mining problems we consider map to this model and how they can be solved by using the basic operations we propose. We give an example of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm not only is efficient in discovering classification rules but also has accuracy comparable to ID3, one of the current best classifiers.  Index Terms. database mining, knowledge discovery, classification, associations, sequences, decision trees   Current address: Computer Science De...
1068|Fast Similarity Search in the Presence of Noise, Scaling, and Translation in Time-Series Databases|We introduce a new model of similarity of time sequences that captures the intuitive notion that two sequences should be considered similar if they have enough non-overlapping time-ordered pairs of subsequences thar are similar. The model allows the amplitude of one of the two sequences to be scaled by any suitable amount and its offset adjusted appropriately. Two subsequences are considered similar if one can be enclosed within an envelope of a specified width drawn around the other. The model also allows non-matching gaps in the matching subsequences. The matching subsequences need not be aligned along the time axis. Given this model of similarity,we present fast search techniques for discovering all similar sequences in a set of sequences. These techniques can also be used to find all (sub)sequences similar to a given sequence. We applied this matching system to the U.S. mutual funds data and discovered interesting matches.
1069|Hierarchical Models of Object Recognition in Cortex|The classical model of visual processing in cortex is a hierarchy of increasingly sophisticated  representations, extending in a natural way the model of simple to complex cells of Hubel and Wiesel.  Somewhat surprisingly, little quantitative modeling has been done in the last 15 years to explore the  biological feasibility of this class of models to explain higher level visual processing, such as object  recognition. We describe a new hierarchical model that accounts well for this complex visual task, is  consistent with several recent physiological experiments in inferotemporal cortex and makes testable  predictions. The model is based on a novel MAX-like operation on the inputs to certain cortical neurons  which may have a general role in cortical function.  
1070|Neural Network-Based Face Detection| We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates. 
1071|Competitive mechanisms subserve attention in macaque areas V2 and V4|It is well established that attention modulates visual processing in extrastriate cortex. However, the underlying neural mechanisms are unknown. A consistent observation is that attention has its greatest impact on neuronal responses when multiple stimuli appear together within a cell’s receptive field. One way to explain this is to assume that multiple stimuli activate competing populations of neurons and that attention biases this competition in favor of the attended stimulus. In the absence of competing stimuli, there is no competition to be resolved. Accordingly, attention has a more limited effect on the neuronal response to a single stimulus. To test this interpretation, we measured the responses of neurons in macaque areas V2 and V4 using a behavioral paradigm that allowed us to isolate automatic sensory processing mechanisms from attentional effects. First, we measured each cell’s response to a single
1072|Learning Invariance From Transformation Sequences|Introduction  How can we consistently recognize objects when changes in the viewing angle, eye position, distance, size, orientation, relative position, or deformations of the object itself (e.g., of a newspaper or a gymnast) can change their retinal projections so significantly? The visual system must contain knowledge about such transformations in order to be able to generalize correctly. Part of this knowledge is probably determined genetically, but it is also likely that the visual system learns from its sensory experience, which contains plenty of examples of such transformations. Electrophysiological experiments suggest that the invariance properties of perception may be due to the receptive field characteristics of individual cells in the visual system. Complex cells in the primary visual cortex exhibit approximate invariance to position within a limited range (Hubel and Wiesel 1962), while cells in higher visual areas in the temporal cortex show more complex forms of invariance
1074|Nonlinear Neural Networks: Principles, Mechanisms, and Architectures| An historical discussion is provided of the intellectual trends that caused nineteenth century interdisciplinary studies of physics and psychobiology by leading scientists such as Helmholtz, Maxwell, and Mach to splinter into separate twentieth-century scientific movements. The nonlinear, nonstationary, and nonlocal nature of behavioral and brain data are emphasized. Three sources of contemporary neural network research-the binary, linear, and continuous-nonlinear models-are noted. The remainder of the article describes results about continuous-nonlinear models: Many models of content-addressable memory are shown to be special cases of the Cohen-Grossberg model and global Liapunov function, including the additive, brain-state-in-a-box, McCulloch-Pitts, Boltzmann machine, Hartline-Ratliff-Millet; shunting, maskingfield, bidirectional associative memory, Volterra-Lotka, Gilpin-Ayala, and Eigen-Schuster models. A Liapunov functional method is described for proving global limit or oscillation theorems for nonlinear competitive systems when their decision schemes are globally consistent or inconsistent, respectively. The former case is illustrated by a model of a globally stable economic market, and the latter case is illustrated by a model of the voting paradox. Key properties of shunting competitive feedback networks are summarized, including the role of sigmoid signalling, automatic gain control, competitive choice and quantization, tunable filtering, total activity normalization, and noise suppression in pattern transformation and memory storage applications. Connections to models of competitive learning, vector quantization, and categorical perception are noted. Adaptive resonance
1075|Visual properties of neurons in a polysensory area in superior temporal sulcus of the macaque|dorsal bank and fundus of the anterior por-tion of the superior temporal sulcus, an area we term the superior temporal polysensory area (STP). Five macaques were studied under anesthesia ( N20) and immobilization in repeated recording sessions. 2. Almost all of the neurons were visually responsive, and over half responded to more than one sensory modality; 21 % responded to visual and auditory stimuli, 17 % re-sponded to visual and somesthetic stimuli, 17 % were trimodal, and 41 % were exclu-sively visual. 3. Almost all the visual receptive fields extended into both visual half-fields, and the majority approached the size of the visual field of the monkey, including both monoc-ular crescents. Somesthetic receptive fields were also bilateral and usually included most of the body surface. 4. Virtually all neurons responded better to moving visual stimuli than to stationary visual stimuli, and almost half were sensitive to the direction of movement. Several classes of directional neurons were found, including a) neurons selective for a single direction of movement throughout their receptive field, b) neurons selective for directions of move-ment radially symmetric about the center of gaze, and c) neurons selective for movement in depth. 5. The majority of neurons (70%) had lit-tle or no preference for stimulus size, shape, orientation, or contrast. The minority (30%) responded best to particular stimuli. Some of these appeared to be selective for faces. 6. The properties of most STP neurons, such as large receptive fields, sensitivity to movement, insensitivity to form, and poly-modal responsiveness, suggest that STP is more involved in orientation and spatial functions than in pattern recognition.
1076|SEEMORE: Combining Color, Shape, and Texture Histogramming in a Neurally Inspired Approach to Visual Object Recognition|this article.
1077|Are cortical models really bound by the “Binding Problem|Address correspondence to T.P. The usual description of visual processing in cortex is an extension of the simple to complex hi-erarchy postulated by Hubel and Wiesel — a feedforward sequence of more and more complex and invariant features. The capability of this class of models to perform higher level visual processing such as viewpoint-invariant object recognition in cluttered scenes has been questioned in recent years by several researchers, who in turn proposed an alternative class of models based on the synchro-nization of large assemblies of cells, within and across cortical areas. The main implicit argument for this novel and controversial view was the assumption that hierarchical models cannot deal with the computational requirements of high level vision and suffer from the so-called “binding problem”. We review the present situation and discuss theoretical and experimental evidence showing that the perceived weaknesses of hierarchical models are not true. In particular, we show that recognition of multiple objects in cluttered scenes, arguably among the most difficult tasks in vision, can be done in a hierarchical feedforward model. 1
1078|A Model of Invariant Object Recognition in the Visual System|Neurons in the ventral stream of the primate visual system exhibit responses to the images of objects which are invariant with respect to natural transformations such as translation, size, and view. Anatomical and neurophysiological evidence suggests that this is achieved through a series of hierarchical processing areas. In an attempt to elucidate the manner in which such representations are established, we have constructed a model of cortical visual processing which seeks to parallel many features of this system, specifically the multi-stage hierarchy with its topologically constrained convergent connectivity. Each stage is constructed as a competitive network utilising a modified Hebb-like learning rule, called the trace rule, which incorporates previous as well as current neuronal activity. The trace rule enables neurons to learn about whatever is invariant over short time periods (e.g. 0.5 s) in the representation of objects as the objects transform in the real world. The trace ru...
1079|Neural models for part-whole hierarchies|We present a connectionist method for representing images that explicitly addresses their hierarchical nature. It blends data from neuroscience about whole-object viewpoint sensitive cells in inferotem-poral cortex8 and attentional basis- eld modulation in V43 with 5, 11 ideas about hierarchical descriptions based on microfeatures. The resulting model makes critical use of bottom-up and top-down pathways for analysis and synthesis. 6 We illustrate the model with a simple example of representing information about faces. 1 Hierarchical Models Images of objects constitute an important paradigm case of a representational hierarchy, inwhich `wholes&#039;, such as faces, consist of `parts&#039;, such aseyes, noses and mouths. The representation and manipulation of part-whole hierarchical information in xed hardwareisaheavy millstone around connectionist necks, and has
1080|Just One View: Invariances in Inferotemporal Cell Tuning|In macaque inferotemporal cortex (IT), neurons have been found to respond selectively to complex shapes while showing broad tuning (&#034;invariance &#034;) with respect to stimulus transformations such as translation and scale changes and a limited tuning to rotation in depth. Training monkeys with novel, paperclip-like objects, Logothetis et al.  9  could investigate whether these invariance properties are due to experience with exhaustively many transformed instances of an object or if there are mechanisms that allow the cells to show response invariance also to previously unseen instances of that object. They found object-selective cells in anterior IT which exhibited limited invariance to various transformations after training with single object views. While previous models accounted for the tuning of the cells for rotations in depth and for their selectivity to a specific object relative to a population of distractor objects,  14,1  the model described here attempts to explain in a biologi...
1081|Approximating discrete probability distributions with dependence trees|A method is presented to approximate optimally an n-dimensional discrete probability distribution by a product of second-order distributions, or the distribution of the first-order tree dependence. The problem is to find an optimum set of n-1 first order dependence relationship among the n variables. It is shown that the procedure derived in this paper yields an approximation of a minimum difference in information. It is further shown that when this procedure is applied to empirical observations from an unknown distribution of tree dependence, the procedure is the maximum-likelihood estimate of the distribution.
1082|Fusion, Propagation, and Structuring in Belief Networks|Belief networks are directed acyclic graphs in which the nodes represent propositions (or variables), the arcs signify direct dependencies between the linked propositions, and the strengths of these dependencies are quantified by conditional probabilities. A network of this sort can be used to represent the generic knowledge of a domain expert, and it turns into a computational architecture if the links are used not merely for storing factual knowledge but also for directing and activating the data flow in the computations which manipulate this knowledge. The first part of the paper deals with the task of fusing and propagating the impacts of new information through the networks in such a way that, when equilibrium is reached, each proposition will be assigned a measure of belief consistent with the axioms of probability theory. It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network. The second part of the paper deals with the problem of finding a tree-structured representation for a collection of probabilistically coupled propositions using auxiliary (dummy) variables, colloquially called &#034;hidden causes. &#034; It is shown that if such a tree-structured representation exists, then it is possible to uniquely uncover the topology of the tree by observing pairwise dependencies among the available propositions (i.e., the leaves of the tree). The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to n log n, where n is the number of leaves.  
1083|Connectionist Learning Procedures|A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks. 
1085|An algorithm for fast recovery of sparse causal graphs|An algorithm for fast recovery of sparse causal graphs
1086|Decision Theory in Expert Systems and Artificial Intelligence|Despite their different perspectives, artificial intelligence (AI) and the disciplines of decision science have common roots and strive for similar goals. This paper surveys the potential for addressing problems in representation, inference, knowledge engineering, and explanation within the decision-theoretic framework. Recent analyses of the restrictions of several traditional AI reasoning techniques, coupled with the development of more tractable and expressive decisiontheoretic representation and inference strategies, have stimulated renewed interest in decision theory and decision analysis. We describe early experience with simple probabilistic schemes for automated reasoning, review the dominant expert-system paradigm, and survey some recent research at the crossroads of AI and decision science. In particular, we present the belief network and influence diagram representations. Finally, we discuss issues that have not been studied in detail within the expert-systems sett...
1087|Constructor: A system for the induction of probabilistic models|The probabilistic network technology is a knowledgebased technique which focuses on reasoning under uncertainty. Because of its well defined semantics and solid theoretical foundations, the technology is finding increasing application in fields such as medical diagnosis, machine vision, military situation assessment, petroleum exploration, and information retrieval. However, like other knowledge-based techniques, acquiring the qualitative and quantitative information needed to build these networks can be highly labor-intensive. CONSTRUCTQR integrates techniques and concepts from probabilistic networks, artificial intelligence, and statistics in order to induce Markov networks (i.e., undirected probabilistic networks). The resulting networks are useful both qualitatively for concept organization and quantitatively for the assessment of new data. The primary goal of CONSTRUCTOR is to find qualitative structure from data. CONSTRUCTOR finds structure by first, modeling each feature in a data set as a node in a Markov network and secondly, by finding the neighbors of each node in the network. In Markov networks, the neighbors of a node have the property of being the smallest set of nodes which “shield ” the node from being affected by other nodes in the graph. This property is used in a heuristic search to identify each node’s neighbors. The traditional x2 test for independence is used to test if a set of nodes “shield ” another node. Cross-validation is used to estimate the quality of alternative structures.
1088|Advances in probabilistic reasoning|This paper discuses multiple Bayesian networks representation paradigms for encoding asymmetric independence assertions. We offer three contributions: (1) an inference mechanism that makes explicit use of asymmetric independence to speed up computations, (2) a simplified definition of similarity networks and extensions of their theory, and (3) a generalized representation scheme that encodes more types of asymmetric independence assertions than do similarity networks. 1
1089|Update on the Pathfinder project|This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to pubs-permissions@ieee.org. By choosing to view this document, you agree to all provisions of the copyright laws protecting it. Update on the Pathfinder Project *
1090|Latent variables, causal models and overidentifying constraints|When is a statistical dependency between two variables best explained by the supposition that one of these variables causes the other, as opposed to the supposition that there is a (possibly unmeasured) common cause acting on both variables? In this paper, we describe an approach towards model specification developed more fully in our book Discovering Cuud Structure, and illustrate its application to the aforementioned question. Briefly, the approach is to determine constraints satisfied by the variance-covariance matrix of a sample, and then to conduct a quasi-automated search for the causal specifications that will best explain those constraints, 1.
1091|Causal Structure Among Measured Variables Preserved with Unmeasured Variables|Causal structure among measured variables preserved with unmeasured variables
1092|Transactional Memory: Architectural Support for Lock-Free Data Structures |A shared data structure is lock-free if its operations do not require mutual exclusion. If one process is interrupted in the middle of an operation, other processes will not be prevented from operating on that object. In highly concurrent systems, lock-free data structures avoid common problems associated with conventional locking techniques, including priority inversion, convoying, and difficulty of avoiding deadlock. This paper introduces transactional memory, a new multiprocessor architecture intended to make lock-free synchronization as efficient (and easy to use) as conventional techniques based on mutual exclusion. Transactional memory allows programmers to define customized read-modify-write operations that apply to multiple, independently-chosen words of memory. It is implemented by straightforward extensions to any multiprocessor cache-coherence protocol. Simulation results show that transactional memory matches or outperforms the best known locking techniques for simple benchmarks, even in the absence of priority inversion, convoying, and deadlock.  
1094|Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors|Scalable shared-memory multiprocessors distribute memory among the processors and use scalable interconnection networks to provide high bandwidth and low latency communication. In addition, memory accesses are cached, buffered, and pipelined to bridge the gap between the slow shared memory and the fast processors. Unless carefully controlled, such architectural optimizations can cause memory accesses to be executed in an order different from what the programmer expects. The set of allowable memory access orderings forms the memory consistency model or event ordering model for an architecture.
1095|Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors|Busy-wait techniques are heavily used for mutual exclusion and barrier synchronization in shared-memory parallel programs. Unfortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnect contention, introducing performance bottlenecks that become markedly more pronounced as applications scale. We argue that this problem is not fundamental, and that one can in fact construct busy-wait synchronization algorithms that induce no memory or interconnect contention. The key to these algorithms is for every processor to spin on separate locally-accessible ag variables, and for some other processor to terminate the spin with a single remote write operation at an appropriate time. Flag variables may be locally-accessible as a result of coherent caching, or by virtue of allocation in the local portion of physically distributed shared memory. We present a new scalable algorithm for spin locks that generates O(1) remote references per lock acquisition, independent of the number of processors attempting to acquire the lock. Our algorithm provides reasonable latency in the absence of contention, requires only a constant amount of space per lock, and requires no hardware support other than
1096|Distributed packet switching for local computer networks|Abstract: Ethernet is a branching broadcast communication system for carrying digital data packets among locally distributed computing stations. The packet transport mechanism provided by Ethernet has been used to build systems which can be viewed as either local computer networks or loosely coupled multiprocessors. An Ethernet&#039;s shared communication facility, its Ether, is a passive broadcast medium with no central control. Coordination of access to the Ether for packet broadcasts is distributed among the contending transmitting stations using controlled statistical arbitration. Switching of packets to their destinations on the Ether is distributed among the receiving stations using packet address recognition. Design principles and implementation are described based on experience.with an operating Ethernet of1 00 nodes along a kilometer of coaxial cable. A model for estimating performance under heavy loads and a packet protocol for error-controlled communication are included for completeness.
1097|A Methodology for Implementing Highly Concurrent Data Objects| A concurrent object is a data structure shared by concurrent processes. Conventional techniques for implementing concurrent objects typically rely on critical sections: ensuring that only one process at a time can operate on the object. Nevertheless, critical sections are poorly suited for asynchronous systems: if one process is halted or delayed in a critical section, other, nonfaulty processes will be unable to progress. By contrast, a concurrent object implementation is lock free if it always guarantees that some process will complete an operation in a finite number of steps, and it is wait free if it guarantees that each process will complete an operation in a finite number of steps. This paper proposes a new methodology for constructing lock-free and wait-free implementations of concurrent objects. The object’s representation and operations are written as stylized sequential programs, with no explicit synchronization. Each sequential operation is automatically transformed into a lock-free or wait-free operation using novel synchronization and memory management algorithms. These algorithms are presented for a multiple instruction/multiple data (MIMD) architecture in which n processes communicate by applying atomic read, wrzte, load_linked, and store_conditional operations to a shared memory.
1098|Memory access buffering in multiprocessors|In highly-pipelined machines, instructions and data are prefetched and buffered in both the processor and the cache. This is done to reduce the average memory access la-tency and to take advantage of memory interleaving. Lock-up free caches are designed to avoid processor blocking on a cache miss. Write buffers are often included in a pipelined machine to avoid processor waiting on writes. In a shared memory multiprocessor, there are more advantages in buffering memory requests, since each memory access has to traverse the memory- processor interconnection and has to compete with memory requests issued by different processors. Buffering, however, can cause logical problems in multipro-cessors. These problems are aggravated if each processor has a private memory in which shared writable data may be present, such as in a cache-based system or in a system with a distributed global memory. In this paper, we analyze the benefits and problems associated with the buffering of memory requests in shared memory multiprocessors. We show that the logical problem of buffering is directly related to the problem of synchronization. A simple model is presented to evaluate the performance improvement result-ing from buffering. 1.
1099|LimitLESS Directories: A Scalable Cache Coherence Scheme|Caches enhance the performance of multiprocessors by reducing network tra#c and average memory access latency. However, cache-based systems must address the problem of cache coherence. We propose the LimitLESS directory protocol to solve this problem. The LimitLESS scheme uses a combination of hardware and software techniques to realize the performance of a full-map directory with the memory overhead of a limited directory. This protocol is supported by Alewife, a large-scale multiprocessor. We describe the architectural interfaces needed to implement the LimitLESS directory, and evaluate its performance through simulations of the Alewife machine.
1100|Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors |The memory consistency model supported by a multiprocessor architecture determines the amount of buffering and pipelining that may be used to hide or reduce the latency of memory accesses. Several different consistency models have been proposed. These range from sequential consistency on one end, allowing very limited buffering, to release consistency on the other end, allowing extensive buffering and pipelining. The processor consistency and weak consistency models fall in between. The advantage of the less strict models is increased performance potential. The disadvantage is increased hardware complexity and a more complex programming model. To make an informed decision on the above tradeoff requires performance data for the various models. This paper addresses the issue of performance benefits from the above four consistency models. Our results are based on simulation studies done for three applications. The results show that in an environment where processor reads are blocking and writes are buffered, a significant performance increase is achieved from allowing reads to bypass previous writes. Pipelining of writes, which determines the rate at which writes are retired from the write buffer, is of secondary importance. As a result, we show that the sequential consistency model performs poorly relative to all other models, while the processor consistency model provides most of the benefits of the weak and release consistency models. 
1101|The Expandable Split Window Paradigm for Exploiting Fine-Grain Parallelism|We propose a new processing paradigm, called the Expandable Split Window (ESW) paradigm, for exploiting finegrain parallelism. This paradigm considers a window of instructions (possibly having dependencies) as a single unit, and exploits fine-grain parallelism by overlapping the execution of multiple windows. The basic idea is to connect multiple sequential processors, in a decoupled and decentralized manner, to achieve overall multiple issue. This processing paradigm shares a number of properties of the restricted dataflow machines, but was derived from the sequential von Neumann architecture. We also present an implementation of the Expandable Split Window execution model, and preliminary performance results. 1.
1102|A Lock-Free Multiprocessor OS Kernel|Typical shared-memory multiprocessor OS kernels use interlocking, implemented as spinlocks or waiting semaphores. We have implemented a complete multiprocessor OS kernel (including threads, virtual memory, and I/O including a window system and a file system) using only lock-free synchronization methods based on Compare-and-Swap. Lock-free synchronization avoids many serious problems caused by locks: considerable overhead, concurrency bottlenecks, deadlocks, and priority inversion in real-time scheduling. Measured numbers show the low overhead of our implementation, competitive with user-level thread management systems.  Contents  1 Introduction 1 2 Synchronization in OS Kernels 1  2.1 Disabling Interrupts : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 1 2.2 Locking Synchronization Methods : : : : : : : : : : : : : : : : : : : : : : : : : : 2 2.3 Lock-Free Synchronization Methods : : : : : : : : : : : : : : : : : : : : : : : : : 2  3 Lock-Free Quajects 3  3.1 LIFO ...
1103|Detecting Violations of Sequential Consistency|The performance of a multiprocessor is directly affected by the choice of the memory consistency model supported. Several different consistency models have been proposed in the literature. These range from sequential consistency on one end, allowing limited buffering of memory accesses, to release consistency on the other end, allowing extensive buffering and pipelining. While the relaxed models such as release consistency provide potential for higher performance, they present a more complex programming model than sequential consistency. Previous research has addressed this tradeoff by showing that a release consistent architecture provides sequentially consistent executions for programs that are free of data races. However, the burden of guaranteeing that the program is free of data races remains with the programmer or compiler.
1104|Lattice-Based Access Control Models|The objective of this article is to give a tutorial on lattice-based  access control models for computer security. The paper begins with a review  of Denning&#039;s axioms for information flow policies, which provide a theoretical  foundation for these models. The structure of security labels in the military and  government sectors, and the resulting lattice is discussed. This is followed by a  review of the Bell-LaPadula model, which enforces information flow policies by  means of its simple-security and *-properties. It is noted that information flow  through covert channels is beyond the scope of such access controls. Variations  of the Bell-LaPadula model are considered. The paper next discusses the Biba  integrity model, examining its relationship to the Bell-LaPadula model. The  paper then reviews the Chinese Wall policy, which arises in a segment of the  commercial sector. It is shown how this policy can be enforced in a lattice  framework.
1105|Role-Based Access Control|While Mandatory Access Controls (MAC) are appropriate for multilevel secure military applications, Discretionary Access Controls (DAC) are often perceived as meeting the security processing needs of industry and civilian government. This paper argues that reliance on DAC as the principal method of access control is unfounded and inappropriate for many commercial and civilian government organizations. The paper describes a type of non-discretionary access control - role-based access control (RBAC) - that is more central to the secure processing needs of non-military systems then DAC. 1 Introduction  The U.S. government has been involved in developing security technology for computer and communications systems for some time. Although advances have been great, it is generally perceived that the current state of security technology has, to some extent failed to address the needs of all. [1], [2] This is especially true of organizations outside the Department of Defense (DoD). [3] The curre...
1106|The Typed Access Matrix Model|The access matrix model as formalized by Harrison, Ruzzo, and Ullman (HRU) has broad expressive power. Unfortunately, HRU has weak safety properties (i.e., the determination of whether or not a given subject can ever acquire access to a given object). Most security policies of practical interest fall into the undecidable cases of HRU. This is true even for monotonic policies (i.e., where access rights can be deleted only if the deletion is itself reversible). In this paper we define the typed access matrix (TAM) model by introducing strong typing into HRU (i.e., each subject or object is created to be of a particular type which thereafter does not change). We prove that monotonic TAM (MTAM) has strong safety properties similar to Sandhu&#039;s Schematic Protection Model. Safety in MTAM&#039;s decidable case is, however, NP-hard. We develop a model called ternary MTAM which has polynomial safety for its decidable case, and which nevertheless retains the full expressive power of MTAM. There is compelling evidence that the decidable safety cases of ternary MTAM are quite adequate for modeling practial monotonic security policies.
1107|Access Rights Administration in Role-Based Security Systems|This paper examines the concept of role-based protection and, in particular, role  organization. From basic role relationships, a model for role organization is developed.  The role graph model, its operator semantics based on graph theory and algorithms for  role administration are proposed. The role graph model, in our view, presents a very  generalized form of role organization for access rights administration. It is shown how  the model simulates other organizational structures such as hierarchies [TDH92] and  privilege graphs [Bal90]. 
1108|Conceptual Foundations for a Model of Task-based Authorizations|In this paper we describe conceptual foundations to address integrity issues in computerized information systems from the enterprise perspective. Our motivation for this effort stems from the recognition that existing models are formulated at too low a level of abstraction, to be useful for modeling organizational requirements, policy aspects, and internal controls, pertaining to maintenance of integrity in information systems. In particular, these models are primarily concerned with the integrity of internal data components within computer systems, and thus lack the constructs necessary to model enterprise level integrity principles. The starting point in our investigation is the notion of authorization functions and tasks associated with business activities carried out in the enterprise. These functions identify the authorization requirements while the authorization tasks embody the concepts required to carry out such authorizations. We believe a model of task-based autho...
1109|A Lattice Interpretation Of The Chinese Wall Policy|The Chinese Wall policy was identi#ed and so named by Brewer and Nash #2#.  This policy arises in the segment of the commercial sector which provides consulting  services to other companies. Consultants naturally have to deal with con#dential company  information for their clients. The objective of the Chinese Wall policy is to prevent  information #ows which cause con#ict of interest for individual consultants. Brewer and  Nash develop a mathematical model of the Chinese Wall policy, on the basis of which  they claim that this policy #cannot be correctly represented by a Bell-LaPadula model.&#034;  In this paper we demonstrate that the Brewer-Nash model is too restrictivetobeemployed  in a practical system. This is due to their treatment of users and subjects as  synonymous concepts, with the consequence that they do not distinguish security policy  as applied to human users versus security policy as applied to computer subjects. By  maintaining a careful distinction between users, princip...
1110|Delegation Of Authority|This paper is concerned with the specification of discretionary access control policy for commercial security and the delegation of access control authority in a way which gives flexibility while retaining management control. Large distributed processing systems have very large numbers of users and resource objects so that it is impractical to specify access control policy in terms of individual objects or individual users. We need to be able to specify it as relationships between groups of users and groups of objects. The systems typically consist of multiple interconnected networks and span a number of different organisations. Authority cannot be delegated or imposed from one central point, but has to be negotiated between independent managers who wish to cooperate but who may have a very limited trust in each other. The paper proposes the use of access rules to specify, in terms of their domain memberships, what operations a user can perform on a target object. The delegation of aut...
1111|Exploiting Generative Models in Discriminative Classifiers|Generative probability models such as hidden Markov models provide a principled way of treating missing information and dealing with variable length sequences. On the other hand, discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification performance superior to that of the model based approaches. An ideal classifier should combine these two complementary approaches. In this paper, we develop a natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models. We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of DNA and protein sequence analysis.
1112|The Nature of Statistical Learning Theory| Statistical learning theory was introduced in the late 1960’s. Until the 1990’s it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990’s new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more
1113|Multiple alignment using hidden Markov models|eddy~genetics.wustl.edu A simulated annealing method is described for training hidden Markov models and producing multiple sequence alignments from initially unaligned protein or DNA sequences. Simulated annealing in turn uses a dynamic programming algorithm for correctly sampling suboptimal multiple alignments according to their probability and a Boltzmann temperature factor. The quality of simulated annealing alignments is evaluated on structural alignments of ten different protein families, and compared to the performance of other HMM training methods and the ClnstalW program. Simulated annealing is better able to find near-global optima in the multiple alignment probability landscape than the other tested HMM training methods. Neither ClustalW nor simulated annealing produce consistently better alignments compared to each other. Examination of the specific cases in which ClustalW outperforms simulated annealing, and vice versa, provides insight into the strengths and weaknesses of current hidden Markov model approaches.
1114|Predicting protein structure using hidden Markov models|We discuss how methods based on hidden Markov models performed in the fold recognition section of the CASP2 experiment. Hidden Markov models were built for a set of about a thousand structures from the PDB database, and each CASP2 target sequence was scored against this library of hidden Markov models. In addition, a hidden Markov model was built for each of the target sequences, and all of the sequences in PDB were scored against that target model. Having high scores from both methods was found to be highly indicative of the target and a structure being homologous. Predictions were made based on several criteria: the scores with the structure models, the scores with the target models, consistency between the secondary structure in the known structure and predictions for the target (using the program PhD), human examination of predicted alignments between target and structure (using RASMOL), and solvation preferences in the alignment of the target and structure. The method worked well in comparison to other methods used at CASP2 for targets of moderate difficulty, where the closest structure in PDB could be aligned to the target with at least 15 % residue identity. There was no evidence for the method&#039;s e ectiveness for harder cases, where the residue identity was much lower than 15%.
1115|Modeling Term Structures of Defaultable Bonds|This article presents convenient reduced-form models of the valuation of contingent claims subject to default risk, focusing on applications to the term structure of interest rates for corporate or sovereign bonds. Examples include the valuation of a credit-spread option
1118|Transform Analysis and Asset Pricing for Affine Jump-Diffusions|In the setting of ‘‘affine’ ’ jump-diffusion state processes, this paper provides an analytical treatment of a class of transforms, including various Laplace and Fourier transforms as special cases, that allow an analytical treatment of a range of valuation and econometric problems. Example applications include fixed-income pricing models, with a role for intensity-based models of default, as well as a wide range of option-pricing applications. An illustrative example examines the implications of stochastic volatility and jumps for option valuation. This example highlights the impact on option ‘smirks ’ of the joint distribution of jumps in volatility and jumps in the underlying asset price, through both jump amplitude as well as jump timing.
1119|A yield-factor model of interest rates|This paper presents a consistent and arbitrage-free multifactor model of the term structure of interest rates in which yields at selected fixed maturities follow a parametric multivariate Markov diffusion process with “stochastic volatility. ” The yield of any zero-coupon bond is taken to be a maturitydependent affine combination of the selected “basis ” set of yields. We provide necessary and sufficient conditions on the stochastic model for this affine representation. We include numerical techniques for solving the model, as wcll as numerical techniques for calculating the prices of term-structure derivative prices. The case of jump diffusions i \ also considered. I.
1120|Specification Analysis of Affine Term Structure Models|This paper explores the structural differences and relative goodness-of-fits of affine term structure models (ATSMs55). Within the family of ATSMs there is a tradeoff between flexibility in modeling the conditional correlations and volatilities of the risk factors. This trade-off is formalized by our classification of N-factor affine family into N + 1 non-nested subfamilies of models. Specializing to three-factor ATSMs, our analysis suggests, based on theoretical considerations and empirical evidence, that some subfamilies of ATSMs are better suited than others to explaining historical interest rate behavior. 
1122|Term structures of credit spreads with incomplete accounting information|  We study the implications of imperfect information for term structures of credit spreads on corporate bonds. We suppose that bond investors cannot observe the issuer’s assets directly, and receive instead only periodic and imperfect accounting reports. For a setting in which the assets of the firm are a geometric Brownian motion until informed equityholders optimally liquidate, we derive the conditional distribution of the assets, given accounting data and survivorship. Contrary to the perfect-information case, there exists a default-arrival intensity process. That intensity is calculated in terms of the conditional distribution of assets. Credit yield spreads are characterized in terms of accounting information. Generalizations are provided.   
1123|The relation between treasury yields and corporate bond yield spreads|Because the option to call a corporate bond should rise in value when bond yields fall, the relation between noncallable Treasury yields and spreads of corporate bond yields over Treasury yields should depend on the callability of the corporate bond. I confirm this hypothesis for investment-grade corporate bonds. Although yield spreads on both callable and noncallable corporate bonds fall when Treasury yields rise, this relation is much stronger for callable bonds. This result has important implications for interpreting the behavior of yields on commonly used corporate bond indexes, which are composed primarily of callable bonds. COMMONLY USED INDEXES OF CORPORATE bond yields, such as those produced by Moody’s or Lehman Brothers, are constructed using both callable and noncallable bonds. Because the objective of those producing the indexes is to track the universe of corporate bonds, this methodology is sensible. Until the mid-1980s, few corporations issued noncallable bonds, hence an index designed to measure the yield on a typical corporate bond would have to be
1125|The Central Tendency: A Second Factor in Bond Yields,&#034; The Review of Economics and Statistics|you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at
1126|The Valuation of Default Risk in Corporate Bonds and Interest Rate Swaps|: This paper implements a model for the valuation of the default risk implicit in the prices of corporate bonds. The analytical approach considers the two essential ingredients in the valuation of corporate bonds: interest rate uncertainty and default risk. The former is modeled as a diffusion process. The latter is modeled as a spread following a diffusion process, with the magnitude of this spread impacting on the probability of a Poisson process governing the arrival of the default event. We apply two variants of this model to the valuation of fixed-for-floating swaps. In the first, the swap is default-free, and the spread represents the appropriate discounted expected value of the instantaneous TED spread; in the second, we allow the swap to incorporate default risk. We propose to test our models using the entire term structure of corporate bonds prices for different ratings and industry categories, as well as the term structure of fixed-for-floating swaps.  The Valuation of Defau...
1127|A tutorial on hidden Markov models and selected applications in speech recognition|Although initially introduced and studied in the late 1960s and early 1970s, statistical methods of Markov source or hidden Markov modeling have become increasingly popular in the last several years. There are two strong reasons why this has occurred. First the models are very rich in mathematical structure and hence can form the theoretical basis for use in a wide range of applications. Sec-ond the models, when applied properly, work very well in practice for several important applications. In this paper we attempt to care-fully and methodically review the theoretical aspects of this type of statistical modeling and show how they have been applied to selected problems in machine recognition of speech.  
1128|An Algorithm that Learns What&#039;s in a Name|In this paper, we present IdentiFinder^TM, a hidden Markov model that learns to recognize and classify names, dates, times, and numerical quantities. We have evaluated the model in English (based on data from the Sixth and Seventh Message Understanding Conferences [MUC-6, MUC-7] and broadcast news) and in Spanish (based on data distributed through the First Multilingual Entity Task [MET-1]), and on speech input (based on broadcast news). We report results here on standard materials only to quantify performance on data available to the community, namely, MUC-6 and MET-1. Results have been consistently better than reported by any other learning algorithm. IdentiFinder&#039;s performance is competitive with approaches based on handcrafted rules on mixed case text and superior on text where case information is not available. We also present a controlled experiment showing the effect of training set size on performance, demonstrating that as little as 100,000 words of training data is adequate to get performance around 90% on newswire. Although we present our understanding of why this algorithm performs so well on this class of problems, we believe that significant improvement in performance may still be possible.
1129|Maximum Entropy Models for Natural Language Ambiguity Resolution|The best aspect of a research environment, in my opinion, is the abundance of bright people with whom you argue, discuss, and nurture your ideas. I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas. I hope that Ihave kept the good ideas in this thesis, and left the bad ideas out! Iwould like toacknowledge the following people for their contribution to my education: I thank my advisor Mitch Marcus, who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing, and also gave me direction when necessary. I also thank Mitch for many fascinating conversations, both personal and professional, over the last four years at Penn. I thank all of my thesis committee members: John La erty from Carnegie Mellon University, Aravind Joshi, Lyle Ungar, and Mark Liberman, for their extremely valuable suggestions and comments about my thesis research. I thank Mike Collins, Jason Eisner, and Dan Melamed, with whom I&#039;ve had many stimulating and impromptu discussions in the LINC lab. Iowe them much gratitude for their valuable feedback onnumerous rough drafts of papers and thesis chapters.
1130|Adaptive Statistical Language Modeling: A Maximum Entropy Approach|Language modeling is the attempt to characterize, capture and exploit regularities in natural language. In statistical language modeling, large amounts of text are used to automatically determine the model&#039;s parameters. Language modeling is useful in automatic speech recognition, machine translation, and any other application that processes natural language with incomplete knowledge.  In this thesis, I view language as an information source which emits a stream of symbols from a finite alphabet (the vocabulary). The goal of language modeling is then to identify and exploit sources of information in the language stream, so as to minimize its perceived entropy.  Most existing statistical language models exploit the immediate past only. To extract information from further back in the document&#039;s history, I use trigger pairs as the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse.  Next, statistical evidence from many sources must...
1131|Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition|This paper describes a novel statistical namedentity (i.e. &#034;proper name&#034;) recognition system built around a maximum enti W framework. By working within the framework of maximum entropy. theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. These knowledge sources include capitalization features, lexical features, features in- dicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-wtrd terms. The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems. However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published.
1132|Information Extraction Using Hidden Markov Models|This thesis shows how to design and tune a hidden Markov model to extract factual information from a corpus of machine-readable English prose. In particular, the thesis presents a HMM that classifies and parses natural language assertions about genes being located at particular positions on chromosomes. The facts extracted by this HMM can be inserted into biological databases. The HMM is trained on a small set of sentence fragments chosen from the collected scientific abstracts in the OMIM (On-Line Mendelian Inheritance in Man) database and judged to contain the target binary relationship between gene names and gene locations. Given a novel sentence, all contiguous fragments are ranked by log-odds score, i.e. the log of the ratio of the probability of the fragment according to the target HMM to that according to a &#034;null&#034; HMM trained on all OMIM sentences. The most probable path through the HMM gives bindings for the annotations with precision as high as 80%. In contrast with traditional natural language processing methods, this stochastic approach makes no use either of part-of-speech taggers or dictionaries, instead employing non-emitting states to assemble modules roughly corresponding to noun, verb, and prepostional phrases. Algorithms for reestimating parameters for HMMs with non-emitting states are presented in detail. The ability to tolerate new words and recognize a wide variety of syntactic forms arises from the judicious use of &#034;gap&#034; states.
1133|Question Answering from Frequently-Asked Question Files: Experiences with the FAQ Finder System|This paper describes FAQ Finder, a natural language question-answering system that uses files of frequently-asked questions as its knowledge base. Unlike AI question-answering systems that focus on the generation of new answers, FAQ Finder retrieves existing ones found in frequently-asked question files. Unlike information retrieval approaches that rely on a purely lexical metric of similarity between query and document, FAQ Finder uses a semantic knowledge base (WordNet) to improve its ability to match question and answer. We describe the design considerations that have entered into the system and various experiments that influence the system&#039;s current implementation. We include results from an evaluation of the system&#039;s performance against a corpus of user questions, and show that a combination of semantic and statistical techniques works better than any single approach. Introduction  In the vast information space of the Internet, individuals and groups have created small pockets of ...
1134|Efficient Sampling and Feature Selection in Whole Sentence Maximum Entropy Language Models  |Conditional Maximum Entropy models have been successfully
1135|Markov Processes on Curves for Automatic Speech Recognition|We investigate a probabilistic framework for automatic speech recognition based on the intrinsic geometric properties of curves. In particular, we analyze the setting in which two variables---one continuous (x), one discrete (s)---evolve jointly in time. We suppose that the vector x traces out a smooth multidimensional curve and that the variable s evolves stochastically as a function of the  arc length traversed along this curve. Since arc length does not depend on the rate at which a curve is traversed, this gives rise to a family of Markov processes whose predictions, Pr[sjx], are invariant to nonlinear warpings of time. We describe the use of such models, known as Markov processes on curves (MPCs), for automatic speech recognition, where x are acoustic feature trajectories and s are phonetic transcriptions. On two tasks---recognizing New Jersey town names and connected alpha-digits---we find that MPCs yield lower word error rates than comparably trained hidden Markov models. 1 Intr...
1137|A Heteroskedasticity-Consistent Covariance Matrix Estimator And A Direct Test For Heteroskedasticity|This paper presents a parameter covariance matrix estimator which is consistent even  when the disturbances of a linear regression model are heteroskedastic. This estimator  does not depend on a formal model of the structure of the heteroskedasticity. By  comparing the elements of the new estimator to those of the usual covariance estimator,  one obtains a direct test for heteroskedasticity, since in the absence of heteroskedasticity,  the two estimators will be approximately equal, but will generally diverge otherwise. The  test has an appealing least squares interpretation
1138|How much should we trust differences-in-differences estimates?|Most papers that employ Differences-in-Differences estimation (DD) use many years of data and focus on serially correlated outcomes but ignore that the resulting standard errors are incon-sistent. To illustrate the severity of this issue, we randomly generate placebo laws in state-level data on female wages from the Current Population Survey. For each law, we use OLS to compute the DD estimate of its “effect” as well as the standard error of this estimate. These conventional DD standard errors severely understate the standard deviation of the estimators: we find an “effect ” significant at the 5 percent level for up to 45 percent of the placebo interventions. We use Monte Carlo simulations to investigate how well existing methods help solve this problem. Econometric corrections that place a specific parametric form on the time-series process do not perform well. Bootstrap (taking into account the auto-correlation of the data) works well when the number of states is large enough. Two corrections based on asymptotic approximation of the variance-covariance matrix work well for moderate numbers of states and one correction that collapses the time series information into a “pre” and “post” period and explicitly takes into account the effective sample size works well even for small numbers of states.
1139|Market Timing and Capital Structure|It is well known that firms are more likely to issue equity when their market values are high, relative to book and past market values, and to repurchase equity when their market values are low. We document that the resulting effects on capital structure are very persistent. As a consequence, current capital structure is strongly related to historical market values. The results suggest the theory that capital structure is the cumulative outcome of past attempts to time the equity market. 
1140|Disappearing Dividends: Changing Firm Characteristics or Lower Propensity to Pay?|The percent of firms paying cash dividends falls from 66.5 in 1978 to 20.8 in 1999. The decline is due in part to the changing characteristics of publicly traded firms. Fed by new lists, the population of publicly traded firms tilts increasingly toward small firms with low profitability and strong growth opportunities -- characteristics typical of firms that have never paid dividends. More interesting, we also show that controlling for characteristics, firms become less likely to pay dividends. This lower propensity to pay is at least as important as changing characteristics in the declining incidence of dividend payers.  *  Graduate School of Business, University of Chicago (Fama) and Sloan School of Management, MIT (French). We acknowledge the comments of John Graham, Douglas Hannah, Anil Kashyap, Tobias Moskowitz, G. William Schwert, Andrei Shleifer, Paul Zarowin, two anonymous (and especially helpful) referees, and seminar participants at Harvard University, the University of Chica...
1141|Robust Inference with Multi-way Clustering|In this paper we propose a new variance estimator for OLS as well as for nonlinear estimators such as logit, probit and GMM. This variance estimator enables cluster-robust inference when there is two-way or multi-way clustering that is nonnested. The variance estimator extends the standard cluster-robust variance estimator or sandwich estimator for one-way clustering (e.g. Liang and Zeger (1986), Arellano (1987)) and relies on similar relatively weak distributional assumptions. Our method is easily implemented in statistical packages, such as Stata and SAS, that already offer cluster-robust standard errors when there is one-way clustering. The method is demonstrated by a Monte Carlo analysis for a two-way random effects model; a Monte Carlo analysis of a placebo law that extends the state-year effects example of Bertrand et al. (2004) to two dimensions; and by application to two studies in the empirical public/labor literature where two-way clustering is present.
1142|Testing Tradeoff and Pecking Order Predictions about Dividends and Debt|We test the dividend and leverage predictions of the tradeoff and pecking order models. As both models predict, more profitable firms have higher long-term dividend payouts, and firms with more investments have lower payouts. Confirming the pecking order model but contradicting the tradeoff model, more profitable firms are less levered. Firms with more investment opportunities are also less levered, which is in line with the tradeoff model and a complex version of the pecking order model. Firms with more investments have lower long-term dividend payouts, but dividends do not vary to accommodate short-term variation in investment. Confirming the pecking order model, short-term variation in investment and earnings is mostly absorbed by variation in debt.  *  Graduate School of Business, University of Chicago (Fama) and Sloan School of Management, MIT (French).  The finance literature offers two competing models of financing decisions. In the tradeoff model, firms identify their optimal l...
1143|Does the source of capital affect capital structure |Prior work on leverage implicitly assumes capital availability depends solely on firm characteristics. However, market frictions that make capital structure relevant may also be associated with a firm’s source of capital. Examining this intuition, we find firms that have access to the public bond markets, as measured by having a debt rating, have significantly more leverage. Although firms with a rating are fundamen-tally different, these differences do not explain our findings. Even after controlling for firm characteristics that determine observed capital structure, and instrumenting for the possible endogeneity of having a rating, firms with access have 35 % more debt. Under the tradeoff theory of capital structure, firms determine their preferred leverage ratio by calculating the tax advantages, costs of financial distress, mispricing, and incentive effects of debt versus equity. The empirical literature has searched for evidence that firms choose their capital structure, as this theory predicts, by estimating firm leverage as a function of firm characteristics. Firms for whom the tax shields of debt are greater, the costs of financial distress lower, and the mispricing of debt relative to equity more favorable are expected to be more highly levered. When these firms discover that the net benefit of debt is positive, they will move toward their preferred capital structure by issuing additional debt and/or reducing their equity. The implicit assumption has been that a firm’s leverage is completely a function of a firm’s demand for debt. In
1144|Do conglomerate firms allocate resources inefficiently across industries? Theory and evidence|We develop a profit-maximizing neoclassical model of optimal firm size and growth across different industries based on differences in industry fundamentals and firm productivity. In the model, a conglomerate discount is consistent with profit maximization. The model predicts how conglomerate firms will allocate resources across divisions over the business cycle and how their responses to industry shocks will differ from those of single-segment firms. Using plant level data, we find that growth and investment of conglomerate and single-segment firms is related to fundamental industry factors and individual segment level productivity. The majority of conglomerate firms exhibit growth across industry segments that is consistent with optimal behavior.  
1145|The Effects of Government Ownership on Bank Lending|This paper studies the effects of government ownership on bank lending behavior. Using information on individual loan contracts, I compare the interest rate charged to two sets of companies with identical characteristics borrowing respectively from stateowned and privately owned banks. State-owned banks charge lower interest rates than do privately owned banks to similar or identical firms, even if the company is able to borrow more from privately owned banks. State-owned banks mostly favor firms located in depressed areas and large firms. The lending behavior of state-owned banks is affected by the electoral results of the party affiliated with the bank: the stronger the political party in the area where the firm is borrowing, the lower the interest rates charged. This result is robust to including bank and firm fixed effects. I am indebted to Andrei Shleifer for guidance and encouragement. I also thank Alberto Alesina, Paul
1146|Simple formulas for standard errors that cluster by both firm and time|When estimating finance panel regressions, it is common practice to adjust stan-dard errors for correlation either across firms or across time. These procedures are valid only if the residuals are correlated either across time or across firms, but not across both. This note shows that it is very easy to calculate standard errors that are robust to simultaneous correlation along two dimensions, such as firms and time. The covariance estimator is equal to the estimator that clusters by firm, plus the the estimator that clusters by time, minus the usual heteroskedasticity-robust OLS covariance matrix. Any statistical package with a clustering command can be used to easily calculate these standard errors.
1147|Stock Valuation and Learning about Profitability|We develop a simple approach to valuing stocks in the presence of learning about average profitability. The market-to-book ratio (M/B) increases with uncertainty about average profitability, especially for firms that pay no dividends. M/B is predicted to decline over a firm&#039;s lifetime due to learning, with steeper decline when the firm is young. These predictions are confirmed empirically. Data also support the predictions that younger stocks and stocks that pay no dividends have more volatile returns. Firm profitability has become more volatile recently, helping explain the puzzling increase in average idiosyncratic return volatility observed over the past few decades.
1148|What Drives Firm-Level Stock Returns?|I use a vector autoregressive model (VAR) to decompose an individual firm’s stock return into two components: changes in cash-flow expectations (i.e., cash-flow news) and changes in discount rates (i.e., expected-return news). The VAR yields three main results. First, firm-level stock returns are mainly driven by cash-flow news. For a typical stock, the variance of cash-flow news is more than twice that of expected-return news. Second, shocks to expected returns and cash flows are positively correlated for a typical small stock. Third, expected-return-news series are highly correlated across firms, while cash-flow news can largely be diversified away in aggregate portfolios. 
1149|An Empirical Analysis of Personal Bankruptcy and Delinquency|This paper uses a unique new panel data set of credit card accounts to analyze credit card delinquency and more generally personal bankruptcy and the stability of credit risk models. We estimate duration models for default and assess the relative importance of different variables in predicting default. We investigate how the propensity to default has changed over time, disentangling the two leading explanations for the recent increase in default rates -- a deterioration in the risk-composition of borrowers versus a reduction in the social stigma of default. Even after controlling for risk-composition and other economic fundamentals, the propensity to default significantly increased between 1995 and 1997. By contrast, increases in credit limits and other changes in risk-composition explain only a small part of the change in default rates. Standard default models appear to have missed an important time-varying default factor, consistent with the stigma effect.  JEL classification: E21; E...
1150|Conditioning manager alphas on economic information: Another look at the persistence of performance|This article presents evidence on persistence in the relative investment performance of large, institutional equity managers. Similar to existing evidence for mutual funds, we find persistent performance concentrated in the managers with poor prior-period performance measures. A conditional approach, using time-varying measures of risk and abnormal performance, is better able to detect this persistence and to predict the future performance of the funds than are traditional methods.
1151|Forecasting crashes: Trading volume, past returns and conditional skewness in stock prices| This paper is an investigation into the determinants of asymmetries in stock returns. We develop a series of cross-sectional regression specifications which attempt to forecast skewness in the daily returns of individual stocks. Negative skewness is most pronounced in stocks that have experienced: 1) an increase in trading volume relative to trend over the prior six months; and 2) positive returns over the prior thirty-six months. The first finding is consistent with the model of Hong and Stein (1999), which predicts that negative asymmetries are more likely to occur when there are large differences of opinion among investors. The latter finding fits with a number of theories, most notably Blanchard and Watson’s (1982) rendition of stockprice bubbles. Analogous results also obtain when we attempt to forecast the skewness of the aggregate stock market, though our statistical power in this case is limited.
1153|Informed trading in stock and option markets|We investigate the contribution of option markets to price discovery, using a modification of Hasbrouck’s (1995) “information share ” approach. Based on five years of stock and options data for 60 firms, we estimate the option market’s contribution to price discovery to be about 17 percent on average. Option market price discovery is related to trading volume and spreads in both markets, and stock volatility. Price discovery across option strike prices is related to leverage, trading volume, and spreads. Our results are consistent with theoretical arguments that informed investors trade in both stock and option markets, suggesting an important informational role for options.
1154|Asymptotic properties of a robust variance matrix estimator for panel data when T is large. Journal of Econometrics. forthcoming|Abstract. In this paper, I consider the asymptotic properties of a robust covariance ma-trix estimator which is commonly advocated for use in panel data. The estimator is a generalization of the conventional heteroskedasticity consistent covariance matrix estimator for panel data which allows arbitrary correlation within each individual. Under the usual panel asymptotics where the cross-section dimension, n, grows large with the time series dimension, T, fixed, this estimator has good properties while allowing an essentially uncon-strained time series pattern of correlation. However, many panel data sets are characterized by a non-negligible time dimension. I extend the usual analysis by examining cases where n and T go to infinity jointly, considering both non-mixing and mixing cases, and show that conventional t and F tests based on the robust covariance matrix estimator are consistent. In addition, when T ? 8 with n fixed and other regularity conditions are satisfied, I show that the usual t and F statistics can be used for inference despite the fact that the robust covariance matrix estimator is not consistent but converges in distribution to a limiting random variable. I also explore the properties of a direct test of the assumptions underlying simpler covariance matrix estimators analogous to that of White (1980). The properties of the robust covariance matrix estimator and tests based upon are it examined in a short Monte Carlo study.
1155|2000): Do Behavioral Biases Affect Prices |This paper documents strong evidence of behavioral biases among Chicago Board of Trade proprietary traders and investigates the effect these biases have on prices. Our traders appear highly loss-averse. Traders who experience morning losses are about 15 percent more likely to assume above-average afternoon risk than traders with morning gains. This behavior has important short-term consequences for afternoon prices, as losing traders actively purchase contracts at higher prices and sell contracts at lower prices than those that prevailed previously. However, during the Þve minutes that follow these trades, prices revert strongly to their earlier levels. Consistent with these Þndings, short-term afternoon price volatility is positively related to the prevalence of morning losses among locals, but overall afternoon price volatility is not.
