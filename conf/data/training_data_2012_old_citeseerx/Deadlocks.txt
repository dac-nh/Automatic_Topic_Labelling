ID|Title|Summary
1|Transactional Memory: Architectural Support for Lock-Free Data Structures |A shared data structure is lock-free if its operations do not require mutual exclusion. If one process is interrupted in the middle of an operation, other processes will not be prevented from operating on that object. In highly concurrent systems, lock-free data structures avoid common problems associated with conventional locking techniques, including priority inversion, convoying, and difficulty of avoiding deadlock. This paper introduces transactional memory, a new multiprocessor architecture intended to make lock-free synchronization as efficient (and easy to use) as conventional techniques based on mutual exclusion. Transactional memory allows programmers to define customized read-modify-write operations that apply to multiple, independently-chosen words of memory. It is implemented by straightforward extensions to any multiprocessor cache-coherence protocol. Simulation results show that transactional memory matches or outperforms the best known locking techniques for simple benchmarks, even in the absence of priority inversion, convoying, and deadlock.  
3|Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors|Scalable shared-memory multiprocessors distribute memory among the processors and use scalable interconnection networks to provide high bandwidth and low latency communication. In addition, memory accesses are cached, buffered, and pipelined to bridge the gap between the slow shared memory and the fast processors. Unless carefully controlled, such architectural optimizations can cause memory accesses to be executed in an order different from what the programmer expects. The set of allowable memory access orderings forms the memory consistency model or event ordering model for an architecture.
4|Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors|Busy-wait techniques are heavily used for mutual exclusion and barrier synchronization in shared-memory parallel programs. Unfortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnect contention, introducing performance bottlenecks that become markedly more pronounced as applications scale. We argue that this problem is not fundamental, and that one can in fact construct busy-wait synchronization algorithms that induce no memory or interconnect contention. The key to these algorithms is for every processor to spin on separate locally-accessible ag variables, and for some other processor to terminate the spin with a single remote write operation at an appropriate time. Flag variables may be locally-accessible as a result of coherent caching, or by virtue of allocation in the local portion of physically distributed shared memory. We present a new scalable algorithm for spin locks that generates O(1) remote references per lock acquisition, independent of the number of processors attempting to acquire the lock. Our algorithm provides reasonable latency in the absence of contention, requires only a constant amount of space per lock, and requires no hardware support other than
5|Distributed packet switching for local computer networks|Abstract: Ethernet is a branching broadcast communication system for carrying digital data packets among locally distributed computing stations. The packet transport mechanism provided by Ethernet has been used to build systems which can be viewed as either local computer networks or loosely coupled multiprocessors. An Ethernet&#039;s shared communication facility, its Ether, is a passive broadcast medium with no central control. Coordination of access to the Ether for packet broadcasts is distributed among the contending transmitting stations using controlled statistical arbitration. Switching of packets to their destinations on the Ether is distributed among the receiving stations using packet address recognition. Design principles and implementation are described based on experience.with an operating Ethernet of1 00 nodes along a kilometer of coaxial cable. A model for estimating performance under heavy loads and a packet protocol for error-controlled communication are included for completeness.
6|A Methodology for Implementing Highly Concurrent Data Objects| A concurrent object is a data structure shared by concurrent processes. Conventional techniques for implementing concurrent objects typically rely on critical sections: ensuring that only one process at a time can operate on the object. Nevertheless, critical sections are poorly suited for asynchronous systems: if one process is halted or delayed in a critical section, other, nonfaulty processes will be unable to progress. By contrast, a concurrent object implementation is lock free if it always guarantees that some process will complete an operation in a finite number of steps, and it is wait free if it guarantees that each process will complete an operation in a finite number of steps. This paper proposes a new methodology for constructing lock-free and wait-free implementations of concurrent objects. The object’s representation and operations are written as stylized sequential programs, with no explicit synchronization. Each sequential operation is automatically transformed into a lock-free or wait-free operation using novel synchronization and memory management algorithms. These algorithms are presented for a multiple instruction/multiple data (MIMD) architecture in which n processes communicate by applying atomic read, wrzte, load_linked, and store_conditional operations to a shared memory.
7|Memory access buffering in multiprocessors|In highly-pipelined machines, instructions and data are prefetched and buffered in both the processor and the cache. This is done to reduce the average memory access la-tency and to take advantage of memory interleaving. Lock-up free caches are designed to avoid processor blocking on a cache miss. Write buffers are often included in a pipelined machine to avoid processor waiting on writes. In a shared memory multiprocessor, there are more advantages in buffering memory requests, since each memory access has to traverse the memory- processor interconnection and has to compete with memory requests issued by different processors. Buffering, however, can cause logical problems in multipro-cessors. These problems are aggravated if each processor has a private memory in which shared writable data may be present, such as in a cache-based system or in a system with a distributed global memory. In this paper, we analyze the benefits and problems associated with the buffering of memory requests in shared memory multiprocessors. We show that the logical problem of buffering is directly related to the problem of synchronization. A simple model is presented to evaluate the performance improvement result-ing from buffering. 1.
8|LimitLESS Directories: A Scalable Cache Coherence Scheme|Caches enhance the performance of multiprocessors by reducing network tra#c and average memory access latency. However, cache-based systems must address the problem of cache coherence. We propose the LimitLESS directory protocol to solve this problem. The LimitLESS scheme uses a combination of hardware and software techniques to realize the performance of a full-map directory with the memory overhead of a limited directory. This protocol is supported by Alewife, a large-scale multiprocessor. We describe the architectural interfaces needed to implement the LimitLESS directory, and evaluate its performance through simulations of the Alewife machine.
9|Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors |The memory consistency model supported by a multiprocessor architecture determines the amount of buffering and pipelining that may be used to hide or reduce the latency of memory accesses. Several different consistency models have been proposed. These range from sequential consistency on one end, allowing very limited buffering, to release consistency on the other end, allowing extensive buffering and pipelining. The processor consistency and weak consistency models fall in between. The advantage of the less strict models is increased performance potential. The disadvantage is increased hardware complexity and a more complex programming model. To make an informed decision on the above tradeoff requires performance data for the various models. This paper addresses the issue of performance benefits from the above four consistency models. Our results are based on simulation studies done for three applications. The results show that in an environment where processor reads are blocking and writes are buffered, a significant performance increase is achieved from allowing reads to bypass previous writes. Pipelining of writes, which determines the rate at which writes are retired from the write buffer, is of secondary importance. As a result, we show that the sequential consistency model performs poorly relative to all other models, while the processor consistency model provides most of the benefits of the weak and release consistency models. 
10|The Expandable Split Window Paradigm for Exploiting Fine-Grain Parallelism|We propose a new processing paradigm, called the Expandable Split Window (ESW) paradigm, for exploiting finegrain parallelism. This paradigm considers a window of instructions (possibly having dependencies) as a single unit, and exploits fine-grain parallelism by overlapping the execution of multiple windows. The basic idea is to connect multiple sequential processors, in a decoupled and decentralized manner, to achieve overall multiple issue. This processing paradigm shares a number of properties of the restricted dataflow machines, but was derived from the sequential von Neumann architecture. We also present an implementation of the Expandable Split Window execution model, and preliminary performance results. 1.
11|A Lock-Free Multiprocessor OS Kernel|Typical shared-memory multiprocessor OS kernels use interlocking, implemented as spinlocks or waiting semaphores. We have implemented a complete multiprocessor OS kernel (including threads, virtual memory, and I/O including a window system and a file system) using only lock-free synchronization methods based on Compare-and-Swap. Lock-free synchronization avoids many serious problems caused by locks: considerable overhead, concurrency bottlenecks, deadlocks, and priority inversion in real-time scheduling. Measured numbers show the low overhead of our implementation, competitive with user-level thread management systems.  Contents  1 Introduction 1 2 Synchronization in OS Kernels 1  2.1 Disabling Interrupts : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 1 2.2 Locking Synchronization Methods : : : : : : : : : : : : : : : : : : : : : : : : : : 2 2.3 Lock-Free Synchronization Methods : : : : : : : : : : : : : : : : : : : : : : : : : 2  3 Lock-Free Quajects 3  3.1 LIFO ...
12|Detecting Violations of Sequential Consistency|The performance of a multiprocessor is directly affected by the choice of the memory consistency model supported. Several different consistency models have been proposed in the literature. These range from sequential consistency on one end, allowing limited buffering of memory accesses, to release consistency on the other end, allowing extensive buffering and pipelining. While the relaxed models such as release consistency provide potential for higher performance, they present a more complex programming model than sequential consistency. Previous research has addressed this tradeoff by showing that a release consistent architecture provides sequentially consistent executions for programs that are free of data races. However, the burden of guaranteeing that the program is free of data races remains with the programmer or compiler.
13|The dangers of replication and a solution|Update anywhere-anytime-anyway transactional replication has unstable behavior as the workload scales up: a ten-fold increase in nodes and traflc gives a thousand fold increase in deadlocks or reconciliations. Master copy replication (primary copyj schemes reduce this problem. A simple analytic model demonstrates these results. A new two-tier replication algorithm is proposed that allows mobile (disconnected) applications to propose tentative update transactions that are later applied to a master copy. Commutative update transactions avoid the instability of other replication schemes.
14|Weighted Voting for Replicated Data|In a new algorithm for maintaining replicated data, every copy of a replicated file is assigned some number of votes. Every transaction collects a read quorum of r votes to read a file, and a write quorum of w votes to write a file, such that r+w is greater than the total number number of votes assigned to the file. This ensures that there is a non-null intersection between every read quorum and every write quorum. Version numbers make it possible to determine which copies are current. The reliability and performance characteristics of a replicated file can be controlled by appropriately choosing r, w, and the file&#039;s voting configuration. The algorithm guarantees serial consistency, admits temporary copies in a natural way by the introduction of copies of an application system called Violet.
15|How to assign votes in a distributed system|Abstract. In a distributed system, one strategy for achieving mutual exclusion of groups of nodes without communication is to assign to each node a number of votes. Only a group with a majority of votes can execute the critical operations, and mutual exclusion is achieved because at any given time there is at most one such group. A second strategy, which appears to be similar to votes, is to define a priori a set of groups that intersect each other. Any group of nodes that finds itself in this set can perform the restricted operations. In this paper, both of these strategies are studied in detail and it is shown that they are not equivalent in general (although they are in some cases). In doing so, a number of other interesting properties are proved. These properties will be of use to a system designer who is selecting a vote assignment or a set of groups for a specific application.
16|Distributed Snapshots: Determining Global States of Distributed Systems|This paper presents an algorithm by which a process in a distributed system determines a global state of the system during a computation. Many problems in distributed systems can be cast in terms of the problem of detecting global states. For instance, the global state detection algorithm helps to solve an important class of problems: stable property detection. A stable property is one that persists: once a stable property becomes true it remains true thereafter. Examples of stable properties are “computation has terminated,”  “the system is deadlocked” and “all tokens in a token ring have disappeared.” The stable property detection problem is that of devising algorithms to detect a given stable property. Global state detection can also be used for checkpointing. 
17|Time, Clocks, and the Ordering of Events in a Distributed System|The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.  
18|Distributed deadlock detection|Distributed deadlock models are presented for resource and communication deadlocks. Simple distributed algorithms for detection of these deadlocks are given. We show that all true deadlocks are detected and that no false deadlocks are reported. In our algorithms, no process maintains global information; all messages have an identical short length. The algorithms can be applied in distributed database and other message communication systems.
19|Distributed Computation on Graphs: Shortest Path Algorithms|We use the paradigm of diffusing computation, intro-duced by Dijkstra and Scholten, to solve a class of graph problems. We present a detailed solution to the problem of computing shortest paths from a single vertex to all other vertices, in the presence of negative cycles.
20|Distributed Deadlock Detection Algorithm|This paper employs the same terminology. All words that originate with the author are enclosed in quotation marks at their first mention and appear with initial capital letters throughout
21|Termination detection of diffusing computations in communicating sequential processes|In this paper it is shown how the Dijkstra-Scholten scheme for termination detection in a diffusing computation can be adapted to detect termination or deadlock in a network of communicating sequential processes as defined by Hoare. Categories and Subject Descriptors: D.2.4 [Software Engineering]: Program Verification-correct. ness proofs; D.3.3 [Programming Languages]: Language Constructs--concurrent programming
22|Real-Time Systems|Collision avoidance is an important topic in multi-robot systems. Existing multi-robot pathfinding approaches ignore sideswipe collisions among robots (i.e., only consider the collision which two agents try to occupy the same node during the same time-step) [1, 3, 4], and allow diagonal move between two adjacent nodes (e.g., Figure 1(b)). However, in many real world applications, sideswipe collisions may also block robots ’ movements or cause deadlocks. For example, as shown in Figure 1, if the size of two robots is as big as the grid size they occupied, collisions will happen not only between robots R1 and R2 in the situation depicted in Figure 1(a), but also that in Figure 1(b), which is typically not considered as a collision in existing multi-robot systems. (a) (b) (c) Figure 2: Illustration of deadloop. The green square and the red square are the robot positions and the goal positions for two robots, respectively. R1 and R2 are robot 1 and robot 2. (a) The initial position for two robots. (b) and (c) The dead looping condition is encountered and repeated in-between (b) and (c) infinitely as each robot makes a move that mirrors the other robot’s.
23|XSB as an Efficient Deductive Database Engine|This paper describes the XSB system, and its use as an in-memory deductive database engine. XSB began from a Prolog foundation, and traditional Prolog systems are known to have serious deficiencies when used as database systems. Accordingly, XSB has a fundamental bottom-up extension, introduced through tabling (or memoing) [5], which makes it appropriate as an underlying query engine for deductive database systems. Because it eliminates redundant computation, the tabling extension makes XSB able to compute all modularly stratified datalog programs finitely and with polynomial data complexity. For non-stratified programs, a metainterpreter with the same properties is provided. In addition XSB significantly extends and improves the indexing capabilities over those of standard Prolog. Finally, its syntactic basis in HiLog [2], lends it flexibility for data modelling. The implementation of XSB derives from the WAM, the most common Prolog engine. XSB inherits the WAM&#039;s efficiency and can ta...
24|The LDL System Prototype|The LDL system provides a declarative logic-based language and integrates relational database and logic programming technologies so as to support advanced data and knowledge-based applications. This paper contains a comprehensive overview of the system and contains a description of the LDL language and the compilation techniques employed to translate LDL queries into target queries on the stored data. The paper further contains a description of the architecture and runtime environment of the system and the optimization techniques employed in order to improve the performance and assure the safety of the compiled queries. The paper concludes with an account of the experience gained so far with the system, and discusses application areas where the LDL approach appears to be particularly effective.
25|The Aditi deductive database system| Deductive databases generalize relational databases by providing sup-port for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting par-allelism on shared-memory multiprocessors. The back-end uses relational tech-nology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.
26|Rule ordering in bottom-up fixpoint evaluation of logic programs|Abstract Logic programs can be evaluated bottom-up by repeatedly applying all rules, in &amp;quot;iterations&amp;quot;, until the fixpoint is reached. However, it is often desirable--and in some cases, e.g. programs with stratified negation, even necessary to guarantee the semantics--to apply the rules in some order. We present two algorithms that apply rules in a specified order without repeating inferences. One of them (GSN) is capable of dealing with a wide range of rule orderings but with a little more overhead than the well-known semi-naive algorithm (which we call BSN). The other (PSN) handles a smaller class of rule orderings, but with no overheads beyond those in BSN. We also demonstrate that by choosing a good ordering, we can reduce the number of rule applications (and thus joins). We present a theoretical analysis of rule orderings and identify orderings that minimize the number of rule applications (for all possible instances of the base relations) with respect to a class of orderings called fair orderings. We also show that while non-fair orderings may do a little better on some data sets, they can do much worse on others. The analysis is supplemented by performance results.
27|Relationlog: A Typed Extension to Datalog with Sets and Tuples (Extended Abstract)  (1995) |)  Mengchi Liu  Department of Computer Science  University of Regina  Regina, Saskatchewan, Canada S4S 0A2  mliu@cs.uregina.ca  Abstract  This paper presents a novel logic-based language for nested relations. It stands in the same relationship to the nested relation model as Datalog stands to the relational model. The main novelties of the language are the mechanisms for representing both partial and complete information on sets and tuples, and the introduction of a new ordering on interpretations that captures the intended semantics for nested sets, tuples and relations. Under appropriate stratification restrictions, it is shown that the unique minimal and supported model, if it exists, can be computed bottom-up, and therefore used as the intended semantics of the program.  1 Introduction  In the past decade, there has been a lot of interest in nested relations, whose tuple components may be sets, tuples or even relations [2, 8, 11, 12, 14, 15]. The extended relational algebra and cal...
28|Design and Implementation of the Glue-Nail Database System|We describe the design and implementation of the Glue-Nail database system. The Nail language is a purely declarative query language; Glue is a procedural language used for non-query activities. The two languages combined are sufficient to write a complete application. Nail and Glue code both compile into the target language IGlue. The Nail compiler uses variants of the magic sets algorithm, and supports well-founded models. Static optimization is performed by the Glue compiler using techniques that include peephole methods and data flow analysis. The IGlue code is executed by the IGlue interpreter, which features a run-time adaptive optimizer. The three optimizers each deal with separate optimization domains, and experiments indicate that an effective synergism is achieved. The Glue-Nail system is largely complete and has been tested using a suite of representative applications. 
29|LogicBase: A Deductive Database System Prototype|A deductive database system prototype, LogicBase, has been developed, with an emphasis on efficient compilation and query evaluation of application-oriented recursions in deductive databases. The system identifies different classes of recursions and compiles recursions into chain or pseudo-chain forms when appropriate. Queries posed to the compiled recursions are analyzed systematically with efficient evaluation plans generated and executed, mainly based on a chain-based query evaluation method. The system has been tested using sophisticated recursions and queries with satisfactory performance. This paper introduces the general design principles and implementation techniques of the system and discusses its strength and limitations. 1 Introduction As an important extension to relational approach, research into deductive database systems represents a direction towards declarative query processing, high-level database programming, and integration of logic programming and relational databa...
30|Design and Implementation of the Relationlog Deductive Database System|We describe the design and implementation of Relationlog, a persistent deductive database system. Unlike other related systems such as Aditi, CORAL, LDL, LOLA and Nail-Glue, Relationlog supports effective storage, efficient access and inference of large amounts of data with complex structures and provides declarative query language that can define recursive views involving complex data and also a declarative data manipulation language to update databases.
31|Direction Maps for Cooperative Pathfinding |Cooperative behavior is a desired trait in many fields from computer games to robotics. Yet, achieving cooperative behavior is often difficult, as maintaining shared information about the dynamics of agents in the world can be complex. We focus on the specific task of cooperative pathfinding and introduce a new approach based on the idea of “direction maps ” that learns about the movement of agents in the world. This learned data then is used to produce implicit cooperation between agents. This approach is less expensive and has better performance than several existing cooperative algorithms.
32|Extending Datalog with Declarative Updates|The semantics of static deductive databases is well understood based on the work in logic programming. In the past decade, various methods to incorporate update constructs into logic programming and deductive databases have been proposed. However, there is still no consensus about the appropriate treatment of dynamic behavior in deductive databases. In this paper, we propose a language called DatalogU, which is a minimal but powerful extension of Datalog with updates to base relations. DatalogU allows the user to program set-oriented complex database transactions with concurrent, disjunctive and sequential update operations in a simple and direct way. It has a simple and intuitive declarative semantics that naturally accounts for set-oriented updates in deductive databases.
33|Ownership Types for Safe Programming: Preventing Data Races and Deadlocks|This paper presents a new static type system for multi-threaded programs; well-typed programs in our system are guaranteed to be free of data races and deadlocks. Our type system allows programmers to partition the locks into a fixed number of equivalence classes and specify a partial order among the equivalence classes. The type checker then statically verifies that whenever a thread holds more than one lock, the thread acquires the locks in the descending order. Our system also allows...
34|A Syntactic Approach to Type Soundness|We present a new approach to proving type soundness for Hindley/Milner-style polymorphic type systems. The keys to our approach are (1) an adaptation of subject reduction theorems from combinatory logic to programming languages, and (2) the use of rewriting techniques for the specification of the language semantics. The approach easily extends from polymorphic functional languages to imperative languages that provide references, exceptions, continuations, and similar features. We illustrate the technique with a type soundness theorem for the core of Standard ML, which includes the first type soundness proof for polymorphic exceptions and continuations.  
35|Bugs as Deviant Behavior: A General Approach to Inferring Errors in Systems Code|A major obstacle to finding program errors in a real system is knowing what correctness rules the system must obey. These rules are often undocumented or specified in an ad hoc manner. This paper demonstrates tech-niques that automatically extract such checking information from the source code itself, rather than the programmer, thereby avoiding the need for a priori knowledge of system rules. The cornerstone of our approach is inferring programmer &#034;beliefs&#034; that we then cross-check for contradictions. Beliefs are facts implied by code: a dereference of a pointer, p, implies a belief that p is non-null, a call to &#034;unlock(1)&#034; implies that 1 was locked, etc. For beliefs we know the programmer must hold, such as the pointer dereference above, we immediately flag contra-
36|Enforcing High-Level Protocols in Low-Level Software|The reliability of infrastructure software, such as operating systems and web servers, is often hampered by the mismanagement of resources, such as memory and network connections. The Vault programming language allows a programmer to describe resource management protocols that the compiler can statically enforce. Such a protocol can specify that operations must be performed in a certain order and that certain operations must be performed before accessing a given data object. Furthermore, Vault enforces statically that resources cannot be leaked. We validate the utility of our approach by enforcing protocols present in the interface between the Windows 2000 kernel and its device drivers.
37|Ownership Types for Flexible Alias Protection|Object-oriented programming languages allow inter-object aliasing. Although necessary to construct linked data structures and networks of interacting objects, aliasing is problematic in that an aggregate object&#039;s state can change via an alias to one of its components, without the aggregate being aware of any aliasing. Ownership types form a static type system that indicates object ownership. This provides a flexible mechanism to limit the visibility of object references and restrict access paths to objects, thus controlling a system&#039;s dynamic topology. The type system is shown to be sound, and the specific aliasing properties that a system&#039;s object graph satisfies are formulated and proven invariant for well-typed programs. Keywords Alias protection, sharing, containment, ownership, representation exposure, programming language design 1
39|Solving Shape-Analysis Problems in Languages with Destructive Updating|This paper concerns the static analysis of programs that perform destructive updating on heap-allocated storage. We give an algorithm that conservatively solves this problem by using a finite shape-graph to approximate the possible “shapes” that heap-allocated structures in a program can take on. In contrast with previous work, our method M even accurate for certain programs that update cyclic data structures. For example, our method can determine that when the input to a program that searches a list and splices in a new element is a possibly circular list, the output is a possibly circular list. 
40|Classes and Mixins|While class-based object-oriented programming languages provide a flexible mechanism for re-using and managing related pieces of code, they typically lack linguistic facilities for specifying a uniform extension of many classes with one set of fields and methods. As a result, programmers are unable to express certain abstractions over classes. In this paper we develop a model of class-to-class functions that we refer to as mixins. A mixin function maps a class to an extended class by adding or overriding fields and methods. Programming with mixins is similar to programming with single inheritance classes, but mixins more directly encourage programming to interfaces. The paper develops these ideas within the context of Java. The results are 1. an intuitive model of an essential Java subset; 2. an extension that explains and models mixins; and 3. type soundness theorems for these languages.
41|Type-based race detection for Java |This paper presents a static race detection analysis for multithreaded Java programs. Our analysis is based on a formal type system that is capable of capturing many common synchronization patterns. These patterns include classes with internal synchronization, classes that require client-side synchronization, and thread-local classes. Experience checking over 40,000 lines of Java code with the type system demonstrates that it is an effective approach for eliminating races conditions. On large examples, fewer than 20 additional type annotations per 1000 lines of code were required by the type checker, and we found a number of races in the standard Java libraries and other test programs. 1
42|Efficient and Precise Datarace Detection for Multithreaded Object-Oriented Programs|We present a novel approach to dynamic datarace detection for multithreaded object-oriented programs. Past techniques for onthe -fly datarace detection either sacrificed precision for performance,  leading to many false positive datarace reports, or maintained precision but incurred significant overheads in the range of 3# to 30#. In contrast, our approach results in very few false positives and runtime overhead in the 13% to 42% range, making it both efficient  and precise. This performance improvement is the result of a unique combination of complementary static and dynamic optimization techniques.
43|Typed Memory Management in a Calculus of Capabilities|Region-based memory management is an alternative to standard tracing garbage collection that  makes potentially dangerous operations such as memory deallocation explicit but verifiably safe. In  this article, we present a new compiler intermediate language, called the Capability Calculus, that  supports region-based memory management and enjoys a provably safe type system. Unlike previous  region-based type systems, region lifetimes need not be lexically scoped and yet the language may  be checked for safety without complex analyses. Therefore, our type system may be deployed in  settings such as extensible operating systems where both the performance and safety of untrusted  code is important.
44|A Parameterized Type System for Race-Free Java Programs|...programs; any well-typed program in our system is free of data races. Our type system is significantly more expressive than previous such type systems. In particular, our system lets programmers write generic code to implement a class, then create different objects of the same class that have different protection mechanisms. This flexibility enables programmers to reduce the number of unnecessary synchronization operations in a program without risking data races. We also support default types which reduce the burden of writing the extra type annotations. Our experience indicates that our system provides a promising approach to make multithreaded programs more reliable and efficient.
45|Alias Annotations for Program Understanding|One of the primary challenges in building and evolving large object-oriented systems is dealing with aliasing between objects. Unexpected aliasing can lead to broken invariants, mistaken assumptions, security holes, and surprising side effects, all of which may lead to software defects and complicate software evolution.
46|Region-Based Memory Management in Cyclone|Cyclone is a type-safe programming language derived from C. The primary design goal of Cyclone is to let programmers control data representation and memory management without sacrificing type-safety. In this paper, we focus on the region-based memory management of Cyclone and its static typing discipline. The design incorporates several advancements, including support for region subtyping and a coherent integration with stack allocation and a garbage collector. To support separate compilation, Cyclone requires programmers to write some explicit region annotations, but a combination of default annotations, local type inference, and a novel treatment of region e#ects reduces this burden. As a result, we integrate C idioms in a region-based framework. In our experience, porting legacy C to Cyclone has required altering about 8% of the code; of the changes, only 6% (of the 8%) were region annotations.
48|Linear Types Can Change the World!|The linear logic of J.-Y. Girard suggests a new type system for functional  languages, one which supports operations that &#034;change the world&#034;. Values belonging  to a linear type must be used exactly once: like the world, they cannot be  duplicated or destroyed. Such values require no reference counting or garbage collection, and safely admit destructive array update. Linear types extend Schmidt&#039;s  notion of single threading; provide an alternative to Hudak and Bloss&#039; update  analysis; and offer a practical complement to Lafont and Holmström&#039;s elegant linear languages.
49|Ownership, Encapsulation and the Disjointness of Type and Effect|Ownership types provide a statically enforceable notion of object-level encapsulation. We extend ownership types with computational e#ects to support reasoning about objectoriented programs. The ensuing system provides both access control and e#ects reporting. Based on this type system, we codify two formal systems for reasoning about aliasing and the disjointness of computational e#ects. The first can be used to prove that evaluation of two expressions will never lead to aliases, while the latter can be used to show the non-interference of two expressions.
50|Balloon Types: Controlling Sharing of State in Data Types|. Current data abstraction mechanisms are not adequate to control sharing of state in the general case involving objects in linked structures. The pervading possibility of sharing is a source of errors and an obstacle to language implementation techniques. We present a general extension to programming languages which makes the ability to share state a first class property of a data type, resolving a long-standing flaw in existing data abstraction mechanisms. Balloon types enforce a strong form of encapsulation: no state reachable (directly or transitively) by a balloon object is referenced by any external object. Syntactic simplicity is achieved by relying on a non-trivial static analysis as the checking mechanism. Balloon types are applicable in a wide range of areas such as program transformation, memory management and distributed systems. They are the key to obtaining self-contained composite objects, truly opaque data abstractions and value types---important concepts for the develo...
52|A Generic Type System for the Pi-Calculus|We propose a general, powerful framework of type systems for the #-calculus, and show that  we can obtain as its instances a variety of type systems guaranteeing non-trivial properties like  deadlock-freedom and race-freedom. A key idea is to express types and type environments as  abstract processes: We can check various properties of a process by checking the corresponding  properties of its type environment. The framework clarifies the essence of recent complex type  systems, and it also enables sharing of a large amount of work such as a proof of type preservation,  making it easy to develop new type systems.
53|Data Abstraction and Information Hiding|This paper describes an approach for verifying programs in the presence of data abstraction and information hiding, which are key features of modern programming languages with objects and modules. The paper focuses on the property of modular soundness, that is, the property that the separate verifications of the individual modules of the program suffice to ensure the correctness of the composite program. The paper introduces a new specification language construct, the abstraction  dependency, and argues that it is needed to achieve modular soundness in the presence of data abstraction and information hiding. This paper discusses in detail two varieties of abstraction dependencies: static and dynamic. The paper also presents a new technical definition of modular soundness as a monotonicity property of verifiability with respect to scope and uses this technical definition to formally prove the modular soundness of a programming discipline for static dependencies.  
54|Adding Type Parameterization to the Java Language|Although the Java programming language has achieved widespread acceptance, one feature that seems sorely missed is the ability to use type parameters (as in Ada generics, C++ templates, and ML polymorphic functions or data types) to allow a general concept to be instantiated to one or more specific types. In this paper, we propose parameterized classes and interfaces in which the type parameter may be constrained to either implement a given interface or extend a given class. This design allows the body of a parameterized class to refer to methods on objects of the parameter type, without introducing any new type relations into the language. We show that these Java extensions may be implemented by expanding parameterized classes at class load time, without any extension or modification to existing Java bytecode, verifier or bytecode interpreter.  1 Introduction  In Ada generics [US 80], C++ templates [ES90], and ML polymorphic functions and data types [Mil85, Ull94], type parameterizati...
56|Types as Models: Model Checking Message-Passing Programs|Abstraction and composition are the fundamental issues in making model checking viable for software. This paper proposes new techniques for automating abstraction and decomposition using source level type information provided by the programmer. Our system includes two novel components to achieve this end: (1) a new behavioral type-and-effect system for the pi-calculus, which extracts sound models as types, and (2) a new assume-guarantee proof rule for carrying out compositional model checking on the types. Open simulation between CCS processes is used as both the subtyping relation in the type system and the abstraction relation for compositional model checking. We have implemented these ideas in a tool -- Piper. Piper exploits type signatures provided by the programmer to partition the model checking problem, and emit model checking obligations that are discharged using the Spin model checker. We present the details on applying Piper on two examples: (1) the SIS standard for managing trouble tickets across multiple organizations and (2) a file reader from the pipelined implementation of a web server.
57|Simple Ownership Types for Object Containment|Containment of objects is a natural concept that has been poorly supported in object-oriented programming languages. For a predefined set of ownership contexts, this paper presents a type system that enforces certain containment relationships for run-time objects. A fixed ordering relationship is presumed between the owners. The formalisation of ownership types has developed from our work with flexible alias protection together with an investigation of structural properties of object graphs based on dominator trees. Our general ownership type system permits fresh ownership contexts to be created at run-time. Here we present a simplified system in which the ownership contexts are predefined. This is powerful enough to express and enforce constraints about a system&#039;s high-level structure. Our formal system is presented in an imperative variant of the object calculus. We present type preservation and soundness results. Furthermore we highlight how these type theoretic results establish a containment invariant for objects, in which access to contained objects is only permitted via their owners. In effect, the predefined ownership ordering restricts the permissible inter-object reference structure.
58|Data groups: Specifying the modification of extended state|This paper explores the interpretation of specifications in the context of an object-oriented programming language with subclassing and method overrides. In particular, the paper considers annotations for describing what variables a method may change and the interpretation of these annotations. The paper shows that there is a problem to be solved in the specification of methods whose overrides may modify additional state introduced in subclasses. As a solution to this problem, the paper introduces data groups, which enable modular checking and rather naturally capture a programmer&#039;s design decisions.
59|Towards Alias-Free Pointers|. This paper argues that pointer-induced aliasing can be avoided in many cases by means of a concept of unique pointer. The use of such pointers is expected to fortify the concept of encapsulation, to make systems easier to reason about, to provide better control over the interaction between threads, and to make storage management safer and more efficient. We show that unique pointers can be implemented by means of few minor and virtually costless modifications in conventional OO languages, such as Eiffel or C++; and that they can be used conveniently in a broad range of algorithms and data structures. Key Words and Phrases: pointer-induced aliasing, hiding, encapsulation, programming with threads, storage management. ?  Work supported by NSF grant No. CCR-9308773.  1 Introduction  Dynamic objects, i.e., objects allocated on the heap and addressed by means of pointers, are widely considered a mixed blessing in imperative programming. A blessing, because dynamic objects have some very ...
60|Subtypes vs. Where Clauses: Constraining Parametric Polymorphism|All object-oriented languages provide support for subtype polymorphism, which allows the writing of generic code that works for families of related types. There is also a need, however, to write code that is generic across types that have no real family relationship. To satisfy this need a programming language must provide a mechanism for parametric polymorphism, allowing for types as parameters to routines and types. We show that to support modular programming and separate compilation there must be a mechanism for constraining the actual parameters of the routine or type. We describe a simple and powerful constraint mechanism and compare it with constraint mechanisms in other languages in terms of both ease of use and semantic expressiveness. We also discuss the interaction between subtype and parametric polymorphism: we discuss the subtype relations that can exist between instantiations of parameterized types, and which of those relations are useful and can be implemented efficiently...
61|Guava: A Dialect of Java without Data Races|We introduce Guava, a dialect of Java whose rules statically guarantee that parallel threads access shared data only through synchronized methods. Our dialect distinguishes three categories of classes: (1) monitors, which may be referenced from multiple threads, but whose methods are accessed serially; (2) values, which cannot be referenced and therefore are never shared; and (3) objects, which can have multiple references but only from within one thread, and therefore do not need to be synchronized. Guava circumvents the problems associated with today&#039;s Java memory model, which must define behavior when concurrent threads access shared memory without synchronization.
62|Report on the programming language euclid|© Copyright Xerox Corporation 1981 This report describes a new programming language called Euclid, intended for the expression of system programs which are to be verified.
64|Safe Runtime Downcasts With Ownership Types|The possibility of aliasing between objects constitutes one of  the primary challenges in understanding and reasoning about correctness  of object-oriented programs. Ownership types provide a principled way of  specifying statically enforcable restrictions on object aliasing. Ownership  types have been used to aid program understanding and evolution, verify  absence of data races and deadlocks in multithreaded programs, and  verify absence of memory errors in programs with explicit deallocation. This paper
65|Ownership Types and Safe Lazy Upgrades in Object-Oriented Databases|... object encapsulation and enable local reasoning about program correctness in object-oriented languages. However, a type system that enforces strict object encapsulation is too constraining: it does not allow efficient implementation of important constructs like iterators. This paper argues that the right way to solve the problem is to allow objects of classes defined in the same module to have privileged access to each other&#039;s representations; we show how to do this for inner classes. This approach allows programmers to express constructs like iterators and yet supports local reasoning about the correctness of the classes, because a class and its inner classes together can be reasoned about as a module. The paper also sketches how we use our variant of ownership types to enable efficient software upgrades in persistent object stores.
66|Nested Transactions: An Approach to Reliable Distributed Computing|Distributed computing systems are being built and used more and more frequently. This distributod computing revolution makes the reliability of distributed systems an important concern. It is fairly well-understood how to connect hardware so that most components can continue to work when others are broken, and thus increase the reliability of a system as a whole. This report addressos the issue of providing software for reliable distributed systems. In particular, we examine how to program a system so that the software continues to work in tho face of a variety of failures of parts of the system. The design presented
67|A majority consensus approach to concurrency control for multiple copy databases|A “majority consensus ” algorithm which represents a new solution to the update synchronization problem for multiple copy databases is presented. The algorithm embodies distributed control and can function effectively in the presence of communication and database site outages. The correctness of the algorithm is demonstrated and the cost of using it is analyzed. Several examples that illustrate aspects of the algorithm operation are included in the Appendix.
68|State Restoration in Systems of Communicating Processes|Abstract-In systems of asynchronous processes using messagelists with SEND-RECEIVE primitives for interprocess communication recovery primitives are defined to perform state restoration: MARK saves a particular point in the execution of the program; RESTORE resets the system state to an earlier point (saved by MARK); and PURGE discards redundant information when it is no longer needed for possible state restoration. Errors may be propagated through the system, requiring state restoration also to be propagated. Different types of propagation of state restoration are identified. Data structures and procedures are sketched that Implement the recovery primitives. In ill-structured systems the domino effect can occur, resulting in a catastrophic avalanche of backup activity and causing many messagelist operations to be undone. Sufficient conditions are developed for a system to be domino-free. Explicit bounds on the amount of unnecessary restoration are determined for certain classes of systems, including systems where the sequence of recovery and messagelist primitives is described by the regular expression (MARK; RECEIVE*; SEND*)*. Index Terms-Backup, domino effect, error recovery, parallel backtralcking, process communication, recovery blocks, state restoration. I.
69|Recovery techniques for database systems|A survey of techniques and tools used in filing systems, database systems, and operating systems for recovery, backing out, restart, the mamtenance of consistency, and for the provismn of crash resistance is given. A particular view on the use of recovery techmques in a database system and a
70|On Linguistic Support for Distributed Programs|Technological advances have made it possible to construct systems from collections of computers connected by a network. At present, however, there is little support for the construction and execution of software to run on such a system. Our research concerns the development of an integrated language/system whose goal is to provide the needed support. This paper discusses a number of issues that must be addressed in such a language. The major focus of our work and this paper is support for the construction of robust software that survives node, network, and media failures.
72|A theory of communicating sequential processes|  A mathematical model for communicating sequential processes is given, and a number of its interesting and useful properties are stated and proved. The possibilities of nondetermimsm are fully taken into account.
73|Statecharts: A Visual Formalism For Complex Systems|We present a broad extension of the conventional formalism of state machines and state diagrams, that is relevant to the specification and design of complex discrete-event systems, such as multi-computer real-time systems, communication protocols and digital control units. Our diagrams, which we call statecharts, extend conventional state-transition diagrams with essentially three olements, dealing, respectively, with the notions of hierarchy, concurrency and communication. These transform the language of state diagrams into a highly structured&#039; and economical description language. Statecharts are thus compact and expressive--small diagrams can express complex behavior--as well as compositional and modular. When coupled with the capabilities of computerized graphics, statecharts enable viewing the description at different levels of detail, and make even very large specifications manageable and comprehensible. In fact, we intend to demonstrate here that statecharts counter many of the objections raised against conventional state diagrams, and thus appear to render specification by diagrams an attractive and plausible approach. Statecharts can be used either as a stand-alone behavioral description or as part of a more general design methodology that deals also with the system&#039;s other aspects, such as functional decomposition and data-flow specification. We also discuss some practical experience that was gained over the last three years in applying the statechart formalism to the specification of a particularly complex system.
74|A calculus of mobile processes, I|We present the a-calculus, a calculus of communicating systems in which one can naturally express processes which have changing structure. Not only may the component agents of a system be arbitrarily linked, but a communication between neighbours may carry information which changes that linkage. The calculus is an extension of the process algebra CCS, following work by Engberg and Nielsen, who added mobility to CCS while preserving its algebraic properties. The rr-calculus gains simplicity by removing all distinction between variables and constants; communication links are identified by names, and computation is represented purely as the communication of names across links. After an illustrated description of how the n-calculus generalises conventional process algebras in treating mobility, several examples exploiting mobility are given in some detail. The important examples are the encoding into the n-calculus of higher-order functions (the I-calculus and com-binatory algebra), the transmission of processes as values, and the representation of data structures as processes. The paper continues by presenting the algebraic theory of strong bisimilarity and strong equivalence, including a new notion of equivalence indexed by distinctions-i.e., assumptions of inequality among names. These theories are based upon a semantics in terms of a labeled transition system and a notion of strong bisimulation, both of which are expounded in detail in a companion paper. We also report briefly on work-in-progress based upon the corresponding notion of weak bisimulation, in which internal actions cannot be observed.  
75|Symbolic Model Checking: 10^20 States and Beyond|Many different methods have been devised for automatically verifying finite state systems by examining state-graph models of system behavior. These methods all depend on decision procedures that explicitly represent the state space using a list or a table that grows in proportion to the number of states. We describe a general method that represents the state space symbolical/y instead of explicitly. The generality of our method comes from using a dialect of the Mu-Calculus as the primary specification language. We describe a model checking algorithm for Mu-Calculus formulas that uses Bryant’s Binary Decision Diagrams (Bryant, R. E., 1986, IEEE Trans. Comput. C-35) to represent relations and formulas. We then show how our new Mu-Calculus model checking algorithm can be used to derive efficient decision procedures for CTL model checking, satistiability of linear-time temporal logic formulas, strong and weak observational equivalence of finite transition systems, and language containment for finite w-automata. The fixed point computations for each decision procedure are sometimes complex. but can be concisely expressed in the Mu-Calculus. We illustrate the practicality of our approach to symbolic model checking by discussing how it can be used to verify a simple synchronous pipeline circuit.  
77|Specifying Distributed Software Architectures|There is a real need for clear and sound design specifications of distributed systems at the architectural level. This is the level of the design which deals with the high-level organisation of computational elements and the interactions between those elements. The paper presents the Darwin notation for specifying this high-level organisation. Darwin is in essence a declarative binding language which can be used to define hierarchic compositions of interconnected components. Distribution is dealt with orthogonally to system structuring. The language supports the specification of both static structures and dynamic structures which may evolve during execution. The central abstractions managed by Darwin are components and services. Services are the means by which components interact. In addition to its use in specifying the architecture of a distributed system, Darwin has an operational semantics for the elaboration of specifications such that they may be used at runtime to di...
78|Specification and analysis of system architecture using Rapide|  Rapide is an event-based concurrent, object-oriented language specifically designed for prototyping system architectures. Two principle design goals are (1) to provide constructs for defining executable prototypes of architectures, and (2) to adopt an execution model in which the concurrency, synchronization, dataflow, and timing properties of a prototype are explicitly represented. This paper describes the partially ordered event set (poset) execution model and outlines with examples some of the event-based features for defining communication architectures and relationships between architectures. Various features of Rapide are illustrated by excerpts from a prototype of the X/Open distributed transaction processing reference architecture.
79|A Formal Approach to Software Architecture|As software systems become more complex, the overall system structure---or software architecture---becomes a central design problem. A system&#039;s architecture provides a model of the system that suppresses implementation detail, allowing the architect to concentrate on the analyses and decisions that are most crucial to structuring the system to satisfy its requirements.  Unfortunately, current representations of software architecture are informal and ad hoc. While architectural concepts are often embodied in infrastructure to support specific architectural styles and in the initial conceptualization of a system configuration, the lack of an explicit, independently-characterized architecture or architectural style significantly limits the benefits of software architectural design in current practice.  In this dissertation, I show that an Architecture Description Language based on a formal, abstract model of system behavior can provide a practical means of describing and analyzing softwar...
80|Abstractions for Software Architecture and Tools to Support Them|Architectures for software use rich abstractions and idioms to describe system components, the nature of interactions among the components, and the patterns that guide the composition  of components into systems. These abstractions are higher-level than the elements usually  supported by programming languages and tools. They capture packaging and interaction issues  as well as computational functionality. Well-established (if informal) patterns guide architectural design of systems. We sketch a model for defining architectures and present an implementation of the basic level of that model. Our purpose is to support the abstractions  used in practice by software designers. The implementation provides a testbed for experiments  with a variety of system construction mechanisms. It distinguishes among different  types of components and different ways these components can interact. It supports abstract  interactions such as data flow and scheduling on the same footing as simple procedure...
81|Petri Nets|Over the last decade, the Petri net has gamed increased usage and acceptance as a basic model of systems of asynchronous concurrent computation. This paper surveys the basic concepts and uses of Petm nets. The structure of Petri nets, their markings and execution, several examples of Petm net models of computer hardware and software, and
82|Regular Types for Active Objects|Previous work on type-theoretic foundations for object-oriented programming languages has mostly focused on applying or extending functional type theory to functional &#034;objects.&#034; This approach, while benefiting from a vast body of existing literature, has the disadvantage of dealing with state change either in a roundabout way or not at all, and completely sidestepping issues of concurrency. In particular, dynamic issues of non-uniform service availability and conformance to protocols are not addressed by functional types. We propose a new type framework that characterizes objects as regular (finite state) processes that provide guarantees of service along public channels. We also propose a new notion of subtyping for active objects, based on Brinksma&#039;s notion of extension, that extends Wegner and Zdonik&#039;s &#034;principle of substitutability&#034; to non-uniform service availability. Finally, we formalize what it means to &#034;satisfy a client&#039;s expectations,&#034; and we show how regular types canbe used...
83|The Polylith Software Bus|We describe a system called Polylith that helps programmers prepare and interconnect mixed-language software components for execution in heterogeneous environments. Polylith&#039;s principal benefit is that programmers are free to implement functional requirements separately from their treatment of interfacing requirements; this means that once an application has been developed for use in one execution environment (such as a distributed network) it can be adapted for reuse in other environments (such as a shared-memory multiprocessor) by automatic techniques. This flexibility is provided without loss of performance. We accomplish this by creating a new run-time organization for software. An abstract decoupling agent, called the software bus, is introduced between the system components. Heterogeneity in language and architecture is accommodated since program units are prepared to interface directly to the bus, not to other program units. Programmers specify application structure in terms of ...
85|Formal Specification and Analysis of Software Architectures Using the Chemical Abstract Machine Model|We are exploring an approach to formally specifying and analyzing software architectures that is based on viewing software systems as chemicals whose reactions are controlled by explicitly stated rules. This powerful metaphor was devised in the domain of theoretical computer science by Banatre and Le M&#039;etayer and then reformulated as the Chemical Abstract Machine, or CHAM, by Berry and Boudol. The CHAM formalism provides a framework for developing operational specifications that does not bias the described system toward any particular computational model. It also encourages the construction and use of modular specifications at different levels of detail. We illustrate the use of the CHAM for architectural description and analysis by applying it to two different architectures for a simple, but familiar, software system, the multiphase compiler. 
86|Software Interconnection Models|We present a formulation of interconnection models and present the unit and syntactic models --- the primary models used for managing the evolution of large software systems. We discuss various tools that use these models and evaluate how well these models support the management of system evolution. We then introduce the semantic interconnection model. The semantic interconnection model incorporates the advantages of the unit and syntactic interconnection models and provides extremely useful extensions to them. By refining the grain of interconnections to the level of semantics (that is, to the predicates that define aspects of behavior) we provide tools that are better suited to manage the details of evolution in software systems and that provide a better understanding of the implications of changes. We do this by using the semantic interconnection model to formalize the semantics of program construction, the semantics of changes, and the semantics of version equivalence and compatibi...
87|Formulations and Formalisms in  Software Architecture|  Software architecture is the level of software design that addresses the overall structure and properties of software systems. It provides a focus for certain aspects of design and development that are not appropriately addressed within the constituent modules. Architectural design depends heavily on accurate specifications of subsystems and their interactions. These specifications must cover a wide variety of properties, so the specification notations and associated methods must be selected or developed to match the properties of interest. Unfortunately, the available formal methods are only a partial match for architectural needs, which entail description of structure, packaging, environmental assumptions, representation, and performance as well as functionality. A prerequisite for devising or selecting a formal method is sound understanding of what needs to be formalized. For software architecture, much of this understanding is arising through progressive codification, which begins with real-world examples and creates progressively more precise models that eventually support formalization. This paper explores the progressive
88|Partial Orderings of Event Sets and Their Application to Prototyping Concurrent, Timed Systems|Rapide is a concurrent, object-oriented language specifically designed for prototyping large concurrent systems. One of the principle design goals has been to adopt a computation model in which the synchronization, concurrency, dataflow, and timing aspects of a prototype are explicitly represented and easily accessible both to the prototype itself and to the prototyper. This paper describes the partially ordered event set (poset) computation model, and the features of Rapide for using posets in reactive prototypes and for automatically checking posets. An example prototyping scenario illustrates uses of the poset computation model, with and without timing.  keywords: Rapide, partial orders, prototyping, concurrency, real-time, architecture, programming languages. Principle contact: Larry M. Augustin ERL 414, M/C 4055 Computer Systems Laboratory Stanford University Stanford, CA 94305 Tel: (415) 723--9285 Fax: (415) 725--6949 Email: lma@dayton.Stanford.EDU 1  This research was supported ...
89|Static Scheduling of Synchronous Data Flow Programs for Digital Signal Processing|Large grain data flow (LGDF) programming is natural and convenient for describing digital signal processing (DSP) systems, but its runtime overhead is costly in real time or cost-sensitive applications. In some situations, designers are not willing to squander computing resources for the sake of program-mer convenience. This is particularly true when the target machine is a programmable DSP chip. However, the runtime overhead inherent in most LGDF implementations is not required for most signal processing systems because such systems are mostly synchronous (in the DSP sense). Synchronous data flow (SDF) differs from traditional data flow in that the amount of data produced and consumed by a data flow node is specified a priori for each input and output. This is equivalent to specifying the relative sample rates in signal processing system. This means that the scheduling of SDF nodes need not be done at runtime, but can be done at compile time (statically), so the runtime overhead evaporates. The sample rates can all be different, which is not true of most current data-driven digital signal processing programming methodologies. Synchronous data flow is closely related to computation graphs, a special case of Petri nets. This self-contained paper develops the theory necessary to statically schedule SDF programs on single or multiple proces-sors. A class of static (compile time) scheduling algorithms is proven valid, and specific algorithms are given for scheduling SDF systems onto single or multiple processors. 
90|Randomized Algorithms|Randomized algorithms, once viewed as a tool in computational number theory, have by now found widespread application. Growth has been fueled by the two major benefits of randomization: simplicity and speed. For many applications a randomized algorithm is the fastest algorithm available, or the simplest, or both. A randomized algorithm is an algorithm that uses random numbers to influence the choices it makes in the course of its computation. Thus its behavior (typically quantified as running time or quality of output) varies from
92|Expected Time Bounds for Selection|A new selection algorithm is presented which is shown to be very efficient on the average, both theoretically and practically. The number of comparisons used to select the ith smallest of n numbers is n q- min(i,n--i) q- o(n). A lower bound within 9 percent of the above formula is also derived.
93|Efficient randomized pattern-matching algorithms|We present randomized algorithms to solve the
following string-matching problem and some of its generalizations: Given a string X of length n (the pattern) and a string Y (the text), find the first occurrence of X as a consecutive block within Y.  The algorithms represent strings of length n by much shorter strings called fingerprints, and achieve their efficiency by manipulating fingerprints instead of longer strings.  The algorithms require a constant number of storage locations, and essentially run in real time.  They are conceptually simple and easy to implement.  The method readily generalizes to higher-dimensional pattern-matching problems.
94|Approximating the permanent|Abstract. A randomised approximation scheme for the permanent of a 0-1 matrix is presented. The task of estimating a permanent is reduced to that of almost uniformly generating perfect matchings in a graph; the latter is accomplished by simulating a Markov chain whose states are the matchings in the graph. For a wide class of 0-1 matrices the approximation scheme is fully-polynomial, i.e., runs in time polynomial in the size of the matrix and a parameter that controls the accuracy of the output. This class includes all dense matrices (those that contain sufficiently many l’s) and almost all sparse matrices in some reasonable probabilistic model for 0-1 matrices of given density. For the approach sketched above to be computationally efficient, the Markov chain must be rapidly mixing: informally, it must converge in a short time to its stationary distribution. A major portion of the paper is devoted to demonstrating that the matchings chain is rapidly mixing, apparently the first such result for a Markov chain with genuinely complex structure. The techniques used seem to have general applicability, and are applied again in the paper to validate a fully-polynomial randomised approximation scheme for the partition function of an arbitrary monomer-dimer system.
95|The NP-completeness column: an ongoing guide|This is the nineteenth edition of a (usually) quarterly column that covers new developments in the theory of NP-completeness. The presentation is modeled on that used by M. R. Garey and myself in our book &#034;Computers and Intractability: A Guide to the Theory of NP-Completeness,&#034; W. H. Freeman &amp; Co., New York, 1979 (hereinafter referred to as &#034;[G&amp;J]&#034;; previous columns will be referred to by their dates). A background equivalent to that provided by [G&amp;J] is assumed, and, when appropriate, cross-references will be given to that book and the list of problems (NP-complete and harder) presented there. Readers who have results they would like mentioned (NP-hardness, PSPACE-hardness, polynomial-time-solvability, etc.) or open problems they would like publicized, should
96|Matching is as Easy as Matrix Inversion|A new algorithm for finding a maximum matching in a general graph is presented; its special feature being that the only computationally non-trivial step required in its execution is the inversion of a single integer matrix. Since this step can be parallelized, we get a simple parallel (RNC2) algorithm. At the heart of our algorithm lies a probabilistic lemma, the isolating lemma. We show applications of this lemma to parallel computation and randomized reductions. 
97|An introduction to randomized algorithms|Research conducted over the past fifteen years has amply demonstrated the advantages of algorithms that make random choices in the course of their execution. This paper presents a wide variety of examples intended to illustrate the range of applications of randomized algorithms, and the general principles and approaches that are of greatest use in their construction. The examples are drawn from many areas, including number theory, algebra, graph theory, pattern matching, selection, sorting, searching, computational geometry, combinatorial enumeration, and parallel and distributed computation. 1. Foreword This paper is derived from a series of three lectures on randomized algorithms presented by the author at a conference on combinatorial mathematics and algorithms held at George Washington University in May, 1989. The purpose of the paper is to convey, through carefully selected examples, an understanding of the nature of randomized algorithms, the range of their applications and the principles underlying their construction. It is not our goal to be encyclopedic, and thus the paper should not be regarded as a comprehensive survey of the subject. This paper would not have come into existence without the magnificent efforts of Professor Rodica Simion, the organizer of the conference at George Washington University. Working from the tape-recorded lectures, she created a splendid transcript that served as the first draft of the paper. Were it not for her own reluc-tance she would be listed as my coauthor.
98|On optimistic methods for concurrency control|Most current approaches to concurrency control in database systems rely on locking of data objects as a control mechanism. In this paper, two families of nonlocking concurrency controls are presented. The methods used are “optimistic ” in the sense that they rely mainly on transaction backup as a control mechanism, “hoping ” that conflicts between transactions will not occur. Applications for which these methods should be more efficient than locking are discussed.
99|Efficient Locking for Concurrent Operations on B-Trees|The B-tree and its variants have been found to be highly useful (both theoretically and in practice) for storing large amounts ofinformation, especially on secondary storage devices. We examine the problem of overcoming the inherent difficulty of concurrent operations on such structures, using a practical storage model. A single additional “link ” pointer in each node allows a process to easily recover from tree modifications performed by other concurrent processes. Our solution compares favorably with earlier solutions in that the locking scheme is simpler (no read-locks are used) and only a (small) constant number of nodes are locked by any update process at any given time. An informal correctness proof for our system is given,
100|Concurrent manipulation of binary search trees|The concurrent manipulation of a binary search tree is considered in this paper. The systems presented can support any number of concurrent processes which perform searching, insertion, deletion, and rotation (reorganization) on the tree, but allow any process to lock only a constant number of nodes at any time. Also, in the systems, searches are essentially never blocked. The concurrency control techniques introduced in the paper include the use of special nodes and pointers to redirect searches, and the use of copies of sections of the tree to introduce many changes simultaneously and therefore avoid unpredictable interleaving. Methods developed in this paper may provide new insights into other problems in the area of concurrent database manipulation.
101|An optimality theory of concurrency control for databases|In many database applications it is desirable that the database system be time-shared among multiple users who access the database in an interactive way. In such a systewi the arriving requests for the execution of steps in
102|Eraser: a dynamic data race detector for multithreaded programs|Multi-threaded programming is difficult and error prone. It is easy to make a mistake in synchronization that produces a data race, yet it can be extremely hard to locate this mistake during debugging. This paper describes a new tool, called Eraser, for dynamically detecting data races in lock-based multi-threaded programs. Eraser uses binary rewriting techniques to monitor every shared memory reference and verify that consistent locking behavior is observed. We present several case studies, including undergraduate coursework and a multi-threaded Web search engine, that demonstrate the effectiveness of this approach. 1
103|Atom: A system for building customized program analysis tools|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
104|Monitors: An Operating System Structuring Concept|This is a digitized copy derived from an ACM copyrighted work. It is not guaranteed to be an accurate copy of the author&#039;s original work. This paper develops Brinch-Hansen&#039;s concept of a monitor as a method of structuring an operating system. It introduces a form of synchronization, describes a possible rnctltotl of implementation in terms of semaphorcs and gives a suitable proof rule. Illustrative examples include a single rcsourcc scheduler, a bounded buffer, an alarm clock, a buffer pool, a disk head optimizer, and a version of the problem of readers and writers. Key Words and Phrases: monitors, operating systems,schcduling, mutual exclusion, synchronization, system implementation langua yes, structured multiprogramming CR Categories:
105|Extensibility, safety and performance in the SPIN operating system|This paper describes the motivation, architecture and performance of SPIN, an extensible operating system. SPIN provides an extension infrastructure, together with a core set of extensible services, that allow applications to safely change the operating system&#039;s interface and implementation. Extensions allow an application to specialize the underlying operating system in order to achieve a particular level of performance and functionality. SPIN uses language and link-time mechanisms to inexpensively export ne-grained interfaces to operating system services. Extensions are written in a type safe language, and are dynamically linked into the operating system kernel. This approach o ers extensions rapid access to system services, while protecting the operating system code executing within the kernel address space. SPIN and its extensions are written in Modula-3 and run on DEC Alpha workstations. 1
106|Petal: Distributed Virtual Disks|The ideal storage system is globally accessible, always available, provides unlimited performance and capacity for a large number of clients, and requires no management. This paper describes the design, implementation, and performance of Petal, a system that attempts to approximate this ideal in practice through a novel combination of features. Petal consists of a collection of networkconnected servers that cooperatively manage a pool of physical disks. To a Petal client, this collection appears as a highly available block-level storage system that provides large abstract containers called virtual disks. A virtual disk is globally accessible to all Petal clients on the network. A client can create a virtual disk on demand to tap the entire capacity and performance of the underlying physical resources. Furthermore, additional resources, such as servers and disks, can be automatically incorporated into Petal. We have an initial Petal prototype consisting of four 225 MHz DEC 3000/700 work...
107|Shasta: A LowOverhead Software-Only Approach to Fine-Grain Shared Memory|Digital Equipment Corporation This paper describes Shasta, a system that supports a shared address space in software on clusters of computers with physically distributed memory. A unique aspect of Shasta compared to most other software distributed shared memory systems is that shared data can be kept coherent at a fine granularity. In addition, the system allows the coherence granularity to vary across different shared data structures in a single application. Shasta implements the shared address space by transparently rewriting the application executable to intercept loads and stores. For each shared load or store, the inserted code checks to see if the data is available locally and communicates with other processors if necessary. The system uses numerous techniques to reduce the run-time overhead of these checks. Since Shasta is implemented entirely in software,
108|Experience with Processes and Monitors in Mesa|The use of monitors for describing concurrency has been much discussed in the literature. When monitors are used in real systems of any size, however, a number of problems arise which have not been adequately dealt with: the semantics of nested monitor calls; the various ways of defining the meaning of WAIT; priority scheduling; handling of timeouts, aborts and other exceptional conditions; interactions with process creation and destruction; monitoring large numbers of small objects. These problems are addressed by the facilities described here for concurrent programming in Mesa. Experience with several substantial applications gives us some confidence in the validity of our solutions.
109|On-the-fly Detection of Data Races for Programs with Nested Fork-Join Parallelism|Detecting data races in shared-memory parallel programs is an important debugging problem. This paper presents a new protocol for run-time detection of data races in executions of shared-memory programs with nested fork-join parallelism and no other interthread synchronization. This protocol has significantly smaller worst-case run-time overhead than previous techniques. The worst-case space required by our protocol when monitoring an execution of a program P is O(V N), where V is the number of shared variables in P , and N is the maximum dynamic nesting of parallel constructs in P &#039;s execution. The worst-case time required to perform any monitoring operation is O(N). We formally prove that our new protocol always reports a non-empty subset of the data races in a monitored program execution and describe how this property leads to an e#ective debugging strategy.
110|Online Data-Race Detection via Coherency Guarantees|We present the design and evaluation of an on-thefly data-race-detection technique that handles applications written for the lazy release consistent (LRC) shared memory model. We require no explicit association between synchronization and shared memory. Hence, shared accesses have to be tracked and compared at the minimum granularity of data accesses, which is typically a single word.  The novel aspect of this system is that we are able to leverage information used to support the underlying memory abstraction to perform on-the-fly data-race detection, without compiler support. Our system consists of a minimally modified version of the CVM distributed shared memory system, and instrumentation code inserted by the ATOM code re-writer.  We present an experimental evaluation of our technique by using our system to look for data races in four unaltered programs. Our system correctly found read-write data races in a program that allows unsynchronized read access to a global tour bound, and a write-write race in a program from a standard benchmark suite. Overall, our mechanism reduced program performance by approximately a factor of two.   
111|Compile-time Support for Efficient Data Race Detection in Shared-Memory Parallel Programs|Data race detection strategies based on software runtime monitoring are notorious for dramatically inflating execution times of shared-memory parallel programs. Without significant reductions in the execution overhead incurred when using these techniques, there is little hope that they will be widely used. A promising approach to this problem is to apply compile-time analysis to identify variable references that need not be monitored at run time because they will never be involved in a data race. In this paper we describe eraser, a data race instrumentation tool that uses aggressive program analysis to prune the number of references to be monitored. To quantify the effectiveness of our analysis techniques, we compare the overhead of race detection with three levels of compile-time analysis ranging from little analysis to aggressive interprocedural analysis for a suite of test programs. For the programs tested, using interprocedural analysis and dependence analysis dramatically reduced ...
112|Distributed Database Systems |this article, we discuss the fundamentals of distributed DBMS technology. We address the data distribution and architectural design issues as well as the algorithms that need to be implemented to provide the basic DBMS functions such as query processing, concurrency control, reliability, and replication control.
113|DISTRIBUTED SYSTEMS|Growth of distributed systems has attained unstoppable momentum. If we better understood how to think about, analyze, and design distributed systems, we could direct their implementation with more confidence.
114|Composable memory transactions|Atomic blocks allow programmers to delimit sections of code as ‘atomic’, leaving the language’s implementation to enforce atomicity. Existing work has shown how to implement atomic blocks over word-based transactional memory that provides scalable multiprocessor performance without requiring changes to the basic structure of objects in the heap. However, these implementations perform poorly because they interpose on all accesses to shared memory in the atomic block, redirecting updates to a thread-private log which must be searched by reads in the block and later reconciled with the heap when leaving the block. This paper takes a four-pronged approach to improving performance: (1) we introduce a new ‘direct access ’ implementation that avoids searching thread-private logs, (2) we develop compiler optimizations to reduce the amount of logging (e.g. when a thread accesses the same data repeatedly in an atomic block), (3) we use runtime filtering to detect duplicate log entries that are missed statically, and (4) we present a series of GC-time techniques to compact the logs generated by long-running atomic blocks. Our implementation supports short-running scalable concurrent benchmarks with less than 50 % overhead over a non-thread-safe baseline. We support long atomic blocks containing millions of shared memory accesses with a 2.5-4.5x slowdown. Categories and Subject Descriptors D.3.3 [Programming Languages]:
115|Linearizability: a correctness condition for concurrent objects|A concurrent object is a data object shared by concurrent processes. Linearizability is a correctness condition for concurrent objects that exploits the semantics of abstract data types. It permits a high degree of concurrency, yet it permits programmers to specify and reason about concurrent objects using known techniques from the sequential domain. Linearizability provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object’s operations can be given by pre- and post-conditions. This paper defines linearizability, compares it to other correctness conditions, presents and demonstrates a method for proving the correctness of implementations, and shows how to reason about concurrent objects, given they are linearizable.
116|Language Support for Lightweight Transactions|Concurrent programming is notoriously di#cult. Current abstractions are intricate and make it hard to design computer systems that are reliable and scalable. We argue that these problems can be addressed by moving to a declarative style of concurrency control in which programmers directly indicate the safety properties that they require.
117|Software transactional memory for dynamic-sized data structures|We propose a new form of software transactional memory (STM) designed to support dynamic-sized data structures, and we describe a novel non-blocking implementation. The non-blocking property we consider is obstruction-freedom. Obstruction-freedom is weaker than lock-freedom; as a result, it admits substantially simpler and more efficient implementations. A novel feature of our obstruction-free STM implementation is its use of modular contention managers to ensure progress in practice. We illustrate the utility of our dynamic STM with a straightforward implementation of an obstruction-free red-black tree, thereby demonstrating a sophisticated non-blocking dynamic data structure that would be difficult to implement by other means. We also present the results of simple preliminary performance experiments that demonstrate that an &#034;early release &#034; feature of our STM is useful for reducing contention, and that our STM lends itself to the effective use of modular contention managers. 
118|Virtualizing Transactional Memory|Writing concurrent programs is difficult because of the complexity of ensuring proper synchronization. Conventional lock-based synchronization suffers from wellknown limitations, so researchers have considered nonblocking transactions as an alternative. Recent hardware proposals have demonstrated how transactions can achieve high performance while not suffering limitations of lock-based mechanisms. However, current hardware proposals require programmers to be aware of platform-specific resource limitations such as buffer sizes, scheduling quanta, as well as events such as page faults, and process migrations. If the transactional model is to gain wide acceptance, hardware support for transactions must be virtualized to hide these limitations in much the same way that virtual memory shields the programmer from platform-specific limitations of physical memory. This paper proposes Virtual Transactional Memory (VTM), a user-transparent system that shields the programmer from various platform-specific resource limitations. VTM maintains the performance advantage of hardware transactions, incurs low overhead in time, and has modest costs in hardware support. While many system-level challenges remain, VTM takes a step toward making transactional models more widely acceptable.  
119|Speculative Lock Elision: Enabling Highly Concurrent Multithreaded Execution|Serialization of threads due to critical sections is a fundamental bottleneck to achieving high performance in multithreaded programs. Dynamically, such serialization may be unnecessary because these critical sections could have safely executed concurrently without locks. Current processors cannot fully exploit such parallelism because they do not have mechanisms to dynamically detect such false inter-thread dependences. We propose Speculative Lock Elision (SLE), a novel micro-architectural technique to remove dynamically unnecessary lock-induced serialization and enable highly concurrent multithreaded execution. The key insight is that locks do not always have to be acquired for a correct execution. Synchronization instructions are predicted as being unnecessary and elided. This allows multiple threads to concurrently execute critical sections protected by the same lock. Misspeculation due to inter-thread data conflicts is detected using existing cache mechanisms and rollback is used for recovery. Successful speculative elision is validated and committed without acquiring the lock. SLE can be implemented entirely in microarchitecture without instruction set support and without system-level modifications, is transparent to programmers, and requires only trivial additional hardware support. SLE can provide programmers a fast path to writing correct high-performance multithreaded programs.  
120|Transactional Lock-Free Execution of Lock-Based Programs|This paper is motivated by the difficulty in writing correct high-performance programs. Writing shared-memory multithreaded programs imposes a complex trade-off between programming ease and performance, largely due to subtleties in coordinating access to shared data. To ensure correctness programmers often rely on conservative locking at the expense of performance. The resulting serialization of threads is a performance bottleneck. Locks also interact poorly with thread scheduling and faults, resulting in poor system performance.
121|Modern Concurrency Abstractions for C#|Polyphonic C# is an extension of the C# language with new asynchronous concurrency constructs, based on the join calculus. We describe the design and implementation of the language and give examples of its use in addressing a range of concurrent programming problems. 
122|Thin Locks: Featherweight Synchronization for Java|Language-supported synchronization is a source of serious performance problems in many Java programs. Even singlethreaded applications may spend up to half their time performing useless synchronization due to the thread-safe nature of the Java libraries. We solve this performance problem with a new algorithm that allows lock and unlock operations to be performed with only a few machine instructions in the most common cases. Our locks only require a partial word per object, and were implemented without increasing object size. We present measurements from our implementation in the JDK 1.1.2 for AIX, demonstrating speedups of up to a factor of 5 in micro-benchmarks and up to a factor of 1.7 in real programs. 1 Introduction Monitors [5] are a language-level construct for providing mutually exclusive access to shared data structures in a multithreaded environment. However, the overhead required by the necessary locking has generally restricted their use to relatively &#034;heavy-weight&#034; object...
123|Transactional monitors for concurrent objects |Abstract. Transactional monitors are proposed as an alternative to monitors based on mutual-exclusion synchronization for object-oriented programming languages. Transactional monitors have execution semantics similar to mutualexclusion monitors but implement monitors as lightweight transactions that can be executed concurrently (or in parallel on multiprocessors). They alleviate many of the constraints that inhibit construction of transparently scalable and robust applications. We undertake a detailed study of alternative implementation schemes for transactional monitors. These different schemes are tailored to different concurrent access patterns, and permit a given application to use potentially different implementations in different contexts. We also examine the performance and scalability of these alternative approaches in the context of the Jikes Research Virtual Machine, a state-of-the-art Java implementation. We show that transactional monitors are competitive with mutualexclusion synchronization and can outperform lock-based approaches up to five times on a wide range of workloads. 1
124|An Efficient Meta-lock for Implementing Ubiquitous Synchronization|Programs written in concurrent object-oriented languages, espe-cially ones that employ thread-safe reusable class libraries, can execute synchronization operations (lock, notify, etc.) at an amaz-ing rate. Unless implemented with utmost care, synchronization can become a performance bottleneck. Furthermore, in languages where every object may have its own monitor, per-object space overhead must be minimized. To address these concerns, we have developed a meta-lock to mediate access to synchronization data. The meta-lock is fast (lock + unlock executes in 11 SPARCTM architecture instructions), compact (uses only two bits of space), robust under contention (no busy-waiting), and flexible (supports a variety of higher-level synchronization operations). We have vali-dated the meta-lock with an implementation of the synchronization operations in a high-performance product-quality JavaTM virtual machine and report performance data for several large programs.
125|Design tradeoffs in modern software transactional memory systems|Software Transactional Memory (STM) is a generic nonblocking synchronization construct that enables automatic conversion of correct sequential objects into correct concurrent objects. Because it is nonblocking, STM avoids traditional performance and correctness problems due to thread failure, preemption, page faults, and priority inversion. In this paper we compare and analyze two recent objectbased STM systems, the DSTM of Herlihy et al. and the FSTM of Fraser, both of which support dynamic transactions, in which the set of objects to be modified is not known in advance. We highlight aspects of these systems that lead to performance tradeoffs for various concurrent data structures. More specifically, we consider object ownership acquisition semantics, concurrent object referencing style, the overhead of ordering and bookkeeping, contention management versus helping semantics, and transaction validation. We demonstrate for each system simple benchmarks on which it outperforms the other by a significant margin. This in turn provides us with a preliminary characterization of the applications for which each system is best suited.  
126|Lock Coarsening: Eliminating Lock Overhead in Automatically Parallelized Object-Based Programs|Atomic operations are a key primitive in parallel computing systems. The standard implementation mechanism for atomic operations uses mutual exclusion locks. In an object-based programming system the natural granularity is to give each object its own lock. Each operation can then make its execution atomic by acquiring and releasing the lock for the object that it accesses. But this fine lock granularity may have high synchronization overhead because it maximizes the number of executed acquire and release constructs. To achieve good performance it may be necessary to reduce the overhead by coarsening the granularity at which the computation locks objects. In this paper we describe a static analysis technique --- lock coarsening --- designed to automatically increase the lock granularity in object-based programs with atomic operations. We have implemented this technique in the context of a parallelizing compiler for irregular, object-based programs and used it to improve the generated pa...
127|Transactional Execution of Java Programs|Parallel programming is difficult due to the complexity of dealing with conventional lock-based synchronization. To simplify parallel programming, there have been a number of proposals to support transactions directly in hardware and eliminate locks completely. Although hardware support for transactions has the potential to completely change the way parallel programs are written, initially transactions will be used to execute existing parallel programs. In this paper we investigate the implications of using transactions to execute existing parallel Java programs. Our results show that transactions can be used to support all aspects of Java multithreaded programs. Moreover, the conversion of a lock-based application into transactions is largely straightforward. The performance that these converted applications achieve is equal to or sometimes better than the original lock-based implementation.
128|Relaxed balanced red-black trees|Abstract. Relaxed balancing means that, in a dictionary stored as a balanced tree, the necessary rebalancing after updates may be delayed. This is in contrast to strict balancing meaning that rebalancing is performed immediately after the update. Relaxed balancing is important for efficiency in highly dynamic applications where updates can occur in bursts. The rebalancing tasks can be performed gradually after all urgent updates, allowing the concurrent use of the dictionary even though the underlying tree structure is not completely in balance. In this paper we propose a new scheme of how to make known rebalancing techniques relaxed in an efficient way. The idea is applied to the red-black trees, but can be applied to any class of balanced trees. The key idea is to accumulate insertions and deletions such that they can be settled in arbitrary order using the same rebalancing operations as for standard balanced search trees. As a result it can be shown that the number of needed rebalancing operations known from the strict balancing scheme carry over to relaxed balancing. 1
129|Integrating support for undo with exception handling|One of the important tasks of exception handling is to restore program state and invariants. Studies suggest that this is often done incorrectly. We introduce a new language construct that integrates automated memory recovery with exception handling. When an exception occurs, memory can be automatically restored to its previous state. We also provide a mechanism for applications to extend the automatic recovery mechanism with callbacks for restoring the state of external resources. We describe a logging-based implementation and evaluate its effect on performance. The implementation imposes no overhead on parts of the code that do not make use of this feature.
130|Implementing fast Java monitors with relaxed-locks |The Java  Programming Language permits synchronization operations (lock, unlock, wait, notify) on any object. Synchronization is very common in applications and is endemic in the library code upon which applications depend. It is therefore critical that a monitor implementation be both space-efficient and time-efficient. We present a locking protocol, the Relaxed-Lock, that satisfies those requirements. The Relaxed-Lock is reasonably compact, using only one word in the object header. It is fast, requiring in the uncontested case only one atomic compare-and-swap to lock a monitor and no atomic instructions to release a monitor. The Relaxed-Lock protocol is unique in that it permits a benign data race in the monitor unlock path (hence its name) but detects and recovers from the race and thus maintains correct mutual exclusion. We also introduce speculative deflation, a mechanism for releasing a monitor when it is no longer needed. 1
131|The synchronous dataflow programming language LUSTRE|This paper describes the language Lustre, which is a dataflow synchronous language, designed for programming reactive systems --- such as automatic control and monitoring systems --- as well as for describing hardware. The dataflow aspect of Lustre makes it very close to usual description tools in these domains (block-diagrams, networks of operators, dynamical samples-systems, etc: : : ), and its synchronous interpretation makes it well suited for handling time in programs. Moreover, this synchronous interpretation allows it to be compiled into an efficient sequential program. Finally, the Lustre formalism is very similar to temporal logics. This allows the language to be used for both writing programs and expressing program properties, which results in an original program verification methodology. 1 Introduction Reactive systems Reactive systems have been defined as computing systems which continuously interact with a given physical environment, when this environment is unable to sy...
132|Automatic verification of finite-state concurrent systems using temporal logic specifications|We give an efficient procedure for verifying that a finite-state concurrent system meets a specification expressed in a (propositional, branching-time) temporal logic. Our algorithm has complexity linear in both the size of the specification and the size of the global state graph for the concurrent system. We also show how this approach can be adapted to handle fairness. We argue that our technique can provide a practical alternative to manual proof construction or use of a mechanical theorem prover for verifying many finite-state concurrent systems. Experimental results show that state machines with several hundred states can be checked in a matter of seconds.
133|LUSTRE: A declarative language for programming synchronous systems|LUSTRE is a synchronous data-flow language for programming syetema which interact. with their environments in real-time. After an informal presentation of the language, we describe its semantics by means of structural inference rules. Moreover, we ehow how to use this semantics in order to generate efficient, sequential code, namely, a finite state automaton which represents the control of the program. Formal rules for program transformation are also presented.
134|Functional Specification of Time Sensitive Communicating Systems|A formal model and a logical framework for the functional specification of time  sensitive communicating systems and their interacting components is outlined. The  specification method is modular with respect to sequential composition, parallel  composition, and communication feedback. Nondeterminism is included by  underspecification. The application of the specification method to timed  communicating functions is demonstrated. Abstractions from time are studied. In  particular a rational is given for the chosen concepts of the functional specification  technique. The relationship between system models based on nondeterminism and  system models based on explicit time notions is investigated. Forms of reasoning are  considered. The alternating bit protocol is used as a running example.
135|Nonparametric model for background subtraction|Abstract. Background subtraction is a method typically used to seg-ment moving regions in image sequences taken from a static camera by comparing each new frame to a model of the scene background. We present a novel non-parametric background model and a background subtraction approach. The model can handle situations where the back-ground of the scene is cluttered and not completely static but contains small motions such as tree branches and bushes. The model estimates the probability of observing pixel intensity values based on a sample of intensity values for each pixel. The model adapts quickly to changes in the scene which enables very sensitive detection of moving targets. We also show how the model can use color information to suppress detec-tion of shadows. The implementation of the model runs in real-time for both gray level and color imagery. Evaluation shows that this approach achieves very sensitive detection with very low false alarm rates. Key words: visual motion, active and real time vision, motion detection, non-parametric estimation, visual surveillance, shadow detection 1
136|Pfinder: Real-time tracking of the human body|Pfinder is a real-time system for tracking people and interpreting their behavior. It runs at 10Hz on a standard SGI Indy computer, and has performed reliably on thousands of people in many different physical locations. The system uses a multiclass statistical model of color and shape to obtain a 2D representation of head and hands in a wide range of viewing conditions. Pfinder has been successfully used in a wide range of applications including wireless interfaces, video databases, and low-bandwidth coding. 
137|Image segmentation in video sequences: A probabilistic approach|&#034;Background subtraction&#034; is an old technique for finding moving objects in a video sequence---for example, cars driving on a freeway. The idea is that subtracting the current image from a time-averaged background image will leave only nonstationary objects. It is, however, a crude approximation to the task of classifying each pixel of the current image
138|Using adaptive tracking to classify and monitor activities in a site|We describe a vision system that monitors activity in a site over extended periods of time. The system uses a distributed set of sensors to cover the site, and an adaptive tracker detects multiple moving objects in the sensors. Our hypothesis is that motion tracking is sufficient to support a range of computations about site activities. We demonstrate using the tracked motion data: to calibrate the distributed sensors, to construct rough site models, to classify detected objects, to learn common patterns of activity for different object classes, and to detect unusual activities. 
139|Bandera: Extracting Finite-state Models from Java Source Code|Finite-state verification techniques, such as model checking, have shown promise as a cost-effective means for finding defects in hardware designs. To date, the application of these techniques to software has been hindered by several obstacles. Chief among these is the problem of constructing a finite-state model that approximates the executable behavior of the software system of interest. Current best-practice involves handconstruction of models which is expensive (prohibitive for all but the smallest systems), prone to errors (which can result in misleading verification results), and difficult to optimize (which is necessary to combat the exponential complexity of verification algorithms).  In this paper, we describe an integrated collection of program analysis and transformation components, called Bandera, that enables the automatic extraction of safe, compact finite-state models from program source code. Bandera takes as input Java source code and generates a program model in the input language of one of several existing verification tools; Bandera also maps verifier outputs back to the original source code. We discuss the major components of Bandera and give an overview of how it can be used to model check correctness properties of Java programs.  
140|The model checker SPIN|Abstract—SPIN is an efficient verification system for models of distributed software systems. It has been used to detect design errors in applications ranging from high-level descriptions of distributed algorithms to detailed code for controlling telephone exchanges. This paper gives an overview of the design and structure of the verifier, reviews its theoretical foundation, and gives an overview of significant practical applications. Index Terms—Formal methods, program verification, design verification, model checking, distributed systems, concurrency.
141|Interprocedural Slicing Using Dependence Graphs|... This paper concerns the problem of interprocedural slicing---generating a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system dependence  graph, which extends previous dependence representations to incorporate collections of procedures (with procedure calls) rather than just monolithic programs. Our main result is an algorithm for interprocedural slicing that uses the new representation. (It should be noted that our work concerns a somewhat restricted kind of slice: Rather than permitting a program to be sliced with respect to program point p and an arbitrary variable, a slice must be taken with respect to a variable that is defined or used at p.) The chief
142|PVS: A Prototype Verification System|PVS is a prototype system for writing specifications and constructing proofs. Its development has been shaped by our experiences studying or using several other systems and performing a number of rather substantial formal verifications (e.g., [5,6,8]). PVS is fully implemented and freely available. It has been used to construct proofs of nontrivial difficulty with relatively modest amounts of human effort. Here, we describe some of the motivation behind PVS and provide some details of the system. Automated reasoning systems typically fall in one of two classes: those that provide powerful automation for an impoverished logic, and others that feature expressive logics but only limited automation. PVS attempts to tread the middle ground between these two classes by providing mechanical assistance to support clear and abstract specifications, and readable yet sound proofs for difficult theorems. Our goal is to provide mechanically-checked specificati
143|Patterns in Property Specifications for Finite-state Verification|Model checkers and other finite-state verification tools allow developers to detect certain kinds of errors automatically. Nevertheless, the transition of this technology from research to practice has been slow. While there are a number of potential causes for reluctance to adopt such formal methods, we believe that a primary cause is that practitioners are unfamiliar with specification processes, notations, and strategies. In a recent paper, we proposed a pattern-based approach to the presentation, codification and reuse of property specifications for finite-state verification. Since then, we have carried out a survey of available specifications, collecting over 500 examples of property specifications. We found that most are instances of our proposed patterns. Furthermore, we have updated our pattern system to accommodate new patterns and variations of existing patterns encountered in this survey. This paper reports the results of the survey and the current status of our pattern system.
144|Model Checking Java Programs Using Java PathFinder|. This paper describes a translator called Java PathFinder (Jpf), from Java to Promela, the modeling language of the Spin model checker. Jpf translates a given Java program into a Promela model, which then can be model checked using Spin. The Java program may contain assertions, which are translated to similar assertions in the Promela model. The Spin model checker will then look for deadlocks and violations of any stated assertions. Jpf generates a Promela model with the same state space characteristics as the Java program. Hence, the Java program must have a finite and tractable state space. This work should be seen in a broader attempt to make formal methods applicable within NASA&#039;s areas such as space, aviation, and robotics. The work is a continuation of an effort to formally analyze, using Spin, a multi--threaded operating system for the Deep-Space 1 space craft, and of previous work in applying existing model checkers and theorem provers to real applications. Key words: Program...
145|Protocol Verification as a Hardware Design Aid|The role of automatic formal protocol verification  in hardware design is considered. Principles are identified that maximize the benefits of protocol verification while minimizing the labor and computation required. A new protocol description language and verifier (both called Mur&#039;) are described, along with experiences in applying them to two industrial protocols that were developed as  part of hardware designs. 
146|Evaluating Deadlock Detection Methods for Concurrent Software|Static analysis of concurrent programs has been hindered by the well known state explosion problem. Although many different techniques have been proposed to combat this state explosion, there is little empirical data comparing the performance of the methods. This information is essential for assessing the practical value of a technique and for choosing the best method for a particular problem. In this paper, we carry out an evaluation of three techniques for combating the state explosion problem in deadlock detection: reachability search with a partial order state space reduction, symbolic model checking, and inequality necessary conditions. We justify the method used for the comparison, and carefully analyze several sources of potential bias. The results of our evaluation provide valuable data on the kinds of programs to which each technique might best be applied. Furthermore, we believe that the methodological issues we discuss are of general significance in comparison of analysis te...
147|Slicing Software for Model Construction|Applying finite-state verification techniques (e.g., model checking) to software requires that program  source code be translated to a finite-state transition system that safely models program behavior.  Automatically checking such a transition system for a correctness property is typically very costly,  thus it is necessary to reduce the size of the transition system as much as possible. In fact, it is often  the case that much of a program&#039;s source code is irrelevant for verifying a given correctness property.  In this paper, we apply program slicing techniques to remove automatically such irrelevant code  and thus reduce the size of the corresponding transition system models. We give a simple extension of  the classical slicing definition, and prove its safety with respect to model checking of linear temporal  logic (LTL) formulae. We discuss how this slicing strategy fits into a general methodology for deriving  effective software models using abstraction-based program specializati...
148|Formal Analysis of a Space Craft Controller using SPIN|Abstract. This report documents an application of the nite state model checker Spin to formally verify a multi{threaded plan execution programming language. The plan execution language is one componentof NASA&#039;s New Millennium Remote Agent, an arti cial intelligence based spacecraft control system architecture that is scheduled to launch inDecember of 1998 as part of the Deep Space 1 mission to Mars. The language is concretely named Esl (Executive Support Language) and is basically a language designed to support the construction of reactive control mechanisms for autonomous robots and space crafts. It o ers advanced control constructs for managing interacting parallel goal-andevent driven processes, and is currently implemented as an extension to amulti-threaded Common Lisp. A total of 5 errors were in fact identi ed, 4 of which were important. This is regarded as a very successful result. According to the Remote Agent programming team the e ort has had a major impact, locating errors that would probably not have been
149|Software Model Checking -- Extracting Verification Models  from source code|To formally verify a large software application, the standard method is to  invest a considerable amount of time and expertise into the manual  construction of an abstract model, which is then analyzed for its properties by  either a mechanized or by a human prover. There are two main problems with  this approach. The first problem is that this verification method can be no  more reliable than the humans that perform the manual steps. If rate of error  for human work is a function of problem size, this holds not only for the  construction of the original application, but also for the construction of the  model. This means that the verification process tends to become unreliable for  larger applications. The second problem is one of timing and relevance. Software
150|A Formal Study of Slicing for Multi-threaded Programs with JVM Concurrency Primitives|. Previous work has shown that program slicing can be a useful  step in model-checking software systems. We are interested in applying  these techniques to construct models of multi-threaded Java programs.  Past work does not address the concurrency primitives found in Java,  nor does it provide the rigorous notions of slice correctness that are necessary  for reasoning about programs with non-deterministic behaviour  and potentially infinite computation traces.  In this paper, we define the semantics of a simple multi-threaded language  with concurrency primitives matching those found in the Java  Virtual Machine, we propose a bisimulation-based notion of correctness  for slicing in this setting, we identify notions of dependency that are  relevant for slicing multi-threaded Java programs, and we use these dependencies  to specify a program slicer for the language presented in the  paper. Finally, we discuss how these dependencies can be refined to take  into account common programmin...
151|Constructing Compact Models of Concurrent Java Programs|Finite-state verification technology (e.g., model checking) provides a powerful means to detect concurrency errors, which are often subtle and difficult to reproduce. Nevertheless, widespread use of this technology by developers is unlikely until tools provide automated support for extracting the required finite-state models directly from program source. In this paper, we explore the extraction of compact concurrency models from Java code. In particular, we show how static pointer analysis, which has traditionally been used for computing alias information in optimizers, can be used to greatly reduce the size of finite-state models of concurrent Java programs.
152|Verification of Erlang Programs using Abstract Interpretation and Model Checking|We present an approach for the verification of Erlang programs using abstract interpretation and model checking. In general model checking for temporal logics like LTL and Erlang programs is undecidable. Therefore we define a framework for abstract interpretations for a core fragment of Erlang. operational semantics preserves all paths of the standard operational semantics. We consider properties that have to hold on all paths of a system, like properties in LTL. If these properties can be proved for the abstract operational semantics, they also hold for the Erlang program. They can be proved with model checking if the abstract operational semantics is a finite transition system. Therefore we introduce a example abstract interpretation, which has this property. We have implemented this approach as a prototype and were able to prove properties like mutual exclusion or the absence of deadlocks and lifelocks for some Erlang programs.
153|An Optimizing Compiler for Efficient Model Checking|Different model checking tools offer a variety of specification languages to encode systems. These specifications are compiled into an intermediate form from which the global automata are derived at verification time. Some tools, such as SPIN, provide the user with constructs that can be used to affect the size of the global automata. In other tools, such as Mur&#039;, the user specifies a system directly in terms of its global automata using a guarded command language, and hence has complete control over the automata sizes. Our experience shows that using low-level specifications we can significantly reduce verification times. The question then is, whether we can derive the low-level representations directly from a high-level specification without user intervention or dependence on user annotations. We address this problem in this paper. We develop an optimizing compilation technique that transforms high-level specifications based on value-passing CCS into rules  from which transitions of ...
154|Specializing Configurable Systems for Finite-state Verification|As finite-state verification techniques and tools, such as model checkers, continue to mature, researchers and practitioners attempt to apply them in increasingly realistic software development settings. Concurrent applications, and components of those applications, are often implemented as configurable systems (i.e., where size, structure or selected behavior aspects are taken as system inputs). These systems are typically implemented using dynamically allocated data and threads of control. This use of dynamism makes it very difficult to render behavioral models of configurable systems that would be suitable as input to finite-state verification tools. Currently, configurable systems can only be verified by performing hand-transformations of the source code that are often time-consuming, tedious, and error-prone. In this paper, we apply partial evaluation techniques to transform source code automatically into a form from which finite-state systems can be extracted. We illustrate these...
155|A classification and comparison framework for software architecture description languages|Software architectures shift the focus of developers from lines-of-code to coarser-grained architectural elements and their overall interconnection structure. Architecture description languages (ADLs) have been proposed as modeling notations to support architecture-based development. There is, however, little consensus in the research community on what is an ADL, what aspects of an architecture should be modeled in an ADL, and which of several possible ADLs is best suited for a particular problem. Furthermore, the distinction is rarely made between ADLs on one hand and formal specification, module interconnection, simulation, and programming languages on the other. This paper attempts to provide an answer to these questions. It motivates and presents a definition and a classification framework for ADLs. The utility of the definition is demonstrated by using it to differentiate ADLs from other modeling notations. The framework is used to classify and compare several existing ADLs, enabling us in the process to identify key properties of ADLs. The comparison highlights areas where existing ADLs provide extensive support and those in which they are deficient, suggesting a research agenda for the future.
156|Foundations for the Study of Software Architecture|The purpose of this paper is to build the foundation for software architecture. We first develop an intuition for software architecture by appealing to several well-established architectural disciplines. On the basis of this intuition, we present a model of software architec-ture that consists of three components: elements, form, and rationale. Elements are either processing, data, or connecting elements. Form is defined in terms of the properties of, and the relationships among, the elements-- that is, the constraints on the elements. The ratio-nale provides the underlying basis for the architecture in terms of the system constraints, which most often derive from the system:requirements. We discuss the compo-nents of the model in the context of both architectures and architectural styles and present an extended exam-ple to illustrate some important architecture and style considerations. We conclude by presenting some of the benefits of our approach to software architecture, sum-marizing our contributions, and relating our approach to other current work.  
158|Architectural Mismatch or Why it&#039;s hard to build systems out of existing parts|Many would argue that future breakthroughs in software productivity will depend on our ability to combine existing pieces of software to produce new applications. An important step towards this goal is the development of new techniques to detect and cope with mismatches in the assembled parts. Some problems of composition are due to low-level issues of interoperability, such as mismatches in programming languages or database schemas. However, in this paper we highlight a different, and in manywaysmore pervasive, class of problem: architectural mismatch. Specifically, we use our experience in building a family of software design environments from existing parts to illustrate a variety of types of mismatch that center around the assumptions a reusable part makes about the structure of the application in which is to appear. Based on this experience we show how an architectural view of the mismatch problem exposes some fundamental, thorny problems for software composition and suggests poss...
159|Acme: An Architecture Description Interchange Language|Numerous architectural description languages (ADLs) have been developed, each providing complementary capabilities for architectural development and analysis. Unfortunately, each ADL and supporting toolset operates in isolation, making it di cult to integrate those tools and share architectural descriptions. Acme is being developed as a joint e ort of the software architecture research community as a common interchange format for architecture design tools. Acme provides a structural framework for characterizing architectures, together with annotation facilities for additional ADLspeci c information. This scheme permits subsets of ADL tools to share architectural information that is jointly understood, while tolerating the presence of information that falls outside their common vocabulary. In this paper we describe Acme&#039;s key features, rationale, and technical innovations. 1
160|Larch: Languages and tools for formal specification|  Building software often seems harder than it ought to be. It takes longerthan expected, the software&#039;s functionality and performance are not as wonderful as hoped, and the software is not particularly malleable or easyto maintain. It does not have to be that way. This book is about programming, and the role that formal specificationscan play in making programming easier and programs better. The intended audience is practicing programmers and students in undergraduate or basicgraduate courses in software engineering or formal methods. To make the book accessible to such an audience, we have not presumed that thereader has formal training in mathematics or computer science. We have, however, presumed some programming experience.
161|Software Reuse|Software reuse is the process ofcreating software systems from existing software rather than building software systems from scratch. ‘l’his simple yet powerful vision was introduced in 1968. Software reuse has, however, failed to become a standard software engineering practice. In an attempt to understand why, researchers have renewed their interest in software reuse and in the obstacles to implementing it. This paper surveys the different approaches to software reuse found in the research literature. It uses a taxonomy to describe and compare the different approaches and make generalizations about the field of software reuse. The taxonomy characterizes each reuse approach interms of its reusable artifacts and the way these artifacts are abstracted, selected, speciahzed, and integrated. Abstraction plays a central role in software reuse. Concise and expressive abstractions are essential if software artifacts are to be effectively reused. The effectiveness of a reuse technique can be evaluatedin terms of cognztzue dwtance-an intuitive gauge of the intellectual effort required to use the technique. Cognitive distance isreduced in two ways: (l) Higher level abstractions ina reuse technique
162|An Event-Based Architecture Definition Language|This paper discusses general requirements for architecture definition languages, and describes the syntax and semantics of the subset of the Rapide language that is designed to satisfy these requirements. Rapide is a concurrent event-based simulation language for defining and simulating the behavior of system architectures. Rapide is intended for modelling the architectures of concurrent and distributed systems, both hardware and software. In order to represent the behavior of distributed systems in as much detail as possible, Rapide is designed to make the greatest posible use of event-based modelling by producing causal event simulations. When a Rapide model is executed it produces a simulation that shows not only the events that make up the model&#039;s behavior, and their timestamps, but also which events caused other events, and which events happened independently.  The architecture definition features of Rapide are described here: event patterns, interfaces, architectures and event pa...
163|Architecture-based runtime software evolution|Continuous availability is a critical requirement for an important class of software systems. For these systems, runtime system evolution can mitigate the costs and risks associated with shutting down and restarting the system for an update. We present an architecture-based approach to runtime software evolution and highlight the role of software connectors in supporting runtime change. An initial implementation of a tool suite for supporting the runtime modification of software architectures, called ArchStudio, is presented. 1
164|Dynamic structure in software architectures|Much of the recent work on Architecture Description Languages (ADL) has concentrated on specifying organisations of components and connectors which are static. When the ADL specification is used to drive system construction, then the structure of the resulting system in terms of its component instances and their interconnection is fixed. This paper examines ADL features which permit the description of dynamic software architectures in which the organisation of components and connectors may change during system execution. The paper outlines examples of language features which support dynamic structure. These examples are taken from Darwin, a language used to describe distributed system structure. An operational semantics for these features is presented in the n-calculus, together with a discussion of their advantages and limitations. The paper discusses some general approaches to dynamic architecture description suggested by these examples. 1
165|Correct Architecture Refinement|A method is presented for the stepwise refinement of an abstract architecture into a relatively correct lower-level architecture that is intended to implement it. A refinement step involves the application of a predefined refinement pattern that provides a routine solution to a standard architectural design problem. A pattern contains an abstract architecture schema and a more detailed schema intended to implement it. The two schemas usually contain very different architectural concepts (from different architectural styles). Once a refinement pattern is proven correct, instances of it can be used without proof in developing specific architectures. Individual refinements are compositional, permitting incremental development and local reasoning. A special correctness criterion is defined for the domain of software architecture, as well as an accompanying proof technique. A useful syntactic form of correct composition is defined. The main points are illustrated by means of familiar archit...
166|Exploiting Style in Architectural Design Environments|As the design of software architectures emerges as a discipline within software engineering, it will become increasingly important to support architectural description and analysis with tools and environments. In this paper we describe a system for developing architectural design environments that exploit architectural styles to guide software architects in producing specific systems. The primary contributions of this research are: (a) a generic object model for representing architectural designs; (b) the characterization of architectural styles as specializations of this object model; and (c) a toolkit for creating an open architectural design environment from a description of a specific architectural style. We use our experience in implementing these concepts to illustrate how style-oriented architectural design raises new challenges for software support environments. 
167|A Component- and Message-Based Architectural Style for GUI Software|While a large fraction of application code is devoted to graphical user interface (GUI) functions, support for reuse in this domain has largely been confined to the creation of GUI toolkits (&#034;widgets&#034;). We present a novel architectural style directed at supporting larger grain reuse and flexible system composition. Moreover, the style supports design of distributed, concurrent applications. Asynchronous notification messages and asynchronous request messages are the sole basis for inter-component communication. A key aspect of the style is that components are not built with any dependencies on what typically would be considered lower-level components, such as user interface toolkits. Indeed, all components are oblivious to the existence of any components to which notification messages are sent. While our focus has been on applications involving graphical user interfaces, the style has the potential for broader applicability. Several trial applications using the style are described.
168|A Language and Environment for Architecture-Based Software Development and Evolution|Software architectures have the potential to substantially improve the development and evolution of large, complex, multi-lingual, multi-platform, long-running systems. However, in order to achieve this potential, specific techniques for architecture-based modeling, analysis, and evolution must be provided. Furthermore, one cannot fully benefit from such techniques unless support for mapping an architecture to an implementation also exists. This paper motivates and presents one such approach, which is an outgrowth of our experience with systems developed and evolved according to the C2 architectural style. We describe an architecture description language (ADL) specifically designed to support architecturebased evolution and discuss the kinds of evolution the language supports. We then describe a component-based environment that enables modeling, analysis, and evolution of architectures expressed in the ADL, as well as mapping of architectural models to an implementation infrastructure. The architecture of the environment itself can be evolved easily to support multiple ADLs, kinds of analyses, architectural styles, and implementation platforms. Our approach is fully reflexive: the environment can be used to describe, analyze, evolve, and (partially) implement itself, using the very ADL it supports. An existing architecture is used throughout the paper to provide illustrations and examples. Keywords Software architecture, architecture description language,
169|A survey of architecture description languages|1.1 Background: system architecture for system development? The characteristic approach in mature engineering disciplines (e.g. civil and chemical engineering) is to build systems (e.g., buildings or chemical plants) from known solutions such as
170|Using Style to Understand Descriptions of Software Architecture|The software architecture of most systems is described informally and diagrammatically. In order for these descriptions to be meaningful at all, figures are understood by interpreting the boxes and lines in specific, conventionalized ways [5]. The imprecision of these interpretations has a number of limitations. In this paper we consider these conventionalized interpretations as architectural styles and provide a formal framework for their uniform definition. In addition to providing a template for precisely defining new architectural styles, this framework allows for the proof that the notational constraints on a style are sufficient to guarantee the meanings of all described systems and provides a unified semantic base through which different stylistic interpretations can be compared.
171|Structuring Parallel and Distributed Programs|The paper presents a structuring language, The paper presents a structuring language, Darwin Darwin , which allows distributed , which allows distributed and parallel programs to be structured in terms of groups of process instances and parallel programs to be structured in terms of groups of process instances which communicate by message passing. In addition to expressing static which communicate by message passing. In addition to expressing static structure, Darwin can be used to express structures which change dynamically structure, Darwin can be used to express structures which change dynamically as execution progresses. The paper presents a set of examples illustrating the as execution progresses. The paper presents a set of examples illustrating the use of Darwin in constructing parallel programs. Since processes can be use of Darwin in constructing parallel programs. Since processes can be considered to be an abstraction of physical processors, Darwin can also be used considered...
172|The CODE 2.0 Graphical Parallel Programming Language |CODE 2.0 is a graphical parallel programming system that targets the three goals of ease of use, portability, and production of efficient parallel code. Ease of use is provided by an integrated graphical/textual interface, a powerful dynamic model of parallel computation, and an integrated concept of program component reuse. Portability is approached by the declarative expression of synchronization and communication operators at a high level of abstraction in a manner which cleanly separates overall computation structure from the primitive sequential computations that make up a program. Execution efficiency is approached through a systematic class hierarchy that supports hierarchical translation refinement including special case recognition. This paper reports results obtained through experimental use of a prototype implementation of the CODE 2.0 system. CODE 2.0 represents a major conceptual advance over its predecessor systems (CODE 1.0 and CODE 1.2) in terms of the expressive power ...
173|Integrating Architecture Description Languages with a Standard Design Method|Software architecture descriptions are high-level models of software systems. Some researchers have proposed special-purpose architectural notations that have a great deal of expressive power but are not well integrated with common development methods. Others have used mainstream development methods that are accessible to developers, but lack semantics needed for extensive analysis. We describe an approach to combining the advantages of these two ways of modeling architectures. We present two examples of extending UML, an emerging standard design notation, for use with two architecture description languages, C2 and Wright. Our approach suggests a practical strategy for bringing architectural modeling into wider use, namely by incorporating substantial elements of architectural models into a standard design method. Keywords Software architecture, object-oriented design, architecture description languages, constraint languages, incremental development 1
174|Using object-oriented typing to support architectural design in the C2 style|Abstract-- Software architectures enable large-scale software development. Component reuse and substitutability, two key aspects of large-scale development, must be planned for during software design. Object-oriented (OO) type theory supports reuse by structuring inter-component relationships and verifying those relationships through type checking in an architecture definition language (ADL). In this paper, we identify the issues and discuss the ramifications of applying OO type theory to the C2 architectural style. This work stems from a series of experiments that were conducted to investigate component reuse and substitutability in C2. We also discuss the limits of applicability of OO typing to C2 and how we addressed them in the C2 ADL. 1
176|Formal Connectors|As software systems become more complex the overall system structure -- or software architecture -- becomes a central design problem. An important step towards an engineering discipline of software is a formal basis for describing and analyzing these designs. In this paper we present a theory for one aspect of architectural description: the interactions between components. The key idea is to define architectural connectors as explicit semantic entities. These are specified as a collection of protocols that characterize each of the participant roles in an interaction and how these roles interact. We illustrate how this scheme can be used to define a variety of common architectural connectors. We further provide a formal semantics and show how this leads to a system in which architectural compatibility can be checked in a way analogous to type checking in programming languages.
177|Abstractions and implementations for architectural connections|The architecture of a software system shows how the system is realized by a collection of components and the interactions among these components. Conventional design focuses on defining the components, but the properties of the system depend critically on the character of the interactions. Although software designers have good informal abstractions for these interactions, the abstractions are poorly supported by the available languages and tools. As a result, the choice of interaction is often defaulted or implicit rather than deliberate choice; further, interactions may be defined in terms of underlying mechanisms rather than the designers ’ natural abstractions. UniCon provides a rich selection of abstractions for the connectors that mediate interactions among components. To create systems using these connector abstractions, you need to produce and integrate not only the object code for components, but also a variety of other run-time products. To extend the set of connectors supported by UniCon, you need to identify and isolate many kinds of information in the compiler, graphical editor, and associated tools. This paper describes the role of connector abstractions in software design, the connector abstractions currently supported by UniCon, and implementation issues associated with supporting an open-ended collection of connectors.
179|Using Off-the-Shelf Middleware to Implement Connectors in Distributed Software Architectures|Software architectures promote development focused on modular building blocks and their interconnections. Since architecture-level components often contain complex functionality, it is reasonable to expect that their interactions will also be complex. Modeling and implementing software connectors thus becomes a key aspect of architecture-based development. Software interconnection and middleware technologies such as RMI, CORBA, ILU, and ActiveX provide a valuable service in building applications from components. The relation of such services to software connectors in the context of software architectures, however, is not well understood. To understand the tradeoffs among these technologies with respect to architectures, we have evaluated several off-the-shelf middleware technologies and identified key techniques for utilizing them in implementing software connectors. Our platform for investigation was C2, a component- and message-based architectural style. By encapsulating middleware functionality within software connectors, we have coupled C2’s existing benefits such as component interchangeability, substrate independence and structural guidance with new capabilities of multi-lingual, multi-process and distributed application development in a manner that is transparent to architects.
180|Domain-Specific Software Architectures for Guidance, Navigation and Control|We describe two integrated languages and associated tools for capturing and analyzing two different views of the architecture of an embedded system. One language is tailored to address guidance, navigation and control issues, while the other is tailored to address real-time, fault-tolerance, secure partitioning and multi-processor system issues. Both languages have tools that perform analyses appropriate for the issues each addresses, and tools to automatically configure the  application software from a sufficiently detailed specification. The integrated languages and tools are intended to support an architecture reuse development process, in which the development of a new product in a family of similar products starts from a generic or reusable architecture specification for that product family.   
181|ADLs and Dynamic Architecture Changes|Existing ADLs typically support only static architecture specification and do not provide facilities for the support of dynamically changing architectures. This paper presents a possible solution to this problem: in order to adequately support dynamic architecture changes, ADLs can leverage techniques used in dynamic programming languages. In particular, changes to ADL specifications should be interpreted. To enable interpretation, an ADL should have an architecture construction component that supports explicit and incremental specification of architectural changes, in addition to the traditional architecture description facilities. This will allow software architects to specify the changes to an architecture after it has been built. The paper expands upon the results from an ongoing project-- building a development environment for C2-style architectures. 1 I.
182|Extending design environments to software architecture design|Domain-oriented design environments are cooperative problem-solving systems that support designers in complex design tasks. In this paper we present the facilities and architecture of Argo, a domain-oriented design environment for software architecture. Argo’s architecture is motivated by the desire to achieve reuse and extensibility of the design environment. It separates domain-neutral code from domain-oriented code, which is distributed among intelligent design materials as opposed to being centralized in the design environment. Argo’s facilities are motivated by the observed cognitive needs of designers. These facilities extend previous work in design environments to support reflection-in-action, opportunistic design, and comprehension and problem-solving. Keywords: Domain-oriented design environments, critics, software architectures, architectural styles, humancomputer interaction, human cognitive skills.
183|Characteristics of Higher-level Languages for Software Architecture|As the size and complexity of software systems increases, the design and specification of overall system structure -- or software architecture -- emerges as a central concern. Architectural issues include the gross organization of the system, protocols for communication and data access, assignmentof functionality to design elements, and selection among design alternatives. Currently system designers have at their disposal two primary ways of defining software architecture: they can use the modularization facilities of existing programming languages and module interconnection languages; or they can describe their designs using informal diagrams and idiomatic phrases (such as &#034;client-server organization&#034;). In this paper we explain why neither alternative is adequate. We consider the nature of architectural description as it is performed informally by systems designers. Then we show that regularities in these descriptions can form the basis for architectural description languages. Next we ...
184|Reuse of Off-the-Shelf Components in C2-Style Architectures |Abstract-- Reuse of large-grain software components offers the potential for significant savings in application development cost and time. Successful component reuse and substitutability depends both on qualities of the components reused as well as the software context in which the reuse is attempted. Disciplined approaches to the structure and design of software applications offers the potential of providing a hospitable setting for such reuse. We present the results of a series of exercises designed to determine how well “offthe-shelf” components could be reused in applications designed in accordance with the C2 software architectural style. The exercises involved the reuse of two user-interface constraint solvers, two graphics toolkits, a World Wide Web browser, and a persistent object manager. A subset of these components was used to construct numerous variations of a single application (thus an application family). The exercises also included construction of a simple development environment for locating and downloading a component off the Web and incorporating it into an application. The paper summarizes the style rules that facilitate reuse and presents the results from the exercises. The exercises were successful in a variety of dimensions; one conclusion is that the C2 style offers significant reuse potential to application developers. At the same time, wider trials and additional tool support are needed. 1 Index Terms-- software reuse, architectural styles, messagebased architectures, component-based development, graphical user interfaces (GUI). I.
185|Formal Modeling and Analysis of the HLA Component Integration Standard|An increasingly important trend in the engineering of complex systems is the design of component integration standards. Such standards define rules of interaction and shared communication infrastructure that permit composition of systems out of independently-developed parts. A problem with these standards is that it is often difficult to understand exactly what they require and provide, and to analyze them in order to understand their deeper properties. In this paper we use our experience in modeling the High Level Architecture (HLA) for Distributed Simulation to show how one can capture the structured protocol inherent in an integration standard as a formal architectural model that can be analyzed to detect anomalies, race conditions, and deadlocks.   KEYWORDS  Component integration standards, component-based software, protocol families, software architecture, formal specification.   1 Introduction  Component integration standards are becoming increasingly important for commercial sof...
186|Formal modeling of software architectures at multiple levels of abstraction|Software architectures are multi-dimensional entities that can be fully understood only when viewed and analyzed at four different levels of abstraction: (1) internal functionality of a component, (2) the interface(s) exported by the component to the rest of the system, (3) interconnection of architectural elements in an architecture, and (4) rules of the architectural style. This paper presents the characteristics of each of the four levels of architectural abstraction, outlines the kinds of analyses that need to be performed at each level, and discusses the kinds of formal notations that are suitable at each level. We use the pipe-and-filter and Chiron-2 (C2) architectural styles as illustrations. In particular, we present formal models of C2 at the last three levels of abstraction as a first step in enabling a C2 design environment to perform the necessary analyses of architectures. We discuss the benefits of the formal definitions and our experience to date. 1 Keywords Software architectures, architectural styles, formalism, architecture definition languages, interface definition languages
187|Assessing the Suitability of a Standard Design Method for Modeling Software Architectures| Software architecture descriptions are high-level models of software systems.  Most existing special-purpose architectural notations have a great deal of  expressive power but are not well integrated with common development  methods. Conversely, mainstream development methods are accessible to  developers, but lack the semantics needed for extensive analysis. In our  previous work, we described an approach to combining the advantages of  these two ways of modeling architectures. While this approach suggested a  practical strategy for bringing architectural modeling into wider use, it  introduced specialized extensions to a standard modeling notation, which  could also hamper wide adoption of the approach. This paper attempts to  assess the suitability of a standard design method &#034;as is&#034; for modeling  software architectures.  
188|Issues in the Runtime Modification of Software Architectures|Existing software architecture research has focused on static architectures, where the system architecture is not expected to change during system execution. We argue that the architectures of many systems, especially long running or mission critical systems, evolve during execution, and thus cannot be accurately modeled and analyzed using static architectures. To overcome these problems, we propose the use of dynamic architectures, where the system architecture may change during execution. In this paper, we identify the issues involved in supporting dynamic architectures. Although some of these issues may be addressed by augmenting current models (i.e., adding constructs that support dynamism to existing architectural description languages), many are new to dynamic architectures (i.e., runtime support for modifying architectures). We describe an initial implementation of our tool, ArchShell, that supports the runtime modification of C2-style software architectures. 1
189|Specifying Dynamism in Software Architectures|A critical issue for complex component-based systems design is the modeling and analysis of architecture. One of the complicating factors in developing architectural models is accounting for systems whose architecture changes dynamically (during run time). This is because dynamic changes to architectural structure may interact in subtle ways with on-going computations of the system. In this paper we argue that it is possible and valuable to provide a modeling approach that accounts for the interactions between architectural recon guration and non-recon guration system functionality, while maintaining a separation of concerns between these two aspects of a system. The key to the approach is to use a uniform notation and semantic base for both recon guration and steady-state behavior, while at the same time providing syntactic separation between the two. As we will show, this permits us to view the architecture in terms of a set possible architectural snapshots, each with its own steady-state behavior. Transitions between these snapshots are accounted for by special recon guration-triggering events.
190|Multilanguage Interoperability in Distributed Systems: EXPERIENCE REPORT|The Q system provides interoperability support for multilingual, heterogeneous component-based software systems. Initial development of Q began in 1988, and was driven by the very pragmatic need for a communication mechanism between a client program written in Ada and a server written in C. The initial design was driven by language features present in C, but not in Ada, or vice-versa. In time our needs and aspirations grew and Q evolved to support other languages, such as C++, Lisp, and Prolog. As a result of pervasive usage by the Arcadia SDE research project, usage levels and modes of the Q system grew and so more emphasis was placed upon portability, reliability, and performance. In that context we identified specific ways in which programming language support systems can directly impede effective interoperability. This necessitated extensive changes to both our conceptual model, and our implementation, of the Q system. We also discovered the need to support modes of interoperabilit...
191|Style-Based Refinement for Software Architecture |A question that frequently arises for architectural design is &#034;When can I implement a design in style S1 using a design in style S2?&#034; In this paper I propose a technique for structuring a solution to this kind of problem using the idea of substyles. This technique leads to a two-step process in which first, useful subsets of a family of architectures are identied, and second, refinement rules specific to these subsets are established. I will argue that this technique, in combination with an unconventional interpretation of refinement, clarifies how engineers actually carry out architectural refinement and provides a formal framework for establishing the correctness of those methods.
192|Secure Software Architectures|The computer industry is increasingly dependent on open architectural standards for their competitive success. This paper describes a new approach to secure system design in which the various representations of the architecture of a software system are described formally and the desired security properties of the system are proven to hold at the architectural level. The main ideas are illustrated by means of the X/Open Distributed Transaction Processing reference architecture, which is formalized and extended for secure access control as defined by the Bell-LaPadula model. The extension allows vendors to develop individual components independently and with minimal concern about security. Two important observations were gleaned on the implications of incorporating security into software architectures. Keywords: secure systems, software architecture, X/Open DTP, formal methods, access control 1. Introduction In recent years, there has been a growing demand for vendor-neutral, open system...
193|Domains of Concern in Software Architectures and Architecture Description Languages|Software architectures shift the focus of developers from lines-of-code to coarser-grained elements and their interconnection structure. Architecture description languages (ADLs) have been proposed as domain-specific languages for the domain of software architecture. There is still little consensus in the research community on what problems are most important to address in a study of software architecture, what aspects of an architecture should be modeled in an ADL, or even what an ADL is. To shed light on these issues, we provide a framework of architectural domains, or areas of concern in the study of software architectures. We evaluate existing ADLs with respect to the framework and study the relationship between architectural and application domains. One conclusion is that, while the architectural domains perspective enables one to approach architectures and ADLs in a new, more structured manner, further understanding of architectural domains, their tie to application domains, and their specific influence on ADLs is needed.
194|Three Concepts of System Architecture|Abstract  An architecture is a specification of the components of a system and the communication between them. Systems are constrained to conform to an architecture. An architecture should guarantee certain behavioral properties of a conforming system, i.e., one whose components are configured according to the architecture. An architecture should also be useful in various ways during the process of building a system. This paper presents three alternative concepts of architecture: object connection architecture, interface connection architecture, and plug and socket architecture. We describe different concepts of interface  and connection that are needed for each of the three kinds of architecture, and different conformance requirements of each kind. Simple examples are used to compare the usefulness of each kind of architecture in guaranteeing properties of conforming systems, and in correctly modifying a conforming system. In comparing the three architecture concepts the principle of ...
195|Software architecture design from the perspective of human cognitive needs|Software architectures are useful, in part, because they use the appropriate level of abstraction to support the design of complex systems. Software architecture research has quickly evolved to the degree that design environments have been implemented to support software architects in creating new designs. We report on a software architecture design environment named Argo that differs from other approaches by paying attention to the human, cognitive needs of software architects, as much as to the representation and manipulation of the architecture itself. We emphasize the primary considerations by contrasting the human cognitive design process to the systems-oriented software design process. Human-centered features in Argo focus on the application of critics for providing design feedback, design processes for supporting critics, and multiple architectural perspectives for aiding human designers.
197|HLA: A Standards Effort as Architectural Style|In this position paper we introduce a case study, the DoD “High Level Architecture for Simulations (HLA), ” and briefly discuss our efforts to apply WRIGHT, an architectural description language, to the HLA. Our work on HLA has focused on understanding the HLA as an architectural style, concentrating on the Interface Specification (IFSpec) description of the “Runtime Infrastructure (RTI) ” as the central architectural design issue. Specifically, we have used WRIGHT, a formal architectural description language, to characterize the RTI and analyze a number of its properties. By providing an analysis of the properties of the RTI as described by the IFSpec, we can help the standards committee to determine whether the IFSpec ensures the properties that they want and to discover inconsistencies or other weaknesses of the specification. 1
199|Architectural Style: An Object-Oriented Approach|Software system builders are increasingly recognizing the importance of exploiting design knowledge in the engineering of new systems. One way to do this is to define an architectural style for a collection of related systems. The style determines a coherent vocabulary of system design elements and rules for their composition. By structuring the design space for a family of related systems a style can, in principle, drastically simplify the process of building a system, reduce costs of implementation through reusable infrastructure, and improve system integrity through style-specific analyses and checks. In this article we describe one way to realize these benefits. Specifically, we describe Aesop, an environment for architectural design that supports the definition and use of architectural styles. Aesop adopts an object-oriented approach to the representation of both styles and designs. It also provides a repository of reusable architectural design fragments. In this way it...
200|Reusing Offthe-Shelf Components to Develop a Family of Applications in the C2 Architectural Style|Abstract-- Reuse of large-grain software components offers the potential for significant savings in application development cost and time. Successful reuse of components and component substitutability depends both on qualities of the components reused as well as the software context in which the reuse is attempted. Disciplined approaches to the structure and design of software applications offers the potential of providing a hospitable setting for such reuse. We present the results of a series of exercises designed to determine how well “off-the-shelf” constraint solvers could be reused in applications designed in accordance with the C2 software architectural style. The exercises involved the reuse of SkyBlue and Amulet’s one-way formula constraint solver. We constructed numerous variations of a single application (thus an application family). The paper summarizes the style and presents the results from the exercises. The exercises were successful in a variety of dimensions; one conclusion is that the C2 style offers significant potential for the development of application families and that wider trials are warranted. 1 Index Terms-- architectural styles, message-based architectures, application families, graphical user interfaces (GUIs), constraint management, component-based development. I.
201|Virtual Time and Global States of Distributed Systems|A distributed system can be characterized by the fact that the global state is distributed and that a common time base does not exist. However, the notion of time is an important concept in every day life of our decentralized &#034;real world&#034; and helps to solve problems like getting a consistent population census or determining the potential causality between events. We argue that a linearly ordered structure of time is not (always) adequate for distributed systems and propose a generalized non-standardmodel of time which consists of vectors of clocks. These clock-vectors arepartially orderedand  form a lattice. By using timestamps and a simple clock update mechanism the structure of causality is represented in an isomorphic way. The new model of time has a close analogy to Minkowski&#039;s relativistic spacetime and leads among others to an interesting characterization of the global state problem. Finally, we present a new algorithm to compute a consistent global snapshot of a distributed system where messages may bereceived out of order.
202|ON DISTRIBUTED SNAPSHOTS|We develop an efficient snapshot algorithm that needs no control messages and does not require channels to be first-in-first-out. We also show that several stable properties (e.g., termination, deadlock) can be detected with uncoordinated distributed snapshots. For such properties, our algorithm can be further simplified.
203|Guarded Commands, Nondeterminacy and Formal Derivation of Programs|So-called &#034;guarded commands&#034; are introduced as a building block for alternative and repetitive constructs that allow nondeterministic program components for which at least the activity evoked, but possibly even the final state, is not necessarily uniqilely determined by the initial state. For the formal derivation of programs expressed in terms of these constructs, a calculus will be be shown. 
204|An axiomatic basis for computer programming|In this paper an attempt is made to explore the logical founda-tions of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This in-volves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantages, both theoretical and prac-tical, may follow from a pursuance of these topics.
205|Testing Equivalences for Processes|Abstract. Given a set of processes and a set of tests on these processes we show how to define in a natural way three different eyuitalences on processes. ThesP equivalences are applied to a particular language CCS. We give associated complete proof systems and fully abstract models. These models have a simple representation in terms of trees.
206|Proving the Correctness of Multiprocess Programs|The inductive assertion method is generalized to permit formal, machine-verifiable proofs of correctness for multiprocess programs. Individual processes are represented by ordinary flowcharts, and no special synchronization mechanisms are assumed, so the method can be applied to a large class of multiprocess programs. A correctness proof can be designed together with the program by a hierarchical process of stepwise refinement, making the method practical for larger programs. The resulting proofs tend to be natural formalizations of the informal proofs that are now used.
207|A term model for CCS|In a series of papers [Hen2, Mill, Mi14-7] Milner and his colleagues have studied a model of parallelism in which concurrent systems communicate by sending and receiving values along lines. Communication is synchronised in that the exchange of values takes place only when the sender and receiver are both ready, and the exchange
208|Ptolemy: A Framework for Simulating and Prototyping Heterogeneous Systems|Ptolemy is an environment for simulation and prototyping of heterogeneous systems. It uses modern object-oriented software technology (C++) to model each subsystem in a natural and efficient manner, and to integrate these subsystems into a whole. Ptolemy encompasses practically all aspects of designing signal processing and communications systems, ranging from algorithms and communication strategies, simulation, hardware and software design, parallel computing, and generating real-time prototypes. To accommodate this breadth, Ptolemy must support a plethora of widely-differing design styles. The core of Ptolemy is a set of object-oriented class definitions that makes few assumptions about the system to be modeled; rather, standard interfaces are provided for generic objects and more specialized, application-specific objects are derived from these. A basic abstraction in Ptolemy is the Domain, which realizes a computational model appropriate for a particular type of subsystem. Current e...
209|Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks|Dryad is a general-purpose distributed execution engine for coarse-grain data-parallel applications. A Dryad applica-tion combines computational “vertices ” with communica-tion “channels ” to form a dataflow graph. Dryad runs the application by executing the vertices of this graph on a set of available computers, communicating as appropriate through files, TCP pipes, and shared-memory FIFOs. The vertices provided by the application developer are quite simple and are usually written as sequential programs with no thread creation or locking. Concurrency arises from Dryad scheduling vertices to run simultaneously on multi-ple computers, or on multiple CPU cores within a computer. The application can discover the size and placement of data at run time, and modify the graph as the computation pro-gresses to make efficient use of the available resources. Dryad is designed to scale from powerful multi-core sin-gle computers, through small clusters of computers, to data centers with thousands of computers. The Dryad execution engine handles all the difficult problems of creating a large distributed, concurrent application: scheduling the use of computers and their CPUs, recovering from communication or computer failures, and transporting data between ver-tices.
210|The Google File System |We have designed and implemented the Google File Sys-tem, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous dis-tributed file systems, our design has been driven by obser-vations of our application workloads and technological envi-ronment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore rad-ically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our ser-vice as well as research and development efforts that require large data sets. The largest cluster to date provides hun-dreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use. Categories and Subject Descriptors D [4]: 3—Distributed file systems
211|The click modular router| Click is a new software architecture for building flexible and configurable routers. A Click router is assembled from packet processing modules called elements. Individual elements implement simple router functions like packet classification, queueing, scheduling, and interfacing with network devices. A router configuration is a directed graph with elements at the vertices; packets flow along the edges of the graph. Configurations are written in a declarative language that supports user-defined abstractions. This language is both readable by humans and easily manipulated by tools. We present language tools that optimize router configurations and ensure they satisfy simple invariants. Due to Click’s architecture and language, Click router configurations are modular and easy to extend. A standards-compliant Click IP router has sixteen elements on its forwarding path. We present extensions to this router that support dropping policies, fairness among flows, quality-of-service, and
212|PVM: A Framework for Parallel Distributed Computing|The PVM system is a programming environment for the development and execution of large concurrent or parallel applications that consist of many interacting, but relatively independent, components. It is intended to operate on a collection of heterogeneous computing elements interconnected by one or more networks. The participating processors may be scalar machines, multiprocessors, or special-purpose computers, enabling application components to execute on the architecture most appropriate to the algorithm. PVM provides a straightforward and general interface that permits the description of various types of algorithms (and their interactions), while the underlying infrastructure permits the execution of applications on a virtual computing environment that supports multiple parallel computation models. PVM contains facilities for concurrent, sequential, or conditional execution of application components, is portable to a variety of architectures, and supports certain forms of error dete...
213|Cilk: An Efficient Multithreaded Runtime System|Cilk (pronounced &#034;silk&#034;) is a C-based runtime system for multithreaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the &#034;work&#034; and &#034;critical-path length&#034; of a Cilk computation can be used to model performance accurately. Consequently, a Cilk programmer can focus on reducing the computation&#039;s work and critical-path length, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of &#034;fully strict&#034; (well-structured) programs, the Cilk scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The Cilk
214|Parallel database systems: the future of high performance database systems|Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1.
215|Distributed Computing in Practice: The Condor Experience|Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational grid. In this chapter, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course traveled by research ideas as they grow into production systems.
216|Cluster-Based Scalable Network Services|This paper has benefited from the detailed and perceptive comments of our reviewers, especially our shepherd Hank Levy. We thank Randy Katz and Eric Anderson for their detailed readings of early drafts of this paper, and David Culler for his ideas on TACC&#039;s potential as a model for cluster programming. Ken Lutz and Eric Fraser configured and administered the test network on which the TranSend scaling experiments were performed. Cliff Frost of the UC Berkeley Data Communications and Networks Services group allowed us to collect traces on the Berkeley dialup IP network and has worked with us to deploy and promote TranSend within Berkeley. Undergraduate researchers Anthony Polito, Benjamin Ling, and Andrew Huang implemented various parts of TranSend&#039;s user profile database and user interface. Ian Goldberg and David Wagner helped us debug TranSend, especially through their implementation of the rewebber
218|Interpreting the Data: Parallel Analysis with Sawzall |Very large data sets often have a flat but regular structure and span multiple disks and machines. Examples include telephone call records, network logs, and web document repositories. These large data sets are not amenable to study using traditional database techniques, if only because they can be too large to fit in a single relational database. On the other hand, many of the analyses done on them can be expressed using simple, easily distributed computations: filtering, aggregation, extraction of statistics, and so on. We present a system for automating such analyses. A filtering phase, in which a query is expressed using a new procedural programming language, emits data to an aggregation phase. Both phases are distributed over hundreds or even thousands of computers. The results are then collated and saved to a file. The design—including the separation into two phases, the form of the programming language, and the properties of the aggregators—exploits the parallelism inherent in having data and computation distributed across many machines. 1
219|The Gamma database machine project|This paper describes the design of the Gamma database machine and the techniques employed in its implementation. Gamma is a relational database machine currently operating on an Intel iPSC/2 hypercube with 32 processors and 32 disk drives. Gamma employs three key technical ideas which enable the architecture to be scaled to 100s of processors. First, all relations are horizontally partitioned across multiple disk drives enabling relations to be scanned in parallel. Second, novel parallel algorithms based on hashing are used to implement the complex relational operators such as join and aggregate functions. Third, dataflow scheduling techniques are used to coordinate multioperator queries. By using these techniques it is possible to control the execution of very complex queries with minimal coordination- a necessity for configurations involving a very large number of processors. In addition to describing the design of the Gamma software, a thorough performance evaluation of the iPSC/2 hypercube version of Gamma is also presented. In addition to measuring the effect of relation size and indices on the response time for selection, join, aggregation, and update queries, we also analyze the performance of Gamma relative to the number of processors employed when the sizes of the input relations are kept constant (speedup) and when the sizes of the input relations are increased proportionally to the number of processors (scaleup). The speedup results obtained for both selection and join queries are linear; thus, doubling the number of processors
220|Programming Parallel Algorithms|In the past 20 years there has been treftlendous progress in developing and analyzing parallel algorithftls. Researchers have developed efficient parallel algorithms to solve most problems for which efficient sequential solutions are known. Although some ofthese algorithms are efficient only in a theoretical framework, many are quite efficient in practice or have key ideas that have been used in efficient implementations. This research on parallel algorithms has not only improved our general understanding ofparallelism but in several cases has led to improvements in sequential algorithms. Unf:ortunately there has been less success in developing good languages f:or prograftlftling parallel algorithftls, particularly languages that are well suited for teaching and prototyping algorithms. There has been a large gap between languages
221|Programmable stream processors|The Imagine Stream Processor is a single-chip programmable media processor with 48 parallel ALUs. At 400 MHz, this translates to a peak arithmetic rate of 16 GFLOPS on single-precision data and 32 GOPS on 16bit fixed-point data. The scalability of Imagine’s programming model and architecture enable it to achieve such high arithmetic rates. Imagine executes applications that have been mapped to the stream programming model. The stream model decomposes applications into a set of computation kernels that operate on data streams. This mapping exposes the inherent locality and parallelism in the application, and Imagine exploits the locality and parallelism to provide a scalable architecture that supports 48 ALUs on a single chip. This paper presents the Imagine architecture and programming model in the first half, and explores the scalability of the Imagine architecture in the second half. 1.
222|Accelerator: using data parallelism to program GPUs for general-purpose uses|GPUs are difficult to program for general-purpose uses. Programmers can either learn graphics APIs and convert their applications to use graphics pipeline operations or they can use stream programming abstractions of GPUs. We describe Accelerator, a system that uses data parallelism to program GPUs for general-purpose uses instead. Programmers use a conventional imperative programming language and a library that provides only high-level data-parallel operations. No aspects of GPUs are exposed to programmers. The library implementation compiles the data-parallel operations on the fly to optimized GPU pixel shader code and API calls. We describe the compilation techniques used to do this. We evaluate the effectiveness of using data parallelism to program GPUs by providing results for a set of compute-intensive benchmarks. We compare the performance of Accelerator versions of the benchmarks against hand-written pixel shaders. The speeds of the Accelerator versions are typically within 50 % of the speeds of hand-written pixel shader code. Some benchmarks significantly outperform C versions on a CPU: they are up to 18 times faster than C code running on a CPU.
223|Using Cohort Scheduling to Enhance Server Performance|A server application is commonly organized as a collection of concurrent threads, each of which executes the code necessary to process a request. This software architecture, which causes frequent control transfers between unrelated pieces of code, decreases instruction and data locality, and consequently reduces the effec- tiveness of hardware mechanisms such as caches, TLBs, and branch predictors. Numerous measurements demonstrate this effect in server applications, which often utilize only a fraction of a modern processor&#039;s computational throughput.
224|Stream Computations Organized for Reconfigurable Execution (SCORE): Introduction and Tutorial  (2000) |A primary impediment to wide-spread exploitation of reconfigurable computing is the lack of a unifying computational model which allows application portability and longevity without sacrificing a substantial fraction of the raw capabilities. We introduce SCORE (Stream Computation Organized for Reconfigurable Execution), a streambased compute model which virtualizes reconfigurable computing resources (compute, storage, and communication) by dividing a computation up into fixed-size &#034;pages&#034; and time-multiplexing the virtual pages on available physical hardware. Consequently, SCORE applications can scale up or down automatically to exploit a wide range of hardware sizes. We hypothesize that the SCORE model will ease development and deployment of reconfigurable applications and expand the range of applications which can benefit from reconfigurable execution. Further, we believe that a well engineered SCORE implementation can be efficient, wasting little of the capabilities of the raw hardw...
225|Paralex: An Environment for Parallel Programming in Distributed Systems|Modern distributed systems consisting of powerful workstations and high-speed interconnection networks are an economical alternative to special-purpose super computers. The technical issues that need to be addressed in exploiting the parallelism inherent in a distributed system include heterogeneity, high-latency communication, fault tolerance and dynamic load balancing. Current software systems for parallel programming provide little or no automatic support towards these issues and require users to be experts in fault-tolerant distributed computing. The Paralex system is aimed at exploring the extent to which the parallel application programmer can be liberated from the complexities of distributed systems. Paralex is a complete programming environment and makes extensive use of graphics to define, edit, execute and debug parallel scientific applications. All of the necessary code for distributing the computation across a network and replicating it to achieve fault tolerance and dynamic load balancing is automatically generated by the system. In this paper we give an overview of Paralex and present our experiences with a prototype implementation.
226|Parallel and Distributed Haskells|Parallel and distributed languages specify computations on multiple processors and have a computation language to describe the algorithm, i.e. what to compute, and a coordination language to describe how to organise the computations across the processors. Haskell has been used as the computation language for a wide variety of parallel and distributed languages, and this paper is a comprehensive survey of implemented languages. We outline parallel and distributed language concepts and classify Haskell extensions using them. Similar example programs are used to illustrate and contrast the coordination languages, and the comparison is facilitated by the common computation language. A lazy language is not an obvious choice for parallel or distributed computation, and we address the question of why Haskell is a common functional computation language.
227|DB2 Parallel Edition|The rate of increase in database size and response time requirements has outpaced  advancements in processor and mass storage technology. One way to satisfy the increasing  demand for processing power and I/O bandwidth in database applications is to have a  number of processors, loosely or tightly coupled, serving database requests concurrently.  Technologies developed during the last decade have made commercial parallel database  systems a reality and these systems have made an inroad into the stronghold of traditionally  mainframe based large database applications. This paper describes the parallel database  project initiated at IBM Research at Hawthorne and the DB2/AIX-PE product based on  it.  1 Introduction  Large scale parallel processing technology has made giant strides in the past decade and there is no doubt that it has established a place for itself. However, almost all of the applications harnessing this technology are scientific or engineering applications. The lack of com...
228|Highly Available, Fault-Tolerant, Parallel Dataflows|We present a technique that masks failures in a cluster to provide high availability and fault-tolerance for long-running, parallelized dataflows. We can use these dataflows to implement a variety of continuous query (CQ) applications that require high-throughput, 24x7 operation. Examples include network monitoring, phone call processing, click-stream processing, and online financial analysis. Our main contribution is a scheme that carefully integrates traditional query processing techniques for partitioned parallelism with the process-pairs approach for high availability. This delicate integration allows us to tolerate failures of portions of a parallel dataflow  without sacrificing result quality. Upon failure, our technique provides quick fail-over, and automatically recovers the lost pieces on the fly. This piecemeal recovery provides minimal disruption to the ongoing dataflow computation and improved reliability as compared to the straight-forward application of the process-pairs technique on a per dataflow basis. Thus, our technique provides the high availability necessary for critical CQ applications. Our techniques are encapsulated in a reusable dataflow operator called Flux, an extension of the Exchange that is used to compose parallel dataflows. Encapsulating the fault-tolerance logic into Flux minimizes modifications to existing operator code and relieves the burden on the operator writer of repeatedly implementing and verifying this critical logic. We present experiments illustrating these features with an implementation of Flux in the TelegraphCQ code base [8].
229|A comparison of stream-oriented high availability algorithms|Recently, significant efforts have focused on developing novel data-processing systems to support a new class of applications that commonly require sophisticated and timely processing of high-volume data streams. Early work in stream processing has primarily focused on streamoriented languages and resource-constrained, one-pass query-processing. High availability, an increasingly important goal for virtually all data processing systems, is yet to be addressed. In this paper, we first describe how the standard highavailability approaches used in data management systems can be applied to distributed stream processing. We then propose a novel stream-oriented approach that exploits the unique data-flow nature of streaming systems. Using analysis and a detailed simulation study, we characterize the performance of each approach and demonstrate that the stream-oriented algorithm significantly reduces runtime overhead at the expense of a small increase in recovery time. 1
230|The STATEMATE Semantics of Statecharts|This article describes the semantics of the language of statecharts as implenented in the STATEMATE system [Harel et al. 1990; Harel and Politi 1996]. The initial version of this semantics was developed by a team about.10 years ago. With the added experience of the users of the system it has since been extended and modified. This executable semantics has been in operation in driving the simulation, dynamic tests, and code generation tDols of STATEMATE since 1987, and a technical report describing it has been available from i-Logix, Inc. since 1989. We have now decided to revise and publish the report so as to make it more widely accessible, to alleviate some of the confusion about the &#034;official semantics of the language, and to counter a number of incorrect comments made in the literature about the way statecharts have been implemented. For example, the survey [yon der Beek 1994] does not mention the STATEMATE implementation of statecharts or the semantics adopted for it at all, although this semantics is different from the ones surveyed therein (and was developed earlier than all of them except for Harel et al. [1987]). As another example, Leveson et al. [1995] describe a case that exhibits an unacceptable kind of behavior in a statechart, which they say is what the &#034;semantics of statecharts&#034; leads to (pp. 695-697). Unfortunately, they base their discussion of statechart semantics on one of the many semantics proposed by various authors (that of Pnueli and Shalev [1991]) and give the reader the impression that this is the official semantics of the language
231|Extending State Transition Diagrams for the Specification of Human-Computer Interaction |Abstract-User Software Engineering is a methodology for the speci-fication and implementation of interactive information systems. An early step in the methodology is the creation of a formal executable description of the user interaction with the system, based onaugmented state transition diagrams. This paper shows the derivation of the, USE transition diagrams based on perceived shortcomings of the &#034;pure&#034; state transition diagram approach. In this way, the features of the USE specification notation are gradually presented and illustrated. The paper shows both the graphical notation and the textual equivalent of the notation, and briefly describes the automated tools that support direct execution of the specification. This specification is easily encoded in a machine-processable form to create an executable form of the computer-human interaction. Index Terms-Executable specifications, interactive information sys-tems, rapid prototyping, software development methodology, transition diagrams, user interfaces, User Software Engineenng. I.
232|On the use of transition diagrams in the design of a user interface for an interactive computer system|This paper deals with what might be called the top level design of an interactive computer system. It examines some problems which arise in trying to specify what the user interface of such a system should be. It proposes a concept--the terminal state--and a notatlon--the terminal state transition diagram--which make the design of the top level somewhat easier. It also proposes a user interface in which the notion of terminal state is explicit. This user interface seems to provide a great improvement in flexibility and ease of adding sybsystems to a general purpose system.
233|Using Formal Specifications in the Design of a Human-Computer   Interface|Formal and semiformal specification techniques have been applied to many aspects of software development. Their value is that they permit a designer to describe precisely the external
234|Software Transactional Memory|As we learn from the literature, flexibility in choosing synchronization operations greatly simplifies the task of designing highly concurrent programs. Unfortunately, existing hardware is inflexible and is at best on the level of a Load Linked/Store Conditional operation on a single word. Building on the hardware based transactional synchronization methodology of Herlihy and Moss, we offer  software transactional memory (STM), a novel software method for supporting flexible transactional programming of synchronization operations. STM is non-blocking, and can be implemented on existing machines using only a  Load Linked/Store Conditional operation. We use STM to provide a general highly concurrent method for translating sequential object implementations to lock-free ones based on implementing a k-word compare&amp;swap STM-transaction. Empirical evidence collected on simulated multiprocessor architectures shows that the our method always outperforms all the lock-free translation methods in ...
235|Hierarchical correctness proofs for distributed algorithms|Abstract: We introduce the input-output automaton, a simple but powerful model of computation in asynchronous distributed networks. With this model we are able to construct modular, hierarchical correctness proofs for distributed algorithms. We de ne this model, and give aninteresting example of how itcan be used to construct such proofs. 1
236|The Drinking Philosophers Problem|The problem of resolving conflicts between processes in distributed systems is of practical importance. A conflict between a set of processes must be resolved in favor of some (usually one) process and against the others: a favored process must have some property that distinguishes it from others. To guarantee fairness, the distinguishing property must be such that the process selected for favorable treatment is not always the same. A distributed implementation of an acyclic precedence graph, in which the depth of a process (the longest chain of predecessors) is a distinguishing property, is presented. A simple conflict resolution rule coupled with the acyclic graph ensures fair resolution of all conflicts. To make the problem concrete, two paradigms are presented: the well-known distributed dining philosophers problem and a generalization of it, the distributed drinking philosophers problem.
237|The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor|The Alewife multiprocessor project focuses on the architecture and design of a large-scale parallel machine. The machine uses a low-dimensional direct interconnection network to provide scalable communication bandwidth, while allowing the exploitation of locality. Despite its distributed-memory architecture, Alewife allows efficient shared-memory programming through a multilayered approach to locality management. A new scalable cache-coherence scheme called LimitLESS directories allows the use of caches for reducing communication latency and network bandwidth requirements. Alewife also employs run-time and compile-time methods for partitioning and placement of data and processes to enhance communication locality. While the above methods attempt to minimize communication latency, communication with distant processors cannot be completely avoided. Alewife&#039;s processor, Sparcle, is designed to tolerate these latencies by rapidly switching between threads of computation. This paper describe...
238|Adding Networks|  An adding network is a distributed data structure that supports a concurrent, lock-free, low-contention implementation of a fetch&amp;add counter; a counting network is an instance of an adding network that supports only fetch&amp;increment. We present a lower bound showing that adding networks have inherently high latency. Any adding network powerful enough to support addition by at least two values a and b, where |a |&gt; |b |&gt; 0, has sequential executions in which each token traverses ?(n/c) switching elements, where n is the number of concurrent processes, and c is a quantity we call one-shot contention; for a large class of switching networks and for conventional counting networks the one-shot contention is constant. On the contrary, counting networks have O(log n) latency [4,7]. This bound is tight. We present the first concurrent, lock-free, lowcontention networked data structure that supports arbitrary fetch&amp;add operations.  
239|Synchronization Without Contention|Conventional wisdom holds that contention due to busy-wait synchronization is a major obstacle to scalability and acceptable performance in large shared-memory multiprocessors. We argue the contrary, and present fast, simple algorithms for contention-free mutual exclusion, reader-writer control, and barrier synchronization. These algorithms, based on widely available fetch_and_phi instructions, exploit local access to shared memory to avoid contention. We compare our algorithms to previous approaches in both qualitative and quantitative terms, presenting their performance on the Sequent Symmetry and BBN Butterfly multiprocessors. Our results highlight the importance of local access to shared memory, provide a case against the construction of so-called &#034;dance hall&#034; machines, and suggest that special-purpose hardware support for synchronization is unlikely to be cost effective on machines with sequentially consistent memory.
240|A simple load balancing scheme for task allocation in parallel machines |A collection of local workpiles (task queues) and a simple load balancing scheme is well suited for scheduling tasks in shared memory parallel machines. Task scheduling on such machines has usually been done through a single, globally accessible, workpile. The scheme introduced in this paper achieves a balancing comparable to that of a global workpile, while minimizing the overheads. In many parallel computer architectures, each processor has some memory that it can access more efficiently, and so it is desirable that tasks do not mirgrate frequently. The load balancing is simple and distributed: Whenever a processor accesses its local workpile, it performs a balancing operation with probability inversely proportional to the size of its workpile. The balancing operation consists of examining the workpile of a random processor and exchanging tasks so as to equalize the size of the two workpiles. The probabilistic analysis of the performance of the load balancing scheme proves that each tasks in the system receives its fair share of computation time. Specifically, the expected size of each local task queue is within a small constant factor of the average, i.e. total number of tasks in the system divided by the number of processors. 1
241|A method for implementing lock-free shared data structures|barnesQmpi-sb.mpg.de
242|Diffracting trees|Shared counters are among the most basic coordination structures in multiprocessor computation, with applications ranging from barrier synchronization to concurrent-data-structure design. This article introduces diffracting trees, novel data structures for shared counting and load balancing in a distributed/parallel environment. Empirical evidence, collected on a simulated distributed shared-memory machine and several simulated message-passing architectures, shows that diffracting trees scale better and are more robust than both combining trees and counting networks, currently the most effective known methods for implementing concurrent counters in software. The use of a randomized coordination method together with a combinatorial data structure overcomes the resiliency drawbacks of combining trees. Our simulations show that to handle the same load, diffracting trees and counting networks should have a similar width w, yet the depth of a diffracting tree is O(log w), whereas counting networks have depth O(log 2 w). Diffracting trees have already been used to implement highly efficient producer/consumer queues, and we believe diffraction will prove to be an effective alternative paradigm to combining and queue-locking in the design of many concurrent data structures.
243|A performance evaluation of lock-free synchronization protocols|In this paper, we investigate the practical performance of lock-free techniques that provide synchronization on shared-memory multiprocessors. Our goal is to provide a technique to allow designers of new protocols to quickly determine an algorithm’s performance characteristics. We develop a simple analytical performance model based on the architectural observations that memory accesses are expensive, synchronization instructions are more expensive, and that optimistic synchronization policies result in wasted communication bandwidth which can slow the system as a whole. Using our model, we evaluate the performance of five existing lock-free synchronization protocols. We validate our analysis by comparing our results with simulations of a parallel machine. Given this analysis, we identify those protocols which show promise of good performance in practice. In addition, we note that no existing protocols provide insensitivity to common delays while still offering performance equivalent to locks. Accordingly, we introduce a protocol, based on a combination of existing lock-free techniques, which satisfies these criteria. 1
244|Cache Coherence Protocols for Large-Scale Multiprocessors|in partial ful llment of the requirements for the degree of
245|Reliable Communication in the Presence of Failures|The design and correctness of a communication facility for a distributed computer system are reported on. The facility provides support for fault-tolerant process groups in the form of a family of reliable multicast protocols that can be used in both local- and wide-area networks. These protocols attain high levels of concurrency, while respecting application-specific delivery ordering constraints, and have varying cost and performance that depend on the degree of ordering desired. In particular, a protocol that enforces causal delivery orderings is introduced and shown to be a valuable alternative to conventional asynchronous communication protocols. The facility also ensures that the processes belonging to a fault-tolerant process group will observe consistent orderings of events affecting the group as a whole, including process failures, recoveries, migration, and dynamic changes to group properties like member rankings. A review of several uses for the protocols in the ISIS system, which supports fault-tolerant resilient objects and bulletin boards, illustrates the significant simplification of higher level algorithms made possible by our approach.
246|Fail-Stop Processors: An Approach to Designing Fault-Tolerant Computing Systems|This paper was originally submitted to ACM Transactions on Programming Languages and Systems. The responsible editor was Susan L. Graham. The authors and editor kindly agreed to transfer the paper to the ACM Transactions on Computer Systems
247|Atomic Broadcast: From Simple Message Diffusion to Byzantine Agreement|In distributed systems subject to random communication delays and component failures, atomic broadcast can be used to implement the abstraction of synchronous replicated storage, a distributed storage that displays the same contents at every correct processor as of any clock time. This paper presents a systematic derivation of a family of atomic broadcast protocols that are tolerant of increasingly general failure classes: omission failures, timing failures, and authentication-detectable Byzantine failures. The protocols work for arbitrary point-to-point network topologies, and can tolerate any number of link and process failures up to network partitioning. After proving their correctness, we also prove two lower bounds that show that the protocols provide in many cases the best possible termination times. Keywords and phrases: Atomic Broadcast, Byzantine Agreement, Computer Network, Correctnesss, Distributed System, Failure Classification, Fault-Tolerance, Lower Bound, Real-Time Syste...
248|Distributed Process Groups in the V Kernel|The V kernel supports an abstraction of processes, with operations for interprocess communication, process management, and memory management. This abstraction is used as a software base for constructing distributed systems. As a distributed kernel, the V kernel makes intermachine bound-aries largely transparent. In this environment of many cooperating processes on different machines, there are many logical groups of processes. Examples include the group of tile servers, a group of processes executing a particular job, and a group of processes executing a distributed parallel computation. In this paper we describe the extension of the V kernel to support process groups. Operations on groups include group interprocess communication, which provides an application-level abstraction of network multicast. Aspects of the implementation and performance, and initial experience with applications are discussed.
249|Maintaining Availability in Partitioned Replicated Databases|In a replicated database, a data item may have copies residing on several sites. A replica control protocol is necessary to ensure that data items with several copies behave as if they consist of a single copy, as far as users can tell. We describe a new replica control protocol that allows the accessing of data in spite of site failures and network partitioning. This protocol provides the database designer with a large degree of flexibility in deciding the degree of data availability, as well as the cost of accessing data.
250|The Application of Petri Nets to Workflow Management|Workflow management promises a new solution to an age-old problem: controlling, monitoring, optimizing and supporting business processes. What is new about workflow management is the explicit representation of the business process logic which allows for computerized support. This paper discusses the use of Petri nets in the context of workflow management. Petri nets are an established tool for modeling and analyzing processes. On the one hand, Petri nets can be used as a design language for the specification of complex workflows. On the other hand, Petri net theory provides for powerful analysis techniques which can be used to verify the correctness of workflow procedures. This paper introduces workflow management as an application domain for Petri nets, presents state-of-the-art results with respect to the verification of workflows, and highlights some Petri-net-based workflow tools.  
251|The Entity-Relationship Model: Toward a Unified View of Data|A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, infor-mation retrieval, and data manipulation are discussed. The entity-relationship model can be used as a basis for unification of different views of data: t,he network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented. Key Words and Phrases: database design, logical view of data, semantics of data, data models, entity-relationship model, relational model, Data Base Task Group, network model, entity set
252|THREE GOOD REASONS FOR USING A PETRI-NET-BASED WORKFLOW MANAGEMENT SYSTEM |Currently, the Dutch Customs Department is building a nationwide information system to handle all kinds of declarations related to the import and export of goods. For this purpose the Petri-net-based Work ow Management System (WFMS) named COSA has been selected. During the selection process, it turned out that there are several reasons for insisting on a Petri-net-based WFMS. The three main reasons for selecting a Petri-net-based WFMS are discussed in this paper. In our opinion these reasons are also relevant for many other projects involved in the selection or implementation of a WFMS. 
253|Complexity Results for 1-safe Nets|We study the complexity of several standard problems for 1-safe Petri nets and some of its subclasses. We prove that reachability, liveness, and deadlock are all PSPACE-complete for 1-safe nets. We also prove that deadlock is NP-complete for free-choice nets and for 1-safe free-choice nets. Finally, we prove that for arbitrary Petri nets, deadlock is equivalent to reachability and liveness.  This paper is to be presented at FST&amp;TCS 13, Foundations of Software Technology &amp; Theoretical Computer Science, to be held 1517 December 1993, in Bombay, India. A version of the paper with most proofs omitted is to appear in the proceedings. 1 Introduction Petri nets are one of the oldest and most studied formalisms for the investigation of concurrency [33]. Shortly after the birth of complexity theory, Jones, Landweber, and Lien studied in their classical paper [24] the complexity of several fundamental problems for Place/Transition nets (called in [24] just Petri nets). Some years later, Howell,...
254|Structural Characterizations of Sound Workflow Nets|this paper we present a method based on Petri nets. This analysis method exploits the structure of the Petri net to find potential errors in the design of the workflow. Moreover, the analysis method allows for the compositional verification of workflows.
255|The Simple Control Property of Business Process Models|this paper we are only concerned with the specification and analysis of process behavior so we do not mention other process&#039; perspectives. In particular, we will specify behavior in a so-called coordination model independently from the organizational context. Kellner [2] defines behavior as &#034;when the process elements are performed (e.g., sequencing), as well as aspects of how they are performed through feedback loops, iteration, complex decision-making conditions, entry and exit criteria, and so forth.&#034; In most workflow systems a user represents control flow graphically usually with a notation that resembles both a control flow diagram and a PERT net; the semantics is usually defined in terms of Petri nets (e.g., ICN [6], Macronets [9]). Complex processes can create behavioral anomalies. Examples of anomalous behavior are processes that deadlock (cannot proceed and have not yielded a result) or activities whose results are not used by other activities. Sequential processes cannot have control anomalies (except for infinite loops). On the other hand, parallel processes without choice---which are basically PERT charts--- cannot have control anomalies [16]. Thus, it is not surprising that the usual way to avoid these anomalies is by defining simple-minded models that inhibit the natural parallelism of activities and/or that abstract away the handling of exceptional cases. This article reports on a study of behavioral properties of process models, part of a larger effort to develop a modelling tool and method for the development of safe process models. Our tool
256|Temporal and modal logic|We give a comprehensive and unifying survey of the theoretical aspects of Temporal and  modal logic.
257|Design and Synthesis of Synchronization Skeletons Using Branching Time Temporal Logic|We propose a method of constructing concurrent programs in which the synchroni-zation skeleton of the program ~s automatically synthesized from a high-level (branching time) Temporal Logic specification. The synchronization skeleton is an abstraction of the actual program where detail irrelevant to synchronization is
258|Knowledge and Common Knowledge in a Distributed Environment|: Reasoning about knowledge seems to play a fundamental role in distributed systems. Indeed, such reasoning is a central part of the informal intuitive arguments used in the design of distributed protocols. Communication in a distributed system can be viewed as the act of transforming the system&#039;s state of knowledge. This paper presents a general framework for formalizing and reasoning about knowledge in distributed systems. We argue that states of knowledge of groups of processors are useful concepts for the design and analysis of distributed protocols. In particular, distributed knowledge  corresponds to knowledge that is &#034;distributed&#034; among the members of the group, while  common knowledge corresponds to a fact being &#034;publicly known&#034;. The relationship between common knowledge and a variety of desirable actions in a distributed system is illustrated. Furthermore, it is shown that, formally speaking, in practical systems common knowledge cannot be attained. A number of weaker variants...
259|Checking that finite state concurrent programs satisfy their linear specification|We present an algorithm for checking satisfiabil-ity of a linear time temporal logic formula over a finite state concurrent program. The running time of the al-gorithm is exponential in the size of the formula but lin-ear in the size of the checked program. The algorithm yields also a formal proof in case the formula is valid over the program. The algorithm has four versions that check satisfiability by unrestricted, impartial, just and fair computations of the given program.
260|Decision Procedures and Expressiveness in the Temporal Logic of Branching Time|We consider the computation tree logic (CTL) proposed in (Set. Comput. Programming 2
261|Reasoning about networks with many identical  processes|Consider a distributed mutual exclusion algorithm fur processes ar-ranged in a ring network in which mutual exclusion is guaranteed by means of a token that is passed around the ring ( [6], [10], [12]). How can we determine that such a system of processes is correct? Our first attempt might be to consider a reduced system with one or two processes. If we can show that the reduced system is correct and if the individual processes are really identical, then we are tempted to con-elude that the entire system will be correct.. In fact, this type of informal argument is used quite frequently by designers in constructing systems that contain large numbers of identical processing elements. Of course, it is easy to contrive an example in which some pathological behavior only occurs when, say, 100 processes are connected together. By ex-amining a system with only one or two processes it might even be quite difficult to determine that this behavior is possible. Nevertheless, one has the feeling that in many cases this kind of intuitive reasoning does
262|The Logic of Distributed Protocols|A propositional logic of distributed protocols is introduced which includes both the  logic of knowledge and temporal logic. Phenomena in distributed computing systems  such as asynchronous time, incomplete knowledge by the computing agents in the  system, and game-like behavior among the computing agents are all modeled in the  logic. Two versions of the logic, the linear logic of protocols (LLP) and the tree logic  of protocols (TLP) are investigated. The main result is that the set of valid formulas  in LLP is undecidable.
263|Requirements for Internet Hosts - Communication Layers|This RFC is an official specification for the Internet community. It incorporates by reference, amends, corrects, and supplements the primary protocol standards documents relating to hosts. Distribution of this document is unlimited. Summary This is one RFC of a pair that defines and discusses the requirements for Internet host software. This RFC covers the communications protocol layers: link layer, IP layer, and transport layer; its companion RFC-1123 covers the application and support protocols.
264|TCP Extensions for Long-Delay Paths|This memo proposes a set of extensions to the TCP protocol to provide efficient operation over a path with a high bandwidth*delay product. These extensions are not proposed as an Internet standard at this time. Instead, they are intended as a basis for further experimentation and research on transport protocol performance. Distribution of this memo is unlimited. 1.
265|A Note on Reliable Full-Duplex Transmission over ~;-+.~;~plex Lines|A simple procedure for achieving reliable full-duplex transmission over half-duplex links is proposed. The scheme is compared with another of the same type, which has recently been described in the literature. Finally, some comments are made on another group of related transmission procedures which have been shown to be unreliable under some circumstances.
266|A Review of Current Routing Protocols for Ad-Hoc Mobile Wireless Networks   |An ad-hoc mobile network is a collection of mobile nodes that are dynamically and arbitrarily located in such a manner that the interconnections between nodes are capable of changing on a continual basis. In order to facilitate communication within the network, a routing protocol  is used to discover routes between nodes. The primary goal of such an ad-hoc network routing protocol is correct and efficient route establishment between a pair of nodes so that messages may  be delivered in a timely manner. Route construction should be done with a minimum of overhead  and bandwidth consumption. This paper examines routing protocols for ad-hoc networks and  evaluates these protocols based on a given set of parameters. The paper provides an overview of  eight different protocols by presenting their characteristics and functionality, and then provides a comparison and discussion of their respective merits and drawbacks.
267|Ad-hoc On-Demand Distance Vector Routing|An ad-hoc network is the cooperative engagement of a collection of mobile nodes without the required intervention of any centralized access point or existing infrastructure. In this paper we present Ad-hoc On Demand Distance Vector Routing (AODV), a novel algorithm for the operation of such ad-hoc networks. Each Mobile Host operates as a specialized router, and routes are obtained as needed (i.e., on-demand) with little or no reliance on periodic advertisements. Our new routing algorithm is quite suitable for a dynamic selfstarting network, as required by users wishing to utilize ad-hoc networks. AODV provides loop-free routes even while repairing broken links. Because the protocol does not require global periodic routing advertisements, the demand on the overall bandwidth available to the mobile nodes is substantially less than in those protocols that do necessitate such advertisements. Nevertheless we can still maintain most of the advantages of basic distance-vector routing mechanisms. We show that our algorithm scales to large populations of mobile nodes wishing to form ad-hoc networks. We also include an evaluation methodology and simulation results to verify the operation of our algorithm.
268|Dynamic source routing in ad hoc wireless networks|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any established infrastructure or centralized administration. In such an environment, it may be necessary for one mobile host to enlist the aid of other hosts in forwarding a packet to its destination, due to the limited range of each mobile host’s wireless transmissions. This paper presents a protocol for routing in ad hoc networks that uses dynamic source routing. The protocol adapts quickly to routing changes when host movement is frequent, yet requires little or no overhead during periods in which hosts move less frequently. Based on results from a packet-level simulation of mobile hosts operating in an ad hoc network, the protocol performs well over a variety of environmental conditions such as host density and movement rates. For all but the highest rates of host movement simulated, the overhead of the protocol is quite low, falling to just 1 % of total data packets transmitted for moderate movement rates in a network of 24 mobile hosts. In all cases, the difference in length between the routes used and the optimal route lengths is negligible, and in most cases, route lengths are on average within a factor of 1.01 of optimal. 1.
269|Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers  (1994) |An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize adhoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.  
270|A Highly Adaptive Distributed Routing Algorithm for Mobile Wireless Networks|We present a new distributed routing protocol for mobile, multihop, wireless networks. The protocol is one of a family of protocols which we term &#034;link reversal&#034; algorithms. The protocol&#039;s reaction is structured as a temporally-ordered sequence of diffusing computations; each computation consisting of a sequence of directed l i nk reversals. The protocol is highly adaptive, efficient and scalable; being best-suited for use in large, dense, mobile networks. In these networks, the protocol&#039;s reaction to link failures typically involves only a localized &#034;single pass&#034; of the distributed algorithm. This capability is unique among protocols which are stable in the face of network partitions, and results in the protocol&#039;s high degree of adaptivity. This desirable behavior is achieved through the novel use of a &#034;physical or logical clock&#034; to establish the &#034;temporal order&#034; of topological change events which is used to structure (or order) the algorithm&#039;s reaction to topological changes. We refer to the protocol as the Temporally-Ordered Routing Algorithm (TORA).
271|Cognitive networks|Abstract — This paper presents a definition and framework for a novel type of adaptive data network: the cognitive network. In a cognitive network, the collection of elements that make up the network observes network conditions and then, using prior knowledge gained from previous interactions with the network, plans, decides and acts on this information. Cognitive networks are different from other “intelligent ” communication technologies because these actions are taken with respect to the end-to-end goals of a data flow. In addition to the cognitive aspects of the network, a specification language is needed to translate the user’s end-to-end goals into a form understandable by the cognitive process. The cognitive network also depends on a Software Adaptable Network that has both an external interface accessible to the cognitive network and network status sensors. These devices are used to provide control and feedback. The paper concludes by presenting a simple case study to illustrate a cognitive network and its framework. I.
272|Location-Aided Routing (LAR) in mobile ad hoc networks  (1998) |A mobile ad hoc network consists of wireless hosts that may move often. Movement of hosts results in a change in routes, requiring some mechanism for determining new routes. Several routing protocols have already been proposed for ad hoc networks. This paper suggests an approach to utilize location information (for instance, obtained using the global positioning system) to improve performance of routing protocols for ad hoc networks. By using location information, the proposed Location-Aided Routing (LAR) protocols limit the search for a new route to a smaller “request zone” of the ad hoc network. This results in a significant reduction in the number of routing messages. We present two algorithms to determine the request zone, and also suggest potential optimizations to our algorithms.
273|Power-Aware Routing in Mobile Ad Hoc Networks|In this paper we present a case for using new power-aware metrics for determining routes in wireless  ad hoc networks. We present five different metrics based on battery power consumption at nodes. We  show that using these metrics in a shortest-cost routing algorithm reduces the cost/packet of routing  packets by 5-30% over shortest-hop routing (this cost reduction is on top of a 40-70% reduction in energy  consumption obtained by using PAMAS, our MAC layer protocol). Furthermore, using these new metrics  ensures that the mean time to node failure is increased significantly. An interesting property of using  shortest-cost routing is that packet delays do not increase. Finally, we note that our new metrics can  be used in most traditional routing protocols for ad hoc networks.  1 
274|An Efficient Routing Protocol for Wireless Networks|We present
275|Routing In Clustered Multihop, Mobile Wireless Networks With Fading Channel|A clusterhead-token infrastructure for multihop, mobile wireless networks has been designed. Traditional routing algorithms in wireline networks are not feasible for mobile wireless environment due to the dynamic change in link connectivity. To gain better performance for clustered multihop, mobile wireless networks, routing must take into account radio channel access, code scheduling, and channel reservation. In this paper, we propose some heuristic routing schemes for clustered multihop, mobile wireless networks. A packet delay improvement up to fourfold has been observed in our simulations compared with shortest-pathscheme, making multimedia tra c viable. A radio channel model has been included to investigate the impact of channel fading on our protocols. To reduce the run time, a parallel simulator has been designed. Speedups of up to tenfold have been observed on a 16 processor SP/2.   
276|Signal Stability based Adaptive Routing (SSA) for Ad-Hoc Mobile Networks  (1997) |Unlike static networks, ad-hoc networks have no spatial hierarchy and suffer from frequent link failures which prevent mobile hosts from using traditional routing schemes. Under these conditions, mobile hosts must find routes to destinations without the use of designated routers and also must dynamically adapt the routes to the current link conditions. This paper proposes a distributed adaptive routing protocol for finding and maintaining stable routes based on signal strength and location stability in an ad-hoc network and presents an architecture for its implementation. 1 Introduction  Mobility is becoming increasingly important for users of computing systems. Technology has made possible wireless devices and smaller, less expensive, and more powerful computers. As a result users gain flexibility and the ability to maintain connectivity to their primary computer while roaming through a large area. The number of users with portable laptops and personal communications devices is increa...
277|Asynchronous Multimedia Multihop Wireless Networks+|Personal communications and mobile computing will require a wireless network infrastructure which is fast deployable, possibly multihop, and capable of multimedia service support. The first infrastructure of this type was the Packet Radio Network (PRNET), developed in the 70&#039;s to address the battlefield and disaster recovery communication requirements. PRNET was totally asynchronous and was based on a completely distributed architecture. It handled datagram traffic reasonably well, but did not offer efficient multimedia support. Recently, under the WAMIS and Glomo ARPA programs several mobile, multimedia, multihop (M  3  ) wireless network architectures have been developed, which assume some form of synchronous, time division infrastructure. The synchronous time frame leads to efficient multimedia support implementations. However, it introduces more complexity and is less robust in the face of mobility and channel fading. In this paper, we examine the impact of synchronization on wirel...
278|Adaptive Shared Tree Multicast in Mobile Wireless Networks|Shared Tree multicast is a well established concept used in several multicast protocols for wireline networks (eg. Core Base Tree, PIM sparse mode etc). In this paper, we extend the Shared Tree concept to wireless, mobile, multihop networks for applications ranging from ad hoc networking to disaster recovery and battlefield. The main challenge in wireless, mobile networks is the rapidly changing environment. We address this issue in our design by: (a) using &#034;soft state&#034;; (b) assigning different roles to nodes depending on their mobility (two level mobility model); (c) proposing an adaptive scheme which combines shared tree and source tree benefits. A detailed wireless simulation model is used to evaluate the proposed schemes and compare them with source based tree (as opposed to shared tree) multicast. Both uniform and 2-level mobility models are used in the comparison. The results show that shared tree protocols have low overhead and are very robust to mobility. In particular, the Ada...
279|Tree Multicast Strategies in Mobile, Multihop Wireless Networks|this paper, we extend the tree multicast concept to wireless, mobile, multihop networks for applications ranging from ad hoc networking to disaster recovery and battlefield. The main challenge in wireless, mobile networks is the rapidly changing environment. We address this issue in our design by: (a) using &#034;soft state&#034;; (b) assigning different roles to nodes depending on their mobility (2-level mobility model); (c) proposing an adaptive scheme which combines shared tree and per-source tree benefits, and; (d) dynamically relocating the shared tree Rendezvous Point (RP ). A detailed wireless simulation model is used to evaluate various multicast schemes. The results show that per-source trees perform better in heavy loads because of the more efficient traffic distribution; while shared trees are more robust to mobility and are more scalable to large network sizes. The adaptive tree multicast scheme, a hybrid between shared tree and per-source tree, combines the advantages of both and performs consistently well across all load and mobility scenarios. The main contributions of this study are: the use of a 2-level mobility model to improve the stability of the shared tree; the development of a hybrid, adaptive per-source and shared tree scheme, and; the dynamic relocation of the RP in the shared tree.
280|Loop-Free Internet Routing Using Hierarchical Routing Trees|We present a new hierarchical routing algorithm that combines the loop-free path-finding algorithm (LPA) with the area-based hierarchical routing scheme first proposed by McQuillan for distance-vector algorithms. The new algorithm, which we call the Hierarchical Information Path-based Routing (HIPR) algorithm, accommodates an arbitrary number of aggregation levels and can be viewed as a distributed version of Dijkstra&#039;s algorithm running over a hierarchical graph. HIPR is verified to be loop-free and correct. Simulations are used to show that HIPR is much more efficient than OSPF in terms of speed, communication and processing overhead required to converge to correct routing tables. HIPR constitutes the basis for future Internet routing protocols that are as simple as RIPv2, but with no looping and better performance than protocols based on link-states.  1. Introduction  Routing information maintained at each router has to be updated frequently to adapt to network dynamics. In a flat r...
281|Concurrent Constraint Programming|This paper presents a new and very rich class of (con-current) programming languages, based on the notion of comput.ing with parhal information, and the con-commitant notions of consistency and entailment. ’ In this framework, computation emerges from the inter-action of concurrently executing agents that communi-cate by placing, checking and instantiating constraints on shared variables. Such a view of computation is in-teresting in the context of programming languages be-cause of the ability to represent and manipulate partial information about the domain of discourse, in the con-text of concurrency because of the use of constraints for communication and control, and in the context of AI because of the availability of simple yet powerful mechanisms for controlling inference, and the promise that very rich representational/programming languages, sharing the same set of abstract properties, may be pos-sible. To reflect this view of computation, [Sar89] develops the cc family of languages. We present here one mem-ber of the family, CC(.L,+) (pronounced “cc with Ask and Choose”) which provides the basic operations of blocking Ask and atomic Tell and an algebra of be-haviors closed under prefixing, indeterministic choice, interleaving, and hiding, and provides a mutual recur-sion operator. cc(.L,-t) is (intentionally!) very similar to Milner’s CCS, but for the radically different under-lying concept of communication, which, in fact, pro-’ The class is founded on the notion of “constraint logic pro-gramming ” [JL87,Mah87], fundamentally generalizes concurrent logic programming, and is the subject of the first author’s disser-tation [Sar89], on which this paper is substantially based.
282|The Semantics Of Constraint Logic Programs|This paper presents for the first time  the semantic foundations of CLP in a self-contained and complete package.  The main contributions are threefold. First, we extend the original conference  paper by presenting definitions and basic semantic constructs from  first principles, giving new and complete proofs for the main lemmas. Importantly,  we clarify which theorems depend on conditions such as solution  compactness, satisfaction completeness and independence of constraints.  Second, we generalize the original results to allow for incompleteness of the  constraint solver. This is important since almost all CLP systems use an  incomplete solver. Third, we give conditions on the (possibly incomplete)  solver which ensure that the operational semantics is confluent, that is, has  independence of literal scheduling.  
283|Guarded Horn Clauses|This thesis introduces the programming language Guarded Horn Clauses which is abbreviated to GHC. Guarded Horn Clauses was born from the examination of existing logic programming languages and logic programming in general, with special attention paid to parallelism. The main feature of
284|A Feature Logic with Subsorts|This paper presents a set description logic with subsorts, feature selection  (the inverse of unary function application), agreement, intersection,  union and complement. We define a model theoretic open world  semantics and show that sorted feature structures constitute a canonical  model, that is, without loss of generality subsumption and consistency  of set descriptions can be considered with respect to feature  structures only. We show that deciding consistency of set descriptions  is an NP-complete problem.  To appear in:  J. Wedekind and C. Rohrer (eds.), Unification in Grammar.  The MIT Press, 1992  This text is a minor revision of LILOG Report 33, May 1988, IBM Deutschland, IWBS, Postfach 800880, 7000 Stuttgart 80, Germany. The research reported here has been done while the author was with IBM Deutschland. The author&#039;s article [23] is a more recent work on feature logics.  1  1 Introduction  This paper presents a set description logic that generalizes and integrates formalisms...
285|The emotional dog and its rational tail: a social intuitionist approach to moral judgment|This is the manuscript that was published, with only minor copy-editing alterations, as: Haidt, J. (2001). The emotional dog and its rational tail: A social intuitionist approach to moral judgment. Psychological Review. 108, 814-834 Copyright 2001, American Psychological Association To obtain a reprint of the final type-set article, please go through your library’s journal services, or contact the author directly Research on moral judgment has been dominated by rationalist models, in which moral judgment is thought to be caused by moral reasoning. Four reasons are given for considering the hypothesis that moral reasoning does not cause moral judgment; rather, moral reasoning is usually a post-hoc construction, generated after a judgment has been reached. The social intuitionist model is presented as an alternative to rationalist models. The model is a social model in that it de-emphasizes the private reasoning done by individuals, emphasizing instead the importance of social and cultural influences. The model is an intuitionist model in that it states that moral judgment is generally the result of quick, automatic evaluations (intuitions). The model is more consistent than rationalist models with recent findings in social, cultural, evolutionary, and biological psychology, as well as anthropology and primatology. Author notes
287|Reasoning the fast and frugal way: Models of bounded rationality|Humans and animals make inferences about the world under limited time and knowledge. In contrast, many models of rational inference treat the mind as a Laplacean Demon, equipped with unlimited time, knowledge, and computational might. Following H. Simon’s notion of satisficing, the authors have proposed a family of algorithms based on a simple psychological mechanism: one reason decision making. These fast and frugal algorithms violate fundamental tenets of classical rationality: They neither look up nor integrate all information. By computer simulation, the authors held a competition between the satisficing “Take The Best ” algorithm and various “rational ” inference procedures (e.g., multiple regression). The Take The Best algorithm matched or outperformed all competitors in inferential speed and accuracy. This result is an existence proof that cognitive mechanisms capable of successful performance in the real world do not need to satisfy the classical norms of rational inference. Organisms make inductive inferences. Darwin (1872/1965) observed that people use facial cues, such as eyes that waver and lids that hang low, to infer a person’s guilt. Male toads, roaming through swamps at night, use the pitch of a rival’s croak to infer its size when deciding whether to fight (Krebs &amp; Davies, 1987). Stock brokers must make fast decisions about which of several stocks to trade or invest when only limited information is available. The list goes on. Inductive
288|The unbearable automaticity of being|What was noted by E. J. hanger (1978) remains true today: that much of contemporary psychological research is based on the assumption that people are consciously and systematically processing incoming information in order to construe and interpret their world and to plan and engage in courses of action. As did E. J. hanger, the authors question this assumption. First, they review evidence that the ability to exercise such conscious, intentional control is actually quite limited, so that most of moment-to-moment psychological life must occur through nonconscious means if it is to occur at all. The authors then describe the different possible mechanisms that produce automatic, environmental control over these various phenomena and review evidence establishing both the existence of these mechanisms as well as their consequences for judgments, emotions, and
289|Integration of the cognitive and the psychodynamic unconscious|Cognitive-experiential self-theory integrates the cognitive and the psychodynamic unconscious by assuming the ex-istence of two parallel, interacting modes of information processing: a rational system and an emotionally driven experiential system. Support for the theory is provided by the convergence of a wide variety of theoretical positions on two similar processing modes; by real-life phenom-ena—such as conflicts between the heart and the head; the appeal of concrete, imagistic, and narrative represen-tations; superstitious thinking; and the ubiquity of religion throughout recorded history—and by laboratory research, including the prediction of new phenomena in heuristic reasoning. N early 100 years ago, Freud introduced a dualtheory of information processing that placeddeviant behavior squarely in the realm of the natural sciences and, more particularly, in psychology.
290|Thin slices of expressive behavior as predictors of interpersonal consequences: A meta-analysis|A meta-analysis was conducted on the accuracy of predictions of various objective outcomes in the areas of social and clinical psychology from short observations of expressive behavior (under 5 min). The overall effect size (/) for the accuracy of predictions for 38 different results was.39. Studies using longer periods of behavioral observation did not yield greater predictive accuracy; predictions based on observations under Vi min in length did not differ significantly from predic-tions based on 4- and 5-min observations. The type of behavioral channel (such as the face, speech, the body, tone of voice) on which the ratings were based was not related to the accuracy of predic-tions. Accuracy did not vary significantly between behaviors manipulated in a laboratory and more naturally occurring behavior. Last, effect sizes did not differ significantly for predictions in the areas of clinical psychology, social psychology, and the accuracy of detecting deception. The way in which people move, talk, and gesture—their fa-cial expressions, posture, and speech—all contribute to the for-mation of impressions about them. Many of the judgments we make about others in our everyday lives are based on cues from these expressive behaviors. Gordon Allport (1937) believed that
291|What is Beautiful is Good|A person&#039;s physical appearance, along with his sexual identity, is the personal characteristic that is most obvious and accessible to others in social inter-action. The present experiment was designed to determine whether physically attractive stimulus persons, both male and female, are (a) assumed to possess more socially desirable personality traits than physically unattractive stimulus persons and (6) expected to lead better lives (e.g., be more competent husbands and wives, be more successful occupationally, etc.) than unattrac-tive stimulus persons. Sex of Subject X Sex of Stimulus Person interactions along these dimensions also were investigated. The present results indicate a &#034;what is beautiful is good &#034; stereotype along the physical attractiveness dimen-sion with no Sex of Judge X Sex of Stimulus interaction. The implications of such a stereotype on self-concept development and the course of social inter-action are discussed. A person&#039;s physical appearance, along with his sexual identity, is the personal character-
292|Evidence for terror management theory II: The effects of mortality salience on reactions to those who threaten or bolster the cultural worldview|Three experiments were conducted to test the hypothesis, derived from terror management theory, that reminding people of their mortality increases attraction to those who consensually validate their beliefs and decreases attraction to those who threaten their beliefs. In Study 1, subjects with a Chris-tian religious background were asked to form impressions of Christian and Jewish target persons. Before doing so, mortality was made salient to half of the subjects. In support of predictions, mortal-ity salience led to more positive evaluations of the in-group member (the Christian) and more nega-tive evaluations of the out-group member (the Jew). In Study 2, mortality salience led to especially negative evaluations of an attitudinally dissimilar other, but only among subjects high in authoritari-anism. In Study 3, mortality salience led to especially positive reactions to someone who directly praised subjects &#039; cultural worldviews and especially negative reactions to someone who criticized them. The implications of these findings for understanding in-group favoritism, prejudice, and intol-erance of deviance are discussed. One of the most destructive and perplexing problems facing contemporary society is the pervasive tendency of people to re-spond with hostility and disdain toward those who are different
293|Getting at the truth or getting along: Accuracy- versus impression-motivated heuristic and systematic processing|Two studies examined the heuristic and systematic processing of accuracy- versus impression-motivated individuals expecting a discussion with a partner believed to hold either a favorable or unfavorable opinion on the discussion issue. Given the goal of having a pleasant interaction, impressionmotivated (versus accuracy-motivated) participants in both studies were particularly likely to express attitudes that were evaluatively consistent with the partner&#039;s opinion, reflecting their selective use of a &#034;go along to get along &#034; heuristic. Study 2 yielded stronger evidence for the distinct nature of heuristic and systematic processing in the service of accuracy versus impression goals. In this study, the evaluative implication of impression-motivated participants &#039; low-effort application of a &#034;go along to get along &#034; heuristic biased their more effortful, systematic processing, leading to attitudes consistent with the partner&#039;s views. In contrast, given the goal of determining an accurate issue opinion, accuracy-motivated participants exhibited relatively evenhanded systematic processing, resulting in attitudes unbiased by the partner&#039;s opinion. The results underscore the utility of a dual-process approach to understanding motivated cognition. Intuition and experience suggest that various motives can influence the way in which people process information and the
295|Weak Ordering -- A New Definition|A memory model for a shared memory, multiprocessor commonly and often implicitly assumed by programmers is that of sequential consistency. This model guarantees that all memory accesses will appear to execute atomically and in program order. An alternative model, weak ordering, offers greater performance potential. Weak ordering was first defined by Dubois, Scheurich and Briggs in terms of a set of rules for hardware that have to be made visible to software. The central hypothesis of this work is that programmers prefer to reason about sequentially consistent memory, rather than having to think about weaker memory, or even write buffers. Following this hypothesis, we re-define weak ordering as a contract between software and hardware. By this contract, software agrees to some formally specified constraints, and hardware agrees to appear sequentially consistent to at least the software that obeys those constraints. We illustrate the power of the new definition with a set of software constraints that forbid data races and an imple-mentation for cache-coherent systems chat is not allowed by the old definition. 
296|An Evaluation of Directory Schemes for Cache Coherence|The problem of cache coherence in shared-memory multiprocessors has been addressed using two basic approaches: directory schemes and snoopy cache schemes. Directory schemes have been given less attention in the past several years, while snoopy cache methods have become extremely popular. Directory schemes for cache coherence are potentially attractive in large multiprocessor systems that are beyond the scaling limits of the snoopy cache schemes. Slight modifications to directory schemes can make them competitive in performance with snoopy cache schemes for small multiprocessors. Trace driven simulation, using data collected from several real multiprocessor applications, is used to compare the performance of standard directory schemes, modifications to these schemes, and snoopy cache protocols. 1 Introduction In the past several years, shared-memory multiprocessors have gained wide-spread attention due to the simplicity of the shared-memory parallel programming model. However, allowing...
298|A tree-based algorithm for distributed mutual exclusion|We present an algorithm for distributed mutual exclusion in a computer network of N nodes that communicate by messages rather than shared memory. The algorithm uses a spanning tree of the computer network, and the number of messages exchanged per critical section depends on the topology of this tree. However, typically the number of messages exchanged is O(log N) under light demand, and reduces to approximately four messages under saturated demand. Each node holds information only about its immediate neighbors in the spanning tree rather than information about all nodes, and failed nodes can recover necessary information from their neighbors. The algorithm does not require sequence numbers as it operates correctly despite message overtaking.
299|The Performance Implications of Thread Management Alternatives for Shared-Memory Multiprocessors|Threads (“lightweight ” processes) have become. a common element of new languages and operating systems. This paper examines the performance implications of several data structure and algorithm alternatives for thread management in shared-memory multiprocessors. Both experimental measurements and analytical model projections are presented For applications with fine-grained parallelism, small differences in thread management are shown to have significant performance impact, often posing a tradeoff between throughput and latency. Per-processor data structures can be used to improve throughput, and in some circumstances to avoid locking, improving latency as well. The method used by processors to queue for locks is also shown to affect performance significantly. Normal methods of critical resource waiting can substantially degrade performance with moderate numbers of waiting processors. We present an Ethernet-style backoff algo rithm that largely eliminates~tiis effect. 1.
300|Efficient Synchronization on Multiprocessors with Shared Memory|A new formalism is given for read-modify-write (RMW) synchronization operations. This formalism is used to extend the memory reference combining mechanism, introduced in the NYU Ultracomputer, to arbitrary RMW operations. A formal correctness proof of this combining mechanism is given. General requirements for the practicality of combining are discussed. Combining is shown to be practical for many useful memory access operations. This includes memory updates of the form mem_val := mem_val op val, where op need not be associative, and a variety of synchronization primitives. The computation involved is shown to be closely related to parallel prefix evaluation. 1. INTRODUCTION  Shared memory provides convenient communication between processes in a tightly coupled multiprocessing system. Shared variables can be used for data sharing, information transfer between processes, and, in particular, for coordination and synchronization. Constructs such as the semaphore introduced by Dijkstra in ...
301|Synchronization in Distributed Programs|this paper, one aspect of the construction of distributed programs is ad- This research was supported in part by National Science Foundation Grant MCS 76-22360
302|Multi-model parallel programming in Psyche|Many different parallel programming models, including lightweight processes that communicate with shared memory and heavyweight processes that communicate with messages, have been used to implement parallel applications. Unfortunately, operating systems and languages designed for parallel programming typically support only one model. Multi-model parallel programming is the simultaneous use of several different models, both across programs and within a single program. This paper describes multi-model parallel programming in the Psyche multiprocessor operating system. We explain why multi-model programming is desirable and present an operating system interface designed to support it. Through a series of three examples, we illustrate how the Psyche operating system supports different models of parallelism and how the different models are able to interact. 1.
303|Alternating-time Temporal Logic|Temporal logic comes in two varieties: linear-time temporal logic assumes implicit universal quantification over all paths that are generated by system moves; branching-time temporal logic allows explicit existential and universal quantification over all paths. We introduce a third, more general variety of temporal logic: alternating-time temporal logic offers selective quantification over those paths that are possible outcomes of games, such as the game in which the system and the environment alternate moves. While linear-time and branching-time logics are natural specification languages for closed systems, alternating-time logics are natural specification languages for open systems. For example, by preceding the temporal operator &#034;eventually&#034; with a selective path quantifier, we can specify that in the game between the system and the environment, the system has a strategy to reach a certain state. Also the problems of receptiveness, realizability, and controllability can be formulated as model-checking problems for alternating-time formulas.
304|A theory of timed automata|Model checking is emerging as a practical tool for automated debugging of complex reactive systems such as embedded controllers and network protocols (see [23] for a survey). Traditional techniques for model checking do not admit an explicit modeling of time, and are thus, unsuitable for analysis of real-time systems whose correctness depends on relative magnitudes of different delays. Consequently, timed automata [7] were introduced as a formal notation to model the behavior of real-time systems. Its definition provides a simple way to annotate state-transition graphs with timing constraints using finitely many real-valued clock variables. Automated analysis of timed automata relies on the construction of a finite quotient of the infinite space of clock valuations. Over the years, the formalism has been extensively studied leading to many results establishing connections to circuits and logic, and much progress has been made in developing verification algorithms, heuristics, and tools. This paper provides a survey of the theory of timed automata, and their role in specification and verification of real-time systems.
305|Detectability of Discrete Event Systems  |In this paper, we investigate the detectability problem in discrete event systems. We assume that we do not know initially which state the system is in. The problem is to determine the current and subsequent states of the system based on a sequence of observation. The observation includes partial event observation and/or partial state observation, which leads to four possible cases. We further define four types of detectabilities: strong detectability, (weak) detectability, strong periodic detectability, and (weak) periodic detectability. We derive necessary and sufficient conditions for these detectabilities. These conditions can be checked by constructing an observer, which models the estimation of states under different observations. The theory developed in this paper can be used in feedback control and diagnosis. If the system is detectable, then the observer can be used as a diagnoser to diagnose the failure states of the system.
307|Symbolic Model Checking for Real-time Systems|We describe finite-state programs over real-numbered time in a guarded-command  language with real-valued clocks or, equivalently, as finite automata with  real-valued clocks. Model checking answers the question which states of a real-time  program satisfy a branching-time specification (given in an extension of CTL with clock  variables). We develop an algorithm that computes this set of states symbolically as a  fixpoint of a functional on state predicates, without constructing the state space.  For this purpose, we introduce a -calculus on computation trees over real-numbered  time. Unfortunately, many standard program properties, such as response for all  nonzeno execution sequences (during which time diverges), cannot be characterized  by fixpoints: we show that the expressiveness of the timed -calculus is incomparable  to the expressiveness of timed CTL. Fortunately, this result does not impair the  symbolic verification of &#034;implementable&#034; real-time programs---those whose safety...
308|An Automata-Theoretic Approach to Branching-Time Model Checking|Translating linear temporal logic formulas to automata has proven to be an effective  approach for implementing linear-time model-checking, and for obtaining many extensions  and improvements to this verification method. On the other hand, for branching temporal  logic, automata-theoretic techniques have long been thought to introduce an exponential  penalty, making them essentially useless for model-checking. Recently, Bernholtz and Grumberg  have shown that this exponential penalty can be avoided, though they did not match  the linear complexity of non-automata-theoretic algorithms. In this paper we show that  alternating tree automata are the key to a comprehensive automata-theoretic framework  for branching temporal logics. Not only, as was shown by Muller et al., can they be used  to obtain optimal decision procedures, but, as we show here, they also make it possible  to derive optimal model-checking algorithms. Moreover, the simple combinatorial structure  that emerges from the a...
309|Model-Checking in Dense Real-time|Model-checking is a method of verifying concurrent systems in which a state-transition graph model of the system behavior is compared with a temporal logic formula. This paper extends model-checking for the branching-time logic CTL to the analysis of real-time systems, whose correctness depends on the magnitudes of the timing delays. For specifications, we extend the syntax of CTL to allow quantitative temporal operators such as 93!5 , meaning &#034;possibly within 5 time units.&#034; The formulas of the resulting logic, Timed CTL  (TCTL), are interpreted over continuous computation trees, trees in which paths are maps from the set of nonnegative reals to system states. To model finitestate systems we introduce timed graphs --- state-transition graphs annotated with timing constraints. As our main result, we develop an algorithm for model-checking, for determining the truth of a TCTL-formula with respect to a timed graph. We argue that choosing a dense domain instead of a discrete domain to mo...
310|Reactive Modules|We present a formal model for concurrent systems. The model represents  synchronous and asynchronous components in a uniform framework that supports compositional  (assume-guarantee) and hierarchical (stepwise-refinement) design and verification.  While synchronous models are based on a notion of atomic computation step,  and asynchronous models remove that notion by introducing stuttering, our model is  based on a flexible notion of what constitutes a computation step: by applying an abstraction  operator to a system, arbitrarily many consecutive steps can be collapsed into  a single step. The abstraction operator, which may turn an asynchronous system into a  synchronous one, allows us to describe systems at various levels of temporal detail. For  describing systems at various levels of spatial detail, we use a hiding operator that may  turn a synchronous system into an asynchronous one. We illustrate the model with diverse  examples from synchronous circuits, asynchronous shared-m...
311|On the Synthesis of Discrete Controllers for Timed Systems|Abstract. This paper presents algorithms for the automatic synthesis of real-time controllers by nding a winning strategy for certain games de ned by the timed-automata of Alur and Dill. In such games, the outcome depends on the players &#039; actions as well as on their timing. We believe that these results will pave theway for the application of program synthesis techniques to the construction of real-time embedded systems from their speci cations. 1
312|On the Synthesis of Strategies in Infinite Games|. Infinite two-person games are a natural framework for the study of reactive nonterminating programs. The effective construction of winning strategies in such games is an approach to the synthesis of reactive programs. We describe the automata theoretic setting of infinite games (given by &#034;game graphs&#034;), outline a new construction of winning strategies in finite-state games, and formulate some questions which arise for games over effectively presented infinite graphs. 1 Introduction One of the origins of automata theory over infinite strings was the interest in verifying and synthesizing switching circuits. These circuits were considered as transforming infinite input sequences into output sequences, and systems of restricted arithmetic served as specification formalisms ([Ch63]). With Buchi&#039;s decision procedure for the monadic second-order theory S1S of one successor ([Bu62]), it turned out that the &#034;solution problem&#034; (in more recent terminology: the verification problem or model ch...
313|CONTROLLER SYNTHESIS FOR TIMED AUTOMATA  | In this work we tackle the following problem: given a timed automaton, restrict its transition relation in a systematic way so that all the remaining behaviors satisfy certain properties. This is an extension of the problem of controller synthesis for discrete event dynamical systems, where in addition to choosing among actions, the controller have the option of doing nothing and let the time pass. The problem is formulated using the notion of a real-time game, and a winning strategy is constructed as a fixed-point of an operator on the space of states and clock configurations. 
314|Module Checking|. In computer system design, we distinguish between closed and open systems. A  closed system is a system whose behavior is completely determined by the state of the system. An  open system is a system that interacts with its environment and whose behavior depends on this interaction. The ability of temporal logics to describe an ongoing interaction of a reactive program with its environment makes them particularly appropriate for the specification of open systems. Nevertheless, model-checking algorithms used for the verification of closed systems are not appropriate for the verification of open systems. Correct model checking of open systems should check the system with respect to arbitrary environments and should take into account uncertainty regarding the environment. This is not the case with current model-checking algorithms and tools. In this paper we introduce and examine the problem of model checking of open systems (mod-  ule checking, for short). We show that while module che...
315|Liveness in Timed and Untimed Systems|When proving the correctness of algorithms in distributed systems, one generally considers safety conditions and liveness conditions. The Input/Output (I/O) automaton model and its timed version have been used successfully, but have focused on safety conditions and on a restricted form of liveness called fairness. In this paper we develop a new I/O automaton model, and a new timed I/O automaton model, that permit the verification of general liveness properties on the basis of existing verification techniques. Our models include a notion of environment-freedom which generalizes the idea of receptiveness of other existing formalisms, and enables the use of compositional verification techniques.
316|Efficient on-the-fly algorithms for the analysis of timed games| In this paper, we propose a first efficient on-the-fly algorithm for solving games based on timed game automata with respect to reachability and safety properties. The algorithm we propose is a symbolic extension of the on-the-fly algorithm suggested by Liu &amp; Smolka [15] for linear-time model-checking of finite-state systems. Being on-the-fly, the symbolic algorithm may terminate long before having explored the entire state-space. Also the individual steps of the algorithm are carried out efficiently by the use of so-called zones as the underlying data structure. Various optimizations of the basic symbolic algorithm are proposed as well as methods for obtaining time-optimal winning strategies (for reachability games). Extensive evaluation of an experimental implementation of the algorithm yields very encouraging performance results. 
317|Discrete-Time Control for Rectangular Hybrid Automata  |Rectangular hybrid automata model digital control programs of analog plant environments. We study rectangular hybrid automata where the plant state evolves continuously in real-numbered time, and the controller samples the plant state and changes the control state discretely, only at the integer points in time. We prove that rectangular hybrid automata have nite bisimilarity quotients when all control transitions happen at integer times, even if the constraints on the derivatives of the variables vary between control states. This is in contrast with the conventional model where control transitions may happen at any real time, and already the reachability problem is undecidable. Based on the nite bisimilarity quotients, we give an exponential algorithm for the symbolic sampling-controller synthesis of rectangular automata. We show our algorithm to be optimal by proving the problem to be EXPTIME-hard. We also show that rectangular automata form a maximal class of systems for which the sampling-controller synthesis problem can be solved algorithmically. 
318|Optimal strategies in priced timed game automata|Abstract. Priced timed (game) automata extend timed (game) automata with costs on both locations and transitions. In this paper we focus on reachability games for priced timed game automata and prove that the optimal cost for winning such a game is computable under conditions concerning the non-zenoness of cost and we prove that it is decidable. Under stronger conditions (strictness of constraints) we prove that in case an optimal strategy exists, we can compute a state-based winning optimal strategy. 1
319|The Element of Surprise in Timed Games |We consider concurrent two-person games played in real time, in which the players decide both which action to play, and when to play it. Such timed games differ from untimed games in two essential ways. First, players can take each other by surprise, because actions are played with delays that cannot be anticipated by the opponent. Second, a player should not be able to win the game by preventing time from diverging. We present a model of timed games that preserves the element of surprise and accounts for time divergence in a way that treats both players symmetrically and applies to all !-regular winning conditions.
320|Timed Control with Partial Observability|We consider the problem of synthesizing controllers for timed systems  modeled using timed automata. The point of departure from earlier work is that  we consider controllers that have only a partial observation of the system that  it controls. In discrete event systems (where continuous time is not modeled),  it is well known how to handle partial observability, and decidability issues do  not differ from the complete information setting. We show however that timed  control under partial observability is undecidable even for internal specifications  (while the analogous problem under complete observability is decidable) and we  identify a decidable subclass.
321|Module checking revisited|Abstract. When we verify the correctness of an open system with respect to a desired requirement, we should take into consideration the different environments with which the system may interact. Each environment induces a different behavior of the system, and we want all these behaviors to satisfy the requirement. Module checking is an algorithmic method that checks, given an open system (modeled as a finite structure) and a desired requirement (specified by a temporal-logic formula), whether the open system satisfies the requirement with respect to all environments. In this paper we extend the module-checking method with respect to two orthogonal issues. Both issues concern the fact that often we are not interested in satisfaction of the requirement with respect to all environments, but only with respect to these that meet some restriction. We consider the case where the environment has incomplete information about the system; i.e., when the system has internal variables, which are not readable by its environment, and the case where some assumptions are known about environment; i.e., when the system is guaranteed to satisfy the requirement only when its environment satisfies certain assumptions. We study the complexities of the extended module-checking problems. In particular, we show that for universal temporal logics (e.g., LTL,  ¥ CTL, and ¥ CTL ¦), module checking with incomplete information coincides with module checking, which by itself coincides with model checking. On the other hand, for non-universal temporal logics (e.g., CTL and CTL ¦), module checking with incomplete information is harder than module checking, which is by itself harder than model checking. 1
322|On optimal timed strategies|Abstract. In this paper, we study timed games played on weighted timed automata. In this context, the reachability problem asks if, given a set T of locations and a cost C, Player 1 has a strategy to force the game into T with a cost less than C no matter how Player 2 behaves. Recently, this problem has been studied independently by Alur et al and by Bouyer et al. In those two works, a semi-algorithm is proposed to solve the reachability problem, which is proved to terminate under a condition imposing the non-zenoness of cost. In this paper, we show that in the general case the existence of a strategy for Player 1 to win the game with a bounded cost is undecidable. Our undecidability result holds for weighted timed game automata with five clocks. On the positive side, we show that if we restrict the number of clocks to one and we limit the form of the cost on locations, then the semi-algorithm proposed by Bouyer et al always terminates. 1
323|Dense Real-time Games|The rapid development of complex and safety-critical systems requires the use of reliable verification methods and tools for system design (synthesis). Many systems of interest are reactive, in the sense that their behavior depends on the interaction with the environment. A natural framework to model them is a two-player game: the system versus the environment. In this context, the central problem is to determine the existence of a winning strategy according to a given winning condition. We focus on real-time systems, and choose to model the related game as a nondeterministic timed automaton. We express winning conditions by formulas of the branching-time temporal logic TCTL. While timed games have been studied in the literature, timed games with dense-time winning conditions constitute a new research topic. The main result of this paper is an exponential-time algorithm to check for the existence of a winning strategy for TCTL games where equality is not allowed in the timing constraints. Our approach consists on translating to timed tree automata both the game graph and the winning condition, thus reducing the considered decision problem to the emptiness problem for this class of automata. The proposed algorithm matches the known lower bound on timed games. Moreover, if we relax the limitation we have placed on the timing constraints, the problem becomes undecidable.
324|Automata-theoretic Decision of Timed Games|The solution of games is a key decision problem in the context of verification of open systems and program synthesis. Given a game graph and a specification, we wish to determine if there exists a strategy of the protagonist that allows to select only behaviors fulfilling the specification. In this paper, we consider timed games, where the game graph is a timed automaton and the specification is given by formulas of the temporal logics Ltl and Ctl. We present an automata-theoretic approach to solve the addressed games, extending to the timed framework a successful approach to solve discrete games. The main idea of this approach is to translate the timed automaton A, modeling the game graph, into a tree automaton AT accepting all trees that correspond to a strategy of the protagonist. Then, given an automaton corresponding to the specification, we intersect it with the tree automaton AT and check for the nonemptiness of the resulting automaton. Our approach yields a decision algorithm running in exponential time for Ctl and in double exponential time for Ltl. The obtained algorithms are optimal in the sense that their computational complexity matches the known lower bounds.
325|Average reward timed games|Abstract. We consider real-time games where the goal consists, for each player, in maximizing the average amount of reward he or she receives per time unit. We consider zero-sum rewards, so that a reward of +r to one player corresponds to a reward of -r to the other player. The games are played on discrete-time game structures which can be specified using a two-player version of timed automata whose locations are labeled by rewards. Even though the rewards themselves are zero-sum, the games are not, due to the requirement that time must progress along a play of the game. Since we focus on control applications, we define the value of the game to a player to be the maximal average reward per time unit that the player can ensure. We show that in general the values to players 1 and 2 do not sum to zero. We provide algorithms for computing the value of the game for either player; the al-gorithms are based on the relationship between the original, infinite-round, game, and a derived game that is played for only finitely many rounds. As positional op-timal strategies exist for both players in both games, we show that the problem of computing the value of the game is in NPncoNP. 1
326|Model checking timed ATL for durational concurrent game structures|Abstract. We extend the framework of ATL model-checking to “simply timed ” concurrent game structures, i.e., multi-agent structures where each transition carry an integral duration (or interval thereof). While the case of single durations is easily handled from the semantics point of view, intervals of durations raise several interesting questions. Moreover subtle algorithmic problems have to be handled when dealing with model checking. We propose a semantics for which we develop efficient (PTIME) algorithms for timed ATL without equality constraints, while the general case is shown to be EXPTIME-complete.
327|Multiscalar Processors|Multiscalar processors use a new, aggressive implementation paradigm for extracting large quantities of instruction level parallelism from ordinary high level language programs. A single program is divided into a collection of tasks by a combination of software and hardware. The tasks are distributed to a number of parallel processing units which reside within a processor complex. Each of these units fetches and executes instructions belonging to its assigned task. The appearance of a single logical register file is maintained with a copy in each parallel processing unit. Register results are dynamically routed among the many parallel pro-cessing units with the help of compiler-generated masks. Memory accesses may occur speculatively without knowledge of preceding loads or stores. Addresses are disambiguated dynamically, many in parallel, and processing waits only for true data dependence. This paper presents the philosophy of the multi scalar paradigm, the structure of multiscalar programs, and the hardware architecture of a multiscalar processor. The paper also discusses performance issues in the mttltiscalar model. and compares the multiscalar paradigm with other paradigms. Experimental results evaluating the performance of a sample of multiscalar organizations are also presented. 1.
328|The Impact of Synchronization and Granularity on Parallel Systems|In this paper, we study the impact of synchronization and granularity on the performance of parallel systems using an execution-driven simulation technique. We find that even though there can be a lot of parallelism at the fine grain level, synchronization and scheduling strategies determine the ultimate performance of the system. Loop-iteration level parallelism seems to be a more appropriate level when those factors are considered. We also study barrier synchronization and data synchronization at the loopiteration level and found both schemes are needed for a better performance.
329|On the Criteria To Be Used in Decomposing Systems into Modules|This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a “modularization ” is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.
330|The 4+1 view model of architecture|The 4+1 View Model organizes a description of a software architecture using five concurrent views, each of which
addresses a specific set of concerns. Architects capture their design decisions in four views and use the fifth view to illustrate and validate them.
331|The design and implementation of hierarchical software systems with reusable components|We present a domain-independent model of hierarchical software system design and construction that is based on interchangeable software components and largescale reuse. The model unifies the conceptualizations of two independent projects, Genesis and Avoca, that are successful examples of software component/building-block technologies and domain modeling. Building-block technologies exploit large-scale reuse, rely on open architecture software, and elevate the granularity of programming to the subsystem level. Domain modeling formalizes the similarities and differences among systems of a domain. We believe our model is a blue-print for achieving software component technologies in many domains.
332|The X Window System|The X Window System, Version 11, is the standard window system on Linux and UNIX systems. X11, designed in 1987, was “state of the art ” at that time. From its inception, X has been a network transparent window system in which X client applications can run on any machine in a network using an X server running on any display. While there have been some significant extensions to X over its history (e.g. OpenGL support), X’s design lay fallow over much of the 1990’s. With the increasing interest in open source systems, it was no longer sufficient for modern applications and a significant overhaul is now well underway. This paper describes revisions to the architecture of the window system used in a growing fraction of desktops and embedded systems 1
333|A Field Guide to Boxology: Preliminary Classification of Architectural Styles for Software Systems|Software architects use a number of commonly-recognized “styles” to guide their design of system structures. Each of these is appropriate for some classes of problems, but none is suitable for all problems. How, then, does a software designer choose an architecture suitable for the problem at hand? Two kinds of information are required: (1) careful discrimination among the candidate architectures and (2) design guidance on how to make appropriate choices. Here we support careful discrimination with a preliminary classification of styles. We use a two-dimensional classification strategy with control and data issues as the dominant organizing axes. We position the major styles within this space and use finer-grained discriminations to elaborate variations on the styles. This provides a framework for organizing design guidance, which we partially flesh out with rules of thumb.
334|Paradigms for process interaction in distributed programs|Distributed computations are concurrent programs in which processes communicate by message passing. Such programs typically execute on network architectures such as networks of workstations ordistributed memory parallel machines (i. e, multicomputers such ashypercubes). Several paradigms—examples or models—for process interaction
335|Formalizing design spaces: Implicit invocation mechanisms |An important goal of software engineering is to exploit commonalities in system design in order to reduce the complexity of building new systems, support largescale reuse, and provide automated assistance for system development. A significant roadblock to accomplishing this goal is that common properties of systems are poorly understood. In this paper we argue that formal specification can help solve this problem. A formal definition of a design framework can identify the common properties of a family of systems and make clear the dimensions of specialization. New designs can then be built out of old ones in a principled way, at reduced cost to designers and implementors. To illustrate these points, we present a formalization of a system integration technique called implicit invocation. We show how many previously unrelated systems can be viewed as instances of the same underlying framework. Then we briefly indicate how the formalization allows us to reason about certain properties of those systems as well as the relationships between different systems. 1
336|Software Requirements Negotiation and Renegotiation Aids: A Theory-W Based Spiral Approach|A major problem in requirements engineering is obtaining requirements that address the concerns of multiple stakeholders. An approach to such a problem is the Theory-W based Spiral Model. One key element of this model is stakeholder collaboration and negotiation to obtain win-win requirements. This paper focuses on the problem of developing a support system for such a model. In particular it identifies needs and capabilities required to address the problem of negotiation and renegotiation that arises when the model is applied to incremental requirements engineering. The paper formulates elements of the support system, called WinWin, for providing such capabilities. These elements were determined by experimenting with versions of WinWin and understanding their merits and deficiencies. The key elements of WinWin are described and their use in incremental requirements engineering are demonstrated, using an example renegotiation scenario from the domain of software engineering environments...
337|Reconciling the Needs of Architectural Description with Object-Modelling Notations|Abstract. Complex software systems require expressive notations for representing their software architectures. Two competing paths have emerged. One is to use a specialized notation for architecture – or architecture description language (ADL). The other is to adapt a general-purpose modeling notation, such as UML. The latter has a number of benefits, including familiarity to developers, close mapping to implementations, and commercial tool support. However, it remains an open question as to how best to use object-oriented notations for architectural description, and, indeed, whether they are sufficiently expressive, as currently defined. In this paper we take a systematic look at these questions, examining the space of possible mappings from ADLs into object notations. Specifically, we describe (a) the principle strategies for representing architectural structure in UML; (b) the benefits and limitations of each strategy; and (c) aspects of architectural description that are intrinsically difficult to model in UML using the strategies. 1
338|Describing Software Architecture with UML|: This paper describes our experience using UML, the Unified Modeling Language, to describe the software architecture of a system. We found that it works well for communicating the static structure of the architecture: the elements of the architecture, their relations, and the variability of a structure. These static properties are much more readily described with it than the dynamic properties. We could easily describe a particular sequence of activities, but not a general sequence. In addition, the ability to show peer-to-peer communication is missing from UML.  Keywords: software architecture, UML, architecture descriptions, multiple views  1. INTRODUCTION  UML, the Unified Modeling Language, is a standard that has wide acceptance and will likely become even more widely used. Although its original purpose was for detailed design, its ability to describe elements and the relations between them makes it potentially applicable much more broadly. This paper describes our experience usin...
339|Using tool abstraction to compose systems|paradigms support the evolution of large-scale software systems. Data abstraction eases design changes in the representation of data structures, while tool abstraction does the same with system functions. M anaging complexity and supporting evolution are two fundamental “i, problems with large-scale software systems. ’ Although modularization,. has long been accepted as the basic approach to managing complexity, as David Parnas observed nearly 20 years ago, not all modularizations are equally good at handling evolution.’ Data abstraction is a popular, important style of modularization. In this style, an abstract data type is defined by an explicit interface that specifies operations on
340|Experience with a course on architectures for software systems|Abstract. As software systems grow in size and complexity their design problem extends beyond algorithms and data structures to issues of system design. This area receives little or no treatment in existing computer science curricula. Although courses about speci c systems are usually available, there is no systematic treatment of the organizations used to assemble components into systems. These issues { the software architecture level of software design { are the subject of a new course that we taught for the rst time in Spring 1992. This paper describes the motivation for the course, the content and structure of the current version, and our plans for improving the next version. 1
341|Aladdin: A Tool for Architecture-Level Dependence Analysis of Software Systems|The emergence of formal architecture description languages provides an opportunity to perform analyses at high levels of abstraction, as well as early in the development process. Previous research has primarily focused on developing techniques such as algebraic and transition-system analysis to detect component mismatches or global behavioral incorrectness. In this paper, we present Aladdin, a tool that implements chaining, a static dependence analysis technique for use with architectural descriptions. Dependence analysis has been used widely at the implementation level to aid program optimization, anomaly checking, program understanding, testing, and debugging. We investigate the definition and application of dependence analysis at the architectural level. We illustrate the utility of chaining, through the use of Aladdin, by showing how the technique can be used to answer various questions one might pose of a Rapide architecture specification. 
342|The impact of Mesa on system design|The Mesa programming language supports program modularity in ways that permit subsystems to be developed separately but to be bound together with complete type safety. Separate and explicit interface definitions provide an effective means of communication, both between programs and between programmers. A configuration language describes the organization of a system and controls the scopes of interfaces. These facilities have had a profound impact on the way we design systems and Organize development projects. This paper reports our recent experience with Mesa, particularly its use in the development of an operating system. It illustrates techniques for designing interfaces, for using the interface language as a specification language, and for organizing a ~ystem to achieve the practical benefits of program modularity without sacrificing strict type-checking.
343|U-Net: A User-Level Network Interface for Parallel and Distributed Computing|The U-Net communication architecture provides processes with a virtual view of a network interface to enable userlevel access to high-speed communication devices. The architecture, implemented on standard workstations using offthe-shelf ATM communication hardware, removes the kernel from the communication path, while still providing full protection. The model presented by U-Net allows for the construction of protocols at user level whose performance is only limited by the capabilities of network. The architecture is extremely flexible in the sense that traditional protocols like TCP and UDP, as well as novel abstractions like Active Messages can be implemented efficiently. A U-Net prototype on an 8-node ATM cluster of standard workstations offers 65 microseconds round-trip latency and 15 Mbytes/sec bandwidth. It achieves TCP performance at maximum network bandwidth and demonstrates performance equivalent to Meiko CS-2 and TMC CM-5 supercomputers on a set of Split-C benchmarks. 1
344|A Reliable Multicast Framework for Light-weight Sessions and Application Level Framing|This paper... reliable multicast framework for application level framing and light-weight sessions. The algorithms of this framework are efficient, robust, and scale well to both very large networks and very large sessions. The framework has been prototype in wb, a distributed whiteboard application, and has been extensively tested on a global scale with sessions ranging from a few to more than 1000 participants. The paper describes the principles that have guided our design, including the 1P multicast group delivery model, an end-to-end, receiver-based model of reliability, and the application level framing protocol model. As with unicast communications, the performance of a reliable multicast delivery algorithm depends on the underlying topology and operational environment. We investigate that dependence via analysis and simulation, and demonstrate an adaptive algorithm that uses the results of previous loss recovery events to adapt the control parameters used for future loss recovery. Whh the adaptive algorithm, our reliable multicast delivery algorithm provides good performance over a wide range of underlying topologies. 
345|Active Messages: a Mechanism for Integrated Communication and Computation|The design challenge for large-scale multiprocessors is (1) to minimize communication overhead, (2) allow communication to overlap computation, and (3) coordinate the two without sacrificing processor cost/performance. We show that existing message passing multiprocessors have unnecessarily high communication costs. Research prototypes of message driven machines demonstrate low communication overhead, but poor processor cost/performance. We introduce a simple communication mechanism, Active Messages, show that it is intrinsic to both architectures, allows cost effective use of the hardware, and offers tremendous flexibility. Implementations on nCUBE/2 and CM-5 are described and evaluated using a split-phase shared-memory extension to C, Split-C. We further show that active messages are sufficient to implement the dynamically scheduled languages for which message driven machines were designed. With this mechanism, latency tolerance becomes a programming/compiling concern. Hardware support for active messages is desirable and we outline a range of enhancements to mainstream processors.  
346|An Analysis of TCP Processing Overhead|networks, have been getting faster, perceived throughput at the application has not always increased accordingly. Various performance bottlenecks have been encountered, each of which has to be analyzed and corrected. One aspect of networking often suspected of contributing to low throughput is the transport layer of the protocol suite. This layer, especially in connectionless protocols, has considerable functionality, and is typically executed in software by the host processor at the end points of the network. It is thus a likely source of processing overhead. While this theory is appealing, a preliminary examination suggested to us that other aspects of networking may be a more serious source of overhead. To test this proposition, a detailed study was made of a popular transport protocol, Transmission Control Protocol (TCP) [I]. This paper provides results of that
347|Fbufs: A High-Bandwidth Cross-Domain Transfer Facility|We have designed and implemented a new operating system facility for I/O buffer management and data transfer across protection domain boundaries on shared memory machines. This facility, called fast buffers (fbufs), combines virtual page remapping with shared virtual memory, and exploits locality in I/O traffic to achieve high throughput withoutcompromising protection, security, or modularity. Its goal is to help deliver the high bandwidth afforded by emerging high-speed networks to user-level processes, both in monolithic and microkernel-based operating systems.  This paper outlines the requirements for a cross-domain transfer facility, describes the design of the fbuf mechanism that meets these requirements, and experimentally quantifies the impact of fbufs on network performance. 1 Introduction  Optimizing operations that cross protection domain boundaries has received a great deal of attention recently [2, 3]. This is because an efficient cross-domain invocation facility enables a ...
348|Dynamics of TCP Traffic over ATM Networks|We investigate the performance of TCP connections over ATM networks without ATM-level congestion control, and compare it to the performance of TCP over packet-based networks. For simulations of congested networks, the effective throughput of TCP over ATM can be quite low when cells are dropped at the congested ATM switch. The low throughput is due to wasted bandwidth as the congested link transmits cells from `corrupted&#039; packets, i.e., packets in which at least one cell is dropped by the switch. We investigate two packet discard strategies which alleviate the effects of fragmentation. Partial Packet Discard, in which remaining cells are discarded after one cell has been dropped from a packet, somewhat improves throughput. We introduce Early Packet Discard, a strategy in which the switch drops whole packets prior to buffer overflow. This mechanism prevents fragmentation and restores throughput to maximal levels.  
349|Experiences with a High-Speed Network Adaptor: A Software Perspective|This paper describes our experiences, from a software perspective, with the OSIRIS network adaptor. It first identifies the problems we encountered while programming OSIRIS and optimizing network performance, and outlines how we either addressed them in the software, or had to modify the hardware. It then describes the opportunities provided by OSIRIS that we were able to exploit in the host operating system (OS); opportunities that suggested techniques for making the OS more effective in delivering network data to application programs. The most novel of these techniques, called application device channels, gives application programs running in user space direct access to the adaptor. The paper concludes with the lessons drawn from this work, which we believe will benefit the designers of future network adaptors. 1 Introduction  With the emergence of high-speed network facilities, several research efforts are focusing on the design and implementation of network adaptors [5, 2, 3, 16, 2...
350|Protocol Service Decomposition for High-Performance Networking|In this paper we describe a new approach to implementing network protocols that enables them to have high performance and high flexibility, while retaining complete conformity to existing application programming interfaces. The key insight behind our work is that an application&#039;s interface to the network is distinct and separable from its interface to the operating system. We have separated these interfaces for two protocol implementations, TCP/IP and UDP/IP, running on the Mach 3.0 operating system and UNIX server. Specifically, library code in the application&#039;s address space implements the network protocols and transfers data to and from the network, while an operating system server manages the heavyweight abstractions that applications use when manipulating the network through operations other than send and receive. On DECstation 5000/200 This research was sponsored in part by the Advanced Research Projects Agency, Information Science and Technology Office, under the title &#034;Research...
351|The importance of Non-Data Touching Processing Overheads|We present detailed measurements of various processing overheads of the TCP/IP and UDP/IP protocol stacks on a DECstation 5000/200 running the Ultrix 4.2a operating system. These overheads include data-touching operations, such as the checksum computation and data movemen ~ which are well known to be major time consumers. In this stud y, we also considered overheads due to non-data touching operations, such as network buffer manipulation, protocol-specific processing, operating system functions, data structure manipulations (other than network buffers), and error checking. We show that when one considers realistic message size dktributions, where the majority of messages are small, the cumulative time consumed by the nondata touching overheads represents the majority of processing time. We assert that it will be difficult to significantly reduce the cumulative processing time due to non-data touching overheads. The goal of this study is to determine the relative importance of various processing overheads in network software, in particular, the TCP/IP and UDPAP protocol stacks. In the prrsg significant focus has been placed on maximizing throughput noting that “data
352|Distributed Network Computing over Local ATM Networks|Communication between processors has long been the bottleneck of distributed network computing. However, recent progress in switch-based high-speed Local Area Networks (LANs) may be changing this situation. Asynchronous Transfer Mode (ATM) is one of the most widely-accepted and emerging high-speed network standards which can potentially satisfy the communication needs of distributed network computing. In this paper, we investigate distributed network computing over local ATM networks. We first study the performance characteristics involving end-to-end communication in an environment that includes several types of workstations interconnected via a Fore Systems&#039; ASX-100 ATM Switch. We then compare the communication performance of four different Application Programming Interfaces (APIs). The four APIs were Fore Systems ATM API, BSD socket programming interface, Sun&#039;s Remote Procedure Call (RPC), and the Parallel Virtual Machine (PVM) message passing library. Each API represents distribute...
353|Separating Data and Control Transfer in Distributed Operating Systems|Advances in processor architecture and technology have resulted in workstations in the 100+ MIPS range. As well, newer local-area networks such as ATM promise a ten- to hundred-fold increase in throughput, much reduced latency, greater scalability, and greatly increased reliability, when compared to current LANs such as Ethernet. We believe that these new network and processor technologies will permit tighter coupling of distributed systems at the hardware level, and that distributed systems software should be designed to benefit from that tighter coupling. In this paper, we propose an alternative way of structuring distributed systems that takes advantage of a communication model based on remote network access (reads and writes) to protected memory segments. A key feature of the new structure, directly supported by the communication model, is the separation of data transfer and control transfer. This is in contrast to the structure of traditional distributed systems, which are typical...
354|Protocol Implementation Using Integrated Layer Processing|Integrated Layer Processing (ILP) is an implementation concept which &amp;quot;permit[s] the implementor the option of performing all the [data] manipulation steps in one or two integrated processing loops &amp;quot; [1]. To estimate the achievable benefits of ILP, a file transfer application with an encryption function on top of a user-level TCP has been implemented and the performance of the application in terms of throughput and packet processing times has been measured. The results show that it is possible to obtain performance benefits by integrating marshalling, encryption and TCP checksum calculation. They also show that the benefits are smaller than in simple experiments, where ILP effects have not been evaluated in a complete protocol environment. Simulations of memory access and cache hit rate show that the main benefit of ILP is reduced memory accesses rather than an improved cache hit rate. The results further show that data manipulation characteristics may significantly influence the cache behavior and the achievable performance gain of ILP. 1
355|User-space protocols deliver high performance to applications on a low-cost gb/s lan|Two important questions in high-speed networking are firstly, how to provide GbitJs networking at low cost and secondly, how to provide a flexible low-level network inter-face so that applications can control their data from the instant it arrives. We describe some work that addresses both of these ques-tions. The Jetstream Gbit/s LAN is an experimental, low-cost network interface that provides the services required by delay-sensitive traffic as well as meeting the performance needs of current applications. Jetstream is a combination of traditional shared-medium LAN technology and more recent ATM cell- and switch-based technology. Jetstream frames contain a channel identifier so that the net-work driver can immediately associate an incoming frame with its application. We have developed such a driver that enables applications to control how their data should be managed without the need to first move the data into the application’s address space. Consequently, applications can elect to read just a part of a frame and then instruct the driver to move the remainder directly to its destination. Individual channels can elect to receive frames that have failed their CRC, while applications can specify frame-drop policies on a per-channel basis. Measured results show that both kernel- and user-space pro-tocols can achieve very good throughput: applications using both TCP and our own reliable byte-stream protocol have demonstrated throughputs in excess of 200 Mbit/s. The ben-efits of running protocols in user-space are well known- the drawback has often been a severe penalty in the perform-ance achieved. In this paper we show that it is possible to have the best of both worlds. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given
356|Experience with Active Messages on the Meiko CS-2|Active messages provide a low latency communication architecture which on modern parallel machines achieves more than an order of magnitude performance improvement over more traditional communication libraries. This paper discusses the experience we gained while implementing active messages on the Meiko CS-2, and discusses implementations for similar architectures. During our work we have identified that architectures which only support efficient remote write operations (or DMA transfers as in the case of the CS-2) make it difficult to transfer both data and control as required by active messages. Traditional network interfaces avoid this problem because they have a single point of entry which essentially acts as a queue. To efficiently support active messages on modern network communication co-processors, hardware primitives are required which support this queue behavior. We overcame this problem by producing specialized code which runs on the communications co-processor and supports ...
357|Virtual-memorymapped network interfaces|In today’s multicomputers, software overhead dominates the message-passing latency cost. We designed two multicomputer network interfaces that signif~cantiy reduce this overhead. Both support vMual-memory-mapped communication, allowing user processes to communicate without expensive buffer management and without making system calls across the protection boundary separating user processes from the operating system kerneL Here we compare the two interfaces and discuss the performance trade-offs between them.
359|Software processes are software too |The major theme of this meeting is the exploration of the importance of.ul process as a vehicle for improving both the quality of software products and the the way in which we develop and evolve them. In beginning this exploration it seems important to spend at least a short time examining the nature of process and convincing ourselves that this is indeed a promising vehicle. We shall take as our elementary notion of a process that it is a systematic approach to the creation of a product or the accomplishment of some task. We observe that this characterization describes the notion of process commonly used in operating systems-- namely that a process is a computational task executing on a single computing device. Our characterization is much broader, however, describing any mechanism used to carry out work or achieve a goal in an orderly way.
360|DistEdit: A Distributed Toolkit for Supporting Multiple Group Editors|The purpose of our project is to provide toolkits for building applications that support collaboration between people in distributed environments. In this paper, we describe one such toolkit, called DistEdit, that can be used to build interactive group editors for distributed environments. This toolkit has the ability to support different editors simultaneously and provides a high degree of fault-tolerance against machine crashes. To evaluate the toolkit, we modified two editors to make use of the toolkit. The resulting editors allow users to take turns at making changes while other users observe the changes as they occur. We give an evaluation of the toolkit based on the development and use of these editors.
361|Computer Support for COOPERATIVE DESIGN|Computer support for design as cooperative work is the subject of our discussion in the context of our research program on Computer Support in Cooperative Design and Communication. We outline our theoretical perspective on design as cooperative work, and we exemplify our approach with reflections from a project on computer support for envisionment in design -- the APLEX and its use. We see envisionment facilities as support for both experiments with and communication about the future use situation. As a background we sketch the historical roots of our program -- the Scandinavian collective resource approach to design and use of computer artifacts, and make some critical reflections on the rationality of computer support for cooperative work.
362|The process group approach to reliable distributed computing|The difficulty of developing reliable distributed softwme is an impediment to applying distributed computing technology in many settings. Expeti _ with the Isis system suggests that a structured approach based on virtually synchronous _ groups yields systems that are substantially easier to develop, exploit sophisticated forms of cooperative computation, and achieve high reliability. This paper reviews six years of resemr,.hon Isis, describing the model, its impl_nentation challenges, and the types of applicatiom to which Isis has been appfied. 1 In oducfion One might expect the reliability of a distributed system to follow directly from the reliability of its con-stituents, but this is not always the case. The mechanisms used to structure a distributed system and to implement cooperation between components play a vital role in determining how reliable the system will be. Many contemporary distributed operating systems have placed emphasis on communication performance, overlooking the need for tools to integrate components into a reliable whole. The communication primitives supported give generally reliable behavior, but exhibit problematic semantics when transient failures or system configuration changes occur. The resulting building blocks are, therefore, unsuitable for facilitating the construction of systems where reliability is impo/tant. This paper reviews six years of research on Isis, a syg_,,m that provides tools _ support the construction of reliable distributed software. The thesis underlying l._lS is that development of reliable distributed software can be simplified using process groups and group programming too/_. This paper motivates the approach taken, surveys the system, and discusses our experience with real applications.
364|Transis: A Communication Sub-System for High Availability|This paper describes Transis, a communication sub-system for high availability. Transis is a transport layer package that supports a variety of reliable multicast message passing services between processors. It provides highly tuned multicast and control services for scalable systems with arbitrary topology. The communication domain comprises of a set of processors that can initiate multicast messages to a chosen subset. Transis delivers them reliably and maintains the  membership of connected processors automatically, in the presence of arbitrary communication delays, of message losses and of processor failures and joins. The contribution of this paper is in providing an aggregate definition of communication and control services over broadcast domains. The main benefit is the efficient implementation of these services using the broadcast capability. In addition, the membership algorithm has a novel approach in handling partitions and remerging; in allowing the regular flow of messages...
365|Using Process Groups to Implement Failure Detection in Asynchronous Environments|Agreement on the membership of a group of processes in a distributed system is a basic problem that arises in a wide range of applications. Such groups occur when a set of processes co-operate to perform some task, share memory, monitor one another, subdivide a computation, and so forth. In this paper we discuss the Group Membership Problem as it relates to failure detection in asynchronous, distributed systems. We present a rigorous, formal specification for group membership under this interpretation. We then present a solution for this problem that improves upon previous work.
366|An Efficient Reliable Broadcast Protocol|Many distributed and parallel applications can make good use of  broadcast communication. In this paper we present a (software) protocol  that simulates reliable broadcast, even on an unreliable network. Using  this protocol, application programs need not worry about lost messages.  Recovery of communication failures is handled automatically and transparently  by the protocol. In normal operation, our protocol is more  efficient than previously published reliable broadcast protocols. An initial  implementation of the protocol on 10 MC68020 CPUs connected by a 10  Mbit/sec Ethernet performs a reliable broadcast in 1.5 msec.   
367|The Distributed V Kernel and its Performance for Diskless Workstations|The distributed V kernel is a message-oriented kernel that provides uniform local and network interprocess communication. It is primarily being used in an environment of diskless workstations connected by a high-speed local network to a set of file servers. We describe a performance evaluation of the kernel, with particular emphasis on the cost of network file access. Our results show that over a local network: 1. Diskless workstations can access remote files with minimal performance penalty. 2. The V message facility can be used to access remote files at comparable cost to any well-tuned specialized file access protocol. We conclude that it is feasible to build a distributed system with all network communication using the V message facility even when most of the network nodes have no secondary storage. 1.
368|Highly-available distributed services and fault-tolerant distributed garbage collection|This paper describes two techniques that are only loosely related. The first is a method for constructing a highly available service for use in a distributed system. The service presents its clients with a consistent view of its state, but the view may contain old information. Clients can indicate how recent the information must be. The method can be used in any application in which the property of interest is stable: once the property becomes true, it remains true forever. The paper also describes a fault-tolerant garbage collection method for a distributed heap. The method is practical and efficient. Each computer that contains part of the heap does local garbage collection independently, using whatever algorithm it chooses, and without needing to communicate with the other computers that contain parts of the heap. The highly available central service is used to store information about inter.computer references. 1.
369|Designing Application Software in Wide Area Network Settings|T(&#039;VED FOR PUBLIC RELEASE
371|Prospect theory: An analysis of decisions under risk|Your use of the JSTOR archive indicates your acceptance of JSTOR&#039;s Terms and Conditions of Use, available at
373|Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment|Perhaps the simplest and the most basic qualitative law of probability is the conjunction rule: The probability of a conjunction, P(A&amp;B), cannot exceed the probabilities of its constituents, P(A) and.P(B), because the extension (or the possibility set) of the conjunction is included in the extension of its constituents. Judgments under uncertainty, however, are often mediated by intuitive heuristics that are not bound by the conjunction rule. A conjunction can be more representative than one of its constituents, and instances of a specific category can be easier to imagine or to retrieve than instances of a more inclusive category. The representativeness and availability heuristics therefore can make a conjunction appear more probable than one of its constituents. This phenomenon is demonstrated in a variety of contexts including estimation of word frequency, personality judgment, medical prognosis, decision under risk, suspicion of criminal acts, and political forecasting. Systematic violations of the conjunction rule are observed in judgments of lay people and of experts in both between-subjects and within-subjects comparisons. Alternative interpretations of the conjunction fallacy are discussed and attempts to combat it are explored. Uncertainty is an unavoidable aspect of the the last decade (see, e.g., Einhorn &amp; Hogarth, human condition. Many significant choices must be based on beliefs about the likelihood
374|Confidence in judgment: Persistence of the illusion of validity|An accumulating body of research on clinical judgment, decision making, and probability estimation has documented a substantial lack of ability of both ex-perts and nonexperts. However, evidence shows that people have great confi-dence in their fallible judgment. This article examines how this contradiction can be resolved and, in so doing, discusses the relationship between learning and experience. The basic tasks that are considered involve judgments made for the purpose of choosing between actions. At some later time, outcome feedback is used for evaluating the accuracy of judgment. The manner in which judgments of the contingency between predictions and outcomes are made is discussed and is related to the difficulty people have in searching for discommoning informa-tion to test hypotheses. A model for learning and maintaining confidence in one&#039;s own judgment is developed that includes the effects of experience and both the frequency and importance of positive and negative feedback. Everyone complains of his memory and no one com-plains of his judgment. (La Rochefoucauld, 1959, p. 49) Although the study and cataloguing of judgmental fallability has had a long history in psychology (see, e.g., Guilford, 1954, chap. 12; Johnson, 1972), an accumulating body of recent research on clinical judgment, decision making, and probability estimation has docu-mented a substantial lack of ability across both individuals and situations (Slovic, Fisch-hoff,  &amp; Lichtenstein, 1977; Slovic &amp; Lichten-stein, 1971). For example, predictive ability has been shown to have low (and even zero) validity in clinical settings (see, e.g., Einhorn, 1972; Goldberg, 1968, and his references).
375|Bayes Rule as a Descriptive Model: The Representative Heuristic,Quarterly|Results of experiments designed to test the claim of psychologists that expected utility theory does not provide a good descriptive model are reported. The deviation from tested theory is that, in revising beliefs, individuals ignore prior or base-rate in-formation contrary to Bayes rule. Flaws in the evidence in the psychological literature are noted, an experiment avoiding these difficulties is designed and carried out, and the psychologists &#039; predictions are stated in terms of a more general model. The psy-chologists &#039; predictions are confirmed for inexperienced or financially unmotivated subjects, but for others the evidence is less clear. There are a number of areas of economic research for which the nature of individual decision processes is important. In addition, there are some substantive areas to which economic theory has been applied in which market forces cannot be relied upon to &#034;discipline &#034; the be-havior of economic agents. Recently economists have begun to study the sensitivity of market equilibria to imperfect information. These studies have considered labor markets [Mortensen, 1976, and Wilde, 1977] and consumer products market [Salop and Stiglitz, 1977, and Wilde and Schwartz, 1979). One of the conclusions which has emerged from this literature is that the properties of market equilibria are sensitive to the search strategies used by individual consumers or workers. For example, consider Salop and Stiglitz [1977] versus Wilde and Schwartz [1979]. Knowledge about individual behavior under uncertainty can also be of importance in several policy areas. Exam-ples are interventions in markets where information is incomplete [Schwartz and Wilde, 1979], the provision of insurance against natural disasters [Slovic et al., 1977b], a variety of &#034;truth in lending &#034; or la-beling-type issues, etc. Also, note the use of economic theory in non-market settings, e.g., the analysis of voting [Riker and Ordeshook, 1973], and legislative behavior [Fiorina, 1974]. In addition, there are situations to which economic theory of individual behavior has been applied in which no trades or arbitrage is possible, e.g., such areas as
376|Predicting frames|way in which a decision problem is formulated, or &#034;framed, &#034; can have strong and predictable effects on the perceived attractiveness of the options it offers. At times, the relative attractiveness of two options may be reversed as the result of a reframing that should make no difference at all, according to traditional economic theories of choice. To predict people&#039;s behavior with the theory, one must be able to predict what frame they will impose on a particular problem. The seven studies reported here explored different ways of predicting frames, with results that were generally discouraging for the prediction of individuals &#039; choices, generally encouraging for the prediction of group choices. With their &#034;prospect theory, &#034; Kahneman and Tversky (1979; Tversky &amp; Kahneman, 1981) have provided a significant new tool for the analysis of choices made under conditions of uncertainty. The primitives of this descriptive theory of decision making are a value function, v(x), which attaches a subjective worth to each possible outcome of a gamble or prospect, and a weighting function, ir(p), which expresses the subjective importance attached to the probability of obtaining a particular outcome. The attractiveness, V, of a gamble that offers a chance of p to gain (or to lose) x and a chance of q to gain (or to lose) y would equal-ir(p)v(x) + ir(q)v(y). One important feature of the value function is that it assesses outcomes in terms of the change they represent from some reference point, which could represent one&#039;s current status, one&#039;s anticipated status, or some other psychologically significant point. A second feature is that the function is steeper for losses than for gains, meaning that a given change in one&#039;s status hurts more as a loss
378|The Existence of Refinement Mappings|Refinement mappings are used to prove that a lower-level specification correctly implements a higher-level one. We consider specifications consisting of a state machine (which may be infinite-state) that specifies safety requirements, and an arbitrary supplementary property that specifies liveness requirements. A refinement mapping from a lower-level specification S 1 to a higher-level one S 2 is a mapping from S 1 &#039;s state space to S 2 &#039;s state space. It maps steps of S 1 &#039;s state machine to steps of S 2 &#039;s state machine and maps behaviors allowed by S 1 to behaviors allowed by S 2 . We show that, under reasonable assumptions about the specifications, if S 1 implements S 2 , then by adding auxiliary variables to S 1 we can guarantee the existence of a refinement mapping. This provides a completeness result for a practical, hierarchical specification method. Capsule Review  This report deals with the problem of proving that implementations satisfy their specifications. Suppose, for exa...
379|Semantical considerations on Floyd-Hoare Logic|This paper deals with logics of programs. The objective is to formalize a notion of program description, and to give both plausible (semantic) and effective (syntactic) criteria for the notion of truth of a description. A novel feature of this treatment is the development of the mathematics underlying Floyd-Hoare axiom systems independently of such systems. Other directions that such research might take are considered. 
380|An Old-Fashioned Recipe for Real Time|this paper appeared in ACM Transactions on Programming Languages and Systems 16, 5 (September 1994) 1543-- 1571. The appendix was published electronically by the ACM.  Contents
381|Autonet: A high-speed, self-configuring local area network using point-to-point links|Read it as an adjunct to the lectures on distributed systems, links, and switching. It gives a fairly complete description of a working highly-available switched network providing daily service to about 100 hosts. The techniques used to obtain high reliability and fault-tolerance are characteristic of many distributed systems, not just of networks. The paper also makes clear the essential role of software in modern networks.
382|A Guide to LP, The Larch Prover|This guide provides an introduction to LP (the Larch Prover), Release 2.2. It describes how LP can be used to axiomatize theories in a subset of multisorted first-order logic and to provide assistance in proving theorems. It also contains a tutorial overview of the equational term-rewriting technology that provides, along with induction rules and other user-supplied nonequational rules of inference, part of LP&#039;s inference engine.  
383|A Simple Approach to Specifying Concurrent Systems|In the transition axiom method, safety properties of a concurrent system can be specified by programs; liveness properties are specified by assertions in a simple temporal logic. The method is described with some simple examples, and its logical foundation is informally explored through a careful examination of what it means to implement a specification. Language issues and other practical details are largely ignored. 
384|Impossibility of Distributed Consensus with One Faulty Process|The consensus problem involves an asynchronous system of processes, some of which may be unreliable. The problem is for the reliable processes to agree on a binary value. We show &#039;that every protocol for this problem has the possibility of nontermination, even with only one faulty process. By way of contrast, solutions are known for the synchronous case, the &#034;Byzantine Generals&#034; problem.
385|A logic of implicit and explicit belief|As part of an on-going project to understand the found* tions of Knowledge Representation, we are attempting to characterize a kind of belief that forms a more appropriate basis for Knowledge Representation systems than that cap tured by the usual possible-world formalizations begun by Hintikka. In this paper, we point out deficiencies in current semantic treatments of knowledge and belief (including re-cent syntactic approaches) and suggest a new analysis in the form of a logic that avoids these shortcomings and is also more viable computationally. The kind of belief that underlies terms in AI such as ‘Know!-edge Representation ” or “knowledge base ” has never been ade-quately characterized. r As we discuss below, the major existing formal model of belief (originated by Hintikka in [l]) requires the
386|Reasoning about Knowledge and Probability|: We provide a model for reasoning about knowledge and probability together. We allow explicit mention of probabilities in formulas, so that our language has formulas that essentially say &#034;according to agent i, formula  &#039; holds with probability at least b.&#034; The language is powerful enough to allow reasoning about higher-order probabilities, as well as allowing explicit comparisons of the probabilities an agent places on distinct events. We present a general framework for interpreting such formulas, and consider various properties that might hold of the interrelationship between agents&#039; probability assignments at different states. We provide a complete axiomatization for reasoning about knowledge and probability, prove a small model property, and obtain decision procedures. We then consider the effects of adding common knowledge and a probabilistic variant of common knowledge to the language.   A preliminary version of this paper appeared in the Proceedings of the Second Conference on T...
387|Programming Simultaneous Actions Using Common Knowledge|This work applies the theory of knowledge in distributed systems to the design of efficient fault-tolerant protocols. We define a large class of problems requiring coordinated, simultaneous action in synchronous systems, and give a method of transforming specifications of such problems into protocols that are optimal in all runs: for every possible input to the system and faulty processor behavior, these protocols are guaranteed to perform the simultaneous actions as soon as any other protocol could possibly perform them. This transformation is performed in two steps. In the first step, we extract directly from the problem specification a high-level protocol programmed using explicit tests for common knowledge. In the second step, we carefully analyze when facts become common knowledge, thereby providing a method of efficiently implementing these protocols in many variants of the omissions failure model. In the generalized omissions model, however, our analysis shows that testing for common knowledge is NP-hard. Given the close correspondence between common knowledge and simultaneous actions, we are able to show that no optimal protocol for any such problem can be computationally efficient in this model. The analysis in this paper exposes many subtle differences between the failure models, including the precise point at which this gap in complexity occurs.
388|Algorithmic Knowledge|: The standard model of knowledge in multi-agent systems suffers from what has been called the logical omniscience problem: agents know all tautologies, and know all the logical consequences of their knowledge. For many types of analysis, this turns out not to be a problem. Knowledge is viewed as being ascribed by the system designer to the agents; agents are not assumed to compute their knowledge in any way, nor is it assumed that they can necessarily answer questions based on their knowledge. Nevertheless, in many applications that we are interested in, agents need to act on their knowledge. In such applications, an externally ascribed notion of knowledge is insufficient: clearly an agent can base his actions only on what he explicitly knows. Furthermore, an agent that has to act on his knowledge has to be able to compute this knowledge; we do need to take into account the algorithms available to the agent, as well as the &#034;effort&#034; required to compute knowledge. In this paper, we show...
389|What Can Machines Know? On the Properties of Knowledge in Distributed Systems|It has been argued that knowledge is a useful tool for designing and analyzing  complex systems. The notion of knowledge that seems most relevant in this context  is an external, information-based notion that can be shown to satisfy all the axioms  of the modal logic S5. We carefully examine the properties of this notion of knowledge  and show that they depend crucially, and in subtle ways, on assumptions we  make about the system and about the language used for describing knowledge. We  present a formal model in which we can capture various assumptions frequently  made about systems, such as whether they are deterministic or nondeterministic,  whether knowledge is cumulative (which means that processes never &#034;forget&#034;), and  whether or not the &#034;environment&#034; affects the state transitions of the processes. We  then show that under some assumptions about the system and the language, certain  states of knowledge are not attainable and the axioms of S5 do not completely  characterize the pr...
390|Reasoning about knowledge and probability: preliminary report|Abstract: We provide a model for reasoning about knowledge anti probabil-ity together. We a.llow explicit mention of probabilities in formulas, so that our language has formulas tha.t essentia.lly say &amp;quot;a.ccording to agent i, formula. (p holds with probability a.t least o~. &amp;quot; The language is powerfid enough to allow reason-ing a~bout higher-order probabilities, as well as allowing explicit comparisons of the probabilities an agent places on distinct events. We present a general framework for interpreting such formulas, a.nd consider various properties that might hold of the in-terrelationship between agents &#039; subjective probability spaces at different states. We provide a. complete a.xiomatiza.tion for rea.soning about knowledge a.nd probability, prove a. small model property, and obtain decision procedures. We then consider the effects of adding common knowledge and a. probabilistic va.ria.nt of common knowledge to the language.
391|What processes know: definitions and proof methods|The importance of the notion of knowledge in reasoning about distributed systems has been recently pointed out by several works. It has been argued that a distributed computation can be understood and analyzed by considering how it affects the state of knowledge of the system. We show that there are a variety of definitions which can reasonably be applied to what a process can know about he global state. We also move beyond the semantic definitions, and present the first proof methods for proving knowledge asser-tions. Both shared memory and message passing models are considered. 1.
392|What can machines know? on the epistemic properties of machines|Abstract: It has been argued that knowledge is a useful tool for designing and analyzing complex systems in AI. The notion of knowledge that seems most relevant in this context is an external, information-based notion that can be shown to satisfy all the axioms of the modal logic S5. We carefully examine the properties of this notion of knowledge, and show that they depend crucially, and in subtle ways, on assumptions we make about the system. We present a formal model in which we can capture the types of assumptions frequently made about systems (such as whether they are deterministic or nondeterministic, whether knowledge is cumulatiw, and whether or not the environment affects the transitions of the system). We then show that under some assump-tions certain states of knowledge are not attainable, and the axioms of S5 do not completely characterize the properties of knowledge; extra axioms are needed. We provide complete axiomatizations for knowledge in a number of cases of interest. 1.
393|Knowledge consistency: a useful suspension of disbelief|The study of knowledge is of great use in distributed computer systems. It has led to better understanding of existing algorithms for such systems, as well as the development of new knowledge-based algorithm.~. The ability to achieve certain states of knowledge (e.g., common knowledge) provides a powerful tool for designing such algorithms. Un-fortunately, it has been shown that for many systems it is impossible to achieve these states of knowledge. In this paper we consider alternative interpretations of knowl-edge under which these states can be achieved. We explore the notion of consistent interpretations, and show how they can be used to circumvent the known impossibility results in a number of cases. This may lead to greater applicability of knowledge-based algorithms.
394|Formalising trust as a computational concept|Trust is a judgement of unquestionable utility — as humans we use it every day of our lives. However, trust has suffered from an imperfect understanding, a plethora of definitions, and informal use in the literature and in everyday life. It is common to say “I trust you, ” but what does that mean? This thesis provides a clarification of trust. We present a formalism for trust which provides us with a tool for precise discussion. The formalism is implementable: it can be embedded in an artificial agent, enabling the agent to make trust-based decisions. Its applicability in the domain of Distributed Artificial Intelligence (DAI) is raised. The thesis presents a testbed populated by simple trusting agents which substantiates the utility of the formalism. The formalism provides a step in the direction of a proper understanding and definition of human trust. A contribution of the thesis is its detailed exploration of the possibilities of future work in the area. Summary 1. Overview This thesis presents an overview of trust as a social phenomenon and discusses it formally. It argues that trust is: • A means for understanding and adapting to the complexity of the environment. • A means of providing added robustness to independent agents. • A useful judgement in the light of experience of the behaviour of others. • Applicable to inanimate others. The thesis argues these points from the point of view of artificial agents. Trust in an artificial agent is a means of providing an additional tool for the consideration of other agents and the environment in which it exists. Moreover, a formalisation of trust enables the embedding of the concept into an artificial agent. This has been done, and is documented in the thesis. 2. Exposition There are places in the thesis where it is necessary to give a broad outline before going deeper. In consequence it may seem that the subject is not receiving a thorough treatment, or that too much is being discussed at one time! (This is particularly apparent in the first and second chapters.) To present a thorough understanding of trust, we have proceeded breadth first in the introductory chapters. Chapter 3 expands, depth first, presenting critical views of established researchers.
396|Trust as a social reality|Although trust is an underdeveloped concept in sociology, promising theoretical formulations are available in the recent work of Luhmann and Barber. This socio-logica! version complements the psychological and attitudinal conceptualizations of experimental and survey researchers. Trust is seen to include both emotional and cognitive dimensions and to function as a deep assumption underwriting so-cial order. Contemporary examples such as lying, family exchange, monetary atti-tudes, and litigation illustrate the centrality of trust as a sociological reality. In recent years, sociologists have begun to treat trust as a sociological topic
397|Familiarity, Confidence, Trust: Problems and Alternatives|F14.33&gt; Gemeinschaft. It does not give any new insight into the particularities of trusting relations. To gain such insights we need further conceptual clarification.  Bernard Barber at least perceives this need. In his recent monograph The Logic and Limits of Trust (1983; see also Barber 1985) he tries for the first time to provide some kind of ordering. He proposes to distinguish between three different dimensions in which trusting expectations may fail: the continuity of the natural and the moral order, the &lt;&lt;95&gt;&gt; technical competence of actors in roles, and the fiduciary obligations of actors, that is, their duty and their motives to place the interests of others before their own. This distinction refers to the content of expectations and, indirectly, to causes of disappointment. It leaves unspecified, however, the social mechanisms which generate trust in spite of possible disappointment. It is this question, and in a more general sense the problem of the&lt;F
398|TouringMachines: An Architecture for Dynamic, Rational, Mobile Agents|ion-Partitioned Evaluator (APE) architecture which has been tested in a simulated, single-agent, indoor navigation domain [SH90].  The APE architecture is composed of a number of concurrent, hierarchically abstract action control layers, each representing and reasoning about some particular aspect of the agent&#039;s task domain. Implemented as a parallel blackboard-based planner, the five layers --- sensor/motor, spatial, temporal, causal, and conventional (general knowledge) --- effectively partition the agent&#039;s data processing duties along a number of dimensions including temporal granularity, information/resource use, and functional abstraction. Perceptual information flows strictly from the agent sensors (connected to the sensor /motor level) toward the higher levels, while command or goal-achievement information flows strictly downward towards the agent&#039;s effectors (also connected to the sensor/motor level).  Besides mechanisms for communicating with other layers, each layer in the AP...
399|Multi-Agent Simulation as a Tool for Modeling Societies: Application to Social Differentiation |Abstract. This paper presents the notion of multi-agent simulation that is based on the definition of computational agents that represent individual organisms (or groups of organisms) in a one to one correspondence. We discuss the properties of multi-agent simulation. We then present a multiagent simulation system based on the definition of reactive agents whose behavior is governed by the selection of simple competing tasks due to stimulus&#039;s perception. An example of a simulation of an ant colony follows as an illustration of the multiple domains in which multi-agent simulation may be used. 1.
400|Individuals, Interpersonal Relations, and Trust|ctive, and will ignore much of the fine detail in the attempt to frame a rather general picture. All of this will be prefaced by a consideration of the psychological consequences of trust for the individual, and a brief discussion of what I am taking trust to be.  &lt;&lt;32&gt;&gt;  I  The background to the observations and proposals which follow is the clear and simple fact that, without trust, the everyday social life which we take for granted is simply not possible. Luhmann makes this point in the opening chapter of his Trust and Power (1979), and Garfinkel (1963) illustrated the importance of trusting other persons claims as to the nature of social reality in his (by now infamous) rule-breaching studies. In these, Garfinkels confederates behaved as if the nature of a social situation was other than that which it might be taken to be by the unwitting subjects with whom they were interacting. The subjects (or, perhaps more appropriately, victims) 
401|Mafia: the Price of Distrust|this paper is to reconcile individual rationality with protracted collective disaster. If anything, in this case, the latter results from an excess of individual rationality. This paper is an account of the remarkable responses to a generalized absence of trust and of the mechanisms by which such responses, while reinforcing distrust, have none the less brought about a relatively stable social structure. Its major underlying assumption is that the mafia - although by no means its only element - represents the quintessence of this structure, in which all the crucial behavioural patterns converge to form an indissoluble but explosive mixture. In addition, the mafia is exemplary of those cases where the public interest &lt;&lt;159&gt;&gt; lies in collapsing rather than building internal trust and cooperation (Schelling 1984)
402|Trust and Political Agency|action needs to be stated with some care. Defenders of absolutism throughout the ages, from Bodin, Richelieu, and Louis XIV to Stalin and Mao Tse-tung, have sought to present their own putatively legitimate political authority as founded in fact upon the profound and pervasive trust of its faithful and law-abiding subjects, contested only by the wilfully and inexcusably &lt;&lt;74&gt;&gt; contumacious. But one may doubt in fact whether the passion of trust can ever have been a very prominent characteristic of intricate and massively inegalitarian political relations - perhaps indeed of any political relations of substantial demographic or geographic scope.  2  Certainly it is scarcely a prominent feature, or a natural consequence of either of the leading forms of contemporary state: the huckstering interest brokerage of advanced capitalist democracies or the petulant accents of monopolistic party authority in existing socialist states. Nor, it may be as well to add, would there be 
403|Envelopes as a Vehicle for Improving the Efficiency of Plan Execution|Envelopes are structures which capture expectations of the progress of a plan. By  comparing expected progress with actual progress, envelopes can notify the planner  when the plan violates those expectations. The planner then has the opportunityto  modify the plan to increase its efficiency given the unexpected progress. This paper  presents a specific example of the construction and use of an envelope, followed by  a discussion of the general utility of envelopes for improving the efficiency of plan  execution.
404|Trust and Reliance in Multi-Agent Systems: A Preliminary Report|This paper presents a notion of trust for use in multi-agent systems. The role trust can play in various forms of interaction is considered. Trust allows interactions between agents where previously there could be none, and allows the trusting parties to acknowledge that, whilst there is a risk in relationships with potentially malevolent agents, some form of interaction may produce benefits, where no interaction at all may not. In addition, accepting the risk allows the trusting agent to prepare itself for possibly irresponsible or untrustworthy behaviour, thus minimizing the potential damage caused. An introductory notation to refer to trusting relationships is presented and further work is discussed.  MAAMAW&#039;92, S. Martino al Cimino, Italy, 1992 1 1 Introduction  Why cooperate? With whom? To what extent? And when? Previous work in Distributed Artificial Intelligence (DAI) has concentrated on the first of these questions, with little or no thought given to the others. In particular,...
405|Towards a Semantics of Desires|As part of an effort to define a unified formal semantics for beliefs, desires and action, this paper sketches a model theory for the axiological aspects of agent theory: hedonic states, likes, goals and values. Particular attention is paid to modelling the intensity of likes. The main intuition underlying the model theory is that the axiological aspects of agent theory can be modelled through computational generalisations of physical dynamics. Computational analogues of force, mass and potential are offered.  Introduction  An important part of agent theory appears to be the notion of desires. Several formulations of agent theory have adopted beliefs, desires and intentions as a set of basic notions (the so-called BDI models). However, to our knowledge, so far relatively little has been said explicitly in the AI literature about a theory of desires (Cohen and Levesque, 1985 and in press, Moore, 1985a; Kiss, 1988, Shoham, 1989). This paper takes some initial steps towards the explicit f...
406|Exploiting virtual synchrony in distributed systems|Abstract: We describe applications of a virtually synchro-nous environment for distributed programming, which underlies a collection of distributed programming tools in the 1SIS2 system. A virtually synchronous environment allows processes to be structured into process groups, and makes events like broadcasts to the group as an entity, group membership changes, and even migration of an activity from one place to another appear to occur instan-taneously-- in other words, synchronously. A major advantage to this approach is that many aspects of a dis-tributed application can be treated independently without compromising correctness. Moreover, user code that is designed as if the system were synchronous can often be executed concurrently. We argue that this approach to building distributed and fault-tolerant software is more straightforward, more flexible, and more likely to yield correct solutions than alternative approaches. 1. A toolkit for distributed systems Consider the design of a distributed system for factory automation, say for VLSI chip fabrication. Such a system would need to group control &#039;processes into services responsible for different aspects of the fabrication procedure. One service might accept batches of chips needing photographic emulsions, another oversee transport of chips from station to sta-tion, etc. Within a service, algorithms would be needed for scheduling work, replicating data, coordi-
407|Preserving and Using Context Information in Interprocess Communication|ion  Psync is based on a conversation abstraction that provides a shared message space through which a collection of processes exchange messages. The general form of this message space is defined by a directed acyclic graph that preserves the partial order of the exchanged messages. For the purpose of this section, we view a conversation as an abstract data type that is implemented in shared memory; Section 3 gives an algorithm for implementing a conversation in an unreliable network.  A conversation behaves much like any connection-oriented IPC abstraction: A well-defined set of processes---called participants---explicitly open a conversation, exchange messages through it, and close the conversation. Only processes that have been identified as participants may exchange message through the conversation, and this set is fixed for the duration of the conversation. Processes begin a conversation with the operations:  conv = active open(participant set)  conv = passive open(pid)  The first...
408|The locus distributed operating system|LOCUS Is a distributed operating system which supports transparent access to data through a network wide fllesystem, permits automatic replication of storaget supports transparent distributed process execution, supplies a number of high reliability functions such as nested transactions, and is upward compatible with Unix. Partitioned operation of subnetl and their dynamic merge is also supported. The system has been operational for about two years at UCLA and extensive experience In its use has been obtained. The complete system architecture is outlined in this paper, and that experience is summarized. 1
409|Lazy Replication: Exploiting the Semantics of Distributed Services|To provide high availability for services such as mail or bulletin boards, data must be replicated. One way to guarantee consistency of replicated data is to force service operations to occur in the same order at all sites, but this approach is expensive. In this paper, we propose lazy replication as a way to preserve consistency by exploiting the semantics of the service&#039;s operations to relax the constraints on ordering. Three kinds of operations are supported: operations for which the clients define the required order dynamically during the execution, operations for which the service defines the order, and operations that must be globally ordered with respect to both client ordered and service ordered operations. The method performs well in terms of response time, amount of stored state, number of messages, and availability. It is especially well suited to applications in which most operations require only the client-defined order.
410|Programming with process groups: Group and multicast semantics|Process groups are a natural tool for distributed programming, and are increasingly important in dis-tributed computing environments. However, there is little agreement on the most appropriate semantics for process group membership and group communication. These issues are of special importance in the Isis system, a toolkit for distributed programming. Isls supports several styles of process group, and a collection of group communication protocols spanning a range of atomicity and ordering properties. This flexibility makes Isis adaptable to a variety of applications, but is also a source of complexity that limits performance. This paper reports on a new architecture that arose from an effort to simplify Isis process group semantics. Our findings include a refined notion of how the clients of a group should be treated, what the properties of a multicast primitive should be when systems contain large numbers of overlapping groups, and a new construct callex! the causality domain. A system based on this architecture is now being implemented in collaboration with the Chorus and Mach projects.
411|Scale and performance in a distributed file system|The Andrew File System is a location-transparent distributed tile system that will eventually span more than 5000 workstations at Carnegie Mellon University. Large scale affects performance and complicates system operation. In this paper we present observations of a prototype implementation, motivate changes in the areas of cache validation, server process structure, name translation, and low-level storage representation, and quantitatively demonstrate Andrew’s ability to scale gracefully. We establish the importance of whole-file transfer and caching in Andrew by comparing its performance with that of Sun Microsystem’s NFS tile system. We also show how the aggregation of files into volumes improves the operability of the system.
412|GPFS: A Shared-Disk File System for Large Computing Clusters|GPFS is IBM&#039;s parallel, shared-disk file system for cluster computers, available on the RS/6000 SP parallel supercomputer and on Linux clusters. GPFS is used on many of the largest supercomputers in the world. GPFS was built on many of the ideas that were developed in the academic community over the last several years, particularly distributed locking and recovery technology. To date it has been a matter of conjecture how well these ideas scale. We have had the opportunity to test those limits in the context of a product that runs on the largest systems in existence. While in many cases existing ideas scaled well, new approaches were necessary in many key areas. This paper describes GPFS, and discusses how distributed locking and recovery techniques were extended to scale to large clusters.
413|Serverless Network File Systems|In this paper, we propose a new paradigm for network file system design, serverless network file systems. While traditional network file systems rely on a central server machine, a serverless system utilizes workstations cooperating as peers to provide all file system services. Any machine in the system can store, cache, or control any block of data. Our approach uses this location independence, in combination with fast local area networks, to provide better performance and scalability than traditional file systems. Further, because any machine in the system can assume the responsibilities of a failed component, our serverless design also provides high availability via redundant data storage. To demonstrate our approach, we have implemented a prototype serverless network file system called xFS. Preliminary performance measurements suggest that our architecture achieves its goal of scalability. For instance, in a 32-node xFS system with 32 active clients, each client receives nearly as much read or write throughput as it would see if it were the only active client. 
414| Frangipani: A Scalable Distributed File System |The ideal distributed file system would provide all its users with coherent, shared access to the same set of files,yet would be arbitrarily scalable to provide more storage space and higher performance to a growing user community. It would be highly available in spite of component failures. It would require minimal human administration, and administration would not become more complex as more components were added. Frangipani is a new file system that approximates this ideal, yet was relatively easy to build because of its two-layer structure. The lower layer is Petal (described in an earlier paper), a distributed storage service that provides incrementally scalable, highly available, automatically managed virtual disks. In the upper layer, multiple machines run the same Frangipani file system code on top of a shared Petal virtual disk, using a distributed lock service to ensure coherence. Frangipaniis meant to run in a cluster of machines that are under a common administration and can communicate securely. Thus the machines trust one another and the shared virtual disk approach is practical. Of course, a Frangipani file system can be exported to untrusted machines using ordinary network file access protocols. We have implemented Frangipani on a collection of Alphas running DIGITAL Unix 4.0. Initial measurements indicate that Frangipani has excellent single-server performance and scales well as servers are added.  
415|A cost-effective, high-bandwidth storage architecture|(NASD) storage architecture, prototype implementations oj NASD drives, array management for our architecture, and three,filesystems built on our prototype. NASD provides scal-able storage bandwidth without the cost of servers used primarily,fijr trut&amp;rring data from peripheral networks (e.g. SCSI) to client networks (e.g. ethernet). Increasing datuset sizes, new attachment technologies, the convergence of peripheral and interprocessor switched networks, and the increased availability of on-drive transistors motivate and enable this new architecture. NASD is based on four main principles: direct transfer to clients, secure interfaces via cryptographic support, asynchronous non-critical-path oversight, and variably-sized data objects. Measurements of our prototype system show that these services can be cost-#ectively integrated into a next generation disk drive ASK. End-to-end measurements of our prototype drive andfilesys-terns suggest that NASD cun support conventional distrib-uted filesystems without per$ormance degradation. More importantly, we show scaluble bandwidth for NASD-special-ized filesystems. Using a parallel data mining application, NASD drives deliver u linear scaling of 6.2 MB/s per client-drive pair, tested with up to eight pairs in our lab.
417|Cluster I/O with River: Making the Fast Case Common|We introduce River, a data-flow programming environment and I/O substrate for clusters of computers. River is designed to provide maximum performance in the common case --- even in the face of nonuniformities in hardware, software, and workload. River is based on two simple design features: a high-performance distributed queue, and a storage redundancy mechanism called graduated declustering. We have implemented a number of data-intensive applications on River, which validate our design with near-ideal performance in a variety of non-uniform performance scenarios. 
418|Swift: Using distributed disk striping to provide high I/O data rates|We present an I/O architecture, called Swift, that addresses the problem of data rate mismatches between the requirements of an application, storage devices, and the interconnection medium. The goal of Swift is to support high data rates in general purpose distributed systems. Swift uses a high-speed interconnection medium to provide high data rate transfers by using multiple slower storage devices in parallel. It scales well when using multiple storage devices and interconnections, and can use any appropriate storage technology, including high-performance devices such as disk arrays. To address the problem of partial failures, Swift stores data redundantly. Using the UNIX operating system, we have constructed a simplified prototype of the Swift architecture. The prototype provides data rates that are significantly faster than access to the local SCSI disk, limited by the capacity of a single Ethernet segment, or in the case of multiple Ethernet segments by the ability of the client to drive them. We have constructed a simulation model to demonstrate how the Swift architecture can exploit advances in processor, communication and storage technology. We consider the effects of processor speed, interconnection capacity, and multiple storage agents on the utilization of the components and the data rate of the system. We show that the data rates scale well in the number of storage devices, and that by replacing the most highly stressed components by more powerful ones the data rates of the entire system increase significantly.
419|The Global File System|The Global File System (GFS) is a prototype design for a distributed file system in which cluster nodes physically share storage devices connected via a network like Fibre Channel. Networks and network attached storage devices have advanced to a level of performance and extensibility that the once believed disadvantages of “shared disk ” architectures are no longer valid. This shared storage architecture attempts to exploit the sophistication of device technologies where as the client–server architecture diminishes a device’s role to a simple components. GFS distributes the file system responsibilities across the processing nodes, storage across the devices, and file system resources across the entire storage pool. GFS caches data on the storage devices instead of the main memories of the machines. Consistency is established by using a locking mechanism maintained by the storage device controllers to facilitate atomic read–modify– write operations. The locking mechanism is being prototyped on Seagate disks drives and Ciprico disk arrays. GFS is implemented in the Silicon Graphics IRIX operating system and is accessed using standard Unix commands and utilities.
420|Hybrid Automata: An Algorithmic Approach to the Specification and Verification of Hybrid Systems|We introduce the framework of hybrid automata as a model and specification language for hybrid systems. Hybrid automata can be viewed as a generalization of timed automata, in which the behavior of variables is governed in each state by a set of differential equations. We show that many of the examples considered in the workshop can be defined by hybrid automata. While the reachability problem is undecidable even for very restricted classes of hybrid automata, we present two semidecision procedures for verifying safety properties of piecewise-linear hybrid automata, in which all variables change at constant rates. The two procedures are based, respectively, on minimizing and computing fixpoints on generally infinite state spaces. We show that if the procedures terminate, then they give correct answers. We then demonstrate that for many of the typical workshop examples, the procedures do terminate and thus provide an automatic way for verifying their properties. 1 Introduction  More and...
422|A Compositional Approach to Performance Modelling|Performance modelling is concerned with the capture and analysis of the dynamic behaviour of computer and communication systems. The size and complexity of many modern systems result in large, complex models. A compositional approach decomposes the system into subsystems that are smaller and more easily modelled. In this thesis a novel compositional approach to performance modelling is presented. This approach is based on a suitably enhanced process algebra, PEPA (Performance Evaluation Process Algebra). The compositional nature of the language provides benefits for model solution as well as model construction. An operational semantics is provided for PEPA and its use to generate an underlying Markov process for any PEPA model is explained and demonstrated. Model simplification and state space aggregation have been proposed as means to tackle the problems of large performance models. These techniques are presented in terms of notions of equivalence between modelling entities. A framewo...
423|Parametric Shape Analysis via 3-Valued Logic|Shape Analysis concerns the problem of determining &#034;shape invariants&#034;...
424|Mobile ambients|We introduce a calculus describing the movement of processes and devices, including
425|Counterexample-guided Abstraction Refinement|We present an automatic iterative abstraction-refinement methodology  in which the initial abstract model is generated by an automatic analysis of  the control structures in the program to be verified. Abstract models may admit  erroneous (or &#034;spurious&#034;) counterexamples. We devise new symbolic techniques  which analyze such counterexamples and refine the abstract model correspondingly.
426|Systematic design of program analysis frameworks|Semantic analysis of programs is essential in optimizing compilers and program verification systems. It encompasses data flow analysis, data type determination, generation of approximate invariant
428|Descriptive Complexity|book is dedicated to Daniel and Ellie. Preface This book should be of interest to anyone who would like to understand computa-tion from the point of view of logic. The book is designed for graduate students or advanced undergraduates in computer science or mathematics and is suitable as a textbook or for self study in the area of descriptive complexity. It is of particular interest to students of computational complexity, database theory, and computer aided verification. Numerous examples and exercises are included in the text, as well as a section at the end of each chapter with references and suggestions for further reading. The book provides plenty of material for a one semester course. The core of the book is contained in Chapters 1 through 7, although even here some sections can be omitted according to the taste and interests of the instructor. The remain-ing chapters are more independent of each other. I would strongly recommend including at least parts of Chapters 9, 10, and 12. Chapters 8 and 13 on lower bounds include some of the nicest combinatorial arguments. Chapter 11 includes a wealth of information on uniformity; to me, the low-level nature of translations between problems that suffice to maintain completeness is amazing and provides powerful descriptive tools for understanding complexity. I assume that most read-ers will want to study the applications of descriptive complexity that are introduced in Chapter 14.
429|Flow analysis and optimization of LISP-like structures|In [12] the authors introduced
430|Symmetry and Model Checking|We show how to exploit symmetry in model checking for concurrent systems containing many identical or isomorphic components. We focus in particular on those composed of many isomorphic processes. In many cases we are able to obtain significant, even exponential, savings in the complexity of model checking.  1 Introduction  In this paper, we show how to exploit symmetry in model checking. We focus on systems composed of many identical (isomorphic) processes. The global state transition graph M of such a system exhibits a great deal of symmetry, characterized by the group of graph automorphisms of M.  The basic idea underlying our method is to reduce model checking over the original structure M,  to model checking over a smaller quotient structure M, where symmetric states are identified. In the following paragraphs, we give a more detailed but still informal account of a &#034;group-theoretic&#034; approach to exploiting symmetry. More precisely, the symmetry of M is reflected in the group, Aut M...
431|Compiler-Based Prefetching for Recursive Data Structures|Software-controlled data prefetching offers the potential for bridging the ever-increasing speed gap between the memory subsystem and today&#039;s high-performance processors. While prefetching has enjoyed considerable success in array-based numeric codes, its potential in pointer-based applications has remained largely unexplored. This paper investigates compilerbased prefetching for pointer-based applications---in particular, those containing recursive data structures. We identify the fundamental problem in prefetching pointer-based data structures and propose a guideline for devising successful prefetching schemes. Based on this guideline, we design three prefetching schemes, we automate the most widely applicable scheme (greedy prefetching) in an optimizing research compiler, and we evaluate the performance of all three schemes on a modern superscalar processor similar to the MIPS R10000. Our results demonstrate that compiler-inserted prefetching can significantly improve the execution...
432|Detecting Parallelism in C Programs with Recursive Data Structures|In this paper we present techniques to detect three common patterns of parallelism in C programs that use recursive data structures. These patterns include, function calls that access disjoint sub-pieces of tree-like data structures, pointer-chasing loops that traverse list-like data structures, and array-based loops which operate on an array of pointers pointing to disjoint data structures. We design dependence tests using a family of three existing pointer analyses, namely points-to, connection and shape analyses, with special emphasis on shape analysis. To identify loop parallelism, we introduce special tests for detecting loop-carried dependences in the context of recursive data structures. We have implemented the tests in the framework of our McCAT C compiler, and we present some preliminary experimental results.
433|Experience with Predicate Abstraction|This reports some experiences with a recently-implemented  prototype system for verification using predicate abstraction, based on  the method of Graf and Saidi [9]. Systems are described using a language  of iterated guarded commands, called MurOE  \Gamma\Gamma  (since it is a simplified  version of our MurOE protocol description language). The system makes  use of two libraries: SVC [1] (an efficient decision procedure for quantifierfree  first-order logic) and the CMU BDD library. The use of these libraries  increases the scope of problems that can be handled by predicate  abstraction through increased efficiency, especially in SVC, which is typically  called thousands of times. The verification system also provides  limited support for quantifiers in formulas. The system ...
434|TVLA: A System for Implementing Static Analyses|We present TVLA (Three-Valued-Logic Analysis engine). TVLA is a &#034;YACC&#034;-like framework for automatically constructing static-analysis algorithms from an operational semantics, where the operational semantics is specified using logical formulae. TVLA was implemented in Java and was successfully used to perform shape analysis on programs manipulating linked data structures (singly and doubly linked lists), to prove safety properties of Mobile Ambients, and to verify the partial correctness of several sorting programs.
435|Pointer-induced aliasing: A problem classification|A?iasing occurs at some program point during execu-tion when two or more names exist for the same loca-tion. We have isolated various programming language mechanisms which create aliases. We have classified the complexity of the fllas problem induced by each mech-anism alone and in combination, as AfP-hard, comple-ment tip-hard, or polynomial (’P). We present our problem classification, give an overview of our proof that finding interprocedural aliases in the presence of single level pointers is in 7, and present a represent tive proof for the NP-hard problems. 1
436|A flexible approach to interprocedural data flow analysis and programs with recursive data structures|A new approach to data flow analysis of procedural pro-grams and programs with recursive data structures is described. The method depends on simulation of the in-terpreter for the subject programming language using a retrieval function to approximate a program’s data structures. 1.
437|Putting static analysis to work for verification: A case study|Abstract We study how program analysis can be used to:* Automatically prove partial correctness of correct programs.* Discover, locate, and diagnose bugs in incorrect programs. Specifically, we present an algorithm that analyzes sorting programs that manipulate linked lists. A prototype of the algorithm has been implemented. We show that the algorithm is sufficiently precise to discover that (correct versions) of bubble-sort and insertion-sort procedures do, in fact, produce correctly sorted lists as outputs, and that the invariant &#034;is-sorted &#034; is maintained by listmanipulation operations such as element-insertion, elementdeletion, and even destructive list reversal and merging of two sorted lists. When we run the algorithm on erroneous versions of bubble-sort and insertion-sort procedures, it is able to discover and sometimes even locate and diagnose the error. 1 Introduction This paper shows that static analysis can be employed to* Automatically prove partial correctness of correct programs.*
438|Abstractions for Recursive Pointer Data Structures: Improving the Analysis and Transformation of Imperative Programs|Even though impressive progress has been made...
439|Verifying Safety Properties of Concurrent Java Programs Using 3-Valued Logic|We provide a parametric framework for verifying safety properties of concurrent Java programs. The framework combines thread-scheduling information with information about the shape of the heap. This leads to error-detection algorithms that are more precise than existing techniques. The framework also provides the most precise shape-analysis algorithm for concurrent programs. In contrast to existing verification techniques, we do not put a bound on the number of allocated objects. The framework even produces interesting results when analyzing Java programs with an unbounded number of threads. The framework is applied to successfully verify the following properties of a concurrent program:
440|Shape Types|Type systems currently available for imperative languages are too weak to detect a significant class of programming errors. For example, they cannot express the property that a list is doubly-linked or circular. We propose a solution to this problem based on a notion of shape types defined as context-free graph grammars. We define graphs in settheoretic terms, and graph modifications as multiset rewrite rules. These rules can be checked statically to ensure that they preserve the structure of the graph specified by the grammar. We provide a syntax for a smooth integration of shape types in C. The programmer can still express pointer manipulations with the expected constant time execution and benefits from the additional guarantee that the property specified by the shape type is an invariant of the program.  
441|Multivalued Logics: A Uniform Approach to Inference in Artificial Intelligence|This paper describes a uniform formalization of much of the current work in AI on inference systems. We show that many of these systems, including first-order theorem provers, assumption-based truth maintenance systems (atms&#039;s) and unimplemented formal systems such as default logic or circumscription can be subsumed under a single general framework. We begin by defining this framework, which is based on a mathematical structure known as a bilattice.  We present a formal definition of inference using this structure, and show that this definition generalizes work involving atms&#039;s and some simple nonmonotonic logics. Following the theoretical description, we describe a constructive approach to inference in this setting; the resulting generalization of both conventional inference and atms&#039;s is achieved without incurring any substantial computational overhead. We show that our approach can also be used to implement a default reasoner, and discuss a combination of default and atms methods th...
442|Automatic Verification of Pointer Programs using Monadic Second-Order Logic|We present a technique for automatic verification of pointer programs based on a decision procedure for the monadic second-order logic on finite strings. We are concerned with a while-fragment of Pascal, which includes recursively-defined pointer structures but excludes pointer arithmetic. We define a logic of stores with interesting basic predicates such as pointer equality, tests for nil pointers, and garbage cells, as well as reachability along pointers. We present a complete decision procedure for Hoare triples based on this logic over loop-free code. Combined with explicit loop invariants, the decision procedure allows us to answer surprisingly detailed questions about small but non-trivial programs. If a program fails to satisfy a certain property, then we can automatically supply an initial store that provides a counterexample. Our technique has been fully and e#ciently implemented for linear linked lists, and extends in principle to tree structures. The resulting system can be used to verify extensive properties of smaller pointer programs and could be particularly useful in a teaching environment. # detex paper.tex | wc | cut-d&#039; &#039; -f2 = 4821 1  1 
443|Dyn-FO: A Parallel, Dynamic Complexity Class|Traditionally, computational complexity has considered only static problems. Classical Complexity Classes such as NC, P, and NP are defined in terms of the complexity of checking -- upon presentation of an entire input -- whether the input satisfies a certain property. For many applications of computers it is more appropriate to model the process as a dynamic one. There is a fairly large object being worked on over a period of time. The object is repeatedly modified by users and computations are performed. We develop a theory of Dynamic Complexity. We study the new complexity class, Dynamic First-Order Logic (Dyn-FO). This is the set of properties that can be maintained and queried in first-order logic, i.e. relational calculus, on a relational database. We show that many interesting properties are in Dyn-FO including multiplication, graph connectivity, bipartiteness, and the computation of minimum spanning trees. Note that none of these problems is in static FO, and this f...
444|Shape Analysis for Mobile Ambients|. The ambient calculus is a calculus of computation that allows active processes to move between sites. We present an analysis inspired by state-of-the-art pointer analyses that safely and accurately predicts which processes may turn up at what sites during the execution of a composite system. The analysis models sets of processes by sets of regular tree grammars enhanced with context-dependent counts, and it obtains its precision by combining a powerful redex materialisation with a strong redex reduction (in the manner of the strong updates performed in pointer analyses). The underlying ideas are flexible and scale up to general tree structures admitting powerful restructuring operations. 1 Introduction Aims. The ambient calculus [3, 4] is a process algebra based on the #-calculus. Rather than focussing on communication of values or channels, it focuses on the movement of  active processes (in the form of mobile ambients) between sites modelling various administrative domains (also in...
445|Analysis of Dynamic Structures for Efficient Parallel Execution|Programs written in high-level programming languages and in particular object-oriented languages  make heavy use of references and dynamically allocated structures. As a result, precise analysis  of such features is critical for producing efficient implementations. The information produced  by this analysis is invaluable for compiling programs for both sequential and parallel machines.  This paper presents a new structure analysis technique handling references and dynamic structures  which enables precise analysis of infinite recursive data structures. The precise analysis depends  on an enhancement of Chase et al.&#039;s Storage Shape Graph (SSG) called the Abstract Storage Graph  (ASG) which extends SSG&#039;s with choice nodes, identity paths, and specialized storage nodes and references.  These extensions allow ASG&#039;s to precisely describe singly- and multiply-linked lists as well  as a number of other pointer structures such as octrees, and to analyze programs which manipulate  them.  We des...
446|Compile-Time Debugging of C Programs Working on Trees|. We exhibit a technique for automatically verifying the safety  of simple C programs working on tree-shaped data structures. We do  not consider the complete behavior of programs, but only attempt to  verify that they respect the shape and integrity of the store. A verified  program is guaranteed to preserve the tree-shapes of data structures, to  avoid pointer errors such as NULL dereferences, leaking memory, and  dangling references, and furthermore to satisfy assertions specified in a  specialized store logic.  A program is transformed into a single formula in WSRT, a novel extension  of WS2S that is decided by the MONA tool. This technique is  complete for loop-free code, but for loops and recursive functions we rely  on Hoare-style invariants. A default well-formedness invariant is supplied  and can be strengthened as needed by programmer annotations. If a program  fails to verify, a counterexample in the form of an initial store that  leads to an error is automatically generated...
447|A Kleene Analysis of Mobile Ambients|We show how a program analysis technique originally developed for C-like pointer structures can be adapted to analyse the hierarchical structure of processes in the ambient calculus. The technique is based on modeling the semantics of the language in a two-valued logic; by reinterpreting the logical formulae in Kleene&#039;s three-valued logic we obtain an analysis allowing us to reason about may as well as must properties. The correctness of the approach follows from a general Embedding Theorem for Kleene&#039;s logic; furthermore embeddings allow us to reduce the size of structures so as to control the time and space complexity of the analysis.
448|LTL Model Checking for Systems with Unbounded Number of Dynamically Created Threads and Objects|. One of the stumbling blocks to applying model checking to a concurrent language such as Java is that a program&#039;s data structures (as well as the number of threads) can grow and shrink dynamically, with no fixed upper bound on their size or number. This paper presents a method for verifying LTL properties of programs written in such a language. It uses a powerful abstraction mechanism based on 3-valued logic, and handles dynamic allocation of objects (including thread objects) and references to objects. This allows us to verify many programs that dynamically allocate thread objects, and even programs that create an unbounded number of threads. 1 
451|On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games|The purpose of this paper is to study the fundamental mechanism humans use in argumentation and its role in different major approaches to commonsense reasoning in AI and logic programming. We present three novel results: We develop a theory for argumentation in which the acceptability of arguments is precisely defined. We show that logic programming and nonmonotonic reasoning in AI are different forms of argumentation. We show that argumentation can be viewed as a special form of logic programming with negation as failure. This result introduces a general method for generating metainterpreters for argumentation systems. 1.
452|Classical negation in logic programs and disjunctive databases|An important limitation of traditional logic programming as a knowledge representation tool, in comparison with classical logic, is that logic programming does not allow us to deal directly with incomplete information. In order to overcome this limitation, we extend the class of general logic programs by including classical negation, in addition to negation-as-failure. The semantics of such extended programs is based on the method of stable models. The concept of a disjunctive database can be extended in a similar way. We show that some facts of commonsense knowledge can be represented by logic programs and disjunctive databases more easily when classical negation is available. Computationally, classical negation can be eliminated from extended programs by a simple preprocessor. Extended programs are identical to a special case of default theories in the sense of Reiter. 1
453|Bilattices and the Semantics of Logic Programming|Bilattices, due to M. Ginsberg, are a family of truth value spaces that allow elegantly for missing or conflicting information. The simplest example is Belnap&#039;s four-valued logic, based on classical two-valued logic. Among other examples are those based on finite many-valued logics, and on probabilistic valued logic. A fixed point semantics is developed for logic programming, allowing any bilattice as the space of truth values. The mathematics is little more complex than in the classical two-valued setting, but the result provides a natural semantics for distributed logic programs, including those involving confidence factors. The classical two-valued and the Kripke/Kleene three-valued semantics become special cases, since the logics involved are natural sublogics of Belnap&#039;s logic, the logic given by the simplest bilattice. 1 Introduction  Often useful information is spread over a number of sites (&#034;Does anybody know, did Willie wear a hat when he left this morning?&#034;) that can be speci...
454|Complexity and Expressive Power of Logic Programming|This paper surveys various complexity results on different forms of logic programming. The main focus is on decidable forms of logic programming, in particular, propositional logic programming and datalog, but we also mention general logic programming with function symbols. Next to classical results on plain logic programming (pure Horn clause programs), more recent results on various important extensions of logic programming are surveyed. These include logic programming with different forms of negation, disjunctive logic programming, logic programming with equality, and constraint logic programming. The complexity of the unification problem is also addressed. 
455|On the Power of Magic|This paper considers the efficient evaluation of recursive queries expressed using Horn Clauses. We define sideways information passing formally and show how a query evaluation algorithm may be defined in terms of sideways information passing and control. We then consider a class of information passing strategies that suffices to describe most query evaluation algorithms in the database literature, and show that these strategies may always be implemented by rewriting a given program and evaluating the rewritten program bottom-up. We describe in detail several algorithms for rewriting a program. These algorithms generalize the Counting and Magic Sets algorithms to work with arbitrary programs. Safety and optimality of the algorithms are also considered. 1. Introduction  The evaluation of recursive queries expressed as sets of Horn Clauses over a database has recently received much attention. Consider the following program:  anc (X, Y) :- par (X, Y) anc (X, Y) :- par (X, Z), anc (Z, Y)  ...
456|Logic Programming and Negation: A Survey|We survey here various approaches which were proposed to incorporate negation in logic  programs. We concentrate on the proof-theoretic and model-theoretic issues and the relationships  between them.  
457|Logic Programming and Knowledge Representation|In this paper, we review recent work aimed at the application of declarative logic  programming to knowledge representation in artificial intelligence. We consider exten-  sions of the language of definite logic programs by classical (strong) negation, disjunc-  tion, and some modal operators and show how each of the added features extends the  representational power of the language.
458|Consistency of Clark&#039;s Completion and Existence of Stable Models|The most general notion of canonical model for a logic program with negation is the one of stable model [9]. In [7] the stable models of a logic program are characterized by the well-supported Herbrand models of the program, and a new fixed point semantics that formalizes the bottom-up truth maintenance procedure of [4] is based on that characterization. Here we focus our attention on the abstract notion of well-supportedness in order to derive sufficient conditions for the existence of stable models. We show that if a logic program \Pi is positive-order-consistent (i.e. there is no infinite decreasing chain w.r.t. the positive dependencies in the atom dependency graph of \Pi) then the Herbrand models of comp(\Pi) coincide with the stable models of \Pi. From this result and the ones of [10] [17] [2] on the consistency of Clark&#039;s completion, we obtain sufficient conditions for the existence of stable models for positive-order-consistent programs. Then we show that a negative cycle free ...
459|On the Computational Cost of Disjunctive Logic Programming: Propositional Case|This paper addresses complexity issues for important problems arising with disjunctive  logic programming. In particular, the complexity of deciding whether a disjunctive logic  program is consistent is investigated for a variety of well-known semantics, as well as the  complexity of deciding whether a propositional formula is satised by all models according  to a given semantics. We concentrate on nite propositional disjunctive programs  with as wells as without integrity constraints, i.e., clauses with empty heads; the problems  are located in appropriate slots of the polynomial hierarchy. In particular, we show  that the consistency check is   P  2 -complete for the disjunctive stable model semantics  (in the total as well as partial version), the iterated closed world assumption, and the  perfect model semantics, and we show that the inference problem for these semantics is    P  2 -complete; analogous results are derived for the an
460|Encoding Planning Problems in Nonmonotonic Logic Programs|. We present a framework for encoding planning problems in logic programs with negation as failure, having computational efficiency as our major consideration. In order to accomplish our goal, we bring together ideas from logic programming and the planning systems  graphplan and satplan. We discuss different representations of planning problems in logic programs, point out issues related to their performance, and show ways to exploit the structure of the domains in these representations. For our experimentation we use an existing implementation of the stable models semantics called smodels. It turns out that for careful and compact encodings, the performance of the method across a number of different domains, is comparable to that of planners like  graphplan and satplan. 1 Introduction  Nonmonotonic reasoning was originally motivated by the need to capture in a formal logical system aspects of human commonsense reasoning that enable us to withdraw previous conclusions when new informat...
461|Reasoning about priorities in default logic, in|In this paper we argue that for realistic applications involving default reasoning it is necessary to reason about the priorities of defaults. Existing approaches require the knowledge engineer to explicitly state all relevant priorities which are then handled in an extra-logical manner, or they are restricted to priorities ba-sed on specificity, neglecting other relevant criteria. We present an approach where priority information can be represented within the logical language. Our approach is based on PDL, a prioritized exten-sion of Reiter’s Default Logic recently proposed by the same author. In PDL the generation of extensions is controlled by an ordering of the defaults. This pro-perty is used here in the following way: we first build Reiter extensions of a given default theory. These ex-tensions contain explicit information about the prio-rities of defaults. We then eliminate every extension E that cannot be reconstructed as a PDL extension based on a default ordering that is compatible with the priority information in E. An example from legal reasoning illustrates the power of our approach. 1.
462|Reasoning Agents In Dynamic Domains|The paper discusses an architecture for intelligent agents based on the use of A-Prolog - a language of logic programs under the answer set semantics. A-Prolog is used to represent the agent&#039;s knowledge  about the domain and to formulate the agent&#039;s reasoning tasks. We outline how these tasks can be reduced to answering questions about  properties of simple logic programs and demonstrate the methodology  of constructing these programs.  Keywords: Intelligent agents, logic programming and nonmonotonic reasoning.  1 INTRODUCTION  This paper is a report on the attempt by the authors to better understand  the design of software components of intelligent agents capable of reasoning, planning and acting in a changing environment. The class of such agents includes, but is not limited to, intelligent mobile robots, softbots, immobots, intelligent information systems, expert systems, and decision-making systems. The ability to design intelligent agents (IA) is crucial for such diverse tasks as ...
463|Dualities between Alternative Semantics for Logic Programming and Nonmonotonic Reasoning|The Gelfond-Lifschitz operator [GL88] associated with a logic program (and likewise the operator associated with default theories by Reiter) exhibits oscillating behavior. In the case of logic programs, there is always at least one finite, non-empty collection of Herbrand interpretations around which the Gelfond-Lifschitz [GL88] operator &#034;bounces around&#034;. The same phenomenon occurs with default logic when Reiter&#039;s operator \Gamma \Delta  is considered. Based on this, a &#034;stable class&#034; semantics and &#034;extension class&#034; semantics was proposed in [BS90]. The main advantage of this semantics was that it was defined for all logic programs (and default theories), and that this definition was modelled using the standard operators existing in the literature such as Reiter&#039;s \Gamma \Delta operator. In this paper, our primary aim is to prove that there is a very interesting duality between stable class theory and the well founded semantics for logic programming. In the stable class semantics, class...
464|An A-Prolog decision support system for the Space Shuttle|The goal of this paper is to test if a programming methodology  based on the declarative language A-Prolog, algorithms  for computing answer sets of programs of A-Prolog, and  programming systems implementing these algorithms can  be successfully applied to the development of medium size  knowledge-intensive applications. We report on a successful  design and development of such a system controlling some of  the functions of the Space Shuttle.  Introduction  The research presented in this paper is rooted in recent developments in several areas of AI. Advances in the work on semantics of negation in logic programming (Gelfond &amp; Lifschitz 1988; 1991) and on formalization of common-sense reasoning (Reiter 1980; Moore 1985) led to the development of the declarative language, A-Prolog, used in this paper to encode the domain knowledge, and to an A-Prolog based methodology for representing defaults. Insights on the nature of causality and its relationship with answer sets of logic programs (...
465|Default Reasoning System DeReS|In this paper, we describe an automated reasoning  system, called DeReS. DeReS implements  default logic of Reiter by supporting  several basic reasoning tasks such as testing  whether extensions exist, finding one or all  extensions (if at least one exists) and querying  if a formula belongs to one or all extensions.  If an input theory is a logic program,  DeReS computes stable models of this program  and supports queries on membership of  an atom in some or all stable models. The  paper contains an account of our preliminary  experiments with DeReS and a discussion  of the results. We show that a choice  of a propositional prover is critical for the  efficiency of DeReS. We also present a general  technique that eliminates the need for  some global consistency checks and results  in substantial speedups. We experimentally  demonstrate the potential of the concept of  relaxed stratification for making automated  reasoning systems practical.  1 INTRODUCTION  The area of nonmonotonic l...
466|Reasoning with Prioritized Defaults|The purpose of this paper is to investigate the methodology of reasoning with prioritized defaults in the language of logic programs under the answer set semantics. We present a domain independent system of axioms, written as an extended logic program, which defines reasoning with prioritized defaults. These axioms are used in conjunction with a description of a particular domain encoded in a simple language allowing representation of defaults and their priorities. Such domain descriptions are of course domain dependent and should be specified by the users. We give sufficient conditions for consistency of domain descriptions and illustrate the use of our system by formalizing various examples from the literature. Unlike many other approaches to formalizing reasoning with priorities ours does not require development of the new semantics of the language. Instead, the meaning of statements in the domain description is given by the system of (domain independent) axioms. We believe that in ...
467|On Logic Program Semantics with Two Kinds of Negation|Recently several authors have stressed and showed the importance of having a second kind of negation in logic programs for use in deductive databases, knowledge representation, and nonmonotonic reasoning [6, 7, 8, 9, 13, 14, 15, 24].  Different semantics for logic programs extended with :-negation (extended logic programs) have appeared [1, 4, 6, 9, 11, 12, 17, 19, 24] but, contrary to what happens with semantics for normal logic programs, there is no general comparison among them, specially in what concerns the use and meaning of the newly introduced :-negation.  The goal of this paper is to contrast a variety of these semantics in what concerns their use and meaning of :-negation, and its relation to classical negation and to the default negation of normal programs, here denoted by not :  To this purpose we define a parametrizeable schema to encompass and characterize a diversity of proposed semantics for extended logic programs, where the parameters are two: one the axioms AX: defin...
468|Mixed Integer Programming Methods for Computing Nonmonotonic Deductive Databases|Though the declarative semantics of both explicit and nonmonotonic negation in logic programs has been studied extensively, relatively little work has been done on computation and implementation of these semantics. In this paper, we study three different approaches to computing stable models of logic programs based on mixed integer linear programming methods for automated deduction introduced by R. Jeroslow. We subsequently discuss the relative efficiency of these algorithms. The results of experiments with a prototype compiler implemented by us tend to confirm our theoretical discussion. In contrast to resolution, the mixed integer programming methodology is both fully declarative and handles reuse of old computations gracefully. We also introduce, compare, implement, and experiment with linear constraints corresponding to four semantics for &#034;explicit&#034; negation in logic programs: the four-valued annotated semantics [3], the Gelfond-Lifschitz semantics [12], the over-determined models ...
469|Default Logic as a Query Language|| Research in non-monotonic reasoning has focused largely on the idea of representing knowledge about the world via rules that are generally true but can be defeated. Even if relational databases are nowadays the main tool for storing very large sets of data, the approach of using non-monotonic AI formalisms as relational database query languages has been investigated to a much smaller extent. In this work we propose a novel application of Reiter&#039;s default logic by introducing a default query language (DQL) for nite relational databases, which is based on default rules. The main result of this paper is that DQL is as expressive as SO 98 , the existential-universal fragment of secondorder logic. This result is not only of theoretical importance: We exhibit queries {which are useful in practice{ that can be expressed with DQL and can not with other query languages based on non-monotonic logics such as DATALOG with negation under the stable model semantics. In particular, we show that DQ...
470|Computation of Stable Models and its Integration with Logical Query Processing|The well-founded semantics and the stable model semantics capture intuitions of the  skeptical and credulous semantics in nonmonotonic reasoning, respectively. They represent  the two dominant proposals for the declarative semantics of deductive databases and logic  programs. However, neither semantics seems to be suitable for all applications. We have  developed an efficient implementation of goal-oriented effective query evaluation under the  well-founded semantics. It produces a residual program for subgoals that are relevant to a  query, which contains facts for true instances and clauses with body literals for undefined  instances. This paper presents a simple method of stable model computation that can be  applied to the residual program of a query to derive answers with respect to stable models.  The method incorporates both forward and backward chaining to propagate the assumed  truth values of ground atoms, and derives multiple stable models through backtracking.  Users are ab...
471|Is Intractability of Non-Monotonic Reasoning a Real Drawback?|Several studies about computational complexity of non-monotonic reasoning (NMR) showed that non-monotonic inference is significantly harder than classical, monotonic inference. This contrasts with the general idea that NMR can be used to make knowledge representation and reasoning simpler, not harder. In this paper we show that, to some extent, NMR fulfills the representation  goal. In particular, we prove that non-monotonic formalisms such as circumscription and default logic allow for a much more compact and natural representation of propositional knowledge than propositional calculus. Proofs are based on a suitable definition of compilable inference problem, and on non-uniform complexity classes. Some results about intractability of circumscription and default logic can therefore be interpreted as the price one has to pay for having such an extra-compact representation. On the other hand, intractability of inference and compactness of representation are not equivalent notions: we ex...
472|Approximate Reasoning About Actions in Presence of Sensing and Incomplete Information|Sensing actions are important for planning with incomplete information. A solution for the frame problem for sensing actions was proposed by Scherl and Levesque. They adapt the possible world model of knowledge to situation calculus. In this paper we propose a high level language in the spirit of the language A, that allows sensing actions. We then present two approximation semantics of this language and their translation to logic programs. Unlike, A, where states are two valued interpretations, and unlike the approach in Scherl and Levesque where states are Kripke models, in our approach states are three valued interpretations. 1 Introduction and Motivation  Sensing actions are important for planning with incomplete information [2, 5, 8, 7]. Consider a robot which is asked to get milk. The robot does not know if there is milk in the fridge or not, and of course would prefer to get the milk from the fridge rather than from the neighborhood 24 hr store. A conditional plan of the robot t...
473|Logic Programming and Reasoning with Incomplete Information|The purpose of this paper is to expand the syntax and semantics of logic programs and disjunctive databases to allow for the correct representation of incomplete information in the presence of multiple extensions. The language of logic programs with classical negation, epistemic disjunction, and negation by failure is further expanded by new modal operators K and M (where for the set of rules T and formula  F , KF stands for &#034;F is known to be true by a reasoner with a set of premises T &#034; and MF means &#034; F may be believed to be true&#034; by the same reasoner). Sets of rules in the extended language will be called epistemic specifications. We will define the semantics of epistemic specifications (which expands the semantics of disjunctive databases from [GL91]) and demonstrate their applicability to formalization of various forms of commonsense reasoning. In particular, we suggest a new formalization of the closed world assumption which seems to better correspond to the assumption&#039;s intuitive...
474|Experimenting with Nonmonotonic Reasoning|In this paper, we describe a system, called TheoryBase, whose goal is to facilitate experimental studies of nonmonotonic reasoning systems. TheoryBase generates test default theories and logic programs. It has an identification system for generated theories, which allows us to reconstruct a logic program or a default theory from its identifier. Hence, exchanging test cases requires only exchanging identifiers. TheoryBase can generate a large variety of examples of default theories and logic programs. We believe that its universal adoption may significantly advance experimental studies of nonmonotonic reasoning systems.
475|The dlv System: Model Generator and Application Frontends|During the last years, much research has been done concerning semantics and complexity of Disjunctive Deductive Databases (DDDBs). While DDDBs --- function-free disjunctive logic programs with negation in rule bodies allowed --- are now generally considered a powerful tool for common-sense reasoning and knowledge representation, there has been a shortage of actual (let alone efficient) implementations ([ST94, ADN97]). This paper presents a brief overview of the architecture of the dlv (datalog with disjunction) system system currently developed at TU Wien in the FWF project P11580-MAT &#034;A Query System for Disjunctive Deductive Databases&#034;, especially focusing on the Model Generator -- the &#034;heart&#034; of the  dlv system -- and the integrated frontends for diagnostic reasoning and SQL3.  Keywords: Deductive Databases Systems, Disjunctive Logic Programming, Non-Monotonic Reasoning, Implementation. 1 System Overview  An outline of the general architecture of our system is depicted in Figure 1. T...
476|Semantics and complexity of abduction from default theories (Extended Abstract)  (1997) |Since logical knowledge representation is commonly based on nonclassical formalisms like default logic, autoepistemic logic, or circumscription, it is necessary to perform abductive reasoning from theories of nonclassical logics. In this paper, we investigate how abduction can be performed from theories in default logic. Different modes of abduction are plausible, based on credulous and skeptical default reasoning; they appear useful for different applications such as diagnosis and planning. Moreover, we analyze the complexity of the main abductive reasoning tasks. They are intractable in the general case; we also present known classes of default theories for which abduction is tractable.  
478|Complexity Aspects of Various Semantics for Disjunctive Databases|This paper addresses complexity issues for important problems arising with disjunctive databases. In particular, the complexity of inference of a literal and a formula from a propositional disjunctive database under a variety of wellknown disjunctive database semantics is investigated, as well deciding whether a disjunctive database has as model under a particular semantics. The problems are located in appropriate slots of the polynomial hierarchy.  1 Introduction  Allowing to store disjunctions in a logical database is indispensable for dealing with disjunctive information. Accordingly, the meaning of a disjunctive database is expressed by a set of models instead of a single model as in case of nondisjunctive databases. A variety of different semantics for disjunctive databases has been proposed in the literature; see [9] for a comprehensive overview. We will deal with the following ones.  ffl The Generalized Closed World Assumption (GCWA) by Minker [16].  ffl The Extended Generalized...
479|Super Logic Programs|Recently, considerable interest and research e#ort has been given to the problem of finding a suitable extension of the logic programming paradigm beyond the class of normal logic programs. In order to demonstrate that a class of programs can be justifiably called an extension of logic programs one should be able to argue that:  .  the proposed syntax of such programs resembles the syntax of logic programs but it applies to a significantly broader class of programs;  .  the proposed semantics of such programs constitutes an intuitively natural extension of the semantics of normal logic programs;  .  there exists a reasonably simple procedural mechanism allowing, at least in principle, to compute the semantics;  .  the proposed class of programs and their semantics is a special case of a more general non-monotonic formalism which clearly links it to other well-established non-monotonic formalisms. In this paper we propose a specific class of extended logic programs which will be (modestly) called super logic programs or just super-programs. We will argue that the class of super-programs satisfies all of the above conditions, and, in addition, is su#ciently flexible to allow various application-dependent extensions and modifications. We also provide a brief description of a Prolog implementation of a query-answering interpreter for the class of super-programs which is available via FTP and WWW.  Keywords: Non-Monotonic Reasoning, Logics of Knowledge and Beliefs, Semantics of Logic Programs and Deductive Databases.  # An extended abstract of this paper appeared in the Proceedings of the Fifth International Conference on Principles of Knowledge Representation and Reasoning (KR&#039;96), Boston, Massachusetts, 1996, pp. 529--541.  + Partially supported by the National Science Fou...
480|Reasoning in Open Domains|In this paper we modify the semantics of epistemic specifications (and hence the answer set semantics of extended logic program and disjunctive databases) to allow for reasoning in the absence of domain--closure assumption.  This modification increases the expressive power of the language and allows one to explicitly state the domain--closure and other assumptions about the domain of discourse in the language of epistemic specifications. The power of the language is demonstrated by way of examples. In particular we show how open domain assumption can be used to formalize default reasoning in the presence of anonymous exceptions to defaults. 1 Introduction  Epistemic specifications were introduced in [4] as a tool for knowledge representation. They can be viewed as a generalization of &#034;extended disjunctive databases&#034; from [6] capable of expressing powerful forms of introspection. The semantics of an epistemic specification 5 has been given via the notion of a world view of 5 - a collect...
481|Disjunctive LP + Integrity Constraints = Stable Model Semantics|We show that stable models of logic programs may be viewed as minimal models of programs that satisfy certain additional constraints. To do so, we transform the normal programs into disjunctive logic programs and sets of integrity constraints. We show that the stable models of the normal program coincide with the minimal models of the disjunctive program that satisfy  the integrity constraints. As a consequence, the stable model semantics can be characterized using the Extended Generalized Closed World Assumption for disjunctive logic programs. Using this result, we develop a bottom-up algorithm for function free logic programs to find all stable models of a normal program by computing the perfect models of a disjunctive stratified logic program and checking them for consistency with the integrity constraints. The integrity constraints provide a rationale as to why some normal logic programs have no stable models.  1 Introduction  We consider the problem of finding the stable models o...
482|Transformations of logic programs related to causality and planning|Abstract. We prove two properties of logic programs under the answer set semantics that may be useful in connection with applications of logic programming to representing causality and to planning. One theorem is about the use of disjunctive rules to express that an atom is exogenous. The other provides an alternative way of expressing that a plan does not include concurrently executed actions. 1 Introduction In this note we prove two properties of logic programs under the answer set semantics [3] that may be useful in connection with applications of logic programming to representing causality and to planning. According to the first of the two theorems, replacing a disjunctive rule of the form
483|Weight Constraints as Nested Expressions|We compare two recent extensions of the answer set (stable model) semantics  of logic programs. One of them, due to Lifschitz, Tang and Turner,  allows the bodies and heads of rules to contain nested expressions. The other,  due to Niemela and Simons, uses weight constraints. We show that there is  a simple, modular translation from the language of weight constraints into  the language of nested expressions that preserves the program&#039;s answer sets.  This translation can be used to study equivalent transformations of logic programs  written in the input language of the answer set programming system  SMODELS.  Keywords: answer sets, cardinality constraints, SMODELS, stable models,  weight constraints.  1 
484|Towards a Theory of Elaboration Tolerance: Logic Programming Approach|This paper is an attempt at mathematical investigation of software development process in the context of declarative logic programming. We introduce notions of specification and specification constructor which are developed from natural language description of a problem. Generalizations of logic programs, called lp-functions are introduced to represent these specifications. We argue that the process of constructing lp-function representing a specification S should be supported by certain types of mathematical results which we call representation theorems. We present two such theorems to illustrate the idea. 1 Introduction  This paper is written in the framework of declarative logic programming paradigm (see, for instance, [17, 14]) which strives to reduce a substantial part of a programming process to the description of objects comprising the domain of interest and relations between these objects. After such description is produced by a programmer it can be queried to establish truth o...
485|Complexity of Query Answering in Logic Databases with Complex Values|This paper characterizes the computational complexity of nonrecursive queries in logic databases with complex values. Queries are represented by Horn clause logic programs. Complex values are represented by terms in equational theories (finite sets and multisets are examples of such complex values). We show that the problem of whether a query has a nonempty answer is NEXP-hard for nonrecursive range-restricted queries. We also show that this problem is in NEXP if complex values satisfy the following condition: the solvability problem for equations in the corresponding equational theory is in NP. Since trees, finite sets and multisets satisfy this condition, the query answering problem for logic databases with trees, finite sets and multisets is shown to be NEXP-complete.  2 2  Copyright c fl 1997, 1998 Evgeni Dantsin and Andrei Voronkov. This technical report and other technical reports in this series can be obtained at http://www.csd.uu.se/papers/reports.html or at ftp.csd.uu.se in th...
486|Expanding Queries to Incomplete Databases By Interpolating General Logic Programs.|In databases, queries are usually defined on complete databases. In this paper we introduce and motivate the notion of extended queries that are defined on incomplete databases. We argue that the language of extended logic program is appropriate for representing extended queries. We show through examples that given a query, a particular extension of it has important characteristics which corresponds to removal of the CWA from the original specification of the query. We refer to this particular extension as the expansion of the original query. Normally queries are expressed as general logic programs. We develop an algorithm that given a general logic program (satisfying certain syntactic properties) expressing a query constructs an extended logic program that expresses the expanded query. The extended logic program is referred to as the interpolation of the given general logic program.  1 Introduction and Motivation  In this paper we introduce the concept of interpolating general logic ...
487|Reasoning About Actual and Hypothetical Occurances of Concurrent and Non-Deterministic Actions|We propose extension L 2 of the action description language L 1 that can express both actual and hypothetical situations, concurrent execution of actions, observations of the truth values of fluents in these situations (as opposed to hypothetical values of fluents expressible in A and AC ), observations of actual occurrences of (possibly non-deterministic combination of) actions. The corresponding entailment relation formalizes various types of common-sense reasoning about actions and their effects not modeled by the previous approaches. We then present a translation of domain descriptions in L 2 to disjunctive logic programs. 1 Introduction  To perform nontrivial reasoning an intelligent agent situated in a changing domain needs the knowledge of causal laws that describe effects of actions changing the domain, the ability to observe and record occurrences of these actions, and the truth values of fluents  1  at particular moments of time. Discovery of methods of representing this kind...
488|Embedding Revision Programs in Logic Programming Situation Calculus|Revision programs were introduced by Marek and Truszczynski to specify change in knowledge bases. In this paper we show how to embed revision programs in logic programs with situation calculus notation. We extend Marek and Truszczynski&#039;s approach to allow incomplete initial knowledge base and extend the rules of revision programs to depend both on the initial and the final knowledge base. We show how revision programs and its proposed extension can be incorporated in theories of actions and how our usage of situation calculus notation makes this easier and elegant.  1 Introduction  Revision Programs were introduced by Marek and Truszczynski [MT94c, MT94a, MT94b] to specify revision/change in knowledge bases (databases) and belief sets. Unlike earlier approaches in belief revision where &#034;updates&#034;  1  were represented by classical theories, revision programs are a collection of rules, similar to rules in logic programs and have a non-classical semantics of &#034;/&#034;. Although revision program...
489|Relating the tms to autoepistemic logic|Truth maintenance systems have been studied by many authors and have become powerful tools in AI reasoning systems. From the viewpoint of commonsense reasoning, Doyle&#039;s TMS seems most interesting, for it allows nonmonotonic justifications. Its semantics, however, has remained unclear. In this paper, we shall give its declarative description in terms of autoepistemic logic, a kind of nonmonotonic logic. That is, we shall exhibit a one-to-one correspondence between states acceptable to the TMS and stable expansions of autoepistemic formulas attached to justifications. Thus, the TMS turns out to be a theorem prover of autoepistemic logic. For the practical interest, our result also suggests the possibility of implementing better TMS algorithms by using the theorem proving method of autoepistemic logic. 1
490|Planning Algorithms|This book presents a unified treatment of many different kinds of planning algorithms. The subject lies at the crossroads between robotics, control theory, artificial intelligence, algorithms, and computer graphics. The particular subjects covered include motion planning, discrete planning, planning under uncertainty, sensor-based planning, visibility, decision-theoretic planning, game theory, information spaces, reinforcement learning, nonlinear systems, trajectory planning, nonholonomic planning, and kinodynamic planning.
491|Speeding Up the Convergence of Value Iteration in Partially Observable Markov Decision Processes|Partially observable Markov decision processes (POMDPs) have recently become popular  among many AI researchers because they serve as a natural model for planning under  uncertainty. Value iteration is a well-known algorithm for finding optimal policies for  POMDPs. It typically takes a large number of iterations to converge. This paper proposes  a method for accelerating the convergence of value iteration. The method has been evaluated  on an array of benchmark problems and was found to be very effective: It enabled  value iteration to converge after only a few iterations on all the test problems.  1. Introduction  POMDPs model sequential decision making problems where effects of actions are nondeterministic and the state of the world is not known with certainty. They have attracted many researchers in Operations Research and Artificial Intelligence because of their potential applications in a wide range of areas (Monahan 1982, Cassandra 1998b), one of which is planning under uncertai...
492|An Improved Grid-Based Approximation Algorithm for POMDPs|Although a partially observable Markov decision  process (POMDP) provides an appealing model for  problems of planning under uncertainty, exact algorithms  for POMDPs are intractable. This motivates  work on approximation algorithms, and grid-based  approximation is a widely-used approach. We describe  a novel approach to grid-based approximation  that uses a variable-resolution regular grid, and  show that it outperforms previous grid-based approaches  to approximation.  1 
493|Continuous Motion Plans for Robotic Systems with Changing Dynamic Behavior|this paper is to address motion planning for systems in which the dynamic equations describing the evolution of the system change in different regions of the state space. We adopt the control theory point of view and focus on the planning of open loop trajectories that can be used as nominal inputs for control. Systems with changing dynamic behavior are characterized by: (a) equality and inequality constraints that partition the state space into regions (discrete states); and (b) trajectories that are governed by different dynamic equations as the system traverses different regions in the state space. The motion plan therefore consists of the sequence of regions (discrete states) as well as continuous trajectory (evolution of the continuous state) within each of the regions. Since the task may require that the system trajectories and the inputs are sufficiently smooth, we formulate the motion planning problem as an optimal control problem and achieve the smoothness by specifying an appropriate cost function. We present a formal framework for describing systems with changing dynamic behavior borrowing from the literature on hybrid systems. We formulate the optimal control problem for such systems, develop a novel technique for simplifying this problem when the sequence of discrete states is known, and suggest a numerical method for dealing with inequality constraints. The approach is illustrated with two examples. We first consider the coordination between mobile manipulators carrying an object while avoiding obstacles. We show that the obstacle avoidance translates to inequality constraints on the state and the input. In this task no changes in the dynamic equations occur since no physical interaction between the manipulators and the obstacles takes place. In our second...
494|Probabilistic models of dead-reckoning error in nonholonomic mobile robots|Abstract - In this paper, dead-reckoning error in mobile robots is studied in the context of several different models. These models are derived first in the form of stochastic differential equations (SDEs). Corresponding Fokker-Planck equations are derived, and desired probability density functions (PDFs) of robot pose are computed by using the Fourier transform for SE(2). I.
495|Stabilization of systems with changing dynamics by means of switching| We present a framework for designing stable control schemes for systems whose dynamics change. The idea is to develop a controller for each of the regions defined by different dynamic characteristics and design a switching scheme that guarantees the stability of the overall system. We derive sufficient conditions for the stability of the switching scheme for systems evolving on a sequence of embedded manifolds. An important feature of the proposed framework is that if the conditions are satisfied by pairs of controllers adjacent in the hierarchy, the overall system will be stable. This makes the application of our results particularly straight forward. The methodology is applied to stabilization of a shimmying wheel, where changes in the dynamic behavior are due to switches between sliding and rolling. 
496|RacerX: Effective, Static Detection of Race Conditions and Deadlocks|This paper describes RacerX, a static tool that uses flowsensitive, interprocedural analysis to detect both race conditions and deadlocks. It is explicitly designed to find errors in large, complex multithreaded systems. It aggressively infers checking information such as which locks protect which operations, which code contexts are multithreaded, and which shared accesses are dangerous. It tracks a set of code features which it uses to sort errors both from most to least severe. It uses novel techniques to counter the impact of analysis mistakes. The tool is fast, requiring between 2-14 minutes to analyze a 1.8 million line system. We have applied it to Linux, FreeBSD, and a large commercial code base, finding serious errors in all of them.
497|Compositional Model Checking|We describe a method for reducing the complexity of temporal logic model checking in systems composed of many parallel processes. The goal is to check properties of the components of a system and then deduce global properties from these local properties. The main difficulty with this type of approach is that local properties are often not preserved at the global level. We present a general framework for using additional interface processes to model the environment for a component. These interface processes are typically much simpler than the full environment of the component. By composing a component with its interface processes and then checking properties of this composition, we can guarantee that these properties will be preserved at the global level. We give two example compositional systems based on the logic CTL*.
498|Interprocedural dataflow analysis via graph reachability|The paper shows how a large class of interprocedural dataflow-analysis problems can be solved precisely in poly-nomial time by transforming them into a special kind of graph-reachability problem. The only restrictions are that the set of dataflow facts must be a finite set, and that the dataflow functions must distribute over the confluence operator (either union or intersection). This class of prob-lems includes—but is not limited to—the classical separ-able problems (also known as “gen/kill ” or “bit-vector” problems)—e.g., reaching definitions, available expres-sions, and live variables. In addition, the class of problems that our techniques handle includes many non-separable problems, including truly-live variables, copy constant pro-pagation, and possibly-uninitialized variables. Results are reported from a preliminary experimental study of C programs (for the problem of finding possibly-uninitialized variables). 1.
499|A System and Language for Building System-Specific, Static Analyses|This paper presents a novel approach to bug-finding analysis and an implementation of that approach. Our goal is to find as many serious bugs as possible. To do so, we designed a flexible, easy-to-use extension language for specifying analyses and an efficent algorithm for executing these extensions. The language, metal, allows the users of our system to specify a broad class of analyses in terms that resemble the intuitive description of the rules that they check. The system, xgcc, executes these analyses efficiently using a context-sensitive, interprocedural analysis.
500|Type-safe multithreading in Cyclone|We extend Cyclone, a type-safe polymorphic language at the C level of abstraction, with threads and locks. Data races can violate type safety in Cyclone. An extended type system statically guarantees their absence by enforcing that thread-shared data is protected via locking and that threadlocal data does not escape the thread that creates it. The extensions interact smoothly with parametric polymorphism and region-based memory management. We present a formal abstract machine that models the need to prevent races, a polymorphic type system for the machine that supports thread-local data, and a corresponding type-safety result.
501|Detecting Data Races in Cilk Programs  that Use Locks|When two parallel threads holding no locks in common access the same memory location and at least one of the threads modifies the location, a “data race ” occurs, which is usually a bug. This paper describes the algorithms and strategies used by a debugging tool, called the Nondeterminator-2, which checks for data races in pro-grams coded in the Cilk multithreaded language. Like its predeces-sor, the Nondeterminator, which checks for simple “determinacy” races, the Nondeterminator-2 is a debugging tool, not a verifier, since it checks for data races only in the computation generated by a serial execution of the program on a given input. We give an algorithm, ALL-SETS, that determines whether the computation generated by a serial execution of a Cilk program on a given input contains a race. For a program that runs serially in time T, accesses V shared memory locations, uses a total of n locks, and holds at most k &lt;&lt; n locks simultaneously, ALL-SETS runs in
502|Path-Sensitive Program Verification in Polynomial Time|In this paper, we present a new algorithm for program verification that runs in polynomial time and space. We are interested in checking that a program satisfies a given temporal safety property. Our insight is that by accurately modeling only those branches in a program for which the propertyrelated behaviour of the program differs along the arms of the branch, we can design an algorithm that is accurate enough for verification without paying the exponential cost of full path-sensitive analysis.
503|Finding Stale-Value Errors in Concurrent Programs|Concurrent programs can suffer from many types of errors, not just the wellstudied  problems of deadlocks and simple race conditions on variables. This paper  addresses a kind of race condition that arises from reading a variable whose  value is possibly out-of-date. The paper introduces a simple technique for detecting  such stale values, and reports on the encouraging experience with a compile-time  checker that uses the technique.
504|A Taxonomy of Race Detection Algorithms.|This paper presents a taxonomy that categorizes methods for determining event
505|The implementation of the cilk-5 multithreaded language|The fth release of the multithreaded language Cilk uses a provably good \work-stealing &#034; scheduling algorithm similar to the rst system, but the language has been completely re-designed and the runtime system completely reengineered. The eciency of the new implementation was aided by a clear strategy that arose from a theoretical analysis of the scheduling algorithm: concentrate on minimizing overheads that contribute to the work, even at the expense of overheads that contribute to the critical path. Although it may seem counterintuitive to move overheads onto the critical path, this \work-rst &#034; principle has led to a portable Cilk-5 im-plementation in which the typical cost of spawning a parallel thread is only between 2 and 6 times the cost of a C function call on a variety of contemporary machines. Many Cilk pro-grams run on one processor with virtually no degradation compared to equivalent C programs. This paper describes how the work-rst principle was exploited in the design of Cilk-5&#039;s compiler and its runtime system. In particular, we present Cilk-5&#039;s novel \two-clone &#034; compilation strategy and its Dijkstra-like mutual-exclusion protocol for implementing the ready deque in the work-stealing scheduler.
506|Scheduling Multithreaded Computations by Work Stealing|This paper studies the problem of efficiently scheduling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is “work stealing,&#034; in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies. Specifically, our analysis shows that the ezpected time Tp to execute a fully strict computation on P processors using our work-stealing scheduler is Tp = O(TI/P + Tm), where TI is the minimum serial eze-cution time of the multithreaded computation and T, is the minimum ezecution time with an infinite number of processors. Moreover, the space Sp required by the execution satisfies Sp 5 SIP. We also show that the ezpected total communication of the algorithm is at most O(TmS,,,P), where S, is the site of the largest activation record of any thread, thereby justify-ing the folk wisdom that work-stealing schedulers are more communication eficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.
508|The parallel evaluation of general arithmetic expressions|ABSTRACT. It is shown that arithmetic expressions with n&gt; 1 variables and constants; operations of addition, multiplication, and division; and any depth of parenthesis nesting can be evaluated in time 4 log2n + 10(n- 1)/p using p&gt; 1 processors which can independently perform arithmetic operations in unit time. This bound is within a constant factor of the best possible. A sharper result is given for expressions without the division operation, and the question of numerical stability is discussed. KEY WORDS AND PHRASES: arithmetic expressions, compilation of arithmetic expressions, compu-tational complexity, general arithmetic expressions, numerical stability, parallel computatioR,
509|Lazy Task Creation: A Technique for Increasing the Granularity of Parallel Programs|Many parallel algorithms are naturally expressed at a fine level of granularity, often finer than a MIMD parallel system can exploit efficiently. Most builders of parallel systems have looked to either the programmer or a parallelizing compiler to increase the granularity of such algorithms. In this paper we explore a third approach to the granularity problem by analyzing two strategies for combining parallel tasks dynamically at run-time. We reject the simpler load-based inlining method, where tasks are combined based on dynamic load level, in favor of the safer and more robust lazy task creation method, where tasks are created only retroactively as processing resources become available. These strategies grew out of work on Mul-T [15], an efficient parallel implementation of Scheme, but could be used with other languages as well. We describe our Mul-T implementations of lazy task creation for two contrasting machines, and present performance statistics which show the method&#039;s effectiveness. Lazy task creation allows efficient execution of naturally expressed algorithms of a substantially finer grain than possible with previous parallel Lisp systems. 
510|Thread scheduling for multiprogrammed multiprocessors|We present a user-level thread scheduler for shared-memory multiprocessors, and we analyze its performance under multiprogramming. We model multiprogramming with two scheduling levels: our scheduler runs at user-level and schedules threads onto a fixed collection of processes, while below, the operating system kernel schedules processes onto a fixed collection of processors. We consider the kernel to be an adversary, and our goal is to schedule threads onto processes such that we make efficient use of whatever processor resources are provided by the kernel. Our thread scheduler is a non-blocking implementation of the work-stealing algorithm. For any multithreaded computation with work ¢¤ £ and critical-path length ¢¦ ¥ , and for any number § of processes, our scheduler executes the computation in expected time ¨?©?¢?£???§¤????¢?¥?§???§¤?? ? , where §? ? is the average number of processors allocated to the computation by the kernel. This time bound is optimal to within a constant factor, and achieves linear speedup whenever § is small relative to the parallelism 1
511|Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine|Abstract: In this paper, we present a relatively primitive execution model for ne-grain parallelism, in which all synchronization, scheduling, and storage management is explicit and under compiler control. This is de ned by a threaded abstract machine (TAM) with a multilevel scheduling hierarchy. Considerable temporal locality of logically related threads is demonstrated, providing an avenue for e ective register use under quasi-dynamic scheduling. A prototype TAM instruction set, TL0, has been developed, along with a translator to a variety of existing sequential and parallel machines. Compilation of Id, an extended functional language requiring ne-grain synchronization, under this model yields performance approaching that of conventional languages on current uniprocessors. Measurements suggest that the net cost of synchronization on conventional multiprocessors can be reduced to within a small factor of that on machines with elaborate hardware support, such as proposed data ow architectures. This brings into question whether tolerance to latency and inexpensive synchronization require speci c hardware support or merely an appropriate compilation strategy and program representation. 1
513|Efficient detection of determinacy races in Cilk programs|A parallel multithreaded program that is ostensibly deterministic
514|Lazy Threads: Implementing a Fast Parallel Call|In this paper we describe lazy threads, a new approach for implementing multi-threaded execution models on conventional machines. We show how they can implement a parallel call at nearly the efficiency of a sequential call. The central idea is to specialize the representation of a parallel call so that it can execute as a parallel-ready sequential call. This allows excess parallelism to degrade into sequential calls with the attendant efficient stack management and direct transfer of control and data, yet a call that truly needs to execute in parallel, gets its own thread of control. The efficiency of lazy threads is achieved through a careful attention to storage management and a code generation strategy that allows us to represent potential parallel work with no overhead.
515|The Cilk System for Parallel Multithreaded Computing|Although cost-effective parallel machines are now commercially available, the widespread use of parallel processing is still being held back, due mainly to the troublesome nature of parallel programming. In particular, it is still diiticult to build eiticient implementations of parallel applications whose communication patterns are either highly irregular or dependent upon dynamic information. Multithreading has become an increasingly popular way to implement these dynamic, asynchronous, concurrent programs. Cilk (pronounced &#034;silk&#034;) is our C-based multithreaded computing system that provides provably good performance guarantees. This thesis describes the evolution of the Cilk language and runtime system, and describes applications which affected the evolution of the system.
516|Polling Efficiently on Stock Hardware|Two strategies for supporting asynchronous interrupts are: the use of the processor&#039;s hardware interrupt system and the use of polling. The advantages of polling include: portability, simplicity, and low cost for handling interrupts. Unfortunately, polling has an overhead for the explicit interrupt checks inserted in the code. This paper describes balanced polling , a method for placing the interrupt checks which has a low overhead and also guarantees an upper bound on interrupt latency. This method has been used by Gambit (an optimizing native code compiler for Scheme) to support a number of features including multiprocessing and stack overflow detection. The overhead of balanced polling is less than for call-return polling which places interrupt checks at every procedure entry and exit. The overhead of call-return polling is typically 70% larger (but sometimes over 400% larger) than the overhead of balanced polling.  1 Introduction  In this paper, the term interrupt is defined as an ...
517|Whole-Program Optimization for Time and Space Efficient Threads|Modern languages and operating systems often encourage programmers to use threads, or independent control streams, to mask the overhead of some operations and simplify program structure. Multitasking operating systemsuse threads to mask communication latency, either with hardwares devices or users. Client-server applications typically use threads to simplify the complex controlflow that arises when multiple clients are used. Recently, the scientific computing community has started using threads to mask network communication latency in massively parallel architectures, allowing computation and communication to be overlapped. Lastly, some architectures implement threads in hardware, using those threads to tolerate memory latency. In general, it would be desirable if threaded programs could be written to expose the largest degree of parallelism possible, or to simplify the program design. However, threads incur time and space overheads, and programmers often compromise simple designs for ...
518|Deadlock|checking by a behavioral effect system for lock handling
519|Perspectives on Program Analysis|eing analysed. On the negative side, the semantic correctness of the analysis is seldom established and therefore there is often no formal justification for the program transformations for which the information is used.  The semantics based approach [1; 5] is often based on domain theory in the form of abstract domains modelling sets of values, projections, or partial equivalence relations. The approach tends to focus more directly on discovering the extensional properties of interest: for constant propagation it might operate on sets of values with constancy corresponding to singletons, and for neededness analysis it might perform a strictness analysis and use the strictness information for neededness (or make use of the &#034;absence&#034; notion from projection analysis and attempt to discover the di#erence). On the positive side, this usually gives rise to provably correct analyses, although there are sometimes complications (due to deciding what information to stick onto the
521|System Deadlocks|A problem of increasing importance in the design of large multiprogramming systems is the, so-called, deadlock or deadly-embrace problem. In this arliele we survey the work that has been done on the treatment of deadlocks from bolh the theoretical and practical points of view.
522|Ownership Types for Safe Region-Based Memory Management in Real-Time Java|The Real-Time Specification for Java (RTSJ) allows a program to create real-time threads with hard real-time constraints. Real-time threads use region-based memory management to avoid unbounded pauses caused by interference from the garbage collector. The RTSJ uses runtime checks to ensure that deleting a region does not create dangling references and that real-time threads do not access references to objects allocated in the garbage-collected heap. This paper presents a static type system that guarantees that these runtime checks will never fail for well-typed programs. Our type system therefore 1) provides an important safety guarantee for real-time programs and 2) makes it possible to eliminate the runtime checks and their associated overhead. Our system also makes several contributions over previous work on region types. For object-oriented programs, it combines the benefits of region types and ownership types in a unified type system framework. For multithreaded programs, it allows long-lived threads to share objects without using the heap and without memory leaks. For real-time programs, it ensures that real-time threads do not interfere with the garbage collector. Our experience indicates that our type system is suciently expressive and requires little programming overhead, and that eliminating the RTSJ runtime checks using a static type system can significantly decrease the execution time of real-time programs.
523|Conditional must not aliasing for static race detection|Abstract Race detection algorithms for multi-threaded programs using thecommon lock-based synchronization idiom must correlate locks with the memory locations they guard. The heart of a proof ofrace freedom is showing that if two locks are distinct, then the memory locations they guard are also distinct. This is an exampleof a general property we call conditional must not aliasing: Under the assumption that two objects are not aliased, prove that twoother objects are not aliased. This paper introduces and gives an algorithm for conditional must not alias analysis and discussesexperimental results for sound race detection of Java programs.
524|Reasoning about threads communicating via locks|Abstract. We propose a new technique for the static analysis of concurrent programs comprised of multiple threads. In general, the problem is known to be undecidable even for programs with only two threads but where the threads communicate using CCS-style pairwise rendezvous [10]. However, in practice, a large fraction of concurrent programs can either be directly modeled as threads communicating solely using locks or can be reduced to such systems either by applying standard abstract interpretation techniques or by exploiting separation of control from data. For such a framework, we show that for the commonly occurring case of threads with nested access to locks, the problem is efficiently decidable. Our technique involves reducing the analysis of a concurrent program with multiple threads to individually analyzing augmented versions of the given threads. Thus not only yields decidability but also avoids construction of the state space of the concurrent program at hand and thus bypasses the state explosion problem making our technique scalable. We go on to show that for programs with threads that have non-nested access to locks, the static analysis problem for programs with even two threads becomes undecidable even for reachability, thus sharpening the result of [10]. As a case study, we consider the Daisy file system [1] which is a benchmark for analyzing the efficacy of different methodologies for debugging concurrent programs and show the existence of several bugs. 1
525|A new type system for deadlock-free processes|Abstract. We extend a previous type system for the p-calculus that guarantees deadlock-freedom. The previous type systems for deadlockfreedom either lacked a reasonable type inference algorithm or were not strong enough to ensure deadlock-freedom of processes using recursion. Although the extension is fairly simple, the new type system admits type inference and is much more expressive than the previous type systems that admit type inference. In fact, we show that the simply-typed ?calculus with recursion can be encoded into the deadlock-free fragment of our typed p-calculus. To enable analysis of realistic programs, we also present an extension of the type system to handle recursive data structures like lists. Both extensions have already been incorporated into the recent release of TyPiCal, a type-based analyzer for the p-calculus. 1
526|Refinement of Actions and Equivalence Notions for Concurrent Systems|This paper combines and extends the material of [GG-a/c/d/e], except for the part in [GG-c] on refinement of transitions in Petri nets and the discussion of TCSP-like parallel composition in [GG-e]. An informal presentation of some basic ingredients of this paper appeared as [GG-b]. Among others, the treatment of action refinement in stable and non-stable event structures is new. The research reported here was supported by Esprit project 432 (METEOR), Esprit Basic Research Action 3148 (DEMON), Sonderforschungsbereich 342 of the TU Munchen, ONR grant N00014-92-J-1974 and the Human Capital and Mobility Cooperation Network EXPRESS (Expressiveness of Languages for Concurrency).  Contents
527|Object types against races|Abstract. This paper investigates an approach for statically preventing race conditions in an object-oriented language. The setting of this work is a variant of Gordon and Hankin&#039;s concurrent object calculus. We enrich that calculus with a form of dependent object types that enables us to verify that threads invoke and update methods only after acquiring appropriate locks. We establish that well-typed programs do not have race conditions. 1 Introduction Concurrent object-oriented programs suffer from many of the errors common inconcurrent programs of other sorts. In particular, the use of objects does not diminish the importance of careful synchronization. With objects or withoutthem, improper synchronization may lead to race conditions (that is, two processes accessing a shared resource simultaneously) and ultimately to incorrectbehavior. A standard approach for eliminating race conditions consists in protectingeach shared resource with a lock, requiring that a process acquires the corresponding lock before using the resource [5]. Object-oriented programs oftenrely on this approach, but with some peculiar patterns. It is common to group related resources into an object, and to attach the lock that protects the re-sources to this object. Processes may acquire the lock before invoking the methods of the object; alternatively, the methods may acquire this lock at the start oftheir execution. With constructs such as Java&#039;s
528|Type-Based Information Flow Analysis for the Pi-Calculus|We propose a new type system for information flow analysis for the
529|Static deadlock detection for Java libraries|1 Introduction Deadlock is a condition under which the progress of a program is halted as eachthread in a set attempts to acquire a lock already held by another thread in the set. Because deadlock prevents an entire program from working, it is a seriousproblem.
530|An Implicitly-Typed Deadlock-Free Process Calculus|We extend Kobayashi and Sumii&#039;s type system for the deadlock -free #-calculus and develop a type reconstruction algorithm. Kobayashi and Sumii&#039;s type system helps high-level reasoning about concurrent programs by guaranteeing that communication on certain channels will eventually succeed. It can ensure, for example, that a process implementing a function really behaves like a function. However, because it lacked a type reconstruction algorithm and required rather complicated type annotations, applying it to real concurrent languages was impractical. We have therefore developed a type reconstruction algorithm for an extension of the type system. The key novelties that made it possible are generalization of usages (which specifies how each communication channel is used) and a subusage relation. 1 
531|Checking race freedom via linear programming |We present a new static analysis for race freedom and race detection. The analysis checks race freedom by reducing the problem to (rational) linear programming. Unlike conventional static analyses for race freedom or race detection, our analysis avoids explicit computation of locksets and lock linearity/must-aliasness. Our analysis can handle a variety of synchronization idioms that more conventional approaches often have difficulties with, such as thread joining, semaphores, and signals. We achieve efficiency by utilizing modern linear programming solvers that can quickly solve large linear programming instances. This paper reports on the formal properties of the analysis and the experience with applying an implementation to real world C programs.
532|Symbolic Data Flow Analysis for Detecting Deadlocks in Ada Tasking Programs|It is well accepted that designing and analyzing concurrent  software-components are tedious tasks. Assuring the quality of such software  requires formal methods, which can statically detect deadlocks. This paper
533|Searching for Deadlocks while Debugging Concurrent Haskell Programs|This paper presents an approach to searching for deadlocks in Concurrent Haskell programs. The search is based on a redefinition of the IO monad which allows the reversal of Concurrent Haskells concurrency primitives. Hence, it is possible to implement this search by a backtracking algorithm checking all possible schedules of the system. It is integrated in the Concurrent Haskell Debugger (CHD), and automatically searches for deadlocks in the background while debugging. The tool is easy to use and the small modifications of the source program are done by a preprocessor. In the tool we use iterative deepening as search strategy which quickly detects deadlocks close to the actual system configuration and utilizes idle time during debugging at the best.
534|Static Deadlock Prevention in Dynamically Configured Communication Networks |We propose a technique to avoid deadlocks in a system of communicating processes. Our network model is very general. It supports dynamic process and channel creation and the ability to send channel endpoints over channels, thereby allowing arbitrary dynamically configured networks. Deadlocks happen in such networks if there is a cycle created by a set of channels, and processes along the cycle circularly wait for messages from each other. Our approach allows cycles of channels to be created, but avoids circular waiting by ensuring that for every cycle C, some process P breaks circular waits by selecting to communicate on both endpoints involved in the cycle C at P. We formalize this strategy as a calculus with a type system. Our type system keeps track of markers called obstructions where wait cycles are intended to be broken. Programmers annotate message types with design decisions on how obstructions are managed. Using these annotations, our type checker works modularly and independently on each process, without suffering any state space explosion. We prove the soundness of the analysis (namely deadlock freedom) on a simple but realistic language that captures the essence of such communication networks. We also describe how the technique can be applied to a substantial example.
535|On kinetic waves: II) A theory of traffic Flow on long crowded roads|This paper uses the method of kinematic waves, developed in part I, but may be read independently. A functional relationship between flow and concentration for traffic on crowded arterial roads has been postulated for some time, and has experimental backing (? 2). From this a theory of the propagation of changes in traffic distribution along these roads may be deduced (??2, 3). The theory is applied (?4) to the problem of estimating how a &#039;hump&#039;, or region of increased concentration, will move along a crowded main road. It is suggested that it will move slightly slower than the mean vehicle speed, and that vehicles passing through it will have to reduce speed rather suddenly (at a &#039;shock wave&#039;) on entering it, but can increase speed again only very gradually as they leave it. The hump gradually spreads out along the road, and the time scale of this process is estimated. The behaviour of such a hump on entering a bottleneck, which is too narrow to admit the increased flow, is studied (?5), and methods are obtained for estimating the extent and duration of the resulting hold-up. The theory is applicable principally to traffic behaviour over a long stretch of road, but the paper concludes (? 6) with a discussion of its relevance to problems of flow near junctions, including a discussion of the starting flow at a controlled junction. In the introductory sections 1 and 2, we have included some elementary material on the quantitative study of traffic flow for the benefit of scientific readers unfamiliar with the subject. 1.
536|STATEMATE: A Working Environment for the Development of Complex Reactive Systems|This paper provides an overview of the STATEMATE  system, constructed over the past several years by the authors and their colleagues at Ad Cad Ltd., the R&amp;D subsidiary of i-Logix, Inc. STATEMATE is a set of tools, with a heavy graphical orientation, in- tended for the specification, analysis, design, and documentation of large and complex reactive systems, such as real-time embedded sys- tems, control and communication systems, and interactive software or hardware. It enables a user to prepare, analyze, and debug diagram- matic, yet precise, descriptions of the system under development from three interrelated points of view, capturing structure, functionality, and behavior. These views are represented by three graphical languages, the most intricate of which is the language of statecharts [4], used to depict reactive behavior over time. In addition to the use of statecharts, the main novelty of STATEMATE is in the fact that it &#034;understands &#034; the entire descriptions perfectly, to the point of being able to analyze them for crucial dynamic properties, to carry out rigorous ex- ecutions and simulations of the described system, and to create run- ning code automatically. These features are invaluable when it comes to the quality and reliability of the final outcome.
537|Structure and Complexity of Relational Queries|This paper is an attempt at laying the foundations for the classification of queries on  relational data bases according to their structure and their computational complexity. Using  the operations of composition and fixpoints, a Z--// hierarchy of height w 2, called the  fixpoint query hierarchy, is defined, and its properties investigated. The hierarchy includes  most of the queries considered in the literathre including those of Codd and Aho and Ullman
538|Fusion, Propagation, and Structuring in Belief Networks|Belief networks are directed acyclic graphs in which the nodes represent propositions (or variables), the arcs signify direct dependencies between the linked propositions, and the strengths of these dependencies are quantified by conditional probabilities. A network of this sort can be used to represent the generic knowledge of a domain expert, and it turns into a computational architecture if the links are used not merely for storing factual knowledge but also for directing and activating the data flow in the computations which manipulate this knowledge. The first part of the paper deals with the task of fusing and propagating the impacts of new information through the networks in such a way that, when equilibrium is reached, each proposition will be assigned a measure of belief consistent with the axioms of probability theory. It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network. The second part of the paper deals with the problem of finding a tree-structured representation for a collection of probabilistically coupled propositions using auxiliary (dummy) variables, colloquially called &#034;hidden causes. &#034; It is shown that if such a tree-structured representation exists, then it is possible to uniquely uncover the topology of the tree by observing pairwise dependencies among the available propositions (i.e., the leaves of the tree). The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to n log n, where n is the number of leaves.  
539|Approximating discrete probability distributions with dependence trees|A method is presented to approximate optimally an n-dimensional discrete probability distribution by a product of second-order distributions, or the distribution of the first-order tree dependence. The problem is to find an optimum set of n-1 first order dependence relationship among the n variables. It is shown that the procedure derived in this paper yields an approximation of a minimum difference in information. It is further shown that when this procedure is applied to empirical observations from an unknown distribution of tree dependence, the procedure is the maximum-likelihood estimate of the distribution.
540|A sufficient condition for backtrack-free search| A constraint satisfaction problem revolves finding values for a set of variables subject of a set of constraints (relations) on those variables Backtrack search is often used to solve such problems. A relationship involving the structure of the constraints i described which characterizes tosome degree the extreme case of mimmum backtracking (none) The relationship involves a concept called &#034;width,&#034; which may provide some guidance in the representation f constraint satisfaction problems and the order m which they are searched The width concept is studied and applied, in particular, to constraints which form tree structures.
541|An interactive activation model of context effects in letter perception: Part 2. The contextual enhancement effect and some tests and extensions of the model|The interactive activation model of cor. text effects in letter perception is reviewed, elaborated, and tested. According to the model context aids the perception of target letters as they are processed in the perceptual system. The implication that the duration and timing of the context in which a letter occurs should greatly influence the perceptibility of the target is confirmed by a series of experiments demonstrating that early or enhanced presentations of word and pronounceablepseudoword contexts greatly increase the perceptibility of target letters. Also according to the model, letters in strings that share several letters with words should be equally perceptible whether they are orthographically regular and pronounceable (SLET) or irregular (SLNT) and should be much more perceptible than letters in contexts that share few letters with any word (XLQJ). This prediction is tested and confirmed. The basic results of all the experiments are accounted for, with some modification of parameters, although there are some discrepancies in detail. Several recent findings that seem to challenge the model are considered and a number of extensions are proposed.
542|Reverend Bayes on inference engines: a distributed hierarchical approach|This paper presents generalizations of Bayes likelihood-ratio updating rule which facilitate an asynchronous propagation of the impacts of new beliefs and/or new evidence in hierarchically organized inference structures with multi-hypotheses variables. The computational scheme proposed specifies a set of belief parameters, communication messages and updating rules which guarantee that the diffusion of updated beliefs is accomplished in a single pass and complies with the tenets of Bayes calculus.
543|Truth Maintenance|General purpose truth maintenance systems have received considerable attention in the past few years. This paper discusses the functionality of truth maintenance systems and compares various existing algorithms. Applications and directions for future research are also discussed. Introduction  In 1978 Jon Doyle wrote a masters thesis at the MIT AI Laboratory entitled &#034;Truth Maintenance Systems for Problem Solving&#034; [ Doyle, 1979 ] . In this thesis Doyle described an independent module called a truth maintenance system, or TMS, which maintained beliefs for general problem solving systems. In the twelve years since the appearance of Doyle&#039;s TMS a large body of literature has accumulated on truth maintenance. The seminal idea appears not to have been any particular technical mechanism but rather the general concept of an independent module for truth (or belief) maintenance. All truth maintenance systems manipulate proposition symbols and relationships between proposition symbols. I will use...
544|The anatomy of easy problems: a constraint-satisfaction formulation|This work aims towards the automatic generation of advice to guide the solution of difficult constraintsatisfaction problems (CSPs). The advice is generated by consulting relaxed, easy models which are backtrackfree. We identify a subset of CSPs whose syntactic and semantic properties make them easy to solve. The syntactic properties involve the structure of the constraint graph, while the semantic properties guarantee some local consistencies among the constraints. In particular, problems supported by tree-like constraint graphs, and some width-2 graphs, can be easily solved and are therefore chosen as the target model for the relaxation scheme. Optimal algorithms for solving easy problems are presented and analyzed. Finally, an efficient method is introduced for extracting advice from easy problems and using it to speedup the solution of hard problems. I
545|Shared memory consistency models: A tutorial|Parallel systems that support the shared memory abstraction are becoming widely accepted in many areas of computing. Writing correct and efficient programs for such systems requires a formal specification of memory semantics, called a memory consistency model. The most intuitive model—sequential consistency—greatly restricts the use of many performance optimizations commonly used by uniprocessor hardware and compiler designers, thereby reducing the benefit of using a multiprocessor. To alleviate this problem, many current multiprocessors support more relaxed consistency models. Unfortunately, the models supported by various systems differ from each other in subtle yet important ways. Furthermore, precisely defining the semantics of each model often leads to complex specifications that are difficult to understand for typical users and builders of computer systems. The purpose of this tutorial paper is to describe issues related to memory consistency models in a way that would be understandable to most computer professionals. We focus on consistency models proposed for hardware-based shared-memory systems. Many of these models are originally specified with an emphasis on the system optimizations they allow. We retain the system-centric emphasis, but use uniform and simple terminology to describe the different models. We also briefly discuss an alternate programmer-centric view that describes the models in terms of program behavior rather than specific system optimizations. 1
546|Dynamic Class Loading in the Java Virtual Machine|Class loaders are a powerful mechanism for dynamically loading software components on the Java platform. They are unusual in supporting all of the following features: laziness, type-safe linkage, user-defined extensibility, and multiple communicating namespaces.  We present the notion of class loaders and demonstrate some of their interesting uses. In addition, we discuss how to maintain type safety in the presence of user-defined dynamic class loading.  1 Introduction  In this paper, we investigate an important feature of the Java virtual machine: dynamic class loading. This is the underlying mechanism that provides much of the power of the Java platform: the ability to install software components at runtime. An example of a component is an applet that is downloaded into a web browser.  While many other systems [16] [13] also support some form of dynamic loading and linking, the Java platform is the only system we know of that incorporates all of the following features:  1. Lazy loadi...
547|Obstruction-free synchronization: Double-ended queues as an example|We introduce obstruction-freedom, a new nonblocking property for shared data structure implementations. This property is strong enough to avoid the problems associated with locks, but it is weaker than previous nonblocking properties—specifically lock-freedom and wait-freedom— allowing greater flexibility in the design of efficient implementations. Obstruction-freedom admits substantially simpler implementations, and we believe that in practice it provides the benefits of wait-free and lock-free implementations. To illustrate the benefits of obstruction-freedom, we present two obstruction-free CAS-based implementations of double-ended queues (deques); the first is implemented on a linear array, the second on a circular array. To our knowledge, all previous nonblocking deque implementations are based on unrealistic assumptions about hardware support for synchronization, have restricted functionality, or have operations that interfere with operations at the opposite end of the deque even when the deque has many elements in it. Our obstruction-free implementations have none of these drawbacks, and thus suggest that it is much easier to design obstruction-free implementations than lock-free and waitfree ones. We also briefly discuss other obstruction-free data structures and operations that we have implemented. 1.
548|A Practical Multi-Word Compare-and-Swap Operation|Work on non-blocking data structures has proposed extending processor designs with a compare-and-swap primitive, CAS2, which acts on two arbitrary memory locations. Experience suggested that current operations, typically single-word compare-and-swap (CAS1), are not expressive enough to be used alone in an efficient manner. In this paper we build CAS2 from CAS1 and, in fact, build an arbitrary multi-word compare-and-swap (CASN). Our design requires only the primitives available on contemporary systems, reserves a small and constant amount of space in each word updated (either 0 or 2 bits) and permits nonoverlapping updates to occur concurrently. This provides compelling evidence that current primitives are not only universal in the theoretical sense introduced by Herlihy, but are also universal in their use as foundations for practical algorithms. This provides a straightforward mechanism for deploying many of the interesting non-blocking data structures presented in the literature that have previously required CAS2.
549|Distributed processes: A concurrent programming concept|A language concept for concurrent processes without common variables is introduced. These processes communicate and synchronize by means of procedure calls and guarded regions. This concept is proposed for real-time applications controlled by microcomputer networks with distributed storage. The paper gives several examples of distributed processes and shows that they include
550|The repeat offender problem: A mechanism for supporting dynamic-sized, lock-free data structures|We define the Repeat Offender Problem (ROP). Elsewhere, we have presented the first dynamic-sized lock-free data structures that can free memory to any standard memory allocator—even after thread failures—without requiring special support from the operating system, the memory allocator, or the hardware. These results depend on a solution to the ROP problem. Here we present the first solution to the ROP problem and its correctness proof. Our solution is implementable in most modern shared memory multiprocessors. M/S MTV29-01
551|Transparent Support for Wait-Free Transactions|. This paper concerns software support for non-blocking transactions in shared-memory multiprocessors. We present mechanisms that convert sequential transactions into lock-free or wait-free ones. In contrast to some previous mechanisms, ours support transactions for which the set of memory locations accessed cannot be determined in advance. Our implementations automatically detect and resolve conflicts between concurrent transactions, and allow transactions that do not conflict to execute in parallel. The key to the efficiency of our wait-free implementation lies in using a lock-free (but not wait-free) multi-word compareand -swap (MWCAS) operation. By introducing communication between a high-level helping mechanism and the lock-free MWCAS, we show that an expensive wait-free MWCAS is not necessary to ensure wait-freedom. 1 Introduction  The use of locking to coordinate accesses to shared data in multiprocessor applications has a number of associated pitfalls including a lack of concur...
552|Correction of a memory management method for lock-free data structures|Memory reuse in link-based lock-free data structures requires special care. Many lock-free algorithms require deleted nodes not to be reused until no active pointers point to them. Also, most lock-free algorithms use the compare and swap atomic primitive, which can suffer from the “ABA problem ” [1] associated with memory reuse. Valois [3] proposed a memory management method for link-based data structures that addresses these problems. The method associates a reference count with each node of reusable memory. A node is reused only when no processes or data structures point to it. The method solves the ABA problem for acyclic link-based data structures, and allows lock-free algorithms more flexibility as nodes are not required to be freed immediately after a delete operation (e.g. dequeue, pop, delete min, etc.). However, there are race conditions that may corrupt data structure that use this method. In this report we correct these race conditions and present a corrected version of Valois’s method.
553|Design Choices For Language-Based Transactions|This report discusses two design choices which arose in our recent work on introducing a new atomic keyword as an extension to the Java programming language. We discuss the extent to which programs using atomic blocks should be provided with an explicit `abort&#039; operation to roll-back the e#ects of the current block. We also discuss mechanisms for supporting blocks that perform I/O operations or external database transactions.
554|Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism|Threads are the vehicle,for concurrency in many approaches to parallel programming. Threads separate the notion of a sequential execution stream from the other aspects of traditional UNIX-like processes, such as address spaces and I/O descriptors. The objective of this separation is to make the expression and control of parallelism sufficiently cheap that the programmer or compiler can exploit even fine-grained parallelism with acceptable overhead. Threads can be supported either by the operating system kernel or by user-level library code in the application address space, but neither approach has been fully satisfactory. This paper addresses this dilemma. First, we argue that the performance of kernel threads is inherently worse than that of user-level threads, rather than this being an artifact of existing implementations; we thus argue that managing par- allelism at the user level is essential to high-performance parallel computing. Next, we argue that the lack of system integration exhibited by user-level threads is a consequence of the lack of kernel support for user-level threads provided by contemporary multiprocessor operating systems; we thus argue that kernel threads or processes, as currently conceived, are the wrong abstraction on which to support user- level management of parallelism. Finally, we describe the design, implementation, and performance of a new kernel interface and user-level thread package that together provide the same functionality as kernel threads without compromis- ing the performance and flexibility advantages of user-level management of parallelism.
555|MULTILISP: a language for concurrent symbolic computation|Multilisp is a version of the Lisp dialect Scheme extended with constructs for parallel execution. Like Scheme, Multilisp is oriented toward symbolic computation. Unlike some parallel programming languages, Multilisp incorporates constructs for causing side effects and for explicitly introducing parallelism. The potential complexity of dealing with side effects in a parallel context is mitigated by the nature of the parallelism constructs and by support for abstract data types: a recommended Multilisp programming style is presented which, if followed, should lead to highly parallel, easily understandable programs. Multilisp is being implemented on the 32-processor Concert multiprocessor; however, it is ulti-mately intended for use on larger multiprocessors. The current implementation, called Concert Multilisp, is complete enough to run the Multilisp compiler itself and has been run on Concert prototypes including up to eight processors. Concert Multilisp uses novel techniques for task scheduling and garbage collection. The task scheduler helps control excessive resource utilization by means of an unfair scheduling policy; the garbage collector uses a multiprocessor algorithm based on the incremental garbage collector of Baker.
556|The v distributed system|The V distributed System was developed at Stanford University as part of a research project to explore issues in distributed systems. Aspects of the design suggest important directions for the design of future operating systems and communication systems. 
557|The Amber System: Parallel Programming on a Network of Multiprocessors|Microprocessor-based shared-memory multiprocessors are becoming widely available and promise to provide cost-effective high-performance computing. This paper describes a programming system called Amber which permits a single application program to use a homogeneous network of multiprocessors in a uniform way, making the network appear to the application as an integrated, non-uniform memory access, shared-memory multiprocessor. This simplifies the development of applications and allows compute-intensive parallel programs to effectively harness the potential of multiple nodes. Amber programs are written using an object-oriented subset of the C++ programming language, supplemented with primitives for managing concurrency and distribution. Amber provides a network-wide shared-object virtual memory in which coherence is provided by hardware means for locally-executing threads, and by software means for remote accesses. Amber runs on top of the Topaz operating system on a network of DEC SRC ...
558|Performance of Firefly RPC|In this paper, we report on the performance of the remote procedure call implementation for the Firefly multiprocessor and analyze the implemen-tation to account precisely for all measured latency. From the analysis and measurements, we estimate how much faster RPC could be if certain improve-ments were made. The elapsed time for an inter-machine call to a remote procedure that accepts no arguments and produces no results is 2.66 milliseconds. The elapsed time for an RPC that has a single 1440-byte result (the maximum result that will fit in a single packet) is 6.35 milliseconds. Maximum inter-machine throughput using RPC is 4.65 megabits/second, achieved with 4 threads making parallel RPCs that return the maximum sized single packet result. CPU utilization at maximum throughput is about 1.2 on the calling machine and a little less on the server. These measurements are for RPCs from user space on one machine to user space on another, using the installed system and a 10 megabit/second E...
559|First-Class User-Level Threads|It is often desirable, for reasons of clarity, portability, and efficiency, to write parallel programs in which the number of processes is independent of the number of available processors. Several modern operating systems support more than one process in an address space, but the overhead of creating and synchronizing kernel processes can be high. Many runtime environments implement lightweight processes (threads) in user space, but this approach usually results in second-class status for threads, making it difficult or impossible to perform scheduling operations at appropriate times (e.g. when the current thread blocks in the kernel). In addition, a lack of common assumptions may also make it difficult for parallel programs or library routines that use dissimilar thread packages to communicate with each other, or to synchronize access to shared data. We describe a set of kernel mechanisms and conventions designed to accord first-class status to user-level threads, allowing them to be used in any reasonable way that traditional kernel-provided processes can be used, while leaving the details of their implementation to userlevel code. The key features of our approach are (1) shared memory for asynchronous communication between the kernel and the user, (2) software interrupts for events that might require action on the part of a user-level scheduler, and (3) a scheduler interface convention that facilitates interactions in user space between dissimilar kinds of threads. We have incorporated these mechanisms in the Psyche parallel operating system, and have used them to implement several different kinds of user-level threads. We argue for our approach in terms of both flexibility and performance.
560|Empirical Studies of Competitive Spinning for a Shared-Memory Multiprocessor|A common operation in multiprocessor programs is acquiring a lock to protect access to shared data. Typically, the requesting thread is blocked if the lock it needs is held by another thread. The cost of blocking one thread and activating another can be a substantial part of program execution time. Alternatively, the thread could spin until the lock is free, or spin for a while and then block. This may avoid context-switch overhead, but processor cycles may be wasted in unproductive spinning. This paper studies seven strategies for determining whether and how long to spin before blocking. Of particular interest are competitive strategies, for which the performance can be shown to be no worse than some constant factor times an optimal off-line strategy. The performance of five competitive strategies is compared with that of always blocking, always spinning, or using the optimal off-line algorithm. Measurements of lock-waiting time distributions for five parallel programs were used to co...
561|Synchronization Primitives for a Multiprocessor: A Formal Specification|Formal specifications of operating system interfaces can be a useful part of their documentation. We illustrate this by documenting the Threads synchronization primitives of the Taos operating system. We start with an informal description, present a way to formally specify interfaces in concurrent systems, and then give a formal specification of the synchronization primitives. We briefly discuss both the implementation and what we have learned from using the specification for more than a year. Our main conclusion is that programmers untrained in reading formal specifications have found this one helpful in getting their work done. iii  Introduction  The careful documentation of interfaces is an important step in the production of software upon which other software is to be built. If people are to use software without having to understand its implementation, documentation must convey semantic as well as syntactic information. When the software involves concurrency, adequate documentatio...
563|MapReduce: Simplified Data Processing on Large Clusters|MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.  
564|Route Packets, Not Wires: On-Chip Interconnection Networks|Using on-chip interconnection networks in place of ad-hoc global wiring structures the top level wires on a chip and facilitates modular design. With this approach, system modules (processors, memories, peripherals, etc...) communicate by sending packets to one another over the network. The structured network wiring gives well-controlled electrical parameters that eliminate timing iterations and enable the use of high-performance circuits to reduce latency and increase bandwidth. The area overhead required to implement an on-chip network is modest, we estimate 6.6%. This paper introduces the concept of on-chip networks, sketches a simple network, and discusses some challenges in the architecture and design of these networks.  1
565|The design and implementation of FFTW3|FFTW is an implementation of the discrete Fourier transform (DFT) that adapts to the hardware in order to maximize performance. This paper shows that such an approach can yield an implementation that is competitive with hand-optimized libraries, and describes the software structure that makes our current FFTW3 version flexible and adaptive. We further discuss a new algorithm for real-data DFTs of prime size, a new way of implementing DFTs by means of machine-specific single-instruction, multiple-data (SIMD) instructions, and how a special-purpose compiler can derive optimized implementations of the discrete cosine and sine transforms automatically from a DFT algorithm. 
566|FFTW: An Adaptive Software Architecture For The FFT|FFT literature has been mostly concerned with minimizing the number of floating-point operations performed by an algorithm. Unfortunately, on present-day microprocessors this measure is far less important than it used to be, and interactions with the processor pipeline and the memory hierarchy have a larger impact on performance. Consequently, one must know the details of a computer architecture in order to design a fast algorithm. In this paper, we propose an adaptive FFT program that tunes the computation automatically for any particular hardware. We compared our program, called FFTW, with over 40 implementations of the FFT on 7 machines. Our tests show that FFTW&#039;s self-optimizing approach usually yields significantly better performance than all other publicly available software. FFTW also compares favorably with machine-specific, vendor-optimized libraries. 1. INTRODUCTION The discrete Fourier transform (DFT) is an important tool in many branches of science and engineering [1] and...
567|Low-Power CMOS Digital Design| Motivated by emerging battery-operated applications that demand intensive computation in portable environments, techniques are investigated which reduce power consumption in CMOS digital circuits while maintaining computational throughput. Techniques for low-power operation are shown which use the lowest possible supply voltage coupled with architectural, logic style, circuit, and technology optimizations. An architectural-based scaling strategy is presented which indicates that the optimum voltage is much lower than that determined by other scaling considerations. This optimum is achieved by trading increased silicon area for reduced power consumption. 
568|Garp: A MIPS Processor with a Reconfigurable Coprocessor|Typical reconfigurable machines exhibit shortcomings that make them less than ideal for general-purpose computing. The Garp Architecture combines reconfigurable hardware with a standard MIPS processor on the same die to retain the better features of both. Novel aspects of the architecture are presented, as well as a prototype software environment and preliminary performance results. Compared to an UltraSPARC, a Garp of similar technology could achieve speedups ranging from a factor of 2 to as high as a factor of 24 for some useful applications.  
569|Single-ISA Heterogeneous Multi-Core Architectures: The Potential for Processor Power Reduction|This paper proposes and evaluates single-ISA heterogeneous multi-core architectures as a mechanism to reduce processor power dissipation. Our design incorporates heterogeneous cores representing different points in the power/performance design space; during an application &#039;s execution, system software dynamically chooses the most appropriate core to meet specific performance and power requirements.
570|Optimizing Matrix Multiply using PHiPAC: a Portable, High-Performance, ANSI C Coding Methodology|Modern microprocessors can achieve high performance on linear algebra kernels but this currently requires extensive machine-specific hand tuning. We have developed a methodology whereby near-peak performance on a wide range of systems can be achieved automatically for such routines. First, by analyzing current machines and C compilers, we&#039;ve developed guidelines for writing Portable, High-Performance, ANSI C (PHiPAC, pronounced &#034;fee-pack&#034;). Second, rather than code by hand, we produce parameterized code generators. Third, we write search scripts that and the best parameters for a given system. We report on a BLAS GEMM compatible multi-level cache-blocked matrix multiply generator which produces code that achieves around 90% of peak on the Sparcstation-20/61, IBM RS/6000-590, HP 712/80i, SGI Power Challenge R8k, and SGI Octane R10k, and over 80% of peak on the SGI Indigo R4k. The resulting routines are competitive with vendoroptimized BLAS GEMMs.
571|A supernodal approach to sparse partial pivoting|We investigate several ways to improve the performance of sparse LU factorization with partial pivoting, as used to solve unsymmetric linear systems. To perform most of the numerical computation in dense matrix kernels, we introduce the notion of unsymmetric supernodes. To better exploit the memory hierarchy, weintroduce unsymmetric supernode-panel updates and two-dimensional data partitioning. To speed up symbolic factorization, we use Gilbert and Peierls&#039;s depth- rst search with Eisenstat and Liu&#039;s symmetric structural reductions. We have implemented a sparse LU code using all these ideas. We present experiments demonstrating that it is signi cantly faster than earlier partial pivoting codes. We also compare performance with Umfpack, which uses a multifrontal approach; our code is usually faster.
572|Energy Dissipation In General Purpose Microprocessors|In this paper we investigate possible ways to improve the energy efficiency of a general purpose microprocessor. We show that the energy of a processor depends on its performance, so we chose the energy-delay product to compare different processors. To improve the energy-delay product we explore methods of reducing energy consumption that do not lead to performance loss (i.e., wasted energy), and explore methods to reduce delay by exploiting instruction level parallelism. We found that careful design reduced the energy dissipation by almost 25%. Pipelining can give approximately a 22 improvement in energydelay product. Superscalar issue, however, does not improve the energy-delay product any further since the overhead required offsets the gains in performance. Further improvements will be hard to come by since a large fraction of the energy (50--80%) is dissipated in the clock network and the on-chip memories. Thus, the efficiency of processors will depend more on the technology being ...
573|Measuring the gap between fpgas and asics|This paper presents experimental measurements of the dif-ferences between a 90nm CMOS FPGA and 90nm CMOS Standard Cell ASICs in terms of logic density, circuit speed and power consumption. We are motivated to make these measurements to enable system designers to make better in-formed choices between these two media and to give insight to FPGA makers on the deficiencies to attack and thereby improve FPGAs. In the paper, we describe the methodology by which the measurements were obtained and we show that, for circuits containing only combinational logic and flip-flops, the ratio of silicon area required to implement them in FPGAs and ASICs is on average 40. Modern FPGAs also contain “hard ” blocks such as multiplier/accumulators and block memories and we find that these blocks reduce this average area gap significantly to as little as 21. The ratio of critical path delay, from FPGA to ASIC, is roughly 3 to 4, with less influence from block memory and hard multipli-ers. The dynamic power consumption ratio is approximately 12 times and, with hard blocks, this gap generally becomes smaller.
574|Synchronization and Communication in the T3E Multiprocessor|This paper describes the synchronization and communication primitives of the Cray T3E multiprocessor, a shared memory system scalable to 2048 processors. We discuss what we have learned from the T3D project (the predecessor to the T3E) and the rationale behind changes made for the T3E. We include performance measurements for various aspects of communication and synchronization. The T3E augments the memory interface of the DEC 21164 microprocessor with a large set of explicitly-managed, external registers (E-registers). E-registers are used as the source or target for all remote communication. They provide a highly pipelined interface to global memory that allows dozens of requests per processor to be outstanding. Through E-registers, the T3E provides a rich set of atomic memory operations and a flexible, user-level messaging facility. The T3E also provides a set of virtual hardware barrier/ eureka networks that can be arbitrarily embedded into the 3D torus interconnect. 
575|The optimal logic depth per pipeline stage is 6 to 8 FO4 inverter delays|Microprocessor clock frequency has improved by nearly 40% annually over the past decade. This improvement has been provided, in equal measure, by smaller technologies and deeper pipelines. From our study of the SPEC 2000 bench-marks, we find that for a high-performance architecture imple-mented in lOOnm technology, the optimal clock period is ap-proximately 8fan-out-of-four ( F04) inverter delays for integer benchmarks, comprised of 6 F04 of useful work and an over-head of about 2 F04. The optimal clock period for floating-point benchmarks is 6F04. We find these optimal points to be insensitive to latch and clock skew overheads. Our study indi-cates that further pipelining can at best improve performance of integer programs by a factor of 2 over current designs. At these high clock frequencies it will be difficult to design the instruction issue window to operate in a single cycle. Con-sequently, we propose and evaluate a high-frequency design called a segmented instruction window. 1
576|private communication |Integral inequality. In this paper, an integral inequality is studied. An answer to an open problem proposed by Feng Qi and Yin Chen and John Kimball is given. Many thanks to Professor Feng Qi for his comments. The authors also want to give their deep gratitude to the anonymous referee for his/her valuable comments and suggestions on the proof of Theorem 2.2 which made the article more readable. Special thanks goes to the research assistant for the quick responsibility. Notes on an Open Problem Quô ´ c Anh Ngô and Pham Huy Tung vol. 8, iss. 2, art. 41, 2007 Title Page
577|An Updated Set of Basic Linear Algebra Subprograms (BLAS)  (2001) |This paper summarizes the BLAS Technical Forum Standard, a speci- #cation of a set of kernel routines for linear algebra, historically called the Basic Linear Algebra Subprograms and commonly known as the BLAS. The complete standard can be found in #1#, and on the BLAS Technical Forum webpage #http:##www.netlib.org#blas#blast-forum##
578|Superoptimizer—a look at the smallest program|Given an instruction set, the superoptimizer finds the shortest program to compute a function. Startling programs have been generated, many of them engaging in convoluted bit-fiddling bearing little resemblance to the source programs which defined the functions. The key idea in the superoptimizer is a probabilistic test that makes exhaustive searches practical for programs of useful size. The search space is defined by the processor&#039;s instruction set, which may include the whole set, but it is typically restricted to a subset. By constraining the instructions and observing the effect on the output program, one can gain insight into the design of instruction sets. In addition, superoptimized programs may be used by peephole optimizers to improve the quality of generated code, or by assembly language programmers to improve manually written code. 1.
579|Methods for Evaluating and Covering the Design Space during Early Design Development|This paper gives an overview of methods used for Design Space Exploration (DSE) at the  system- and micro-architecture levels. The DSE problem is considered to be two orthogonal  issues: (I) How could a single design point be evaluated, (II) how could the design space be  covered during the exploration process? The latter question arises since an exhaustive exploration  of the design space by evaluating every possible design point is usually prohibitive due to  the sheer size of the design space. We therefore reveal trade-o#s linked to the choice of appropriate  evaluation and coverage methods. The designer has to balance the following issues: the  accuracy of the evaluation, the time it takes to evaluate one design point (including the implementation  of the evaluation model), the precision/granularity of the design space coverage,  and last but not least the possibilities for automating the exploration process. We also list common  representations of the design space and compare current system and micro-architecture  level design frameworks. This review thus eases the choice of a decent exploration policy by  providing a comprehensive survey and classification of recent related work. It is focused on  System-on-a-Chip designs, particularly those used for network processors. These systems are  heterogeneous in nature using multiple computation, communication, memory, and peripheral  resources.
580|A Case for Intelligent RAM|This article reviews the state of microprocessors and DRAMs today, explores some of the opportunities and challenges for IRAMs, and finally estimates performance and energy efficiency of three IRAM designs.
581|Communication Characteristics of Large-Scale Scientific Applications for Contemporary Cluster Architectures|This paper examines the explicit communication characteristics of several sophisticated scientific  applications, which, by themselves, constitute a representative suite of publicly available benchmarks for large cluster architectures. By focusing on the Message Passing Interface (MPI) and by using hardware counters on the microprocessor, we observe each application&#039;s inherent behavioral characteristics: point-to-point and collective communication, and floating-point operations. Furthermore, we explore the sensitivities of these characteristics to both problem  size and number of processors. Our analysis reveals several striking similarities across our diverse set of applications including the use of collective operations, especially those collectives with very small data  payloads. We also highlight a trend of novel  applications parting with regimented, static  communication patterns in favor of dynamically evolving patterns, as evidenced by our experiments on applications that use implicit linear solvers and adaptive mesh refinement. Overall, our study contributes a better understanding of the requirements of current and emerging paradigms of scientific computing in terms of their computation and communication demands.
582|Programming by Sketching for Bit-Streaming Programs|This paper introduces the concept of programming with sketches, an approach for the rapid development of high-performance applications. This approach allows a programmer to write clean and portable reference code, and then obtain a high-quality implementation by simply sketching the outlines of the desired implementation. Subsequently, a compiler automatically fills in the missing details while also ensuring that a completed sketch is faithful to the input reference code. In this paper, we develop StreamBit as a sketching methodology for the important class of bit-streaming programs (e.g., coding and cryptography). A sketch is a partial specification of the implementation, and as such, it affords several benefits to programmer in terms of productivity and code robustness. First, a sketch is easier to write compared to a complete implementation. Second, sketching allows the programmer to focus on exploiting algorithmic properties rather
583|The Cascade High Productivity Language|The strong focus of recent High End Computing efforts on performance has resulted in a low-level parallel programming paradigm characterized by explicit control over message-passing in the framework of a fragmented programming model. In such a model, object code performance is achieved at the expense of productivity, conciseness, and clarity. This paper describes the design of Chapel, the Cascade High Productivity Language, which is being developed in the DARPA-funded HPCS project Cascade led by Cray Inc. Chapel pushes the state-of-the-art in languages for HEC system programming by focusing on productivity, in particular by combining the goal of highest possible object code performance with that of programmability offered by a high-level user interface. The design of Chapel is guided by four key areas of language technology: multithreading, locality-awareness, object-orientation, and generic programming. The Cascade architecture, which is being developed in parallel with the language, provides key architectural support for its efficient implementation. 1.
585|Automatically Tuned Collective Communications|The performance of the MPI&#039;s collective communications is critical in most MPI-based applications. A general algorithm for a given collective communication operation may not give good performance on all systems due to the di#erences in architectures, network parameters and the storage capacity of the underlying MPI implementation. In this paper, we discuss an approach in which the collective communications are tuned for a given system by conducting a series of experiments on the system. We also discuss a dynamic topology method that uses the tuned static topology shape, but re-orders the logical addresses to compensate for changing run time variations. A series of experiments were conducted comparing our tuned collective communication operations to various native vendor MPI implementations. The use of the tuned collective communications resulted in about 30%-650% improvement in performance over the native MPI implelementations.  1. INTRODUCTION  This project developed out of an attempt...
586|Statistical Scalability Analysis of Communication Operations in Distributed Applications |Current trends in high performance computing suggest that users will soon have widespread access to clusters of multiprocessors with hundreds, if not thousands, of processors. This unprecedented degree of parallelism will undoubtedly expose scalability limitations in existing applications, where scalability is the ability of a parallel algorithm on a parallel architecture to effectively utilize an increasing number of processors. Users will need precise and automated techniques for detecting the cause of limited scalability. This paper addresses this dilemma. First, we argue that users face numerous challenges in understanding application scalability: managing substantial amounts of experiment data, extracting useful trends from this data, and reconciling performance information with their application&#039;s design. Second, we propose a solution to automate this data analysis problem by applying fundamental statistical techniques to scalability experiment data. Finally, we evaluate our operational prototype on several applications, and show that statistical techniques offer an effective strategy for assessing application scalability. In particular, we find that non-parametric correlation of the number of tasks to the ratio of the time for communication operations to overall communication time provides a reliable measure for identifying communication operations that scale poorly. 1
587|Optimizing pipelines for power and performance|During the concept phase and definition of next generation high-end processors, power and performance will need to be weighted appropriately to deliver competitive cost/performance. It is not enough to adopt a CPI-centric view alone in early-stage definition studies. One of the fundamental issues confronting the architect at this stage is the choice of pipeline depth and target frequency. In this paper we present an optimization methodology that starts with an analytical power-performance model to derive optimal pipeline depth for a superscalar processor. The results are validated and further refined using detailed simulation based analysis. As part of the power-modeling methodology, we have developed equations that model the variation of energy as a function of pipeline depth. Our results using a set of SPEC2000 applications show that when both power and performance are considered for optimization, the optimal clock period is around 18 FO4. We also provide a detailed sensitivity analysis of the optimal pipeline depth against key assumptions of these energy models. 1
588|Hardware/Software Instruction Set Configurability for System-on-Chip Processors|New application-focused system-on-chip platforms motivate new application-specific processors. Configurable and extensible processor architectures offer the efficiency of tuned logic solutions with the flexibility of standard high-level programming methodology. Automated extension of processor function units and the associated software environment -- compilers, debuggers, simulators and real-time operating systems -- satisfies these needs. At the same time, designing at the level of software and instruction set architecture significantly shortens the design cycle and reduces verification effort and risk. This paper describes the key dimensions of extensibility within the processor architecture, the instruction set extension description language and the means of automatically extending the software environment from that description. It also describes two groups of benchmarks, EEMBC&#039;s Consumer and Telecommunications suites, that show 20 to 40 times acceleration of a broad set of algorithms through application-specific instruction set extension, relative to high performance RISC processors.
589|Parallel Programmer Productivity: A Case Study of Novice|In developing High-Performance Computing (HPC) software, time to solution is an important metric. This metric is comprised of two main components: the human effort required developing the software, plus the amount of machine time required to execute it. To date, little empirical work has been done to study the first component: the human effort required and the effects of approaches and practices that may be used to reduce it. In this paper, we describe a series of studies that address this problem. We instrumented the development process used in multiple HPC classroom environments. We analyzed data within and across such studies, varying factors such as the parallel programming model used and the application being developed, to understand their impact on the development process. 1
590|An Empirical Performance Evaluation of Scalable Scientific Applications|We investigate the scalability, architectural requirements, and performance characteristics of eight scalable scientific applications. Our analysis is driven by empirical measurements using statistical and tracing instrumentation for both communication and computation. Based on these measurements, we refine our analysis into precise explanations of the factors that influence performance and scalability for each application; we distill these factors into common traits and overall recommendations for both users and designers of scalable platforms. Our experiments demonstrate that some traits, such as improvements in the scaling and performance of MPI&#039;s collective operations, will benefit most applications. We also find specific characteristics of some applications that limit performance. For example, one application&#039;s intensive use of a 64-bit, floating-point divide instruction, which has high latency and is not pipelined on the POWER3, limits the performance of the application&#039;s primary computation. 1
591|Analyzing Ultra-Scale Application Communication Requirements for a Reconfigurable Hybrid Interconnect|The path towards realizing peta-scale computing is increasingly dependent on scaling up to unprecedented numbers of processors. To prevent the interconnect architecture between processors from dominating the overall cost of such systems, there is a critical need for interconnect solutions that both provide performance to ulta-scale applications and have costs that scale linearly with system size. In this work we propose the Hybrid Flexibly Assignable Switch Topology (HFAST) infrastructure. The HFAST approach uses both passive (circuit switch) and active (packet switch) commodity switch components to deliver all of the flexibility and fault-tolerance of a fully-interconnected network (such as a fat-tree), while preserving the nearly linear cost scaling associated with traditional low-degree interconnect networks. To understand the applicability of this technology, we perform an in-depth study of the communication requirements across a broad spectrum of important scientific applications, whose computational methods include: finitedifference, lattice-bolzmann, particle in cell, sparse linear algebra, particle mesh ewald, and FFT-based solvers. We use the IPM (Integrated Performance Monitoring) profiling layer to gather detailed messaging statistics with minimal impact to code performance. This profiling provides us sufficiently detailed communication topology and message volume data to evaluate these applications in the context of the proposed hybrid interconnect. Overall results show that HFAST is a promising approach for practically addressing the interconnect requirements of future peta-scale systems. 1.
592|A Component Architecture for High-Performance Computing|The Common Component Architecture (CCA) provides a means for developers to manage the complexity of  large-scale scientific software systems and to move toward a &#034;plug and play&#034; environment for high-performance  computing. The CCA model allows for a direct connection between components within the same process to maintain  performance on inter-component calls. It is neutral with respect to parallelism, allowing components to use whatever  means they desire to communicate within their parallel &#034;cohort.&#034; We will discuss in detail the importance of  performance in the design of the CCA and will analyze the performance costs associated with features of the CCA.
593|NP-Click: A Productive Software Development Approach for Network Processors|Application-specific integrated circuit design is too risky and prohibitively expensive for many applications. This trend, combined with increasing silicon capability on a die, is fueling the emergence of application-specific programmable architectures. Researchers and developers have demonstrated examples of such architectures in networking, multimedia, and graphics. To meet the high performance demands in these application domains, many of these devices use complex architectural constructs. For example, network processors have many complicated architectural features: multiple processing elements each with multiple hardware-supported threads, distributed memories, special-purpose hardware, and a variety of on-chip communication mechanisms. 1 This focus on architecture design for network processors has made programming them an arduous task. Current network processors require in-depth knowledge of the architecture just to begin programming the device. However, for network processors to succeed, programmers must efficiently implement high-performance applications on them. Ideally, we would like to program network processors with a popular, domain-specific language for networking, such as Click. 2 Although Click is natural for the application designer to use, it is challenging to implement on a network processor. To address this problem, we create an abstraction of the underlying architecture that exposes enough detail to write efficient code, yet hides less-essential complexity. We call this abstraction a programming model. Existing software development approaches More than 30 distinct architectures for network processing have been introduced in the
594|An Experiment in Measuring the Productivity of Three Parallel Programming Languages|In May 2005, a 4.5 day long productivity study was performed at the Pittsburgh Supercomputing Center as part of the IBM HPCS/PERCS project, comparing the productivity of three parallel programming languages: C+MPI, UPC, and the IBM PERCS project’s x10 lan-guage. 27 subjects were divided into 3 comparable groups (one per language) and all were asked to par-allelize the same serial algorithm: Smith-Waterman local sequence matching – a bio-informatics kernel in-spired from the Scalable Synthetic Compact Applica-tions (SSCA) Benchmark, number 1. Two days of tu-torials were given for each language, followed by two days of intense parallel programming for the main prob-lem, and a half day of exit interviews. The study par-ticipants were mostly Science and CS students from the University of Pittsburgh, with limited or no parallel programming experience. There were two typical ways of solving the sequence matching problem: a wavefront algorithm, which was not scalable because of data dependencies, and yet posed programming challenges because of the frequent syn-chronization requirements. However, the given problem was shown to also have a subtle domain-specific prop-erty, which allowed some ostensible data dependences to be ignored in exchange for redundant computation, and boosted scalability by a great extent. This prop-erty was also given as a written hint to all the par-ticipants, encouraging them to obtain higher degrees of
595|High-Level Programming Language Abstractions for Advanced and Dynamic Parallel Computations|This thesis presents a combination of p-independent and p-dependent extensions to ZPL.
596|Understanding Ultra-Scale Application Communication Requirements|As thermal constraints reduce the pace of CPU performance improvements, the cost and scalability of future HPC architectures will be increasingly dominated by the interconnect. In this work we perform an in-depth study of the communication requirements across a broad spectrum of important scientific applications, whose computational methods include: finite-difference, lattice-bolzmann, particle in cell, sparse linear algebra, particle mesh ewald, and FFT-based solvers. We use the IPM (integrated Performance Monitoring) profiling framework to collect detailed statistics on communication topology and message volume with minimal impact to code performance. By characterizing the parallelism and communication requirements of such a diverse set of applications, we hope to guide architectural choices for the design and implementation of interconnects for future HPC systems. 1.
597|ProtoFlex: Co-Simulation for Component-wise FPGA Emulator Development|This paper presents PROTOFLEX, a hardware/software cosimulation methodology to facilitate the systematic development of RTL components for an FPGA-based multiprocessor emulator. PROTOFLEX relies on FLEXUS
598|ATLAS: A Scalable Emulator for Transactional Parallel Systems|With uniprocessor systems running into instruction-level par-allelism (ILP) limits and fundamental VLSI constraints, multi-processor architectures provide a realistic path towards scalable performance. Nevertheless, the key factors limiting the potential of multiprocessor systems are the difficulty of parallel application development and the hardware complexity of large-scale systems. Several groups have recently proposed parallel software and hardware based on the concept of transactions as a novel ap-proach for addressing the problems of multiprocessor systems [1-6]. Transactions have the potential to simplify parallel program-ming by eliminating the need for manual orchestration of parallel tasks using locks and messages [5]. Transactions have the poten-tial to improve parallel hardware by allowing for speculative par-allelism and increasing the granularity of the coherence and con-sistency protocols [4].
599|Power-Optimal Pipelining in Deep Submicron Technology|This paper explores the effectiveness of pipelining as a power saving tool, where the reduction in logic depth per stage is used to reduce supply voltage at a fixed clock frequency. We examine poweroptimal pipelining in deep submicron technology, both analytically and by simulation. Simulation uses a 70 nm predictive process with a fanout-of-four inverter chain model including input/output flipflops, and results are shown to match theory well. The simulation results show that power-optimal logic depth is 6 to 8 FO4 and optimal power saving varies from 55 to 80 % compared to a 24 FO4 logic depth, depending on threshold voltage, activity factor, and presence of clock-gating. We decompose the power consumption of a circuit into three components, switching power, leakage power, and idle power, and present the following insights into power-optimal pipelining. First, power-optimal logic depth decreases and optimal power savings increase for larger activity factors, where switching power dominates over leakage and idle power. Second, pipelining is more effective with lower threshold voltages at high activity factors, but higher threshold voltages give better results at lower activity factors where leakage current dominates. Lastly, clock-gating enables deeper pipelining and more power saving because it reduces timing element overhead when the activity factor is low.
600|A Productive Programming Environment for Stream Computing|This paper presents StreamIt and the StreamIt Development Tool. The development tool is an IDE designed to improve the coding, debugging, and visualization of streaming applications by exploiting the ability of the StreamIt language to naturally represent streaming codes as structured, hierarchical graphs. The StreamIt Development Tool aims to emulate the best of traditional debuggers and IDEs while moving toward hierarchical visualization and debugging concepts specialized for streaming applications. As such, it provides utilities for stream graph examination, tracking of data flow between streams, and deterministic execution of parallel streams. These features are in addition to more conventional tools for creating and editing codes, integrated compilers, setting breakpoints, watchpoints, and stepby-step program execution. A user study evaluating StreamIt and the development tool was held at MIT during which participants were given erroneous programs and asked to resolve the programming errors. We compared the productivity of the users when using the StreamIt Development Tool and its graphical features to those who were restricted to lineoriented debugging strategies, and we found that the former produced ten more correct solutions compared to the latter set of users. Furthermore, our data suggests that the graphical tool chain helped to mitigate user frustration and encouraged participants to invest more time tracking and fixing programming errors. 1
603|Model Checking for Programming Languages using VeriSoft|Verification by state-space exploration, also often referred to as &#034;model checking&#034;, is an effective method for analyzing the correctness of concurrent reactive systems (e.g., communication protocols). Unfortunately, existing model-checking techniques are restricted to the verification of properties of models, i.e., abstractions, of concurrent systems.  In this paper, we discuss how model checking can be extended to deal directly with &#034;actual&#034; descriptions of concurrent systems, e.g., implementations of communication protocols written in programming languages such as C or C++. We then introduce a new search technique that is suitable for exploring the state spaces of such systems. This algorithm has been implemented in VeriSoft, a tool for systematically exploring the state spaces of systems composed of several concurrent processes executing arbitrary C code. As an example of application, we describe how VeriSoft successfully discovered an error in a 2500-line C program controlling rob...
604|Symbolic Boolean manipulation with ordered binary-decision diagrams|Ordered Binary-Decision Diagrams (OBDDS) represent Boolean functions as directed acyclic graphs. They form a canonical representation, making testing of functional properties such as satmfiability and equivalence straightforward. A number of operations on Boolean functions can be implemented as graph algorithms on OBDD
605|Partial-Order Methods for the Verification of Concurrent Systems - An Approach to the State-Explosion Problem|State-space exploration techniques are increasingly being used for debugging and proving correct finite-state concurrent reactive systems. The reason for this success is mainly the simplicity of these techniques. Indeed, they are easy to understand, easy to implement and, last but not least, easy to use: they are fully automatic. Moreover, the range of properties that they can verify has been substantially broadened thanks to the development of model-checking methods for various temporal logics. The main limit of state-space exploration verification techniques is the often excessive size of the state space due, among other causes, to the modeling of concurrency by interleaving. However, exploring all interleavings of concurrent events is not a priori necessary for verification: interleavings corresponding to the same concurrent execution contain related information. One can thus hope to be able to verify properties of a concurrent system without exploring all interleavings of its concu...
606|Techniques for Debugging Parallel Programs with Flowback Analysis|Flowback analysis is a powerful technique for debugging programs. It allows the programmer to examine dynamic dependences in a program&#039;s execution history without having to re-execute the program. The goal is to present to the programmer a graphical view of the dynamic program dependences. We are building a system, called PPD, that performs flowback analysis while keeping the execution time overhead low. We also extend the semantics of flowback analysis to parallel programs. This paper describes details of the graphs and algorithms needed to implement efficient flowback analysis for parallel programs. Execution time overhead is kept low by recording only a small amount of trace during a program&#039;s execution. We use semantic analysis and a technique called incremental tracing to keep the time and space overhead low. As part of the semantic analysis, PPD uses a static program dependence graph structure that reduces the amount of work done at compile time and takes advantage of the dynamic...
607|Tracing Protocols|Automated protocol validation tools are by necessity often based on some form of  symbolic execution. The complexity of the analysis problem however imposes restrictions  on the scope of these tools. The paper studies the nature of these restrictions and  explicitly addresses the problem of finding errors in data communication protocols of  which the size precludes analysis by traditional means. The protocol
608|State-Space Caching Revisited|State-space caching is a verification technique for finite-state concurrent systems. It performs  an exhaustive exploration of the state-space of the system being checked while storing  only all states of just one execution sequence plus as many other previously visited states  as available memory allows. So far, this technique has been of little practical significance:  it allows one to reduce memory usage by only two to three times, before an unacceptable  blow-up of the run-time overhead sets in. The explosion of the run-time requirements is  due to redundant multiple explorations of unstored parts of the state-space. Indeed, almost  all states in the state-space of concurrent systems are typically reached several times during  the search.  In this paper, we present a method to tackle the main cause of this prohibitive state  matching: the exploration of all possible interleavings of concurrent executions of the system,  which all lead to the same state. Then, we show that, in many ...
609|Model Checking in Practice: An Analysis of the ACCESS.bus&amp;trade; Protocol using SPIN|. This paper presents a case study of the use of model checking for analyzing an industrial protocol, the ACCESS.bus  TM  protocol. Our analysis of this protocol was carried out using SPIN, an automated verification system which includes an implementation of model-checking algorithms. A model of the protocol was developed, and properties expressed by linear-time temporal-logic formulas were checked on this model. This analysis revealed subtle flaws in the design of the protocol. Developers who worked on implementations of ACCESS.bus  TM  were unaware of these flaws at a very late stage of their development process. We also present suggestions for solving the detected problems. 1 Introduction  State-space exploration techniques are increasingly being used for debugging and proving correct finite-state concurrent reactive systems (cf. [Rud87, Liu89, HK90, Hol91, DDHY92, FGM  +  92]). These techniques consist of exploring a global state graph, called the state space, representing the comb...
610|Constructing Two-Writer Atomic Registers|In this paper, we construct a 2-writer, n-reader atomic memory register from two l-writer, (n + l)-reader atomic memory registers. There are no restrictions on the size of the constructed register. The simulation requires only a single extra bit per real register, and can survive the failure of any set of readers and writers. This construction is a part of a systematic investigation of register simulations, by several researchers.  
611|ON THE CORRECTNESS OF ORPHAN ELIMINATION ALGORITHMS|In a distributed system, node crashes and network delays can result in orphaned computations:  computations that are still running but whose results are no longer needed. Several algorithms have been  proposed to detect and eliminate such computations before they can see inconsistent states of the shared,  concurrently accessed data. In this paper we analyze two orphan elimination algorithms that have been  proposed for nested transaction systems. We describe the algorithms formally, and present complete detailed proofs of correctness. Our proofs are remarkably simple, and show that the fundamental  concepts underlying the two algorithms are quite similar. In addition, we show formally that the  algorithms can be used in combination with any correct concurrency control technique, thus providing  formal justification for the informal claims made by the algorithms&#039; designers. Our results are a  significant advance over earlier work in the area, in which it was extremely difficult to state and prove  comparable results.
612|Proving Boolean Combinations of Deterministic Properties|This paper gives a method for proving that a program satisfies a temporal property that has been specified in terms of Buchi automata. The method permits extraction of proof obligations for a property formulated as the Boolean combination of properties, each of which is specified by a deterministic Buchi automaton, directly from the individual automata. The proof obligations can be formulated as Hoare triples. The method is proved sound and relatively complete. A simple example illustrates applica- tion of the method.
613|Nested Transactions and Read/Write Locking|We give a clear yet rigorous correctness proof for Moss&#039;s algorithm for managing data in a nested transaction system. The algorithm, which is the basis of concurrency control and recovery in the Argus system, uses read- and write-locks and a stack of versions of each object to ensure the serializability and recoverability of transactions accessing the data. Our proof extends earlier work on exclusive locking to prove that Moss&#039;s algorithm generates serially correct executions in the presence of concurrency and transaction aborts. The key contribution is the identification of a simple property of cead operations, called transparency, that permits shared locks to be used for read operations.
614|Obliq - A language with distributed scope|computation. An Obliq computation may involve multiple threads of control within an address space, multiple address spaces on a machine, heterogeneous machines over a local network, and multiple networks over the Internet. Obliq objects have state and are local to a site. Obliq computations can roam over the network, while maintaining network connections.
615|SELF: The power of simplicity| SELF is an object-oriented language for exploratory programming based on a small number of simple and concrete ideas: prototypes, slots, and behavior. Prototypes combine inheritance and instantiation to provide a framework that is simpler and more flexible than most object-oriented languages. Slots unite variables and procedures into a single construct. This permits the inheritance hierarchy to take over the function of lexical scoping in conventional languages. Finally, because SELF does not distinguish state from behavior, it narrows the gaps between ordinary objects, procedures, and closures. SELF’s simplicity and expressiveness offer new insights into objectoriented computation.  
616|Fine-grained Mobility in the Emerald System|Emerald is an object-based language and system designed for the construction of distributed programs. An explicit goal of Emerald is support for object mobility; objects in Emerald can freely move within the system to take advantage of distribution and dynamically changing environments. We say that Emerald has fine-grained mobility because Emerald objects can be small data objects as well as process objects. Fine-grained mobility allows us to apply mobility in new ways but presents imple-mentation problems as well. This paper discusses the benefits of tine-grained mobility, the Emerald language and run-time mechanisms that support mobility, and techniques for implementing mobility that do not degrade the performance of local operations. Performance measurements of the current implementation are included.
617|Using Prototypical Objects to Implement Shared Behavior in Object Oriented Systems|A traditional philosophical controversy between representing general concepts as abstract sets or classes and representing concepts as concrete prototypes is reflected in a controversy between two mechanisms for sharing behavior between objects in object oriented programming languages. Inheritance splits the object world into classes, which encode behavior shared among a group of instances, which represent individual members of these sets. The class/instance distinction is not needed if the alternative of using prototypes is adopted. A prototype represents the default behavior for a concept, and new objects can re-use part of the knowledge stored in the prototype by saying how the new object differs from the prototype. The prototype approach seems to hold some advantages for representing default knowledge, and incrementally and dynamically modifying concepts. Delegation is the mechanism for implementing this in object oriented languages. After checking its idiosyncratic behavior, an ob...
618|Orca: A language for parallel programming of distributed systems|Orca is a language for implementing parallel applications on loosely coupled distributed systems. Unlike most languages for distributed programming, it allows processes on different machines to share data. Such data are encapsulated in data-objects, which are instances of user-defined abstract data types. The implementation of Orca takes care of the physical distribution of objects among the local memories of the processors. In particular, an implementation may replicate and/or migrate objects in order to decrease access times to objects and increase parallelism. This paper gives a detailed description of the Orca language design and motivates the design choices. Orca is intended for applications programmers rather than systems programmers. This is reflected in its design goals to provide a simple, easy to use language that is type-secure and provides clean semantics. The paper discusses three example parallel applications in Orca, one of which is described in detail. It also describes one of the existing implementations, which is based on reliable broadcasting. Performance measurements of this system are given for three parallel applications. The measurements show that significant speedups can be obtained for all three applications. Finally, the paper compares Orca with several related languages and systems. 1.
621|A theory of primitive objects: Untyped and first-order systems|We introduce simple object calculi that support method override and object subsumption. We give an untyped calculus, typing rules, and equational rules. We illustrate the expressiveness of our calculi and the pitfalls that we avoid. 1.
622|Distributed Garbage Collection for Network Objects|In this report we present a fault-tolerant and efficient algorithm for distributed garbage collection and prove its correctness. The algorithm is a generalization of reference counting; it maintains a set of identifiers for processes with references to an object. The set is maintained with pair-wise communication between processes, so no global synchronization is required. The primary cost for maintaining the set is one remote procedure call when an object reference is transferred to a new process for the first time. The distributed collector collaborates with the local collector in detecting garbage; any local collector may be used, so long as it can be extended to provide notification when an object is collected. In fact, the distributed collector could be used without a local collector; in that case, the programmer would insert explicit dispose commands to release an object. The algorithm was designed and implemented as part of the Modula-3 network objects system, but it should be s...
623|A Library for Visualizing Combinatorial Structures|This report describes ANIM3D, a 3D animation library targeted at visualizing combinatorial structures. In particular, we are interested in algorithm animation. Constructing a new view for an algorithm typically takes dozens of design iterations, and can be very time-consuming. Our library eases the programmer&#039;s burden by providing high-level constructs for performing animations, and by offering an interpretive environment that eliminates the need for recompilations. This report also illustrates ANIM3D&#039;s expressiveness by developing a 3D animation of Dijkstra&#039;s shortest-path algorithm in just 70 lines of code. An accompanying videotape shows the library in use.  1 Background  Algorithm animation is concerned with visualizing the internal operations of a running program in such a way that the user gains some understanding of the workings of the algorithm. Due to lack of adequate hardware, early algorithm animation systems were restricted to black-and-white animations at low frame rates ...
624|The 1993 SRC Algorithm Animation Festival|This report describes the 1993 SRC Algorithm Animation Festival. The festival continues an experiment in developing algorithm animations bynon-experts,started the previous year, and described in SRC Research Report #98. This year nineteen researchers at Digital Equipment Corporation&#039;s Systems Research Center worked for two weeks on animating algorithms. Most of the participants had little (if any) experience writing programs that involved graphics. This report explains why we organized the festival, and describes the logistics of the festival and the advances in our algorithm animation system. This report presents the complete code for a simple, but non-trivial, animation of first-fit binpacking. Finally, this report contains snapshots from the animations produced during the festival.  1 Background  In SRC Research Report #98, we reported on the 1992 SRC Algorithm Animation Festival [2], held during the summer of &#039;92 at Digital Equipment Corporation&#039;s Systems Research Center (SRC). Th...
625|Preventing Recursion Deadlock in Concurrent Object-Oriented Systems|This paper presents solutions to the problem of deadlock due to recursion in concurrent objectoriented programming languages. Two language-independent, system-level mechanisms for solving this problem are proposed: a novel technique using multi-ported objects, and a namedthreads  scheme that borrows from previous work in distributed computing. We compare the solutions and present an analysis of their relative merits. Keywords: deadlock, recursion, object-oriented systems, programming languages, concurrency c fl Massachusetts Institute of Technology 1992 This work was supported in part by the National Science Foundation under grant CCR-8716884, by the Defense Advanced Research Projects Agency (DARPA) under Contract N00014-89-J1988, and by an equipment grant from Digital Equipment Corporation. Eric A. Brewer was supported by an Office of Naval Research Fellowship. Massachusetts Institute of Technology Laboratory for Computer Science Cambridge, Massachusetts  2 1 INTRODUCTION  1 Introduc...
626|Programming Semantics for Multiprogrammed Computations|... between an assembly language and an advanced algebraic language.  
627|Non-blocking Algorithms and Preemption-Safe Locking on Multiprogrammed Shared Memory Multiprocessors|Most multiprocessors are multiprogrammed in order to achieve acceptable response time and to increase their uti-lization. Unfortunately, inopportune preemption may significantly degrade the performance of synchronized parallel applications. To address this problem, researchers have developed two principal strategies for concurrent, atomic update of shared data structures: (1) preemption-safe locking and (2) non-blocking (lock-free) algorithms. Preemption-safe locking requires kernel support. Non-blocking algorithms generally require a universal atomic primitive such as compare-and-swap orload-linked/store-conditional, and are widely regarded as inefficient. We evaluate the performance of preemption-safe lock-based and non-blocking implementations of important data structures—queues, stacks, heaps, and counters—including non-blocking and lock-based queue algorithms of our own, in micro-benchmarks and real applications on a 12-processor SGI Challenge multiprocessor. Our results indicate that our non-blocking queue consistently outperforms the best known alternatives, and that data-structure-specific non-blocking algorithms, which exist for queues, stacks, and counters, can work extremely well. Not only do they outperform preemption-safe lock-based algorithms on multiprogrammed machines, they also outperform ordinary locks on dedicated machines. At the same time, since general-purpose non-blocking techniques do not yet appear to be practical, preemption-safe locks remain the preferred alternative for complex data structures: they outperform
628|Nonblocking k-compare-single-swap|The current literature offers two extremes of nonblocking software synchronization support for concurrent data struc-ture design: intricate designs of specific structures based on single-location operations such as compare-and-swap (CAS), and general-purpose multilocation transactional memory im-plementations. While the former are sometimes efficient, they are invariably hard to extend and generalize. The lat-ter are flexible and general, but costly. This paper aims at a middle ground: reasonably efficient multilocation operations that are general enough to reduce the design difficulties of algorithms based on CAS alone. We present an obstruction-free implementation of an atomic k-location-compare single-swap (KCSS) operation. KCSS allows for simple nonblocking manipulation of linked data structures by overcoming the key algorithmic difficulty in their design: making sure that while a pointer is be-ing manipulated, neighboring parts of the data structure remain unchanged. Our algorithm is efficient in the com-mon uncontended case: A successful k-location KCSS oper-ation requires only two CAS operations, two stores, and 2k noncached loads when there is no contention. We therefore believe our results lend themselves to efficient and flexible nonblocking manipulation of list-based data structures in today’s architectures.
629|ADEPT flex - Supporting Dynamic Changes of Workflows Without Loosing Control|. Today&#039;s workflow management systems (WFMSs) are only applicable in a secure and safe manner if the business process (BP) to be supported is well-structured and there is no need for ad hoc deviations at runtime. As only few BPs are static in this sense, this significantly limits the applicability of current workflow (WF) technology. On the other hand, to support dynamic deviations from premodeled task sequences must not mean that the responsibility for the avoidance of consistency problems and run-time errors is now completely shifted to the (naive) end user. In this paper we present a formal foundation for the support of dynamic structural changes of running WF instances. Based upon a formal WF model (ADEPT), we define a complete and minimal set of change operations (ADEPT flex ) that support users in modifying the structure of a running WF, while maintaining its (structural) correctness and consistency. The correctness properties defined by ADEPT are used to determine whether a spec...
630|Workflow Evolution|A basic step towards flexibility in workflow systems is the consistent and effective management of  workflow evolution, i.e., of changing existing workflows while they are operational. In this area, the most challenging issue is the handling of running instances when their schema is modified: simple solutions can be devised, but these often imply loosing all the work done or failing in capturing the advantages offered by workflow modifications; this is unacceptable for many applications. In this paper we address the problem of workflow evolution, from both a static and a dynamic point of view. We define a complete, minimal, and consistent set of modification primitives that allow modifications of workflow schemata and we introduce a taxonomy of policies to manage evolution of running instances when the corresponding workflow schema is modified. Formal criteria are introduced, based on a simple workflow conceptual model, in order to determine which running instances can be transparently...
631|Advanced Transaction Models in Workflow Contexts|In recent years, numerous transaction models have been proposed to address the problems posed by advanced database applications, but only a few of these models are being used in commercial products. In this paper, we make the case that such models may be too centered around databases to be useful in real environments. Advanced applications raise a variety of issues that are not addressed at all by transaction models. These same issues, however, are the basis for existing workflow systems, which are having considerable success as commercial products in spite of not having a solid theoretical foundation. We explore some of these issues and show that, in many aspects, workflow models are a superset of transaction models and have the added advantage of incorporating a variety of ideas that to this date have remained outside the scope of traditional transaction processing.  
632|The Workflow Activity Model WAMO|Workflow technology has not yet lived up to its expectations not only because of social problems but also because of technical problems, like inflexible and rigid process specification and execution mechanisms and insufficient possibilities to handle exceptions. The aim of this paper is to present a workflow model which significantly facilitates the design and reliable management of complex business processes supported by an automatic mechanism to handle exceptions. The strength of the model is its simplicity and the application independent transaction facility (advanced control mechanism for workflow units) which guarantees reliable execution of workflow activities. 1 Introduction  Among cooperative information systems, workflow management is one of the key technologies for providing efficiency and effectiveness in the office. A Workflow Management System (WFMS) is a system that completely defines, manages and executes workflow processes through the execution of software whose order o...
633|A Formal Foundation for Distributed Workflow Execution Based on State Charts|. This paper provides a formal foundation for distributed workflow executions. The state chart formalism is adapted to the needs of a workflow model in order to establish a basis for both correctness rea-  soning and run--time support for complex and large--scale workflow applications. To allow for the distributed execution of a workflow across  different workflow servers, which is required for scalability and organizational decentralization, a method for the partitioning of workflow specifications is developed. It is proven that the partitioning preserves the  original state chart&#039;s behavior. 1 Introduction  Workflow management is a rapidly growing research and development area of very high practical relevance [GHS95, Mo96, VB96, WfMC95, Sh96]. Typical examples of  (semi--automated) workflows are the processing of a credit request in a bank, the edito-  rial handling and refereeing process for papers in an electronic journal, or the medical treatment of patients in a hospital. Informa...
634|Verification Problems in Conceptual Workflow Specifications|Most of today&#039;s business requirements can only be accomplished through integration of various autonomous systems which were initially designed to serve the needs of particular applications. In the literature workflows are proposed to design these kinds of applications. The key tool for designing such applications is a powerful conceptual specification language. Such a language should be capable of capturing interactions and cooperation between component tasks of workflows among others. These include sequential execution, iteration, choice, parallelism and synchronisation. The central focus of this paper is the verification  of such process control aspects in conceptual workflow specifications. As it is generally agreed upon that the later in the software development process an error is detected, the more it will cost to correct it, it is of vital importance to detect errors as early as possible in the systems development process. In this paper some typical verification problems in work...
635|Workflow Applications to Research Agenda: Scalable and Dynamic Work Coordination and Collaboration Systems|A workflow is an activity involving the coordinated execution of multiple tasks performed by different processing entities [KS95]. These tasks could be manual, or automated, either created specifically for the purpose of the workflow application being developed, or possibly already existing as legacy
636|DYNAMITE: Dynamic Task Nets for Software Process Management|Managing the software development and maintenance process has been identified as a great challenge for several years. Software processes are highly dynamic and can only rarely be planned completely in advance. Dynamic task nets take this into account. They are built and modified incrementally as a software process is executed. Dynamic task nets have been designed to solve important problems of process dynamics, including product-dependent structural evolution, feedbacks, and concurrent engineering. In order to describe editing and enactment (and their interaction) in a uniform way, task nets are formally defined by means of a programmed graph rewriting system. 
637|ConTracts - A Low-Level Mechanism for Building General-Purpose Workflow Management-Systems|this paper, is characterized by three very simple insights: . Extending the transaction model does not solve the problem, because a long-lived computation is much more complex than an ACID transaction. On the other hand, nobody is helped by a type of transaction, which is neither A, nor C, nor I, nor D. . Managing long-lived computations requires more systematic approaches than pragmatic extensions to existing operating systems and TP-monitors. . Apart from its close relationship to databases, handling long-lived computations is a problem of proper language abstractions. Whereas simple modules (and transaction programs, for that matter) have a system residence time of 10
638|WorkFlow Systems: a few definitions and a few suggestions|This paper hopes to make a contribution on three aspects of workflow systems: we stress the fact that there is a broken symetry between the level of the specification of the procedures and the level of their enactment; we propose some ways of classifying activities and exceptions; and we propose some run-time functionalities to help users deal with exceptions.  
639|A Distributed Execution Environment for Large-Scale Workflow Management Systems with Subnets and Server Migration|If the number of users within a workflow management system (WFMS) increases, a central workflow server (WF-- server) and a single local area network (LAN) may become overloaded. The approach presented in this paper describes an execution environment which is able to manage a growing number of users by adding new servers and subnets. The basic idea is to decompose processes into parts which are controlled by different WF--servers. That is, during the execution of a workflow instance its execution (step) control may migrate from one WF--server to another. By selecting the appropriate physical servers (for hosting the WF--servers) in the appropriate LANs, communication costs and individual WF--server workload can be reduced significantly. 1. Introduction  Since a couple of years there has been a growing interest in using WFMS for implementing process--oriented application systems. As the benefit of such application systems increases with the number of applications being served, the number...
640|A Framework for Dynamic Changes in Workflow Management Systems|Current workflow management systems (WFMSs) are only applicable in a reliable and secure manner, if the business process (BP) to be supported is wellstructured. As ad hoc deviations from preplanned BPs are rather the norm and form a key part of process flexibility, this limits the applicability of today&#039;s workflow (WF) technology significantly. In this paper we present a framework for the support of ad hoc structural changes of WFs. Basic to our approach is a conceptual, graph-based WF model which has a formal foundation in its syntax and operational semantics. Based upon this model we have developed a complete and minimal set of change operations which support users in modifying the structure of WFs at runtime, while preserving their correctness and consistency. 1. Introduction  WF technology offers a promising approach for the realization of process-centered application systems. A characteristic feature of &#034;real&#034; WFMSs (as opposed to e.g., Lotus Notes) is the separation of the applic...
641|Experiences with the DOMINO office procedure system|Abstract: The Domino office procedure system has been equipped with a new user interface, and has been put to use for the support of purchasing. In this paper, we describe the system, the user interface, and the experiences we made during the practical use of the system. We also briefly discuss the consequences for our own research. 1
642|WIDE Workflow model and architecture|Workflow management is emerging as a challenging area for databases, stressing database technology beyond its current capabilities. Workflow management systems need to be more integrated with data management technology, in particular as it concerns the access to external databases and as a support technology for workflow management, to support intelligent exception handling and transaction management. Thus, a convergence between workflow management and databases is occurring. In order to make such convergence effective, however, it is required to improve and strengthen the specification of workflows at the conceptual level, by formalizing within a unique model their &#034;internal behavior&#034; (e.g., interaction and cooperation between tasks), their relationship to the environment (e.g., assignment of work task to agents) and the access to external databases. In the WIDE (Workflow on Intelligent and Distributed database Environment) project 1 , a rich conceptual model is proposed, including ...
643|Beyond Formal Processes: Augmenting Workflow with Group Interaction Techniques|The main scope of workflow systems has been the automation of formal procedures in the workplace. On the other hand, Communication and Group Support systems have addressed the informal aspects of organizational interactions. We argue that the formal versus informal separation is artificial and a cause of systems ineffectiveness. This paper proposes an approach to increase mutual awareness when integrating support for workflow systems and group interaction techniques. KEYWORDS Workflow systems, group interaction, group decision support systems. 1 INTRODUCTION The intensive and growing use of information technologies in the workplace and organizational settings has been a worldwide trend in the last decades. The investment in technology made by companies and institutions has however been questioned regarding its effectiveness in terms of productivity growth [7]. The management theories and practices have evolved in a scenario of coexistence between human, social and economic factors ...
644|G.: Flexible Real-time Meeting Support for Workflow Management Systems|uni-ulm.de Two different areas of distributed group work are supported by workflow management systems and by real-time meeting systems. Workflow management is best suited for tasks which proceed in a predefinable way, and whose recurring appearance justify the cost of an a priori modelling. A problem of a priori modelling is that such models are static. Activities which arise spontaneously cannot be easily covered. Real-time meeting support systems are best suited for such ad-hoc group activities. Audiovisual connectivity and shared documents enable flexible group processes. To combine the advantages of both types of systems an integration concept and prototype system is introduced. Meetings are embedded into workflows using different categories of meeting models. Monitoring and checklisting support the course of meetings. The coupling of workflow management and meeting support is achieved by a meeting broker. 1.
645|Interaction Expressions -- A Powerful Formalism for Describing Inter-Workflow Dependencies|Current workflow management technology does not provide adequate means for describing and implementing workflow ensembles, i. e., dynamically evolving collections of more or less independent workflows which have to synchronize only now and then. Interaction expressions are proposed as a simple yet powerful formalism to remedy this shortcoming. Besides operators for sequential composition, iteration, and selection, which are well-known from regular expressions, they provide parallel composition and iteration, conjunction, and several advanced features like parametric expressions, multipliers, and quantifiers. The paper introduces interaction expressions semi-formally, gives examples of their typical use, and describes their implementation and integration with state-of-the-art workflow technology. Major design principles, such as orthogonality and implicit and predictive choice, are discussed and compared with several related approaches. 1.
646|An Overview|We show that the expected time for a random walk on a (multi-)graph G to traverse all m edges of G, and return to its starting point, is at most 2m 2; if each edge must be traversed in both directions, the bound is 3m 2. Both bounds are tight and may be applied to graphs with arbitrary edge lengths, with implications for Brownian motion on a finite or infinite network of total edge-length m.
647|Practical Byzantine fault tolerance and proactive recovery|Our growing reliance on online services accessible on the Internet demands highly available systems that provide correct service without interruptions. Software bugs, operator mistakes, and malicious attacks are a major cause of service interruptions and they can cause arbitrary behavior, that is, Byzantine faults. This article describes a new replication algorithm, BFT, that can be used to build highly available systems that tolerate Byzantine faults. BFT can be used in practice to implement real services: it performs well, it is safe in asynchronous environments such as the Internet, it incorporates mechanisms to defend against Byzantine-faulty clients, and it recovers replicas proactively. The recovery mechanism allows the algorithm to tolerate any number of faults over the lifetime of the system provided fewer than 1/3 of the replicas become faulty within a small window of vulnerability. BFT has been implemented as a generic program library with a simple interface. We used the library to implement the first Byzantine-fault-tolerant NFS file system, BFS. The BFT library and BFS perform well because the library incorporates several important optimizations, the most important of which is the use of symmetric cryptography to authenticate messages. The performance results show that BFS performs 2 % faster to 24 % slower than production implementations of the NFS protocol that are not replicated. This supports our claim that the
648|The Byzantine Generals Problem|Reliable computer systems must handle malfunctioning components that give conflicting information to different parts of the system. This situation can be expressed abstractly in terms of a group of generals of the Byzantine army camped with their troops around an enemy city. Communicating only by messenger, the generals must agree upon a common battle plan. However, one of more of them may be traitors who will try to confuse the others. The problem is to find an algorithm to ensure that the loyal generals will reach agreement. It is shown that, using only oral messages, this problem is solvable if and only if more than two-thirds of the generals are loyal; so a single traitor can confound two loyal generals. With unforgeable written messages, the problem is solvable for any number of generals and possible traitors. Applications of the solutions to reliable computer systems are then discussed.
649|Multicast Routing in Datagram Internetworks and Extended LANs|Multicasting, the transmission of a packet to a group of hosts, is an important service for improving the efficiency and robustness of distributed systems and applications. Although multicast capability is available and widely used in local area networks, when those LANs are interconnected by store-and-forward routers, the multicast service is usually not offered across the resulting internetwork. To address this limitation, we specify extensions to two common internetwork routing algorithms-distance-vector routing and link-state routing-to support low-delay datagram multicasting beyond a single LAN. We also describe modifications to the single-spanning-tree routing algorithm commonly used by link-layer bridges, to reduce the costs of multicasting in large extended LANs. Finally, we discuss how the use of multicast scope control and hierarchical multicast routing allows the multicast service to scale up to large internetworks.
650|Practical Byzantine Fault Tolerance |This paper describes a new replication algorithm that is able to tolerate Byzantine faults. We believe that Byzantinefault-tolerant algorithms will be increasingly important in the future because malicious attacks and software errors are increasingly common and can cause faulty nodes to exhibit arbitrary behavior. Whereas previous algorithms assumed a synchronous system or were too slow to be used in practice, the algorithm described in this paper is practical: it works in asynchronous environments like the Internet and incorporates several important optimizations that improve the response time of previous algorithms by more than an order of magnitude. We implemented a Byzantine-fault-tolerant NFS service using our algorithm and measured its performance. The results show that our service is only 3 % slower than a standard unreplicated NFS.  
651|Reaching Agreement in the Presence of Faults| The problem addressed here concerns a set of isolated processors, some unknown subset of which may be faulty, that communicate only by means of two-party messages. Each nonfaulty processor has a private value of reformation that must be communicated to each other nonfanlty processor. Nonfaulty processors always communicate honestly, whereas faulty processors may lie The problem is to devise an algorithm in which processors communicate their own values and relay values received from others that allows each nonfaulty processor to refer a value for each other processor The value referred for a nonfanlty processor must be that processor&#039;s private value, and the value inferred for a faulty one must be consistent wRh the corresponding value inferred by each other nonfanlty processor It is shown that the problem is solvable for, and only for, n&gt; _ 3m + 1, where m IS the number of faulty processors and n is the total number. It is also shown that if faulty processors can refuse to pass on reformation but cannot falsely relay information, the problem is solvable for arbitrary n _&gt; m _&gt; 0. This weaker assumption can be approxunated m practice using cryptographic methods.
652|Design and Implementation or the Sun Network Filesystem|this paper we discuss the design and implementation of the/&#039;fiesystem interface in the kernel and the NF$ virtual/&#039;fiesystem. We describe some interesting design issues and how they were resolved, and point out some of the shortcomings of the current implementation. We conclude with some ideas for future enhancements
653|Small Byzantine Quorum Systems|In this paper we present two protocols for asynchronous Byzantine Quorum Systems (BQS) built on top of reliable channels---one for self-verifying data and the other for any data. Our protocols tolerate Byzantine failures with fewer servers than existing solutions by eliminating nonessential work in the write protocol and by using read and write quorums of different sizes. Since engineering a reliable network layer on an unreliable network is difficult, two other possibilities must be explored. The first is to strengthen the model by allowing synchronous networks that use time-outs to identify failed links or machines. We consider running synchronous and asynchronous Byzantine Quorum protocols over synchronous networks and conclude that, surprisingly, &#034;self-timing&#034; asynchronous Byzantine protocols may offer significant advantages for many synchronous networks when network time-outs are long. We show how to extend an existing Byzantine Quorum protocol to eliminate its dependency on reliable networking and to handle message loss and retransmission explicitly.
654|The exact security of digital signatures: How to sign with RSA and Rabin|We describe an RSA-based signing scheme called PSS which combines essentially optimal efficiency with attractive security properties. Signing takes one RSA decryption plus some hashing, verification takes one RSA encryption plus some hashing, and the size of the signature is the size of the modulus. Assuming the underlying hash functions are ideal, our schemes are not only provably secure, but are so in a tight way — an ability to forge signatures with a certain amount of computational resources implies the ability to invert RSA (on the same size modulus) with about the same computational effort. Furthermore, we provide a second scheme which maintains all of the above features and in addition provides message recovery. These ideas extend to provide schemes for Rabin signatures with analogous properties; in particular their security can be tightly related to the hardness of factoring.
655|Experimental Quantum Cryptography|We describe results from an apparatus and protocol designed to implement  quantum key distribution, by which two users, who share no secret information initially: 1) exchange a random quantum transmission, consisting of very faint flashes of polarized light; 2) by subsequent public discussion of the sent and received versions of this transmission estimate the extent of eavesdropping that might have taken place on it, and finally 3) if this estimate is small enough, distill from the sent and received versions a smaller body of shared random information, which is certifiably secret in the sense that any third party&#039;s expected information on it is an exponentially small fraction of one bit. Because the system depends on the uncertainty principle of quantum physics, instead of usual mathematical assumptions such as the difficulty of factoring, it remains secure against an adversary with unlimited computing power.   A preliminary version of this paper was presented at Eurocrypt &#039;90, May 21 ...
656|Separating key management from file system security|No secure network file system has ever grown to span the In-ternet. Existing systems all lack adequate key management for security at a global scale. Given the diversity of the In-ternet, any particular mechanism a file system employs to manage keys will fail to support many types of use. We propose separating key management from file system security, letting the world share a single global file system no matter how individuals manage keys. We present SFS, a se-cure file system that avoids internal key management. While other file systems need key management to map file names to encryption keys, SFS file names effectively contain public keys, making them self-certifying pathnames. Key manage-ment in SFS occurs outside of the file system, in whatever procedure users choose to generate file names. Self-certifying pathnames free SFS clients from any notion of administrative realm, making inter-realm file sharing triv-ial. They let users authenticate servers through a number of different techniques. The file namespace doubles as a key certification namespace, so that people can realize many key management schemes using only standard file utilities. Fi-nally, with self-certifying pathnames, people can bootstrap one key management mechanism using another. These prop-erties make SFS more versatile than any file system with built-in key management.  
657|PROACTIVE SECRET SHARING Or: How to Cope With Perpetual Leakage|Secret sharing schemes protect secrets by distributing them over different locations  (share holders). In particular, in k out of n threshold schemes, security is assured if  throughout the entire life-time of the secret the adversary is restricted to compromise  less than k of the n locations. For long-lived and sensitive secrets this protection may  be insufficient.  We propose an efficient proactive secret sharing scheme, where shares are periodically  renewed (without changing the secret) in such a way that information gained by  the adversary in one time period is useless for attacking the secret after the shares  are renewed. Hence, the adversary willing to learn the secret needs to break to all k  locations during the same time period (e.g., one day, a week, etc.). Furthermore, in order  to guarantee the availability and integrity of the secret, we provide mechanisms to  detect maliciously (or accidentally) corrupted shares, as well as mechanisms to secretly  recover the correct...
660|Asynchronous Consensus and Broadcast Protocols|Abstract. A consensus protocol enables a system of n asynchronous processes, some of which are faulty, to reach agreement. There are two kinds of faulty processes: fail-stop processes that can only die and malicious processes that can also send false messages. The class of asynchronous systems with fair schedulers is defined, and consensus protocols that terminate with probability I for these systems are investigated. With fail-stop processes, it is shown that r(n + 1)/21 correct processes are necessary and sufficient to reach agreement. In the malicious case, it is shown that r(2n + 1)/31 correct processes are necessary and sufficient to reach agreement. This is contrasted with an earlier result, stating that there is no consensus protocol for the fail-stop case that always terminates within a bounded number of steps, even if only one process can fail. The possibility of reliable broadcast (Byzantine Agreement) in asynchronous systems is also investigated. Asynchronous Byzantine Agreement is defined, and it is shown that I(2n + 1)/31 correct processes are necessary and sufficient to achieve it.
661|The Rampart toolkit for building high-integrity services|Abstract. Rampart is a toolkit of protocols to facilitate the development ofhigh-integrity services, i.e., distributed services that retain their availability and correctness despite the malicious penetration of some component servers by an attacker. At the core of Rampart are new protocols that solve several basic problems in distributed computing, including asynchronous group membership, reliable multicast (Byzantine agreement), and atomic multicast. Using these protocols, Rampart supports the development of high-integrity services via the technique of state machine replication, and also extends this technique with a new approach to server output voting. In this paper we give a brief overview of Rampart, focusing primarily on its protocol architecture. We also sketch its performance in our prototype implementation and ongoing work. 1
662|UMAC: Fast and Secure Message Authentication|Abstract. We describe a message authentication algorithm, UMAC, which can authenticate messages (in software, on contemporary machines) roughly an order of magnitude faster than current practice (e.g., HMAC-SHA1), and about twice as fast as times previously reported for the universal hash-function family MMH. To achieve such speeds, UMAC uses a new universal hash-function family, NH, and a design which allows effective exploitation of SIMD parallelism. The “cryptographic ” work of UMAC is done using standard primitives of the user’s choice, such as a block cipher or cryptographic hash function; no new heuristic primitives are developed here. Instead, the security of UMAC is rigorously proven, in the sense of giving exact and quantitatively strong results which demonstrate an inability to forge UMAC-authenticated messages assuming an inability to break the underlying cryptographic primitive. Unlike conventional, inherently serial MACs, UMAC is parallelizable, and will have ever-faster implementation speeds as machines offer up increasing amounts of parallelism. We envision UMAC as a practical algorithm for next-generation message authentication. 1
663|The SecureRing Protocols for Securing Group Communication|The SecureRing group communication protocols provide reliable ordered message delivery and group membership services despite Byzantine faults such as might be caused by modifications to the programs of a group member following illicit access to, or capture of, a group member. The protocols multicast messages to groups of processors within an asynchronous distributed system and deliver messages in a consistent total order to all members of the group. They ensure that correct members agree on changes to the membership, that correct processors are eventually included in the membership, and that processors that exhibit detectable Byzantine faults are eventually excluded from the membership. To provide these message delivery and group membership services, the protocols make use of an unreliable Byzantine fault detector. 1.
664|Checking the Correctness of Memories|We extend the notion of program checking to include programs which alter their  environment. In particular, we consider programs which store and retrieve data from  memory. The model we consider allows the checker a small amount of reliable memory.  The checker is presented with a sequence of requests (on-line) to a data structure which  must reside in a large but unreliable memory. We view the data structure as being  controlled by an adversary. We want the checker to perform each operation in the input  sequence using its reliable memory and the unreliable data structure so that any error  in the operation of the structure will be detected by the checker with high probability.  We present checkers for various data structures. We prove lower bounds of log n  on the amount of reliable memory needed by these checkers where n is the size of  the structure. The lower bounds are information theoretic and apply under various  assumptions. We also show time-space tradeoffs for checking random access memories  as a generalization of those for coherent functions.  1 
665|SIFT: Design and Analysis of a Fault-Tolerant Computer for Aircraft Control|ldtmdme Coreputer fa criticd.ircnlt caltrd appkdom that rhievesf.ulttdenncebytherep€hthoft&amp;aamongproedng units. The rmin procesing units are off-the-shelf minicomputers, with sturdud microcomputers serving as the interface to the YO systean.
666|A new paradigm for collision-free hashing: incrementality at reduced cost|We present a simple, new paradigm for the design of collision-free hash functions. Any function emanating from this paradigm is incremental. (This means that if a message x which Ihave previously hashed is modi ed to x 0 then rather than having to re-compute the hash of x 0 from scratch, I can quickly \update &amp;quot; the old hash value to the new one, in time proportional to the amount of modi cation made in x to get x 0.) Also any function emanating from this paradigm is parallelizable, useful for hardware implementation. We derive several speci c functions from our paradigm. All use a standard hash function, assumed ideal, and some algebraic operations. The rst function, MuHASH, uses one modular multiplication per block of the message, making it reasonably e cient, and signi cantly faster than previous incremental hash functions. Its security is proven, based on the hardness of the discrete logarithm problem. A second function, AdHASH, is even faster, using additions instead of multiplications, with security proven given either that approximation of the length of shortest lattice vectors is hard or that the weighted subset sum problem is hard. A third function, LtHASH, is a practical variant of recent lattice based functions, with security proven
667|Axioms for concurrent objects|The copyright law of the United States (title 17, U.S. Code) governs the making of photocopies or other reproductions of copyrighted material. Any copying of this document without permission of its author may be prohibited by law.
668|Using Time Instead of Timeout for Fault-Tolerant Distributed Systems|SRI International A general method is described for implementing a distributed system with any desired degree of fault-tolerance. Instead of relying upon explicit timeouts, processes execute a simple clock-driven algorithm. Reliable clock synchronization and a solution to the Byzantine Generals Problem are assumed.
669|Proactive Public Key and Signature Systems|Emerging applications like electronic commerce and secure communications over open networks have made clear the fundamental role of public key cryptography as a unique enabler for world-wide scale security solutions. On the other hand, these solutions clearly expose the fact that the protection of private keys is a security bottleneck in these sensitive applications. This problem is further worsened in the cases where a single and unchanged private key must be kept secret for very long time (such is the case of certification authority keys, bank and e-cash keys, etc.). One crucial defense against exposure of private keys is offered by threshold cryptography where the private key functions (like signatures or decryption) are distributed among several parties such that a predetermined number of parties must cooperate in order to correctly perform these operations. This protects keys from any single point of failure. An attacker needs to break into a multiplicity of locations before it ca...
670|Secure and Scalable Replication in Phalanx|)   Dahlia Malkhi Michael K. Reiter AT&amp;T Labs Research, Florham Park, NJ, USA  fdalia,reiterg@research.att.com Abstract  Phalanx is a software system for building a persistent, survivable data repository that supports shared data abstractions (e.g., variables, mutual exclusion) for clients. Phalanx implements data abstractions that ensure useful properties without trusting the servers supporting these abstractions or the clients accessing them, i.e., Phalanx can survive even the arbitrarily malicious corruption of clients and (some number of) servers. At the core of the system are survivable replication techniques that enable efficient scaling to hundreds of Phalanx servers. In this paper we describe the implementation of some of the data abstractions provided by Phalanx, discuss their ability to scale to large systems, and describe an example application. 1. Introduction  In this paper we introduce Phalanx, a software system for building persistent services that support shared data ab...
671|Viewstamped replication: A new primary copy method to support highly available distrbuted systems|One of the potential benefits of distributed systems is their use in providing highly-available services that are likely to be usable when needed. Availabilay is achieved through replication. By having inore than one copy of information, a service continues to be usable even when some copies are inaccessible, for example, because of a crash of the computer where a copy was stored. This paper presents a new replication algorithm that has desirable performance properties. Our approach is based on the primary copy technique. Computations run at a primary. which notifies its backups of what it has done. If the primary crashes, the backups are reorganized, and one of the backups becomes the new primary. Our method works in a general network with both node crashes and partitions. Replication causes little delay in user computations and little information is lost in a reorganization; we use a special kind of timestamp called a viewstamp to detect lost information. 1
672|Unreliable Intrusion Detection in Distributed Computations|Distributed coordination is difficult, especially when the system may suffer intrusions that corrupt some component processes. In this paper we introduce the abstraction of a failure detector that a process can use to (imperfectly) detect the corruption (Byzantine failure) of another process. In general, our failure detectors can be unreliable, both by reporting a correct process to be faulty or by reporting a faulty process to be correct. However, we show that if these detectors satisfy certain plausible properties, then the well-known distributed consensus problem can be solved. We also present a randomized protocol using failure detectors that solves the consensus problem if either the requisite properties of failure detectors hold or if certain highly probable events eventually occur. This work can be viewed as a generalization of benign failure detectors popular in the distributed computing literature. 1 Introduction  In this paper we consider how to defend the integrity of a dist...
673|Efficient Message Ordering in Dynamic Networks|We present an algorithm for totally ordering messages in the face of network partitions and site failures. The algorithm always allows a majority of connected processors in the network to make progress (i.e. to order messages), if they remain connected for sufficiently long, regardless of past failures. Furthermore, our algorithm always allows processors to initiate messages, even when they are not members of a connected majority component in the network. Thus, messages can eventually become totally ordered even if their initiator is never a member of a majority component. The algorithm guarantees that when a majority is connected, each message is ordered within two communication rounds, if no failures occur during these rounds.  1 Introduction  Consistent order is a powerful paradigm for the design of fault tolerant applications, e.g. consistent replication [Sch90, Kei94]. We present an efficient algorithm for consistent message ordering in the face of network partitions and site fail...
674|A Security Risk of Depending on Synchronized Clocks|. Many algorithms or protocols, in particular cryptographic protocols such as authentication protocols, use synchronized clocks and depend on them for correctness. This note describes a scenario where a clock synchronization failure renders a protocol vulnerable to an attack even after the faulty clock has been resynchronized. The attack exploits a postdated message by first suppressing it and replaying it later. 1 Introduction  Synchronized clocks have become a reality in distributed systems. Many algorithms or protocols use them to improve performance; some depend on them for correctness [7]. This note is particularly concerned with cryptographic protocols, such as some authentication protocols, which depend on synchronized clocks to timestamp messages so that the recipients can verify the timeliness of the messages and recognize and reject replays of messages communicated in the past [2, 4, 8]. Clocks can become unsynchronized due to sabotage on or faults in the clocks or the synchr...
675|An Architecture for Survivable Coordination in Large Distributed Systems|Coordination among processes in a distributed system can be rendered very complex in a large-scale system where messages may be delayed or lost, and when processes may participate only transiently or behave arbitrarily, e.g., after suffering a security breach. In this paper, we propose a scalable architecture to support coordination in such extreme conditions. Our architecture consists of a collection of persistent data servers that implement simple shared data abstractions for clients, without trusting the clients or even the servers themselves. We show that by interacting with these untrusted servers, clients can solve distributed consensus, a powerful and fundamental coordination primitive. Our architecture is very practical, and we describe the implementation of its main components in a system called Phalanx. 1 Introduction  In this paper we propose a system architecture that supports efficient and scalable coordination among distributed clients. Our architecture strives to support...
676|A High-Throughput Secure Reliable Multicast Protocol|A (secure) reliable multicast protocol enables a process to multicast a message to a group of processes in a way that ensures that all honest destination-group members receive the same message, even if some group members and the multicast initiator are maliciously faulty. Reliable multicast has been shown to be useful for building multiparty cryptographic protocols and secure distributed services. We present a high-throughput reliable multicast protocol that tolerates the malicious behavior of up to fewer than one-third of the group members. Our protocol achieves high-throughput using a novel technique for chaining multicasts, whereby the cost of ensuring agreement on each multicast message is amortized over many multicasts. This is coupled with a novel flow-control mechanism that yields low multicast latency. 1. Introduction  Reliable multicast is a fundamental communication protocol that underlies many forms of secure distributed computation. A (secure) reliable multicast protocol en...
677|Maintaining Authenticated Communication in the Presence of Break-ins|We study the problem of maintaining authenticated communication over untrusted communication  channels, in a scenario where the communicating parties may be occasionally and  repeatedly broken into for transient periods of time. Once a party is broken into, its cryptographic  keys are exposed and perhaps modified. Yet, we want parties whose security is thus  compromised to regain their ability to communicate in an authenticated way aided by other  parties. In this work we present a mathematical model for this highly adversarial setting, exhibiting  salient properties and parameters, and then describe a practically-appealing protocol  for the task of maintaining authenticated communication in this model.  A key element in our solution is devising proactive distributed signature (PDS) schemes in  our model. Although PDS schemes are known in the literature, they are all designed for a model  where authenticated communication and broadcast primitives are available. We therefore show  how t...
678|Fault Detection for Byzantine Quorum Systems|In this paper we explore techniques to detect Byzantine server failures in asynchronous  replicated data services. Our goal is to detect arbitrary failures of data  servers in a system where each client accesses the replicated data at only a subset  (quorum) of servers in each operation. In such a system, some correct servers can be  out of date after a write and can therefore return values other than the most up-to-date  value in response to a client&#039;s read request, thus complicating the task of determining  the number of faulty servers in the system at any point in time. We initiate the study  of detecting server failures in this context, and propose two statistical approaches for  estimating the risk posed by faulty servers based on responses to read requests.
679|Muteness Failure Detectors: Specification and Implementation|This paper extends the failures detector approach from crash-stop failures to muteness failures. Muteness failures are malicious failures in which a process stops sending algorithm messages, but might continue to send other messages, e.g., &#034;I-am-alive&#034; messages. The paper presents both the specification of a muteness failure detector, denoted by 3MA , and an implementation of 3MA in a partial synchrony model (there are bounds on message latency and clock skew, but these bounds are unknown and hold only after some point that is itself unknown). We show that, modulo a simple modification, a consensus algorithm that has been designed in a crash-stop model with 3S, can be reused in the presence of muteness failures simply by replacing 3MA with 3S. Key words: failure detectors, Byzantine failures, muteness failures, asynchronous systems.   &#039; Ecole Polytechnique F&#039;ed&#039;erale, Lausanne (Switzerland). Email: assia.doudou@epfl.ch.  y  United Bank of Switzerland, Zurich (Switzerland). Email: benoi...
680|Backoff protocols for distributed mutual exclusion and ordering|We present a simple and efficient protocol for mutual exclusion in synchronous, message-passing distributed systems subject to failures. Our protocol borrows design principles from prior work in backoff protocols for multiple access channels such as Ethernet. Our protocol is adaptive in that the expected amortized system response time— informally, the average time a process waits before entering the critical section—is a function only of the number of clients currently contending and is independent of the maximum number of processes who might contend. In particular, in the contention-free case, a process can enter the critical section after only one round-trip message delay. We use this protocol to derive a protocol for ordering operations on a replicated object in an asynchronous distributed system subject to failures. This protocol is always safe, is probabilistically live during periods of stability, and is suitable for deployment in practical systems. 1
681|A Correctness Proof for a Practical Byzantine-Fault-Tolerant Replication Algorithm|This paper presents a formal specification for the unoptimized version of our algorithm presented in Section 4 of [4] and proves its safety (but not its liveness.) The specification uses the I/O automaton formalism of Tuttle an Lynch [8] and the proof is based on invariant assertions and simulation relations
682|Increasing the Resilience of Distributed and Replicated Database Systems|This paper presents a new atomic commitment protocol, enhanced three phase commit  (E3PC ), that always allows a quorum in the system to make progress. Previously suggested quorum-based protocols (e.g., the quorum-based three phase commit (3PC) [Ske82]) allow a quorum to make progress in case of one failure. If failures cascade, however, and the quorum in the system is &#034;lost&#034; (i.e., at a given time no quorum component exists), a quorum can later become connected and still remain blocked. With our protocol, a connected quorum never blocks. E3PC is based on the quorum-based 3PC [Ske82], and it does not require more time or communication than 3PC. We describe how this protocol can be exploited in a replicated database setting, making the database always available to a majority of the sites. 1 Introduction  Reliability and availability of loosely coupled distributed database systems are becoming requirements for many installations, and fault tolerance is becoming an important aspect of dis...
683|Abstractions for devising Byzantine-resilient state machine replication|State machine replication is a common approach for making a distributed service highly available and resilient to failures, by replicating it on different processes. It is well-known, however, that the difficulty of ensuring the safety and liveness of a replicated service increases significantly when no synchrony assumptions are made, and when processes can exhibit Byzantine behaviors. The contribution of this work is to break the complexity of devising a Byzantine-resilient state machine replication protocol, by decomposing it into key modular abstractions. In addition to being modular, the protocol we propose always preserves safety in presence of less than one third of Byzantine processes, independently of any synchrony assumptions. As for the liveness of our protocol, it relies on a Byzantine failure detector that encapsulates the sufficient amount of synchrony. • Submission Categories: fault-tolerant systems, secure systems, distributed algorithms.
684|Flow-Sensitive Type Qualifiers|We present a system for extending standard type systems with flow-sensitive type qualifiers. Users annotate their programs with type qualifiers, and inference checks that the annotations are correct. In our system only the type qualifiers are modeled flow-sensitively - the underlying standard types are unchanged, which allows us to obtain an efficient constraint-based inference algorithm that integrates flow-insensitive alias analysis, effect inference, and ideas from linear type systems to support strong updates. We demonstrate the usefulness of flow-sensitive type qualifiers by finding a number of new locking bugs in the Linux kernel.
685|A theory of type polymorphism in programming|The aim of this work is largely a practical one. A widely employed style of programming, particularly in structure-processing languages which impose no discipline of types, entails defining procedures which work well on objects of a wide variety. We present a formal type discipline for such polymorphic procedures in the context of a simple pro-gramming language, and a compile time type-checking algorithm w which enforces the discipline. A Semantic Soundness Theorem (based on a formal semantics for the language) states that well-type programs cannot “go wrong ” and a Syntactic Soundness Theorem states that if fl accepts a program then it is well typed. We also discuss extending these results to richer languages; a type-checking algorithm based on w is in fact already implemented and working, for the metalanguage ML in the Edinburgh LCF system, 1.
686|Extended Static Checking for Java|Software development and maintenance are costly endeavors. The cost can be reduced if more software defects are detected earlier in the development cycle. This paper introduces the Extended Static Checker for Java (ESC/Java), an experimental compile-time program checker that finds common programming errors. The checker is powered by verification-condition generation and automatic theoremproving techniques. It provides programmers with a simple annotation language with which programmer design decisions can be expressed formally. ESC/Java examines the annotated software and warns of inconsistencies between the design decisions recorded in the annotations and the actual code, and also warns of potential runtime errors in the code. This paper gives an overview of the checker architecture and annotation language and describes our experience applying the checker to tens of thousands of lines of Java programs.
688|Efficient Context-Sensitive Pointer Analysis for C Programs|This paper proposes an efficient technique for contextsensitive pointer analysis that is applicable to real C programs. For efficiency, we summarize the effects of procedures using partial transfer functions. A partial transfer function (PTF) describes the behavior of a procedure assuming that certain alias relationships hold when it is called. We can reuse a PTF in many calling contexts as long as the aliases among the inputs to the procedure are the same. Our empirical results demonstrate that this technique is successful---a single PTF per procedure is usually sufficient to obtain completely context-sensitive results. Because many C programs use features such as type casts and pointer arithmetic to circumvent the high-level type system, our algorithm is based on a low-level representation of memory locations that safely handles all the features of C. We have implemented our algorithm in the SUIF compiler system and we show that it runs efficiently for a set of C benchmarks. 1 Introd...
689|Context-Sensitive Interprocedural Points-to Analysis in the Presence of Function Pointers|This paper reports on the design, implementation, and empirical results of a new method for dealing with the aliasing problem in C. The method is based on approximating the points-to relationships between accessible stack locations, and can be used to generate alias pairs, or used directly for other analyses and transformations. Our method provides context-sensitive interprocedural information based on analysis over invocation graphs that capture all calling contexts including recursive and mutually-recursive calling contexts. Furthermore, the method allows the smooth integration for handling general function pointers in C.
691|CCured: Type-Safe Retrofitting of Legacy Code|In this paper we propose a scheme that combines type inference and run-time checking to make existing C programs type safe. We describe the CCured type system, which extends that of C by separating pointer types according to their usage. This type system allows both pointers whose usage can be verified statically to be type safe, and pointers whose safety must be checked at run time. We prove a type soundness result and then we present a surprisingly simple type inference algorithm that is able to infer the appropriate pointer kinds for existing C programs. Our experience with the CCured system shows that the inference is very effective for many C programs, as it is able to infer that most or all of the pointers are statically verifiable to be type safe. The remaining pointers are instrumented with efficient run-time checks to ensure that they are used safely. The resulting performance loss due to run-time checks is 0–150%, which is several times better than comparable approaches that use only dynamic checking. Using CCured we have discovered programming bugs in established C programs such as several SPECINT95 benchmarks.
692|ESP: Path-Sensitive Program Verification in Polynomial Time|In this paper, we present a new algorithm for partial program verification that runs in polynomial time and space. We are interested in checking that a program satisfies a given temporal safety property. Our insight is that by accurately modeling only those branches in a program for which the property-related behavior differs along the arms of the branch, we can design an algorithm that is accurate enough to verify the program with respect to the given property, without paying the potentially exponential cost of full pathsensitive analysis. We have implemented this “property simulation ” algorithm as part of a partial verification tool called ESP. We present the results of applying ESP to the problem of verifying the file I/O behavior of a version of the GNU C compiler (gcc, 140,000 LOC). We are able to prove that all of the 646 calls to fprintf in the source code of gcc are guaranteed to print to valid, open files. Our results show that property simulation scales to large programs and is accurate enough to verify meaningful properties.
693|A Core Calculus of Dependency|Notions of program dependency arise in many settings: security, partial evaluation, program slicing, and call-tracking. We argue that there is a central notion of dependency common to these settings that can be captured within a single calculus, the Dependency Core Calculus (DCC), a small extension of Moggi&#039;s computational lambda calculus. To establish this thesis, we translate typed calculi for secure information flow, binding-time analysis, slicing, and call-tracking into DCC. The translations help clarify aspects of the source calculi. We also define a semantic model for DCC and use it to give simple proofs of noninterference results for each case. 
694|Secure information flow in a multi-threaded imperative language|Previously, we developed a type system to ensure secure information flow in a sequential, imperative programming language [VSI96]. Program variables are classified as either high or low security; intuitively, we wish to prevent information from flowing from high variables to low variables. Here, we extend the analysis to deal with a multithreaded language. We show that the previous type system is insufficient to ensure a desirable security property called noninterference. Noninterference basically means that the final values of low variables are independent of the initial values of high variables. By modifying the sequential type system, we are able to guarantee noninterference for concurrent programs. Crucial to this result, however, is the use of purely nondeterministic thread scheduling. Since implementing such scheduling is problematic, we also show how a more restrictive type system can guarantee noninterference, given a more deterministic (and easily implementable) scheduling policy, such as round-robin time slicing. Finally, we consider the consequences of adding a clock to the language.  
696|A type system for Java bytecode subroutines|Java is typically compiled into an intermediate language, JVML, that is interpreted by the Java Virtual Machine. Because mobile JVML code is not always trusted, a bytecode verifier enforces static constraints that prevent various dynamic errors. Given the importance of the bytecode verifier for security, its current descriptions are inadequate. This paper proposes using typing rules to describe the bytecode verifier because they are more precise than prose, clearer than code, and easier to reason about than either. JVML has a subroutine construct used for the compilation of Java’s try-finally statement. Subroutines are a major source of complexity for the bytecode verifier because they are not obviously last-in/first-out and because they require a kind of polymorphism. Focusing on subroutines, we isolate an interesting, small subset of JVML. We give typing rules for this subset and prove their correctness. Our type system constitutes a sound basis for bytecode verification and a rational reconstruction of a delicate part of Sun’s bytecode verifier. 1 Bytecode verification and typing rules The Java language is typically compiled into an intermediate language that is interpreted by the Java Virtual Machine (VM) [LY96]. This intermediate language, which we call JVML, is an object-oriented language similar to Java. Its features include packages, classes with single inheritance, and interfaces with multiple inheritance. However, unlike method bodies in Java, method bodies in JVML are sequences of bytecode instructions. These instructions are fairly high-level but, compared to the structured statements used in Java, they are more compact and easier to interpret. JVML code is often shipped across networks to Java VMs embedded in web browsers and other applications. Mobile JVML code is not always trusted by the VM that receives it. Therefore, a bytecode verifier enforces static constraints on mobile JVML code. These constraints rule out type errors (such as dereferencing an integer), access control violations (such as accessing a private method from outside its class),
697|Alias Types|Linear type systems allow destructive operations such as object deallocation and imperative  updates of functional data structures. These operations and others, such as the ability to reuse  memory at di#erent types, are essential in low-level typed languages. However, traditional linear  type systems are too restrictive for use in low-level code where it is necessary to exploit pointer  aliasing. We present a new typed language that allows functions to specify the shape of the store  that they expect and to track the flow of pointers through a computation. Our type system is  expressive enough to represent pointer aliasing and yet safely permit destructive operations.
698|Adoption and focus: practical linear types for imperative programming |All in-text	references	underlined	in	blue	are	linked	to	publications	on	ResearchGate, letting you	access	and	read	them	immediately.
699|A type-based approach to pro-gram security|Abstract. This paper presents a type system which guarantees that well-typed programs in a procedural programming language satisfy a noninterference security property. With all program inputs and outputs classified at various security levels, the property basically states that a program output, classified at some level, can never change as a result of modifying only inputs classified at higher levels. Intuitively, this means the program does not “leak ” sensitive data. The property is similar to a notion introduced years ago by Goguen and Meseguer to model security in multi-level computer systems [7]. We also give an algorithm for inferring and simplifying principal types, which document the security requirements of programs. 1
700|The Pointer Assertion Logic Engine|We present a new framework for verifying partial specifications of programs in order to catch type and memory errors and check data structure invariants. Our technique can verify a large class of data structures, namely all those that can be expressed as graph types. Earlier versions were restricted to simple special cases such as lists or trees. Even so, our current implementation is as fast as the previous specialized tools. Programs are annotated with partial specifications expressed in Pointer Assertion Logic, a new notation for expressing properties of the program store. We work in the logical tradition by encoding the programs and partial specifications as formulas in monadic second-order logic. Validity of these formulas is checked by the MONA tool, which also can provide explicit counterexamples to invalid formulas. To make verification decidable, the technique requires explicit loop and function call invariants. In return, the technique is highly modular: every statement of a given program is analyzed only once. The main target applications are safety-critical data-type algorithms, where the cost of annotating a program with invariants is justified by the value of being able to automatically verify complex properties of the program. 
701|Alias Types for Recursive Data Structures|Linear type systems permit programmers to deallocate or explicitly recycle memory, but they are severly restricted by the fact that they admit no aliasing. This paper describes a pseudo-linear type system that allows a degree of aliasing and memory reuse as well as the ability to define complex recursive data structures. Our type system can encode conventional linear data structures such as linear lists and trees as well as more sophisticated data structures including cyclic and doubly-linked lists and trees. In the latter cases, our type system is expressive enough to represent pointer aliasing and yet safely permit destructive operations such as object deallocation. We demonstrate the flexibility of our type system by encoding two common compiler optimizations: destination-passing style and Deutsch-Schorr-Waite or &#034;link-reversal&#034; traversal algorithms.
702|Type Inference with Polymorphic Recursion|The Damas-Milner Calculus is the typed A-calculus underlying the type system for ML and several other strongly typed polymorphic functional languages such as Mirandal and Haskell. Mycroft has extended its problematic monomorphic typing rule for recursive definitions with a polymorphic typing rule. He proved the resulting type system, which we call the Milner-Mycroft Calculus, sound with respect to Milner’s semantics, and showed that it preserves the principal typing property of the Damas-Milner Calculus. The extension is of practical significance in typed logic programming languages and, more generally, in any language with (mutually) recursive definitions. In this paper we show that the type inference problem for the Milner-Mycroft Calculus is log-space equivalent to semiunification, the problem of solving subsumption inequations between first-order terms. This result has been proved independently by Kfoury et al. In connection with the recently established undecidability of semiunification this implies that typability in the Milner-Mycroft Calculus is undecidable. We present some reasons why type inference with polymorphic recursion appears to be practical despite its undecidability. This also sheds some light on the observed practicality of ML
703|Ultra-fast aliasing analysis using CLA: a million lines of C code in a second|We describe the design and implementation of a system for very fast points-to analysis. On code bases of about a million lines of unpreprocessed C code, our system performs eldbased Andersen-style points-to analysis in less than a second and uses less than 10MB of memory. Our tw o main contributions are a database-centric analysis architecture called compile-link-analyze (CLA), and a new algorithm for implementing dynamic transitive closure. Our points-to analysis system is built into a forward data-dependence analysis tool that is deployed within Lucent to help with consistent type modi cations to large legacy C code bases. 1.
704|Graph Types|Recursive data structures are abstractions of simple records and  pointers. They impose a shape invariant, which is verified at compiletime  and exploited to automatically generate code for building, copying,  comparing, and traversing values without loss of efficiency. However,  such values are always tree shaped, which is a major obstacle to  practical use.  We propose a notion of graph types , which allow common shapes,  such as doubly-linked lists or threaded trees, to be expressed concisely  and efficiently. We define regular languages of routing expressions to  specify relative addresses of extra pointers in a canonical spanning  tree. An efficient algorithm for computing such addresses is developed.  We employ a second-order monadic logic to decide well-formedness of  graph type specifications. This logic can also be used for automated  reasoning about pointer structures.  
705|Simple Imperative Polymorphism|. This paper describes a simple extension of the Hindley-Milner polymorphic type discipline to call-by-value languages that incorporate imperative features like references, exceptions, and continuations. This extension sacrifices the ability to type every purely functional expression that is typable in the Hindley-Milner system. In return, it assigns the same type to functional and imperative implementations of the same abstraction. Hence with a module system that separates specifications from implementations, imperative features can be freely used to implement polymorphic specifications. A study of a number of ML programs shows that the inability to type all Hindley-Milner typable expressions seldom impacts realistic programs. Furthermore, most programs that are rendered untypable by the new system can be easily repaired. Keywords: Continuations, functional programming, polymorphism, references, state 1. Polymorphism, Imperative Features, and Modules The Hindley-Milner polymorphic ty...
706|Efficient Type Inference for Higher-Order Binding-Time Analysis|Binding-time analysis determines when variables and expressions in a program can be bound to their values, distinguishing between early (compile-time) and late (run-time) binding. Binding-time information can be used by compilers to produce more efficient target programs by partially evaluating programs at compile-time. Binding-time analysis has been formulated in abstract interpretation contexts and more recently in a type-theoretic setting. In a type-theoretic setting binding-time analysis is a type inference problem: the problem of inferring a completion of a ?-term e with binding-time annotations such that e satisfies the typing rules. Nielson and Nielson and Schmidt have shown that every simply typed ?-term has a unique completion ê that minimizes late binding in TML, a monomorphic type system with explicit binding-time annotations, and they present exponential time algorithms for computing such minimal completions. 1 Gomard proves the same results for a variant of his two-level ?-calculus without a so-called “lifting ” rule. He presents another algorithm for inferring completions in this somewhat restricted type system and states that it can be implemented in time O(n 3). He conjectures that the completions computed are minimal.
707|Type inference with simple subtypes|Subtyping appears in a variety of programming languages, in the form of the &#034;automatic coercion &#034; of integers to reals, Pascal subranges, and subtypes aris-ing from class hierarchies in languages with inheritance. A general framework based on untyped lambda calculus provides a simple semantic model of sub-typing and is used to demonstrate that an extension of Curry’s type inference rules are semantically complete. An algorithm G for computing the most gen-eral typing associated with any giv en expression, and a restricted, optimized algorithm GA using only atomic subtyping hypotheses are developed. Both algorithms may be extended to insert type conversion functions at compile time or allow polymorphic function declarations as in ML. 1.
708|Resource Usage Analysis|program accesses resources in a valid manner. For example, a memory region that has been allocated should be eventually deallocated, and after the deallocation, the region should no longer be accessed. A file that has been opened should be eventually closed. So far, most of the methods to analyze this kind of property have been proposed in rather specific contexts (like studies of memory management and verification of usage of lock primitives), and it was not so clear what is the essence of those methods or how methods proposed for individual problems are related. To remedy this situation, we formalize a general problem of analyzing resource usage as a resource usage analysis problem, and propose a type-based method as a solution to the problem.
709|Scalable Context-Sensitive Flow Analysis Using Instantiation Constraints|This paper shows that a type graph (obtained via polymorphic type inference) harbors explicit directional flow paths between functions. These flow paths arise from the instantiations of polymorphic types and correspond to call-return sequences in first-order programs. We show that flow information can be computed efficiently while considering only paths with well matched call-return sequences, even in the higher-order case. Furthermore, we present a practical algorithm for inferring type instantiation graphs and provide empirical evidence to the scalability of the presented techniques by applying them in the context of points-to analysis for C programs.
710|A Static Vulnerability Scanner for C and C++ Code|We describe ITS4, a tool for statically scanning security-critical C source code for vulnerabilities. Compared to other approaches, our scanning technique stakes out a new middle ground between accuracy and efficiency. This method is efficient enough to offer real-time feedback to developers during coding while producing few false negatives. Unlike other techniques, our method is also simple enough to scan C++ code despite the complexities inherent in the language. Using ITS4 we found new remotelyexploitable vulnerabilities in a widely distributed software package as well as in a major piece of e-commerce software. The ITS4 source distribution is available at http:  //www.rstcorp.com/its4.  1. 
711|Using CQUAL for static analysis of authorization hook placement|The Linux Security Modules (LSM) framework is a set of authorization hooks for implementing flexible access control in the Linux kernel. While much effort has been devoted to defining the module interfaces, little attention has been paid to verifying the correctness of hook placement. This paper presents a novel approach to the verification of LSM authorization hook placement using CQUAL, a type-based static analysis tool. With a simple CQUAL lattice configuration and some GCC-based analyses, we are able to verify complete mediation of operations on key kernel data structures. Our results reveal some potential security vulnerabilities of the current LSM framework, one of which we demonstrate to be exploitable. Our experiences demonstrate that combinations of conceptually simple tools can be used to perform fairly complex analyses. 1
712|Type Inference for Recursively Constrained Types and its Application to OOP|This paper addresses the problem of designing an object-oriented programming language with an effective type inference mechanism. Recently developed programming languages including Standard ML and Haskell incorporate type inference as a core component of the language. However, type inference has yet to achieve practical application to object-oriented programming languages. We strongly feel the core type features necessary to model object-oriented programming with type inference include a notion of subtyping [CW85], and a notion of &#034;recursively constrained polymorphism,&#034; a generalization of F-bounded polymorphism [CHC90, CCH
713|An Overview of the Extended Static Checking System|this paper is organized as follows. Section 2 presents some related work. Section 3 describes the organization of the system. Section 4 briefly describes the specification language, including some interesting issues that arise when multiple levels of abstraction are present in the system. Section 5 describes the theorem prover. Section 6 describes some of the uses to which ESC has been put. Finally, section 7 presents conclusions and future directions.
714|An Effective Theory of Type Refinements|We develop an explicit two level system that allows programmers to reason about the behavior of effectful programs. The first level is an ordinary ML-style type system, which confers standard properties on program behavior. The second level is a conservative extension of the first that uses a logic of type refinements to check more precise properties of program behavior. Our logic is a fragment of intuitionistic linear logic, which gives programmers the ability to reason locally about changes of program state. We provide a generic resource semantics for our logic as well as a sound, decidable, syntactic refinement-checking system. We also prove that refinements give rise to an optimization principle for programs. Finally, we illustrate the power of our system through a number of examples.
715|Pointer Analysis for Programs with Structures and Casting|Type casting allows a program to access an object as if it had a type different from its declared type. This complicates the design of a pointer-analysis algorithm that treats structure fields as separate objects; therefore, some previous pointer-analysis algorithms &#034;collapse&#034; a structure into a single variable. The disadvantage of this approach is that it can lead to very imprecise points-to information. Other algorithms treat each field as a separate object based on its offset and size. While this approach leads to more precise results, the results are not portable because the memory layout of structures is implementation dependent.  This paper first describes the complications introduced by type casting, then presents a tunable pointer-analysis framework for handling structures in the presence of casting. Different instances of this framework produce algorithms with different levels of precision, portability, and efficiency. Experimental results from running our implementations of f...
716|Type-Based Flow Analysis: From Polymorphic Subtyping to CFL-Reachability.|We present a novel approach to scalable implementation of type-based flow analysis with polymorphic subtyping. Using a new presentation of polymorphic subtyping with instantiation constraints, we are able to apply context-free language (CFL) reachability techniques to type-based flow analysis. We develop a CFL-based algorithm for computing flow information in time O(n 3 ), where n is the size of the typed program. The algorithm substantially improves upon the best previously known algorithm for flow analysis based on polymorphic subtyping with complexity O(n 8 ). Our technique also yields the first demand-driven algorithm for polymorphic subtype-based flow-computation. It works directly on higher-order programs with structured data of finite type (unbounded data structures are incorporated via finite approximations), supports context-sensitive, global flow summarization and includes polymorphic recursion.
717|Static Enforcement of Security with Types|A number of security systems for programming languages have recently appeared, including systems for enforcing some form of access  control. The Java JDK 1.2 security architecture is one such system that is widely studied and used. While the architecture has many appealing features, access control checks are all implemented via dynamic method calls. This is a highly non-declarative form of specification which is hard to read, and which leads to additional run-time overhead. In this paper, we present a novel security type system that enforces the same security guarantees as Java Stack Inspection, but via a static type system with no additional run-time checks. The system allows security properties of programs to be clearly expressed within the types themselves. We also define and prove correct an inference algorithm for security types, meaning that the system has the potential to be layered on top of the existing Java architecture, without requiring new syntax.  1. INTRODUCTION  Securit...
718|Tractable Constraints in Finite Semilattices|. We introduce the notion of definite inequality constraints involving  monotone functions in a finite meet-semilattice, generalizing the  logical notion of Horn-clauses, and we give a linear time algorithm for  deciding satisfiability. We characterize the expressiveness of the framework  of definite constraints and show that the algorithm uniformly solves  exactly the set of all meet-closed relational constraint problems, running  with small linear time constant factors for any fixed problem. We give  an alternative technique which reduces inequalities to satisfiability of  Horn-clauses (hornsat) and study its efficiency. Finally, we show that  the algorithm is complete for a maximal class of tractable constraints,  by proving that any strict extension will lead to NP-hard problems in  any meet-semilattice.  Keywords: Finite semilattices, constraint satisfiability, program analysis, tractability, algorithms.  1 Introduction  Many program analysis problems can be solved by generating a...
719|Safety checking of machine code|We show how to determine statically whether it is safe for untrusted machine code to be loaded into a trusted host system. Our safety-checking technique operates directly on the untrusted machine-code program, requiring only that the initial inputs to the untrusted program be annotated with typestate information and linear constraints. This approach opens up the possibility of being able to certify code produced by any compiler from any source language, which gives the code producers more freedom in choosing the language in which they write their programs. It eliminates the dependence of safety on the correctness of the compiler because the final product of the compiler is checked. It leads to the decoupling of the safety policy from the language in which the untrusted code is written, and consequently, makes it possible for safety checking to be performed with respect to an extensible set of safety properties that are specified on the host side. We have implemented a prototype safety checker for SPARC machine-language programs, and applied the safety checker to several examples. The safety checker was able to either prove that an example met the necessary safety conditions, or identify the places where the safety conditions were violated. The checking times ranged from less than a second to 14 seconds on an UltraSPARC machine.
720|Type inference with constrained types|We present a general framework HM(X) for type systems with constraints. The framework stays in the tradition of the Hindley/Milner type system. Its type system instances are sound under a standard untyped compositional semantics. We can give a generic type inference algorithm for HM(X) so that, under sufficient conditions on X, type inference will always compute the principal type of a term. We discuss instances of the framework that deal with polymorphic records, equational theories and subtypes.
721|Trust in the ?-Calculus|This paper introduces trust analysis for higher-order languages. Trust analysis encourages the programmer to make explicit the trustworthiness of data, and in return it can guarantee that no mistakes with respect to trust will  be made at run-time. We present a confluent ?-calculus with explicit trust operations, and we equip it with a trust-type system which has the subject reduction property. Trust information in presented as two annotations of each function type constructor, and type inference is computable in O(n³) time.
722|A Simple, Comprehensive Type System for Java Bytecode Subroutines |A type system with proven soundness is a prerequisite for the  safe and efficient execution of Java bytecode programs. So far,  eftbrts to construct such a type system have followed a &#034;forward  dataflow&#034; approach, in the spirit of the original Java Virtual  Machine bytecode verifier. We present an alternative type  system, based on conventional ideas of type constraints,  polymorphic recursion and continuations. We compare it to Stata and Abadi&#039;s JVML-0 type system for bytecode subroutines, and  demonstrate that our system is simpler and able to type strictly  more programs, including code that could be produced by Java  compilers and cannot be typed in JVML-0. Examination of real  Java programs shows that such code is rare but does occur. We  explain some of the apparently arbitrary constraints imposed by  previous approaches by showing that they are consequences of  our simpler type rules, or actually unnecessary.
723|A Toolkit for Constructing Type- and Constraint-Based Program Analyses|BANE (the Berkeley Analysis Engine) is a publicly available  toolkit for constructing type- and constraint-based program analyses. We describe the goals of the project, the rationale for BANE&#039;s overall  design, some examples coded in BANE, and briefly compare BANE with  other program analysis frameworks. 
724|Transparent proxies for java futures|A proxy object is a surrogate or placeholder that controls access to another target object. Proxies can be used to support distributed programming, lazy or parallel evaluation, access control, and other simple forms of behavioral reflection. However, wrapper proxies (like futures or suspensions for yet-to-be-computed results) can require significant code changes to be used in statically-typed languages, while proxies more generally can inadvertently violate assumptions of transparency, resulting in subtle bugs. To solve these problems, we have designed and implemented a simple framework for proxy programming that employs a static analysis based on qualifier inference, but with additional novelties. Code for using wrapper proxies is automatically introduced via a classfile-to-classfile transformation, and potential violations of transparency are signaled to the programmer. We have formalized our analysis and proven it sound. Our framework has a variety of applications, including support for asynchronous method calls returning futures. Experimental results demonstrate the benefits of our framework: programmers are relieved of managing and/or checking proxy usage, analysis times are reasonably fast, overheads introduced by added dynamic checks are negligible, and performance improvements can be significant. For example, changing two lines in a simple RMI-based peer-to-peer application and then using our framework resulted in a large performance gain. 1
725|Type systems for distributed data structures|Abstract Distributed-memory programs are often written using a global address space: any process can name any memory location on any processor. Some languages completely hide the distinction between local and remote memory, simplifying the programming model at some performance cost. Other languages give the programmer more explicit control, offering better potential performance but sacrificing both soundness and ease of use. Through a series of progressively richer type systems, we formalize the complex issues surrounding sound computation with explicitly distributed data structures. We then illustrate how type inference can subsume much of this complexity, letting programmers work at whatever level of detail is needed. Experiments conducted with the Titanium programming language show that this can result in easier development and significant performance improvements over manual optimization of local and global memory. 1 Introduction While there have been many efforts to design distributed, parallel programming languages, none has been completely satisfactory. Many approaches present the illusion of a single shared, global address space. While easy for programmers to understand, this approach hides the real structure of memory, making it difficult to exploit locality of data. In complex applications where local memory accesses may be orders of magnitude faster than remote accesses, this can seriously harm performance, development time, or both.
726|Single and loving it: Must-alias analysis for higher-order languages|In standard control-flow analyses for higher-order languages, a single abstract binding for a variable represents a set of exact bindings, and a single abstract reference cell represents a set of exact reference cells. While such analyses provide useful may-alias information, they are unable to answer mustalias questions about variables and cells, as these questions ask about equality of specific bindings and references. In this paper, we present a novel program analysis for higher-order languages that answers must-alias questions. At every program point, the analysis associates with each variable and abstract cell a cardinality, which is either single  or multiple. If variable x is single at program point p, then all bindings for x in the heap reachable from the environment at p hold the same value. If abstract cell r is single at p,  then at most one exact cell corresponding to r is reachable from the environment at p.  Must-alias information facilitates various program optimizations...
728|The Complexity of Subtype Entailment for Simple Types|A subtyping     0  is entailed by a set of subtyping constraints C, written C j=     0  , if every valuation (mapping of type variables to ground types) that satisfies C also satisfies     0  .  We study the complexity of subtype entailment for simple types over lattices of base types. We show that:  ffl deciding C j=     0  is coNP-complete.  ffl deciding C j= ff  fi for consistent, atomic C and ff; fi atomic can be done in linear time.  The structural lower (coNP-hardness) and upper (membership in coNP) bounds as well as the optimal algorithm for atomic entailment are new. The coNP-hardness result indicates that entailment is strictly harder than satisfiability, which is known to be in PTIME for lattices of base types. The proof of coNP-completeness gives an improved algorithm for deciding entailment and puts a precise complexitytheoretic marker on the intuitive &#034;exponential explosion&#034; in the algorithm.  Central to our results is a novel characterization of C j= ff  fi for atomic, co...
729|Satisfiability of Inequalities in a Poset|We consider tractable and intractable cases of the satisfiability  problem for conjunctions of inequalities between variables and constants  in a fixed finite poset. We show that crowns are intractable.
730|Annotated Type Systems for Program Analysis|Interpretation Table 1.2: Annotations in the Thesis In Chapter 2 we present a combined strictness and totality analysis.We  are specifying the analysis as an annotated type system. The type system allows conjunctions of annotated types, but only at the top-level. The analysis is somewhat more powerful than the strictness analysis by Kuo and Mishra [KM89] due to the conjunctions and in that we also consider totality. The analysis is shown sound with respect to a natural-style operational semantics. The analysis is not immediately extendable to full conjunction. The analysis of Chapter 3 is also a combined strictness and totality analysis, however with &#034;full&#034; conjunction. Soundness of the analysis is shown with respect to a denotational semantics. The analysis is more powerful than the strictness analyses by Jensen [Jen92a] and Benton [Ben93] in that it in addition to strictness considers totality. So far we have only specified the analyses, however in order for the analyses to be practically useful we need an algorithm for inferring the annotated types. In Chapter 4 we construct an algorithm for the analysis of Chapter  2  The conjunctions are only allow at the &#034;top-level&#034;.  1.3. OVERVIEW OF THESIS 25 3usingthelazy type approach by Hankin and Le Metayer [HM94a]. The reason for choosing the analysis from Chapter 3 is that the approach not applicable to the analysis from Chapter 2. In Chapter 5 we study a binding time analysis. We take the analysis specified by Nielson and Nielson [NN92] and we construct an more e#cient algorithm than the one proposed in [NN92]. The algorithm collects constraints in a structural manner as the algorithm  T  [Dam85]. Afterwards the minimal solution to the set of constraints is found. The analysis in Chapter 6 is specified by abstract interp...
731|Checking Programmer-Specified Non-Aliasing|We study the new ANSI C type qualifier restrict, which allows programmers to specify pointers that are not aliased to other pointers. The main contribution of this paper is a formal semantics for restrict and a type and effect system for checking that restrict-annotated programs are correct with respect to our semantics. We give an efficient inference algorithm for our type system and describe natural extensions of our type system to include subtyping, parametric polymorphism, and affects clauses that capture the effects of calling a function. We also discuss ways in which our type system differs from the ANSI C standard.
732|Visualizing Type Qualifier Inference with Eclipse|Type qualifiers are a lightweight, practical mechanism for specifying and checking program properties. In previous work, we have developed CQUAL, a tool for adding type qualifiers to C. In this short article, we describe an Eclipse plug-in for CQUAL that allows programmers to visualize the results of CQUAL’s type qualifier inference and thereby quickly understand and resolve potential programming errors.  
733|Logic Programming with Focusing Proofs in Linear Logic|The deep symmetry of Linear Logic [18] makes it suitable for providing abstract models of computation,  free from implementation details which are, by nature, oriented and non symmetrical. I propose here one such  model, in the area of Logic Programming, where the basic computational principle is  Computation = Proof search.
734|Generative communication in Linda|Generative communication is the basis of a new distributed programming langauge that is intended for systems programming in distributed settings generally and on integrated network computers in particular. It differs from previous interprocess communication models in specifying that messages be added in tuple-structured form to the computation environment, where they exist as named, independent entities until some process chooses to receive them. Generative communication results in a number of distinguishing properties in the new language, Linda, that is built around it. Linda is fully distributed in space and distributed in time; it allows distributed sharing, continuation passing, and structured naming. We discuss these properties and their implications, then give a series of examples. Linda presents novel implementation problems that we discuss in Part II. We are particularly concerned with implementation of the dynamic global name space that the generative communication model requires.
735|Light Linear Logic |The abuse of structural rules may have damaging complexity effects.
737|Logic Programming in a Fragment of Intuitionistic Linear Logic|When logic programming is based on the proof theory of intuitionistic logic, it is natural to allow implications in goals and in the bodies of clauses. Attempting to prove a goal of the form D ? G from the context (set of formulas) G leads to an attempt to prove the goal G in the extended context G ?{D}. Thus during the bottom-up search for a cut-free proof contexts, represented as the left-hand side of intuitionistic sequents, grow as stacks. While such an intuitionistic notion of context provides for elegant specifications of many computations, contexts can be made more expressive and flexible if they are based on linear logic. After presenting two equivalent formulations of a fragment of linear logic, we show that the fragment has a goal-directed interpretation, thereby partially justifying calling it a logic programming language. Logic programs based on the intuitionistic theory of hereditary Harrop formulas can be modularly embedded into this linear logic setting. Programming examples taken from theorem proving, natural language parsing, and data base programming are presented: each example requires a linear, rather than intuitionistic, notion of context to be modeled adequately. An interpreter for this logic programming language must address the problem of splitting contexts; that is, when attempting to prove a multiplicative conjunction (tensor), say G1 ? G2, fromthe context ?, the latter must be split into disjoint contexts ?1 and ?2 for which G1 follows from ?1 and G2 follows from ?2. Since there is an exponential number of such splits, it is important to delay the choice of a split as much as possible. A mechanism for the lazy splitting of contexts is presented based on viewing proof search as a process that takes a context, consumes part of it, and returns the rest (to be consumed elsewhere). In addition, we use collections of Kripke interpretations indexed by a commutative monoid to provide models for this logic programming language and show that logic programs admit a canonical model.  
738|Computational Interpretations of Linear Logic|We study Girard&#039;s Linear Logic from the point of view of giving a concrete computational interpretation of the logic, based on the Curry-Howard isomorphism. In the case of Intuitionistic Linear Logic, this leads to a refinement of the lambda calculus, giving finer control over order of evaluation and storage allocation, while maintaining the logical content of programs as proofs, and computation as cut-elimination.
739|Linear Objects: logical processes with built-in inheritance|We present a new framework for amalgamating two successful programming paradigms: logic programming  and object-oriented programming. From the former, we keep the declarative reading of programs. From the  latter, we select two crucial notions: (i) the ability for objects to dynamically change their internal state during  the computation; (ii) the structured representation of knowledge, generally obtained via inheritance graphs  among classes of objects. We start with the approach, introduced in concurrent logic programming languages,  which identifies objects with proof processes and object states with arguments occurring in the goals of a given  process. This provides a clean, side-effect free account of the dynamic behavior of objects in terms of the  search tree --- the only dynamic entity in logic programming languages. We integrate this view of objects with  an extension of logic programming, which we call Linear Objects, based on the possibility of having multiple  literals in the head of a program clause. This contains within itself the basis for a flexible form of inheritance,  and maintains the constructive property of Prolog of returning definite answer substitutions as output of the  proof of non-ground goals. The theoretical background for Linear Objects is Linear Logic, a logic recently  introduced to provide a theoretical basis for the study of concurrency. We also show that Linear Objects can be  considered a constructive restriction of full Classical Logic. We illustrate the expressive power of Linear Objects  compared to Prolog by several examples from the object-oriented domain, but we also show that it can be used  to provide elegant solutions for problems arising in the standard style of logic programming. 
740|The Concurrent Language Shared Prolog|Shared Prolog is a new concurrent logic language. A Shared Prolog system is composed of a set of parallel agents which are Prolog programs extended by a guard mechanism. The programmer controls the granularity of parallelism coordinating communication and synchronization of the agents via a centralized data structure. The communication mechanism is inherited from the blackboard model of problem solving. Intuitively, the granularity of the logic processes to be elaborated in parallel is large, while the resources shared on the blackboard can be very fine-grained.  An operational semantics for Shared Prolog is given in terms of a distributed model. Through an abstract notion of computation, the kinds of parallelism supported by the language as well as properties of infinite computations, such as local deadlocks, are studied.  The expressiveness of the language is shown with respect to the specification of two classes of applications: metaprogramming and blackboard systems.  Categories an...
741|Communication as Fair Distribution of Knowledge|We introduce an abstract form of interobject communication for object-oriented concurrent programming  based on the proof theory of Linear Logic, a logic introduced to provide a theoretical basis for the study  of concurrency. Such a form of communication, which we call forum-based communication, can be seen as a  refinement of blackboard-based communication in terms of a more local notion of resource consumption. Forumbased  communication is introduced as part of a new computational model for the object-oriented concurrent  programming language LO, presented at last year OOPSLA/ECOOP (1990), which exploits the proof-theory  of Linear Logic also to achieve a powerful form of knowledge-sharing.
744|Intelligence Without Representation|Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporateeverything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environ...
745|Intelligent agents: Theory and practice|The concept of an agent has become important in both Artificial Intelligence (AI) and mainstream computer science. Our aim in this paper is to point the reader at what we perceive to be the most important theoretical and practical issues associated with the design and construction of intelligent agents. For convenience, we divide these issues into three areas (though as the reader will see, the divisions are at times somewhat arbitrary). Agent theory is concerned with the question of what an agent is, and the use of mathematical formalisms for representing and reasoning about the properties of agents. Agent architectures can be thought of as software engineering models of agents; researchers in this area are primarily concerned with the problem of designing software or hardware systems that will satisfy the prop-erties specified by agent theorists. Finally, agent languages are software systems for programming and experimenting with agents; these languages may embody principles proposed by theorists. The paper is not intended to serve as a tutorial introduction to all the issues mentioned; we hope instead simply to identify the most important issues, and point to work that elaborates on them. The article includes a short review of current and potential applications of agent technology.
746|Modeling Rational Agents within a BDI-Architecture|Intentions, an integral part of the mental state of an agent, play an important role in 
747|Intelligence without reason|Computers and Thought are the two categories that together define Artificial Intelligence as a discipline. It is generally accepted that work in Artificial Intelligence over the last thirty years has had a strong influence on aspects of computer architectures. In this paper we also make the converse claim; that the state of computer architecture has been a strong influence on our models of thought. The Von Neumann model of computation has lead Artificial Intelligence in particular directions. Intelligence in biological systems is completely different. Recent work in behavior-based Artificial Intelligence has produced new models of intelligence that are much closer in spirit to biological systems. The non-Von Neumann computational models they use share many characteristics with biological computation.
748|The Role of Emotion in Believable Agents|Articial intelligence researchers attempting to create engaging  apparently living creatures may nd important insight in the work of artists who have explored the idea of believable character  In particular  appropriately timed and clearly expressed emotion is a central requirement for believable characters  We discuss these ideas and suggest how they may apply to believable interactive characters  which we call believable agents This work was supported in part by Fujitsu Laboratories and Mitsubishi Electric Research Laborato ries  The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ocial policies  either expressed or implied  of any other parties Keywords  articial intelligence  emotion  believable agents art  animation  believable characters  BELIEVABILITY   Believability There is a notion in the Arts of believable character  It does not mean an honest or reliable character  but one that provides the illusion of life  and thus permits the audience s suspension of disbelief The idea of believability has long been studied and explored in literature  theater lm  radio drama  and other media  Traditional character animators are among those artists who have sought to create believable characters  and the Disney animators of the   	 s made great strides toward this goal  The rst page of the enormous classic reference work on Disney animation Thomas and Johnston     begins with these words Disney animation makes audiences really believe in   characters  whose adventures and misfortunes make people laugh  and even cry  There is a special ingredient in our type of animation that produces drawings that appear to think and make decisions and act of their own volition  it is what creates the illusion of life Many articial intelligence researchers have long wished to build robots  and their cousins called agents  that seem to think  feel  and live  These are creatures with whom you	d want to share some of your life  as with a companion  or a social pet For instance  in his 
749|Plans And Resource-Bounded Practical Reasoning|An architecture for a rational agent must allow for means-end reasoning, for the weighing of competing alternatives, and for interactions between these two forms of reasoning. Such an architecture must also address the problem of resource boundedness. We sketch a solution of the first problem that points the way to a solution of the second. In particular, we present a high-level specification of the practical-reasoning component of an architecture for a resource-bounded rational agent. In this architecture, a major role of the agent&#039;s plans is to constrain the amount of further practical reasoning she must perform.
750|Software Agents|this paper, we discuss these questions and describe some emerging technologies that provide answers. In the final section, we mention some additional issues and summarize the key points of the paper. (For more information on agent-based software engineering, see [Genesereth 1989] and [Genesereth 1992]. See also [Shoham 1993] for a description of a variation of agent-based software engineering known as &#034;agent-oriented programming&#034;.) 2. Agent Communication Language
751|Elephants don&#039;t play chess|Engineering and Computer Science at M.I.T. and a member of the Artificial Intelligence Laboratory where he leads the mobile robot group. He has authored two books, numerous scientific papers, and is the editor of the International Journal of Computer Vision. There is an alternative route to Artificial Intelligence that diverges from the directions pursued under that banner for the last thirty some years. The traditional approach has emphasized the abstract manipulation of symbols, whose grounding, in physical reality has. rarely been achieved. We explore a research methodology which emphasizes ongoing physical interaction with the environment as the primary source of constraint on the design of intelligent systems. We show how this methodology has recently had significant successes on a par with the most successful classical efforts. We outline plausible future work along these lines which can lead to vastly more ambitious systems. 1.
752|Kasbah: An Agent Marketplace for Buying and Selling Goods|While there are many Web services which help users find things to buy,we know of none which actually try to automate the process of buying and selling. Kasbah is a virtual marketplace on the Web where users create autonomous agents to buy and sell goods on their behalf. Users specify parameters to guide and constrain an agent&#039;s overall behavior. A simple prototype has been built to test the viability of this concept.
753|Universal Plans for Reactive Robots in Unpredictable Environments|In: Proc 10th IJCAI, 1987, 1039ff.  To date, reactive robot behavior has been achieved only through manual programming. This paper describes a new kind of plan, called a &#034;universal plan&#034;, which can be synthesized automatically, yet generates appropriate behavior in unpredictable environments. In classical planning work, problems were posed with unique initial and final world states; in my approach a problem specifies only a goal condition. The planner is thus unable to commit to any specific future course of events but must specify appropriate reactions for anticipated situations. An alternative conception is that one universal plan compactly represents every classical plan. Which part of the universal plan is executed depends entirely on how the environment behaves at execution time. Universal plans are constructed from state-space operator schemas by a nonlinear planner. They explicitly identify predicates requiring monitoring at each moment of execution, and provide for sabotage, se...
754|Experiences with an Architecture for Intelligent, Reactive Agents  |This paper describes an implementation of the 3T robot architecture  which has been under development for the last eightyears. The architecture  uses three levels of abstraction and description languages whichare  compatible between levels. The makeup of the architecture helps to coordinate  planful activities with real-time behaviors for dealing with dynamic  environments. In recent years, other architectures have been created with  similar attributes but two features distinguish the 3T architecture: 1) a  variety of useful software tools have been created to help implement this  architecture on multiple real robots;, and 2) this architecture, or parts of it, have been implemented on a varietyofvery different robot systems  using different processors, operating systems, effectors and sensor suites.
755|Agent-Oriented Software Engineering|Software and knowledge... In this article, we argue that intelligent agents and agent-based systems offer novel opportunities for developing effective tools and techniques. Following a discussion on the classic subject of what makes software complex, we introduce intelligent agents as software structures capable of making &#034;rational decisions&#034;. Such rational decision-makers are well-suited to the construction of certain types of software, which mainstream software engineering has had little success with. We then go on to examine a number of prototype techniques proposed for engineering agent systems, including formal specification and verification methods for agent systems, and techniques for implementing agent specifications
756|The distributed vehicle monitoring testbed: A Tool for Integrating Distributed Problem Solving Networks|Cooperative distributed problem solving networks are dist,libuted networks of semi-autonomous processing nodes t,hat work t,ogether t,o solve a smgle problem The Distrihut,ed Vehicle Monitoring Testbed is a flcxihle and fully-inst,rllmeni.ed research tool for empirically evaluating altclnative designs fol these net.works The t.estbed simulates a class of a distributed knowledge-based problem solving systems operating on an abstracted version of a vehicle monitoring task There are two important, aspects Lo the testbed: (1) it implements a novel generic architecture for distributed problem solving net,works that exploits Lhc use of sophisticated local node control aud meta-level control Lo improve global coherence in network problem solving; (2) it serves as an example of how a testbed can be engineered to permit the empirical exp101 ation of design issues in knowledge-based AT systems. The testbed is capable of simulating differen degrees of sophistication iu problem solving knowledge and different. focus-of-attent,ion mechanisms, for varying the distribut,ion and characteristics of error in its (simulat,ed) input data, and for measuring the progress of problem solving. Node configurations and communication channel charact,eristics can also he independent,ly varied in the simulated network A project as large and complex as the Distributed Vehicle Monitoring Testbed involved a number of individuals and became itself a distributed problem solving task The efforts of Richard Brooks, Eva Hudlicka, Larry Lefkowitz, Raam Mukunda,.Jasmina Pavlin, and Scott Reed contributed to the success of the testbcd We would also like to acknowledge Lee &amp;man’s collaboration on the initial formulation of the Functionally Accmate, Cooperative approach and his work on the pilot
757|Commitment and effectiveness of situated agents|Recent research in real-time Artificial Intelligence has focussed upon the design of situated agents and, in particular, how to achieve effective and robust behaviour with limited computational resources. A range of architectures and design principles has been proposed to solve this problem. This has led to the development of simulated worlds that can serve as testbeds in which the effectiveness of different agents can be evaluated. We report here an experimental program that aimed to investigate how commitment to goals contributes to effective behaviour and to compare the properties of different strategies for reacting to change. Our results demonstrate the feasibility of developing systems for empirical measurement of agent performance that are stable, sensitive, and capable of revealing the effect of &amp;quot;high-level&amp;quot; agent characteristics such as commitment. 1
758|A Survey of Concurrent METATEM - The Language and its Applications|. In this paper we present a survey of work relating to the Concurrent  METATEM programming language. In addition to a description of the basic Concurrent  METATEM system, which incorporates the direct execution of temporal formulae, a variety of extensions that have either been implemented or proposed are outlined. Although still in the development stage, there appear to be many areas where such a language could be applied. We present a variety of sample applications, highlighting the particular features of Concurrent METATEM that we believe will make it appropriate for use in these areas. 1 Introduction  Concurrent METATEM is a language based upon the direct execution of temporal formulae [15]. It consists of two distinct aspects: an execution mechanism for temporal formulae in a particular form; and an operational model that treats single executable temporal logic programs as asynchronously executing objects in a concurrent objectbased system. The motivation for the development of t...
759|Provably Bounded-Optimal Agents|Since its inception, artificial intelligence has relied upon a theoretical foundation centred around perfect rationality as the desired property of intelligent systems. We argue, as others have done, that this foundation is inadequate because it imposes fundamentally unsatisfiable requirements. As a result, there has arisen a wide gap between theory and practice in AI, hindering progress in the field. We propose instead a property called bounded optimality. Roughly speaking, an agent is bounded-optimal if its program is a solution to the constrained optimization problem presented by its architecture and the task environment. We show how to construct agents with this property for a simple class of machine architectures in a broad class of real-time environments. We illustrate these results using a simple model of an automated mail sorting facility. We also define a weaker property,  asymptotic bounded optimality (ABO), that generalizes the notion of optimality in classical complexity th...
760|METATEM: A Framework for Programming in Temporal Logic|In this paper we further develop the methodology of temporal logic as an executable imperative language, presented by Moszkowski [Mos86] and Gabbay [Gab87, Gab89] and present a concrete framework, called METATEM for executing (modal and) temporal logics. Our approach is illustrated by the development of an execution mechanism for a propositional temporal logic and for a restricted first order temporal logic.
761|Manufacturing Experience with the Contract Net|We are implementing a control system for a discrete manufacturing environment that partitions tasks using a negotiation protocol like the contract net described by Smith and Davis [24,25,26,3]. The application domain differs in interesting ways from those to which contract nets have previously been applied. This report
762|Social Plans: A Preliminary Report|The formalization of multi-agent autonomous systems requires a rich ontology for capturing a variety of collective behaviours and a powerful semantics for distinguishing between collective agents having, executing, and jointly intending a plan. In this paper, we introduce the notion of social agents and social plans. A definition of joint intentions is provided that avoids some of the problems encountered by previous formalizations. In particular, it models cooperation by requiring that agents adopt a joint goal and a joint plan of action before forming a joint intention. The paper also stresses the planning capability of agents and outlines a process for means-end reasoning by multiple agents.  1 Introduction  Situated agents are systems embedded in dynamic environments; they continuously sense their environment and effect changes to it by performing actions. These agents have to balance the time they devote to thinking against the time they take acting. Also they need to balance the...
763|Foundations of a Logical Approach to Agent Programming|This paper describes a novel approach to high-level agent programming based on a highly  developed logical theory of action. The user provides a specification of the agents&#039; basic actions  (preconditions and effects) as well as of relevant aspects of the environment, in an extended  version of the situation calculus. He can then specify behaviors for the agents in terms of these  actions in a programming language where one can refer to conditions in effect in the environment.  When an implementation of the basic actions is provided, the programs can be executed in a  real environment; otherwise, a simulated execution is still possible. The interpreter automatically  maintains the world model required to execute programs based on the specification. The theoretical  framework includes a solution to the frame problem, allows agents to have incomplete knowledge  of their environment, and handles perceptual actions. The theory can also be used to prove  programs correct. A simple meeting sc...
764|A Situated View of Representation and Control|Intelligent agents are systems that have a complex, ongoing interaction with an environment that is dynamic and imperfectly predictable. Agents are typically difficult to program because the correctness of a program depends on the details of how the agent is situated in its environment. In this paper, we present a methodology for the design of situated agents that is based on situated automata theory. This approach allows designers to describe the informational content of an agent&#039;s computational states in a semantically rigorous way without requiring a commitment to conventional runtime symbolic processing. We start by outlining this situated view of representation, then show how it contributes to design methodologies for building systems that track perceptual conditions and take purposeful actions in their environments. 1 Introduction  Humans, delivery robots, and automated factories are all systems that have an intelligent, ongoing interaction with environments that are dynamic and ...
765|Asymmetry Thesis and Side-Effect Problems in Linear-Time and Branching-Time Intention Logics|In this paper, we examine the relationships between beliefs, goals, and intentions. In particular, we consider the formalization of the Asymmetry Thesis as proposed by Bratman [ 1987 ] . We argue that the semantic characterization of this principle determines if the resulting logic is capable of handling other important problems, such as the side-effect problem of belief-goal-intention interaction. While Cohen and Levesque&#039;s [ 1990 ] formalization faithfully models some aspects of the asymmetry thesis, it does not solve all the side-effect problems; on the other hand the formalization provided by Rao and Georgeff [ 1991 ] solves all the side-effect problems, but only models a weak form of the asymmetry thesis. In this paper, we combine the intuition behind both these approaches and provide a semantic account of the asymmetry thesis, in both linear-time and branching-time logics, for solving many of these problems.  1 Introduction  Formalizations of intentions and their relationships w...
766|Transforming Standalone Expert Systems into a Community of Cooperating Agents|Distributed Artificial Intelligence (DAI) systems in which multiple problem solving agents cooperate to achieve a common objective is a rapidly emerging and promising technology. However, as yet, there have been relatively few reported cases of such systems being employed to tackle real-world problems in realistic domains. One of the reasons for this is that DAI researchers have given virtually no consideration to the process of incorporating pre-existing systems into a community of cooperating agents. Yet reuse is a primary consideration for any organisation with a large software base. To redress the balance, this paper reports on an experiment undertaken at the CERN laboratories in which two pre-existing and standalone expert systems for diagnosing faults in a particle accelerator were transformed into a community of cooperating agents. The experiences and insights gained during this process provide a valuable first step towards satisfying the needs of potential users of DAI technolo...
767|Toward Agent Programs with Circuit Semantics|New ideas are presented for computing and organizing actions for autonomous agents in dynamic environmentsPnvironments in which the agent’s current situation cannot always be accurately discerned and in which the effects of actions cannot always be reliably predicted. The notion of “circuit semantics ” for programs based on “teleo-reactive trees ” is introduced. Program execution builds a combinational circuit which receives sensory inputs and controls actions. These formalisms embody a high degree of inherent conditionality and thus yield programs that are suitably reactive to their environments. At the same time, the actions computed by the programs are guided by the overall goals of the agent. The paper also speculates about how programs using these ideas could be automatically generated by artificial intelligence planning systems and adapted by learning methods. I. Control Theory and Computer Science
768|Using ARCHON to develop real-world DAI applications for electricity transportation management and particle accelerator control|ARCHON ^TM (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe&#039;s largest ever project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Two of these applications, electricity transportation management and particle accelerator control, have been run successfully on-line in the organisation for which they were developed (respectively, Iberdrola an electricity utility in the north of Spain and CERN the European Centre for high energy physics research near Geneva). This paper recounts the problems, insights and experiences gained whilst deploying ARCHON technology in these real-world industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applications and highlights the key design forces which shape work in this important domain. Secondly, the...
769|An Agent-based Approach to Health Care Management|The provision of medical care typically involves a number of individuals, located in a number of different institutions, whose decisions and actions need to be coordinated if the care is to be effective and efficient. To facilitate this decision making and to ensure the coordination process runs smoothly, the use of software support is becoming increasingly widespread. To this end, this paper describes an agent-based system which was developed to help manage the care process in real world settings. The agents themselves are implemented using a layered architecture, called AADCare, which combines a number of AI and agent techniques: a symbolic decision procedure for decision making with incomplete and conflicting information, a concept of accountability for task allocation, the notions of commitments and conventions for managing coherent cooperation, and a set of communication primitives for interagent interaction. The utility of this approach is demonstrated through the development of ...
770|Applications of Distributed Artificial Intelligence in Industry|In many industrial applications, large centralized software systems are not as effective as distributed networks of relatively simpler computerized agents. For example, to compete effectively in today&#039;s markets, manufacturers must be able to design, implement, reconfigure, resize, and maintain manufacturing facilities rapidly and inexpensively. Because modern manufacturing depends heavily on computer systems, these same requirements apply to manufacturing control software, and are more easily satisfied by small modules than by large monolithic systems.  This paper reviews industrial needs for Distributed Artificial Intelligence (DAI), giving special attention to systems for manufacturing scheduling and control. It describes a taxonomy of such systems, gives case studies of several advanced research applications and actual industrial installations, and identifies steps that need to be taken to deploy these technologies more broadly.
771|Industrial Applications of Distributed AI|This article argues that a DAI approach can be used to cope with the complexity of industrial applications. DAI techniques are beginning to have a broad impact; the current introduction of these techniques by an ESPRIT project, a Palo Alto consortium, ARPA, Carnegie Mellon University, MCC, and others are good examples. In the near future, other industrial products will emerge from the application of DAI techniques to other domains, including distributed databases, computer-supported cooperative work, and air traffic control. An important advantage of a DAI approach is the ability to integrate existing standalone knowledge-based systems. This factor is important because software for industrial applications is often developed in an ad hoc fashion. Thus, organizations possess a large number of standalone systems developed at different times by different people using different techniques. These systems all operate in the same physical environment, all have expertise that is related but distinct, and all could benefit from cooperation with other such standalone systems
772|Specifying and Verifying Distributed Intelligent Systems|. This paper describes first steps towards the formal specification and verification of Distributed Artificial Intelligence (DAI) systems, through the use of temporal belief logics. The paper first describes Concurrent  MetateM, a programming language for DAI, and then develops a logic that may be used to reason about Concurrent MetateM systems. The utility of this logic for specifying and verifying Concurrent MetateM systems is demonstrated through a number of examples. The paper concludes with a brief discussion of the wider implications of the work, and in particular on the use of similar logics for reasoning about DAI systems in general. 1 Introduction  In the past decade, the discipline of DAI has moved from being a somewhat obscure relation of mainstream AI to being a major research area in its own right. DAI techniques have been applied to domains as diverse as archaeology and economics, as well as more mundane problems such as distributed sensing and manufacturing control [7]. ...
773|Production sequencing as negotiation|The production sequencing problem involves a factory generating a product sequence such that when processed, the sequence will both satisfy current orders, and minimize overall costs. In this paper, we argue that production sequencing may fruitfully be considered as a negotiation problem, in which production cells within a factory negotiate over product sequences in order to fairly distribute costs. We begin by describing and formally defining the production sequencing problem; we then investigate its complexity, and present an analysis of it using the tools of game and negotiation theory. We then define a negotiation algorithm for production sequencing, and discuss what assumptions and simplifications must be made in order to make this algorithm suitable for implementation. We conclude by discussing issues for future work 1
774|A New Theory of Deadlock-Free Adaptive Routing in Wormhole Networks|Abstract- Second generation multicomputers use wormhole routing, allowing a very low channel setup time and drastically reducing the dependency between network latency and internode distance. Deadlock-free routing strategies have been developed, allowing the implementation of fast hardware routers that reduce the communication bottleneck. Also, adaptive routing algorithms with deadlock-avoidance or deadlock-recovery techniques have been proposed for some topologies, being very effective and outperforming static strategies. This paper develops the theoretical background for the design of deadlock-free adaptive routing algorithms for wormhole net-works. Some basic definitions and two theorems are proposed, developing conditions to verify that an adaptive algorithm is deadlock-free, even when there are cycles in the channel de-pendency graph. Also, two design methodologies are proposed.
775|Token flow control |As companies move towards many-core chips, an efficient onchip communication fabric to connect these cores assumes critical importance. To address limitations to wire delay scalability and increasing bandwidth demands, state-of-the-art on-chip networks use a modular packet-switched design with routers at every hop which allow sharing of network channels over multiple packet flows. This, however, leads to packets going through a complex router pipeline at every hop, resulting in the overall communication energy/delay being dominated by the router overhead, as opposed to just wire energy/delay. In this work, we propose token flow control (TFC), a flow control mechanism in which nodes in the network send out tokens in their local neighborhood to communicate information about their available resources. These tokens are then used in both routing and flow control: to choose less congested paths in the network and to bypass the router pipeline along those paths. These bypass paths are formed dynamically, can be arbitrarily long and, are highly flexible with the ability to match to a packetâ??s exact route. Hence, this allows packets to potentially skip all routers along their path from source to destination, approaching the communication energy-delaythroughput of dedicated wires. Our detailed implementation analysis shows TFC to be highly scalable and realizable at an aggressive target clock cycle delay of 21FO4 for large networks while requiring low hardware complexity. Evaluations of TFC using both synthetic traffic and traces from the SPLASH-2 benchmark suite show reduction in packet latency by up to 77.1 % with upto 39.6 % reduction in average router energy consumption as compared to a state-of-theart baseline packet-switched design. For the same saturation throughput as the baseline network, TFC is able to reduce the amount of buffering by 65 % leading to a 48.8 % reduction in leakage energy and a 55.4 % lower total router energy.  
776|Virtual cut-through: a new computer communication switching technique|In this paper a new switching technique called virtual cut-through is proposed and its performance is analyzed. This switching system is very similar to message switching, with the difference that when a message arrives in an intermediate node and its selected outgoing channel is free (just after the reception of the header), then, in contrast o message switching, the message is sent out to the adjacent node towards its destination before it is received completely at the node; only if the message isblocked due to a busy output channel is a message buffered in an intermediate node. Therefore, the delay due to unnecessary buffering in front of an idle channel is avoided. We analyze and compare the performance of this new switching technique with that of mes-sage switching with respect to three measures: network delay, traffic gain and buffer storage requirement. Our analysis hows that cut-through switching is superior (and at worst identical) to message switching with respect o the above three performance measures.
777|Performance Analysis of k-ary n-cube Interconnection Networks|Abstmct- VLSI communication networks are wire-limited. The cost of a network is not a function of the number of switches required, but rather a function of the wiring density required to construct the network. This paper analyzes commu-nication networks of varying dimension under the assumption of constant wire bisection. Expressions for the latency, average case throughput, and hot-spot throughput of k-ary n-cube networks with constant bisection are derived that agree closely with experi-mental measurements. It is shown that low-dimensional networks (e.g., tori) have lower latency and higher hot-spot throughput than high-dimensional networks (e.g., binary n-cubes) with the same bisection width. Index Terms- Communication networks, concurrent comput-ing, interconnection networks, message-passing multiprocessors, parallel processing, VLSI.
778|Formal Modeling and Analysis of an Audio/Video Protocol: An Industrial Case Study Using UPPAAL|A formal and automatic verification of a real-life protocol is presented. The protocol, about 2800 lines of assembler code, has been used in products from the audio/video company Bang &amp; Olufsen throughout more than a decade, and its purpose is to control the transmission of messages between audio/video components over a single bus. Such communications may collide, and one essential purpose of the protocol is to detect such collisions. The functioning is highly dependent on real-time considerations. Though the protocol was known to be faulty in that messages were lost occasionally, the protocol was too complicated in order for Bang &amp; Olufsen to locate the bug using normal testing. However, using the real-time verification tool UPPAAL, an error trace was automatically generated, which caused the detection of &#034;the error&#034; in the implementation. The error was corrected and the correction was automatically proven correct, again using UPPAAL. A future, and more automated, version of the proto...
779|Modeling and Validation of Java Multithreading Applications Using Spin |This paper presents some issues about the design and implementation of a concurrency analysis tool for deadlock detection on Java programs based on Promela and SPIN. An abstract formal model expressed in Promela is generated from the Java source using the Java2Spin translator. Then the model is analyzed by SPIN and possible error traces are converted back to traces of Java statements and reported to the user. We carried out a set of experiments, to evaluate the extent to which this approach is feasible, and found that non-trivial Java programs can be successfully analyzed.
780|Mechanical Verification of a Garbage Collector|Abstract. We describe how the PVS verification system has been used to verify a safety property of a garbage collection algorithm, originally suggested by Ben-Ari. The safety property basically says that “nothing but garbage is ever collected”. Although the algorithm is relatively simple, its parallel composition with a “user ” program that (nearly) arbitrarily modifies the memory makes the verification quite challenging. The garbage collection algorithm and its composition with the user program is regarded as a concurrent system with two processes working on a shared memory. Such concurrent systems can be encoded in PVS as state transition systems, very similar to the models of, for example, UNITY and TLA. The algorithm is an excellent test-case for formal methods, be they based on theorem proving or model checking. Various hand-written proofs of the algorithm have been developed, some of which are wrong. David Russinoff has verified the algorithm in the Boyer-Moore prover, and our proof is an adaption of this proof to PVS. We also model check a finite state version of the algorithm in the Stanford model checker Murphi, and we compare the result with the PVS verification. 1
781|Formal Verification of an Audio/Video Power Controller using the Real-Time Model Checker UPPAAL|An input/output--link control protocol is modeled and analyzed in the real-time model checker Uppaal. The protocol is supposed to sit in an audio/video component and control (read from and write to) a link to neighbour audio/video components. The component may for example be a TV, and a neighbour may be a VCR. The protocol also communicates with the remote--control. The protocol is in addition responsible for the powering up and down of the component in between the arrival of data. It is this power control that is the focus of the modeling and verification demonstrated in this report. The work has been carried out in a collaboration between Aalborg University and the audio/video company B&amp;O, which plans to incorporate the protocol as part of a new product line. The work was carried out in a limited period of 3 weeks, with an attempt to examine how well such a collaboration would proceed. The paper elaborates on the lessons learned. Amongst
782|The Stanford DASH multiprocessor|cache coherence gives
783|Reducing Memory and Traffic Requirements for Scalable Directory-Based Cache Coherence Schemes|As multiprocessors are scaled beyond single bus systems, there is renewed interest in directory-based cache coherence schemes. These schemes rely on a directory to keep track of all processors caching a memory block. When a write to that block occurs, pointto -point invalidation messages are sent to keep the caches coherent. A straightforward way of recording the identities of processors caching a memory block is to use a bit vector per memory block, with one bit per processor. Unfortunately, when the main memory grows linearly with the number of processors, the total size of the directory memory grows as the square of the number of processors, which is prohibitive for large machines. To remedy this problem several schemes that use a limited number of pointers per directory entry have been suggested. These schemes often cause excessive invalidation traffic. In this paper, we propose two simple techniques that significantly reduce invalidation traffic and directory memory requirements. ...
784|Software agents: An overview|Agent software is a rapidly developing area of research. However, the overuse of the word ‘agent ’ has tended to mask the fact that, in reality, there is a truly heterogeneous body of research being carried out under this banner. This overview paper presents a typology of agents. Next, it places agents in context, defines them and then goes on, inter alia, to overview critically the rationales, hypotheses, goals, challenges and state-of-the-art demonstrators of the various agent types in our typology. Hence, it attempts to make explicit much of what is usually implicit in the agents literature. It also proceeds to overview some other general issues which pertain to all the types of agents in the typology. This paper largely reviews software agents, and it also contains some strong opinions that are not necessarily widely accepted by the agent community. 1 1
785|Mediators in the architecture of future information systems|The installation of high-speed networks using optical fiber and high bandwidth messsage forwarding gateways is changing the physical capabilities of information systems. These capabilities must be complemented with corresponding software systems advances to obtain a real benefit. Without smart software we will gain access to more data, but not improve access to the type and quality of information needed for decision making. To develop the concepts needed for future information systems we model information processing as an interaction of data and knowledge. This model provides criteria for a high-level functional partitioning. These partitions are mapped into information process-ing modules. The modules are assigned to nodes of the distributed information systems. A central role is assigned to modules that mediate between the users &#039; workstations and data re-sources. Mediators contain the administrative and technical knowledge to create information needed for decision-making. Software which mediates is common today, but the structure, the interfaces, and implementations vary greatly, so that automation of integration is awkward. By formalizing and implementing mediation we establish a partitioned information sys-
786|BDI Agents: From Theory to Practice|The study of computational agents capable of  rational behaviour has received a great deal of  attention in recent years. Theoretical formalizations  of such agents and their implementations  have proceeded in parallel with little or  no connection between them. This paper explores  a particular type of rational agent, a BeliefDesire  -Intention (BDI) agent. The primary aim  of this paper is to integrate (a) the theoretical  foundations of BDI agents from both a quantitative  decision-theoretic perspective and a symbolic  reasoning perspective; (b) the implementations  of BDI agents from an ideal theoretical perspective  and a more practical perspective; and (c) the  building of large-scale applications based on BDI  agents. In particular, an air-traffic management  application will be described from both a theoretical  and an implementation perspective.  
788|NewsWeeder: Learning to Filter Netnews|A significant problem in many information filtering systems is the dependence on the user for the creation and maintenance of a user profile, which describes the user&#039;s interests. NewsWeeder is a netnews-filtering system that addresses this problem by letting the user rate his or her interest level for each article being read (1-5), and then learning a user profile based on these ratings. This paper describes how NewsWeeder accomplishes this task, and examines the alternative learning methods used. The results show that a learning algorithm based on the Minimum Description Length (MDL) principle was able to raise the percentage of interesting articles to be shown to users from 14% to 52% on average. Further, this performance significantly outperformed (by 21%) one of the most successful techniques in Information Retrieval (IR), termfrequency /inverse-document-frequency (tf-idf) weighting. 1
789|WebWatcher: A Learning Apprentice for the World Wide Web|We describe an information seeking assistant for the world wide web. This agent, called WebWatcher, interactively helps users locate desired information by employing learned knowledge about which hyperlinks are likely to lead to the target information. Our primary focus to date has been on two issues: (1) organizing WebWatcher to provide interactive advice to Mosaic users while logging their successful and unsuccessful searches as training data, and (2) incorporating machine learning methods to automatically acquire knowledge for selecting an appropriate hyperlink given the current web page viewed by the user and the user&#039;s information goal. We describe the initial design of WebWatcher, and the results of our preliminary learning experiments.  1 Overview  Many have noted the need for software to assist people in locating information on the world wide web. This paper  1  presents the initial design and implementation of an agent called WebWatcher that is intended to assist users both by...
790|A Softbot-Based Interface to the Internet|this article, we focus on the ideas underlying the softbot-based interface.
791|Experience With a Learning Personal Assistant|Personal software assistants that help users with tasks like finding information, scheduling calendars, or managing work-flow will require significant customization to each individual user. For example, an assistant that helps schedule a particular user’s calendar will have to know that user’s scheduling preferences. This paper explores the potential of machine learning methods to automatically create and maintain such customized knowledge for personal software assistants. We describe the design of one particular learning assistant: a calendar manager, called CAP (Calendar APprentice), that learns user scheduling preferences from experience. Results are summarized from approximately five user-years of experience, during which CAP has learned an evolving set of several thousand rules that characterize the scheduling preferences of its users. Based on this experience, we suggest that machine learning methods may play an important role in future personal software assistants. 
792|Designing a Family of Coordination Algorithms|Many researchers have shown that there is no single best organization or coordination mechanism  for all environments. This paper discusses the design and implementation of an extendable  family of coordination mechanisms, called Generalized Partial Global Planning (GPGP). The set  of coordination mechanisms described here assists in scheduling activities for teams of cooperative  computational agents. The GPGP approach has several unique features. First, it is not tied to  a single domain. Each mechanism is defined as a response to certain features in the current task  environment. We show that different combinations of mechanisms are appropriate for different  task environments. Secondly, the approach works in conjunction with an agent&#039;s existing local  planner/scheduler. Finally, the initial set of five mechanisms presented here generalizes and extends  the Partial Global Planning (PGP) algorithm. In comparison to PGP, GPGP allows more  agent heterogeneity, it exchanges less global ...
793|Collaborative Interface Agents|Interface agents are semi-intelligent systems which assist users with daily computer-based tasks. Recently, various researchers have proposed a learning approach towards building such agents and some working prototypes have been demonstrated. Such agents learn by `watching over the shoulder&#039; of the user and detecting patterns and regularities in the user&#039;s behavior. Despite the successes booked, a major problem with the learning approach is that the agent has to learn from scratch and thus takes some time becoming useful. Secondly, the agent&#039;s competence is necessarily limited to actions it has seen the user perform. Collaboration between agents assisting different users can alleviate both of these problems. We present a framework for multiagent collaboration and discuss results of a working prototype, based on learning agents for electronic mail. Introduction Learning interface agents are computer programs that employ machine learning techniques in order to provide assistance to a u...
794|Decision-Making in an Embedded Reasoning System |The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate e ectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability toreact rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.  
795|CIRCA: A Cooperative Intelligent Real-Time Control Architecture|Most research into applying AI techniques to real-time control problems has limited the power of AI methods or embedded &#034;reactivity&#034; in an AI system. We present an alternative, cooperative architecture that uses separate AI and real-time subsystems to address the problems for which each is designed; a structured interface allows the subsystems to communicate without compromising their respective performance goals. By reasoning about its own bounded reactivity, CIRCA can  guarantee that it will meet hard deadlines while still using unpredictable AI methods. With its abilities to guarantee or trade off the timeliness, precision, confidence, and completeness of its output, CIRCA provides more flexible performance than previous systems. Index Terms: Real-Time Control; Artificial Intelligence; Reactive Systems; Resource Scheduling; Planning; Cooperation; Intelligent Robotics. To appear in IEEE Transactions on Systems, Man, and Cybernetics, vol. 23, no. 6, 1993. The work reported in this pap...
796|A personal learning apprentice|Abstract requesting certain types of meetings. In order to be Personalized knowledge-based systems have not yet become widespread, despite their potential for valuable assistance in many daily tasks. This is due, in part, to the high cost of developing and maintaining customized knowledge bases. The construction of personal assistants as learning apprentices-- interactive assistants that learn continually from their users-- is one approach which could dramatically reduce the cost of knowledge-based advisors. We present one such personal learning apprentice, called CAP, which assists in managing a meeting calendar. CAP has been used since June 1991 by a secretary in our work place to manage a faculty member’s meeting calendar, and is the first instance of a fielded learning apprentice in routine use. This paper describes the organization of CAP, its performance in initial field tests, and more general lessons learned from this effort about learning apprentice systems.
797|Amalthaea: Information Discovery and Filtering using a Multiagent Evolving Ecosystem|Agents are semi-intelligent programs that assist the user in performing repetitive and time-consuming tasks. Information discovery and information filtering are a suitable domain for applying agent technology. Ideas drawn from the field of autonomous agents and artificial life are combined in the creation of an evolving ecosystem composed of competing and cooperating agents. A co-evolution model of information filtering agents that adapt to the various user&#039;s interests and information discovery agents that monitor and adapt to the various on-line information sources is analyzed. Results from a number of experiments are presented and discussed. Keywords: Agents, Information Filtering, Evolution, World-Wide-Web 1 Introduction The exponential increase of computer systems that are interconnected in on-line networks has resulted in a corresponding exponential increase in the amount of information available on-line. This information is distributed among heterogeneous sources and is often ...
798|Functionally accurate, cooperative distributed systems|A new approach for structuring distributed processing systems, called functionally accurate, cooperative (FA/C), is proposed. The approach differs from conventional ones in its emphasis on handling distribution-caused uncertainty and errors as an integral part of the network problem-solving process. In this approach nodes cooperatively problem solve by exchanging partial tentative results (at various levels of abstraction) within the context of common goals. The approach is especially suited to applications in which the data necessary to achieve a solution cannot be partitioned in such a way that a node can complete a task without seeing the intermediate state of task processing at other nodes. Much of the inspiration for the FA/C approach comes from the mechanisms used in knowledge-based artificial intelligence (AI) systems for resolving uncertainty caused by noisy input data and the use of approximate knowledge. The appropriateness of the FA/C approach is explored in three application domains: distributed interpretation, distributed network traffic-light control, and distributed planning. Additionally, the relationship between the approach and the structure of management organizations is developed. Finally, a number of current research directions necessary to more fully develop the FA/C approach are outlined. These research directions include distributed search, the integration of implicit and explicit forms of control, and distributed planning and organizational self-design. I.
799|The RBSE Spider -- Balancing Effective Search Against Web Load|The design of a Web spider entails many things, including a  concern for reasonable behavior, as well as more technical concerns.  The RBSE Spider is a mechanism for exploring World  Wide Web structure and indexing useful material thereby discovered.  We relate our experience in constructing and operating  this spider.   
800|An Overview of KQML: A Knowledge Query and Manipulation Language|We describe a language and protocol intended to support interoperability among  intelligent agents in a distributed application. Examples of applications envisioned  include intelligentmulti-agent design systems as well as intelligent planning, scheduling  and replanning agents supporting distributed transportation planning and scheduling  applications. The language, KQML for Knowledge Query and Manipulation Language,  is part of a larger DARPA-sponsored Knowledge Sharing effort focused on developing  techniques and tools to promote the sharing on knowledge in intelligent systems. e will  define the concepts which underly KQML and attempt to specify its scope and provide  a model for how it will be used.   
801|MICE: A Flexible Testbed for Intelligent Coordination Experiments|We describe a flexible testbed for studying a variety of different coordination issues. The MICE (Michigan Intelligent Coordination Experiment) testbed extends previous experimental systems that model interactions between agents that inhabit a two-dimensional world. MICE allows an experimenter to specify the constraints and characteristics of an environment in which agents are simulated to act and interact, and does not assume any particular implementation of an agent&#039;s reasoning architecture. MICE therefore provides a platform for investigating and evaluating alternative reasoning architectures and coordination mechanisms in many different simulated environments. We outline the design of MICE and illustrate its flexibility by describing simulated environments that model predators chasing prey, predators attacking each other, agents fighting a fire, and diverse robots that are working together. We conclude by discussing how MICE will act as a foundation for our future research. 0  This...
802|A Multi-Agent Referral System for Matchmaking|Many important and useful applications for software agents require multiple agents on a network that communicate with each other. Such agents must find each other and perform a useful joint computation without having to know about every other such agent on the network. This paper describes a matchmaker system, designed to find people with similar interests and introduce them to each other. The matchmaker is designed to introduce everyone, unlike conventional Internet media which only allow those who take the time to speak in public to be known. The paper details how the agents that make it up the matchmaking system can function in a decentralized fashion, yet can group themselves into clusters which reflect their users&#039; interests; these clusters are then used to make introductions or allow users to send messages to others who share their interests. The algorithm uses referrals from one agent to another in the same fashion that word-of-mouth is used when people are looking for an exper...
803|Ethical Web Agents|As the Web continues to evolve, the sophistication of the programs that are employed in interacting with it will also increase in sophistication. Web agents, programs acting autonomously on some task, are already present in the form of spiders. Agents offer substantial benefits and hazards, and because of this, their development must involve not only attention to technical details, but also the ethical concerns relating to their resulting impact. These ethical concerns will differ for agents employed in the creation of a service and agents acting on behalf of a specific individual. An ethic is proposed that addresses both of these perspectives. The proposal is predicated on the assumption that agents are a reality on the Web, and that there are no reasonable means of preventing their proliferation. 1 -- Introduction  The ease of construction and potential Internet-wide impact of autonomous software agents on the World Wide Web [1] has spawned a great deal of discussion and occasional c...
804|Specification and implementation of a belief desire joint-intention architecture for collaborative problem solving|Systems composed of multiple interacting problem solvers are becoming increasingly pervasive and have been championed in some quarters as the basis of the next generation of intelligent information systems. If this technology is to fulfill its true potential then it is important that the systems which are developed have a sound theoretical grounding. One aspect of this foundation, namely the model of collaborative problem solving, is examined in this paper. A synergistic review of existing models of cooperation is presented, their weaknesses are highlighted and a new model (called joint responsibility) is introduced. Joint responsibility is then used to specify a novel high-level agent architecture for cooperative problem solving in which the mentalistic notions of belief, desire, intention and joint intention play a central role in guiding an individual’s and the group’s problem solving behaviour. An implementation of this high-level architecture is then discussed and its utility is illustrated for the real-world domain of electricity transportation management.
805|MPI: A Message-Passing Interface Standard|process naming to allow libraries to describe their communication in terms suitable to their own data structures and algorithms,  ffl The ability to &#034;adorn&#034; a set of communicating processes with additional user-defined attributes, such as extra collective operations. This mechanism should provide a means for the user or library writer effectively to extend a message-passing notation. In addition, a unified mechanism or object is needed for conveniently denoting communication context, the group of communicating processes, to house abstract process naming, and to store adornments.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  5.1. INTRODUCTION 131 5.1.2 MPI&#039;s Support for Libraries  The corresponding concepts that MPI provides, specifically to support robust libraries, are as follows:  ffl Contexts of communication,  ffl Groups of processes,  ffl Virtual topologies,  ffl Attribute caching,  ffl Commun...
806|The Design and Evolution of Zipcode|Zipcode is a message-passing and process-management system that was designed for multicomputers and homogeneous networks of computers in order to support libraries and large-scale multicomputer software. The system has evolved significantly over the last five years, based on our experiences and identified needs. Features of Zipcode that were originally unique to it, were its simultaneous support of static process groups, communication contexts, and virtual topologies, forming the &#034;mailer&#034; data structure. Point-to-point and collective operations reference the underlying group, and use contexts to avoid mixing up messages. Recently, we have added &#034;gather-send&#034; and &#034;receive-scatter&#034; semantics, based on persistent Zipcode &#034;invoices,&#034; both as a means to simplify message passing, and as a means to reveal more potential runtime optimizations. Key features in Zipcode appear in the forthcoming MPI standard.  Keywords: Static Process Groups, Contexts, Virtual Topologies, Point-to-Point Communica...
807|Communicators: Object-Based Multiparty Interactions for Parallel Programming|Contemporary parallel programming languages often provide only few low-level primitives for  pairwise communication and synchronization. These primitives are not always suitable for the  interactions being programmed. Programming would be easier if it was possible to tailor communication  and synchronization mechanisms to fit the needs of the application, much as abstract  data types are used to create application-specific data structures and operations. This should  also include the possibility of expressing interactions among multiple processes at once. Communicators   support this paradigm by creating abstract communication objects that provide a  framework for interprocess multiparty interactions. The behavior of these objects is defined in  terms of interactions, in which multiple processes can enrole. Interactions are performed when  all the roles are filled by ready processes. Nondeterminism is used when the order of interaction  performance is immaterial. Interactions can also ...
809|Maisie: A Language for the Design of Efficient Discrete-event Simulations|Maisie is a C-based discrete-event simulation language that was designed to cleanly separate a simulation model from the underlying algorithm (sequential or parallel) used for the execution of the model. With few modifications, a Maisie program may be executed using a sequential simulation algorithm, a parallel conservative algorithm or a parallel optimistic algorithm. The language constructs allow the runtime system to implement optimizations that reduce recomputation and state saving overheads for optimistic simulations and synchronization overheads for conservative implementations. This paper presents the Maisie simulation language, describes a set of optimizations and illustrates the use of the language in the design of efficient parallel simulations. 1 Introduction Distributed (or parallel) simulation refers to the execution of a simulation program on parallel computers. A number of algorithms[25, 10, 11, 21, 20] have been suggested for distributed simulation and many experimental...
810|The Department Of Defense High Level Architecture|The High Level Architecture (HLA) provides the specification of a common technical architecture for use across all classes of simulations in the US Department of Defense. It provides the structural basis for simulation interoperability. The baseline definition of the HLA includes (1) the HLA Rules, (2) the HLA Interface Specification, and (3) the HLA Object Model Template. This paper describes the motivations and processes used to develop the High Level Architecture and provides a technical description of key elements of the architecture and supporting software. Services defined in the interface specification for providing time management (TM) and data distribution management (DDM) for distributed simulations are described.  1. INTRODUCTION The Defense Modeling and Simulation Office (DMSO), is addressing the continuing need for interoperability among new and existing simulations within the U.S. Department of Defense through its High Level Architecture (HLA) initiative. The HLA seeks to...
811|SRADS With Local Rollback|There is reason to believe bounded aggressive processing (limiting the degree to which processes act on conditional knowledge) may be a good alternative to unbounded processing. Simulations characterized by substantial variance in logical process processing times can lead to conditions where, without bound, some processes may cause frequent repairs (e.g. rollbacks) to occur. We present an algorithm, SRADS/LR, in which we bound aggressiveness and discuss its expected impact on performance.  
812|Parallelized direct execution simulation of message-passing parallel programs|As massively parallel computers proliferate, there is growing interest in ?nding ways by which performance of massively parallel codes can be e?ciently predicted. This problem arises in diverse contexts such as parallelizing compilers, parallel performance monitoring, and parallel algorithm development. In this paper we describe one solution where one directly executes the application code, but uses a discrete-event simulator to model details of the presumed parallel machine, such as operating system and communication network behavior. Because this approach is computationally expensive, we are interested in its own parallelization, speci?cally the parallelization of the discrete-event simulator. We describe methods suitable for parallelized direct execution simulation of message-passing parallel programs, and report on the performance of such a system, LAPSE ?Large Application Parallel Simulation Environment?, wehave built on the Intel Paragon. On all codes measured to date, LAPSE predicts performance well, typically within 10 ? relative error. Depending on the nature of the application code, we have observed low slowdowns ?relative to natively executing code ? and high relative speedups using up to 64 processors.
813|Probabilistic Adaptive Direct Optimism Control in Time Warp|In a distributed memory environment the communication overhead of Time Warp as induced by the rollback procedure due to &#034;overoptimistic&#034; progression of the simulation is the dominating performance factor. To limit optimism to an extent that can be justified from the inherent model parallelism, an optimism control mechanism is proposed, which by maintaining a history record of virtual time differences from the time stamps carried by arriving messages, and forecasting the timestamps of forthcoming messages, probabilistically delays the execution of scheduled events to avoid potential rollback and associated communication overhead (antimessages). After investigating statistical forecast methods which express only the central tendency of the arrival process, we demonstrate that arrival processes in the context of Time Warp simulations of timed Petri nets have certain predictable and consistent ARIMA characteristics, which encourage the use of sophisticated and recursive forecast procedures...
814|A Case Study in Simulating PCS Networks Using Time Warp|There has been rapid growth in the demand for mobile communications over the past few years. This has led to intensive research and development efforts for complex PCS (personal communication service) networks. Capacity planning and performance modeling is necessary to maintain a high quality of service to the mobile subscriber while minimizing cost to the PCS provider. The need for flexible analysis tools and the high computational requirements of large PCS network simulations make it an excellent candidate for parallel simulation. Here, we describe our experiences in developing two PCS simulation models on a general purpose distributed simulation platform based on the Time Warp mechanism. These models utilize two widely used approaches to simulating PCS networks: (i) the call-initiated and (ii) the portable-initiated models. We discuss design decisions that were made in mapping these models to the Time Warp executive, and characterize the workloads resulting from these models in term...
815|R.D.: Parallel logic simulation of VLSI systems|Fast, efficient logic simulators are an essential tool in modern VLSI system design. Logic simulation is used extensively for design verification prior to fabrication, and as VLSI systems grow in size, the execution time required by simulation is becoming more and more significant. Faster logic simulators will have an appreciable economic impact, speeding time to market while ensuring more thorough system design testing. One approach to this problem is to utilize parallel processing, taking advantage of the concurrency available in the VLSI system to accelerate the logic simulation task. Parallel logic simulation has received a great deal of attention over the past several years, but this work has not yet resulted in effective, high-performance simulators being available to VLSI designers. A number of techniques have been developed to investigate performance issues: formal models, performance modeling, empirical studies, and prototype implementations. Analyzing reported results of these techniques, we conclude that five major factors affect performance: synchronization algorithm, circuit structure, timing granularity, target architecture, and partitioning. After reviewing techniques for parallel simulation, we consider each of these factors using results reported in the literature. Finally we synthesize the results and present directions for future research in the field.
816|Real-Time Causal Message Ordering in Multimedia Systems|In multimedia systems, not only do messages that are sent to and received by multiple sites need to have a consistent order imposed by all sites, but cause and effect relations must be maintained. Causal ordering allows the cause and effect relations of messages to be maintained. This paper presents an algorithm that insures that multimedia data with real-time deadlines are delivered to the application layer in causal order. The algorithm is designed to insure that any message that arrives at a destination site before its deadline will be delivered to the application before the message expires. In addition, by focusing on a form of causal ordering violations caused by &#034;the triangle inequality,&#034; this algorithm has a low overhead with respect to the amount of information that must be appended to each message.  Keywords: Real-time, causal ordering, multimedia systems, delta-causality, triangle inequality.  1. Introduction  Multimedia systems are becoming widespread [2][3][6][7]. They are ...
817|Parallel Discrete Event Simulation Using Space-Time Memory|An abstraction called space-time memory is discussed that allows parallel discrete event simulation  programs using the Time Warp mechanism to be written using shared memory constructs. A  few salient points concerning the implementation and use of space-time memory in parallel simulation  are discussed. It is argued that this abstraction is useful from a programming standpoint  for certain applications, and can yield good performance. Initial performance measurements of a  prototype implementation of the abstraction on a shared-memory multiprocessor are described,  and compared with a conventional, message-based implementation of Time Warp.  
818|Parallel simulation of stochastic petri nets using recurrence equations|0
819|Simon: A simulator of multicomputer networks|A diusion approximation for a generalized Jackson
820|PORTS: A Parallel, Optimistic, Real-Time Simulator|This paper describes issues concerning the design of an optimistic parallel discrete event simulation system that executes in environments that impose real-time constraints on the simulator&#039;s execution. Two key problems must be addressed by such a system. First the timing characteristics of the parallel simulator must be sufficiently  predictable to allow one to guarantee that real-time deadlines for completing simulation computations will be met. Second, the optimistic computation must be able to interact with its surrounding environment with as little latency as possible, necessitating rapid commitment of I/O operations. To address the first question, we show that optimistic simulators that never send incorrect messages (sometimes called &#034;aggressiveno -risk&#034; simulators) provide sufficient predictability to allow traditional schedulability analysis techniques commonly used in realtime systems to be applied. We show that incremental state saving techniques introduce sufficient unpredic...
821|A Sweep Algorithm for Massively Parallel Simulation of Circuit-Switched Networks|p..
823|Optimistic Simulation of Parallel Architectures Using Program Executables|A key tool of computer architects is computer simulation at the level of detail that can execute program executables. The time and memory requirements of such simulations can be enormous, especially when the machine under design---the target---is a parallel machine. Thus, it is attractive to use parallel simulation, as successfully demonstrated by the Wisconsin Wind Tunnel  (WWT). WWT uses a conservative simulation algorithm and eschews network simulation to make lookahead adequate. Nevertheless, we find most of WWT&#039;s slowdown to be due to the synchronization overhead in the conservative simulation algorithm. This paper examines the use of optimistic algorithms to perform parallel simulations of parallel machines. We first show that we can make optimistic algorithms work correctly even with WWT&#039;s direct execution of program executables. We checkpoint processor registers (integer, floating-point, and condition codes) and use executable editing to log the value of memory words just befor...
824|Breadth-First Rollback in Spatially Explicit Simulations|The efficiency of Parallel Discrete Event Simulations that use the optimistic protocol is strongly dependent on the overhead incurred by rollbacks. This paper introduces a novel approach to rollback processing which limits the number of events rolled back as a result of a straggler or antimessage. The method, called Breadth-First Rollback (BFR), is suitable for spatially explicit problems where the space is discretized and distributed among processes and simulation objects move freely in the space. BFR uses incremental state saving, allowing the recovery of causal relationships between events during rollback. These relationships are then used to determine which events need to be rolled back. Our results demonstrate an almost linear speedup--a dramatic improvement over the traditional approach to rollback processing.
825|Optimistic Distributed Simulation Based on Transitive Dependency Tracking|In traditional optimistic distributed simulation protocols, a logical process(LP) receiving a straggler rolls back and sends out anti-messages. Receiver of an anti-message may also roll back and send out more anti-messages. So a single straggler may result in a large number of anti-messages and multiple rollbacks of some LPs. In our protocol, an LP receiving a straggler broadcasts its rollback. On receiving this announcement, other LPs may roll back but they do not announce their rollbacks. So each LP rolls back at most once in response to each straggler. Antimessages are not used. This eliminates the need for output queues and results in simple memory management. It also eliminates the problem of cascading rollbacks and echoing, and results in faster simulation. All this is achieved by a scheme for maintaining transitive dependency information. The cost incurred includes the tagging of each message with extra dependency information and the increased processing time upon receiving a me...
826|A Multidimensional Study on the Feasibility of Parallel Switch-Level Circuit Simulation|This paper presents the results of an experimental study to evaluate the effectiveness of multiple synchronization protocols and partitioning algorithms in reducing the execution time of switch-level models of VLSI circuits. Specific contributions of this paper include: (i) parallelizing an existing switch-level simulator such that the model can be executed using conservative and optimistic simulation protocols with minor changes, (ii) evaluating effectiveness of several partitioning algorithms for parallel simulation, and (iii) demonstrating speedups with both conservative and optimistic simulation protocols for seven circuits, ranging in size from 3K transistors to about 87K transistors.   1 1 Introduction  VLSI circuit designs may be simulated at many levels of detail, ranging from abstract functional specifications to the more detailed gate, switch and circuit levels. In general, both the accuracy and the execution time of a simulation model increase with the level of detail. Paral...
827|A New Approach for Partitioning VLSI Circuits on Transistor Level|For parallel simulation of VLSI circuits on transistor level a sophisticated partitioning of the circuits into subcircuits is crucial. Each net connecting the subcircuits causes additional communication and computation effort. As the slave processors simulating the subcircuits advance synchronously in time, the computation effort for each subcircuit should be approximately the same. In this paper a new approach for partitioning VLSI circuits on transistor level yielding a low number of interconnects between the subcircuits and balanced subcircuit sizes is presented. Simulation of industrial circuits using this partitioning is up to 41% faster than with other known partitioning approaches for parallel analog simulation.
828|Channel Assignment Schemes for Cellular Mobile Telecommunication Systems|This paper provides a detailed discussion of wireless resource and channel allocation schemes. We provide a survey of a large number of published papers in the area of fixed, dynamic and hybrid allocation schemes and compare their trade-offs in terms of complexity and performance. We also investigate these channel allocation schemes based on other factors such as distributed/centralized control and adaptability to traffic conditions. Moreover, we provide a detailed discussion on reuse partitioning schemes, effect of hand-offs and prioritization schemes. Finally, we discuss other important issues in resource allocation such as overlay cells, frequency planning, and power control. 1 Introduction  Technological advances and rapid development of handheld wireless terminals have facilitated the rapid growth of wireless communications and mobile computing. Taking ergonomics and economics factors into account, and considering the new trends in the telecommunications industry to provide ubiqui...
829|Radio Link Admission Algorithms for Wireless Networks with Power Control and Active Link Quality Protection|In this paper we present a distributed power control scheme, which maintains the SIRs of operational (active) links above their required thresholds at all time (link quality protection), while new users are being admitted; furthermore, when new users cannot be successfully admitted, existing ones do not suffer fluctuations of their SIRs below their required thresholds values. We also present two admission /rejection control algorithms, which exercise voluntary drop-out of links inadmissible to the network, so as to reduce interference and possibly facilitate the admission of other links.
830|Workflow Mining: Discovering process models from event logs|Contemporary workflow management systems are driven by explicit process models, i.e., a completely specified workflow design is required in order to enact a given workflow process. Creating a workflow design is a complicated time-consuming process and typically there are discrepancies between the actual workflow processes and the processes as perceived by the management. TherefS3A we have developed techniques fi discovering workflow models. Starting pointfS such techniques is a so-called &#034;workflow log&#034; containinginfg3SfiHfl&#034; about the workflow process as it is actually being executed. We present a new algorithm to extract a process modelf3q such a log and represent it in terms of a Petri net. However, we will also demonstrate that it is not possible to discover arbitrary workflow processes. In this paper we explore a classof workflow processes that can be discovered. We show that the #-algorithm  can successfqFS mine any workflow represented by a so-called SWF-net. Key words: Workflow mining, Workflow management, Data mining, Petri nets. 1 
831|Discovery of frequent episodes in event sequences|Abstract. Sequences of events describing the behavior and actions of users or systems can be collected in several domains. An episode is a collection of events that occur relatively close to each other in a given partial order. We consider the problem of discovering frequently occurring episodes in a sequence. Once such episodes are known, one can produce rules for describing or predicting the behavior of the sequence. We give efficient algorithms for the discovery of all frequent episodes from a given class of episodes, and present detailed experimental results. The methods are in use in telecommunication alarm management. Keywords: event sequences, frequent episodes, sequence analysis 1.
832|Discovering Models of Software Processes from Event-Based Data|this article we describe a Markov method that we developed specifically for process discovery, as well as describe two additional methods that we adopted from other domains and augmented for our purposes. The three methods range from the purely algorithmic to the purely statistical. We compare the methods and discuss their application in an industrial case study.
833|Mining Process Models from Workflow Logs|Modern enterprises increasingly use the workflow paradigm to prescribe how business processes should be performed. Processes are typically modeled as annotated activity graphs. We present an approach for a system that constructs process models from logs of past, unstructured executions of the given process. The graph so produced conforms to the dependencies and past executions present in the log. By providing models that capture the previous executions of the process, this technique allows easier introduction of a workflow system and evaluation and evolution of existing process models. We also present results from applying the algorithm to synthetic data sets as well as process logs obtained from an IBM Flowmark installation.
834|Software Process Validation: Quantitatively Measuring the Correspondence of a Process to a Model|this article.
835|Inductive Inference, DFAs and Computational Complexity|This paper surveys recent results concerning the inference of deterministic finite automata (DFAs). The results discussed determine the extent to which DFAs can be feasibly inferred, and highlight a number of interesting approaches in computational learning theory. 1
836|Event-Based Detection of Concurrency|Understanding the behavior of a system is crucial in being able to modify, maintain, and  improve the system. A particularly difficult aspect of some system behaviors is concurrency. While there are
837|A Machine Learning Approach to Workflow Management|There has recently been some interest in applying machine learning techniques to support the acquisition and adaptation of workflow models. The different learning algorithms, that have been proposed, share some restrictions, which may prevent them from being used in practice. Approaches applying techniques from grammatical inference are restricted to sequential workflows. Other algorithms allowing concurrency require unique activity nodes. This contribution shows how the basic principle of our previous approach to sequential workflow induction can be generalized, so that it is able to deal with concurrency. It does not require unique activity nodes. The presented approach uses a log-likelihood guided search in the space of workflow models, that starts with a most general workflow model containing unique activity nodes.
838|Expressiveness and Suitability of Languages for Control Flow Modelling in Workflows|Workflow management has been a research area that attracted significant interest in the last decade. In spite of this, little consensus has been reached as to what the essential ingredients of workflow specification languages should be. Consequently many workflow management systems have been developed that are based on different paradigms and include different feature sets. These differences result in various levels of suitability and expressive power.
839|Improving Business Process Quality through Exception Understanding, Prediction, and Prevention|Business process automation technologies are  being increasingly used by many companies to  improve the efficiency of both internal processes  as well as of e-services offered to customers. In  order to satisfy customers and employees,  business processes need to be executed with a  high and predictable quality. In particular, it is  crucial for organizations to meet the Service  Level Agreements (SLAs) stipulated with the  customers and to foresee as early as possible the  risk of missing SLAs, in order to set the right  expectations and to allow for corrective actions.  In this paper we focus on a critical issue in  business process quality: that of analyzing,  predicting and preventing the occurrence of  exceptions, i.e., of deviations from the desired or  acceptable behavior. We characterize the  problem and propose a solution, based on data  warehousing and mining techniques. We then  describe the architecture and implementation of a  tool suite that enables exception analysis,  prediction, and prevention. Finally, we show  experimental results obtained by using the tool  suite to analyze internal HP processes.  1. 
840|Integrating Machine Learning and Workflow Management to Support Acquisition and Adaptation of Workflow Models|Current workflow management systems (WFMS) offer little aid for the acquisition of workflow models  and their adaptation to changing requirements. To support these activities we propose to apply techniques  from machine learning, which enable an inductive approach to workflow acquisition and adaptation. We present
841|Discovering Workflow Performance Models from Timed Logs|Contemporary workflow management systems are ch&#039;iven by  explicit process models, i.e., a completely specified workflow design is  required in order to enact a given workflow process. Creating a workflow  design is a complicated time-consuming process and typically there are  discrepancies between the actual workflow processes and the processes as  perceived by the management. Therefore, we have developed techniques  for discovering workflow models. Starting point for such techniques are  so-called &#034;workflow logs&#034; containing information about the workflow process  as it is actually being executed. In this paper, we extend our existing  mining technique c [4] to incorporate time. We assume that events in  workflow logs bear timestamps. This information is used to attribute timing  such as queue times to the discovered workflow model. The approach  is based on Petri nets and timing information is attached to places. This  paper also presents our workflow-mining tool EMIT. This tool translates  the workflow log of several commercial systems (e.g., Staffware) to an independent  XML format. Based on this format the tool mines for causal  relations and produces a graphical workflow model expressed in terms of  Petri nets.
842|Workflow-based Process Monitoring and Controlling - Technical and Organizational Issues|Workflow management systems enable the exact and timely analysis of automated business processes through the analysis of the logged audit trail data. Within the research project CONGO 1 we develop a process analysis tool (PISA) that can be employed to analyze the audit trail data of different workflow management systems in conjunction with target data from business process modeling tools. A working prototype has been completed that integrates data of the ARIS Toolset and IBM MQSeries Workflow. The analysis focuses on three different perspectives – processes and functions, involved resources, and process objects. We outline the economic aspects of workflow-based process monitoring and controlling and the current state of the art in monitoring facilities provided by current workflow management systems and existing standards. After a discussion of the three evaluation perspectives, sample evaluation methods for each perspective are discussed. The concept and architecture of PISA are described and implementation issues are outlined before an outlook on further research is given. 1. Distributed process information systems 1.1. Workflow monitoring and controlling The need to serve customers in global markets and the tendency towards smaller, more flexible, less hierarchical organizations leads to an increasing spatial distribution of companies. This distribution of formerly centralized enterprises on the one side and the (temporary) integration
843|Process Mining Discovering Workflow Models from Event-Based Data|Contemporary workflow management systems are driven by explicit process models,  i.e., a completely specified workflow design is required in order to enact a given  workflow process. Creating a workflow design is a complicated time-consuming  process and typically there are discrepancies between the actual workflow processes  and the processes as perceived by the management. Therefore, we propose a technique  for process mining. This technique uses workflow logs to discover the workflow  process as it is actually being executed. The process mining technique proposed in this  paper can deal with noise and can also be used to validate workflow processes by  uncovering and measuring the discrepancies between prescriptive models and actual  process executions.
844|Business Process Cockpit| Business Process Cockpit (BPC) is a tool that supports real-time monitoring, analysis, management, and optimization of business processes running on top of HP Process Manager, the Business Process Management System developed by Hewlett-Packard. The main goal of the Business Process Cockpit is to enable business users to perform business-level quality analysis, monitoring, and management of business processes. The BPC visualizes process execution data according to different focus points that identify the process entities that are the focus of the analysis, and different perspectives that define a way to look at the information. The BPC also allows users to define new concepts, such as “slow” and “fast” executions, and use those concepts to categorize the viewed data and make it much easier for users to interpret.
845|Dealing with Concurrency in Workflow Induction|Workflow management systems (WFMS) can be used in CE to manage the increasing communication and coordination efforts. For this purpose WFMS require an explicit model of the processes that are to be supported. Creating such a workflow model and adapting it to changing requirements is a time consuming and error prone task. For this reason we are investigating induction techniques from machine learning to support these tasks. Our previous approach (Herbst, 1999) to workflow induction was restricted to sequential processes. In this contribution we show, how the basic idea of our previous approach can be generalized, so that it is able to deal with concurrent processes. The applicability is demonstrated using a simplified version of a release process of the Mercedes Benz passenger car division. 
846|An Inductive Approach to the Acquisition and Adaptation of Workflow Models|Current workflow management systems  (WFMS) offer little aid for the acquisition  of workflow models and their adaptation to  changing requirements. To support these activities  we propose an approach, that induces  workflow models from workflow instances. In  this contribution we focus on the induction  of the workflow structure and report about  ongoing research in this area. We define four  problem classes in terms of characteristics of  the target model. For three of these classes  solutions are presented. These solutions have  been implemented in a research prototype,  which we have validated using artificially  generated workflow instances. The induced  workflow models can be imported by the  commercial business process management  system ADONIS  1  . We also outline a solution  of the fourth and most general problem class.  1 Introduction  Acquisition of Workflow Models  One of the most time consuming tasks within a workflow project is the acquisition of the workflow model. In v...
847|A Data Warehouse for Workflow Logs|Workflow Logs provide a very valuable source of information  about the actual execution of business processes in organizations.
848|Daelemans. Automated discovery of workflow models from hospital data|Abstract. Workflow nets, a subclass of Petri nets, are known as at-tractive models for analysing complex business processes. In a hospi-tal environment, for example, the processes show a complex and dy-namic behavior, which is difficult to control; the workflow net which models such a complex process provides a good insight into it, and due to its formal representation offers techniques for improved con-trol. We propose a method whose main advantage consists in discov-ering the workflow Petri nets automatically from process logs. We il-lustrate the functioning of our method on simulated hospital process logs, containing information about medical actions over time. The re-sults of our experiments indicate that this method is able to discover processes whose underlying models are acyclic and sound WF nets, involving parallel, conditional and sequential constructs. We argue that solutions have to be found for cyclic and free-choice/non-free-choice workflow nets. 1
849|Decomposition of event sequences into independent components|Many real-world processes result in an extensive logs of sequences of events, i.e., events coupled with time of occurrence. Examples of such process logs include alarms produced by a large telecommunication network, web-access data, biostatistics, etc. In many cases, it is useful to decompose the incoming stream of events
850|The Turn Model for Adaptive Routing| This paper presents a model for designing wormhole routing algorithms, A unique feature of the model is th~t lt is not based cm adding physical or virtual channels to direct networks (although it can be applied to networks with extra channels). Instead, the model is based [In analyzlng the directions in which packets can turn m a network and the cycles that the turns can form. Prohibiting just enough turns to brezk all of the cycles produces routing algorithms that are deadlock free, livelock free, mmimal or nonminimal, and highly adaptive. This paper focuses on the two most common network topologies for wormhole routing, ~z-dimensional meshes and k-ary /z-cubes without extra channels. In such networks, just a quarter of the turns must be prohibited to prevent deadlock. The remaining three quarters of the turns allow routing to be fidaptwe, Adaptive routing algorithms are described for two-dimensional meshes, n-dimensional meshes k-my ~t-cubes, and hypercubes. Simulations of adaptive and nonadaptive routing algo-rithms show which algorlthm has the lowest latcncies and highest sustainable throughput depends on the pattern of message traffic. For nonuniform traffic, adaptive routing algorithms generally perform better than nonadaptive ones.
851|APRIL: A Processor Architecture for Multiprocessing|Processors in large-scale multiprocessors must be able to tolerate large communication latencies and synchronization delays. This paper describes the architecture of a rapid-context-switching processor called APRIL with support for fine-grain threads and synchronization. APRIL achieves high single-thread performance and supports virtual dynamic threads. A commercial RISC-based implementation of APRIL and a run-time software system that can switch contexts in about 10 cycles is described. Measurements taken for several parallel applications on an APRIL simulator show that the overhead for supporting parallel tasks based on futures is reduced by a factor of twoover a corresponding implementation on the Encore Multimax. The scalability of a multiprocessor based on APRIL is explored using a performance model. We show that the SPARC-based implementation of APRIL can achieve close to 80# processor utilization with as few as three resident threads per processor in a large-scale cache-based machine with an average base network latency of 55 cycles.
852|Cooperative mobile robotics: Antecedents and directions|There has been increased research interest in systems composed of multiple autonomous mobile robots exhibiting collective behavior. Groups of mobile robots are constructed, with an aim to studying such issues as group architecture, resource conflict, origin of cooperation, learning, and geometric problems. As yet, few applications of collective robotics have been reported, and supporting theory is still in its formative stages. In this paper, we give a critical survey of existing works and discuss open problems in this field, emphasizing the various theoretical issues that arise in the study of cooperative robotics. We describe the intellectual heritages that have guided early research, as well as possible additions to the set of existing motivations.  
853|Markov games as a framework for multi-agent reinforcement learning|In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.
854|From local actions to global tasks: Stigmergy and collective robotics|This paper presents a series of experiments where a group of mobile robots gather 81 randomly distributed objects and cluster them into one pile. Coordination of the agents ’ movements is achieved through stigmergy. This principle, originally developed for the description of termite building behaviour, allows indirect communication between agents through sensing and modification of the local environment which determines the agents’ behaviour. The efficiency of the work was measured for groups of one to five robots working together. Group size is a critical factor. The mean time to accomplish the task decreases for one, two, and three robots respectively, then increases again for groups of four and five agents, due to an exponential increase in the number of interactions between robots which are time consuming and may eventually result in the destruction of existing clusters. We compare our results with those reported by Deneubourg et al. (1990) where similar clusters are observed in ant colonies, generated by the probabilistic behaviour of workers. 
855|Concurrent Online Tracking of Mobile Users|This paper deals with the problem of maintaining a distributed directory server, that enables us to keep track of mobile users in a distributed network in the presence of concurrent requests. The paper uses the graph-theoretic concept of regional matching for implementing efficient tracking mechanisms. The communication overhead of our tracking mechanism is within a polylogarithmic factor of the lower bound. 1 Introduction  Since the primary function of a communication network is to provide communication facilities between users and processes in the system, one of the key problems such a network faces is the need to be able to    Department of Mathematics and Lab. for Computer Science, M.I.T., Cambridge, MA 02139, USA. E-mail: baruch@theory.lcs.mit.edu. Supported by Air Force Contract TNDGAFOSR-86-0078, ARO contract DAAL03-86-K0171, NSF contract CCR8611442, DARPA contract N00014-89J -1988, and a special grant from IBM.  y  Departmentof Applied Mathematicsand Computer Science, The Weizm...
856|Learning and Sequential Decision Making|In this report we show how the class of adaptive prediction methods that Sutton called &#034;temporal difference,&#034; or TD, methods are related to the theory of squential decision making. TD methods have been used as &#034;adaptive critics&#034; in connectionist learning systems, and have been proposed as models of animal learning in classical conditioning experiments. Here we relate TD methods to decision tasks formulated in terms of a stochastic dynamical system whose behavior unfolds over time under the influence of a decision maker&#039;s actions. Strategies are sought for selecting actions so as to maximize a measure of long-term payoff gain. Mathematically, tasks such as this can be formulated as Markovian decision problems, and numerous methods have been proposed for learning how to solve such problems. We show how a TD method can be understood as a novel synthesis of concepts from the theory of stochastic dynamic programming, which comprises the standard method for solving such tasks when a model of the dynamical system is available, and the theory of parameter estimation, which provides the appropriate context for studying learning rules in the form of equations for updating associative strengths in behavioral models, or connection weights in connectionist networks. Because this report is oriented primarily toward the non-engineer interested in animal learning, it presents tutorials on stochastic sequential decision tasks, stochastic dynamic programming, and parameter estimation.
857|Collective Robotics: From Social Insects to Robots|Achieving tasks with a multiple robot system will require a control system that is both simple and scalable as the number of robots increases. Collectivebehavior as demonstrated by social insects is a form of decentralized control that may prove useful in controlling multiple robots. Nature&#039;s several examples of collective behavior have motivated our approach to controlling a multiple robot system using a group behavior. Our mechanisms, used to invoke the group behavior, allow the system of robots to perform tasks without centralized control or explicit communication. We have constructed a system of ve mobile robots capable of achieving simple collective tasks to verify the results obtained in simulation. The results suggest that decentralized control without explicit communication can be used in performing cooperative tasks requiring a collective behavior.  
858|Group Behaviors for Systems with Significant Dynamics|Birds, fish, and many other animals travel as a flock, school, or herd. Animals in these groups must remain in close proximity while avoiding collisions with neighbors and with obstacles. We would like to reproduce this behavior for groups of artificial creatures with significant dynamics. In this paper we describe an algorithm for creatures that move as a group and evaluate the performance of the algorithm with three simulated systems: legged robots, human-like bicycle riders, and point-mass systems. Both the leggedrobots and the bicyclists are dynamic simulations that must control balance, facing direction, and forward speed as well as movement with the group. The point-mass systems have minimal dynamics and are included to facilitate our understanding of the effects of the dynamics on the performance of the algorithms.
859|C ooperation without Communication|Intelligent agents must be able to interact even with-out the benefit of communication. In this paper we examine various constraints on the actions of agents in such situations and discuss the effects of these con-straints on their derived utility. In particular, we de-fine and analyze basic raiionaliiy; we consider various assumptions about independence; and we demonstrate the advantages of extending the definition of rational-ity from individual actions to decision procedures. I
860|From Tom Thumb to the Dockers: Some Experiments with Foraging Robots|In this paper, we experiment, from the point of view of their efficiency, different implementations of the &#034;explorer robots application&#034;. Three types of &#034;Tom Thumb robots&#034;, whose behavior is based on the foraging behaviors of ants are proposed and their results are critically examined. We then introduce chain-making robots (the &#034;dockers&#034;), governed by local perceptions and interactions. This helps us to show that only a few changes in the robots&#039; behavior may greatly improve the efficiency of the population. Introduction  In the research conducted in the field of swarm intelligence, the &#034;explorer robots application&#034; appears to be one of the most common examples found to illustrate the capacity of a population of poorly intelligent creatures to handle with a global goal. In this case study, the goal is to make a team of robots find and collect samples in an unpredictable environment and take them back to a home base. These robots usually operate independently and behave in a quite simpl...
861|A theory of action for multiagent planning|A theory of action suitable for reasoning about events in multiagent or dynamically changing environments is pre-scntcrl. A device called a process model is used to represent the observable behavior of an agent in performing an ac-tion. This model is more general than previous models of act ion, allowing sequencing, selection, nondeterminism, it-eration, and parallelism to be represented. It is shown how this model can be utilized in synthesizing plans and rea-soning about concurrency. In parbicular, conditions are de-rived for determining whether or not concurrent actions are free from mutual interference. It is also indicated how this theory pro!.ides a basis for understanding and reasoning about act,ion sentences in both natural and programming lariguagcs. 1.
862|Collective Robotic Intelligence|In this paper, we examine the problem of controlling multiple behaviour-based autonomous robots. Based on observations made from the study of social insects, we propose ve simple mechanisms used to invoke group behaviour in simple sensor-based mobile robots. The proposed mechanisms allow populations of behaviour-based robots to perform tasks without centralized control or use of explicit communication. We have veri ed our collective control strategies by designing a robot population simulator called SimbotCity. Wehave also constructed a system of ve homogeneous sensor-based mobile robots, capable of achieving simple collective tasks, to demonstrate the feasibility of some of the control mechanisms.
863|Integration of Reactive and Telerobotic Control in Multi-agent Robotic Systems|Multi-agent schema-based reactive robotic systems are complemented with the addition of a new behavior controlled by a teleoperator. This enables the whole society to be affected as a group rather than forcing the operator to control each agent individually. The operator is viewed by the reactive control system as another behavior exerting his/her influence on the society as a whole. Simulation results are presented for foraging, grazing, and herding tasks. Teleautonomous operation of multi-agent reactive systems was demonstrated to be significantly useful for some tasks, less so for others.  1 Introduction  Reactive multi-agent robotic societies can be potentially useful for a wide-range of tasks. This includes operations such as foraging and grazing (e.g., [1,9,6]) which have applicability in service (vacuuming and cleaning), industrial (assembly) and military (convoy and scouting) scenarios. Although promising results have been achieved in these systems to date, purely reactive syst...
864|Cooperative material handling by human and robotic agents: Module development and system synthesis|In this paper we present a collaborative e ort to design and implement a cooperative material handling system by a small team of human and robotic agents in an unstructured indoor environment. Our approach
865|Analyzing Teams of Cooperating Mobile Robots|In [Don4], we described a manipulation task for cooperating mobile robots that can push large, heavy objects. There, we asked whether explicit local and global communication between the agents can be removed from a family of pushing protocols. In this paper, we answer in the affirmative. We do so by using the general methods of [Don4] analyzing information invariants. We discuss several measures for the information complexity of the task of pushing with cooperating mobile robots, and we present a methodology for creating new manipulation strategies out of existing ones. We develop and analyze synchronous and asynchronous manipulation protocols for a small team of cooperating mobile robots than can push large boxes. The protocols we describe have been implemented in several forms on the Cornell mobile robots in our laboratory. 1 Introduction  In this paper, we develop and analyze synchronous and asynchronous manipulation protocols for a small team of cooperating mobile robots than can p...
866|Multi-Robot Cooperation Through Incremental Plan-Merging|Abstract: This paper presents an approach we have recently developed for multi-robot cooperation. It is based on a paradigm where robots incrementally merge their plans into a set of already coordinated plans. This is done through exchange of information about their current state and their future actions. This leads to a generic framework which can be applied toavariety of tasks and applications. The paradigm, called Plan-Merging Paradigm is presented and illustrated through its application to planning, execution and control of a large eet of a autonomous mobile robots for load transport tasks in a structured environment. 1
867|Coordination Of Multiple Behaviors Acquired By A Vision-Based Reinforcement Learning|A method is proposed which accomplishes a whole task consisting of plural subtasks by coordinating multiple behaviors acquired by a vision-based reinforcement learning. First, individual behaviors which achieve the corresponding subtasks are independently acquired by Q-learning, a widely used reinforcement learning method. Each learned behavior can be represented by an action-value function in terms of state of the environment and robot action. Next, three kinds of coordinations of multiple behaviors are considered; simple summation of dierent action-value functions, switching action-value functions according to situations, and learning with previously obtained actionvalue functions as initial values of a new action-value function. A task of shooting a ball into the goal avoiding collisions with an enemy is examined. The task can be decomposed into a ball shooting subtask and a collision avoiding subtask. These subtasks should be accomplished simultaneously, but they are not independe...
868|Cooperation by Observation - The Framework and Basic Task Patterns|A novel framework for multiple robot cooperation called &#034;Cooperation by Observation&#034; is presented. It introduces many interesting issues such as viewpoint constraint and role interchange, as well as novel concepts like &#034;attentional structure&#034;. The framework has the potential to realize a high level of task coordination by decentralized autonomous robots allowing minimum explicit communication. Its source of power lies in an advanced capability given to each robot for recognizing other agent&#039;s actions by (primarily visual) observation. This provides rich information about the current task situation around each robot which facilitates highly-structured task coordination. The basic visuo-motor routines are described. Concrete examples and experiments using real mobile robots are also presented.   1 Introduction  In order to execute complex tasks with multiple mobile robots and achieve superlinear increase of task performance to the number of robots, we need a variety of flexible and highl...
869|Robot Herds: Group Behaviors for Systems with Significant Dynamics|Birds, fish, and many other animals are able to move gracefully and efficiently as a herd, flock, or school. We would like to reproduce this behavior for herds of artificial creatures with significant dynamics. This paper develops an algorithm for grouping behaviors and evaluates the performance of the algorithm on two types of systems: a full dynamic simulation of a legged robot that must balance as well as move with the herd and a point mass with minimal dynamics. Robust control algorithms for group behaviors of dynamic systems will allow us to generate realistic motion for animation using high-level controls, to develop synthetic actors for use in virtual environments, mobile robotics, and perhaps to improve our understanding of the behavior of biological systems. 1 Introduction  To run as a herd, animals must remain in close proximity while changing direction and velocity and while avoiding collisions with other members of the herd and obstacles in the environment. In this paper, w...
870|Stagnation Recovery Behaviours for Collective Robotics|Accomplishing useful tasks with a collection of decentralized mobile robots will require control methods that deal effectively with a number of unique problems that impede the system&#039;s progress. Reactive control architectures can easily cause the problems of stagnation and cyclic behaviour, both characterized by a lack of progress in achieving a task. In this paper we present one possible solution to stagnation recovery, motivated from the study of group transport in ants and demonstrate its use in a box-pushing task. By using stagnation recovery behaviours, which are triggered by a lack of progress in the task-achieving activity of the system, the collective system can monitor its own advancement in a decentralized manner. A set of such behaviours are progressively ordered using timeouts, with each set designed for a specific recovery strategy. The stagnation recovery behaviours have been tested in simulation with the results to be mapped onto a set of ten autonomous robots presently ...
871|Controlling Collective Tasks With An ALN|In this paper, we explore the idea of using an Adaptive Logic Network (ALN) [2] for behaviour arbitration. Our approach is to define the collective task, to be performed by multiple robots, as a group behaviour. The group behaviour is a set of behaviours, each of which specifies a single step in the collective task. An environmental cue can be used to control the transition between behaviours, thus allowing the progress of the collective task to self-govern its execution. We simplify the behaviour arbitration in the collective task by training an ALN implemented using simple combinational logic. We provide a description of our Collective Robotic Intelligence Project (CRIP) including our simulation results and our multi-robot system on which these results will be deployed. 1 Introduction  Can collective tasks be accomplished using group behaviours ? Interest in accomplishing tasks by using multiple robots has resulted in systems designed using cooperative behaviour [11, 8, 1, 7, 17]. Ou...
872|Vision-Based Behaviors for Multi-Robot Cooperation|This paper presents some advanced examples of reactive vision-based cooperative behaviors: (1) Chasing and posing against another robot among others, (2) Unblocking the path of another robot by removing an obstacle, (3) Passing an object from one to another. These behaviors are demonstrated using real mobile robots equipped with CCD cameras, in a complex environment, and with no central controller or explicit communication among the robots. The action observation is based on real time processing of optical flow analysis and stereo tracking (ZDF). An extended behavior-based architecture for &#034;cooperation by observation&#034; is presented. The core extension consists of a mobile space buffer and an image space buffer with manipulable markers which control the internal flow of information, thereby coordinating parallel behaviors and achieving purposive tasks in complex and dynamic environments.  1 Introduction  In decentralized multiple mobile robot systems, it is important to minimize the amou...
873|Moving Multiple Tethered Robots between Arbitrary Configurations|We consider the problem of motion planning for a number of small, disc-like robots in a common planar workspace. Each robot is tethered to a point on the boundary of the worksapce by a flexible cable of finite length. These cables may be pushed and bent by robots that come in contact with them but remain taut at all times. The robots are given a set of target points to which they must move. Upon arrival at these points, a new set of target points are given. Associated with each set of target points is a configuration of the cables that must be achieved when all robots are at these target points. The motion planning task addressed here is to produce relatively short paths for the robots from an initial (nontrivial) configuration of the cables to a configuration corresponding to the next set of target points. An O(n  4  log n) algorithm is presented for achieving this task for n robots. 1 Introduction  We consider the problem of motion planning for multiple tethered robots. These small, ...
874|Distributed sensing and probing with multiple search agents: toward system-level landmine detection solutions|agents: toward system-level landmine detection solutions
875|Unidraw: a framework for building domain-specific graphical editors|Unidraw is a framework for creating graphical editors in domains such as technical and artistic drawing, music composition, and circuit design. The Unidraw architecture simplifies the construction of these editors by providing programming abstractions that are common across domains. Unidraw defines four basic abstractions: comporzents encapsulate the appearance and behavior of objects, tools support direct manipulation of components, commands define operations on components, and external representations define the mapping between components and the file format generated by the editor. Unidraw also supports multiple views, graphical connectivity, and dataflow between components. This paper describes the Unidraw design, implementation issues, and three experimental domain-specific editors we have developed with Unidraw: a drawing editor, a user interface builder, and a schematic capture system. Our results indicate a substantial reduction in implementation time and effort compared with existing tools.
876|Scalable Software Libraries|  Many software libraries (e.g., the Booch C++ Components, libg++, NIHCL, COOL) provide components (classes) that implement data structures. Each component is written by hand and represents a unique combination of features (e.g. concurrency, data structure, memory allocation algorithms) that distinguishes it from other components. We argue that this way of building data structure component libraries is inherently unscalable. Libraries should not enumerate complex components with numerous features; rather, libraries should take a minimalist approach: they should provide only primitive building blocks and be accompanied by generators that can combine these blocks to yield complex custom data structures. In this paper, we describe a prototype data structure generator and the building blocks that populate its library. We also present preliminary experimental results which suggest that this approach does not compromise programmer productivity nor the run-time performance of generated data structures.  
878|The Node Distribution of the Random Waypoint Mobility Model for Wireless Ad Hoc Networks|The random waypoint model is a commonly used mobility model in the simulation of ad hoc networks. It is known that the spatial distribution of network nodes moving according to this model is, in general, nonuniform. However, a closed-form expression of this distribution and an in-depth investigation is still missing. This fact impairs the accuracy of the current simulation methodology of ad hoc networks and makes it impossible to relate simulation-based performance results to corresponding analytical results. To overcome these problems, we present a detailed analytical study of the spatial node distribution generated by random waypoint mobility. More specifically, we consider a generalization of the model in which the pause time of the mobile nodes is chosen arbitrarily in each waypoint and a fraction of nodes may remain static for the entire simulation time. We show that the structure of the resulting distribution is the weighted sum of three independent components: the static, pause, and mobility component. This division enables us to understand how the model?s parameters influence the distribution. We derive an exact equation of the asymptotically stationary distribution for movement on a line segment and an accurate approximation for a square area. The good quality of this approximation is validated through simulations using various settings of the mobility parameters. In summary, this article gives a fundamental understanding of the behavior of the random waypoint model.
879|The capacity of wireless networks| When n identical randomly located nodes, each capable of transmitting at bits per second and using a fixed range, form a wireless network, the throughput @ A obtainable by each node for a randomly chosen destination is 2 bits per second under a noninterference protocol. If the nodes are optimally placed in a disk of unit area, traffic patterns are optimally assigned, and each transmission’s range is optimally chosen, the bit–distance product that can be transported by the network per second is 2 @ A bit-meters per second. Thus even under optimal circumstances, the throughput is only 2 bits per second for each node for a destination nonvanishingly far away. Similar results also hold under an alternate physical model where a required signal-to-interference ratio is specified for successful receptions. Fundamentally, it is the need for every node all over the domain to share whatever portion of the channel it is utilizing with nodes in its local neighborhood that is the reason for the constriction in capacity. Splitting the channel into several subchannels does not change any of the results. Some implications may be worth considering by designers. Since the throughput furnished to each user diminishes to zero as the number of users is increased, perhaps networks connecting smaller numbers of users, or featuring connections mostly with nearby neighbors, may be more likely to be find acceptance. 
880|A Performance Comparison of Multi-Hop Wireless Ad Hoc Network Routing Protocols|An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Due to the limited transmission range of wireless network interfaces, multiple network &amp;quot;hops &amp;quot; may be needed for one node to exchange data with another across the network. In recent years, a variety of new routing protocols targeted specifically at this environment have been developed, but little performance information on each protocol and no realistic performance comparison between them is available. This paper presents the results of a detailed packet-level simulation comparing four multi-hop wireless ad hoc network routing protocols that cover a range of design choices: DSDV, TORA, DSR, and AODV. We have extended the ns-2 network simulator to accurately model the MAC and physical-layer behavior of the IEEE 802.11 wireless LAN standard, including a realistic wireless transmission channel model, and present the results of simulations of networks of 50 mobile nodes. 1
881|Mobility increases the capacity of ad-hoc wireless networks|The capacity of ad-hoc wireless networks is constrained by the mutual interference of concurrent transmissions between nodes. We study a model of an ad-hoc network where n nodes communicate in random source-destination pairs. These nodes are assumed to be mobile. We examine the per-session throughput for applications with loose delay constraints, such that the topology changes over the time-scale of packet delivery. Under this assumption, the per-user throughput can increase dramatically when nodes are mobile rather than fixed. This improvement can be achieved by exploiting node mobility as a type of multiuser diversity.  
882|A Survey of Mobility Models for Ad Hoc Network Research|In the performance evaluation of a protocol for an ad hoc network, the protocol should be tested under realistic conditions including, but not limited to, a sensible transmission range, limited buffer space for the storage of messages, representative data traffic models, and realistic movements of the mobile users (i.e., a mobility model). This paper is a survey of mobility models that are used in the simulations of ad hoc networks. We describe several mobility models that represent mobile nodes whose movements are independent of each other (i.e., entity mobility models) and several mobility models that represent mobile nodes whose movements are dependent on each other (i.e., group mobility models). The goal of this paper is to present a number of mobility models in order to offer researchers more informed choices when they are deciding upon a mobility model to use in their performance evaluations. Lastly, we present simulation results that illustrate the importance of choosing a mobility model in the simulation of an ad hoc network protocol. Specifically, we illustrate how the performance results of an ad hoc network protocol drastically change as a result of changing the mobility model simulated.
883|Geography-informed Energy Conservation for Ad Hoc Routing|We introduce a geographical adaptive fidelity (GAF) algorithm that reduces energy consumption in ad hoc wireless networks. GAF conserves energy by identifying nodes that are equivalent from a routing perspective and then turning off unnecessary nodes, keeping a constant level of routing fidelity. GAF moderates this policy using application- and system-level information; nodes that source or sink data remain on and intermediate nodes monitor and balance energy use. GAF is independent of the underlying ad hoc routing protocol; we simulate GAF over unmodified AODV and DSR. Analysis and simulation studies of GAF show that it can consume 40% to 60% less energy than an unmodified ad hoc routing protocol. Moreover, simulations of GAF suggest that network lifetime increases proportionally to node density; in one example, a four-fold increase in node density leads to network lifetime increase for 3 to 6 times (depending on the mobility pattern). More generally, GAF is an example of adaptive fidelity, a technique proposed for extending the lifetime of self-configuring systems by exploiting redundancy to conserve energy while maintaining application fidelity.   
885|GloMoSim: A Library for Parallel Simulation of Large-scale Wireless Networks|A number of library-based parallel and sequential network simulators have been designed. This paper describes a library, called GloMoSim (for Global Mobile system Simulator), for parallel simulation of wireless networks. GloMoSim has been designed to be extensible and composable: the communication protocol stack for wireless networks is divided into a set of layers, each with its own API. Models of protocols at one layer interact with those at a lower (or higher) layer only via these APIs. The modular implementation enables consistent comparison of multiple protocols at a given layer. The parallel implementation of GloMoSim can be executed using a variety of conservative synchronization protocols, which include the null message and conditional event algorithms. This paper describes the GloMoSim library, addresses a number of issues relevant to its parallelization, and presents a set of experimental results on the IBM 9076 SP, a distributed memory multicomputer. These experiments use mo...
886|Performance comparison of two on-demand routing protocols for ad hoc networks|Abstract — Ad hoc networks are characterized by multihop wireless connectivity, frequently changing network topology and the need for efficient dynamic routing protocols. We compare the performance of two prominent ondemand routing protocols for mobile ad hoc networks — Dynamic Source Routing (DSR) and Ad Hoc On-Demand Distance Vector Routing (AODV). A detailed simulation model with MAC and physical layer models is used to study interlayer interactions and their performance implications. We demonstrate that even though DSR and AODV share a similar on-demand behavior, the differences in the protocol mechanics can lead to significant performance differentials. The performance differentials are analyzed using varying network load, mobility and network size. Based on the observations, we make recommendations about how the performance of either protocol can be improved.
887|Critical Power for Asymptotic Connectivity in Wireless Networks|: In wireless data networks each transmitter&#039;s power needs to be high enough to reach the intended receivers, while generating minimum interference on other receivers sharing the same channel. In particular, if the nodes in the network are assumed to cooperate in routing each others &#039; packets, as is the case in ad hoc wireless networks, each node should transmit with just enough power to guarantee connectivity in the network. Towards this end, we derive the critical power a node in the network needs to transmit in order to ensure that the network is connected with probability one as the number of nodes in the network goes to infinity. It is shown that if n nodes are placed in a disc of unit area in !  2  and each node transmits at a power level so as to cover an area of ßr  2  = (log n + c(n))=n, then the resulting network is asymptotically connected with probability one if and only if c(n) ! +1. 1 Introduction  Wireless communication systems consist of nodes which share a common commu...
888|Analysis of TCP Performance over Mobile Ad Hoc Networks Part I: Problem Discussion and Analysis of Results|Mobile ad hoc networks have gained a lot of attention lately as a means of providing continuous network connectivity to mobile computing devices regardless of physical location. Recently, a large amount of research has focused on the routing protocols needed in such an environment. In this two-part report, we investigate the effects that link breakage due to mobility has on TCP performance. Through simulation, we show that TCP throughput drops significantly when nodes move because of TCP&#039;s inability to recognize the difference between link failure and congestion. We also analyze specific examples, such as a situation where throughput is zero for a particular connection. We introduce a new metric, expected throughput, for the comparison of throughput in multi-hop networks, and then use this metric to show how the use of explicit link failure notification (ELFN) techniques can significantly improve TCP performance. In this paper (Part I of the report), we present the problem and an analysis of our simulation results. In Part II of this report, we present the simulation and results in detail.
889|Random Waypoint Considered Harmful|This study examines the random waypoint model widely used in the simulation studies of mobile ad hoc networks. Our findings show that this model fails to provide a steady state in that the average nodal speed consistently decreases over time, and therefore should not be directly used for simulation. We show how unreliable results can be obtained by using this model. In particular, certain ad hoc routing metrics can drop by as much as 40% over the course of a 900-second simulation using the random waypoint model. We give both an intuitive and a formal explanation for this phenomenon. We also propose a simple fix of the problem and discuss a few alternatives. Our modified random waypoint model is able to reach a steady state and simulation results are presented.
890|A group mobility model for ad hoc wireless networks|In this paper, we present a survey of various mobility models in both cellular networks and multi-hop networks. We show that group motion occurs frequently in ad hoc networks, and introduce a novel group mobility model- Reference Point Group Mobility (RPGM)- to represent the relationship among mobile hosts. RPGM can be readily applied to many existing applications. Moreover, by proper choice of parameters, RPGM can be used to model several mobility models which were previously proposed. One of the main themes of this paper is to investigate the impact of the mobility model on the performance of a specific network protocol or application. To this end, we have applied our RPGM model to two different network protocol scenarios, clustering and routing, and have evaluated network performance under different mobility patterns and for different protocol implementations. As expected, the results indicate that different mobility patterns affect the various protocols in different ways. In particular, the ranking of routing algorithms is influenced by the choice of mobility pattern. 1
891|Multicast Operation of the Ad-hoc On-Demand Distance Vector Routing Protocol|An ad-hoc network is the cooperative engagement of a  collection of (typically wireless) mobile nodes without the required intervention of any centralized access point or existing infrastructure. To provide optimal communication ability, a routing protocol for such a dynamic self-starting network must be capable of unicast, broadcast, and multicast. In this paper we extend Ad-hoc On-Demand Distance Vector Routing (AODV), an algorithm for the operation of such ad-hoc networks, to offer novel multicast capabilities which follow naturally from the way AODV establishes unicast routes. AODV builds  multicast trees as needed (i.e., on-demand) to connect multicast group members. Control of the multicast tree is distributed so that there is no single point of failure. AODV provides loop-free routes for both unicast and multicast, even while repairing broken links. We include an evaluation methodology and simulation results to validate the correct and efficient operation of the AODV algorithm.   
892|On the Minimum Node Degree and Connectivity of a Wireless Multihop Network|This paper investigates two fundamental characteristics of a wireless multihop network: its minimum node degree and its k–connectivity. Both topology attributes depend on the spa-tial distribution of the nodes and their transmission range. Using typical modeling assumptions — a random uniform distribution of the nodes and a simple link model — we de-rive an analytical expression that enables the determination of the required range r0 that creates, for a given node den-sity ?, an almost surely k–connected network. Equivalently, if the maximum r0 of the nodes is given, we can find out how many nodes are needed to cover a certain area with a k–connected network. We also investigate these questions by various simulations and thereby verify our analytical ex-pressions. Finally, the impact of mobility is discussed. The results of this paper are of practical value for re-searchers in this area, e.g., if they set the parameters in a network–level simulation of a mobile ad hoc network or if they design a wireless sensor network. Categories and Subject Descriptors C.2 [Computer-communication networks]: Network architecture and design—wireless communication, network communications, network topology; G.2.2 [Discrete math-ematics]: Graph theory; F.2.2 [Probability and statis-tics]: Stochastic processes
893|Stochastic properties of the random waypoint mobility model |Abstract | The random waypoint model is a commonly used mobility model for simulations of wireless commu-nication networks. In this paper, we present analytical derivations of some fundamental stochastic properties of this model with respect to: (a) the length and duration of a movement epoch, (b) the chosen direction angle at the beginning of a movement epoch, and (c) the cell change rate of the random waypoint mobility model when used within the context of cellular networks. Our results and methods can be used to compare the random waypoint model with other mobility models. The results on the movement epoch duration as well as on the cell change rate enable us to make a statement about the \degree of mobility &#034; of a certain sim-ulation scenario. The direction distribution explains in an analytical manner the eect that nodes tend to move back to the middle of the system area.
894|User mobility modeling and characterization of mobility patterns|Abstract—A mathematical formulation is developed for systematic tracking of the random movement of a mobile station in a cellular environment. It incorporates mobility parameters under the most generalized conditions, so that the model can be tailored to be applicable in most cellular environments. This mobility model is used to characterize different mobility-related traffic parameters in cellular systems. These include the distribution of the cell residence time of both new and handover calls, channel holding time, and the average number of handovers. It is shown that the cell resistance time can be described by the generalized gamma distribution. It is also shown that the negative exponential distribution is a good approximation for describing the channel holding time. Index Terms—Mobile communication. I.
895|An analysis of the optimum node density for ad hoc mobile networks|An ad hoc mobile network is a collection of nodes, each of which communicates over wireless channels and is capable of movement. Wireless nodes have the unique capability of transmission at different power levels. As the transmission power is varied, a tradeoff exists between the number of hops from source to destination and the overall bandwidth available to individual nodes. Because both battery life and channel bandwidth are limited resources in mobile networks, it is important to ascertain the effects different transmission powers have on the overall performance of the network. This paper explores the nature of this transmission power tradeoff in mobile networks to determine the optimum node density for delivering the maximum number of data packets. It is shown that there does not exist a global optimum density, but rather that, to achieve this maximum, the node density should increase as the rate of node movement increases.
896|Smooth is Better than Sharp: A Random Mobility Model for Simulation of Wireless Networks|This paper presents an enhanced random mobility model for simulation-based studies of wireless networks. Our approach makes the movement trace of individual mobile stations more realistic than common approaches for random movement. After giving a survey of mobility models found in the literature, we give a detailed mathematical formulation of our model and outline its advantages. The movement concept is based on random processes for speed and direction control in which the new values are correlated to previous ones. Upon a speed change event, a new target speed is chosen, and an acceleration is set to achieve this target speed. The principles for a direction change are similar. Moreover, we propose two extensions for modeling typical movement patterns of vehicles. Finally, we consider strategies for the nodes&#039; border behavior (i.e., what happens when nodes move out of the simulation area) and point out a pitfall that occurs when using a bounded simulation area.
897|Mobility modeling in wireless networks: Categorization, smooth movement, and border effects |The movement pattern of mobile users plays an important role in performance analysis of wireless computer and communication networks. In this paper, we first give an overview and classification of mobility models used for simulation-based studies. Then, we present an enhanced random mobility model, which makes the movement trace of mobile stations more realistic than common approaches for random mobility. Our movement concept is based on random processes for speed and direction control in which the new values are correlated to previous ones. Upon a speed change event, a new target speed is chosen, and an acceleration is set to achieve this target speed. The principles for direction changes are similar. Finally, we discuss strategies for the stations &#039; border behavior (i.e., what happens when nodes move out of the simulation area) and show the effects of certain border behaviors and mobility models on the spatial user distribution. I.
898|The Critical Transmitting Range for Connectivity in Sparse Wireless Ad Hoc Networks|In this paper, we analyze the critical transmitting range for connectivity in  wireless ad hoc networks. More specifically, we consider the following problem:  assume n nodes, each capable of communicating with nodes within a radius of r,  are randomly and uniformly distributed in a d-dimensional region with a side of  length l; how large must the transmitting range r be to ensure that the resulting  network is connected with high probability? First, we consider this problem for  stationary networks, and we provide tight upper and lower bounds on the critical  transmitting range for one-dimensional networks, and non-tight bounds for two and  three-dimensional networks. Due to the presence of the geometric parameter l in  the model, our results can be applied to dense as well as sparse ad hoc networks,  contrary to existing theoretical results that apply only to dense networks. We  also investigate several related questions through extensive simulations. First, we  evaluate the relationship between the critical transmitting range and the minimum  transmitting range that ensures formation of a connected component containing a  large fraction (e.g. 90%) of the nodes. Then, we consider the mobile version of the   
899|A Comparison of TCP Performance over Three Routing Protocols for Mobile Ad Hoc Networks|We examine the performance of the TCP protocol for bulkdata transfers in mobile ad hoc networks (MANETs). We vary the number of TCP connections and compare the performances of three recently proposed on-demand (AODV and DSR) and adaptive proactive (ADV) routing algorithms. It has been shown in the literature that the congestion control mechanism of TCP reacts adversely to packet losses due to temporarily broken routes in wireless networks. So, we propose a simple heuristic, called fixed RTO, to distinguish between route loss and network congestion and thereby improve the performance of the routing algorithms. Using the ns-2 simulator, we evaluate the performances of the three routing algorithms with the standard TCP Reno protocol and Reno with fixed RTO. Our results indicate that the proactive ADV algorithm performs well under a variety of conditions and that the fixed RTO technique improves the performances of the two on-demand algorithms significantly.  1. 
900|Role-Based Multicast in Highly Mobile but Sparsely Connected Ad Hoc Networks|We present an approach to multicasting messages among highly mobile hosts in ad hoc networks. We suggest a new definition of a role-based multicast that suits the special needs of inter-vehicle communication: Rather than by explicit identification, a multicast group is defined implicitly by location, speed, driving direction and time. As an example, we study a road accident that is reported to nearby vehicles. We focus on sparse deployment of the system which is likely to occur soon after the system is introduced to the market. In this state, the resulting ad hoc network tends to be disconnected. We tailor the proposed algorithm to overcome this problem of network fragmentation. Simulations show us the quality of the proposed protocol by measuring how many vehicles inside a multicast area are informed in time under various conditions.  Keywords---Inter-vehicle communication, mobile ad hoc network, multicast, Global Positioning System  I. 
901|Teletraffic Modeling for Personal Communications Services|This paper presents a realistic teletraffic modeling framework for Personal Communications Services. The framework captures complex human behaviors and has been validated through analysis of actual call and mobility data. Using the proposed framework, a large-scale simulation was performed on a model of the San Francisco Bay Area. Simulation results showing the performance of IS-41 are presented.
902|An Analysis of the Node Spatial Distribution of the Random Waypoint Mobility Model for Ad Hoc Networks|by nodes moving according to the random waypoint model, which is widely used in the simulation of mobile ad hoc networks. We extend an existing analysis for the case in which nodes are continuously moving (i.e., the pause time is 0) to the more general case in which nodes have arbitrary pause times between movements. We also generalize the mobility model, allowing the nodes to remain stationary for the entire simulation time with a given probability. Our analysis shows that the structure of the resulting asymptotic spatial density is composed by three distinct components: the initial, the pause and the mobility component. The relative values of these components depend on the mobility parameters. We derive an explicit formula of the one-dimensional node spatial density, and an approximated formula for the two-dimensional case. The quality of this approximation is veri  ed through experimentation, which shows that the accuracy heavily depends on the choice of the mobility parameters. 1. 
903|A Statistical Analysis of the Long-Run Node Spatial Distribution in Mobile Ad Hoc Networks|In this paper, we analyze the node spatial distribution of a mobile wireless ad hoc networks. Characterizing this distribution is of fundamental importance in the analysis of many relevant properties of mobile ad hoc networks, such as connectivity, average route length, and network capacity. In particular, we have investigated under what conditions the node spatial distribution resulting after a large number of mobility steps resembles the uniform distribution. This is motivated by the fact that the existing theoretical results concerning mobile ad hoc networks are based on this assumption. In order to test this hypothesis, we performed extensive simulations using two well-known mobility models: the random waypoint model, which resembles intentional movement, and a Brownian-like model, which resembles non-intentional movement. Our analysis has shown that in the Brownian-like motion the uniformity assumption does hold, and that the intensity of the concentration of nodes in the center of the deployment region that occurs in the random waypoint model heavily depends on the choice of some mobility parameters. For extreme values of these parameters, the uniformity assumption is impaired.
904|Portable movement modeling for PCS networks|Abstract—In this paper, we propose a new model for the portable movement in personal communications services (PCSs) networks. Based on this model with general interservice time and registration area residence time distributions, analytic expression for the probability that a portable moves across registration areas (RAs) is obtained. Busy-line effect on this quantity is also studied and analytic expression is presented. The result given in this paper is very useful for the cost analysis for location updating and paging. Index Terms—Call holding time, cell residence times, location modeling, mobility, personal communications services (PCS). I.
905|Neighborhood Aware Source Routing|A novel approach to source routing in ad hoc networks is introduced that takes advantage of maintaining information regarding the two-hop neighborhood of a node. The neighborhood aware source routing (NSR) protocol is presented based on this approach, and its performance is compared by simulation with the performance of the Dynamic Source Routing (DSR) protocol. The simulation analysis indicates that NSR requires much fewer control packets while delivering at least as many data packets as DSR. Keywords On-demand routing, source routing, ad hoc networks, wireless mobile networks, link-state information
906|Probability Criterion Based Location Tracking Approach for Mobility Management of Personal Communications Systems|We consider a probability criterion based location tracking scheme for personal communications systems. This scheme seeks to minimize the average signaling cost due to both paging and registration for individual mobile users. A timer-based location update and a single-step dynamic paging procedure are used. Each user-network interaction resets the timer and modifies location record. The user registers when the timer expires. Users are paged over personally constructed  paging areas. The size of these areas is an increasing function of the time since last user-network contact. The shape of the paging area depends on the particular probability distribution of user location. The problem formulation is applicable to arbitrary motion models assuming the associated user location distribution is available or can be estimated. As an illustrative example, we use a Brownian motion process for user motion and assume Poisson call arrivals. We then investigate the influence of user mobility and cal...
907|Implementing remote procedure calls|Remote procedure calls (RPC) appear to be a useful paradig m for providing communication across a network between programs written in a high-level language. This paper describes a package providing a remote procedure call facility, the options that face the designer of such a package, and the decisions ~we made. We describe the overall structure of our RPC mechanism, our facilities for binding RPC clients, the transport level communication protocol, and some performance measurements. We include descriptioro ~ of some optimizations used to achieve high performance and to minimize the load on server machines that have many clients.
908|Replication Methods for Abstract Data Types|Replication can enhance the availability of data in a distributed system. This thesis introduces...
909|Multiagent Systems: A Survey from a Machine Learning Perspective|Distributed Artificial Intelligence (DAI) has existed as a subfield of AI for less than two decades. DAI is
910|Communication in reactive multiagent robotic systems|Abstract. Multiple cooperating robots are able to complete many tasks more quickly and reliably than one robot alone. Communication between the robots can multiply their capabilities and e ectiveness, but to what extent? In this research, the importance of communication in robotic societies is investigated through experiments on both simulated and real robots. Performance was measured for three di erent types of communication for three di erent tasks. The levels of communication are progressively more complex and potentially more expensive to implement. For some tasks, communication can signi cantly improve performance, but for others inter-agent communication is apparently unnecessary. In cases where communication helps, the lowest level of communication is almost as e ective as the more complex type. The bulk of these results are derived from thousands of simulations run with randomly generated initial conditions. The simulation results help determine appropriate parameters for the reactive control system which was ported for tests on Denning mobile robots.
911|Communicative Actions for Artificial Agents|This paper considers the semantics of the agent communication language KQML. By using this language for communication, agents will be able to request and provide services. Indeed, numerous projects have shown how the language can profitably support interoperation among distributed agents. However, before becoming a widely-accepted standard, it would be worthwhile to examine the language in detail, especially the semantical issues it raises. This paper identifies numerous difficulties with the language, and an attempt is made to point to their resolution. The paper illustrates the kind of semantics we believe to be necessary to characterize agent communication languages, identifies an important adequacy condition &#034;compositionality&#034; and shows how to compose a question from a request and an inform. Finally, the paper discusses possible impacts to be felt on various KQML decisions from the semantical issues raised here.
912|Trends in Cooperative Distributed Problem Solving|Introduction Cooperative Distributed Problem-Solving (CDPS) studies how a loosely-coupled network of problem solvers can work together to solve problems that are beyond their individual capabilities. Each problem-solving node in the network is capable of sophisticated problem solving and can work independently, but the problems faced by the nodes cannot be completed without cooperation. Cooperation is necessary because no single node has sufficient expertise, resources, and information to solve a problem, and different nodes might have expertise for solving different parts of the problem. For example, if the problem is to design a house, one node might have expertise on the strength of structural materials, another on the space requirements for different types of rooms, another on plumbing, another on electrical wiring, and so on. Different nodes might have different resources: some might be very fast at computation, others might have connections that speed communication, whil
913|Interaction and Intelligent Behavior|This thesis addresses situated, embodied agents interacting in complex domains. It focuses on two problems: 1) synthesis and analysis of intelligent group behavior, and 2) learning in complex group environments. Basic behaviors, control laws that cluster constraints to achieve particular goals and have the appropriate compositional properties, are proposed as effective primitives for control and learning. The thesis describes the process of selecting such basic behaviors, formally specifying them, algorithmically implementing them, and empirically evaluating them. All of the proposed ideas are validated with a group of up to 20 mobile robots using a basic behavior set consisting of: safe--wandering, following, aggregation, dispersion, and homing. The set of basic behaviors acts as a substrate for achieving more complex high--level goals and tasks. Two behavior combination operators are introduced, and verified by combining subsets of the above basic behavior set to implement collective flocking, foraging, and docking. A methodology is introduced for automatically constructing higher--level behaviors
914|Soccer Server: a tool for research on multi-agent systems|This paper describes Soccer Server, a simulator of the game of soccer designed as a test-bench for evaluating multi-agent systems and cooperative algorithms. In real life, successful soccer teams require many qualities, such as basic ball control skills, the ability to carry out plans, and teamwork. We believe that simulating such behaviors is a significant challenge for Computer Science, Artificial Intelligence and Robotics technologies. It is to promote the development of such technologies, and to help define a new standard problem for research, that we have developed Soccer Server. We demonstrate the potential of Soccer Server by reporting an experiment that uses the system to compare the performance of a neural network architecture and a decision tree algorithm at learning the selection of soccer play-plans. Other researchers using Soccer Server to investigate the nature of cooperative behavior in a multi-agent environment will have the chance to assess their progress at RoboCup-97...
915|purposive behavior acquisition on a real robot by vision-based reinforcement learning|Abstract. This paper presents a method of vision-based reinforcement learning by which a robot learns to shoot a ball into a goal. We discuss several issues in applying the reinforcement learning method to a real robot with vision sensor by which the robot can obtain information about the changes in an environment. First, we construct a state space in terms of size, position, and orientation of a ball and a goal in an image, and an action space is designed in terms of the action commands to be sent to the left and right motors of a mobile robot. This causes a “state-action deviation ” problem in constructing the state and action spaces that reflect the outputs from physical sensors and actuators, respectively. To deal with this issue, an action set is constructed in a way that one action consists of a series of the same action primitive which is successively executed until the current state changes. Next, to speed up the learning time, a mechanism of Learning from Easy Missions (or LEM) is implemented. LEM reduces the learning time from exponential to almost linear order in the size of the state space. The results of computer simulations and real robot experiments are given.
916|Heterogeneous Multi-Robot Cooperation|the
917|Methods for Competitive Co-evolution: Finding Opponents Worth Beating|Co-evolution refers to the simultaneous evolution of two or more genetically distinct populations with coupled fitness landscapes. In this paper we consider &#034;competitive co-evolution,&#034; in which the fitness of an individual in a &#034;host&#034; population is based on direct competition with individual(s) from a &#034;parasite&#034; population. Competitive coevolution is applied to three game-learning problems: Tic-Tac-Toe (TTT), Nim and a small version of Go. Two new techniques in competitive co-evolution are explored. &#034;Competitive fitness sharing&#034; changes the way fitness is measured, and &#034;shared sampling&#034; alters the way parasites are chosen for testing hosts. Experiments using TTT and Nim show a substantial improvement in performance when these methods are used. Preliminary results using co-evolution for the discovery of cellular automata rules for playing Go are presented.   1 Introduction  Co-evolution refers to the simultaneous evolution of two or more genetically distinct populations with coupled fit...
918|Environment Centered Analysis and Design of Coordination Mechanisms| Coordination, as the act of managing interdependencies between activities, is one of the central research issues in Distributed Artificial Intelligence. Many researchers have shown that there is no single best organization or coordination mechanism for all environments. Problems in coordinating the activities of distributed intelligent agents appear in many domains: the control of distributed sensor networks; multi-agent scheduling of people and/or machines; distributed diagnosis of errors in local-area or telephone networks; concurrent engineering; `software agents&#039; for information gathering.  The design of coordination mechanisms for group...
919|CONRO: Toward deployable robots with inter-robot metamorphic capabilities|Abstract. Metamorphic robots are modular robots that can reconfigure their shape. Such capability is desirable in tasks such as earthquake search and rescue and battlefield surveillance and scouting, where robots must go through unexpected situations and obstacles and perform tasks that are difficult for fixed-shape robots. The capabilities of the robots are determined by the design specification of their modules. In this paper, we present the design specification of a CONRO module, a small, self-sufficient and relatively homogeneous module that can be connected to other modules to form complex robots. These robots have not only the capability of changing their shape (intra-robot metamorphing) but also can split into smaller robots or merge with other robots to create a single larger robot (interrobot metamorphing), i.e., CONRO robots can alter their shape and their size. Thus, heterogeneous robot teams can be built with homogeneous components. Furthermore, the CONRO robots can separate the reconfiguration stage from the locomotion stage, allowing the selection of configuration-dependent gaits. The locomotion and automatic inter-module docking capabilities of such robots were tested using tethered prototypes that can be reconfigured manually. We conclude the paper discussing the future work needed to fully realize the construction of these robots. Keywords: module, reconfigurable, autonomous, self-sufficient
920|A taxonomy for multi-agent robotics|A key difficulty in the design of multi-agent robotic systems is the size and complexity of the space of possible designs. In order to make principled design decisions, an understanding of the many possible system configurations is essential. To this end, we present a taxonomy that classifies multiagent systems according to communication, computational and other capabilities. We survey existing efforts involving multi-agent systems according to their positions in the taxonomy. We also present additional results concerning multi-agent systems, with the dual purposes of illustrating the usefulness of the taxonomy in simplifying discourse about robot collective properties, and also demonstrating that a collective can be demonstrably more powerful than a single unit of the collective.
921|Co-Evolving Soccer Softbot Team Coordination with Genetic Programming|. In this paper we explain how we applied genetic programming to behavior-based team coordination in the RoboCup Soccer Server domain. Genetic programming is a promising new method for automatically generating functions and algorithms through natural selection. In contrast to other learning methods, genetic programming&#039;s automatic programming makes it a natural approach for developing algorithmic robot behaviors. The RoboCup Soccer Server was a very challenging domain for genetic programming, but we were pleased with the results. At the end, genetic programming had produced teams of soccer softbots which had learned to cooperate to play a good game of simulator soccer. 1 Introduction  The RoboCup competition pits robots against each other in a simulated soccer tournament [Kitano et al, 1995]. The aim of the RoboCup competition is to foster an interdisciplinary approach to robotics and agent-based Artificial Intelligence by presenting a domain that requires large-scale cooperation and c...
922|A Coevolutionary Approach to Learning Sequential Decision Rules|We present a coevolutionary approach to learning sequential decision rules which appears to have a number of advantages over non-coevolutionary approaches. The coevolutionary approach encourages the formation of stable niches representing simpler subbehaviors. The evolutionary direction of each subbehavior can be controlled independently, providing an alternative to evolving complex behavior using intermediate training steps. Results are presented showing a significant learning rate speedup over a noncoevolutionary approach in a simulated robot domain. In addition, the results suggest the coevolutionary approach may lead to emergent problem decompositions. 1 Introduction  For both natural and artificial organisms the ability to learn complex behavior is desirable, but difficult to achieve. Techniques such as &#034;shaping&#034; are frequently used to construct complex behaviors in stages by breaking them down into simpler behaviors which can be learned more easily, and then using these simpler b...
923|Evolving Behavioral Strategies in Predators and Prey|The predator/prey domain is utilized to conduct research in Distributed Artificial Intelligence. Genetic Programming is used to evolve behavioral strategies for the predator agents. To further the utility of the predator strategies, the prey population is allowed to evolve at the same time. The expected competitive learning cycle did not surface. This failing is investigated, and a simple prey algorithm surfaces, which is consistently able to evade capture from the predator algorithms.
924|Learning to Behave Socially|Our previous work introduced a methodology for synthesizing and analyzing basic behaviors  which served as a substrate for generating a large repertoire of higher--level group interactions (Matari&#039;c 1992, Matari&#039;c 1993). In this paper we describe how, given the substrate, agents can learn to behave socially, i.e. to maximize average individual by maximizing collective benefit. While this is a well--defined problem for rational agents, it is difficult to learn in situated domains. We describe three sources of reinforcement and show their necessity for learning non--greedy social rules. The learning strategy is demonstrated on a group of physical mobile robots learning to yield and share information in a foraging task. 1 Introduction  Our previous work focused on analyzing and synthesizing complex group behaviors from simple social interactions between individuals (Matari&#039;c 1992, Matari&#039;c 1993). We introduced a methodology which involved designing a collection of basic behaviors which se...
925|Hierarchic social entropy: An information theoretic measure of robot group diversity|Abstract. As research expands in multiagent intelligent systems, investigators need new tools for evaluating the artificial societies they study. It is impossible, for example, to correlate heterogeneity with performance in multiagent robotics without a quantitative metric of diversity. Currently diversity is evaluated on a bipolar scale with systems classified as either heterogeneous or homogeneous, depending on whether any of the agents differ. Unfortunately, this labeling doesn’t tell us much about the extent of diversity in heterogeneous teams. How can it be determined if one system is more or less diverse than another? Heterogeneity must be evaluated on a continuous scale to enable substantive comparisons between systems. To enable these types of comparisons, we introduce: (1) a continuous measure of robot behavioral difference, and (2) hierarchic social entropy, an application of Shannon’s information entropy metric to robotic groups that provides a continuous, quantitative measure of robot team diversity. The metric captures important components of the meaning of diversity, including the number and size of behavioral groups in a society and the extent to which agents differ. The utility of the metrics is demonstrated in the experimental evaluation of multirobot soccer and multirobot foraging teams.
926|Evolving team Darwin United|Abstract. The RoboCup simulator competition is one of the most challenging international proving grounds for contemporary AI research. Exactly because of the high level of complexity and a lack of reliable strategic guidelines, the pervasive attitude has been that the problem can most successfully be attacked by human expertise, possibly assisted by some level of machine learning. This led, in RoboCup’97, to a field of simulator teams all of whose level and style of play were heavily influenced by the human designers of those teams. It is the thesis of our work that machine learning, if given the opportunity to design (learn) “everything ” about how the simulator team operates, can develop a competitive simulator team that solves the problem utilizing highly successful, if largely nonhuman, styles of play. To this end, Darwin United is a team of eleven players that have been evolved as a team of coordinated agents in the RoboCup simulator. Each agent is given a subset of the lowest level perceptual inputs and must learn to execute series of the most basic actions (turn, kick, dash) in order to participate as a member of the team. This paper presents our motivation, our approach, and the specific construction of our team that created itself from scratch. 1
927|On the Simultaneous Interpretation of Real World Image Sequences and their Natural Language Description: The System SOCCER|The aim of previous attempts at connecting vision systems and natural language systems has been to provide a retrospective description of the analysed image sequence. The step from such an a posteriori approach towards simultaneous natural language description reveals a problem which has not yet been dealt with in generation systems. Automatic generation of simultaneous descriptions calls for the application of an incremental event recognition strategy and for the adequate coordination of event recognition and language production. In order to enable free interaction between these processes, it is useful to implement them in parallel. In this paper  1  the system Soccer will be presented, which is based upon such a conception. Short sections of soccer games have been chosen as the domain of discourse. In analogy to radio reports, the system generates a description of the game which it is watching and which the listener cannot see.  This paper appeared in: Proc. of the 8th ECAI, pp. 449-...
928|Grounded Symbolic Communication between Heterogeneous Cooperating Robots|. In this paper, we describe the implementation of a heterogeneous cooperative multi-robot system  that was designed with a goal of engineering a grounded symbolic representation in a bottom-up fashion. The  system comprises two autonomous mobile robots that perform cooperative cleaning. Experiments demonstrate  successful purposive navigation, map building and the symbolic communication of locations in a behavior-based  system. We also examine the perceived shortcomings of the system in detail and attempt to understand them in  terms of contemporary knowledge of human representation and symbolic communication. From this  understanding, we propose the Adaptive Symbol Grounding Hypothesis as a conception for how symbolic  systems can be envisioned.  Keywords: cooperative robotics, heterogeneous systems, symbolic communication, symbol grounding, learning,  representation, behavior-based, mobile robots  +  This research was funded primarily by the The University of Wollongong and The Aust...
929|Emergent Coordination through the Use of Cooperative State-Changing Rules|Various researchers in Distributed Artificial Intelligence (DAI) have suggested that it would be worthwhile to isolate &#034;aspects of cooperative behavior,&#034; general rules that would cause agents to act in ways conducive to cooperation. The hypothesis is that when agents act in certain ways (e.g., share information, act in predictable ways, defer globally constraining choices), it will be easier for them to carry out effective joint action. Another kind of cooperative behavior, less explored in the literature, is when agents independently alter the environment to make it easier for everyone to function effectively. Cooperative behavior of this kind might be to put away a hammer that one finds lying on the floor, knowing that another agent will be able to find it more easily later on. In this paper we are concerned with cooperation of this latter type, state-changing behavior that improves the environment for everyone. We examine the effect that a specific &#034;cooperativity rule&#034; has on agents...
930|On Seeing Robots|Good Old Fashioned Artificial Intelligence and Robotics (GOFAIR) relies on a set  of restrictive Omniscient Fortune Teller Assumptions about the agent, the world and their  relationship. The emerging Situated Agent paradigm is challenging GOFAIR by grounding  the agent in space and time, relaxing some of those assumptions, proposing new architectures  and integrating perception, reasoning and action in behavioral modules. GOFAIR is  typically forced to adopt a hybrid architecture for integrating signal-based and symbol-based  approaches because of the inherent mismatch between the corresponding on-line and off-line  computational models. It is argued that Situated Agents should be designed using a unitary  on-line computational model. The Constraint Net model of Zhang and Mackworth satisfies  that requirement. Two systems for situated perception built in our laboratory are described  to illustrate the new approach: one for visual monitoring of a robot&#039;s arm, the other for  real-time visual control of multiple robots competing and cooperating in a dynamic world.
931|Deciding When to Commit To Action During Observation-based Coordination|We have developed a multiagent scheme which utilizes plan recognition as its primary means of acquiring the information necessary to coordinate the activities of agents. Preliminary research has demonstrated that the plan recognition system developed makes coordination of multiple agents possible. An important issue that arises when observation is the primary means of information acquisition is the introduction of uncertainty into the coordination process. We have explored the issue of early versus late commitment to the uncertain information thus gained and the resulting tradeoff between time and effort as the commitment level is changed. Our results show that while in some situations it is worthwhile delaying commitment until uncertainty is reduced, in other situations it is important to act even when uncertainty is high. The long-- term goal of the research is to develop the notion of  coordination through observation, where agents utilize plan recognition to acquire coordination in...
932|Task Environment Centered Simulation|viewpoints. It is a tool for building and testing computational theories of coordination. TÆMS is compatible with both formal computational agent-centered approaches and experimental approaches. The framework allows us to both mathematically analyze (when possible) and quantitatively simulate the behavior of multi-agent systems with respect to interesting characteristics of the computational task environments of which they are part. We believe that it provides the correct level of abstraction for meaningfully evaluating centralized, parallel, and distributed control algorithms, negotiation strategies, and organizational designs. This chapter will briefly describe the TÆMS modeling framework for representing abstract task environments, concentrating particularly on its support for simulation. I will describe how to model each of several different multi-agent problem-solving environments, such as    This work was supported by DARPA contract N0
933|Creatures: Entertainment Software Agents with Artificial Life|We present a technical description of Creatures, a commercial home-entertainment software package. Creatures provides a simulatedenvironment in which exist a number of synthetic agents that a user can interact with in real-time. The agents (known as &#034;creatures&#034;) are intended as sophisticated &#034;virtual pets&#034;. The internal architecture of the creatures is strongly inspired by animal biology. Each creature has a neural network responsible for sensory-motorcoordinationand behavior selection, and an &#034;artificial biochemistry&#034; that models a simple energy metabolism along with a &#034;hormonal&#034; system that interacts with the neural network to model diffuse modulation of neuronal activity and staged ontogenetic development. A biologically inspired learning mechanism allows the neural network to adapt during the lifetime of a creature. Learning includes the ability to acquire a simple verb--object language.
934|Reactive Visual Control of Multiple Non-Holonomic Robotic Agents|We have developed a multiagent robotic system including perception, cognition, and action components to function in a dynamic environment. The system involves the integration and coordination of a variety of diverse functional modules. At the sensing level, our complete multiagent robotic system incorporates detection and recognition algorithms to handle the motion of multiple mobile robots in a noisy environment. At the strategic and decision-making level, deliberative and reactive components take in the processed sensory inputs and select the appropriate actions to reach objectives under the dynamic and changing environmental conditions. At the actuator level, phys- ical robotic effectors execute the motion commands generated by the cognition level. In this paper, we focus on presenting our approach for reactive visual control of multiple mobile robots. We present a tracking and prediction algorithm which handles visually homogeneous agents. We describe our non-holonomic control for single robot navigation, and show how it applies to dynamic path generation to avoid multiple moving obstacles. We illustrate our algorithms with examples from our real implementation. Using the approaches introduced, our robotic team won the RoboCup-97 small-size robot competition at IJCAI-97 in Nagoya, Japan.
935|Learning Organizational Roles in a Heterogeneous Multi-agent System|This paper presents studies in learning a form of organizational knowledge called organizational roles in a multi-agent agent system. It attempts to demonstrate the viability and utility of self-organization in an agent-based system involving complex interactions within the agent set. We present a multi-agent parametric design system called L-TEAM where a set of heterogeneous agents learn their organizational roles in negotiated search for mutually acceptable designs. We tested the system on a steam condenser design domain and empirically demonstrated its usefulness. LTEAM produced better results than its non-learning predecessor, TEAM, which required elaborate knowledge engineering to hand-code organizational roles for its agent set. In addition, we discuss experiments with L-TEAM that highlight the importance of certain learning issues in multi-agent systems.  
936|Character design for soccer commentary|Abstract. In this paper we present early work on an animated talking head commentary system called Byrne 1. The goal of this project is to develop a system which can take the output from the RoboCup soccer simulator, and generate appropriate affective speech and facial expressions, based on the character’s personality, emotional state, and the state of play. Here we describe a system which takes pre-analysed simulator output as input, and which generates text marked-up for use by a speech generator and a face animation system. We make heavy use of inter-system standards, so that future versions of Byrne will be able to take advantage of advances in the technologies that it incorporates. 1
937|Lifelong Adaptation in Heterogeneous Multi-Robot Teams: Response to Continual Variation in Individual Robot Performance|.  Generating teams of robots that are able to perform their tasks over long periods of time requires the robots to be responsive to continual changes in robot team member capabilities and to changes in the state of the environment and mission. In this article, we describe the L-ALLIANCE architecture, which enables teams of heterogeneous robots to dynamically adapt their actions over time. This architecture, which is an extension of our earlier work on ALLIANCE, is a distributed, behavior-based architecture aimed for use in applications consisting of a collection of independent tasks. The key issue addressed in L-ALLIANCE is the determination of which tasks robots should select to perform during their mission, even when multiple robots with heterogeneous, continually changing capabilities are present on the team. In this approach, robots monitor the performance of their teammates performing common tasks, and evaluate their performance based upon the time of task completion. Robots then use this information throughout the lifetime of their mission to automatically update their control parameters. After describing the L-ALLIANCE architecture, we discuss the results of implementing this approach on a physical team of heterogeneous robots performing proof-of-concept box pushing experiments. The results illustrate the ability of L-ALLIANCE to enable lifelong adaptation of heterogeneous robot teams to continuing changes in the robot team member capabilities and in the environment.  1. 
939|Generating multimedia presentations for RoboCup soccer games|Abstract. The automated generation of multimedia reports for timevarying scenes on the basis of visual data constitutes a challenging research goalwith a high potential for many interesting applications. In this paper, we report on our work towards an automatic commentator system for RoboCup, the Robot World-Cup Soccer. Rocco (RoboCup-Commentator) is a prototype system that has emerged from our previous work on high-level scene analysis and intelligent multimedia generation. Based on a general conception for multimedia reporting systems, we describe the initial Rocco version which isintended to generate TV-style live reports for matches of the simulator league. 1
940|Learning from an Automated Training Agent|A learning agent employing reinforcement  learning is hindered because it only receives  the critic&#039;s sparse and weakly informative  training information. We present an approach  in which an automated training agent  may also provide occasional instruction to  the learner in the form of actions for the  learner to perform. The learner has access to  both the critic&#039;s feedback and the trainer&#039;s  instruction. In the experiments, we vary  the level of the trainer&#039;s interaction with the  learner, from allowing the trainer to instruct  the learner at almost every time step, to  not allowing the trainer to respond at all.  We also vary a parameter that controls how  the learner incorporates the trainer&#039;s actions.  The results show significant reductions in the  average number of training trials necessary to  learn to perform the task.  1 INTRODUCTION  In reinforcement learning, an automated agent attempts to develop a policy that indicates the actions to choose in performing a multiple-step ...
941|A Metalevel Coordination Strategy for Reactive Cooperative Planning|In this paper, we propose a metalevel coordination strategy to implement an adaptive organization for reactive cooperative planning. The adaptive organization changes its organizational scheme adaptively as a means of coping with the dynamic problem spaces. Preliminary experiments shows that an adaptive organization can be made to the increase efficiency in dynamic problem spaces. The reason for this works is that reducing the degree of freedom in the problem space, while increasing the degree of interaction, demands greater coordination. However, if the number of effective local plans decrease, it would seem likely that if the agents were to have a better metalevel strategy, they would be better able to search this reduced space efficiently. The metalevel coordination incorporates an agent-wide metalevel heuristic function. In designing the metalevel coordination strategy, we take three aspects of reactive cooperative planning into account. These aspects include: the difference in the...
942|Learning cases to compliment rules for conflict resolution in multiagent systems|Groups of agents following fixed behavioral rules can be limited in performance and etficiency. Adaptability and flexibility are key components of intelligent behav-ior which allow agent groups to improve performance in a given domain using prior problem solving experi-ence. We motivate the usefulness of individual learn-ing by group members in the context of overall group behavior. We propose a framework in which individ-ual group members learn cases to improve their model of other group members. We utilize a testbed prob-lem from the distributed AI literature to show that simultaneous learning by group members can lead to significant improvement in group performance and ef-ficiency over groups following static behavioral rules.
943|Time and the Prisoner&#039;s Dilemma|This paper examines the integration of computational complexity into game theoretic models. The example focused on is the Prisoner&#039;s Dilemma, repeated for a finite length of time. We show that a minimal bound on the players&#039; computational ability is sufficient to enable cooperative behavior. In addition, a variant of the repeated Prisoner&#039;s Dilemma game is suggested, in which players have the choice of opting out. This modification enriches the game and suggests dominance of cooperative strategies. Competitive analysis is suggested as a tool for investigating sub-optimal (but computationally tractable) strategies and game theoretic models in general. Using competitive analysis, it is shown that for bounded players, a sub-optimal strategy might be the optimal choice, given resource limitations. Keywords: Conceptual and theoretical foundations of multiagent systems
944|Building a Dedicated Robotic Soccer System|Robotic Soccer involves multiple agents that need to collaborate in an adversarial environment to achieve specific objectives. We have been building an architecture that addresses this integration of high-level and low-level reasoning as a combined system of mini-robots, a camera for perception, a centralized interface computer, and several client servers as the minds of the mini-robot players. In this paper we focus on the hardware design of our mini-robots. Our main purpose is to provide a detailed description of our design decisions so that others may learn from and replicate our efforts. Communication between the interface computer and the mini-robots is achieved through coded infrared radiation. The minirobots can turn on the spot and move forward and backward with variable speed. Our design also allows a well-balanced use of the on-board power and current supplies.  1 Introduction  As robots become more adept at operating in the real world, the high-level issues of collaborative ...
945|A Role-Based Decision-Mechanism for Teams of Reactive and Coordinating Agents|. In this paper we present a system for developing autonomous agents that combine reactivity to an uncertain and rapidly changing environment with commitment to prespecified tactics involving coordination and team work. The concept of roles in a team is central to the paper. Agents act and coordinate with other agents depending on their roles. The decision-mechanism of an agent is specified in terms of prioritized rules organized in a decision tree. In the decisiontree both reactivity to changes in the environment and commitment to long term courses of actions (behaviors) are present. The coordination between agents is mainly obtained through common tactics, strategies, and observations of actions of team members, rather than explicit communication. Coordinated behaviors can be specified in terms of roles and are encoded simultaneously for all participating agents. 1 Introduction An important issue in the development of multi-agent systems is how to create a decision mechanism for an ...
946|Resolving social dilemmas using genetic algorithms|A challenging problem for designers of agent so-cieties is the problem of providing for public goods (Hardin 1968). Public goods are social benefits that can be accessed by individuals ir-respective of their personal contributions. The dilemma for an individual agent is whether to be a contributor or to be an exploiter (enjoy bene-fits without contributing proportionately). It has been shown that selfish actions on the part of agents in a society can lead to ineffective social systems. In this paper, we evolve agent societies which are able to &#034;solve &#034; the dilemma. In one set of experiments, a genetic algorithm (GA) based approach is used to evolve agent societies that are faced with the Bracss ’ paradox (Irvine 1993). In another scenario, agent groups adapt to effec-tively utilize a pair of resources. Encouraged by this initial explorations, we plan to investigate another variation of the GA that uses less global information than the current approach.
947|Multiagent learning and adaptation in an information filtering market|This paper presents an adaptive model for multiagent coordination based on the metaphor of economic markets. This model has been used to develop SIGMA, a system for filtering Usenet netnews which is able to cope with the non-stationary and partially observable nature of the information filtering task at hand. SIGMA integrates a number of different learn-ing and adaptation techniques, including reinforce-ment learning, bidding price adjustment, and rele-vance feedback. Aspects of these are discussed below.
948|The CS Freiburg Team|. This paper describes the CS Freiburg Team for the RoboCup &#039;98 contest. Our main focus lies in developing reliable and robust perception technology and basic behaviors for playing soccer with mobile robots. We present the hard- and software components of our robots including a self-built device for kicking the ball. Furthermore, we show how some key components from robot navigation, e.g. self-localization, object-recognition and world-modeling, can be specialized for the wellstructured RoboCup environment. These components provide the robots with accurate and reliable information about the world. We also present our multi agent sensor fusion component for building a global world model. The results, so far, show that basic behaviors for playing soccer based on robust and reliable perception enable the robots to actually play soccer, e.g. score goals or keep the ball from rolling into our goal. 1 Introduction  After studying the matches at RoboCup&#039;97, we came to the conclusion that reli...
949|Synchronization and linearity: an algebra for discrete event systems|The first edition of this book was published in 1992 by Wiley (ISBN 0 471 93609 X). Since this book is now out of print, and to answer the request of several colleagues, the authors have decided to make it available freely on the Web, while retaining the copyright, for the benefit of the scientific community. Copyright Statement This electronic document is in PDF format. One needs Acrobat Reader (available freely for most platforms from the Adobe web site) to benefit from the full interactive machinery: using the package hyperref by Sebastian Rahtz, the table of contents and all LATEX cross-references are automatically converted into clickable hyperlinks, bookmarks are generated automatically, etc.. So, do not hesitate to click on references to equation or section numbers, on items of thetableofcontents and of the index, etc.. One may freely use and print this document for one’s own purpose or even distribute it freely, but not commercially, provided it is distributed in its entirety and without modifications, including this preface and copyright statement. Any use of thecontents should be acknowledged according to the standard scientific practice. The
950|Convex Analysis|In this book we aim to present, in a unified framework, a broad spectrum of mathematical theory that has grown in connection with the study of problems of optimization, equilibrium, control, and stability of linear and nonlinear systems. The title Variational Analysis reflects this breadth. For a long time, ‘variational ’ problems have been identified mostly with the ‘calculus of variations’. In that venerable subject, built around the minimization of integral functionals, constraints were relatively simple and much of the focus was on infinite-dimensional function spaces. A major theme was the exploration of variations around a point, within the bounds imposed by the constraints, in order to help characterize solutions and portray them in terms of ‘variational principles’. Notions of perturbation, approximation and even generalized differentiability were extensively investigated. Variational theory progressed also to the study of so-called stationary points, critical points, and other indications of singularity that a point might have relative to its neighbors, especially in association with existence theorems for differential equations.
952|Algebraic Tools for the Performance Evaluation of Discrete Event Systems|In this paper, it is shown that a certain class of Petri nets called event graphs can be represented as linear &#034;time-invariant&#034; finite-dimensional systems using some particular algebras. This sets the ground on which a theory of these systems can be developped in a manner which is very analogous to that of conventional linear system theory. Part 2 of the paper is devoted to showing some preliminary basic developments in that direction. Indeed, there are several ways in which one can consider event graphs as linear systems: these ways correspond to approaches in the time domain, in the event domain and in a two-dimensional domain. In each of these approaches, a di#erent algebra has to be used for models to remain linear. However, the common feature of these algebras is that they all fall into the axiomatic definition of &#034;dioids&#034;. Therefore, Part 1 of the paper is devoted to a unified presentation of basic algebraic results on dioids. 1 Introduction  Definitions and examples of Discrete ...
953|A GSMP formalism for discrete event systems|We describe here a precise mathematical framework for the study of discrete event systems. The idea i s to define a particular type of stochastic process, called a generalized semi-Markov process (GSMP), which captures the essential dynamical structure of a dis-crete event system. The paper also attempts to give a flavor of the qualitative theory and numerical algorithms that can be obtained as a result of viewing discrete event systems as GSMPs. I.
954|Theoremes Asymptotiques En Programmation Dynamique|. On montre l&#039;analogie existant entre le calcul des probabilites et la programmation dynamique. Dans la premiere situation les convolutions iterees de lois de probabilite jouent un role central, dans la seconde les inf-convolutions de fonctions couts ont un role similaire. L&#039;outil d&#039;analyse privilegiedelapremiere situation est la transformee de Fourier, celui de la seconde devrait etre la transform ee de Fenchel. Aux lois gaussiennes --- stables par convolution --- correspondent les formes quadratiques --- stables par inf-convolution. A la loi des grands des nombres et au theoreme de la limite centrale correspondent des theoremes asymptotiques pour la programmation dynamique --- convergence de la fonction valeur de l&#039;etat moyenne vers la fonction caracteristique du minimum du cout instantane, convergencede la fonction valeur de l&#039;ecart au minimun renormalise vers une forme quadratique. ABSTRACT. Asymptotic Theorems in Dynamic Programming  We show the analogy between probability calculu...
955|A linear system theory for systems subject to synchronization and saturation constraints|Abstract A linear system theory is developped for a class of continuous and discrete Systems subject to Synchronization and Saturations that we call S 3. This class provides a generalization of a particular class of timed discrete event systems which can be modeled as event graphs. The development is based on a linear modeling of S 3 in the min-plus algebra. This allows us to extend elementary notions and a number of results of conventional linear systems to S 3. In particular, notions of causality, time-invariance, impulse response, convolution, transfer function and rationality are considered.
956|Convex Analysis And Spectral Analysis Of Timed Event Graphs|Using the algebra of dioids, we further examine the analogy between timed event graphs and conventional linear systems by showing that some periodic inputs of the former behave as cosine inputs for the latter. In particular, we give a meaning to such notions as &#034;phase shift&#034; and &#034;amplification gain&#034;, which allow us to talk about the Black and Bode plots for discrete event systems. In this theory, classical concepts of Convex Analysis such as inf-convolution and Fenchel conjugate play the parts that convolution and Laplace transform play in the conventional case.  1 Introduction  Event graphs constitute a special class of Petri nets in which transitions admit several incoming and outgoing arcs whereas places admit single upstream and downstream arcs. In this way, only synchronization constraints (logical AND) can be represented, whereas alternative choices (corresponding to logical OR) cannot be represented. A fork---several outgoing arcs from a transition--- represents for example the ...
957|DIVA: A Reliable Substrate for Deep Submicron Microarchitecture Design|Building a high-petformance microprocessor presents many reliability challenges. Designers must verify the correctness of large complex systems and construct implementations that work reliably in varied (and occasionally adverse) operating conditions. To&amp;rther complicate this task, deep submicron fabrication technologies present new reliability challenges in the form of degraded signal quality and logic failures caused by natural radiation interference. In this paper; we introduce dynamic verification, a novel microarchitectural technique that can significantly reduce the burden of correctness in microprocessor designs. The approach works by augmenting the commit phase of the processor pipeline with a functional checker unit. Thefunctional checker verifies the correctness of the core processor’s computation, only permitting correct results to commit. Overall design cost can be dramatically reduced because designers need only veri ’ the correctness of the checker unit. We detail the DIVA checker architecture, a design optimized for simplicity and low cost. Using detailed timing simulation, we show that even resource-frugal DIVA checkers have little impact on core processor peflormance. To make the case for reduced verification costs, we argue that the DIVA checker should lend itself to functional and electrical verification better than a complex core processor Finally, future applications that leverage dynamic veri@cation to increase processor performance and availability are suggested. 1
958|Automatic Verification of Pipelined Microprocessor Control|We describe a technique for verifying the control logic of  pipelined microprocessors. It handles more complicated designs, and requires  less human intervention, than existing methods. The technique  automaticMly compares a pipelined implementation to an architectural  description. The CPU time needed for verification is independent of the  data path width, the register file size, and the number of ALU operations.
959|AR-SMT: A Microarchitectural Approach to Fault Tolerance in Microprocessors|This paper speculates that technology trends pose new challenges for fault tolerance in microprocessors. Specifically, severely reduced design tolerances implied by gigaherz clock rates may result in frequent and arbitrary transient faults. We suggest that existing fault-tolerant techniques -- system-level, gate-level, or component-specific approaches -- are either too costly for general purpose computing, overly intrusive to the design, or insufficient for covering arbitrary logic faults. An approach in which the microarchitecture itself provides fault tolerance is required.  We propose a new time redundancy fault-tolerant approach in which a program is duplicated and the two redundant programs simultaneously run on the processor. The technique exploits several significant microarchitectural trends to provide broad coverage of transient faults and restricted coverage of permanent faults. These trends are simultaneous multithreading, control flow and data flow prediction, and hierarchi...
