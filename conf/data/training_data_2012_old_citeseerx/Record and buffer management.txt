ID|Title|Summary
1|Exokernel: An Operating System Architecture for Application-Level Resource Management|We describe an operating system architecture that securely multiplexes machine resources while permitting an unprecedented degree of application-specific customization of traditional operating system abstractions. By abstracting physical hardware resources, traditional operating systems have significantly limited the performance, flexibility, and functionality of applications. The exokernel architecture removes these limitations by allowing untrusted software to implement traditional operating system abstractions entirely at application-level. We have implemented a prototype exokernel-based system that includes Aegis, an exokernel, and ExOS, an untrusted application-level operating system. Aegis defines the low-level interface to machine resources. Applications can allocate and use machine resources, efficiently handle events, and participate in resource revocation. Measurements show that most primitive Aegis operations are 10–100 times faster than Ultrix,a mature monolithic UNIX operating system. ExOS implements processes, virtual memory, and inter-process communication abstractions entirely within a library. Measurements show that ExOS’s application-level virtual memory and IPC primitives are 5–50 times faster than Ultrix’s primitives. These results demonstrate that the exokernel operating system design is practical and offers an excellent combination of performance and flexibility. 1
2|Active Messages: a Mechanism for Integrated Communication and Computation|The design challenge for large-scale multiprocessors is (1) to minimize communication overhead, (2) allow communication to overlap computation, and (3) coordinate the two without sacrificing processor cost/performance. We show that existing message passing multiprocessors have unnecessarily high communication costs. Research prototypes of message driven machines demonstrate low communication overhead, but poor processor cost/performance. We introduce a simple communication mechanism, Active Messages, show that it is intrinsic to both architectures, allows cost effective use of the hardware, and offers tremendous flexibility. Implementations on nCUBE/2 and CM-5 are described and evaluated using a split-phase shared-memory extension to C, Split-C. We further show that active messages are sufficient to implement the dynamically scheduled languages for which message driven machines were designed. With this mechanism, latency tolerance becomes a programming/compiling concern. Hardware support for active messages is desirable and we outline a range of enhancements to mainstream processors.  
3|Efficient Software-Based Fault Isolation|One way to provide fault isolation among cooperating software modules is to place each in its own address space. However, for tightly-coupled modules, this solution incurs prohibitive context switch overhead. In this paper, we present a software approach to implementing fault isolation within a single address space. Our approach has two parts. First, we load the code and data for a distrusted module into its own fault domain, a logically separate portion of the application&#039;s address space. Second, we modify the object code of a distrusted module to prevent it from writing or jumping to an address outside its fault domain. Both these software operations are portable and programming language independent. Our approach poses a tradeo relative to hardware fault isolation: substantially faster communication between fault domains, at a cost of slightly increased execution time for distrusted modules. We demonstrate that for frequently communicating modules, implementing fault isolation in software rather than hardware can substantially improve end-to-end application performance.
4|Lottery Scheduling: Flexible Proportional-Share Resource Management|This paper presents lottery scheduling, a novel randomized resource allocation mechanism. Lottery scheduling provides efficient, responsive control over the relative execution rates of computations. Such control is beyond the capabilities of conventional schedulers, and is desirable in systems that service requests of varying importance, such as databases, media-based applications, and networks. Lottery scheduling also supports modular resource management by enabling concurrent modules to insulate their resource allocation policies from one another. A currency abstraction is introduced to flexibly name, share, and protect resource rights. We also show that lottery scheduling can be generalized to manage many diverse resources, such as I/O bandwidth, memory, and access to locks. We have implemented a prototype lottery scheduler for the Mach 3.0 microkernel, and found that it provides flexible and responsive control over the relative execution rates of a wide range of applications. The overhead imposed by our unoptimized prototype is comparable to that of the standard Mach timesharing policy. 
5|Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism|Threads are the vehicle,for concurrency in many approaches to parallel programming. Threads separate the notion of a sequential execution stream from the other aspects of traditional UNIX-like processes, such as address spaces and I/O descriptors. The objective of this separation is to make the expression and control of parallelism sufficiently cheap that the programmer or compiler can exploit even fine-grained parallelism with acceptable overhead. Threads can be supported either by the operating system kernel or by user-level library code in the application address space, but neither approach has been fully satisfactory. This paper addresses this dilemma. First, we argue that the performance of kernel threads is inherently worse than that of user-level threads, rather than this being an artifact of existing implementations; we thus argue that managing par- allelism at the user level is essential to high-performance parallel computing. Next, we argue that the lack of system integration exhibited by user-level threads is a consequence of the lack of kernel support for user-level threads provided by contemporary multiprocessor operating systems; we thus argue that kernel threads or processes, as currently conceived, are the wrong abstraction on which to support user- level management of parallelism. Finally, we describe the design, implementation, and performance of a new kernel interface and user-level thread package that together provide the same functionality as kernel threads without compromis- ing the performance and flexibility advantages of user-level management of parallelism.
6|Extensibility, safety and performance in the SPIN operating system|This paper describes the motivation, architecture and performance of SPIN, an extensible operating system. SPIN provides an extension infrastructure, together with a core set of extensible services, that allow applications to safely change the operating system&#039;s interface and implementation. Extensions allow an application to specialize the underlying operating system in order to achieve a particular level of performance and functionality. SPIN uses language and link-time mechanisms to inexpensively export ne-grained interfaces to operating system services. Extensions are written in a type safe language, and are dynamically linked into the operating system kernel. This approach o ers extensions rapid access to system services, while protecting the operating system code executing within the kernel address space. SPIN and its extensions are written in Modula-3 and run on DEC Alpha workstations. 1
7|Lightweight remote procedure call|Lightweight Remote Procedure Call (LRPC) is a communication facility designed and optimized for communication between protection domains on the same machine. In contemporary small-kernel operating systems, existing RPC systems incur an unnecessarily high cost when used for the type of communication that predominates-between protection domains on the same machine. This cost leads system designers to coalesce weakly related subsystems into the same protection domain, trading safety for performance. By reducing the overhead of same-machine communication, LRPC encourages both safety and performance. LRPC combines the control transfer and communication model of capability systems with the programming semantics and large-grained protection model of RPC. LRPC achieves a factor-of-three performance improvement over more traditional approaches based on independent threads exchanging messages, reducing the cost of same-machine communication to nearly the lower bound imposed by conventional hardware. LRPC has been integrated into the Taos operating system of the DEC SRC Firefly multiprocessor workstation.
8|Experiences with the amoeba distributed operating system|The Amoeba distributed operating system has been in development and use for over eight years now. In this paper we describe the present system and our experience with it—what we did right, but also what we did wrong. Among the things done right were basing the system on objects, using a single uniform mechanism (capabilities) for naming and protecting them in a location independent way, and designing a completely new, and very fast file system. Among the things done wrong were having threads not be pre-emptable, initially building our own homebrew window system, and not having a multicast facility at the outset.
9|The Packet Filter: An Efficient Mechanism for User-level Network Code|Code to implement network protocols can be either inside the kernel of an operating system or in user-level processes. Kernel-resident code is hard to develop, debug, and maintain, but user-level implementations typically incur significant overhead and perform poorly.  The performance of user-level network code depends on the mechanism used to demultiplex received packets. Demultiplexing in a user-level process increases the rate of context switches and system calls, resulting in poor performance. Demultiplexing in the kernel eliminates unnecessary overhead.  This paper describes the packet filter, a kernel-resident, protocolindependent packet demultiplexer. Individual user processes have great flexibility in selecting which packets they will receive. Protocol implementations using the packet filter perform quite well, and have been in production use for several years.  
10|Hints for Computer Systems Design|Studying the design and implementation of a number of computer has led to some general hints for system design. They are described here and illustrated by many examples, ranging from hardware such as the Alto and the Dorado to application programs such as Bravo and Star. 1.
11|Overview of the CHORUS Distributed Operating Systems|The CHORUS technology has been designed for building new generations of open, distributed, scalable operating systems. CHORUS has the following main characteristics:  # a communication-based architecture, relying on a minimal Nucleus which integrates distributed processing and communication at the lowest level, and which implements generic services used by a set of subsystem servers to extend standard operating system interfaces. A UNIX subsystem has been developed; other subsystems such as objectoriented systems are planned;  # a real-time Nucleus providing real-time services which are accessible to system programmers;   # a modular architecture providing scalability, and allowing, in particular, dynamic configuration of the system and its applications over a wide range of hardware and network configurations, including parallel and multiprocessor systems. CHORUS-V3 is the current version of the CHORUS Distributed Operating System, developed by Chorus systemes. Earlier versions were st...
12|Virtual Memory Primitives for User Programs|Memory Management Units (MMUs) are traditionally used by operating systems to implement disk-paged virtual memory. Some operating systems allow user programs to specify the protection level (inaccessible, readonly. read-write) of pages, and allow user programs t.o handle protection violations. bur. these mechanisms are not. always robust, efficient, or well-mat. ched to the needs of applications.
13|Improving IPC by kernel design|Inter-process communication (ipc) has to be fast and e ective, otherwise programmers will not use remote procedure calls (RPC), multithreading and multitasking adequately. Thus ipc performance is vital for modern operating systems, especially µ-kernel based ones. Surprisingly, most µ-kernels exhibit poor ipc performance, typically requiring 100 µs for a short message transfer on a modern processor, running with 50 MHz clock rate. In contrast, we achieve 5 µs; a twentyfold improvement. This paper describes the methods and principles used, starting from the architectural design and going down to the coding level. There is no single trick to obtaining this high performance; rather, a synergetic approach in design and implementation on all levels is needed. The methods and their synergy are illustrated by applying them to a concrete example, the L3-kernel (an industrial-quality operating system in daily use at several hundred sites). The main ideas are to guide the complete kernel design by the ipc requirements, and to make heavy use of the concept of virtual address space inside the-kernel itself. As the L3 experiment shows, significant performance gains are possible: compared with Mach, they range from a factor of 22 (8-byte messages) to 3 (4-Kbyte messages). Although hardware specific details in uence both the design and implementation, these techniques are applicable to the whole class of conventional general
14|Experiences with a High-Speed Network Adaptor: A Software Perspective|This paper describes our experiences, from a software perspective, with the OSIRIS network adaptor. It first identifies the problems we encountered while programming OSIRIS and optimizing network performance, and outlines how we either addressed them in the software, or had to modify the hardware. It then describes the opportunities provided by OSIRIS that we were able to exploit in the host operating system (OS); opportunities that suggested techniques for making the OS more effective in delivering network data to application programs. The most novel of these techniques, called application device channels, gives application programs running in user space direct access to the adaptor. The paper concludes with the lessons drawn from this work, which we believe will benefit the designers of future network adaptors. 1 Introduction  With the emergence of high-speed network facilities, several research efforts are focusing on the design and implementation of network adaptors [5, 2, 3, 16, 2...
15|Stride Scheduling: Deterministic Proportional-Share Resource Management|This paper presents stride scheduling, a deterministic scheduling technique that efficiently supports the same flexible resource management abstractions introduced by lottery scheduling. Compared to lottery scheduling, stride scheduling achieves significantly improved accuracy over relative throughput rates, with significantly lower response time variability. Stride scheduling implements proportional-share control over processor time and other resources by cross-applying elements of rate-based flow control algorithms designed for networks. We introduce new techniques to support dynamic changes and higher-level resource management abstractions. We also introduce a novel hierarchical stride scheduling algorithm that achieves better throughput accuracy and lower response time variability than prior schemes. Stride scheduling is evaluated using both simulations and prototypes implemented for the Linux kernel.
16|Accent: A Communication Oriented Network Operating System Kernel|Accent is a communication oriented operating system kernel being built at Carnegie-Mellon University to support the distributed personal computing project, Spice, and the development of a fault-tolerant distributed sensor network (DSN). Accent is. built around a single, powerful abstraction of communication between processes, with all kernel functions, such as device access and virtual memory management accessible through messages and distributable throughout a network. In this paper, specific attention is given to system supplied facilities which support transparent network access and fault-tolerant behavior. Many of these facilities are already being provided under a modified version of VAX/UNIX. The Accent system itself is currently being implemented on the Three Rivers Corp. PERQ. Keywords: Inter-process communication, networking,
17|Implementation and Performance of Application-Controlled File Caching|Traditional file system implementations do not allow applications to control file caching replacement decisions.  We have implemented two-level replacement, a scheme that allows applications to control their own cache replacement, while letting the kernel control the allocation of cache space among processes. We designed an interface to let applications exert control on replacement via a set of directives to the kernel. This is effective and requires low overhead.  We demonstrate that for applications that do not perform well under traditional caching policies, the combination of good application-chosen replacement strategies, and our kernel allocation policy LRU-SP, can reduce the number of block I/Os by up to 80%, and can reduce the elapsed time by up to 45%. We also show that LRU-SP is crucial to the performance improvement for multiple concurrent applications: LRUSP fairly distributes cache blocks and offers protection against foolish applications.   
18|PATHFINDER: A Pattern-Based Packet Classifier|This paper describes a pattern-based approach to building packet classifiers. One novelty of the approach is that it can be implemented efficiently in both software and hardware. A performance study shows that the software implementation is about twice as fast as existing mechanisms, and that the hardware implementation is currently able to keep up with OC-12 (622Mbps) network links and is likely to operate at gigabit speeds in the near future. 1 Introduction A packet classifier is a mechanism that inspects incoming network packets, and based on the values found in select header fields, determines how each is to be processed. A classifier can be thought of as assigning a tag (type) to each packet, or alternatively, identifying the flow or path to which the packet belongs. We call this mechanism a packet classifier rather than a packet filter because it more accurately describes the function being performed---it classifies all packets, rather than filtering out select packets. Packet ...
19|Efficient Packet Demultiplexing for Multiple Endpoints and Large Messages|This paper describes a new packet filter mechanism that efficiently dispatches incoming network packets to one of multiple endpoints, for example address spaces. Earlier packet filter systems iteratively applied each installed filter against every incoming packet, resulting in high processing overhead whenever multiple filters existed. Our new packet filter provides an associative match function that enables similar but not identical filters to be combined together into a single filter. The filter mechanism, which we call the Mach Packet Filter (MPF), has been implemented for the Mach 3.0 operating system and is being used to support endpoint-based protocol processing, whereby each address space implements its own suite of network protocols. With large numbers of registered endpoints, MPF outperforms the earlier BSD Packet Filter (BPF) by over a factor of four. MPF also allows a filter program to dispatch fragmented packets, which was quite difficult with previous filter mechanisms.  
20|Sharing and protection in a single-address-space operating system|This article explores memory sharing and protection support in Opal, a single-address-space operating system designed for wide-address (64-bit) architectures. Opal threads execute within protection domains in a single shared virtual address space. Sharing is simplified, because addresses are context independent. There is no loss of protection, because addressability and access are independent; the right to access a segment is determined by the protection domain in which a thread executes. This model enables beneficial code- and data-sharing patterns that are currently prohibitive, due in part to the inherent restrictions of multiple address spaces, and in part to Unix programming style. We have designed and implemented an Opal prototype using the Mach 3.0 microkernel as a base. Our implementation demonstrates how a single-address-space structure can be supported alongside of other environments on a modern microkernel operating system, using modern wide-address architectures. This article justifies the opal model and its goals for sharing and protection, presents the system and its abstractions, describes the prototype implementation,
21|Threads and Input/Output in the Synthesis kernel|The Synthesis operating system kernel combines several techniques to provide high performance, including kernel code synthesis, ne-grain scheduling, and optimistic synchronization. Kernel code synthesis reduces the execution path for frequently used kernel calls. Optimistic synchronization increases concurrency within the kernel. Their combination results in signi cant performance improvement over traditional operating system implementations. Using hardware and software emulating a SUN 3/160 running SUNOS, Synthesis achieves several times to several dozen times speedup for Unix kernel calls and context switch times of 21 microseconds or faster. 1
22|Limits to Low-Latency Communication on High-Speed Networks|The throughput of local area networks is rapidly increasing. For example, the bandwidth of new ATM networks and FDDI token rings is an order of magnitude greater than that of Ethernets. Other network technologies promise a bandwidth increase of yet another order of magnitude in a few years. However, in distributed systems, lowered latency rather than increased throughput is often of primary concern. This paper examines the system-level effects of newer high-speed network technologies on low-latency, cross-machine communications. To evaluate a number of influences, both hardware and software, we designed and imple-mented a new remote procedure call system targeted at providing low latency. We then ported this system to several hardware platforms (DECstation and SPARCstation) with several differ-ent networks and controllers (ATM, FDDI, and Ethernet). Comparing these systems allows us to explore the performance impact of alternative designs in the communication system with respect to achieving low latency, e.g., the network, the network controller, the host architecture and cache system, and the kernel and user-level runtime software. Our RPC system, which achieves substantially reduced call times (170 pseconds on an ATM network using DECstation 5000/200 hosts), allow us to isolate those components of next-
23|Pilot: An Operating System for a Personal Computer|this paper was presented at the 7th ACM Symposium on Operating Systems Principles, Pacific Grove, Calif., Dec. 10-12, 1979
24|Synthesis: An Efficient Implementation of Fundamental Operating System Services|This dissertation shows that operating systems can provide fundamental services an order of magnitude more efficiently than traditional implementations. It describes the implementation of a new operating system kernel, Synthesis, that achieves this level of performance. The Synthesis kernel combines several new techniques to provide high performance without sacrificing the expressive power or security of the system. The new ideas include: ffl Run-time code synthesis --- a systematic way of creating executable machine code at runtime to optimize frequently-used kernel routines --- queues, buffers, context switchers, interrupt handlers, and system call dispatchers --- for specific situations, greatly reducing their execution time. ffl Fine-grain scheduling --- a new process-scheduling technique based on the idea of feedback that performs frequent scheduling actions and policy adjustments (at submillisecond intervals) resulting in an adaptive, self-tuning system that can support real-ti...
25|Architectural support for translation table management in large address space machines|Virtual memoy page translation tables provide mappings from virtual to physical addresses. When the hardware controlled Tratmlation L.ookaside Buffers (TLBs) do not contain a translation, these tables provide the translation. Approaches to the structure and management of these tables vary from full hardware implementations to complete software based algon”thms. The size of the virtual aaliress space used by processes is rapidly growing beyond 32 bits of address. As the utilized address space increases, new problems and issues surjace. Traditional methoak for managing the page translation tables are inappropriate for large address space architectures. The Hashed Page Table (HPI’), described here, provides a very fast and space ejicient translation table that reduces ovdwad by splitting TLB management responsibilities between hardware and software. Measurements demonstrate its applicability to a diverse range of operating systems and workloads and, in particular, to large virtual address space machines. In simulations of over 4 billion instructions, improvements of 5 to IO % were observed. 1.
26|Design Tradeoffs for Software-Managed TLBs|this paper appeared in the Proceedings of the 20th Annual International Symposium on Computer Architecture, San Diego, May 1993. Authors&#039; address: Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109-2122 This work was supported by Defense Advanced Research Projects Agency under DARPA/ARO Contract Number DAAL03-90-C-0028 and a National Science Foundation Graduate Fellowship.  Uhlig et al. . 2 within the kernel. These and related operating system trends place greater stress upon the TLB by increasing miss rates and, hence, decreasing overall system performance. This paper explores these issues by examining design trade-offs for software-managed TLBs and their impact, in conjunction with various operating systems, on overall system performance. To examine issues which cannot be adequately modeled with simulation, we have developed a system analysis tool called Monster, which enables us to monitor actual systems. We have also developed a novel TLB simulator called Tapeworm, which is compiled directly into the operating system so that it can intercept all TLB misses caused by both user process and OS kernel memory references. The information that Tapeworm extracts from the running system is used to obtain TLB miss counts and to simulate different TLB configurations. The remainder of this paper is organized as follows: Section 2 examines previous TLB and OS research related to this work. Section 3 describes our analysis tools, Monster and Tapeworm. The MIPS R2000 TLB structure and its performance under Ultrix, OSF/1 and Mach 3.0 are explored in Section 4. Hardware- and software-based performance improvements are presented in Section 5. Section 6 summarizes our conclusions. 2 RELATED WORK By caching page table entries, TLBs g...
27|SPIN - an extensible microkernel for application-specific operating system services|Application domains, such as multimedia, databases, and parallel computing, require operating system services with high performance and high functionality. Existing operating systems provide fixed interfaces and implementations to system services and resources. This makes them inappropriate for applications whose resource demands and usage patterns are poorly matched by the services provided. The SPIN operating system enables system services to be defined in an application-specific fashion through an extensible microkernel. It offers fine-grained control over a machine&#039;s logical and physical resources to applications through run-time adaptation of the system to application requirements. 1
28|Hardware and Software Support for Efficient Exception Handling|Program-synchronous exceptions, for example, breakpoints, watchpoints, illegal opcodes, and memory access violations, provide information about exceptional conditions, interrupting the program and vectoring to an operating system handler. Over the last decade, however, programs and run-time systems have increasingly employed these mechanisms as a performance optimization to detect normal and expected conditions. Unfortunately, current architecture and operating system structures are designed for exceptional or erroneous conditions, where performance is of secondary importance, rather than normal conditions. Consequently, this has limited the practicality of such hardware-based detection mechanisms. We propose both hardware and software structures that permit efficient handling of synchronous exceptions by user-level code. We demonstrate a software implementation that reduces exceptiondelivery cost by an order-of-magnitude on current RISC processors, and show the performance benefits o...
29|Fast Mutual Exclusion for Uniprocessors|In this paper we describe restartable atomic sequences, an optimistic mechanism for implementing simple atomic operations (such as Test-And-Set) on a uniprocessor. A thread that is suspended within a restartable atomic sequence is resumed by the operating system at the beginning of the sequence, rather than at the point of suspension. This guarantees that the thread eventually executes the sequence atomically. A restartable atomic sequence has signi cantly less overhead than other software-based synchronization mechanisms, such askernel emulation or software reservation. Consequently, it is an attractive alternative for use on uniprocessors that do not support atomic operations. Even on processors that do support atomic operations in hardware, restartable atomic sequences can have lower overhead. We describe di erent implementations of restartable atomic sequences for the Mach 3.0 and Taos operating systems. These systems &#039; thread management packages
30|Software Prefetching and Caching for Translation Lookaside Buffers|A number of interacting trends in operating system structure, processor architecture, and memory systems are increasing both the rate of translation lookaside buffer (TLB) misses and the cost of servicing a miss. This paper presents two novel software schemes, implemented under Mach 3.0, to decrease both the number and the cost of kernel TLB misses (i.e., misses on kernel data structures, including user page tables). The first scheme is a new use of prefetching for TLB entries on the IPC path, and the second scheme is a new use of software caching of TLB entries for hierarchical page table organizations.  For a range of applications, prefetching decreases the number of kernel TLB misses by 40% to 50%, and caching decreases TLB penalties by providing a fast path for over 90% of the misses. Our caching scheme also decreases the number of nested TLB traps due to the page table hierarchy, reducing the number of kernel TLB miss traps for applications by 20% to 40%. Prefetching and caching, ...
31|Tools for the Development of Application-Specific Virtual Memory Management|While many applications incur few page faults, some scientific and database applications perform poorly when running on top of a traditional virtual memory implementation. To help address this problem, several systems have been built to allow each program the flexibility to use its own application-specific page replacement policy, in place of the generic policy provided by the operating system. This has the potential to improve performance for the class of applications limited by virtual memory behavior; however, to realize this performance gain, application developers must re-implement much of the virtual memory system, a non-trivial programming task. Our goal is to make it easy for programmers to develop new application-specific page replacement policies. To do this, we have implemented (i) an extensible object-oriented user-level virtual memory system and (ii) a graphical performance monitor for virtual memory behavior. Together, these help the user to identify problems with an appl...
33|The Operating System Kernel as a Secure Programmable Machine|To provide modularity and performance, operating system kernels should have only minimal embedded functionality. Today&#039;s operating systems are large, inefficient and, most importantly, inflexible. In our view, most operating system performance and flexibility problems can be eliminated simply by pushing the operating system interface lower. Our goal is to put abstractions traditionally implemented by the kernel out into user-space, where user-level libraries and servers abstract the exposed hardware resources. To achieve this goal, we have defined a new operating system structure, exokernel, that safely exports the resources defined by the underlying hardware. To enable applications to benefit from full hardware functionality and performance, they are allowed to download additions to the supervisor-mode execution environment. To guarantee that these extensions are safe, techniques such as code inspection, inlined cross-domain procedure calls, and secure languages are used. To test and ...
34|An open operating system for a single-user machine|The file system and modularization of a single-user operating system are described. The main points of interest are the openness of the system, which establishes no sharp boundary between itself and the user&#039;s programs, and the techniques used to make the system robust. 1.
35|How to Use a 64-Bit Virtual Address Space|Most operating systems execute programs in private address spaces communicating through messages or files. The traditional private address space model was developed for 16- and 32-bit architectures, on which virtual addresses are a scarce resource. The recent appearance of architectures with flat 64-bit virtual addressing opens an opportunity to reconsider our use of virtual address spaces. In this paper we argue for an alternative addressing model, in which all programs and data reside in a single global virtual address space shared by multiple protection domains. Hardware-based memory protection exists within the single address space, providing firewalls as strong as in conventional systems. We explore the tradeoffs in the use of a global virtual address space relative to the private address space model. We contend that a shared address space can eliminate obstacles to efficient sharing and exchange of data structures that are inherent in private address space systems. The shared add...
36|Vino: an integrated platform for operating systems and database research|In 1981, Stonebraker wrote: Operating system services in many existing systems are either too slow or inappropriate. Current DBMSs usually provide their own and make little or no use of those o ered by the operating system. [STON81] The standard operating system model has changed little since that time, and we believe that, at its core, it is the wrong model for DBMS and other resource-intensive applications. The standard model is in exible, uncooperative, and irregular in its treatment of resources. We describe the design of a new system, the VINO kernel, which addresses the limitations of standard operating systems. It focuses on three key ideas: Applications direct policy. Kernel mechanisms are reusable by applications. All resources share a common extensible interface. VINO&#039;s power and exibility make it an ideal platform for the design and implementation of traditional and modern database management systems. 1
37|SPACE: A New Approach to Operating System Abstraction|Object-oriented operating systems, as well as conventional O/S designs, present an overly restrictive level of abstraction to the programmer. Models of objects, processes, concurrency, etc., are embedded within the system in such a way that they are difficult to extend or replace. SPACE is an extensible operating system being developed for research into object-oriented and distributed systems design. SPACE uses capability mechanisms based on the manipulation of address spaces to provide low-level kernel primitives from which higherlevel abstractions can be constructed. Standard micro-kernel abstractions such as processes, virtual memory, interprocess communication, and object models are built outside the kernel in SPACE, using the SPACE-kernel primitives: spaces, domains, and portals. Multiple versions of the standard O/S abstractions can coexist and interact.
39|FUGU: Implementing Translation and Protection in a  Multiuser Multimodel Multiprocessor|Multimodel multiprocessors provide both shared memory and message passing primitives to the  user for efficient communication. In a multiuser machine, translation permits machine resources to be  virtualized and protection permits users to be isolated. The challenge in a multiuser multiprocessor is  to provide translation and protection sufficient for general-purpose computing without compromising  communication performance, particularly the performance of communication between parallel threads  belonging to the same computation. FUGU is a proposed architecture that integrates translation and  protection with a set of communication mechanisms originally designed for high performance on a  single-user, physically-addressed, large-scale, multimodel multiprocessor. Communication
40|Mining Association Rules between Sets of Items in Large Databases|We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm. 
41|Induction of Decision Trees| The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.  
42|A Theory of the Learnable|  Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems.
43|Discovering functional formulas through changing representation|This paper deals with computer generation of numerical functional formulas describing results of scientific experiments (measurements). It describes the methodology for generating functional physical laws called COPER (Kokar 1985a). This method generates only so called &#034;meaningful functions&#034;, i.e., such that fulfill some syntactic conditions. In the case of physical laws these conditions are described in the theory of dimensional analysis, which provides rules for grouping arguments of a function into a (smaller) number of dimensionless monomials. These monomials constitute new arguments for which a functional formula is generated. COPER takes advantage of the fact that the grouping is not unique since it depends on which of the initial arguments are chosen as so called &#034;dimensional base&#034; (representation base). For a given functional formula the final result depends on the base. In its search for a functional formula COPER first performs a search through different representation bases for a fixed form of the function before going into more complex functional formulas. It appears that for most of the physical laws only two classes of functional formulas- linear functions and second degree polynomials- need to be considered to generate a formula exactly matching the law under consideration. 1.
44|The x-Kernel: An Architecture for Implementing Network Protocols|This paper describes a new operating system kernel, called the x-kernel, that provides an  explicit architecture for constructing and composing network protocols. Our experience  implementing and evaluating several protocols in the x-kernel shows that this architecture  is both general enough to accommodate a wide range of protocols, yet efficient enough to  perform competitively with less structured operating systems.  1 Introduction  Network software is at the heart of any distributed system. It manages the communication hardware that connects the processors in the system and it defines abstractions through which processes running on those processors exchange messages. Network software is extremely complex: it must hide the details of the underlying hardware, recover from transmission failures, ensure that messages are delivered to the application processes in the appropriate order, and manage the encoding and decoding of data. To help manage this complexity, network software is divi...
45|The v distributed system|The V distributed System was developed at Stanford University as part of a research project to explore issues in distributed systems. Aspects of the design suggest important directions for the design of future operating systems and communication systems. 
46|The sprite network operating system|Sprite is a new operating system for networked uniprocessor and multiprocessor workstations with large physical memories. It implements a set of kernel calls much like those of 4.3 BSD UNIX, with extensions to allow processes on the same workstation to share memory and to allow processes to migrate between workstations. The implementation of the Sprite kernel contains several interesting features, including a remote procedure call facility for communication between kernels, the use of prefix tables to implement a single file name space and to provide flexibility in administering the network file system, and large variable-size file caches on both client and server machines, which provide high performance even for diskless workstations.
47|A Stream Input-Output System|In a new version of the Unix operating system, a flexible coroutine-based design replaces the traditional rigid connection between processes and terminals or networks. Processing modules may be inserted dynamically into the stream that connects a user&#039;s program to a device. Programs may also connect directly to programs, providing interprocess communication. Introduction  The part of the Unix operating system that deals with terminals and other character devices has always been complicated. In recent versions of the system it has become even more so, for two reasons. 1) Network connections require protocols more ornate than are easily accommodated in the existing structure. A notion of &#034;line disciplines&#034; was only partially successful, mostly because in the traditional system only one line discipline can be active at a time. 2) The fundamental data structure of the traditional character I/O system, a queue of individual characters (the &#034;clist&#034;), is costly because it accepts and dispense...
48|Preserving and Using Context Information in Interprocess Communication|ion  Psync is based on a conversation abstraction that provides a shared message space through which a collection of processes exchange messages. The general form of this message space is defined by a directed acyclic graph that preserves the partial order of the exchanged messages. For the purpose of this section, we view a conversation as an abstract data type that is implemented in shared memory; Section 3 gives an algorithm for implementing a conversation in an unreliable network.  A conversation behaves much like any connection-oriented IPC abstraction: A well-defined set of processes---called participants---explicitly open a conversation, exchange messages through it, and close the conversation. Only processes that have been identified as participants may exchange message through the conversation, and this set is fixed for the duration of the conversation. Processes begin a conversation with the operations:  conv = active open(participant set)  conv = passive open(pid)  The first...
49|Performance of Firefly RPC|In this paper, we report on the performance of the remote procedure call implementation for the Firefly multiprocessor and analyze the implemen-tation to account precisely for all measured latency. From the analysis and measurements, we estimate how much faster RPC could be if certain improve-ments were made. The elapsed time for an inter-machine call to a remote procedure that accepts no arguments and produces no results is 2.66 milliseconds. The elapsed time for an RPC that has a single 1440-byte result (the maximum result that will fit in a single packet) is 6.35 milliseconds. Maximum inter-machine throughput using RPC is 4.65 megabits/second, achieved with 4 threads making parallel RPCs that return the maximum sized single packet result. CPU utilization at maximum throughput is about 1.2 on the calling machine and a little less on the server. These measurements are for RPCs from user space on one machine to user space on another, using the installed system and a 10 megabit/second E...
50|Distributed Process Groups in the V Kernel|The V kernel supports an abstraction of processes, with operations for interprocess communication, process management, and memory management. This abstraction is used as a software base for constructing distributed systems. As a distributed kernel, the V kernel makes intermachine bound-aries largely transparent. In this environment of many cooperating processes on different machines, there are many logical groups of processes. Examples include the group of tile servers, a group of processes executing a particular job, and a group of processes executing a distributed parallel computation. In this paper we describe the extension of the V kernel to support process groups. Operations on groups include group interprocess communication, which provides an application-level abstraction of network multicast. Aspects of the implementation and performance, and initial experience with applications are discussed.
51|An overview of the SR language and implementation|SR is a language for programming distributed systems ranging from operating systems to application programs. On the basis of our experience with the initial version, the language has evolved consider-ably. In this paper we describe the current version of SR and give an overview of its implementation. The main language constructs are still resources and operations. Resources encapsulate processes and variables that they share; operations provide the primary mechanism for process interaction. One way in which SR has changed is that both resources and processes are now created dynamically. Another change is that inheritance is supported. A third change is that the mechanisms for operation invocation-call and send-and operation implementation-proc and in-have been extended and integrated. Consequently, all of local and remote procedure call, rendezvous, dynamic process creation, asynchronous message passing, multicast, and semaphores are supported. We have found this flexibility to be very useful for distributed programming. Moreover, by basing SR on a small number of well-integrated concepts, the language has proved easy to learn and use, and it has a reasonably efficient implementation.
52|Performance of the World&#039;s Fastest Distributed Operating System|Distributed operating systems have been in the experimental stage for a  number of years now, but few have progressed to the point of actually being  used in a production environment. It is our belief that the reason lies primarily  with the performance of these systems---they tend to be fairly slow  compared to traditional single computer systems. The Amoeba system has  been designed with high performance in mind. In this paper some performance  measurements of Amoeba are presented and comparisons are made  with UNIX on the SUN, as well as with some other interesting systems. In  particular, short remote procedure calls take 1.4 msec and long data transfers  achieve a user-to-user bandwidth of 677 kbytes/sec. Furthermore, the file  server is so fast that it is limited by the communication bandwidth to 677  kbytes/sec. The real speed of the file server is too high to measure. To the  best of our knowledge, these are the best figures yet reported in the literature  for the class of hard...
53|Routing in a delay tolerant network|We formulate the delay-tolerant networking routing problem, where messages are to be moved end-to-end across a connectivity graph that is time-varying but whose dynamics may be known in advance. The problem has the added constraints of finite buffers at each node and the general property that no contemporaneous end-to-end path may ever exist. This situation limits the applicability of traditional routing approaches that tend to treat outages as failures and seek to find an existing end-to-end path. We propose a framework for evaluating routing algorithms in such environments. We then develop several algorithms and use simulations to compare their performance with respect to the amount of knowledge they require about network topology. We find that, as expected, the algorithms using the least knowledge tend to perform poorly. We also find that with limited additional knowledge, far less than complete global knowledge, efficient algorithms can be constructed for routing in such environments. To the best of our knowledge this is the first such investigation of routing issues in DTNs.
54|A Performance Comparison of Multi-Hop Wireless Ad Hoc Network Routing Protocols|An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Due to the limited transmission range of wireless network interfaces, multiple network &amp;quot;hops &amp;quot; may be needed for one node to exchange data with another across the network. In recent years, a variety of new routing protocols targeted specifically at this environment have been developed, but little performance information on each protocol and no realistic performance comparison between them is available. This paper presents the results of a detailed packet-level simulation comparing four multi-hop wireless ad hoc network routing protocols that cover a range of design choices: DSDV, TORA, DSR, and AODV. We have extended the ns-2 network simulator to accurately model the MAC and physical-layer behavior of the IEEE 802.11 wireless LAN standard, including a realistic wireless transmission channel model, and present the results of simulations of networks of 50 mobile nodes. 1
55|Cognitive networks|Abstract — This paper presents a definition and framework for a novel type of adaptive data network: the cognitive network. In a cognitive network, the collection of elements that make up the network observes network conditions and then, using prior knowledge gained from previous interactions with the network, plans, decides and acts on this information. Cognitive networks are different from other “intelligent ” communication technologies because these actions are taken with respect to the end-to-end goals of a data flow. In addition to the cognitive aspects of the network, a specification language is needed to translate the user’s end-to-end goals into a form understandable by the cognitive process. The cognitive network also depends on a Software Adaptable Network that has both an external interface accessible to the cognitive network and network status sensors. These devices are used to provide control and feedback. The paper concludes by presenting a simple case study to illustrate a cognitive network and its framework. I.
56|A Delay-Tolerant Network Architecture for Challenged Internets|The highly successful architecture and protocols of today’s Internet may operate poorly in environments characterized by very long delay paths and frequent network partitions. These problems are exacerbated by end nodes with limited power or memory resources. Often deployed in mobile and extreme environments lacking continuous connectivity, many such networks have their own specialized protocols, and do not utilize IP. To achieve interoperability between them, we propose a network architecture and application interface structured around optionally-reliable asynchronous message forwarding, with limited expectations of end-to-end connectivity and node resources. The architecture operates as an overlay above the transport layers of the networks it interconnects, and provides key services such as in-network data storage and retransmission, interoperable naming, authenticated forwarding and a coarse-grained class of service.
57|Energy-Efficient Computing for Wildlife Tracking: Design Tradeoffs and Early Experiences with ZebraNet|Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The  eld of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scienti  c and commercial purposes. This paper examines the research decisions and design tradeos that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.
58|Data mules: Modeling a three-tier architecture for sparse sensor networks|Abstract — This paper presents and analyzes an architecture that exploits the serendipitous movement of mobile agents in an environment to collect sensor data in sparse sensor networks. The mobile entities, called MULEs, pick up data from sensors when in close range, buffer it, and drop off the data to wired access points when in proximity. This leads to substantial power savings at the sensors as they only have to transmit over a short range. Detailed performance analysis is presented based on a simple model of the system incorporating key system variables such as number of MULEs, sensors and access points. The performance metrics observed are the data success rate (the fraction of generated data that reaches the access points) and the required buffer capacities on the sensors and the MULEs. The modeling along with simulation results can be used for further analysis and provide certain guidelines for deployment of such systems. I.
59|Epidemic routing for partially-connected ad hoc networks|Mobile ad hoc routing protocols allow nodes with wireless adaptors to communicate with one another without any pre-existing network infrastructure. Existing ad hoc routing protocols, while robust to rapidly changing network topology, assume the presence of a connected path from source to destination. Given power limitations, the advent of short-range wireless networks, and the wide physical conditions over which ad hoc networks must be deployed, in some scenarios it is likely that this assumption is invalid. In this work, we develop techniques to deliver messages in the case where there is never a connected path from source to destination or when a network partition exists at the time a message is originated. To this end, we introduce Epidemic Routing, where random pair-wise exchanges of messages among mobile hosts ensure eventual message delivery. The goals of Epidemic Routing are to: i) maximize message delivery rate, ii) minimize message latency, and iii) minimize the total resources consumed in message delivery. Through an implementation in the Monarch simulator, we show that Epidemic Routing achieves eventual delivery of 100 % of messages with reasonable aggregate resource consumption in a number of interesting scenarios. 1
60|On-demand Multipath Distance Vector Routing in Ad Hoc Networks|We develop an on-demand, multipath distance vector protocol for mobile ad hoc networks. Specifically, we propose multipath extensions to a well-studied single path routing protocol known as Ad hoc On-demand Distance Vector (AODV). The resulting protocol is referred to as Ad hoc Ondemand Multipath Distance Vector (AOMDV). The protocol computes multiple loop-free and link-disjoint paths. Loopfreedom is guaranteed by using a notion of &#034;advertised hopcount.&#034; Link-disjointness of multiple paths is achieved by using a particular property of flooding. Performance comparison of AOMDV with AODV using ns-2 simulations shows that AOMDV is able to achieve a remarkable improvement in the end-to-end delay --- often more than a factor of two, and is also able to reduce routing overheads by about 20%.  1 
61|An Analysis of Internet Content Delivery Systems|In the span of only a few years, the Internet has experienced an astronomical increase in the use of specialized content delivery systems, such as content delivery networks and peer-to-peer file sharing systems. Therefore, an understanding of content delivery on the Internet now requires a detailed understanding of how these systems are used in practice. This paper examines content delivery from the point of view of four content delivery systems: HTTP web traffic, the Akamai content delivery network, and Kazaa and Gnutella peer-to-peer file sharing traffic. We collected a trace of all incoming and outgoing network traffic at the University of Washington, a large university with over 60,000 students, faculty, and staff. From this trace, we isolated and characterized traffic belonging to each of these four delivery classes. Our results (1) quantify the rapidly increasing importance of new content delivery systems, particularly peerto-peer networks, (2) characterize the behavior of these systems from the perspectives of clients, objects, and servers, and (3) derive implications for caching in these systems. 1
62|Shortest-path and minimumdelay algorithms in networks with time-dependent edge-length|We consider in this paper the shortest-path problem in networks in which the delay (or weight) of the edges changes with time according to arbitrary functions. We present algorithms for finding the shortest-path and minimum-delay under various waiting constraints and investigate the properties of the derived path. We show that if departure time from the source node is unrestricted then a shortest path can be found that is simple and achieves a delay as short as the most unrestricted path. In the case of restricted transit, it is shown that there exist cases where the minimum delay is finite but the path that achieves it is infinite.
63|Trajectory Based Forwarding and Its Applications|Trajectory based forwarding (TBF) is a novel method to forward packets in a dense ad hoc network that  makes it possible to route a packet along a predefined curve. It is a generalization of source based routing  and Cartesian forwarding in that the trajectory is set by the source, but the forwarding decision is based on  the relationship to the trajectory rather than the final destination. The fundamental aspect of TBF is that it  decouples path naming from the actual path, thereby providing a common framework for applications such as:  flooding, unicast, multicast and multipath routing, and discovery in ad hoc networks. TBF requires that nodes  know their position relative to a coordinate system. While a global coordinate system a#orded by a system  such as GPS would be ideal, in this paper we propose Local Positioning System (LPS), a method that only  positions the nodes along the trajectory, by making use of other node capabilities, such as angle of arrival or  range estimations, compasses and accelerometers. We explore several forwarding strategies that are appropriate  for these node capabilities.
64|The Quickest Transshipment Problem|A dynamic network consists of a graph with capacities and transit times on its edges. The  quickest transshipment problem is defined by a dynamic network with several sources and sinks;  each source has a specified supply and each sink has a specified demand. The problem is to send  exactly the right amount of flow out of each source and into each sink in the minimum overall  time. Variations of 
65|Enabling disconnected transitive communication in mobile ad hoc networks|Recently, mobile ad hoc computing has attracted much attention in the research community. Different from most current wireless networks, mobile ad-hoc networks have no fixed infrastructure, all hosts are capable of movement, and the network is continuously reconstructed into multiple disconnected clusters. In such environments, connection is unstable, and communication is unpredictable. Current research mainly focuses on providing the same models of communication in this environment as in fixed networks, focusing on routing protocols for message delivery within connected subsets of hosts, also referred to as clusters. Although this kind of work is crucial, it does not address the possibility of communication across clusters, taking advantage of the movement of mobile hosts which themselves are able to carry messages from one cluster to another. In this paper, we propose a new model of communication model, Disconnected Transitive Communication (DTC), which focuses on cross-cluster communication, and provide the details of a routing protocol to enable it.
66|A Linear Programming Formulation of Flows over Time with Piecewise Constant Capacity and Transit Times| We present an algorithm to solve a deterministic form of a routing problem in delay tolerant networking, in which contact possibilities are known in advance. The algorithm starts with a finite set of contacts with time-varying capacities and transit delays. The output is an optimal schedule assigning messages to edges and times, that respects message priority and minimizes the overall delivery delay. The algorithm consists of two main ingredients: a discretization step in which the raw data provided by the contacts is used to obtain appropriate subdivisions of the relevant time intervals, and a linear program, a dynamic version of the classical multicommodity flow problem, in which transit times are piecewise constant, and where both edges and nodes are capacitated (in the case of edges, with piecewise constant capacities). In fact, we present two equivalent LP formulations, of which one is smaller and runs faster in CPLEX, a general purpose linear solver.  
67|An annotated overview of dynamic network flows| The need for more realistic network models led to the development of the dynamic network flow theory. In dynamic flow models it takes time for the flow to pass an arc, the flow can be delayed at nodes, and the network parameters, e.g., the arc capacities, can change in time. Surprisingly perhaps, despite being closer to reality, dynamic flow models have been overshadowed by the classical, static model. This is largely due to the fact that while very efficient solution methods exist for static flow problems, dynamic flow problems have proved to be more difficult to solve. Our purpose with this overview is to compensate for this eclipse and introduce dynamic flows to the interested reader. To this end, we present the main flow problems that can appear in a dynamic network, and review the literature for existing results about them. Our approach is solution oriented, as opposed to dealing with modelling issues. We intend to provide a survey that can be a first step for readers wondering whether a given dynamic network flow problem has been solved or not. Besides restating the problems, we also describe the main proposed solution methods. An additional feature of this paper is an annotated list of the most important references about the subject.
68|Tinydb: An acquisitional query processing system for sensor networks|We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. Categories and Subject Descriptors: H.2.3 [Database Management]: Languages—Query languages; H.2.4 [Database Management]: Systems—Distributed databases; query processing
69|Directed Diffusion: A scalable and robust communication paradigm for sensor networks|Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.
70|System architecture directions for networked sensors|Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world or spread throughout our environment. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances. 
71|ANALYSIS OF WIRELESS SENSOR NETWORKS FOR HABITAT MONITORING|  We provide an in-depth study of applying wireless sensor networks (WSNs) to real-world habitat monitoring. A set of system design requirements were developed that cover the hardware design of the nodes, the sensor network software, protective enclosures, and system architecture to meet the requirements of biologists. In the summer of 2002, 43 nodes were deployed on a small island off the coast of Maine streaming useful live data onto the web. Although researchers anticipate some challenges arising in real-world deployments of WSNs, many problems can only be discovered through experience. We present a set of experiences from a four month long deployment on a remote island. We analyze the environmental and node health data to evaluate system performance. The close integration of WSNs with their environment provides environmental data at densities previously impossible. We show that the sensor data is also useful for predicting system operation and network failures. Based on over one million 2 Polastre et. al. data readings, we analyze the node and network design and develop network reliability profiles and failure models.
73|The Cricket Location-Support System|This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, locationdependent applications. It allows applications running on mobile and static nodes to learn their physical location by using listeners that hear and analyze information from beacons spread throughout the building. Cricket is the result of several design goals, including user privacy, decentralized administration, network heterogeneity, and low cost. Rather than explicitly tracking user location, Cricket helps devices learn where they are and lets them decide whom to advertise this information to; it does not rely on any centralized management or control and there is no explicit coordination between beacons; it provides information to devices regardless of their type of network connectivity; and each Cricket device is made from off-the-shelf components and costs less than U.S. $10. We describe the randomized algorithm used by beacons to transmit information, the use of concurrent radio and ultrasonic signals to infer distance, the listener inference algorithms to overcome multipath and interference, and practical beacon configuration and positioning techniques that improve accuracy. Our experience with Cricket shows that several location-dependent applications such as in-building active maps and device control can be developed with little effort or manual configuration.  1 
74|The nesC language: A holistic approach to networked embedded systems|We present nesC, a programming language for networked embedded systems that represent a new design space for application developers. An example of a networked embedded system is a sensor network, which consists of (potentially) thousands of tiny, lowpower “motes, ” each of which execute concurrent, reactive programs that must operate with severe memory and power constraints. nesC’s contribution is to support the special needs of this domain by exposing a programming model that incorporates event-driven execution, a flexible concurrency model, and component-oriented application design. Restrictions on the programming model allow the nesC compiler to perform whole-program analyses, including data-race detection (which improves reliability) and aggressive function inlining (which reduces resource consumption). nesC has been used to implement TinyOS, a small operating system for sensor networks, as well as several significant sensor applications. nesC and TinyOS have been adopted by a large number of sensor network research groups, and our experience and evaluation of the language shows that it is effective at supporting the complex, concurrent programming style demanded by this new class of deeply networked systems.
75|NiagaraCQ: A Scalable Continuous Query System for Internet Databases|Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system’s performance and scalability. 1.
76|Timing-Sync Protocol for Sensor Networks|Wireless ad-hoc sensor networks have emerged as an interesting and important research area in the last few years. The applications envisioned for such networks require collaborative execution of a distributed task amongst a large set of sensor nodes. This is realized by exchanging messages that are timestamped using the local clocks on the nodes. Therefore, time synchronization becomes an indispensable piece of infrastructure in such systems. For years, protocols such as NTP have kept the clocks of networked systems in perfect synchrony. However, this new class of networks has a large density of nodes and very limited energy resource at every node; this leads to scalability requirements while limiting the resources that can be used to achieve them. A new approach to time synchronization is needed for sensor networks.
77|TelegraphCQ: Continuous Dataflow Processing for an Uncertan World|Increasingly pervasive networks are leading towards a world where data is constantly in motion. In such a world, conventional techniques for query processing, which were developed under the assumption of a far more static and predictable computational environment, will not be sufficient. Instead, query processors based on adaptive dataflow will be necessary. The Telegraph project has developed a suite of novel technologies for continuously adaptive query processing. The next generation Telegraph system, called TelegraphCQ, is focused on meeting the challenges that arise in handling large streams of continuous queries over high-volume, highly-variable data streams. In this paper, we describe the system architecture and its underlying technology, and report on our ongoing implementation effort, which leverages the PostgreSQL open source code base. We also discuss open issues and our research agenda.
78|The Cougar Approach to In-Network Query Processing in Sensor Networks|The widespread distribution and availability of smallscale sensors, actuators, and embedded processors is transforming the physical world into a computing platform. One such example is a sensor network consisting of a large number of sensor nodes that combine physical sensing capabilities such as temperature, light, or seismic sensors with networking and computation capabilities. Applications range from environmental control, warehouse inventory, and health care to military environments. Existing sensor networks assume that the sensors are preprogrammed and send data to a central frontend where the data is aggregated and stored for offline querying and analysis. This approach has two major drawbacks. First, the user cannot change the behavior of the system on the fly. Second, conservation of battery power is a major design factor, but a central system cannot make use of in-network programming, which trades costly communication for cheap local computation.
79|A Transmission Control Scheme for Media Access in Sensor Networks|We study the problem of media access control in the novel regime of sensor networks, where unique application behavior and tight constraints in computation power, storage, energy resources, and radio technology have shaped this design space to be very different from that found in traditional mobile computing regime. Media access control in sensor networks must not only be energy efficient but should also allow fair bandwidth allocation to the infrastructure for all nodes in a multihop network. We propose an adaptive rate control mechanism aiming to support these two goals and find that such a scheme is most effective in achieving our fairness goal while being energy effcient for both low and high duty cycle of network traffic.
80|Routing indices for peer-to-peer systems|Finding information in a peer-to-peer system currently requires either a costly and vulnerable central index, or ooding the network with queries. In this paper we introduce the concept of Routing Indices (RIs), which allow nodes to forward queries to neighbors that are more likely to have answers. If a node cannot answer a query, it forwards the query to a subset of its neighbors, based on its local RI, rather than by selecting neighbors at random or by ooding the network by forwarding the query to all neighbors. We present three RI schemes: the compound, the hop-count, and the exponential routing indices. We evaluate their performance via simulations, and nd that RIs can improve performance by one or two orders of magnitude vs. a ooding-based system, and by up to 100 % vs. a random forwarding system. We also discuss the tradeo s between the di erent RIschemes and highlight the e ects of key design variables on system performance.
81|Eddies: Continuously Adaptive Query Processing|In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.  In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry  during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates prom...
82|The Gamma database machine project|This paper describes the design of the Gamma database machine and the techniques employed in its implementation. Gamma is a relational database machine currently operating on an Intel iPSC/2 hypercube with 32 processors and 32 disk drives. Gamma employs three key technical ideas which enable the architecture to be scaled to 100s of processors. First, all relations are horizontally partitioned across multiple disk drives enabling relations to be scanned in parallel. Second, novel parallel algorithms based on hashing are used to implement the complex relational operators such as join and aggregate functions. Third, dataflow scheduling techniques are used to coordinate multioperator queries. By using these techniques it is possible to control the execution of very complex queries with minimal coordination- a necessity for configurations involving a very large number of processors. In addition to describing the design of the Gamma software, a thorough performance evaluation of the iPSC/2 hypercube version of Gamma is also presented. In addition to measuring the effect of relation size and indices on the response time for selection, join, aggregation, and update queries, we also analyze the performance of Gamma relative to the number of processors employed when the sizes of the input relations are kept constant (speedup) and when the sizes of the input relations are increased proportionally to the number of processors (scaleup). The speedup results obtained for both selection and join queries are linear; thus, doubling the number of processors
84|An Adaptive Query Execution System for Data Integration|Query processing in data integration occurs over networkbound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the  Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila  to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that the Tukwila architecture extends previous innovations in adaptive execution (such as...
85|Approximate Query Processing Using Wavelets|Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today’s decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing
86|Optimization of nonrecursive queries|State-of-the-art optimization approaches for relational database systems, e.g., those used in systems such as OBE, SQL/DS, and commercial INGRES. when used for queries in non-traditional database applications, suffer from two problems. First, the time complexity of their optimization algorithms, being combinatoric, is exponential in the number of relations to be joined in the query. Their cost is therefore prohibitive in situa-tions such as deductive databases and logic oriented languages for knowledge bases, where hundreds of joins may be required. The second problem with the traditional approaches is that, al-beit effective in their specific domain, it is not clear whether they can be generalized to different scenarios (e.g. parallel evalua-tion) since they lack a formal model to define the assumptions and critical factors on which their valiclity depends. This paper
87|Extensible/Rule Based Query Rewrite Optimization in Starburst|This paper describes the Query Rewrite facility of the Starburst extensible database system, a novel phase of query optimization. We present a suite of rewrite rules used in Starburst to transform queries into equivalent queries for faster execution, and also describe the production rule engine which is used by Starburst to choose and execute these rules. Examples are provided demonstrating that these Query Rewrite transformations lead to query execution time improvements of orders of magnitude, suggesting that Query Rewrite in general --- and these rewrite rules in particular --- are an essential step in query optimization for modern database systems.  1 Introduction  In traditional database systems, query optimization typically consists of a single phase of processing in which access methods, join orders and join methods are chosen to provide an efficient plan for executing a user&#039;s declarative query. We refer to this phase as plan optimization. In this paper we present a distinct ph...
88|Query Processing, Approximation, and Resource Management In a Data Stream Management System|This paper describes our ongoing work developing the Stanford Stream Data Manager (STREAM), a system for executing continuous queries over multiple continuous data streams. The STREAM system supports a declarative query language, and it copes with high data rates and query workloads by providing approximate answers when resources are limited. This paper describes specific contributions made so far and enumerates our next steps in developing a general-purpose Data Stream Management System.
89|Querying in highly mobile distributed environments|New challenges to the database field brought about by the rapidly growing area of wireless personal communication are presented. Preliminary solutions to two of the major problems, control of update volume and integration of query processing and data acquisition, are discussed along with the simulation results. 1
90|Adaptive Query Processing: Technology in Evolution|As query engines are scaled and federated, they must cope with highly unpredictable and changeable environments. In the Telegraph project, we are attempting to architect and implement a continuously adaptive query engine suitable for global-area systems, massive parallelism, and sensor networks. To set the stage for our research, we present a survey of prior work on adaptive query processing, focusing on three characterizations of adaptivity: the frequency of adaptivity, the effects of adaptivity, and the extent of adaptivity. Given this survey, we sketch directions for research in the Telegraph project.  
91|Database System Issues in Nomadic Computing|Mobile computers and wireless networks are emerging technologies that will soon be available to a  wide variety of computer users. Unlike earlier generations of laptop computers, the new generation  of mobile computers can be an integrated part of a distributed computing environment, one in  which users change physical location frequently. The result is a new computing paradigm, nomadic  computing. This paradigm will affect the design of much of our current systems software, including  that of database systems.  This paper discusses in some detail the impact of nomadic computing on a number of traditional  database system concepts. In particular, we point out how the reliance on short-lived batteries  changes the cost assumptions underlying query processing. In these systems, power consumption  competes with resource utilization in the definition of cost metrics. We also discuss how the likelihood  of temporary disconnection forces consideration of alternative transaction processing pr...
92|Optimization techniques for queries with expensive methods|Object-Relational database management systems allow knowledgeable users to de ne new data types, as well as new methods (operators) for the types. This exibility produces an attendant complexity, which must be handled in new ways for an Object-Relational database management system to be e cient. In this paper we study techniques for optimizing queries that contain time-consuming methods. The focus of traditional query optimizers has been on the choice of join methods and orders; selections have been handled by \pushdown &#034; rules. These rules apply selections in an arbitrary order before as many joins as possible, using the assumption that selection takes no time. However, users of Object-Relational systems can embed complex methods in selections. Thus selections may take signi cant amounts of time, and the query optimization model must be enhanced. In this paper, we carefully de ne a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial Object-Relational database management system Illustra, and discuss practical issues that a ect our earlier assumptions. We compare Predicate Migration to a variety of simpler optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we presentmaybe useful for constrained workloads.
93|Cost-based Query Scrambling for Initial Delays|Remote data access from disparate sources across a wide-area network such as the Internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites. Traditional, static, query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays. Query scrambling has been proposed to address this problem. Scrambling modifies query execution plans on-the-fly when delays are encountered during runtime. In its original formulation, scrambling was based on simple heuristics, which although providing good performance in many cases, were also shown to be susceptible to problems resulting from bad scrambling decisions. In this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices. We propose three different approaches to using query optimization for scramblin...
94|Adaptive Parallel Aggregation Algorithms|Aggregation and duplicate removal are common in SQL queries. However, in the parallel query processing literature, aggregate processing has received surprisingly little attention; furthermore, for each of the traditional parallel aggregation algorithms, there is a range of grouping selectivities where the algorithm performs poorly. In this work, we propose new algorithms that dynamically adapt, at query evaluation time, in response to observed grouping selectivities. Performance analysis via analytical modeling and an implementation on a workstation-cluster shows that the proposed algorithms are able to perform well for all grouping selectivities. Finally, we study the effect of data skew and show that for certain data sets the proposed algorithms can even outperform the best of traditional approaches. 1 Introduction  SQL queries are replete with aggregate and duplicate elimination operations. One measure of the perceived importance of aggregation is that in the proposed TPCD benchmark...
96|Bluetooth and Sensor Networks: A Reality Check|The current generation of sensor nodes rely on commodity components. The choice of the radio is particularly important as it impacts not only energy consumption but also software design (e.g., network self-assembly, multihop routing and in-network processing). Bluetooth is one of the most popular commodity radios for wireless devices. As a representative of the frequency hopping spread spectrum radios, it is a natural alternative to broadcast radios in the context of sensor networks. The question is whether Bluetooth can be a viable alternative in practice. In this paper, we report our experience using Bluetooth for the sensor network regime. We describe our tiny Bluetooth stack that allows TinyOS applications to run on Bluetooth-based sensor nodes, we present a multihop network assembly procedure that leverages Bluetooth&#039;s device discovery protocol, and we discuss how Bluetooth favorably impacts in-network query processing. Our results show that despite obvious limitations the Bluetooth sensor nodes we studied exhibit interesting properties, such as a good energy per bit sent ratio. This reality check underlies the limitations and some promises of Bluetooth for the sensor network regime.
97|Aggregation and Relevance in Deductive Databases|In this paper we present a technique to optimize queries on deductive databases that use aggregate operations such as min, max, and “largest Ic values. ” Our approach is based on an extended notion of relevance of facts to queries that takes aggregate operations into account. The approach has two parts: a rewriting part that labels predica.tes with “ag-gregate selections, ” and an evaluat,ion part. t.hat, makes use of “aggregate selections ” to detect that facts are irrelevant and discards them. The rewriting complements standard rewrit-ing algorithms like Magic sets, and the evaluation essentially refines Semi-Naive evaluation. 1
98|Query Optimization In Compressed Database Systems|Over the lastd ecad es, improvements in CPU speed have outpaced improvements in main memory and d isk access rates by ord ers of magnitud , enabling the use ofd ata compression techniques to improve the performance ofd atabase systems. Previous work d scribes the benefits of compression for numerical attributes, whered8 a is stored in compressed  format ond isk. Despite the abund3&amp; e of stringvalued  attributes in relational schemas there is little work on compression for string attributes in ad atabase context. Moreover, none of the previous work suitablyad2 esses the role of the query optimizer: During query execution, dD a is either eagerly d compressed when it is read into main memory, or dD a lazily stays compressed in main memory and is d compressed ond emand only. In this paper, we present an e#ective approach for dD abase compression based on lightweight, attribute-level compression techniques. We propose a Hierarchical ictionary Encod  ing strategy that intelligently selects the most e#ective compression method for string-valued attributes. We show that eager and lazy d compression strategies prod1 e suboptimal plans for queries involving compressed string attributes. We then formalize the problem of compressionaware query optimizationand propose one provably optimal  and two fast heuristic algorithms for selecting a query plan for relational schemas with compressed attributes; our algorithms can easily be integrated into existing cost-based query optimizers. Experiments using TPC-Hd atad emonstrate the impact of our string compression method s and show the importance of compression-aware query optimization. Our approach results in up to an or d r speed up over existing approaches.  1. 
99|The Design and Implementation of the Ariel Active Database Rule System|This paper describes the design and implementation of the Ariel DBMS and it&#039;s tightlycoupled forward-chaining rule system. The query language of Ariel is a subset of POSTQUEL, extended with a new production-rule sublanguage. Ariel supports traditional relational database query and update operations efficiently, using a System Rlike query processing strategy. In addition, the Ariel rule system is tightly coupled with query and update processing. Ariel rules can have conditions based on a mix of patterns, events, and transitions. For testing rule conditions, Ariel makes use of a discrimination network composed of a special data structure for testing single-relation selection conditions efficiently, and a modified version of the TREAT algorithm, called A-TREAT, for testing join conditions. The key modification to TREAT (which could also be used in the Rete algorithm) is the use of virtual ff-memory nodes which save storage since they contain only the predicate associated with the memory n...
100|DOMINO: Databases fOr MovINg Objects tracking|Consider a database that represents information about moving objects and their location. For example, for a database representing the location of taxi-cabs a typical query may be: retrieve the free cabs that are currently within 1 mile of 33 N. Michigan Ave., Chicago (to pick-up a customer); or for a trucking company database a typical query may be: retrieve the trucks that are currently within 1 mile of truck ABT312 (which needs assistance); or for a database representing the current location of objects in a battlefield a typical query may be: retrieve the friendly helicopters that are in a given region, or, retrieve the friendly helicopters that are expected to enter the region within the next 10 minutes. The queries may originate from the moving objects, or from stationary users. We will refer to applications with the above characteristics as moving-objects-database (MOD) applications, and to queries as the ones mentioned above as MOD queries.
101|From ethnography to design in a vineyard, in|This paper summarizes the process from ethnographic study of a vineyard to concept development and interaction design for a ubiquitous computing solution. It provides examples of vineyard interfaces and the lessons learned that could be generally applied to the interaction design of ubiquitous systems. These are: design for multiple perspectives on data, design for multiple access points, and design for varying levels of attention.
102|Query Optimization in Mobile Environments|We consider the issue of optimizing queries for a distributed processing in mobile environment. An interesting characteristic of mobile machines is that they depend on battery as a source of energy which may not be substantial enough. Hence, the appropriate optimization criterion in a mobile environment considers both resource utilization and energy consumption at the mobile client. In this scenario, the optimal plan for a query depends on the residual battery level of the mobile client and the load at the server. We approach this problem by compiling a query into a sequence of candidate plans, such that for any state of the client-server system, the optimal plan is one of the candidate plans. A general solution is proposed by adapting the partial order dynamic programming search algorithm [17, 18] (p.o dp) such that the coverset of the query is the set of candidate plans. We propose two novel algorithms, namely, the linear combinations algorithm and the linearset algorithm (referred t...
103|Online Dynamic Reordering|We present a mechanism for providing dynamic user control during long running, data-intensive operations. Users  can see partial results and dynamically direct the processing to suit their interests, thereby enhancing the interactivity  in several contexts such as online aggregation and large-scale spreadsheets.  In this paper, we develop a pipelining, dynamically tunable reorder operator for providing user control. Users  specify preferences for different data items based on prior results, so that data of interest is prioritized for early  processing. The reordering mechanism is efficient and non-blocking, and can be used over arbitrary data streams  from files and indexes, as well as continuous data feeds. We also investigate several policies for the reordering  based on the performance goals of various typical applications. We present performance results for reordering in the  context of an Online Aggregation implementation in Informix, and in the context of sorting and scrolling in...
104|The interdisciplinary study of coordination|This survey characterizes an emerging research area, sometimes called coordination theory, that focuses on the interdisciplinary study of coordination. Research in this area uses and extends ideas about coordination from disciplines such as computer science, organization theory, operations research, economics, linguistics, and psychology. A key insight of the framework presented here is that coordination can be seen as the process of managing dependencies among activities. Further progress, therefore, should be possible by characterizing different kinds of dependencies and identifying the coordination processes that can be used to manage them. A variety of processes are analyzed from this perspective, and commonalities across disciplines are identified. Processes analyzed include those for managing shared resources, producer/consumer relationships, simultaneity constraints, and tank/subtask dependencies. Section 3 summarizes ways of applying a coordination perspective in three different domains: (1) understanding the effects of information technology on human organizations and markets, (2) designing cooperative work tools, and (3) designing distributed and parallel computer systems. In the final section, elements of a research
105|Theory of the Firm: Managerial Behavior, Agency Costs and Ownership Structure|This paper integrates elements from the theory of agency, the theory of property rights and the theory of finance to develop a theory of the ownership structure of the firm. We define the concept of agency costs, show its relationship to the ‘separation and control’ issue, investigate the nature of the agency costs generated by the existence of debt and outside equity, demonstrate who bears costs and why, and investigate the Pareto optimality of their existence. We also provide a new definition of the firm, and show how our analysis of the factors influencing the creation and issuance of debt and equity claims is a special case of the supply side of the completeness of markets problem. 
106|Electronic Markets and Electronic Hierarchies|This paper analyzes the fundamental changes in market structures that may result from the increasing use of information technology. First, an analytic framework is presented and its usefulness is demonstrated in explaining several major historical changes in American business structures. Then, the framework is used to help explain how electronic markets and electronic hierarchies will allow closer integration of adjacent steps in the value added chains of our economy. The most surprising prediction is that information technology will lead to an overall shift toward proportionately more coordination by markets rather than by internal decisions within firms. Finally, several examples of companies where these changes are already occurring are used to illustrate the likely paths by which new market structures will evolve and the ways in which individual companies can take advantage of these changes.
107|Frameworks for Cooperation in Distributed Problem Solving|Abstract — Two forms of cooperation in distributed problem solving are considered: task-sharing and result-sharing. In the former, nodes assist each other by sharing the computational load for the execution of subtasks of the overall problem. In the latter, nodes assist each other by sharing partial results which are based on somewhat different perspectives on the overall problem. Different perspectives arise because the nodes use different knowledge sources (KS’s) (e.g., syntax versus acoustics in the case of a speech-understanding system) or different data (e.g., data that is sensed at different locations in the case of a distributed sensing system). Particular attention is given to control and to internode communication for the two forms of cooperation. For each, the basic methodology is presented and systems in which it has been used are described. The two forms are then compared and the types of applications for which they are suitable are considered. I. DISTRIBUTED PROBLEM SOLVING
108|Offices Are Open Systems|This paper is intended as a contribution to analysis of the implications of viewing offices as open systems. It takes a prescriptive stance on how to estalish the informationprocessing foundations for taking action and making decisions in office work from an open systems perspective. We propose due process as a central activity in organizational information processing. Computer systems are beginning to play important roles in mediating the ongoing activities of organizations. We expect that these roles will gradually increase in importance as computer systems take on more of the authority and responsibility for ongoing activities. At the same time, we expect computer systems to acquire more of the characteristics and structure of human organizations.
109|Experiments with Oval: A Radically Tailorable Tool for Cooperative Work|This article describes a series of tests of the generality of a “radically tailorable” tool for cooperative work. Users of this system can create applications by combining and modifying four kinds of building blocks: objects, uiezus, agents, and links. We found that user-level tailoring of these primitives can provide most of the functionality found in well-known cooperative work systems such as gIBIS, Coordinator, Lotus Notes, and Information Lens. These primitives, therefore, appear to provide an elementary “tailoring language” out of which a wide variety of integrated information management and collaboration applications can be constructed by end users.
110|A microeconomic approach to optimal resource allocation in distributed computer systems|Abstract-Decentralized algorithms are examined for opti-mally distributing a divisible resource in a distributed computer system. In order to study this problem in a specific context, we consider the problem of optimal file allocation. In this case, the optimization criteria include both the communication cost and average processing delay associated with a file access. Our algorithms have their origins in the field of mathematical economics. They are shown to have several attractive properties, including their simplicity and distributed nature, the computation of feasible and increasingly better resource allocations as the result of each iteration, and in the case of file allocation, rapid convergence. Conditions are formally derived under which the algorithms are guaranteed to converge and their convergence behavior is additionally examined through simulation. Index Terms-Distributed algorithms, distributed systems, file allocation, resource allocation, optimization I.
111|Social Analyses of Computing: Theoretical Perspectives in Recent|Recent empirical studies of computing use in organizations and in public life are examined. The roles of computer technologies in the workplace, in decision making, in altering power relationships, and in influencing personal privacy are examined. In addition, studies that examine the social accountability of computing arrangements to broad publics are reviewed. All studies of computing in social ife make important assumptions about the social world in which computing is embedded. Two broad perspectives are contrasted. Systems rationalism, a collection of approaches including management science, managerial rationalism, and the systems approach, is found to be most helpful in stable settings, when there is considerable consensus over important social values. Segmented-institutionalist nalyses, which assume social conflict rather than consensus, are particularly powerful as the social world of computing use becomes more dynamic and as a wider variety of groups is involved.
112|Mathematische Modelle|The quest for runware: on compositional, executable and intuitive
113|Organizations|Proceedings DEFORM’06 is associated to BMVC’06, the 7th British Machine Vision Conference Preface These are the proceedings of DEFORM’06, the Workshop on Image Registration in Deformable Environments, associated to BMVC’06, the 17th British Machine Vision Conference, held in Edinburgh, UK, in
114|Inter-Organization Computer Networks: Indications of Shifts in Interdependence|As firms increasingly adopt inter-organization computer networks (IONS) to improve coordination, researchers must be concerned about the long term impact of IONS on organizational relationships. This paper reports on an exploratory study of the use of IONS in design and manufacturing activities in the semiconductor industry. We identify the potential interactions between firms that can be facilitated by IONS, and focus on the implications for customer and producer interdependence. Our analysis suggests that the long term impacts of IONS are not technologically determined, and that their use ought to be regarded differently than those of other media. 
115|MULTILISP: a language for concurrent symbolic computation|Multilisp is a version of the Lisp dialect Scheme extended with constructs for parallel execution. Like Scheme, Multilisp is oriented toward symbolic computation. Unlike some parallel programming languages, Multilisp incorporates constructs for causing side effects and for explicitly introducing parallelism. The potential complexity of dealing with side effects in a parallel context is mitigated by the nature of the parallelism constructs and by support for abstract data types: a recommended Multilisp programming style is presented which, if followed, should lead to highly parallel, easily understandable programs. Multilisp is being implemented on the 32-processor Concert multiprocessor; however, it is ulti-mately intended for use on larger multiprocessors. The current implementation, called Concert Multilisp, is complete enough to run the Multilisp compiler itself and has been run on Concert prototypes including up to eight processors. Concert Multilisp uses novel techniques for task scheduling and garbage collection. The task scheduler helps control excessive resource utilization by means of an unfair scheduling policy; the garbage collector uses a multiprocessor algorithm based on the incremental garbage collector of Baker.
116|A Methodology for Implementing Highly Concurrent Data Objects| A concurrent object is a data structure shared by concurrent processes. Conventional techniques for implementing concurrent objects typically rely on critical sections: ensuring that only one process at a time can operate on the object. Nevertheless, critical sections are poorly suited for asynchronous systems: if one process is halted or delayed in a critical section, other, nonfaulty processes will be unable to progress. By contrast, a concurrent object implementation is lock free if it always guarantees that some process will complete an operation in a finite number of steps, and it is wait free if it guarantees that each process will complete an operation in a finite number of steps. This paper proposes a new methodology for constructing lock-free and wait-free implementations of concurrent objects. The object’s representation and operations are written as stylized sequential programs, with no explicit synchronization. Each sequential operation is automatically transformed into a lock-free or wait-free operation using novel synchronization and memory management algorithms. These algorithms are presented for a multiple instruction/multiple data (MIMD) architecture in which n processes communicate by applying atomic read, wrzte, load_linked, and store_conditional operations to a shared memory.
117|The Amber System: Parallel Programming on a Network of Multiprocessors|Microprocessor-based shared-memory multiprocessors are becoming widely available and promise to provide cost-effective high-performance computing. This paper describes a programming system called Amber which permits a single application program to use a homogeneous network of multiprocessors in a uniform way, making the network appear to the application as an integrated, non-uniform memory access, shared-memory multiprocessor. This simplifies the development of applications and allows compute-intensive parallel programs to effectively harness the potential of multiple nodes. Amber programs are written using an object-oriented subset of the C++ programming language, supplemented with primitives for managing concurrency and distribution. Amber provides a network-wide shared-object virtual memory in which coherence is provided by hardware means for locally-executing threads, and by software means for remote accesses. Amber runs on top of the Topaz operating system on a network of DEC SRC ...
118|Experience with Processes and Monitors in Mesa|The use of monitors for describing concurrency has been much discussed in the literature. When monitors are used in real systems of any size, however, a number of problems arise which have not been adequately dealt with: the semantics of nested monitor calls; the various ways of defining the meaning of WAIT; priority scheduling; handling of timeouts, aborts and other exceptional conditions; interactions with process creation and destruction; monitoring large numbers of small objects. These problems are addressed by the facilities described here for concurrent programming in Mesa. Experience with several substantial applications gives us some confidence in the validity of our solutions.
119|The Performance Implications of Thread Management Alternatives for Shared-Memory Multiprocessors|Threads (“lightweight ” processes) have become. a common element of new languages and operating systems. This paper examines the performance implications of several data structure and algorithm alternatives for thread management in shared-memory multiprocessors. Both experimental measurements and analytical model projections are presented For applications with fine-grained parallelism, small differences in thread management are shown to have significant performance impact, often posing a tradeoff between throughput and latency. Per-processor data structures can be used to improve throughput, and in some circumstances to avoid locking, improving latency as well. The method used by processors to queue for locks is also shown to affect performance significantly. Normal methods of critical resource waiting can substantially degrade performance with moderate numbers of waiting processors. We present an Ethernet-style backoff algo rithm that largely eliminates~tiis effect. 1.
120|First-Class User-Level Threads|It is often desirable, for reasons of clarity, portability, and efficiency, to write parallel programs in which the number of processes is independent of the number of available processors. Several modern operating systems support more than one process in an address space, but the overhead of creating and synchronizing kernel processes can be high. Many runtime environments implement lightweight processes (threads) in user space, but this approach usually results in second-class status for threads, making it difficult or impossible to perform scheduling operations at appropriate times (e.g. when the current thread blocks in the kernel). In addition, a lack of common assumptions may also make it difficult for parallel programs or library routines that use dissimilar thread packages to communicate with each other, or to synchronize access to shared data. We describe a set of kernel mechanisms and conventions designed to accord first-class status to user-level threads, allowing them to be used in any reasonable way that traditional kernel-provided processes can be used, while leaving the details of their implementation to userlevel code. The key features of our approach are (1) shared memory for asynchronous communication between the kernel and the user, (2) software interrupts for events that might require action on the part of a user-level scheduler, and (3) a scheduler interface convention that facilitates interactions in user space between dissimilar kinds of threads. We have incorporated these mechanisms in the Psyche parallel operating system, and have used them to implement several different kinds of user-level threads. We argue for our approach in terms of both flexibility and performance.
121|Empirical Studies of Competitive Spinning for a Shared-Memory Multiprocessor|A common operation in multiprocessor programs is acquiring a lock to protect access to shared data. Typically, the requesting thread is blocked if the lock it needs is held by another thread. The cost of blocking one thread and activating another can be a substantial part of program execution time. Alternatively, the thread could spin until the lock is free, or spin for a while and then block. This may avoid context-switch overhead, but processor cycles may be wasted in unproductive spinning. This paper studies seven strategies for determining whether and how long to spin before blocking. Of particular interest are competitive strategies, for which the performance can be shown to be no worse than some constant factor times an optimal off-line strategy. The performance of five competitive strategies is compared with that of always blocking, always spinning, or using the optimal off-line algorithm. Measurements of lock-waiting time distributions for five parallel programs were used to co...
122|Synchronization Primitives for a Multiprocessor: A Formal Specification|Formal specifications of operating system interfaces can be a useful part of their documentation. We illustrate this by documenting the Threads synchronization primitives of the Taos operating system. We start with an informal description, present a way to formally specify interfaces in concurrent systems, and then give a formal specification of the synchronization primitives. We briefly discuss both the implementation and what we have learned from using the specification for more than a year. Our main conclusion is that programmers untrained in reading formal specifications have found this one helpful in getting their work done. iii  Introduction  The careful documentation of interfaces is an important step in the production of software upon which other software is to be built. If people are to use software without having to understand its implementation, documentation must convey semantic as well as syntactic information. When the software involves concurrency, adequate documentatio...
123|Multi-model parallel programming in Psyche|Many different parallel programming models, including lightweight processes that communicate with shared memory and heavyweight processes that communicate with messages, have been used to implement parallel applications. Unfortunately, operating systems and languages designed for parallel programming typically support only one model. Multi-model parallel programming is the simultaneous use of several different models, both across programs and within a single program. This paper describes multi-model parallel programming in the Psyche multiprocessor operating system. We explain why multi-model programming is desirable and present an operating system interface designed to support it. Through a series of three examples, we illustrate how the Psyche operating system supports different models of parallelism and how the different models are able to interact. 1.
124|Taming the Underlying Challenges of Reliable Multihop Routing in Sensor Networks|The dynamic and lossy nature of wireless communication poses major challenges to reliable, self-organizing multihop networks. These non-ideal characteristics are more problematic with the primitive, low-power radio transceivers found in sensor networks, and raise new issues that routing protocols must address. Link connectivity statistics should be captured dynamically through an efficient yet adaptive link estimator and routing decisions should exploit such connectivity statistics to achieve reliability. Link status and routing information must be maintained in a neighborhood table with constant space regardless of cell density. We study and evaluate link estimator, neighborhood table management, and reliable routing protocol techniques. We focus on a many-to-one, periodic data collection workload. We narrow the design space through evaluations on large-scale, high-level simulations to 50-node, in-depth empirical experiments. The most effective solution uses a simple time averaged EWMA estimator, frequency based table management, and cost-based routing.
125|Dynamic source routing in ad hoc wireless networks|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any established infrastructure or centralized administration. In such an environment, it may be necessary for one mobile host to enlist the aid of other hosts in forwarding a packet to its destination, due to the limited range of each mobile host’s wireless transmissions. This paper presents a protocol for routing in ad hoc networks that uses dynamic source routing. The protocol adapts quickly to routing changes when host movement is frequent, yet requires little or no overhead during periods in which hosts move less frequently. Based on results from a packet-level simulation of mobile hosts operating in an ad hoc network, the protocol performs well over a variety of environmental conditions such as host density and movement rates. For all but the highest rates of host movement simulated, the overhead of the protocol is quite low, falling to just 1 % of total data packets transmitted for moderate movement rates in a network of 24 mobile hosts. In all cases, the difference in length between the routes used and the optimal route lengths is negligible, and in most cases, route lengths are on average within a factor of 1.01 of optimal. 1.
126|Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers  (1994) |An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize adhoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.  
128|Topology Control of Multihop Wireless Networks using Transmit Power Adjustment| We consider the problem of adjusting the transmit powers of nodes in a multihop wireless network (also called an ad hoc network) to create a desired topology. We formulate it as a constrained optimization problem with two constraints- connectivity and biconnectivity, and one optimization objective- maximum power used. We present two centralized algorithms for use in static networks, and prove their optimality. For mobile networks, we present two distributed heuristics that adaptively adjust node transmit powers in response to topological changes and attempt to maintain a connected topology using minimum power. We analyze the throughput, delay, and power consumption of our algorithms using a prototype software implementation, an emulation of a power-controllable radio, and a detailed channel model. Our results show that the performance of multihop wireless networks in practice can be substantially increased with topology control.  
129|Approximate Frequency Counts over Data Streams|We present algorithms for computing frequency counts exceeding a user-specified threshold over data streams. Our algorithms are simple and have provably small memory footprints. Although the output is approximate, the error is guaranteed not to exceed a user-specified parameter. Our algorithms can easily be deployed for streams of singleton items like those found in IP network monitoring. We can also handle streams of variable sized sets of items exemplified by a sequence of market basket transactions at a retail store. For such streams, we describe an optimized implementation to compute frequent itemsets in a single pass.
130|Wake on Wireless: An Event Driven Energy Saving Strategy for Battery Operated Devices|The demand for an all-in-one phone with integrated personal information management and data access capabilities is beginning to accelerate. While personal digital assistants (PDAs) with built-in cellular, WiFi, and Voice-Over-IP technologies have the ability to serve these needs in a single package, the rate at which energy is consumed by PDA-based phones is very high. Thus, these devices can quickly drain their own batteries and become useless to their owner. In this paper, we introduce a technique to increase the battery lifetime of a PDA-based phone by reducing its idle power, the power a device consumes in a “standby ” state. To reduce the idle power, we essentially shut down the device and its wireless network card when the device is not being used—the device is powered only when an incoming call is received. Using this technique, we can increase the battery lifetime by up to 115%. In this paper, we describe the design of our “wake-on-wireless ” energy-saving strategy and the prototype device we implemented. To evaluate our technique, we compare it with alternative approaches. Our results show that our technique can provide a significant lifetime improvement over other technologies. Categories and Subject Descriptors
131|Maintaining Stream Statistics over Sliding Windows (Extended Abstract)  (2002) |Mayur Datar  Aristides Gionis  y  Piotr Indyk  z  Rajeev Motwani  x  Abstract  We consider the problem of maintaining aggregates and statistics over data streams, with respect to the last N data elements seen so far. We refer to this model as the sliding window model. We consider the following basic problem: Given a stream of bits, maintain a count of the number of 1&#039;s in the last N elements seen from the stream. We show that using O(  1  ffl log  2  N) bits of memory, we can estimate the number of 1&#039;s to within a factor of 1 + ffl. We also give a matching lower bound of \Omega\Gamma  1  ffl log  2  N) memory bits for any deterministic or randomized algorithms. We extend our scheme to maintain the sum of the last N positive integers. We provide matching upper and lower bounds for this more general problem as well. We apply our techniques to obtain efficient algorithms for the Lp norms (for p 2 [1; 2]) of vectors under the sliding window model. Using the algorithm for the basic counting problem, one can adapt many other techniques to work for the sliding window model, with a multiplicative overhead of O(  1  ffl log N) in memory and a 1 + ffl factor loss in accuracy. These include maintaining approximate histograms, hash tables, and statistics or aggregates such as sum and averages.
132|Frequency estimation of internet packet streams with limited space| We consider a router on the Internet analyzing the statistical properties of a TCP/IP packet stream. A fundamental difficulty with measuring traffic behavior on the Internet is that there is simply too much data to be recorded for later analysis, on the order of gigabytes a second. As a result, network routers can collect only relatively few statistics about the data. The central problem addressed here is to use the limited memory of routers to determine essential features of the network traffic stream. A particularly difficult and representative subproblem is to determine the top k categories to which the most packets belong, for a desired value of k and for a given notion of categorization such as the destination IP address. We present an algorithm that deterministically finds (in particular) all categories having a frequency above 1/(m + 1) using m counters, which we prove is best possible in the worst case. We also present a sampling-based algorithm for the case that packet categories follow an arbitrary distribution, but their order over time is permuted uniformly at random. Under this model, our algorithm identifies flows above a frequency threshold of roughly 1 /  v nm with high probability, where m is the number of counters and n is the number of packets observed. This guarantee is not far off from the ideal of identifying all flows (probability 1/n), and we prove that it is best possible up to a logarithmic factor. We show that the algorithm ranks the identified flows according to frequency within any desired constant factor of accuracy. 
133|Real-World Experiences with an Interactive Ad Hoc Sensor Network|While it is often suggested that moderate-scale ad hoc sensor networks are a promising approach to solving real-world problems, most evaluations of sensor network protocols have focused on simulation, rather than realworld, experiments. In addition, most experimental results have been obtained in limited scale. This paper describes a practical application of moderate-scale ad hoc sensor networks. We explore several techniques for reducing packet loss, including quality-based routing and passive acknowledgment, and present an empirical evaluation of the effect of these techniques on packet loss and data freshness.
134|Eigrp - A Fast Routing Protocol Based On Distance Vectors|Early routing protocols were based on distance vectors; they were very simple and easy to implement  but had the severe drawbacks of counting to infinity and routing loops. These problems were reduced  using such techniques as split horizon and hold-downs; however, for these techniques to work in practice,  long convergence times are introduced. Routing protocols based on link states have been implemented  to address the problem of slow convergence in distance-vector protocols, but they add complexity in  configuration and troubleshooting. We present a new distance-vector protocol that converges as quickly  as current link-state protocols, while maintaining loop freedom at every instant. The protocol is based  on three main elements: a transport algorithm that supports the reliable exchange of messages among  routers, the diffusing update algorithm, which computes shortest paths distributedly, and modules  that permit the operation of the new routing protocol in a multiprotocol environment.
135|The mote connectivity protocol|Abstract — An attractive architecture for sensor networks is to have the sensing devices mounted on small computers, called motes. Motes are battery-powered, and can communicate in a wireless fashion by broadcasting messages over radio frequency. In mote networks, the connectivity of a mote u can be defined by those motes that can receive messages from u with high probability and those motes from which u can receive messages with high probability. In this paper, we describe a protocol that can be triggered by any mote in a mote network in order that each mote in the network computes its connectivity. The protocol is simple and has several energy saving features. We implemented this protocol over TinyOS and discuss the results of some execution runs of this implementation. I.
136|Bro: A System for Detecting Network Intruders in Real-Time|We describe Bro, a stand-alone system for detecting network intruders in real-time by passively monitoring a network link over which the intruder&#039;s traffic transits. We give an overview of the system&#039;s design, which emphasizes highspeed (FDDI-rate) monitoring, real-time notification, clear separation between mechanism and policy, and extensibility. To achieve these ends, Bro is divided into an “event engine” that reduces a kernel-filtered network traffic stream into a series of higher-level events, and a “policy script interpreter” that interprets event handlers written in a specialized language used to express a site&#039;s security policy. Event handlers can update state information, synthesize new events, record information to disk, and generate real-time notifications via syslog. We also discuss a number of attacks that attempt to subvert passive monitoring systems and defenses against these, and give particulars of how Bro analyzes the six applications integrated into it so far: Finger, FTP, Portmapper, Ident, Telnet and Rlogin. The system is publicly available in source code form.  
137|End-to-End Internet Packet Dynamics|  We discuss findings from a large-scale study of Internet packet dynamics conducted by tracing 20 000 TCP bulk transfers between 35 Internet sites. Because we traced each 100-kbyte transfer at both the sender and the receiver, the measurements allow us to distinguish between the end-toend behaviors due to the different directions of the Internet paths, which often exhibit asymmetries. We: 1) characterize the prevalence of unusual network events such as out-of-order delivery and packet replication; 2) discuss a robust receiver-based algorithm for estimating “bottleneck bandwidth ” that addresses deficiencies discovered in techniques based on “packet pair;” 3) investigate patterns of packet loss, finding that loss events are not well modeled as independent and, furthermore, that the distribution of the duration of loss events exhibits infinite variance; and 4) analyze variations in packet transit delays as indicators of congestion periods, finding that congestion periods also span a wide range of time scales. 
138|Empirically-Derived Analytic Models of Wide-Area TCP Connections: Extended Report|We analyze 2.5 million TCP connections that occurred during 14 wide-area traffic traces. The traces were gathered at five &#034;stub&#034; networks and two internetwork gateways, providing a diverse look at wide-area traffic. We derive analytic models describing the random variables associated with telnet, nntp, smtp, and ftp connections, and present a methodology for comparing the effectiveness of the analytic models with empirical models such as tcplib [DJ91]. Overall we find that the analytic models provide good descriptions, generally modeling the various distributions as well as empirical models and in some cases better.
139|A Methodology for Testing Intrusion Detection Systems|Intrusion Detection Systems (IDSs) attempt to identify unauthorized use, misuse, and abuse of computer systems. In response to the growth in the use and development of IDSs, we have developed a methodology for testing IDSs. The methodology consists of techniques from the field of software testing which we have adapted for the specific purpose of testing IDSs. In this paper, we identify a set of general IDS performance objectives which is the basis for the methodology. We present the details of the methodology, including strategies for test-case selection and specific testing procedures. We include quantitative results from testing experiments on the Network Security Monitor (NSM), an IDS developed at UC Davis. We present an overview of the software platform that we have used to create user-simulation scripts for testing experiments. The platform consists of the UNIX tool expect and enhancements that we have developed, including mechanisms for concurrent scripts and a record-and-replay ...
140|Collaborative Load Shedding for Media-Based Applications|The VuSystem is a software-intensive video environment developed on operating systems without real-time features. In the course of our work with this system, interesting questions have been brought up about how resources on a workstation can be best divided between applications, and what resource management features (if any) are required. This paper&#039;s main purpose is to outline a rationale for collaborative load shedding and to create an awareness within the operating system community; it also discusses some our early experiences with simple collaborative load shedding schemes.  1 1.0 Introduction  The VuSystem[3,4] is a software-intensive system for manipulating video streams on workstations. The VuSystem runs on a wide variety of workstations, including Suns, DECStations, DEC Alpha workstations, and Silicon Graphics workstations. Its performance varies  from workstation to workstation, simply trying to do &#034;the best&#034; it can on each platform. This system works very well for workstation...
141|SCRIBE: A large-scale and decentralized application-level multicast infrastructure|This paper presents Scribe, a scalable application-level multicast infrastructure. Scribe supports large numbers of groups, with a potentially large number of members per group. Scribe is built on top of Pastry, a generic peer-to-peer object location and routing substrate overlayed on the Internet, and leverages Pastry&#039;s reliability, self-organization, and locality properties. Pastry is used to create and manage groups and to build efficient multicast trees for the dissemination of messages to each group. Scribe provides best-effort reliability guarantees, but we outline how an application can extend Scribe to provide stronger reliability. Simulation results, based on a realistic network topology model, show that Scribe scales across a wide range of groups and group sizes. Also, it balances the load on the nodes while achieving acceptable delay and link stress when compared to IP multicast.
142|Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications|A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.
143|A Scalable Content-Addressable Network|Hash tables – which map “keys ” onto “values” – are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internet-like scales. The CAN is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation. 
144|Pastry: Scalable, distributed object location and routing for large-scale peer-to-peer systems|This paper presents the design and evaluation of Pastry, a scalable, distributed object location and routing scheme for wide-area peer-to-peer applications. Pastry provides application-level routing and object location in a potentially very large overlay network of nodes connected via the Internet. It can be used to support a wide range of peer-to-peer applications like global data storage, global data sharing, and naming. An insert operation in Pastry stores an object at a user-defined number of diverse nodes within the Pastry network. A lookup operation reliably retrieves a copy of the requested object if one exists. Moreover, a lookup is usually routed to the node nearest the client issuing the lookup (by some measure of proximity), among the nodes storing the requested object. Pastry is completely decentralized, scalable, and self-configuring; it automatically adapts to the arrival, departure and failure of nodes. Experimental results obtained with a prototype implementation on a simulated network of 100,000 nodes confirm Pastry&#039;s scalability, its ability to self-configure and adapt to node failures, and its good network locality properties.
145|A Case for End System Multicast|Abstract — The conventional wisdom has been that IP is the natural protocol layer for implementing multicast related functionality. However, more than a decade after its initial proposal, IP Multicast is still plagued with concerns pertaining to scalability, network management, deployment and support for higher layer functionality such as error, flow and congestion control. In this paper, we explore an alternative architecture that we term End System Multicast, where end systems implement all multicast related functionality including membership management and packet replication. This shifting of multicast support from routers to end systems has the potential to address most problems associated with IP Multicast. However, the key concern is the performance penalty associated with such a model. In particular, End System Multicast introduces duplicate packets on physical links and incurs larger end-to-end delays than IP Multicast. In this paper, we study these performance concerns in the context of the Narada protocol. In Narada, end systems selforganize into an overlay structure using a fully distributed protocol. Further, end systems attempt to optimize the efficiency of the overlay by adapting to network dynamics and by considering application level performance. We present details of Narada and evaluate it using both simulation and Internet experiments. Our results indicate that the performance penalties are low both from the application and the network perspectives. We believe the potential benefits of transferring multicast functionality from end systems to routers significantly outweigh the performance penalty incurred. I.
146|A Reliable Multicast Framework for Light-weight Sessions and Application Level Framing|This paper... reliable multicast framework for application level framing and light-weight sessions. The algorithms of this framework are efficient, robust, and scale well to both very large networks and very large sessions. The framework has been prototype in wb, a distributed whiteboard application, and has been extensively tested on a global scale with sessions ranging from a few to more than 1000 participants. The paper describes the principles that have guided our design, including the 1P multicast group delivery model, an end-to-end, receiver-based model of reliability, and the application level framing protocol model. As with unicast communications, the performance of a reliable multicast delivery algorithm depends on the underlying topology and operational environment. We investigate that dependence via analysis and simulation, and demonstrate an adaptive algorithm that uses the results of previous loss recovery events to adapt the control parameters used for future loss recovery. Whh the adaptive algorithm, our reliable multicast delivery algorithm provides good performance over a wide range of underlying topologies. 
147|Multicast Routing in Datagram Internetworks and Extended LANs|Multicasting, the transmission of a packet to a group of hosts, is an important service for improving the efficiency and robustness of distributed systems and applications. Although multicast capability is available and widely used in local area networks, when those LANs are interconnected by store-and-forward routers, the multicast service is usually not offered across the resulting internetwork. To address this limitation, we specify extensions to two common internetwork routing algorithms-distance-vector routing and link-state routing-to support low-delay datagram multicasting beyond a single LAN. We also describe modifications to the single-spanning-tree routing algorithm commonly used by link-layer bridges, to reduce the costs of multicasting in large extended LANs. Finally, we discuss how the use of multicast scope control and hierarchical multicast routing allows the multicast service to scale up to large internetworks.
148|Storage management and caching in PAST, a large-scale, persistent peer-to-peer storage utility|This paper presents and evaluates the storage management and caching in PAST, a large-scale peer-to-peer persistent storage utility. PAST is based on a self-organizing, Internetbased overlay network of storage nodes that cooperatively route file queries, store multiple replicas of files, and cache additional copies of popular files. In the PAST system, storage nodes and files are each assigned uniformly distributed identifiers, and replicas of a file are stored at nodes whose identifier matches most closely the file’s identifier. This statistical assignment of files to storage nodes approximately balances the number of files stored on each node. However, non-uniform storage node capacities and file sizes require more explicit storage load balancing to permit graceful behavior under high global storage utilization; likewise, non-uniform popularity of files requires caching to minimize fetch distance and to balance the query load. We present and evaluate PAST, with an emphasis on its storage management and caching system. Extensive tracedriven experiments show that the system minimizes fetch distance, that it balances the query load for popular files, and that it displays graceful degradation of performance as the global storage utilization increases beyond 95%.  
149|How to Model an Internetwork|Graphs are commonly used to model the structure of internetworks, for the study of problems ranging from routing to resource reservation. A variety of graph models are found in the literature, including regular topologies such as rings or stars, &#034;well-known&#034; topologies such as the original ARPAnet, and randomly generated topologies. Less common is any discussion of how closely these models correlate with real network topologies. We consider the problem of efficiently generating graph models that accurately reflect the topological properties of real internetworks. We compare properties of graphs generated using various methods with those of real internets. We also propose efficient methods for generating topologies with particular properties, including a Transit-Stub model that correlates well with Internet structure. Improved models for internetwork structure have the potential to impact the significance of simulation studies of internetworking solutions, providing basis for the validi...
150|The many faces of Publish/Subscribe|This paper factors out the common  denominator underlying these variants: full decoupling of the communicating entities  in time, space, and synchronization. We use these three decoupling dimensions to better  identify commonalities and divergences with traditional interaction paradigms. The  many variations on the theme of publish/subscribe are classified and synthesized. In  particular, their respective benefits and shortcomings are discussed both in terms of  interfaces and implementations
151|Overcast: Reliable Multicasting with an Overlay Network|Overcast is an application-level multicasting system that can be incrementally deployed using today&#039;s Internet infrastructure. These properties stem from Overcast&#039;s implementation as an overlay network. An overlay network consists of a collection of nodes placed at strategic locations in an existing network fabric. These nodes implement a network abstraction on top of the network provided by the underlying substrate network. Overcast  provides
152| An Architecture for Wide-Area Multicast Routing |Existing multicast routing mechanisms were intended for use within regions where a group is widely represented or bandwidth is universally plentiful. When group members, and senders to those group members, are distributed sparsely across a wide area, these schemes are not efficient; data packets or membership report information are occasionally sent over many links that do not lead to receivers or senders, respectively. Wehave developed a multicast routing architecture that efficiently establishes distribution trees across wide area internets, where many groups will be sparsely represented. Efficiency is measured in terms of the state, control message processing, and data packet processing, required across the entire network in order to deliver data packets to the members of the group. Our Protocol Independent Multicast (PIM) architecture: (a) maintains the traditional IP multicast service model of receiver-initiated membership; (b) can be configured to adapt to different multicast group and network characteristics; (c) is not dependent on a specific unicast routing protocol; and (d) uses soft-state mechanisms to adapt to underlying network conditions and group dynamics. The robustness, flexibility, and scaling properties of this architecture make it well suited to large heterogeneous inter-networks.
153|Bayeux: An architecture for scalable and fault-tolerant wide-area data dissemination|The demand for streaming multimedia applications is growing at an incredible rate. In this paper, we propose Bayeux, an efficient application-level multicast system that scales to arbitrarily large receiver groups while tolerating failures in routers and network links. Bayeux also includes specific mechanisms for load-balancing across replicate root nodes and more efficient bandwidth consumption. Our simulation results indicate that Bayeux maintains these properties while keeping transmission overhead low. To achieve these properties, Bayeux leverages the architecture of Tapestry, a fault-tolerant, wide-area overlay routing and location network.
154|Application-Level Multicast Using Content-Addressable Networks|Most currently proposed solutions to application-level multicast organize  the group members into an application-level mesh over which a DistanceVector  routing protocol, or a similar algorithm, is used to construct source-rooted  distribution trees. The use of a global routing protocol limits the scalability of  these systems. Other proposed solutions that scale to larger numbers of receivers  do so by restricting the multicast service model to be single-sourced. In this paper,  we propose an application-level multicast scheme capable of scaling to large  group sizes without restricting the service model to a single source. Our scheme  builds on recent work on Content-Addressable Networks (CANs). Extending the  CAN framework to support multicast comes at trivial additional cost and, because  of the structured nature of CAN topologies, obviates the need for a multicast  routing algorithm. Given the deployment of a distributed infrastructure such as a  CAN, we believe our CAN-based multicast scheme offers the dual advantages of  simplicity and scalability.
155|Lightweight probabilistic broadcast|The growing interest in peer-to-peer applications has underlined the importance of scalability in modern distributed systems. Not surprisingly, much research effort has been invested in gossip-based broadcast protocols. These trade the traditional strong reliability guarantees against very good “scalability” properties. Scalability is in that context usually expressed in terms of throughput and delivery latency, but there is only little work on how to reduce the overhead of membership management at large scale. This paper presents Lightweight Probabilistic Broadcast (lpbcast), a novel gossip-based broadcast algorithm which preserves the inherent throughput scalability of traditional gossip-based algorithms and adds a notion of membership management scalability: every process only knows a random subset of fixed size of the processes in the system. We formally analyze our broadcast algorithm in terms of scalability with respect to the size of individual views, and compare the analytical results both with simulations and concrete measurements. 
156|Bimodal Multicast|This paper looks at reliability with a new goal: development of a multicast protocol which is reliable in a sense that can be rigorously quantified and includes throughput stability guarantees. We characterize this new protocol as a &#034;bimodal multicast&#034; in reference to its reliability model, which corresponds to a family of bimodal probability distributions. Here, we introduce the protocol, provide a theoretical analysis of its behavior, review experimental results, and discuss some candidate applications. These confirm that bimodal multicast is reliable, scalable, and that the protocol provides remarkably stable delivery throughput
157|Exploiting network proximity in peer-to-peer overlay networks|In CAN, each node measures its network delay to a set of
158|Security for structured peer-to-peer overlay networks|One Ring to rule them all. One Ring to find them. One Ring to bring them all. And in the darkness bind them. J.R.R. Tolkien Self-organizing, structured peer-to-peer (p2p) overlay networks like CAN, Chord, Pastry and Tapestry offer a novel platform for a variety of scalable and decentralized distributed applications. These systems provide efficient and fault-tolerant routing, object location, and load balancing within a self-organizing overlay network. One major problem with these systems is how to bootstrap them. How do you decide which overlay to join? How do you find a contact node in the overlay to join? How do you obtain the code that you should run? Current systems require that each node that participates in a given overlay supports the same set of applications, and that these applications are pre-installed on each node. In this position paper, we sketch the design of an infrastructure that uses a universal overlay to provide a scalable infrastructure to bootstrap multiple service overlays providing different functionality. It provides mechanisms to advertise services and to discover services, contact nodes, and service code. 1.
159|The impact of routing policy on internet paths|Abstract — The impact of routing policy on Internet paths is poorly understood. In theory, policy can inflate shortest-router-hop paths. To our knowledge, the extent of this inflation has not been previously examined. Using a simplified model of routing policy in the Internet, we obtain approximate indications of the impact of policy routing on Internet paths. Our findings suggest that routing policy does impact the length of Internet paths significantly. For instance, in our model of routing policy, some 20 % of Internet paths are inflated by more than five router-level hops. Keywords—Routing, Routing Policy, Policy Routing, Internet Paths I.
160|Herald: Achieving a Global Event Notification Service|This paper presents the design philosophy and initial design decisions of Herald: a highly scalable global event notification system that is being designed and built at Microsoft Research. Herald is a distributed system designed to transparently scale in all respects, including numbers of subscribers and publishers, numbers of event subscription points, and event delivery rates. Event delivery can occur within a single machine, within a local network or Intranet, and throughout the Internet.
161|Bigtable: A distributed storage system for structured data|Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.  
162|MapReduce: Simplified Data Processing on Large Clusters|MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.  
163|Space/Time Trade-offs in Hash Coding with Allowable Errors|this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash- coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency
164|The Google File System |We have designed and implemented the Google File Sys-tem, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous dis-tributed file systems, our design has been driven by obser-vations of our application workloads and technological envi-ronment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore rad-ically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our ser-vice as well as research and development efforts that require large data sets. The largest cluster to date provides hun-dreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use. Categories and Subject Descriptors D [4]: 3—Distributed file systems
165|The ubiquitous B-tree|B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and nnplemented using B-trees This paper reviews B-trees and shows why they have been so successful It discusses the major variations of the B-tree, especially the B+-tree,
166|Parallel database systems: the future of high performance database systems|Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1.
167|Resource Containers: A New Facility for Resource Management in Server Systems|General-purpose operating systems provide inadequate support for resource management in large-scale servers. Applications lack sufficient control over scheduling and management of machine resources, which makes it difficult to enforce priority policies, and to provide robust and controlled service. There is a fundamental mismatch between the original design assumptions underlying the resource management mechanisms of current general-purpose operating systems, and the behavior of modern server applications. In particular, the operating system’s notions of protection domain and resource principal coincide in the process abstraction. This coincidence prevents a process that manages large numbers of network connections, for example, from properly allocating system resources among those connections. We propose and evaluate a new operating system abstraction called a resource container, which separates the notion of a protection domain from that of a resource principal. Resource containers enable fine-grained resource management in server systems and allow the development of robust servers, with simple and firm control over priority policies. 1
168|Operating System Support for Planetary-Scale Network Services|PlanetLab is a geographically distributed overlay network designed to support the deployment and evaluation of planetary-scale network services. Two high-level goals shape its design. First, to enable a large research community to share the infrastructure, PlanetLab provides distributed virtualization, whereby each service runs in an isolated slice of PlanetLab’s global resources. Second, to support competition among multiple network services, PlanetLab decouples the operating system running on each node from the networkwide services that define PlanetLab, a principle referred to as unbundled management. This paper describes how Planet-Lab realizes the goals of distributed virtualization and unbundled management, with a focus on the OS running on each node. 1
169|Paxos made live: an engineering perspective|We describe our experience building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected algorithmic and engineering problems encountered, and the solutions we found for them. Our measurements indicate that we have built a competitive system. 1
170|Integrating compression and execution in column-oriented database systems|Column-oriented database system architectures invite a reevaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads. In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in roworiented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.
171|Weaving Relations for Cache Performance|Relational database systems have traditionally optimzed for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM) (a.k.a., slotted pages). Recent research, however, indicates that cache utilization and performance is becoming increasingly important on modern platforms. In this paper, we first demonstrate that in-page data placement is the key to high cache performance and that NSM exhibits low cache utilization on modern platforms. Next, we propose a new data organization model called PAX (Partition Attributes Across), that significantly improves cache performance by grouping together all values of each attribute within each page. Because PAX only affects layout inside the pages, it incurs no storage penalty and does not affect I/O behavior. According to our experimental results, when compared to NSM (a) PAX exhibits superior cache and memory bandwidth utilization, saving at least 75% of NSM&#039;s stall time due to data cache accesses, (b) range selection queries and updates on memoryresident relations execute 17-25% faster, and (c) TPC-H queries involving I/O execute 11-48% faster.
172|Data Placement in Bubba|Thus paper examines the problem of data placement in Bubba, a highly-parallel system for data-intensive applications bemg developed at MCC “Highly-parallel” lmplres that load balancmng IS a cntlcal performance issue “Data-mtenave” means data IS so large that operatrons should be executed where the data resides As a result, data placement becomes a cntlcal performance issue In general, determmmg the optimal placement of data across processmg nodes for performance IS a difficult problem We describe our heuristic approach to solvmg the data placement problem w Bubba We then present expenmental results using a specific workload to provide msrght into the problem Several researchers have argued the benefits of deelustering (1 e, spreading each base relation over many nodes) We show that as declustermg IS increased. load balancing contmues to improve However, for transactions mvolvmg complex Joins, further declusterrng reduces throughput because of communications, startup and termmatron overhead We argue that data placement, especially declustermg, m a highly-parallel system must be considered early in the design, so that mechanrsms can be included for supportmg variable declustermg, for mmtmlzmg the most significant overheads associated with large-scale declustenng, and for gathering the required statistics.
173|DB2 Parallel Edition|The rate of increase in database size and response time requirements has outpaced  advancements in processor and mass storage technology. One way to satisfy the increasing  demand for processing power and I/O bandwidth in database applications is to have a  number of processors, loosely or tightly coupled, serving database requests concurrently.  Technologies developed during the last decade have made commercial parallel database  systems a reality and these systems have made an inroad into the stronghold of traditionally  mainframe based large database applications. This paper describes the parallel database  project initiated at IBM Research at Hawthorne and the DB2/AIX-PE product based on  it.  1 Introduction  Large scale parallel processing technology has made giant strides in the past decade and there is no doubt that it has established a place for itself. However, almost all of the applications harnessing this technology are scientific or engineering applications. The lack of com...
174|A high-performance, portable implementation of the MPI message passing interface standard|MPI (Message Passing Interface) is a specification for a standard library for message passing that was defined by the MPI Forum, a broadly based group of parallel computer vendors, library writers, and applications specialists. Multiple implementations of MPI have been developed. In this paper, we describe MPICH, unique among existing implementations in its design goal of combining portability with high performance. We document its portability and performance and describe the architecture by which these features are simultaneously achieved. We also discuss the set of tools that accompany the free distribution of MPICH, which constitute the beginnings of a portable parallel programming environment. A project of this scope inevitably imparts lessons about parallel computing, the specification being followed, the current hardware and software environment for parallel computing, and project management; we describe those we have learned. Finally, we discuss future developments for MPICH, including those necessary to accommodate extensions to the MPI Standard now being contemplated by the MPI Forum. 1
175|CoCheck: Checkpointing and Process Migration for MPI|Checkpointing of parallel applications can be used as the core technology to provide process migration. Both, checkpointing and migration, are an important issue for parallel applications on networks of workstations. The CoCheck environment which we present in this paper introduces a new approach to provide checkpointing and migration for parallel applications. In difference to existing systems CoCheck rather sits on top of the message passing library than inside and achieves consistency at a level above the message passing system. It uses an existing single process checkpointer which is available for a wide range of systems. Hence, CoCheck can be easily adapted to both, different message passing systems and new machines. 
176|User&#039;s Guide for mpich, a Portable Implementation of MPI Version 1.2.1|1 1 Introduction 2 2 Linking and running programs 2 2.1 Scripts to Compile and Link Applications . . . . . . . . . . . . . . . . . . . 3 2.1.1 Fortran 90 and the MPI module . . . . . . . . . . . . . . . . . . . . 4 2.2 Compiling and Linking without the Scripts . . . . . . . . . . . . . . . . . . 4 2.3 Running with mpirun . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3.1 SMP Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3.2 Multiple Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.4 More detailed control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3 Special features of different systems 6 3.1 Workstation clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.1.1 Checking your machines list . . . . . . . . . . . . . . . . . . . . . . . 7 3.1.2 Using the Secure Shell . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.1.3 Using the Secure Server . . . . . . . . . . . . . . . . ....
177|Monitors, Messages, and Clusters: the p4 Parallel Programming System |p4 is a portable library of C and Fortran subroutines for programming parallel computers. It is the current version of a system that has been in use since 1984. It includes features for explicit parallel programming of shared-memory machines, distributed-memory machines (including heterogeneous networks of workstations), and clusters, by which we mean sharedmemory multiprocessors communicating via message passing. We discuss here the design goals, history, and system architecture of p4 and describe briefly a diverse collection of applications that have demonstrated the utility of p4. 1 Introduction  p4 is a library of routines designed to express a wide variety of parallel algorithms portably, efficiently and simply. The goal of portability requires it to use widely accepted models of computation rather than specific vendor implementations of those models. The goal of efficiency requires it to use models of computation relatively close to those provided by the machines themselves and t...
178|The Nexus Task-parallel Runtime System|A runtime system provides a parallel language compiler with an interface to the low-level facilities required to support interaction between concurrently executing program components. Nexus is a portable runtime system for task-parallel programming languages. Distinguishing features of Nexus include its support for multiple threads of control, dynamic processor acquisition, dynamic address space creation, a global memory model via interprocessor references, and asynchronous events. In addition, it supports heterogeneity at multiple levels, allowing a single computation to utilize different programming languages, executables, processors, and network protocols. Nexus is currently being used as a compiler target for two task-parallel languages: Fortran M and Compositional C++ . In this paper, we present the Nexus design, outline techniques used to implement Nexus on parallel computers, showhow it is used in compilers, and compare its performance with that of another runtime system.  
179|Resource Management and Checkpointing for PVM|Checkpoints cannot only be used to increase fault tolerance, but also to migrate  processes. The migration is particularly useful in workstation environments  where machines become dynamically available and unavailable. We introduce the  CoCheck environment which not only allows the creation of checkpoints, but also  provides process migration. The creation of checkpoints of PVM applications is  explained and we show how this service can be used in a resource manager.  
180|The Design and Evolution of Zipcode|Zipcode is a message-passing and process-management system that was designed for multicomputers and homogeneous networks of computers in order to support libraries and large-scale multicomputer software. The system has evolved significantly over the last five years, based on our experiences and identified needs. Features of Zipcode that were originally unique to it, were its simultaneous support of static process groups, communication contexts, and virtual topologies, forming the &#034;mailer&#034; data structure. Point-to-point and collective operations reference the underlying group, and use contexts to avoid mixing up messages. Recently, we have added &#034;gather-send&#034; and &#034;receive-scatter&#034; semantics, based on persistent Zipcode &#034;invoices,&#034; both as a means to simplify message passing, and as a means to reveal more potential runtime optimizations. Key features in Zipcode appear in the forthcoming MPI standard.  Keywords: Static Process Groups, Contexts, Virtual Topologies, Point-to-Point Communica...
181| Performance Analysis of MPI Programs |The Message Passing Interface (MPI) standard has recently been completed. MPI  is a specification for a library of functions that implement the message-passing model of  parallel computation. One novel feature of MPI is its very general &#034;profiling interface,&#034;  that allows users to attach assorted profiling tools to the MPI library even though they  do not have access to the MPI source code. We describe the MPI profiling interface  and describe three profiling libraries that make use of it. These libraries are distributed  with the portable, publicly available implementation of MPI.   
183|Migrating from PVM to MPI, part I: The Unify System |This paper presents a new kind of portability system, Unify, which modifies the PVM message passing system to provide (currrently a subset of) the Message Passing Interface (MPI) standard notation for message passing. Unify is designed to reduce the effort of learning MPI while providing a sensible means to make use of MPI libraries and MPI calls while applications continue to run in the PVM environment. We are convinced that this strategy will reduce the costs of porting completely to MPI, while providing a gradual environment within which to evolve. Furthermore, it will permit immediate use of MPI-based parallel libraries in applications, even those that use PVM for user code. We describe several paradigms for supporting MPI and PVM message passing notations in a single environment, and note related work on MPI and PVM implementations. We show the design options that existed within our chosen paradigm (which is an MPI interface added to the base PVM system), and why we chose that par...
184|Reuse, Portability and Parallel Libraries|Parallel programs are typically written in an explicitly parallel fashion using either message passing or shared memory primitives. Message passing is attractive for performance and portability since shared memory machines can efficiently execute message passing programs, however message passing machines cannot in general effectively execute shared memory programs. In order to write a parallel program using message passing, the programmer is often obliged to develop a significant amount of code which manages distributed data and events and parallel input/output, and such code may have little or nothing to do with the application. However many parallel applications have common structural elements and much of this additional code can be encapsulated within a parallel library and reused in several programs. We discuss the requirements the library writer and user makes of the basic message passing interface and describe how we have addressed these requirements in our Common High-Level Inte...
185|A Secure Routing Protocol for Ad Hoc Networks|Most recent ad hoc network research has focused on providing routing services without considering security. In this paper, we detail security threats against ad hoc routing protocols, specifically examining AODV and DSR. In light of these threats, we identify three different environments with distinct security requirements. We propose a solution to one, the managed-open scenario where no network infrastructure is pre-deployed, but a small amount of prior security coordination is expected. Our protocol, ARAN, is based on certificates and successfully defeats all identified attacks.
186|Ad-hoc On-Demand Distance Vector Routing|An ad-hoc network is the cooperative engagement of a collection of mobile nodes without the required intervention of any centralized access point or existing infrastructure. In this paper we present Ad-hoc On Demand Distance Vector Routing (AODV), a novel algorithm for the operation of such ad-hoc networks. Each Mobile Host operates as a specialized router, and routes are obtained as needed (i.e., on-demand) with little or no reliance on periodic advertisements. Our new routing algorithm is quite suitable for a dynamic selfstarting network, as required by users wishing to utilize ad-hoc networks. AODV provides loop-free routes even while repairing broken links. Because the protocol does not require global periodic routing advertisements, the demand on the overall bandwidth available to the mobile nodes is substantially less than in those protocols that do necessitate such advertisements. Nevertheless we can still maintain most of the advantages of basic distance-vector routing mechanisms. We show that our algorithm scales to large populations of mobile nodes wishing to form ad-hoc networks. We also include an evaluation methodology and simulation results to verify the operation of our algorithm.
187|A Review of Current Routing Protocols for Ad-Hoc Mobile Wireless Networks   |An ad-hoc mobile network is a collection of mobile nodes that are dynamically and arbitrarily located in such a manner that the interconnections between nodes are capable of changing on a continual basis. In order to facilitate communication within the network, a routing protocol  is used to discover routes between nodes. The primary goal of such an ad-hoc network routing protocol is correct and efficient route establishment between a pair of nodes so that messages may  be delivered in a timely manner. Route construction should be done with a minimum of overhead  and bandwidth consumption. This paper examines routing protocols for ad-hoc networks and  evaluates these protocols based on a given set of parameters. The paper provides an overview of  eight different protocols by presenting their characteristics and functionality, and then provides a comparison and discussion of their respective merits and drawbacks.
188|A Highly Adaptive Distributed Routing Algorithm for Mobile Wireless Networks|We present a new distributed routing protocol for mobile, multihop, wireless networks. The protocol is one of a family of protocols which we term &#034;link reversal&#034; algorithms. The protocol&#039;s reaction is structured as a temporally-ordered sequence of diffusing computations; each computation consisting of a sequence of directed l i nk reversals. The protocol is highly adaptive, efficient and scalable; being best-suited for use in large, dense, mobile networks. In these networks, the protocol&#039;s reaction to link failures typically involves only a localized &#034;single pass&#034; of the distributed algorithm. This capability is unique among protocols which are stable in the face of network partitions, and results in the protocol&#039;s high degree of adaptivity. This desirable behavior is achieved through the novel use of a &#034;physical or logical clock&#034; to establish the &#034;temporal order&#034; of topological change events which is used to structure (or order) the algorithm&#039;s reaction to topological changes. We refer to the protocol as the Temporally-Ordered Routing Algorithm (TORA).
189|Wireless Ad Hoc Networks|A mobile ad hoc network is a relatively new term for an old technology - a network that does not rely on pre-existing infrastructure. Roots of this technology could be traced back to the early 1970s with the DARPA PRNet and the SURAN projects. The new twitch is the application of this technology in the non-military communication environments. Additionally, the research community has also recently addressed some extended features of this technology, such as multicasting and security. Also numerous new solutions to the &#034;old&#034; problems of routing and medium access control have been proposed. This survey attempts to summarize the state-ofthe -art of the ad hoc networking technology in four areas: routing, medium access control, multicasting, and security. Where possible, comparison between the proposed protocols is also discussed.
191|An Efficient Routing Protocol for Wireless Networks|We present
192|Intercepting Mobile Communications: The Insecurity of 802.11|The 802.11 standard for wireless networks includes a Wired Equivalent Privacy (WEP) protocol, used to protect link-layer communications from eavesdropping and other attacks. We have discovered several serious security flaws in the protocol, stemming from misapplication of cryptographic primitives. The flaws lead to a number of practical attacks that demonstrate that WEP fails to achieve its security goals. In this paper, we discuss in detail each of the flaws, the underlying security principle violations, and the ensuing attacks.  1. 
193|Using the Fluhrer, Mantin, and Shamir Attack to Break WEP|We implemented an attack against WEP, the link-layer security protocol for 802.11 networks. The attack was described in a recent paper by Fluhrer, Mantin, and Shamir. With our implementation, and permission of the network administrator, we were able to recover the 128 bit secret key used in a production network, with a passive attack. The WEP standard uses RC4 IVs improperly, and the attack exploits this design failure. This paper describes the attack, how we implemented it, and some optimizations to make the attack more efficient. We conclude that 802.11 WEP is totally insecure, and we provide some recommendations.
194|Your 802.11 Wireless network has no clothes|entication information Many firms, based on vendor literature, believe that the security provided by their deployed wireless access points is sufficient to prevent unauthorized access and use. Unfortunately, nothing could be further from the truth. 1 This article was written before the IEEE 802.11
195|A middleware approach to asynchronous and backward compatible detection and prevention of ARP cache poisoning|This paper discusses the Address Resolution Protocol (ARP) and the problem of ARP cache poisoning. ARP cache poisoning is the malicious act, by a host in a LAN, of introducing a spurious IP address to MAC (Ethernet) address mapping in another host’s ARP cache. We discuss design constraints for a solution: the solution needs to be implemented in middleware, without access or change to any operating system source code, be backward-compatible to the existing protocol, and be asynchronous. We present our solution and implementation aspects of it in a Streams based networking subsystem. Our solution can be implemented in non-Streams based networking subsystems also. Our solution comprises two parts: a &#034;bump in the stack &#034; Streams module, and a separate Stream with a driver and user-level application. We also present the algorithm that is executed in the module and application to prevent ARP cache poisoning where possible and detect and raise alarms otherwise. We then discuss some limitations with our approach and present some preliminary performance numbers for our implementation. 1.
196|Human resource bundles and manufacturing performance: Organizational logic and flexible production systems in the world auto industry|you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at.
197|The Learning Bureaucracy: New United Motors Manufacturing|by
198|The impact of economic performance of a transformation in workplace relations|This study examines how a transformation in patterns of conflict and cooperation affected economic performance in 25 work areas of a large, unionized manufacturing facility in the period 1984-87. Unlike most studies of industrial relations and economic performance, this study clearly distinguishes conflict from cooperation but evaluates the two together, rather than focusing on only one. An analysis of data collected from union and employer ecords and interviews strongly suggests that work areas with &#034;traditional &#034; labor-management relations, rooted in adversarial assumptions, had higher costs, more scrap, lower productiv-ity, and a lower return to direct labor hours worked than work areas with &#034;transformational &#034; relations, characterized by increased coopera-tion and improved ispute resolution. M OST recent studies that seek to link industrial relations activities to eco-nomic performance or productivity focus either on new developments in labor-management cooperation or on patterns of industrial conflict. In this study I develop and test the thesis that links between industrial relations and economic
199|TreadMarks: Distributed Shared Memory on Standard Workstations and Operating Systems|TreadMarks is a distributed shared memory (DSM) system for standard Unix systems such as  SunOS and Ultrix. This paper presents a performance evaluation of TreadMarks running on  Ultrix using DECstation-5000/240&#039;s that are connected by a 100-Mbps switch-based ATM LAN  and a 10-Mbps Ethernet. Our objective is to determine the efficiency of a user-level DSM implementation  on commercially available workstations and operating systems.  We achieved good speedups on the 8-processor ATM network for Jacobi (7.4), TSP (7.2), Quicksort  (6.3), and ILINK (5.7). For a slightly modified version of Water from the SPLASH benchmark  suite, we achieved only moderate speedups (4.0) due to the high communication and synchronization  rate. Speedups decline on the 10-Mbps Ethernet (5.5 for Jacobi, 6.5 for TSP, 4.2 for  Quicksort, 5.1 for ILINK, and 2.1 for Water), reflecting the bandwidth limitations of the Ethernet.  These results support the contention that, with suitable networking technology, DSM is a...
200|Memory Coherence in Shared Virtual Memory Systems|This paper studies the memory coherence problem in designing  said inaplementing a shared virtual memory on looselycoupled  multiprocessors. Two classes of aIgoritb. ms for solving  the problem are presented. A prototype shared virtual  memory on an Apollo ring has been implemented based  on these algorithms. Both theoretical and practical results  show tkat the mentory coherence problem cast indeed be  solved efficiently on a loosely-coupled multiprocessor.
201|PVM: A Framework for Parallel Distributed Computing|The PVM system is a programming environment for the development and execution of large concurrent or parallel applications that consist of many interacting, but relatively independent, components. It is intended to operate on a collection of heterogeneous computing elements interconnected by one or more networks. The participating processors may be scalar machines, multiprocessors, or special-purpose computers, enabling application components to execute on the architecture most appropriate to the algorithm. PVM provides a straightforward and general interface that permits the description of various types of algorithms (and their interactions), while the underlying infrastructure permits the execution of applications on a virtual computing environment that supports multiple parallel computation models. PVM contains facilities for concurrent, sequential, or conditional execution of application components, is portable to a variety of architectures, and supports certain forms of error dete...
202|Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors|Scalable shared-memory multiprocessors distribute memory among the processors and use scalable interconnection networks to provide high bandwidth and low latency communication. In addition, memory accesses are cached, buffered, and pipelined to bridge the gap between the slow shared memory and the fast processors. Unless carefully controlled, such architectural optimizations can cause memory accesses to be executed in an order different from what the programmer expects. The set of allowable memory access orderings forms the memory consistency model or event ordering model for an architecture.
203|Implementation and performance of Munin|Munin is a distributed shared memory (DSM) system that allows shared memory parallel programs to be executed efficiently on distributed memory multiprocessors. Munin is unique among existing DSM systems in its use of multiple consistency protocols and in its use of release consistency. In Munin, shared program variables are annotated with their expected access pattern, and these annotations are then used by the runtime system to choose a consistency protocol best suited to that access pattern. Release consistency allows Munin to mask network latency and reduce the number of messages required to keep memory consistent. Munin&#039;s multiprotocol release consistency is implemented in software using a delayed update queue that buffers and merges pending outgoing writes. A sixteen-processor prototype of Munin is currently operational. We evaluate its implementation and describe the execution of two Munin programs that achieve performance within ten percent of message passing implementations of the same programs. Munin achieves this level of performance with only minor annotations to the shared memory programs.  
204|Evaluation of Release Consistent Software Distributed Shared Memory on Emerging Network Technology |We evaluate the effect of processor speed, network characteristics, and software overhead on the performance of release-consistent software distributed shared memory. We examine five different protocols for implementing release consistency: eager update, eager invalidate, lazy update, lazy invalidate, and a new protocol called lazy hybrid. This lazy hybrid protocol combines the benefits of both lazy update and lazy invalidate. Our simulations indicate that with the processors and networks that are becoming available, coarse-grained applications such as Jacobi and TSP perform well, more or less independent of the protocol used. Medium-grained applications, such as Water, can achieve good performance, but the choice of protocol is critical. For sixteen processors, the best protocol, lazy hybrid, performed more than three times better than the worst, the eager update. Fine-grained applications such as Cholesky achieve little speedup regardless of the protocol used because of the frequency of synchronization operations and the high latency involved. While the use of relaxed memory models, lazy implementations, and multiple-writer protocols has reduced the impact of false sharing, synchronization latency remains a serious problem for software distributed shared memory systems. These results suggest that future work on software DSMs should concentrate on reducing the amount ofsynchronization or its effect.  
205|Weak Ordering -- A New Definition|A memory model for a shared memory, multiprocessor commonly and often implicitly assumed by programmers is that of sequential consistency. This model guarantees that all memory accesses will appear to execute atomically and in program order. An alternative model, weak ordering, offers greater performance potential. Weak ordering was first defined by Dubois, Scheurich and Briggs in terms of a set of rules for hardware that have to be made visible to software. The central hypothesis of this work is that programmers prefer to reason about sequentially consistent memory, rather than having to think about weaker memory, or even write buffers. Following this hypothesis, we re-define weak ordering as a contract between software and hardware. By this contract, software agrees to some formally specified constraints, and hardware agrees to appear sequentially consistent to at least the software that obeys those constraints. We illustrate the power of the new definition with a set of software constraints that forbid data races and an imple-mentation for cache-coherent systems chat is not allowed by the old definition. 
206|The Duality of Memory and Communication in the Implementation of a Multiprocessor Operating System|Mach is a multiprocessor operating system being implemented at Carnegie-Mellon University. An important component of the Mach design is the use of memory objects which can be managed either by the kernel or by user programs through a message interface. This feature allows applications such as transaction management systems to participate in decisions regarding secondary storage management and page replacement. This paper explores the goals, design and implementation of Mach and its external memory management facility. The relationship between memory and communication in Mach is examined as it relates to overall performance, applicability of Mach to new multiprocessor architectures, and the structure of application programs. This research was sponsored by the Defense Advanced Research Projects Agency (DOD), ARPA Order No. 4864, monitored by the Space and Naval Warfare Systems Command under contract N00039-85-C-1034. The views expressed are those of the authors alone. Permission to copy...
207|An Implementation of Distributed Shared Memory|Shared memory is a simple yet powerful paradigm for structuring systems. Recently, there has been an interest in extending this paradigm to non-shared memory architectures as well. For example, the virtual address spaces for all objects in a distributed object-based system could be viewed as constituting a global distributed shared memory. We propose a set of primitives for managing distributed shared memory. We present an implementation of these primitives in the context of an object-based operating system as well as on top of Unix.

The purpose of this paper is to present a set of mechanisms for DSM and an implementation of these mechanisms. All the resources of the system are viewed as potentially shared objects. The name space of these objects constitute a distributed shared memory. The objects are composed of segments, where a segment is a logical entity that has attributes such as read-only, and read-write. There is a concept of ownership and the node where a segment is created (the owner node) is responsible for guaranteeing the consistency of the segment. The distributed shared memory
controller (DSMC) to be described next is the entity that provides the mechanisms for managing these segments.
209|A Distributed Implementation Of The Shared Data-Object Model|The shared data-object model is designed to ease the implementation of parallel applications on loosely coupled distributed systems. Unlike most other models for distributed programming (e.g., RPC), the shared data-object model allows processes on different machines to share data. Such data are encapsulated in data-objects, which are instances of user-defined abstract data types. The shared data-object model forms the basis of a new language for distributed programming, Orca, which gives linguistic support for parallelism and data-objects. A distributed implementation of the shared data-object model should take care of the physical distribution of objects among the local memories of the processors. In particular, an implementation may replicate objects in order to decrease access times to objects and increase parallelism.  The intent of this paper is to show that, for several applications, the proposed model is both easy to use and efficient. We first give a brief description of the sh...
210|The Deisgn Philosophy of the DARPA Internet Protocols|The Intemet protocol suite, TCP/IP, was first proposed fifteen years ago, It was developed by the Defense Advanced Research Projects Agency (DARPA), and has been used widely in military and commercial systems. While there have been papers and specifications that describe how the protocols work, it is sometimes difficult to deduce from these why the protocol is as it is. For example, the Intemet protocol is based on a connectionless ordatagram mode of service. The motivation for this has been greatly misunderstood. This paper attempts to capture some of the early reasoning which shaped the Intemet protocols.
211|A Protocol for Packet Network Intercommunication|A protocol that supports the sharing of resources that exists in different packet switching networks is presented. The protocol provides for variation in individual network packet sizes, transmission failures, sequencing, flow control, end-to-end error checking, and the creation and destruction of logical process-to-process connections. Some implementation issues are considered, and problems such as internetwork routing, accounting, and timeouts are exposed.
212|GPFS: A Shared-Disk File System for Large Computing Clusters|GPFS is IBM&#039;s parallel, shared-disk file system for cluster computers, available on the RS/6000 SP parallel supercomputer and on Linux clusters. GPFS is used on many of the largest supercomputers in the world. GPFS was built on many of the ideas that were developed in the academic community over the last several years, particularly distributed locking and recovery technology. To date it has been a matter of conjecture how well these ideas scale. We have had the opportunity to test those limits in the context of a product that runs on the largest systems in existence. While in many cases existing ideas scaled well, new approaches were necessary in many key areas. This paper describes GPFS, and discusses how distributed locking and recovery techniques were extended to scale to large clusters.
213|Petal: Distributed Virtual Disks|The ideal storage system is globally accessible, always available, provides unlimited performance and capacity for a large number of clients, and requires no management. This paper describes the design, implementation, and performance of Petal, a system that attempts to approximate this ideal in practice through a novel combination of features. Petal consists of a collection of networkconnected servers that cooperatively manage a pool of physical disks. To a Petal client, this collection appears as a highly available block-level storage system that provides large abstract containers called virtual disks. A virtual disk is globally accessible to all Petal clients on the network. A client can create a virtual disk on demand to tap the entire capacity and performance of the underlying physical resources. Furthermore, additional resources, such as servers and disks, can be automatically incorporated into Petal. We have an initial Petal prototype consisting of four 225 MHz DEC 3000/700 work...
214| Frangipani: A Scalable Distributed File System |The ideal distributed file system would provide all its users with coherent, shared access to the same set of files,yet would be arbitrarily scalable to provide more storage space and higher performance to a growing user community. It would be highly available in spite of component failures. It would require minimal human administration, and administration would not become more complex as more components were added. Frangipani is a new file system that approximates this ideal, yet was relatively easy to build because of its two-layer structure. The lower layer is Petal (described in an earlier paper), a distributed storage service that provides incrementally scalable, highly available, automatically managed virtual disks. In the upper layer, multiple machines run the same Frangipani file system code on top of a shared Petal virtual disk, using a distributed lock service to ensure coherence. Frangipaniis meant to run in a cluster of machines that are under a common administration and can communicate securely. Thus the machines trust one another and the shared virtual disk approach is practical. Of course, a Frangipani file system can be exported to untrusted machines using ordinary network file access protocols. We have implemented Frangipani on a collection of Alphas running DIGITAL Unix 4.0. Initial measurements indicate that Frangipani has excellent single-server performance and scales well as servers are added.  
215|Recovery and coherency-control protocols for fast intersystem page transfer and fine-granularity locking in a shared disks transaction environment|llbstract This paper proposes schemes for fast page transfer between transaction system Instances In a shared disks (SD) environment where all the sharing Instances can read and modify the same data Fast page transfer improves transaction response time and concur-rency because one or more disk I/OS are avoided while transferring a page from a system which modified it to another system which needs it. The proposed methods work with the steal and no-force buffer management policies, and fine-granularity (e.g., record) locking For each of the page-transfer schemes, we present both recovery and coherency-control protocols Updates can be made to a page by several systems before the page is written to disk. Many subtleties Involved in correctly recovering such a page in the face of single system or complex-wide failures are also discussed. Assuming that each system maintains its own log, some methods require a merged log for restart recovery while others don’t Our proposals should also apply to dlstrihuted. recoverable file systems and distributed virtual memory in the SD environment, and to the currently oopular client-server object-oriented DBMS environments where the clients cache data. 1.
216|Tiger Shark - a scalable file system for multimedia|Tiger Shark is a scalable, parallel file system designed to support interactive multimedia applications, particularly large-scale ones such as interactive television (ITV). Tiger Shark runs under the IBM AIX operating system, on machines ranging from RS/6000 desktop workstations to the SP2 parallel supercomputer. In addition to supporting continuous-time data, Tiger Shark provides scalability, high availability, and on-line system management, all of which are crucial in large-scale video servers. These latter features also enable Tiger Shark to support non-multimedia applications such as scientific computing, data mining, digital library, and scalable network file servers. Tiger Shark has been employed in a number of customer ITV trials. Based on experience obtained from these trials, Tiger Shark has recently been released in several IBM video server products. This paper describes the architecture and implementation of Tiger Shark, discusses the experience gained from trials, and compa...
217|Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks|Dryad is a general-purpose distributed execution engine for coarse-grain data-parallel applications. A Dryad applica-tion combines computational “vertices ” with communica-tion “channels ” to form a dataflow graph. Dryad runs the application by executing the vertices of this graph on a set of available computers, communicating as appropriate through files, TCP pipes, and shared-memory FIFOs. The vertices provided by the application developer are quite simple and are usually written as sequential programs with no thread creation or locking. Concurrency arises from Dryad scheduling vertices to run simultaneously on multi-ple computers, or on multiple CPU cores within a computer. The application can discover the size and placement of data at run time, and modify the graph as the computation pro-gresses to make efficient use of the available resources. Dryad is designed to scale from powerful multi-core sin-gle computers, through small clusters of computers, to data centers with thousands of computers. The Dryad execution engine handles all the difficult problems of creating a large distributed, concurrent application: scheduling the use of computers and their CPUs, recovering from communication or computer failures, and transporting data between ver-tices.
218|The click modular router| Click is a new software architecture for building flexible and configurable routers. A Click router is assembled from packet processing modules called elements. Individual elements implement simple router functions like packet classification, queueing, scheduling, and interfacing with network devices. A router configuration is a directed graph with elements at the vertices; packets flow along the edges of the graph. Configurations are written in a declarative language that supports user-defined abstractions. This language is both readable by humans and easily manipulated by tools. We present language tools that optimize router configurations and ensure they satisfy simple invariants. Due to Click’s architecture and language, Click router configurations are modular and easy to extend. A standards-compliant Click IP router has sixteen elements on its forwarding path. We present extensions to this router that support dropping policies, fairness among flows, quality-of-service, and
219|Cilk: An Efficient Multithreaded Runtime System|Cilk (pronounced &#034;silk&#034;) is a C-based runtime system for multithreaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the &#034;work&#034; and &#034;critical-path length&#034; of a Cilk computation can be used to model performance accurately. Consequently, a Cilk programmer can focus on reducing the computation&#039;s work and critical-path length, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of &#034;fully strict&#034; (well-structured) programs, the Cilk scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The Cilk
220|Distributed Computing in Practice: The Condor Experience|Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational grid. In this chapter, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course traveled by research ideas as they grow into production systems.
221|Cluster-Based Scalable Network Services|This paper has benefited from the detailed and perceptive comments of our reviewers, especially our shepherd Hank Levy. We thank Randy Katz and Eric Anderson for their detailed readings of early drafts of this paper, and David Culler for his ideas on TACC&#039;s potential as a model for cluster programming. Ken Lutz and Eric Fraser configured and administered the test network on which the TranSend scaling experiments were performed. Cliff Frost of the UC Berkeley Data Communications and Networks Services group allowed us to collect traces on the Berkeley dialup IP network and has worked with us to deploy and promote TranSend within Berkeley. Undergraduate researchers Anthony Polito, Benjamin Ling, and Andrew Huang implemented various parts of TranSend&#039;s user profile database and user interface. Ian Goldberg and David Wagner helped us debug TranSend, especially through their implementation of the rewebber
223|Interpreting the Data: Parallel Analysis with Sawzall |Very large data sets often have a flat but regular structure and span multiple disks and machines. Examples include telephone call records, network logs, and web document repositories. These large data sets are not amenable to study using traditional database techniques, if only because they can be too large to fit in a single relational database. On the other hand, many of the analyses done on them can be expressed using simple, easily distributed computations: filtering, aggregation, extraction of statistics, and so on. We present a system for automating such analyses. A filtering phase, in which a query is expressed using a new procedural programming language, emits data to an aggregation phase. Both phases are distributed over hundreds or even thousands of computers. The results are then collated and saved to a file. The design—including the separation into two phases, the form of the programming language, and the properties of the aggregators—exploits the parallelism inherent in having data and computation distributed across many machines. 1
224|Programming Parallel Algorithms|In the past 20 years there has been treftlendous progress in developing and analyzing parallel algorithftls. Researchers have developed efficient parallel algorithms to solve most problems for which efficient sequential solutions are known. Although some ofthese algorithms are efficient only in a theoretical framework, many are quite efficient in practice or have key ideas that have been used in efficient implementations. This research on parallel algorithms has not only improved our general understanding ofparallelism but in several cases has led to improvements in sequential algorithms. Unf:ortunately there has been less success in developing good languages f:or prograftlftling parallel algorithftls, particularly languages that are well suited for teaching and prototyping algorithms. There has been a large gap between languages
225|Programmable stream processors|The Imagine Stream Processor is a single-chip programmable media processor with 48 parallel ALUs. At 400 MHz, this translates to a peak arithmetic rate of 16 GFLOPS on single-precision data and 32 GOPS on 16bit fixed-point data. The scalability of Imagine’s programming model and architecture enable it to achieve such high arithmetic rates. Imagine executes applications that have been mapped to the stream programming model. The stream model decomposes applications into a set of computation kernels that operate on data streams. This mapping exposes the inherent locality and parallelism in the application, and Imagine exploits the locality and parallelism to provide a scalable architecture that supports 48 ALUs on a single chip. This paper presents the Imagine architecture and programming model in the first half, and explores the scalability of the Imagine architecture in the second half. 1.
226|Accelerator: using data parallelism to program GPUs for general-purpose uses|GPUs are difficult to program for general-purpose uses. Programmers can either learn graphics APIs and convert their applications to use graphics pipeline operations or they can use stream programming abstractions of GPUs. We describe Accelerator, a system that uses data parallelism to program GPUs for general-purpose uses instead. Programmers use a conventional imperative programming language and a library that provides only high-level data-parallel operations. No aspects of GPUs are exposed to programmers. The library implementation compiles the data-parallel operations on the fly to optimized GPU pixel shader code and API calls. We describe the compilation techniques used to do this. We evaluate the effectiveness of using data parallelism to program GPUs by providing results for a set of compute-intensive benchmarks. We compare the performance of Accelerator versions of the benchmarks against hand-written pixel shaders. The speeds of the Accelerator versions are typically within 50 % of the speeds of hand-written pixel shader code. Some benchmarks significantly outperform C versions on a CPU: they are up to 18 times faster than C code running on a CPU.
227|Using Cohort Scheduling to Enhance Server Performance|A server application is commonly organized as a collection of concurrent threads, each of which executes the code necessary to process a request. This software architecture, which causes frequent control transfers between unrelated pieces of code, decreases instruction and data locality, and consequently reduces the effec- tiveness of hardware mechanisms such as caches, TLBs, and branch predictors. Numerous measurements demonstrate this effect in server applications, which often utilize only a fraction of a modern processor&#039;s computational throughput.
228|The CODE 2.0 Graphical Parallel Programming Language |CODE 2.0 is a graphical parallel programming system that targets the three goals of ease of use, portability, and production of efficient parallel code. Ease of use is provided by an integrated graphical/textual interface, a powerful dynamic model of parallel computation, and an integrated concept of program component reuse. Portability is approached by the declarative expression of synchronization and communication operators at a high level of abstraction in a manner which cleanly separates overall computation structure from the primitive sequential computations that make up a program. Execution efficiency is approached through a systematic class hierarchy that supports hierarchical translation refinement including special case recognition. This paper reports results obtained through experimental use of a prototype implementation of the CODE 2.0 system. CODE 2.0 represents a major conceptual advance over its predecessor systems (CODE 1.0 and CODE 1.2) in terms of the expressive power ...
229|Stream Computations Organized for Reconfigurable Execution (SCORE): Introduction and Tutorial  (2000) |A primary impediment to wide-spread exploitation of reconfigurable computing is the lack of a unifying computational model which allows application portability and longevity without sacrificing a substantial fraction of the raw capabilities. We introduce SCORE (Stream Computation Organized for Reconfigurable Execution), a streambased compute model which virtualizes reconfigurable computing resources (compute, storage, and communication) by dividing a computation up into fixed-size &#034;pages&#034; and time-multiplexing the virtual pages on available physical hardware. Consequently, SCORE applications can scale up or down automatically to exploit a wide range of hardware sizes. We hypothesize that the SCORE model will ease development and deployment of reconfigurable applications and expand the range of applications which can benefit from reconfigurable execution. Further, we believe that a well engineered SCORE implementation can be efficient, wasting little of the capabilities of the raw hardw...
230|Paralex: An Environment for Parallel Programming in Distributed Systems|Modern distributed systems consisting of powerful workstations and high-speed interconnection networks are an economical alternative to special-purpose super computers. The technical issues that need to be addressed in exploiting the parallelism inherent in a distributed system include heterogeneity, high-latency communication, fault tolerance and dynamic load balancing. Current software systems for parallel programming provide little or no automatic support towards these issues and require users to be experts in fault-tolerant distributed computing. The Paralex system is aimed at exploring the extent to which the parallel application programmer can be liberated from the complexities of distributed systems. Paralex is a complete programming environment and makes extensive use of graphics to define, edit, execute and debug parallel scientific applications. All of the necessary code for distributing the computation across a network and replicating it to achieve fault tolerance and dynamic load balancing is automatically generated by the system. In this paper we give an overview of Paralex and present our experiences with a prototype implementation.
231|Parallel and Distributed Haskells|Parallel and distributed languages specify computations on multiple processors and have a computation language to describe the algorithm, i.e. what to compute, and a coordination language to describe how to organise the computations across the processors. Haskell has been used as the computation language for a wide variety of parallel and distributed languages, and this paper is a comprehensive survey of implemented languages. We outline parallel and distributed language concepts and classify Haskell extensions using them. Similar example programs are used to illustrate and contrast the coordination languages, and the comparison is facilitated by the common computation language. A lazy language is not an obvious choice for parallel or distributed computation, and we address the question of why Haskell is a common functional computation language.
232|Highly Available, Fault-Tolerant, Parallel Dataflows|We present a technique that masks failures in a cluster to provide high availability and fault-tolerance for long-running, parallelized dataflows. We can use these dataflows to implement a variety of continuous query (CQ) applications that require high-throughput, 24x7 operation. Examples include network monitoring, phone call processing, click-stream processing, and online financial analysis. Our main contribution is a scheme that carefully integrates traditional query processing techniques for partitioned parallelism with the process-pairs approach for high availability. This delicate integration allows us to tolerate failures of portions of a parallel dataflow  without sacrificing result quality. Upon failure, our technique provides quick fail-over, and automatically recovers the lost pieces on the fly. This piecemeal recovery provides minimal disruption to the ongoing dataflow computation and improved reliability as compared to the straight-forward application of the process-pairs technique on a per dataflow basis. Thus, our technique provides the high availability necessary for critical CQ applications. Our techniques are encapsulated in a reusable dataflow operator called Flux, an extension of the Exchange that is used to compose parallel dataflows. Encapsulating the fault-tolerance logic into Flux minimizes modifications to existing operator code and relieves the burden on the operator writer of repeatedly implementing and verifying this critical logic. We present experiments illustrating these features with an implementation of Flux in the TelegraphCQ code base [8].
233|A comparison of stream-oriented high availability algorithms|Recently, significant efforts have focused on developing novel data-processing systems to support a new class of applications that commonly require sophisticated and timely processing of high-volume data streams. Early work in stream processing has primarily focused on streamoriented languages and resource-constrained, one-pass query-processing. High availability, an increasingly important goal for virtually all data processing systems, is yet to be addressed. In this paper, we first describe how the standard highavailability approaches used in data management systems can be applied to distributed stream processing. We then propose a novel stream-oriented approach that exploits the unique data-flow nature of streaming systems. Using analysis and a detailed simulation study, we characterize the performance of each approach and demonstrate that the stream-oriented algorithm significantly reduces runtime overhead at the expense of a small increase in recovery time. 1
234|Distributed Database Systems |this article, we discuss the fundamentals of distributed DBMS technology. We address the data distribution and architectural design issues as well as the algorithms that need to be implemented to provide the basic DBMS functions such as query processing, concurrency control, reliability, and replication control.
235|Composable memory transactions|Atomic blocks allow programmers to delimit sections of code as ‘atomic’, leaving the language’s implementation to enforce atomicity. Existing work has shown how to implement atomic blocks over word-based transactional memory that provides scalable multiprocessor performance without requiring changes to the basic structure of objects in the heap. However, these implementations perform poorly because they interpose on all accesses to shared memory in the atomic block, redirecting updates to a thread-private log which must be searched by reads in the block and later reconciled with the heap when leaving the block. This paper takes a four-pronged approach to improving performance: (1) we introduce a new ‘direct access ’ implementation that avoids searching thread-private logs, (2) we develop compiler optimizations to reduce the amount of logging (e.g. when a thread accesses the same data repeatedly in an atomic block), (3) we use runtime filtering to detect duplicate log entries that are missed statically, and (4) we present a series of GC-time techniques to compact the logs generated by long-running atomic blocks. Our implementation supports short-running scalable concurrent benchmarks with less than 50 % overhead over a non-thread-safe baseline. We support long atomic blocks containing millions of shared memory accesses with a 2.5-4.5x slowdown. Categories and Subject Descriptors D.3.3 [Programming Languages]:
236|Linearizability: a correctness condition for concurrent objects|A concurrent object is a data object shared by concurrent processes. Linearizability is a correctness condition for concurrent objects that exploits the semantics of abstract data types. It permits a high degree of concurrency, yet it permits programmers to specify and reason about concurrent objects using known techniques from the sequential domain. Linearizability provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object’s operations can be given by pre- and post-conditions. This paper defines linearizability, compares it to other correctness conditions, presents and demonstrates a method for proving the correctness of implementations, and shows how to reason about concurrent objects, given they are linearizable.
237|Transactional Memory: Architectural Support for Lock-Free Data Structures |A shared data structure is lock-free if its operations do not require mutual exclusion. If one process is interrupted in the middle of an operation, other processes will not be prevented from operating on that object. In highly concurrent systems, lock-free data structures avoid common problems associated with conventional locking techniques, including priority inversion, convoying, and difficulty of avoiding deadlock. This paper introduces transactional memory, a new multiprocessor architecture intended to make lock-free synchronization as efficient (and easy to use) as conventional techniques based on mutual exclusion. Transactional memory allows programmers to define customized read-modify-write operations that apply to multiple, independently-chosen words of memory. It is implemented by straightforward extensions to any multiprocessor cache-coherence protocol. Simulation results show that transactional memory matches or outperforms the best known locking techniques for simple benchmarks, even in the absence of priority inversion, convoying, and deadlock.  
238|Language Support for Lightweight Transactions|Concurrent programming is notoriously di#cult. Current abstractions are intricate and make it hard to design computer systems that are reliable and scalable. We argue that these problems can be addressed by moving to a declarative style of concurrency control in which programmers directly indicate the safety properties that they require.
239|Software transactional memory for dynamic-sized data structures|We propose a new form of software transactional memory (STM) designed to support dynamic-sized data structures, and we describe a novel non-blocking implementation. The non-blocking property we consider is obstruction-freedom. Obstruction-freedom is weaker than lock-freedom; as a result, it admits substantially simpler and more efficient implementations. A novel feature of our obstruction-free STM implementation is its use of modular contention managers to ensure progress in practice. We illustrate the utility of our dynamic STM with a straightforward implementation of an obstruction-free red-black tree, thereby demonstrating a sophisticated non-blocking dynamic data structure that would be difficult to implement by other means. We also present the results of simple preliminary performance experiments that demonstrate that an &#034;early release &#034; feature of our STM is useful for reducing contention, and that our STM lends itself to the effective use of modular contention managers. 
240|Virtualizing Transactional Memory|Writing concurrent programs is difficult because of the complexity of ensuring proper synchronization. Conventional lock-based synchronization suffers from wellknown limitations, so researchers have considered nonblocking transactions as an alternative. Recent hardware proposals have demonstrated how transactions can achieve high performance while not suffering limitations of lock-based mechanisms. However, current hardware proposals require programmers to be aware of platform-specific resource limitations such as buffer sizes, scheduling quanta, as well as events such as page faults, and process migrations. If the transactional model is to gain wide acceptance, hardware support for transactions must be virtualized to hide these limitations in much the same way that virtual memory shields the programmer from platform-specific limitations of physical memory. This paper proposes Virtual Transactional Memory (VTM), a user-transparent system that shields the programmer from various platform-specific resource limitations. VTM maintains the performance advantage of hardware transactions, incurs low overhead in time, and has modest costs in hardware support. While many system-level challenges remain, VTM takes a step toward making transactional models more widely acceptable.  
241|Speculative Lock Elision: Enabling Highly Concurrent Multithreaded Execution|Serialization of threads due to critical sections is a fundamental bottleneck to achieving high performance in multithreaded programs. Dynamically, such serialization may be unnecessary because these critical sections could have safely executed concurrently without locks. Current processors cannot fully exploit such parallelism because they do not have mechanisms to dynamically detect such false inter-thread dependences. We propose Speculative Lock Elision (SLE), a novel micro-architectural technique to remove dynamically unnecessary lock-induced serialization and enable highly concurrent multithreaded execution. The key insight is that locks do not always have to be acquired for a correct execution. Synchronization instructions are predicted as being unnecessary and elided. This allows multiple threads to concurrently execute critical sections protected by the same lock. Misspeculation due to inter-thread data conflicts is detected using existing cache mechanisms and rollback is used for recovery. Successful speculative elision is validated and committed without acquiring the lock. SLE can be implemented entirely in microarchitecture without instruction set support and without system-level modifications, is transparent to programmers, and requires only trivial additional hardware support. SLE can provide programmers a fast path to writing correct high-performance multithreaded programs.  
242|Transactional Lock-Free Execution of Lock-Based Programs|This paper is motivated by the difficulty in writing correct high-performance programs. Writing shared-memory multithreaded programs imposes a complex trade-off between programming ease and performance, largely due to subtleties in coordinating access to shared data. To ensure correctness programmers often rely on conservative locking at the expense of performance. The resulting serialization of threads is a performance bottleneck. Locks also interact poorly with thread scheduling and faults, resulting in poor system performance.
243|Modern Concurrency Abstractions for C#|Polyphonic C# is an extension of the C# language with new asynchronous concurrency constructs, based on the join calculus. We describe the design and implementation of the language and give examples of its use in addressing a range of concurrent programming problems. 
244|Thin Locks: Featherweight Synchronization for Java|Language-supported synchronization is a source of serious performance problems in many Java programs. Even singlethreaded applications may spend up to half their time performing useless synchronization due to the thread-safe nature of the Java libraries. We solve this performance problem with a new algorithm that allows lock and unlock operations to be performed with only a few machine instructions in the most common cases. Our locks only require a partial word per object, and were implemented without increasing object size. We present measurements from our implementation in the JDK 1.1.2 for AIX, demonstrating speedups of up to a factor of 5 in micro-benchmarks and up to a factor of 1.7 in real programs. 1 Introduction Monitors [5] are a language-level construct for providing mutually exclusive access to shared data structures in a multithreaded environment. However, the overhead required by the necessary locking has generally restricted their use to relatively &#034;heavy-weight&#034; object...
245|Transactional monitors for concurrent objects |Abstract. Transactional monitors are proposed as an alternative to monitors based on mutual-exclusion synchronization for object-oriented programming languages. Transactional monitors have execution semantics similar to mutualexclusion monitors but implement monitors as lightweight transactions that can be executed concurrently (or in parallel on multiprocessors). They alleviate many of the constraints that inhibit construction of transparently scalable and robust applications. We undertake a detailed study of alternative implementation schemes for transactional monitors. These different schemes are tailored to different concurrent access patterns, and permit a given application to use potentially different implementations in different contexts. We also examine the performance and scalability of these alternative approaches in the context of the Jikes Research Virtual Machine, a state-of-the-art Java implementation. We show that transactional monitors are competitive with mutualexclusion synchronization and can outperform lock-based approaches up to five times on a wide range of workloads. 1
246|An Efficient Meta-lock for Implementing Ubiquitous Synchronization|Programs written in concurrent object-oriented languages, espe-cially ones that employ thread-safe reusable class libraries, can execute synchronization operations (lock, notify, etc.) at an amaz-ing rate. Unless implemented with utmost care, synchronization can become a performance bottleneck. Furthermore, in languages where every object may have its own monitor, per-object space overhead must be minimized. To address these concerns, we have developed a meta-lock to mediate access to synchronization data. The meta-lock is fast (lock + unlock executes in 11 SPARCTM architecture instructions), compact (uses only two bits of space), robust under contention (no busy-waiting), and flexible (supports a variety of higher-level synchronization operations). We have vali-dated the meta-lock with an implementation of the synchronization operations in a high-performance product-quality JavaTM virtual machine and report performance data for several large programs.
247|Design tradeoffs in modern software transactional memory systems|Software Transactional Memory (STM) is a generic nonblocking synchronization construct that enables automatic conversion of correct sequential objects into correct concurrent objects. Because it is nonblocking, STM avoids traditional performance and correctness problems due to thread failure, preemption, page faults, and priority inversion. In this paper we compare and analyze two recent objectbased STM systems, the DSTM of Herlihy et al. and the FSTM of Fraser, both of which support dynamic transactions, in which the set of objects to be modified is not known in advance. We highlight aspects of these systems that lead to performance tradeoffs for various concurrent data structures. More specifically, we consider object ownership acquisition semantics, concurrent object referencing style, the overhead of ordering and bookkeeping, contention management versus helping semantics, and transaction validation. We demonstrate for each system simple benchmarks on which it outperforms the other by a significant margin. This in turn provides us with a preliminary characterization of the applications for which each system is best suited.  
248|Lock Coarsening: Eliminating Lock Overhead in Automatically Parallelized Object-Based Programs|Atomic operations are a key primitive in parallel computing systems. The standard implementation mechanism for atomic operations uses mutual exclusion locks. In an object-based programming system the natural granularity is to give each object its own lock. Each operation can then make its execution atomic by acquiring and releasing the lock for the object that it accesses. But this fine lock granularity may have high synchronization overhead because it maximizes the number of executed acquire and release constructs. To achieve good performance it may be necessary to reduce the overhead by coarsening the granularity at which the computation locks objects. In this paper we describe a static analysis technique --- lock coarsening --- designed to automatically increase the lock granularity in object-based programs with atomic operations. We have implemented this technique in the context of a parallelizing compiler for irregular, object-based programs and used it to improve the generated pa...
249|Transactional Execution of Java Programs|Parallel programming is difficult due to the complexity of dealing with conventional lock-based synchronization. To simplify parallel programming, there have been a number of proposals to support transactions directly in hardware and eliminate locks completely. Although hardware support for transactions has the potential to completely change the way parallel programs are written, initially transactions will be used to execute existing parallel programs. In this paper we investigate the implications of using transactions to execute existing parallel Java programs. Our results show that transactions can be used to support all aspects of Java multithreaded programs. Moreover, the conversion of a lock-based application into transactions is largely straightforward. The performance that these converted applications achieve is equal to or sometimes better than the original lock-based implementation.
250|Relaxed balanced red-black trees|Abstract. Relaxed balancing means that, in a dictionary stored as a balanced tree, the necessary rebalancing after updates may be delayed. This is in contrast to strict balancing meaning that rebalancing is performed immediately after the update. Relaxed balancing is important for efficiency in highly dynamic applications where updates can occur in bursts. The rebalancing tasks can be performed gradually after all urgent updates, allowing the concurrent use of the dictionary even though the underlying tree structure is not completely in balance. In this paper we propose a new scheme of how to make known rebalancing techniques relaxed in an efficient way. The idea is applied to the red-black trees, but can be applied to any class of balanced trees. The key idea is to accumulate insertions and deletions such that they can be settled in arbitrary order using the same rebalancing operations as for standard balanced search trees. As a result it can be shown that the number of needed rebalancing operations known from the strict balancing scheme carry over to relaxed balancing. 1
251|Integrating support for undo with exception handling|One of the important tasks of exception handling is to restore program state and invariants. Studies suggest that this is often done incorrectly. We introduce a new language construct that integrates automated memory recovery with exception handling. When an exception occurs, memory can be automatically restored to its previous state. We also provide a mechanism for applications to extend the automatic recovery mechanism with callbacks for restoring the state of external resources. We describe a logging-based implementation and evaluate its effect on performance. The implementation imposes no overhead on parts of the code that do not make use of this feature.
252|Implementing fast Java monitors with relaxed-locks |The Java  Programming Language permits synchronization operations (lock, unlock, wait, notify) on any object. Synchronization is very common in applications and is endemic in the library code upon which applications depend. It is therefore critical that a monitor implementation be both space-efficient and time-efficient. We present a locking protocol, the Relaxed-Lock, that satisfies those requirements. The Relaxed-Lock is reasonably compact, using only one word in the object header. It is fast, requiring in the uncontested case only one atomic compare-and-swap to lock a monitor and no atomic instructions to release a monitor. The Relaxed-Lock protocol is unique in that it permits a benign data race in the monitor unlock path (hence its name) but detects and recovers from the race and thus maintains correct mutual exclusion. We also introduce speculative deflation, a mechanism for releasing a monitor when it is no longer needed. 1
253|Static Scheduling of Synchronous Data Flow Programs for Digital Signal Processing|Large grain data flow (LGDF) programming is natural and convenient for describing digital signal processing (DSP) systems, but its runtime overhead is costly in real time or cost-sensitive applications. In some situations, designers are not willing to squander computing resources for the sake of program-mer convenience. This is particularly true when the target machine is a programmable DSP chip. However, the runtime overhead inherent in most LGDF implementations is not required for most signal processing systems because such systems are mostly synchronous (in the DSP sense). Synchronous data flow (SDF) differs from traditional data flow in that the amount of data produced and consumed by a data flow node is specified a priori for each input and output. This is equivalent to specifying the relative sample rates in signal processing system. This means that the scheduling of SDF nodes need not be done at runtime, but can be done at compile time (statically), so the runtime overhead evaporates. The sample rates can all be different, which is not true of most current data-driven digital signal processing programming methodologies. Synchronous data flow is closely related to computation graphs, a special case of Petri nets. This self-contained paper develops the theory necessary to statically schedule SDF programs on single or multiple proces-sors. A class of static (compile time) scheduling algorithms is proven valid, and specific algorithms are given for scheduling SDF systems onto single or multiple processors. 
254|Petri Nets|Over the last decade, the Petri net has gamed increased usage and acceptance as a basic model of systems of asynchronous concurrent computation. This paper surveys the basic concepts and uses of Petm nets. The structure of Petri nets, their markings and execution, several examples of Petm net models of computer hardware and software, and
255|Snort - Lightweight Intrusion Detection for Networks|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
256|Models and issues in data stream systems|In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues. 
257|Randomized Algorithms|Randomized algorithms, once viewed as a tool in computational number theory, have by now found widespread application. Growth has been fueled by the two major benefits of randomization: simplicity and speed. For many applications a randomized algorithm is the fastest algorithm available, or the simplest, or both. A randomized algorithm is an algorithm that uses random numbers to influence the choices it makes in the course of its computation. Thus its behavior (typically quantified as running time or quality of output) varies from
258|The space complexity of approximating the frequency moments|The frequency moments of a sequence containing mi elements of type i, for 1 = i = n, are the numbers Fk = ?n i=1 mki. We consider the space complexity of randomized algorithms that approximate the numbers Fk, when the elements of the sequence are given one by one and cannot be stored. Surprisingly, it turns out that the numbers F0, F1 and F2 can be approximated in logarithmic space, whereas the approximation of Fk for k = 6 requires n?(1) space. Applications to data bases are mentioned as well. 
259|Multiparty Communication Complexity|A given Boolean function has its input distributed among many parties. The aim is to determine which parties to tMk to and what information to exchange with each of them in order to evaluate the function while minimizing the total communication. This paper shows that it is possible to obtain the Boolean answer deterministically with only a polynomial increase in communication with respect to the information lower bound given by the nondeterministic communication complexity of the function.
260|Fast subsequence matching in time-series databases|We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an ecient and eective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.  
261|Mining high-speed data streams|Categories and Subject ?????????? ? ?¨?????????????????????????¦???¦??????????¡¤?? ¡  ? ¡????????????????¦¡¤????§?£????
262|Online Aggregation|Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in postgres. 1 Introduction  Aggregation is an incre...
263|Efficient Filtering of XML Documents for Selective Dissemination of Information|Information Dissemination applications are gaining increasing  popularity due to dramatic improvements in  communications bandwidth and ubiquity. The sheer  volume of data available necessitates the use of selective  approaches to dissemination in order to avoid overwhelming  users with unnecessaryinformation. Existing  mechanisms for selective dissemination typically rely  on simple keyword matching or &#034;bag of words&#034; information  retrieval techniques. The advent of XML as a  standard for information exchangeand the development  of query languages for XML data enables the development  of more sophisticated filtering mechanisms that  take structure information into account. We have developed  several index organizations and search algorithms  for performing efficient filtering of XML documents for  large-scale information dissemination systems. In this  paper we describe these techniques and examine their  performance across a range of document, workload, and  scale scenarios.  1 
264|External Memory Algorithms and Data Structures|Data sets in large applications are often too massive to fit completely inside the computer&#039;s internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this paper, we survey the state of the art in the design and analysis of external memory algorithms and data structures (which are sometimes referred to as &#034;EM&#034; or &#034;I/O&#034; or &#034;out-of-core&#034; algorithms and data structures). EM algorithms and data structures are often designed and analyzed using the parallel disk model (PDM). The three machine-independent measures of performance in PDM are the number of I/O operations, the CPU time, and the amount of disk space. PDM allows for multiple disks (or disk arrays) and parallel CPUs, and it can be generalized to handle tertiary storage and hierarchical memory. We discuss several important paradigms for how to solve batched and online problems efficiently in external memory. Programming tools and environments are available for simplifying the programming task. The TPIE system (Transparent Parallel I/O programming Environment) is both easy to use and efficient in terms of execution speed. We report on some experiments using TPIE in the domain of spatial databases. The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.  
265|Random sampling with a reservoir|We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O(n(1 + log(N/n))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.
266|Stable Distributions, Pseudorandom Generators, Embeddings and Data Stream Computation|In this paper we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular: ffl we show how to maintain (using only O(log n=ffl  2  ) words of storage) a sketch C(p) of a point p 2 l  n  1 under dynamic updates of its coordinates, such that given sketches C(p) and C(q) one can estimate jp \Gamma qj 1 up to a factor of (1 + ffl) with large probability. This solves the main open problem of [10].  ffl we obtain another sketch function C  0  which maps l  n  1 into a normed space l  m  1 (as opposed to C), such that m = m(n) is much smaller than n; to our knowledge this is the first dimensionality reduction lemma for l 1 norm  ffl we give an explicit embedding of l  n  2 into l  n  O(log n) 1  with distortion (1 + 1=n  \Theta(1)  ) and a nonconstructive embedding of l  n  2 into l  O(n)  1 with distortion  (1 + ffl) such that the embedding can be represented using only O(n log  2  n) bits (as opposed to at least...
267|Mining time-changing data streams|Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a station-ary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying pro-cesses generating them changed during this time, sometimes radically. Although a number of algorithms have been pro-posed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes ques-tionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.
268|Continuous Queries over Data Streams|In many recent applications, data may take the form of continuous data streams, rather than finite stored data sets. Several aspects of data management need to be re-considered in the presence of data streams, offering a new research direction for the database community. In this pa-per we focus primarily on the problem of query processing, specifically on how to define and evaluate continuous queries over data streams. We address semantic issues as well as efficiency concerns. Our main contributions are threefold. First, we specify a general and flexible architecture for query processing in the presence of data streams. Second, we use our basic architecture as a tool to clarify alternative semantics and processing techniques for continuous queries. The architecture also captures most previous work on continuous queries and data streams, as
270|Fjording the Stream: An Architecture for Queries over Streaming Sensor Data|If industry visionaries are correct, our lives will soon be full of sensors, connected together in loose conglomerations via wireless networks, each monitoring and collecting data about the environment at large. These sensors behave very differently from traditional database sources: they have intermittent connectivity, are limited by severe power constraints, and typically sample periodically and push immediately, keeping no record of historical information. These limitations make traditional database systems inappropriate for queries over sensors. We present the Fjords architecture for managing multiple queries over many sensors, and show how it can be used to limit sensor resource demands while maintaining high query throughput. We evaluate our architecture using traces from a network of traffic sensors deployed on Interstate 80 near Berkeley and present performance results that show how query throughput, communication costs, and power consumption are necessarily coupled in sensor environments.
271|Continuously Adaptive Continuous Queries over Streams|We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive crossquery sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators -- both selections and join state -- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.
272|Multiple-Query Optimization|Some recently proposed extensions to relational database systems, as well as to deductive database systems, require support for multiple-query processing. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions may expand to more than one actual query that has to be run over the database. It is an interesting problem then to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of the paper is a systematic look at the problem, along with the presentation and analysis of algorithms that can be used for multiple-query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably.
273|Trajectory Sampling for Direct Traffic Observation|Traffic measurement is a critical component for the control and engineering of communication networks. We argue that traffic measurement should make it possible to obtain the spatial flow of traffic through the domain, i.e., the paths followed by packets between any ingress and egress point of the domain. Most resource allocation and capacity planning tasks can benefit from such information. Also, traffic measurements should be obtained without a routing model and without knowledge of network state. This allows the traffic measurement process to be resilient to network failures and state uncertainty.  We propose a method that allows the direct inference of traffic flows through a domain by observing the trajectories of a subset of all packets traversing the network. The key advantages of the method are that (i) it does not rely on routing state, (ii) its implementation cost is small, and (iii) the measurement reporting traffic is modest and can be controlled precisely. The key idea of the method is to sample packets based on a hash function computed over the packet content. Using the same hash function will yield the same sample set of packets in the entire domain, and enables us to reconstruct packet trajectories.  I. 
274|  Wavelet-Based Histograms for Selectivity Estimation |Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations. In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data values give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using
275|Surfing wavelets on streams: One-pass summaries for approximate aggregate queries|Abstract We present techniques for computing small spacerepresentations of massive data streams. These are inspired by traditional wavelet-based approx-imations that consist of specific linear projections of the underlying data. We present general&amp;quot;sketch &amp;quot; based methods for capturing various linear projections of the data and use them to pro-vide pointwise and rangesum estimation of data streams. These methods use small amounts ofspace and per-item time while streaming through the data, and provide accurate representation asour experiments with real data streams show.
276|Space-Efficient Online Computation of Quantile Summaries|An &amp;epsilon;-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of &amp;epsilon;N . We present a new online...
277| Approximate Computation of Multidimensional Aggregates of Sparse Data Using Wavelets |Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries. In this paper, we present anovel method that provides approximate answers to high-dimensional OLAP aggregation queries in massive sparse data sets in a time-efficient and space-efficient manner. We construct a compact data cube, which is an approximate and space-efficient representation of the underlying multidimensional array, based upon a multiresolution wavelet decomposition. In the on-line phase, each aggregation query can generally be answered using the compact data cube in one I/O or a small number of I/Os, depending upon the desired accuracy. We present two I/O-efficient algorithms to construct the compact data cube for the important case of sparse high-dimensional arrays, which often arise in practice. The traditional histogram methods are infeasible for the massive high-dimensional data sets in OLAP applications. Previously developed wavelet techniques are efficient only for dense data. Our on-line query processing algorithm is very fast and capable of refining answers as the user demands more accuracy. Experiments on real data show that our method provides significantly more accurate results for typical OLAP aggregation queries than other efficient approximation techniques such as random sampling.
278|Computing on Data Streams|In this paper we study the space requirement of algorithms that make  only one (or a small number of) pass(es) over the input data. We study such  algorithms under a model of data streams that we introduce here. We give  a number of upper and lower bounds for problems stemming from queryprocessing,  invoking in the process tools from the area of communication  complexity.
279|Processing Complex Aggregate Queries over Data Streams|Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (i.e., data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed.
280|Continual Queries for Internet Scale Event-Driven Information Delivery|In this paper we introduce the concept of continual queries, describe the design of a distributed  event-driven continual query system -- OpenCQ, and outline the initial implementation of OpenCQ  on top of the distributed interoperable information mediation system DIOM [21, 19]. Continual  queries are standing queries that monitor update of interest and return results whenever the update  reaches specified thresholds. In OpenCQ, users may specify to the system the information they would  like to monitor (such as the events or the update thresholds they are interested in). Whenever the  information of interest becomes available, the system immediately delivers it to the relevant users;  otherwise, the system continually monitors the arrival of the desired information and pushes it to the  relevant users as it meets the specified update thresholds. In contrast to conventional pull-based data  management systems such as DBMSs and Web search engines, OpenCQ exhibits two important featu...
281|Join synopses for approximate query answering|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses (join samples) as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known. We also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations. One of our key contributions is a detailed analysis of the error bounds obtained for approximate answers that demonstrates the trade-offs in various methods, as well as the advantages in certain scenarios of a new subsampling method we propose. Our extensive set of experiments on the TPC-D benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper. 1
282|An efficient cost-driven index selection tool for Microsoft SQL Server|In this paper we describe novel techniques that make it possible to build an industrial-strength tool for automating the choice of indexes in the physical design of a SQL database. The tool takes as input a workload of SQL queries, and suggests a set of suitable indexes. We ensure that the indexes chosen are effective in reducing the cost of the workload by keeping the index selection tool and the query optimizer &amp;quot;in step&amp;quot;. The number of index sets that must be evaluated to find the optimal configuration is very large. We reduce the complexity of this problem using three techniques. First, we remove a large number of spurious indexes from consideration by taking into account both query syntax and cost information. Second, we introduce optimizations that make it possible to cheaply evaluate the “goodness ” of an index set. Third, we describe an iterative approach to handle the complexity arising from multicolumn indexes. The tool has been implemented on Microsoft SQL Server 7.0. We performed extensive experiments over a range of workloads, including TPC-D. The results indicate that the tool is efficient and its choices are close to optimal. 1.
283|Reductions in Streaming Algorithms, with an Application to Counting Triangles in Graphs |We introduce reductions in the streaming model as a tool in the design of streaming algorithms. We develop
284|Computing iceberg queries efficiently|Many applications compute aggregate functions...
285|Data-Streams and Histograms|Histograms have been used widely to capture data distribution, to represent the data by a small number of step functions. Dynamic programming algorithms which provide optimal construction of these histograms exist, albeit running in quadratic time and linear space. In this paper we provide linear time construction of 1 + epsilon approximation of optimal histograms, running in polylogarithmic space. Our results extend to the context of data-streams, and in fact generalize to give 1 + epsilon approximation of several problems in data-streams which require partitioning the index set into intervals. The only assumptions required are that the cost of an interval is monotonic under inclusion (larger interval has larger cost) and that the cost can be computed or approximated in small space. This exhibits a nice class of problems for which we can have near optimal data-stream algorithms.
286|XJoin: A Reactively-Scheduled Pipelined Join Operator|Wide-area distribution raises significant performance problems for traditional query processing techniques as data access becomes less predictable due to link congestion, load imbalances, and temporary outages. Pipelined query execution is a promising approach to coping with unpredictability in such environments as it allows scheduling to adjust to the arrival properties of the data. We have developed a non-blocking join operator, called XJoin, which has a small memory footprint, allowing many such operators to be active in parallel. XJoin is optimized to produce initial results quickly and can hide intermittent delays in data arrival by reactively scheduling background processing. We show that XJoin is an effective solution for providing fast query responses to users even in the presence of slow and bursty remote sources.  
287|Rate-Based Query Optimization for Streaming Information Sources|Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences. We present a preliminary validation of our rate-based optimization framework on a prototype XML query engine, though it is generic enough to be used in other database contexts. The results show that rate-based optimization is feasible and can indeed yield correct decisions.
288|Making Views Self-Maintainable for Data Warehousing|A data warehouse stores materialized views over data from one or more sources in order to provide fast access to the integrated data, regardless of the availability of the data sources. Warehouse views need to be maintained inresponse to changes to the base data in the sources. Except for very simple views, maintaining a warehouse view requires access to data that is not available in the view itself. Hence, to maintain the view, one either has to query the data sources or store auxiliary data in the warehouse. We show that by using key and referential integrity constraints, we often can maintain a select-project-join view without going to the data sources or replicating the base relations in their entirety in the warehouse. We derive a set of auxiliary views such that the warehouse view and the auxiliary views together are self-maintainable|they can be maintained without going to the data sources or replicating all base data. In addition, our technique can be applied to simplify traditional materialized view maintenance by exploiting key and referential integrity constraints. 1
289|Approximate Medians and other Quantiles in One Pass and with Limited Memory|We present new algorithms for computing approximate quantiles of large datasets in a single pass. The approximation guarantees are explicit, and apply without regard to the value distribution or the arrival distributions of the dataset. The main memory requirements are smaller than those reported earlier by an order of magnitude.  We also discuss methods that couple the approximation algorithms with random sampling to further reduce memory requirements. With sampling, the approximation guarantees are explicit but probabilistic, i.e., they apply with respect to a (user controlled) confidence parameter.  We present the algorithms, their theoretical analysis and simulation results.  1 Introduction  This article studies the problem of computing order statistics  of large sequences of online or disk-resident data using as little main memory as possible. We focus on computing  quantiles, which are elements at specific positions in the sorted order of the input. The OE-quantile, for OE 2 [0; ...
290|Random Sampling for Histogram Construction: How much is enough?|Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining &#034;How much sampling is enough?&#034; We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a  conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose ...
291|Sampling From a Moving Window Over Streaming Data|We introduce the problem of sampling from a moving window of recent items from a data stream and develop the &#034;chain-sample&#034; and &#034;priority-sample&#034; algorithms for this problem.
292|Tracking join and self-join sizes in limited storage|This paper presents algorithms for tracking (approximate) join and self-join sizes in limited storage, in the presence of insertions and deletions to the data set(s). Such algorithms detect changes in join and self-join sizes without an expensive recomputation from the base data, and without the large space overhead required to maintain such sizes exactly. Query optimizers rely on fast, high-quality estimates of join sizes in order to select between various join plans, and estimates of self-join sizes are used to indicate the degree of skew in the data. For self-joins, we considertwo approaches proposed in [Alon, Matias, and Szegedy. The Space Complexity of Approximating the Frequency Moments. JCSS, vol. 58, 1999, p.137-147], which we denote tug-of-war and sample-count. Wepresent fast algorithms for implementing these approaches, and extensions to handle deletions as well as insertions. We also report on the rst experimental study of the two approaches, on a range of synthetic and real-world data sets. Our study shows that tug-of-war provides more accurate estimates for a given storage limit than sample-count, which in turn is far more accurate than a standard sampling-based approach. For example, tug-of-war needed only 4{256 memory words, depending on the data set, in order to estimate the self-join size
293|Data Cube Approximation and Histograms via Wavelets (Extended Abstract)  (1998) |) Jeffrey Scott Vitter   Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  jsv@cs.duke.edu Min Wang  y Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  minw@cs.duke.edu Bala Iyer  Database Technology Institute IBM Santa Teresa Laboratory P.O. Box 49023 San Jose, CA 95161 USA  balaiyer@vnet.ibm.com Abstract  There has recently been an explosion of interest in the analysis of data in data warehouses in the field of On-Line Analytical Processing (OLAP). Data warehouses can be extremely large, yet obtaining quick answers to queries is important. In many situations, obtaining the exact answer to an OLAP query is prohibitively expensive in terms of time and/or storage space. It can be advantageous to have fast, approximate answers to queries. In this paper, we present an I/O-efficient technique based upon a multiresolution wavelet decomposition that yields an approximate a...
294|The design and implementation of a sequence database system|This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. There are three distinct contributions made in this paper. (1) We describe the specification of sequence queries using the SEQUIN query language. (2) We quantitatively demonstrate the importance of various storage and optimization techniques by studying their effect on performance. (3) We present a novel nested design paradigm used in PREDATOR to combine sequence ‘and relational data.
295|Random sampling techniques for space efficient online computation of order statistics of large datasets|In a recent paper [MRL98], we had described a general framework for single pass approximate quantile nding algorithms. This framework included several known algorithms as special cases. We had identi ed a new algorithm, within the framework, which had a signi cantly smaller requirement for main memory than other known algorithms. In this paper, we address two issues left open in our earlier paper. First, all known and space e cient algorithms for approximate quantile nding require advance knowledge of the length of the input sequence. Many important database applications employing quantiles cannot provide this information. In this paper, we present anovel non-uniform random sampling scheme and an extension of our framework. Together, they form the basis of a new algorithm which computes approximate quantiles without knowing the input sequence length. Second, if the desired quantile is an extreme value (e.g., within the top 1 % of the elements), the space requirements of currently known algorithms are overly pessimistic. We provide a simple algorithm which estimates extreme values using less space than required by the earlier more general technique for computing all quantiles. Our principal observation here is that random sampling is quanti ably better when estimating extreme values than is the case with the median.  
296|Characterizing Memory Requirements for Queries over Continuous Data|This paper deals with continuous conjunctive queries with arithmetic comparisons and optional aggregation over multiple data streams. An algorithm is presented for determining whether or not any given query can be evaluated using a bounded amount of memory for all possible instances of the data streams. For queries that can be evaluated using bounded memory, an execution strategy based on constant-sized synopses of the data streams is proposed. For queries that cannot be evaluated using bounded memory, data stream scenarios are identified in which evaluating the queries requires memory linear in the size of the unbounded streams
297|Congressional samples for approximate answering of group-by queries|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex decision support queries using precomputed summary statistics, such as samples. Decision support queries routinely segment the data into groups and then aggregate the information in each group (group-by queries). Depending on the data, there can be a wide disparity between the number of data items in each group. As a result, approximate answers based on uniform random samples of the data can result in poor accuracy for groups with very few data items, since such groups will be represented in the sample by very few (often zero) tuples. In this paper, we propose a general class of techniques for obtaining fast, highly-accurate answers for group-by queries. These techniques rely on precomputed non-uniform (biased) samples of the data. In particular, we proposecongressional samples, ahybrid union of uniform and biased samples. Given a xed amount of space, congressional samples seek to maximize the accuracy for all possible group-by queries on a set of columns. We present a one pass algorithm for constructing a congressional sample and use this technique to also incrementally maintain the sample up-to-date without accessing the base relation. We also evaluate query rewriting strategies for providing approximate answers from congressional samples. Finally, we conduct an extensive set of experiments on the TPC-D database, which demonstrates the e cacy of the techniques proposed. 1
298|Histogram-Based Approximation of Set-Valued Query Answers|Answering queries approximately has recently  been proposed as a way to reduce query response  times in on-line decision support systems,  when the precise answer is not necessary  or early feedback is helpful. Most of the  work in this area uses sampling-based techniques  and handles aggregate queries, ignoring  queries that return relations as answers. In  this paper, we extend the scope of approximate  query answering to general queries. We propose  a novel and intuitive error measure for  quantifying the error in an approximate query  answer, which can be a multiset in general.
299|Data Integration using Self-Maintainable Views|.  In this paper we de#ne the concept of self-maintainable views  # these are views that can be maintained using only the contents of  the view and the database modi#cations, without accessing any of the  underlying databases. We derive tight conditions under which several  types of select-project-join are self-maintainable upon insertions, deletions  and updates. Self-Maintainability is a desirable property for e#-  ciently maintaining large views in applications where fast response and  high availability are important. One example of suchanenvironment  is data warehousing wherein views are used for integrating data from  multiple databases.  1 Introduction  Most large organizations have related data in distinct databases. Many of these databases may be legacy systems, or systems separated for organizational reasons like funding and ownership. Integrating data from such distinct databases is a pressing business need. A common approach for integration is to de#ne an integrated view and...
300|Monitoring XML Data on the Web|We consider the monitoring of a flow of incoming documents. More precisely, we present here the monitoring used in a very large warehouse built from XML documents found on the web. The flow of documents consists in XML pages (that are warehoused) and HTML pages (that are not). Our contributions are the following:  ffl a subscription language which specifies the monitoring of pages when fetched, the periodical evaluation of continuous queries and the production of XML reports. ffl the description of the architecture of the system we implemented that makes it possible to monitor a flow of millions of pages per day with millions of subscriptions on a single PC, and scales up by using more machines. ffl a new algorithm for processing alerts that can be used in a wider context. We support
302|Expiring data in a warehouse|Data warehouses collect data into materi-alized views for analysis. After some time, some of the data may no longer be needed or may not be of interest. In this pa-per, we handle this by expiring or remov-ing unneeded materialized view tuples. A framework supporting such expiration is presented. Within it, a user or adminis-trator can declaratively request expirations and can specify what type of modifications are expected from external sources. The lat-ter can significantly increase the amount of data that can be expired. We present effi-cient algorithms for determining what data can be expired (data not needed for main-tenance of other views), taking into account the types of updates that may occur. 1
304|Sequence Query Processing|Many applications require the ability to manipulate sequences of data. We motivate the importance of sequence query processing, and present a framework for the optimization of sequence queries based on several novel techniques. These include query transformations, optimizations that utilize meta--data, and caching of intermediate results. We present a bottom-up algorithm that generates an efficient query evaluation plan based on cost estimates. This work also identifies a number of directions in which future research can be directed.  1 Introduction  Many real life applications manipulate data that is inherently sequential. Such data is logically viewed and queried in terms of a sequence abstraction and is often physically stored as a sequence. Databases should (a) allow sequences to be queried in a declarative manner, utilizing the ordered semantics of the data, and (b) take advantage of the opportunities available for query optimization. Relational databases are inadequate in this re...
305|SEQ: A Model for Sequence Databases|This paper presents the model which is the basis for a system to manage various kinds of sequence data. The model separates the data from the ordering information, and includes operators based on two distinct abstractions of a sequence. The main contributions of the model are: (a) it can deal with different types of sequence data, (b) it supports an expressive range of sequence queries, (c) it draws from many of the diverse existing approaches to modeling sequence data. 1
306|Sampling Algorithms: Lower Bounds and Applications (Extended Abstract)  (2001) |]  Ziv Bar-Yossef  y  Computer Science Division  U. C. Berkeley  Berkeley, CA 94720  zivi@cs.berkeley.edu  Ravi Kumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  ravi@almaden.ibm.com  D. Sivakumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  siva@almaden.ibm.com  ABSTRACT  We develop a framework to study probabilistic sampling algorithms that approximate general functions of the form f : A  n  ! B, where A and B are arbitrary sets. Our goal is to obtain lower bounds on the query complexity of functions, namely the number of input variables x i that any sampling algorithm needs to query to approximate f(x1 ; : : : ; xn ).  We define two quantitative properties of functions --- the block sensitivity and the minimum Hellinger distance --- that give us techniques to prove lower bounds on the query complexity. These techniques are quite general, easy to use, yet powerful enough to yield tight results. Our applications include the mean and higher statistical moments, the median and other selection functions, and the frequency moments, where we obtain lower bounds that are close to the corresponding upper bounds.  We also point out some connections between sampling and streaming algorithms and lossy compression schemes.  1. 
307|Approximating a Data Stream for Querying and Estimation: Algorithms and Performance Evaluation|Obtaining fast and good quality approximations to data distributions is a problem of central interest to database management. A variety of popular database applications including, approximate querying, similarity searching and data mining in most application domains, rely on such good quality approximations. Histogram based approximation is a very popular method in database theory and practice to succinctly represent a data distribution in a space efficient manner. In this paper, we place the problem of histogram construction into perspective and we generalize it by raising the requirement of a finite data set and/or known data set size. We consider the case of an infinite data set on which data arrive continuously forming an infinite data stream. In this context, we present the first single pass algorithms capable of constructing histograms of provable good quality. We present algorithms for the fixed window variant of the basic histogram construction problem, supporting incremental maintenance of the histograms. The proposed algorithms trade accuracy for speed and allow for a graceful tradeoff between the two, based on application requirements. In the case of approximate queries on infinite data streams, we present a detailed experimental evaluation comparing our algorithms with other applicable techniques using real data sets, demonstrating the superiority of our proposal. 1
308|A robust, optimization-based approach for approximate answering of aggregate queries|The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are “similar ” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work. 1
309|Online Dynamic Reordering for Interactive Data Processing|Abstract We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing.
311|On Sampling and Relational Operators|A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. We highlight the primary difficulties, summarize the results of some recent work in this area, and indicate directions for future work.   
312|Scale and performance in a distributed file system|The Andrew File System is a location-transparent distributed tile system that will eventually span more than 5000 workstations at Carnegie Mellon University. Large scale affects performance and complicates system operation. In this paper we present observations of a prototype implementation, motivate changes in the areas of cache validation, server process structure, name translation, and low-level storage representation, and quantitatively demonstrate Andrew’s ability to scale gracefully. We establish the importance of whole-file transfer and caching in Andrew by comparing its performance with that of Sun Microsystem’s NFS tile system. We also show how the aggregation of files into volumes improves the operability of the system.
313|Serverless Network File Systems|In this paper, we propose a new paradigm for network file system design, serverless network file systems. While traditional network file systems rely on a central server machine, a serverless system utilizes workstations cooperating as peers to provide all file system services. Any machine in the system can store, cache, or control any block of data. Our approach uses this location independence, in combination with fast local area networks, to provide better performance and scalability than traditional file systems. Further, because any machine in the system can assume the responsibilities of a failed component, our serverless design also provides high availability via redundant data storage. To demonstrate our approach, we have implemented a prototype serverless network file system called xFS. Preliminary performance measurements suggest that our architecture achieves its goal of scalability. For instance, in a 32-node xFS system with 32 active clients, each client receives nearly as much read or write throughput as it would see if it were the only active client. 
314|A cost-effective, high-bandwidth storage architecture|(NASD) storage architecture, prototype implementations oj NASD drives, array management for our architecture, and three,filesystems built on our prototype. NASD provides scal-able storage bandwidth without the cost of servers used primarily,fijr trut&amp;rring data from peripheral networks (e.g. SCSI) to client networks (e.g. ethernet). Increasing datuset sizes, new attachment technologies, the convergence of peripheral and interprocessor switched networks, and the increased availability of on-drive transistors motivate and enable this new architecture. NASD is based on four main principles: direct transfer to clients, secure interfaces via cryptographic support, asynchronous non-critical-path oversight, and variably-sized data objects. Measurements of our prototype system show that these services can be cost-#ectively integrated into a next generation disk drive ASK. End-to-end measurements of our prototype drive andfilesys-terns suggest that NASD cun support conventional distrib-uted filesystems without per$ormance degradation. More importantly, we show scaluble bandwidth for NASD-special-ized filesystems. Using a parallel data mining application, NASD drives deliver u linear scaling of 6.2 MB/s per client-drive pair, tested with up to eight pairs in our lab.
316|Cluster I/O with River: Making the Fast Case Common|We introduce River, a data-flow programming environment and I/O substrate for clusters of computers. River is designed to provide maximum performance in the common case --- even in the face of nonuniformities in hardware, software, and workload. River is based on two simple design features: a high-performance distributed queue, and a storage redundancy mechanism called graduated declustering. We have implemented a number of data-intensive applications on River, which validate our design with near-ideal performance in a variety of non-uniform performance scenarios. 
317|Swift: Using distributed disk striping to provide high I/O data rates|We present an I/O architecture, called Swift, that addresses the problem of data rate mismatches between the requirements of an application, storage devices, and the interconnection medium. The goal of Swift is to support high data rates in general purpose distributed systems. Swift uses a high-speed interconnection medium to provide high data rate transfers by using multiple slower storage devices in parallel. It scales well when using multiple storage devices and interconnections, and can use any appropriate storage technology, including high-performance devices such as disk arrays. To address the problem of partial failures, Swift stores data redundantly. Using the UNIX operating system, we have constructed a simplified prototype of the Swift architecture. The prototype provides data rates that are significantly faster than access to the local SCSI disk, limited by the capacity of a single Ethernet segment, or in the case of multiple Ethernet segments by the ability of the client to drive them. We have constructed a simulation model to demonstrate how the Swift architecture can exploit advances in processor, communication and storage technology. We consider the effects of processor speed, interconnection capacity, and multiple storage agents on the utilization of the components and the data rate of the system. We show that the data rates scale well in the number of storage devices, and that by replacing the most highly stressed components by more powerful ones the data rates of the entire system increase significantly.
318|The Global File System|The Global File System (GFS) is a prototype design for a distributed file system in which cluster nodes physically share storage devices connected via a network like Fibre Channel. Networks and network attached storage devices have advanced to a level of performance and extensibility that the once believed disadvantages of “shared disk ” architectures are no longer valid. This shared storage architecture attempts to exploit the sophistication of device technologies where as the client–server architecture diminishes a device’s role to a simple components. GFS distributes the file system responsibilities across the processing nodes, storage across the devices, and file system resources across the entire storage pool. GFS caches data on the storage devices instead of the main memories of the machines. Consistency is established by using a locking mechanism maintained by the storage device controllers to facilitate atomic read–modify– write operations. The locking mechanism is being prototyped on Seagate disks drives and Ciprico disk arrays. GFS is implemented in the Silicon Graphics IRIX operating system and is accessed using standard Unix commands and utilities.
319|Reliable Communication in the Presence of Failures|The design and correctness of a communication facility for a distributed computer system are reported on. The facility provides support for fault-tolerant process groups in the form of a family of reliable multicast protocols that can be used in both local- and wide-area networks. These protocols attain high levels of concurrency, while respecting application-specific delivery ordering constraints, and have varying cost and performance that depend on the degree of ordering desired. In particular, a protocol that enforces causal delivery orderings is introduced and shown to be a valuable alternative to conventional asynchronous communication protocols. The facility also ensures that the processes belonging to a fault-tolerant process group will observe consistent orderings of events affecting the group as a whole, including process failures, recoveries, migration, and dynamic changes to group properties like member rankings. A review of several uses for the protocols in the ISIS system, which supports fault-tolerant resilient objects and bulletin boards, illustrates the significant simplification of higher level algorithms made possible by our approach.
320|Time, Clocks, and the Ordering of Events in a Distributed System|The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.  
321|Fail-Stop Processors: An Approach to Designing Fault-Tolerant Computing Systems|This paper was originally submitted to ACM Transactions on Programming Languages and Systems. The responsible editor was Susan L. Graham. The authors and editor kindly agreed to transfer the paper to the ACM Transactions on Computer Systems
322|Atomic Broadcast: From Simple Message Diffusion to Byzantine Agreement|In distributed systems subject to random communication delays and component failures, atomic broadcast can be used to implement the abstraction of synchronous replicated storage, a distributed storage that displays the same contents at every correct processor as of any clock time. This paper presents a systematic derivation of a family of atomic broadcast protocols that are tolerant of increasingly general failure classes: omission failures, timing failures, and authentication-detectable Byzantine failures. The protocols work for arbitrary point-to-point network topologies, and can tolerate any number of link and process failures up to network partitioning. After proving their correctness, we also prove two lower bounds that show that the protocols provide in many cases the best possible termination times. Keywords and phrases: Atomic Broadcast, Byzantine Agreement, Computer Network, Correctnesss, Distributed System, Failure Classification, Fault-Tolerance, Lower Bound, Real-Time Syste...
323|Maintaining Availability in Partitioned Replicated Databases|In a replicated database, a data item may have copies residing on several sites. A replica control protocol is necessary to ensure that data items with several copies behave as if they consist of a single copy, as far as users can tell. We describe a new replica control protocol that allows the accessing of data in spite of site failures and network partitioning. This protocol provides the database designer with a large degree of flexibility in deciding the degree of data availability, as well as the cost of accessing data.
324|Agile Application-Aware Adaptation for Mobility|In this paper we show that application-aware adaptation, a collaborative partnership between the operating system and applications, offers the most general and effective approach to mobile information access. We describe the design of Odyssey, a prototype implementing this approach, and show how it supports concurrent execution of diverse mobile applications. We identify agility as a key attribute of adaptive systems, and describe how to quantify and measure it. We present the results of our evaluation of Odyssey, indicating performance improvements up to a factor of 5 on a benchmark of three applications concurrently using remote services over a network with highly variable bandwidth.  
325|The JPEG still picture compression standard|This paper is a revised version of an article by the same title and author which appeared in the April 1991 issue of Communications of the ACM. For the past few years, a joint ISO/CCITT committee known as JPEG (Joint Photographic Experts Group) has been working to establish the first international compression standard for continuous-tone still images, both grayscale and color. JPEG’s proposed standard aims to be generic, to support a wide variety of applications for continuous-tone images. To meet the differing needs of many applications, the JPEG standard includes two basic compression methods, each with various modes of operation. A DCT-based method is specified for “lossy’ ’ compression, and a predictive method for “lossless’ ’ compression. JPEG features a simple lossy technique known as the Baseline method, a subset of the other DCT-based modes of operation. The Baseline method has been by far the most widely implemented JPEG method to date, and is sufficient in its own right for a large number of applications. This article provides an overview of the JPEG standard, and focuses in detail on the Baseline method. 1
326|Disconnected Operation in the Coda File System|Disconnected operation is a mode of operation that enables a client to continue accessing critical data during temporary failures of a shared data repository. An important, though not exclusive, application of disconnected operation is in supporting portable computers. In this paper, we show that disconnected operation is feasible, efficient and usable by describing its design and implementation in the Coda File System. The central idea behind our work is that caching of data, now widely used for performance, can also be exploited to improve availability.
327|Receiver-driven Layered Multicast|State of the art, real-time, rate-adaptive, multimedia applications adjust their transmission rate to match the available network capacity. Unfortunately, this source-based rate-adaptation performs poorly in a heterogeneous multicast environment because there is no single target rate — the conflicting bandwidth requirements of all receivers cannot be simultaneously satisfied with one transmission rate. If the burden of rate-adaption is moved from the source to the receivers, heterogeneity is accommodated. One approach to receiver-driven adaptation is to combine a layered source coding algorithm with a layered transmission system. By selectively forwarding subsets of layers at constrained network links, each user receives the best quality signal that the network can deliver. We and others have proposed that selective-forwarding be carried out using multiple IP-Multicast groups where each receiver specifies its level of subscription by joining a subset of the groups. In this paper, we extend the multiple group framework with a rate-adaptation protocol called Receiver-driven Layered Multicast, or RLM. Under RLM, multicast receivers adapt to both the static heterogeneity of link bandwidths as well as dynamic variations in network capacity (i.e., congestion). We describe the RLM protocol and evaluate its performance with a preliminary simulation study that characterizes user-perceived quality by assessing loss rates over multiple time scales. For the configurations we simulated, RLM results in good throughput with transient short-term loss rates on the order of a few percent and long-term loss rates on the order of one percent. Finally, we discuss our implementation of a software-based Internet video codec and its integration with RLM.
328|Coda: A Highly available File System for a Distributed Workstation Environment|Abstract- Coda is a file system for a large-scale distributed computing environment composed of Unix workstations. It provides resiliency to server and network failures through the use of two distinct but complementary mechanisms. One mechanism, server replication, stores copies of a file at multiple servers. The other mechanism, disconnected operation, is a mode of execution in which a caching site temporarily assumes the role of a replication site. Disconnected operation is particularly useful for supporting portable workstations. The design of Coda optimizes for availability and performance, and strives to provide the highest degree of consistency attainable in the light of these objectives. Measurements from a prototype show that the performance cost of providing high availability in Coda is reasonable. Index Terms- Andrew, availability, caching, disconnected operation, distributed file system, performance, portable computers, scalability, server replication. I.
329|Managing Update Conflicts in Bayou, a Weakly Connected Replicated Storage System|Bayou is a replicated, weakly consistent storage system designed for a mobile computing environment that includes portable machines with less than ideal network connectivity. To maximize availability, users can read and write any accessible replica. Bayou&#039;s design has focused on supporting apphcation-specific mechanisms to detect and resolve the update conflicts that naturally arise in such a system, ensuring that replicas move towards eventual consistency, and defining a protocol by which the resolution of update conflicts stabilizes. It includes novel methods for conflict detection, called dependency checks, and per-write conflict resolution based on client-provided merge procedures. To guarantee eventual consistency, Bayou servers must be able to rollback the effects of previously executed writes and redo them according to a global senalization order. Furthermore, Bayou permits clients to observe the results of all writes received by a server, Including tentative writes whose conflicts have not been ultimately resolved. This paper presents the motivation for and design of these mechanisms and describes the experiences gained with an initial implementation of the system.
330|Vnodes: An architecture for multiple file system types|sun!srk
331|Adapting to Network and Client Variability via On-Demand Dynamic Distillation|The explosive growth of the Internet and the proliferation of smart cellular phones and handheld wireless devices is widening an already large gap between Internet clients. Clients vary in their hardware resources, software sophistication, and quality of connectivity, yet server support for client variation ranges from relatively poor to none at all. In this paper we introduce some design principles that we believe are fundamental to providing “meaningful” Internet access for the entire range of clients. In particular, we show how to perform on-demand datatype-specific lossy compression on semantically typed data, tailoring content to the specific constraints of the client. We instantiate our design principles in a proxy architecture that further exploits typed data to enable application-level management of scarce network resources. Our proxy architecture generalizes previous work addressing all three aspects of client variation by applying well-understood techniques in a novel way, resulting in quantitatively better end-to-end performance, higher quality display output, and new capabilities for lowend clients. 1
333|Rover: A Toolkit for Mobile Information Access|The Rover toolkit combines relocatable dynamic objects and queued remote procedure calls to provide unique services for &#034;roving&#034; mobile applications. A relocatable dynamic object is an object with a well-defined interface that can be dynamically loaded into a client computer from a server computer (or vice versa) to reduce clientserver communication requirements. Queued remote procedure call is a communication system that permits applications to continue to make non-blocking remote procedure call requests even when a host is disconnected, with requests and responses being exchanged upon network reconnection. The challenges of mobile environments include intermittent connectivity, limited bandwidth, and channeluse optimization. Experimental results from a Rover-based mail reader, calendar program, and two non-blocking versions of WorldWide Web browsers show that Rover&#039;s services are a good match to these challenges. The Rover toolkit also offers advantages for workstation applications by providing a uniform distributed object architecture for code shipping, object caching, and asynchronous object invocation.
334|World-Wide Web Proxies|A WWW proxy server, proxy for short, provides access to the Web for people on closed subnets who can only access the Internet through a firewall machine. The hypertext server developed at CERN, cern_httpd, is capable of running as a proxy, providing seamless external access to HTTP, Gopher, WAIS and FTP. cern_httpd has had gateway features for a long time, but only this spring they were extended to support all the methods in the HTTP protocol used by WWW clients. Clients don&#039;t lose any functionality by going through a proxy, except special processing they may have done for nonnative Web protocols such as Gopher and FTP. A brand new feature is caching performed by the proxy, resulting in shorter response times after the first document fetch. This makes proxies useful even to the people who do have full Internet access and don&#039;t really need the proxy just to get out of their local subnet. This paper gives an overview of proxies and reports their current status.  1.0 Introduction  The pri...
335|Mobile Information Access|The ability to access information on demand when mobile will be a critical capability in the 21st century. In this paper, we examine the fundamental forces at work in mobile computing systems and explain how they constrain the problem of mobile information access. From these constraints, we derive the importance of adaptivity as a crucial requirement of mobile clients. We then develop a taxonomy of adaptation strategies, and summarize our research in application-transparent and  application-aware adaptation in the Coda and Odyssey systems respectively.  
336|Processor Capacity Reserves for Multimedia Operating Systems|Multimedia applications have timing requirements that cannot generally be satisfied using time-sharing scheduling algorithms and system structures. To effectively support these types of programs, operating systems must support processor capacity reservation. A capacity reservation and enforcement mechanism isolates programs from the timing and execution characteristics of other programs in the same way that a memory protection system isolates programs from memory access by other programs. In this paper, we characterize the timing requirements and processor capacity reservation requirements for multimedia applications, we describe a scheduling framework to support reservation and admission control, and we introduce a novel reserve abstraction, specifically designed for the microkernel architecture, for controlling processor usage. 
337|Trace-Based Mobile Network Emulation|Subjecting a mobile computing system to wireless network conditions that are realistic yet reproducible is a challenging problem. In this paper, we describe a technique called trace modulation that re-creates the observed end-to-end characteristics of a real wireless network in a controlled and repeatable manner. Trace modulation is transparent to applications and accounts for all network traffic sent or received by the system under test. We present results that show that it is indeed capable of reproducing wireless network performance faithfully.  1 Introduction  How does one subject a mobile computing system to realistic yet reproducible wireless networking conditions? Reproducible behavior is important for three reasons. First, it is essential for thorough evaluation of the performance of a mobile computing system. Second, it is necessary for comparative evaluations of alternative system designs. Third, it is valuable in debugging mobile systems because it enables the re-creation of...
338|The Case for Wireless Overlay Networks|Wireless data services, other than those for electronic mail or paging, have thus far been more promising than successful. We believe that future mobile information systems must be built upon heterogeneous wireless overlay networks&#039;, extending traditional wired and internetworked processing &#034;islands&#034; to hosts on the move over coverage areas ranging from in-room, in-building, campus, metropolitan, and wide-areas. Unfortunately, network planners continue to think in terms of homogeneous wireless communications systems and technologies. In this paper, we describe a new wireless data networking architecture that integrates diverse wireless technologies into a seamless internetwork. In addition, we describe the applications support services needed to make it possible for applications to continue to operate as mobile hosts roam across such networks. The architecture described herein is being implemented in a testbed at the University of California, Berkeley under joint government/industry sponsorship.
339|MOBISAIC: An Information System for A Mobile Wireless Computing Environment|Mobisaic is a World Wide Web information system designed to serve users in a mobile wireless computing environment. Mobisaic extends the Web by allowing documents to both refer and react to potentially changing contextual information, such as current location in the wireless network. Mobisaic relies on clientside processing of HTML documents that support two new concepts: Dynamic Uniform Resource Locators (URLs) and Active Documents. A dynamic URL is one whose results depend upon the state of the user&#039;s mobile context at the time it is resolved. An active document is one that automatically updates its contents in response to changes in a user&#039;s mobile context. This paper describes the design of Mobisaic, the mechanism it uses for representing a user&#039;s mobile context, and the extensions made to the syntax and function of Uniform Resource Locators and HyperText Markup Language documents to support mobility. 
340|Interactive Translation of Conversational Speech|We present JANUS-II, a large scale system effort aimed at interactive spoken language translation. JANUS-II now accepts spontaneous conversational speech in a limited domain in English, German or Spanish and produces output in German, English, Spanish, Japanese and Korean. The challenges of coarticulated, disfluent, ill-formed speech are manifold, and have required advances in acoustic modeling, dictionary learning, language modeling, semantic parsing and generation, to achieve acceptable performance. A semantic “interlingua ” that represents the intended meaning of an input sentence, facilitates the generation of culturally and contextually appropriate translation in the presence of irrelevant or erroneous information. Application of statistical, contextual, prosodic and discourse constraints permits a progressively narrowing search for the most plausible interpretation of an utterance. During translation, JANUS-II produces paraphrases that are used for interactive correction of translation errors. Beyond our continuing efforts to improve robustness and accuracy, we have also begun to study possible forms of deployment. Several system prototypes have been implemented to explore translation needs in different settings: speech translation in one-on-one video conferencing, as portable mobile
341|Packet-Pair Flow Control|This paper presents the packet-pair rate-based feedback flow control scheme. This scheme is designed for networks where individual connections do not reserve bandwidth and for the available bitrate (best-effort) component of integrated networks. We assume a round-robin-like queue service discipline in the output queues of the network&#039;s switches, and propose a linear stochastic model for a single conversation in a network of such switches. These model motivates the Packet-Pair rate probing technique, which forms the basis for provably stable discrete and continuous time rate-based flow control schemes. We present a novel state estimation scheme based on fuzzy logic. We then address several practical concerns: dealing with system startup, retransmission and timeout strategy, and dynamic setpoint probing. We present a finite state machine as well as source code for a model implementation. The dynamics of a single source, the interactions of multiple sources, and the behavior of packet-pai...
342|Issues in Wireless Mobile Computing|le workstation as an X terminal. Currently, several companies sell a poor man&#039;s version of this --- the typical &#034;workstation&#034; is a small alphanumeric pager-like device which limits the user to reading and composing Email. Such an approach has its uses; however, there are some fundamental problems with the idea of building the world entirely with mobile X terminals. One problem is the very limited cellular bandwidth and its inefficient allocation by a circuit-switched, TDMA or FDMA phone network operating with a large cell size. Severe bandwidth constraints limit the number of simultaneous users, the nature of the applications that can be run, or both. (Presently in many areas callers are regularly denied service because no channel is available.) The present Email services may represent the limit of what can be done with analog phone service designed for voice. Another, hopefully temporary, problem with building on cellular telephone systems is that different systems are not always comp
343|Adaptive Service in Mobile Computing Environments|Emerging communication-intensive applications require significant levels of networking resources for efficient operation. In the context of mobile computing environments, limited and dynamically varying available resources, stringent application requirements, and user mobility make it difficult to provide sustained quality of service to applications. This paper addresses the above issues, and proposes a new service model in cellular mobile computing environments. The major results of this paper are the following: ffl An Adaptive Service model in mobile computing environments, which enables the network and applications to adaptively (re)negotiate QoS depending on dynamic network conditions ffl An algorithmic framework that provides cost-effective resource adaptation in the presence of resource and network dynamics ffl A unified implementation architecture for adaptive service support A brief overview of an ongoing testbed implementation is provided. 1 INTRODUCTION Recent years have ...
344|Replication in Ficus Distributed File Systems|Ficus is a replicated general filing environment for Unix intended to scale to very large (nationwide) networks. The system employs an optimistic &#034;one copy availability&#034; model in which conflicting updates to the file system&#039;s directory information are automatically reconciled, while con icting file updates are reliably detected and reported. The system architecture is based on a stackable layers methodology which permits a high degree of modularity and extensibility of file system services. This paper presents the motivations for replication and summarizes the case for optimistic concurrency control for large scale distributed file systems. It presents a brief description of the Ficus file system and concludes with a number of outstanding issues which must be addressed. stackable file system architecture; and an optimistic view of update, in which any file or directory may be updated, so long as some copy is available. Conicts are addressed when reconnection occurs. The system, which we call Ficus, is now operational. It has been constructed in a manner that can be added to an operating system that provides a faithful implementation of the vnode interface [10], even using conventional Unix le systems such as UFS for file storage. Various optimizations are possible through extensions to the basic structure. In the next section, we summarize the motivations for our replication work, providing a view of the goals of the research. The approach to modularity and optimistic operation are then described. Next, the actual Ficus project is summarized, and some observations about the utility of these approaches are offered. Finally, anumber of other issues are raised. 
345|Dynamic Documents: Mobile Wireless Access to the WWW|We propose dynamic documents as an approach to extending and customizing the WWW/Mosaic for mobile computing platforms. Dynamic documents are programs executed on a mobile platform to generate a document; they are implemented as Tcl scripts. We have modified the NCSA Mosaic web client to run the dynamic documents it retrieves through a modified Tcl interpreter. The interpreter is designed to execute only commands that do not violate safety.  To hide the latencies of slow links we have modified the Mosaic client to perform caching and prefetching. The policies for caching and prefetching can be under control of dynamic documents, allowing the strategies to be document-specific. Using dynamic documents, we have built an adaptive email browser that employs application-specific caching and prefetching strategies. Both the browser and the displayed email messages are dynamically customized to the mobile computing environment in which they run.  
346|W4 - the Wireless World Wide Web|4. Choosing wireless communications  In order to separate promises from practice in PDAs From the beginning, we were not interested in wire- and wireless communications, we decided to try to build a less local area communications. We wanted the focus to wireless PDA-based client to access the World Wide Web. be on the PDA and not on the installation and manage- This paper describes our design choices and experience ment of the network. If we got something built, we with W4, the Wireless World Wide Web. wanted to take it on the road. Much of the &#034;look and feel&#034; of mobile computing is lost if you can&#039;t bring the demo to 1. Introduction the customer.  A number of people see personal digital assistants, Press releases from a number of vendors during the PDA&#039;s, with wireless communication as the &#034;next big summer of 1993 suggested that &#034;real soon&#034; there would be thing&#034; in the computer industry. However, the current several communications options to choose from. Winter PDA&#039;s obvious limitatio...
347|Exploiting Weak Connectivity in a Distributed File System|views and conclusions contained in this document are those of the authors and should not be interpreted as
348|Using Dynamic Sets to Reduce the Aggregate Latency of Data Access|Many users of large distributed systems are plagued by high latency when accessing remote data. Latency is particularly problematic for the critical application of search and retrieval, which tends to access many objects and may suffer a long wait for each object accessed. Existing techniques like caching, inferential prefetching, and explicit prefetching are not suited to search, are ineffective at reducing latency for search applications, or greatly increase the complexity of the programming model. This dissertation shows that extending the file system interface to support a new abstraction called dynamic sets can address the problem of latency for search without incurring the penalties of the other techniques. A dynamic set is a lightweight and transitory collection of objects with well-defined semantics. An application creates a dynamic set on-demand to hold the objects it wishes to process. Adding dynamic sets to the system&#039;s interface results in two benefits. First, creation of a...
349|QoS Support for Mobile Computing|In recent years small, completely portable computers have become available on the marketplace. There is demand for such computers, termed walkstations, to access network services while retaining their mobility, and to operate effectively in a wide range of conditions. Future office environments are expected to support wireless networks with bandwidths which are several orders of magnitude greater than are available outdoors. In such environments, there will be powerful compute servers available for a walkstation&#039;s use. This dissertation describes a novel architecture called Notus and its support for applications operating in a mobile environment. The concept of the traded handoff is introduced, where applications are able to participate in the handoff process, rebuilding connections to the most appropriate service. This is expected to benefit walkstations which roam over large distances, where connections to servers would otherwise be strained, and also between heterogeneous networks where cooperation between the networks in performing a handoff might
350|Exploiting Non-Determinism in Set Iterators to Reduce I/O Latency|A key goal of distributed systems is to provide prompt access to shared information repositories. The high latency of remote access is a serious impediment to this goal. We propose a new file system abstraction called dynamic sets that allows the system to transparently reduce I/O latency without relying on reference locality, without modifying DFS serversand protocols, and without unduly complicating the programming model. We present this abstraction, and describe an implementation of it that runs on local and distributed file systems, as well as the World Wide Web. Substantial performance gains are demonstrated -- up to 50% savings in runtime for search on NFS, and up to 90% reduction in I/O latency for Web searches. 1 Introduction  A central problem facing distributed systems is the high latency to access remote data. Latency is problematic because it reduces the benefit typical applications can receive from faster CPUs, and reduces the productivity of users who are forced to wait f...
351|The SpectrumWare Testbed for ATM-based Software Radios|This paper describes the SpectrumWare testbed and two experimental systems we are developing: an ATM-based wideband receiver and the receive side of a GSM base station. These projects leverage recent advances in processor and analog-to-digital conversion technology that have enabled a software-oriented approach to wireless communication and distributed sample processing. 1 Introduction  Traditionally, analog hardware constrains systems to a particular modulation scheme and set of channels assignments. However, advances in processors and A/D conversion technology [1] have made it possible to implement &#034;virtual radios &#034; that sample wide bands of the RF spectrum, and process these samples in application software. In the SpectrumWare   1  architecture, the sample stream is converted into network packets (or ATM cells) at the antenna and forwarded to a cluster of workstations [8]. The workstation-based software can demodulate and decode any number of channels within the band, and apply diff...
352|Requirements for Internet Hosts - Communication Layers|This RFC is an official specification for the Internet community. It incorporates by reference, amends, corrects, and supplements the primary protocol standards documents relating to hosts. Distribution of this document is unlimited. Summary This is one RFC of a pair that defines and discusses the requirements for Internet host software. This RFC covers the communications protocol layers: link layer, IP layer, and transport layer; its companion RFC-1123 covers the application and support protocols.
353|TCP Extensions for Long-Delay Paths|This memo proposes a set of extensions to the TCP protocol to provide efficient operation over a path with a high bandwidth*delay product. These extensions are not proposed as an Internet standard at this time. Instead, they are intended as a basis for further experimentation and research on transport protocol performance. Distribution of this memo is unlimited. 1.
354|The Active Badge Location System|cation is the `pager system&#039;. In order to locate a person a signal is sent out by a central facility that addresses a particular receiver unit (beeper) and produces an audible signal. In addition, it may display a number to which the called-party should phone back (some systems allow a vocal message to be conveyed about the call-back number). It is then up to the recipient to use the conventional telephone system to call-back confirming the signal and determine the required action. Although useful in practice there are still circumstances where it is not ideal. For instance, if the called party does not reply the controller has no idea if they: 1) are in an area where the signal does not penetrate 2) have been completely out of the area for some time 3) have been too busy to reply or 4) have misheard or misread the call-back number. Moreover, in the case where there are a number of people who could respond to a crisis situation, it is not known which one is the nearest to the crisis an
355|Toward a model of text comprehension and production|The semantic structure of texts can be described both at the local microlevel and at a more global macrolevel. A model for text comprehension based on this notion accounts for the formation of a coherent semantic text base in terms of a cyclical process constrained by limitations of working memory. Furthermore, the model includes macro-operators, whose purpose is to reduce the information in a text base to its gist, that is, the theoretical macrostructure. These opera-tions are under the control of a schema, which is a theoretical formulation of the comprehender&#039;s goals. The macroprocesses are predictable only when the control schema can be made explicit. On the production side, the model is con-cerned with the generation of recall and summarization protocols. This process is partly reproductive and partly constructive, involving the inverse operation of the macro-operators. The model is applied to a paragraph from a psycho-logical research report, and methods for the empirical testing of the model are developed. The main goal of this article is to describe the system of mental operations that underlie the processes occurring in text comprehension and in the production of recall and summariza-tion protocols. A processing model will be outlined that specifies three sets of operations. First, the meaning elements of a text become
356|Context and cognition: Knowledge frames and speech act comprehension |This paper is about the cognitive foundations of pragmatic theories. Besides the fact that the usual appropriateness conditions for speech acts, which are given in cognitive terms, such as S knows/believes/wants... (that) p, require empirical investigation, a sound theory of prag-matics must also explain how certain utterances in certain contexts are actually understood as certain speech acts. Speech act comprehension is based on rules and strategies for so-called context analysis, in which (epistemic) frames play an important role in the analysis of social context, social frames, and interaction type. Results of context analysis are then matched with those of pragmatic sentence analysis, viz. the illocutionary act indicating devices. Finally, some results from the cognitive analysis of discourse processing are applied in a brief account of the comprehension of speech act sequences and macro-speech acts. 1. The foundations of pragmatics The philosophical and linguistic study of pragmatics requires an analysis of its foundations. This basis of pragmatic theories is on the one hand conceptual, e.g. in the analysis of action and interaction, and on the other hand empirical, viz. in the investigation of the psychological and social properties of language processing in
357|Investigating the Energy Consumption of a Wireless Network Interface in an Ad Hoc Networking Environment|Energy-aware design and evaluation of network protocols requires knowledge of the energy consumption behavior of actual wireless interfaces. But little practical information is available about the energy consumption behavior of well-known wireless network interfaces and device specifications do not provide information in a form that is helpful to protocol developers. This paper describes a series of experiments which obtained detailed measurements of the energy consumption of an IEEE 802.11 wireless network interface operating in an ad hoc networking environment. The data is presented as a collection of linear equations for calculating the energy consumed in sending, receiving and discarding broadcast and pointto -point data packets of various sizes. Some implications for protocol design and evaluation in ad hoc networks are discussed. Keywords---energy consumption, IEEE 802.11, ad hoc networks I.
358|CEDAR: a Core-Extraction Distributed Ad hoc Routing Algorithm|Absiract—CEDARis an algorithm for QoS routing in ad hoc network environments. It has three key components: (a) the establishment and main-tenance of a setf-organizing routing infrastructure catted the core for per-forming route computations, (b) the propagation of the link-state of stable high-bandwidth links in the core through increase/decrease waves, and (c)a QoS ra,ute computation algorithm that is exeeuted at the core nodes using onty locally available state. Our preliminary performance evaluation shows that CEDAR is a robust and adaptive QoS routing algorithm that reacts effectively to the dynamics of the network white stitl approximating link-state performance for stable networks. Keywords—Ad hoc routing, QoS routing I.
359|A Cluster-based Approach for Routing in Dynamic Networks|The design and analysis of routing protocols is an important issue in dynamic networks such as packet radio and ad-hoc wireless networks. Most conventional protocols exhibit their least desirable behavior for highly dynamic interconnection topologies. We propose a new methodology for routing and topology information maintenance in dynamic networks. The basic idea behind the protocol is to divide the graph into a number of overlapping clusters. A change in the network topology corresponds to a change in cluster membership. We present algorithms for creation of clusters, as well as algorithms to maintain them in the presence of various network events. Compared to existing and conventional routing protocols, the proposed cluster-based approach incurs lower overhead during topology updates and also has quicker reconvergence. The effectiveness of this approach also lies in the fact that existing routing protocols can be directly applied to the network -- replacing the nodes by clusters.  1 ...
360|Power Management Techniques for Mobile Communication|In mobile computing, power is a limited resource. Like other devices, communication devices need to be properly managed to conserve energy. In this paper, we present the design and implementation of an innovative transport level protocol capable of significantly reduc- ing the power usage of the communication device. The protocol achieves power savings by selectively choosing short periods of time to suspend communications and shut down the communication device. It manages the important task of queuing data for future delivery during periods of communication suspension, and decides when to restart communication. We also address the tradeoff between reducing power consumption and reducing delay for incoming data.
361|Quantifying the Energy Consumption of a Pocket Computer and a Java Virtual Machine|In tiffs paper, we examine the energy consumption era stateof -the-art pocket computer. UsLug a data acquisltion system, we measure the energy consumption of the ]tsy Pocket Computer, developed by Compaq Computer Corporation&#039;s Palp Alto Research Labs. We begin by showing that the energy usage characteristics of the Itsy differ markedly from that of a notebook computer. Then, since we expect that flexible software environments will become increasingly pre- valent on pocket computers, we consider applications running iu a Java environment. In particular, we explain some of the Java design tradeoffs applicable to pocket computers, and quantify their energy costs. For the design options we considered and the three workloads we studied, we find a maximum change in energy use of 25%.
362|A Taxonomy for Routing Protocols in Mobile Ad Hoc Networks|A Mobile Ad hoc NETwork (manet) is a mobile, multi-hop wireless network which is capable of autonomous operation. It is characterized by energy-constrained nodes, bandwidth-constrained, variable-capacity wireless links and dynamic topology, leading to frequent and unpredictable connectivity changes. In the absence of a fixed infrastructure, manet nodes cooperate to provide routing services, relying on each other to forward packets to their destination. Routing protocols designed for the fixed network are not effective in the dynamic and resource-constrained manet environment; many alternative routing protocols have been suggested. This report provides an overview of a number of manet routing protocols. More importantly, it defines a taxonomy that is suitable for examining a wide variety of protocols in a structured way and exploring tradeoffs associated with various design choices. The emphasis is on practical design and implementation issues rather than complexity analysis.
363|An Energy-consumption Model for Performance Analysis of Routing Protocols for Mobile Ad Hoc Networks|This report presents a model for evaluating the energy consumption behavior of a mobile ad hoc network. The model was used to examine the energy consumption of two wellknown manet routing protocols. Energy-aware performance analysis is shown to provide new insights into costly protocol behaviors and suggests opportunities for improvement at the protocol and link layers
364|Reducing Power Consumption for the Next Generation of PDAs: It&#039;s in the Network Interface!|The current trends in the computer industryseem to indicate that low-cost, light-weight personal digital assistants (PDAs) will emerge as a major player in the information access and mobile computing markets. An important issue which must be addressed for PDAs, however, is battery longevity. In this paper we examine this issue from the point of view of the possible policies employed by the PDA in controlling the sleep/active states of its network interface (NI) in order to most efficiently utilize its power resources without unduly compromising the timeliness of its user&#039;s ability to access information. In particular, we measured the power usage of two PDAs, the Apple Newton Messagepad and Sony Magic Link, and three NIs, the Metricom Ricochet Wireless Modem, and the AT&amp;T Wavelan operating at 915 MHz and 2.4 GHz. These measurements clearly indicate that the power drained by the network interface consititutes a large fraction of the total power used by the PDA. Based on these measurement...
365|Investigating the Energy Consumption of an IEEE 802.11 Network Interface|This report describes a series of simple experiments which measure the per-packet energy consumption of an IEEE 802.11 wireless network interface. The goal of this work is to develop a solid experimental basis for assumptions that can (or cannot) be made in the design and analysis of network protocols operating in the ad hoc wireless environment.  Keywords: wireless network interface, IEEE 802.11, energy consumption, energy-aware network protocols, ad hoc networks, measurement  1 Motivation  Energy consumption at the network interface is an issue for all mobile computing devices, whether they operate within a base-station infrastructure or in a free-standing mobile ad hoc network (manet).  In a wireless ad hoc network, a host communicates directly with hosts within wireless range and indirectly with all other hosts using a dynamically-computed, multi-hop route via the other hosts of the manet. There has been a great deal of interest in the design and analysis of network protocols for t...
366|DSR: The Dynamic Source Routing Protocol for Multi-Hop Wireless Ad Hoc Networks|The Dynamic Source Routing protocol (DSR) is a simple and efficient routing protocol designed specifically for use in multi-hop wireless ad hoc networks of mobile nodes. DSR allows the network to be completely self-organizing and self-configuring, without the need for any existing network infrastructure or administration. The protocol is composed of the two mechanisms of Route Discovery and Route Maintenance, which work together to allow nodes to discover and maintain source routes to arbitrary destinations in the ad hoc network. The use of source routing allows packet routing to be trivially loop-free, avoids the need for up-to-date routing information in the intermediate nodes through which packets are forwarded, and allows nodes forwarding or overhearing packets to cache the routing information in them for their own future use. All aspects of the protocol operate entirely on-demand, allowing the routing packet overhead of DSR to scale automatically to only that needed to react to changes in the routes currently in use. We have evaluated the operation of DSR through detailed simulation on a variety of movement and communication patterns, and through implementation and significant experimentation in a physical outdoor ad hoc networking testbed we have constructed in Pittsburgh, and have demonstrated the excellent performance of the protocol. In this chapter, we describe the design of DSR and provide a summary of some of our simulation and testbed implementation results for the protocol. 1
367|Location-Aided Routing (LAR) in mobile ad hoc networks  (1998) |A mobile ad hoc network consists of wireless hosts that may move often. Movement of hosts results in a change in routes, requiring some mechanism for determining new routes. Several routing protocols have already been proposed for ad hoc networks. This paper suggests an approach to utilize location information (for instance, obtained using the global positioning system) to improve performance of routing protocols for ad hoc networks. By using location information, the proposed Location-Aided Routing (LAR) protocols limit the search for a new route to a smaller “request zone” of the ad hoc network. This results in a significant reduction in the number of routing messages. We present two algorithms to determine the request zone, and also suggest potential optimizations to our algorithms.
368|MACAW: Media access protocol for wireless lans|In recent years, a wide variety of mobile computing devices has emerged, including portables, palmtops, and personal digit al assistants. Providing adequate network connectivity y for these devices will require a new generation of wireless LAN technology. In this paper we study media access protocols for a single channel wireless LAN being developed at Xerox Corporation’s Palo Alto Research Center. We start with the MACA media access protocol first proposed by Karn [9] and later refined by Biba [3] which uses an RTS-CTS-DATA packet exchange and binary exponential backoff. Using packet-level simulations, we examine various performance and design issues in such protocols, Our analysis leads to a new protocol, MACAW, which uses an RTS-CTS-DS-DATA-ACK message exchange and includes a significantly different backoff algorithm. 1
369|Analysis of TCP Performance over Mobile Ad Hoc Networks Part I: Problem Discussion and Analysis of Results|Mobile ad hoc networks have gained a lot of attention lately as a means of providing continuous network connectivity to mobile computing devices regardless of physical location. Recently, a large amount of research has focused on the routing protocols needed in such an environment. In this two-part report, we investigate the effects that link breakage due to mobility has on TCP performance. Through simulation, we show that TCP throughput drops significantly when nodes move because of TCP&#039;s inability to recognize the difference between link failure and congestion. We also analyze specific examples, such as a situation where throughput is zero for a particular connection. We introduce a new metric, expected throughput, for the comparison of throughput in multi-hop networks, and then use this metric to show how the use of explicit link failure notification (ELFN) techniques can significantly improve TCP performance. In this paper (Part I of the report), we present the problem and an analysis of our simulation results. In Part II of this report, we present the simulation and results in detail.
370|The Performance of Query Control Schemes for the Zone Routing Protocol|In this paper, we study the performance of route query control mechanisms for the Zone Routing Protocol (ZRP) for ad hoc networks. ZRP proactively maintains routing information for a local neighborhood (routing zone), while reactively acquiring routes to destinations beyond the routing zone. This hybrid routing approach can be more efficient than traditional routing schemes. However, without proper query control techniques, the ZRP cannot provide the expected reduction in the control traffic.
371|Routing in Ad Hoc Networks of Mobile Hosts|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any centralized administration or standard support services. In such an environment, it may be necessary for one mobile host to enlist the aid of others in forwarding a packet to its destination, due to the limitedpropagation range of each mobile host’s wireless transmissions. Some previous attempts have been made to use conventional routing protocols for routing in ad hoc networks, treating each mobile host as a router This position paper points out a number of problems with this design and suggests a new approach based on separate route discovery and route maintenance protocols. 1.
372|The Effects of On-Demand Behavior in Routing Protocols for Multi-Hop Wireless Ad Hoc Networks|Abstract—A number of different routing protocols proposed for use in multi-hop wireless ad hoc networks are based in whole or in part on what can be described as on-demand behavior. By ondemand behavior, we mean approaches based only on reaction to the offered traffic being handled by the routing protocol. In this paper, we analyze the use of on-demand behavior in such protocols, focusing on its effect on the routing protocol’s forwarding latency, overhead cost, and route caching correctness, drawing examples from detailed simulation of the dynamic source routing (DSR) protocol. We study the protocol’s behavior and the changes introduced by variations on some of the mechanisms that make up the protocol, examining which mechanisms have the greatest impact and exploring the tradeoffs that exist between them. Index Terms—Communication system routing, computer network performance, dynamic source routing (DSR) protocol, wireless ad hoc networks. I.
373|Experiences designing and building a multi-hop wireless ad hoc network testbed|In this paper, we describe our experiences building a multi-hop wireless ad hoc network of 8 nodes driving around a 700 m by 300 m site. Each node runs the Dynamic Source Routing (DSR) protocol and interfaces seamlessly with existing Internet infrastructure and the Mobile IP protocol. The issues discussed in this paper range from logistical and management issues, to protocol design and performance analysis issues. We also present an early characterization of the testbed performance, and describe a significant new challenge for ad hoc network routing protocols. The major goal of the paper, however, is to share our
374|Query localization techniques for on-demand routing protocols in ad hoc networks|Abstract. Mobile ad hoc networks are characterized by multi-hop wireless links, absence of any cellular infrastructure, and frequent host mobility. Design of efficient routing protocols in such networks is a challenging issue. A class of routing protocols called on-demand protocols has recently found attention because of their low routing overhead. We propose a technique that can reduce the routing overhead even further. The on-demand protocols depend on query floods to discover routes whenever a new route is needed. Our technique utilizes prior routing histories to localize the query flood to a limited region of the network. Simulation results demonstrate excellent reduction of routing overheads with this mechanism. This also contributes to a reduced level of network congestion and better end-to-end delay performance of data packets.
376|Supporting Hierarchy and Heterogeneous Interfaces in Multi-Hop Wireless Ad Hoc Networks|Much progress has been made toward solving the problem of routing packets inside an ad hoc network, but there are presently no complete proposals for connecting ad hoc networks together to form larger networks, or for integrating them with wired internets. This paper describes a technique that allows a single ad hoc network to span across heterogeneouslink layers. Using this technique, we can both integrate ad hoc networks into the hierarchical Internet and support the migration of mobile nodes from the Internet into and out of ad hoc networks via Mobile IP. Taken together, these solutions improve the scalability of flat ad hoc networks by introducing hierarchy, and they enable all nodes participating in the ad hoc network to be reachable from anywhere in the world. We have implemented each of the solutions in a real testbed of 8 nodes using the Dynamic Source Routing (DSR) protocol. Generalizing our solutions, we describe several abstract scenarios and present our ideas for solving them.  
377|Scalable support for transparent mobile host internetworking|This paper considers the problem of providing transparent support for very large numbers of mobile hosts within a large internetwork such as the Internet. The availability of powerful mobile computing devices and wireless networking products and services is increasing dramatically, but internetworking protocols such as IP used in the Internet do not currently support host movement. To address this need, the Internet Engineering Task Force (IETF) is currently developing protocols for mobile hosts in the Internet. This paper analyzes the problem to be solved, reviews the current state of that effort, and discusses its scalability to very large numbers of mobile hosts in a large internetwork. 1.
378|FLIP: an Internetwork Protocol for Supporting Distributed Systems|Most modern network protocols give adequate support for traditional applications such as file transfer and remote login. Distributed applications, however, have different requirements (e.g., efficient atmost-once remote procedure call even in the face of processor failures). Instead of using ad-hoc protocols to meet each of the new requirements, we have designed a new protocol, called the Fast Local Internet Protocol (FLIP), that provides a clean and simple integrated approach to these new requirements. FLIP is an unreliable message protocol that provides both point-to-point communication and multicast communication, and requires almost no network management. Furthermore, by using FLIP we have simplified higher-level protocols such as remote procedure call and group communication, and enhanced support for process migration and security. A prototype implementation of FLIP has been built as part of the new kernel for the Amoeba distributed operating system, and is in daily use. Measurements of its performance are presented. 1.
379|Source Demand Routing: Packet Format and Forwarding Specification (Version 1)&#034;, RFC  (1940) |This memo provides information for the Internet community. This memo does not specify an Internet standard of any kind. Distribution of this memo is unlimited. 1. Overview The purpose of SDRP is to support source-initiated selection of routes to complement the route selection provided by existing routing protocols for both inter-domain and intra-domain routes. This document refers to such source-initiated routes as &#034;SDRP routes&#034;. This document describes the packet format and forwarding procedure for SDRP. It also describes procedures for ascertaining feasibility of SDRP routes. Other components not described here are routing information distribution and route computation. This portion of the protocol may initially be used with manually configured routes. The same packet format and processing will be usable with dynamic route information distribution and computation methods under development. The packet forwarding protocol specified here makes minimal assumptions about the distribution and acquisition of routing information needed to construct the SDRP routes. These minimal assumptions are believed to be sufficient for the existing Internet. Future components of the SDRP protocol will extend capabilities in this area and others in a largely backward-compatible manner. This version of the packet forwarding protocol sends all packets with the complete SDRP route in the SDRP header. Future versions will address route setup and other enhancements and optimizations.
380|Validation of Wireless and Mobile Network Models and Simulation|this paper can be used to compare different runs using simulation,emulation, or measurements from the real world. They seem to be appropriate at least for small- or medium-scale networks, and should be able to be applied to large-scale networks given suitable choices by the modeler. Each validation achieved for a given simulation environment or model also adds to the basis for confidence in other results from the same tools.
381|Network Routing Techniques and Their Relevance to Packet Radio Networks|With packet radio networks, the distance between source and destinat.ion nodes typ-ically necessitat$es one or more nodes to relay data to the final destination. Thus, some form of routing must. take place. This paper explains several current network routing algorithms and shows their relevance to packet, radio networks. In addition, current research at. AFIT concerning the development of an automatic rout.ing algorithnl for Air Force Logistics Command’s (AFLC) HF packet radio network is explained.
382|The process group approach to reliable distributed computing|The difficulty of developing reliable distributed softwme is an impediment to applying distributed computing technology in many settings. Expeti _ with the Isis system suggests that a structured approach based on virtually synchronous _ groups yields systems that are substantially easier to develop, exploit sophisticated forms of cooperative computation, and achieve high reliability. This paper reviews six years of resemr,.hon Isis, describing the model, its impl_nentation challenges, and the types of applicatiom to which Isis has been appfied. 1 In oducfion One might expect the reliability of a distributed system to follow directly from the reliability of its con-stituents, but this is not always the case. The mechanisms used to structure a distributed system and to implement cooperation between components play a vital role in determining how reliable the system will be. Many contemporary distributed operating systems have placed emphasis on communication performance, overlooking the need for tools to integrate components into a reliable whole. The communication primitives supported give generally reliable behavior, but exhibit problematic semantics when transient failures or system configuration changes occur. The resulting building blocks are, therefore, unsuitable for facilitating the construction of systems where reliability is impo/tant. This paper reviews six years of research on Isis, a syg_,,m that provides tools _ support the construction of reliable distributed software. The thesis underlying l._lS is that development of reliable distributed software can be simplified using process groups and group programming too/_. This paper motivates the approach taken, surveys the system, and discusses our experience with real applications.
384|Transis: A Communication Sub-System for High Availability|This paper describes Transis, a communication sub-system for high availability. Transis is a transport layer package that supports a variety of reliable multicast message passing services between processors. It provides highly tuned multicast and control services for scalable systems with arbitrary topology. The communication domain comprises of a set of processors that can initiate multicast messages to a chosen subset. Transis delivers them reliably and maintains the  membership of connected processors automatically, in the presence of arbitrary communication delays, of message losses and of processor failures and joins. The contribution of this paper is in providing an aggregate definition of communication and control services over broadcast domains. The main benefit is the efficient implementation of these services using the broadcast capability. In addition, the membership algorithm has a novel approach in handling partitions and remerging; in allowing the regular flow of messages...
