ID|Title|Summary
1|When to Trigger Active Rules? |Active rules model and enforce enterprise requirements such as situation monitoring using events, conditions and actions. These rules are termed active rules as they make the underlying application or system such as database management system active capable (i.e., react to changes actively using the push paradigm). Events play a critical role in active rules as they define and detect an occurrence of interest in the real world and trigger the associated rules. Currently active rules are triggered only when events occur completely. Though this allows active rules to model enterprise policies they are not sufficient in modeling situations warranted by applications such as information security. In this paper, we motivate the need for extending events and active rules. We introduce and discuss event extensions and rule generalization and show how these extensions allow the modeling of situations warranted by emerging applications. Finally, we discuss algorithms and implementation of the extensions using the Sentinel Local Event Detector system. 1
3|On The Semantics Of Complex Events In Active Database Management Systems|Active database management systems have been introduced for applications needing an automatic reaction in response to certain events. Events can be simple in nature or complex. Complex events rely on simpler ones and are usually specified with the help of operators of an event algebra.  There are quite a few papers dealing with extensions of existing event algebras. However, a systematic and comprehensive analysis of the semantics of complex events is still lacking. As a consequence most proposals suffer from different kinds of peculiarities. Independent aspects are not treated independently leading to shady mixtures of aspects in operators. Moreover, aspects are not always treated uniformly. Operators may have other semantics than expected.  This paper addresses these problems by an extensive and in-depth analysis of the foundations of complex events. As a result of this analysis, a (formal) metamodel for event algebras will be introduced that subdivides the semantics of complex event...
4|Two Approaches to Event Definition|We compare two approaches to event definition, deriving from the Active Database and Knowledge Representation communities. We relate these approaches by taking a system of the former kind, displaying some of its shortcomings, and rectifying them by remodelling the system in the latter style. We further show the extent to which the original system can be recreated within the remodelled system. This bridge between the two approaches should provide a starting point for fruitful interaction between the two communities.
5|Formalization and Detection of Events Using Interval-Based Semantics|Active databases utilize Event-Condition-Action rules to provide active capability to the underlying system. An event was initially defined to be an instantaneous, atomic occurrence of interest and the time of occurrence of the last event in an event expression was used as the time of occurrence for an entire event expression (detection-based semantics), rather than the interval over which an event expression occurs (interval-based semantics). This introduces semantic discrepancy for some operators when they are composed more than once. Currently, all active databases detect events using the detection-based semantics rather than the interval-based semantics. SnoopIB is an interval-based event specification language developed for expressing primitive and composite events that are part of active rules. Algorithms for event detection using interval-based semantics pose some challenges, as not all events are known (especially their starting points). In this paper, we address the following: 1) briefly explain the need for interval-based semantics, 2) formalization of events accumulated over a semantic window and 3) how diversified events (e.g., sliding window, accumulated) are detected using interval-based semantics in the context of Sentinel – an active object oriented database. 1.
6|An Interval-based Algebra for Restricted Event Detection|Abstract. In this article, we propose an interval based algebra for de-tection of complex events. The algebra includes a strong restriction policy in order to comply with the resource requirements of embedded or real-time applications. We prove a set of algebraic properties to justify the novel restriction policy and to establish the relation between the unre-stricted algebra and the restricted version. Finally, we present an efficient algorithm that implements the proposed algebra. 1
7|Semantic characterization of real world events| Reducing the latency of information delivery in an event driven world has always been a challenge. It is often necessary to completely capture the attributes of events and relationships between them, so that the process of retrieval of event related information is efficient. In this paper, we discuss a formal system for representing and analyzing real world events to address these issues. The event representation discussed in this paper accounts for the important event attributes, namely, time, space, and label. We introduce the notion of sequence templates that not only provides event related semantics but also help in semantically analyzing user queries. Finally, we discuss the design for our Query-Event Analysis System, which is an integrated system to (a) identify a best sequence template given a user query; (b) select events based on the best sequence template; and (c) determine content related to the selected events for delivering to users.  
8|Rules, discretion, and reputation in a model of monetary policy|In a discretionary regime the monetary authority can print more money and create more inflation than people expect. But, although these inflation surprises can have some benefits, they cannot arise systematically in equilibrium when people understand the policymakor&#039;s incentives and form their expectations accordingly. Because the policymaker has the power to create inflation shocks ex post, the equilibrium growth rates of money and prices turn out to be higher than otherwise. Therefore, enforced commitments (rules) for monetary behavior can improve matters. Given the repeated interaction between the policymaker and the private agents, it is possible that reputational forces can substitute for formal rules. Here, we develop an example of a reputational equilibrium where the outcomes turn out to be weighted averages of those from discretion and those from the ideal rule. In particular, the rates of inflation and monetary growth look more like those under discretion when the discount rate is high.
10|Expectations and the Neutrality of Money|This paper provides a simple example of an economy in which equi-librium prices and quantities exhibit what may be the central feature of the modern business cycle: a systematic relation between the rate of change in nominal prices and the level of real output. The relationship,
12|Wage indexation: A macroeconomic approach|This essay examines the role of wage indexation in dampening macroeconomic fluctuations in a simple neoclassical model modified to incorporate short-term wage rigidities and uncer-tainty. The analysis departs from most of the previous literature on indexing in its explicit consideration of real disturbances. It is found that while indexing insulates the real sector from the effects of monetary shocks, it may exacerbate the real effects of real shocks. Thus the analysis suggests an optimal degree of partial indexation that depends on the underlying stochastic structure of the economy. Consequently, optimal indexing will not, in general, insulate the real sector from monetary variability. 1. Iutroductioll This paper develops a framework for investigating the role of indexation in dampening macroeconomic fluctuations. The distinguishing feature of the analysis is the emphasis on real as well as monetary disturbances a a source of price and output, variation. 1 It is found that while indexing insulates the real sector from the effects of monetary shocks, it may exacerbate he real effects of real shocks. For an economy subject o both types of disturbances, this result
13|Monetary Policy During a Transition to Rational Expectations|In standard macroeconomic models incorporating the natural rate hypothesis and rational expectations, monetary policy has no effect on real variables. But the rational expectations assumption that economic agents have learned from their mistaken predictions of the past ignores the transition period during which new information is combined with old information in the formation of new beliefs. The purpose of this paper is to examine the possible effects of monetary policy during this transi-tion period. Using a simple momentary Phillips curve model and a particular characterization of monetary policy, it is shown that real variables (in this case unemployment) can be controlled. Further, an optimal monetary policy is computed by simple variational methods. This policy is a randomized rule which matches the marginal gain from future reductions in unemployment to the marginal loss of increased uncertainty about the price level. Unlike the rational expectations equilibrium, this rule will dominate purely deterministic rules, even if the latter are possible. I.
14|The Ponder Policy Specification Language|The Ponder language provides a common means of specifying security policies that map onto various access control implementation mechanisms for firewalls, operating systems, databases and Java. It supports obligation policies that are event triggered conditionaction rules for policy based management of networks and distributed systems. Ponder can also be used for security management activities such as registration of users or logging and auditing events for dealing with access to critical resources or security violations. Key concepts of the language include roles to group policies relating to a position in an organisation, relationships to define interactions between roles and management structures to define a configuration of roles and relationships pertaining to an organisational unit such as a department. These reusable composite policy specifications cater for the complexity of large enterprise information systems. Ponder is declarative, stronglytyped and object-oriented which makes the language flexible, extensible and adaptable to a wide range of management requirements.  
15|Lattice-Based Access Control Models|The objective of this article is to give a tutorial on lattice-based  access control models for computer security. The paper begins with a review  of Denning&#039;s axioms for information flow policies, which provide a theoretical  foundation for these models. The structure of security labels in the military and  government sectors, and the resulting lattice is discussed. This is followed by a  review of the Bell-LaPadula model, which enforces information flow policies by  means of its simple-security and *-properties. It is noted that information flow  through covert channels is beyond the scope of such access controls. Variations  of the Bell-LaPadula model are considered. The paper next discusses the Biba  integrity model, examining its relationship to the Bell-LaPadula model. The  paper then reviews the Chinese Wall policy, which arises in a segment of the  commercial sector. It is shown how this policy can be enforced in a lattice  framework.
16|Policy driven management of distributed systems|Separating management policy from the automated managers which interpret the policy facilitates the dynamic change of behaviour of a distributed management system. This permits it to adapt to evolutionary changes in the system being managed and to new application requirements. Changing the behaviour of automated managers can be achieved by changing the policy without have to reimplement them – this permits the reuse of the managers in different environments. It is also useful to have a clear specification of the policy applying to human managers in an enterprise. This paper describes the work on policy which has come out of two related ESPRIT funded projects, SysMan and IDSM. Two classes of policy are elaborated – authorisation policies define what a manager is permitted to do and obligation policy define what a manager must do. Policies are specified as objects which define a relationship between subjects (managers) and targets (managed objects). Domains are used to group the objects to which a policy applies. Policy objects also have attributes specifying the action to be performed and constraints limiting the applicability of the policy. We show how a number of example policies can be modelled using these objects and briefly mention issues relating to policy hierarchy and conflicts between overlapping policies.
17|Conflicts in Policy-based Distributed Systems Management|Modern distributed systems contain a large number of objects, and must be capable of evolving, without shutting down the complete system, to cater for changing requirements. There is a need for distributed, automated management agents whose behavior also has to dynamically change to reflect the evolution of the system being managed. Policies are a means of specifying and influencing management behavior within a distributed system, without coding the behavior into the manager agents. Our approach is aimed at specifying implementable policies, although policies may be initially specified at the organizational level (c.f. goals) and then refined to implementable actions. We are concerned with two types of policies. Authorization policies specify what activities a manager is permitted or forbidden to do to a set of target objects and are similar to security accesscontrol policies. Obligation policies specify what activities a manager must or must not do to a set of target objects and essen...
18|Role-Based Access Control|The basic concept of role-based access control (RBAC) is that permissions are associated with roles, and users are made members of appropriate roles thereby acquiring the roles&#039; permissions. This idea has been around since the advent of multi-user computing. Until recently, however, RBAC has received little attention from the research community. This article describes the motivations, results and open issues in recent RBAC research. The article focuses on four areas. Firstly, RBAC is a multi-dimensional concept that can range from very simple at one extreme to quite complex and sophisticated at the other. This presents problems in coming up with a definitive model of RBAC. We see how this impasse is resolved by having a family of models which can accommodate all these variations. Secondly, we discuss how RBAC can be used to manage itself. Recent models developed for this purpose are presented. Thirdly, the flexibility of RBAC can be demonstrated in many ways. Here we show how R...
20|Security Issues in Mobile Code Systems|Abstract. In mobile code systems, programs or processes travel from host to host in order to accomplish their goals. Such systems violate some of the assumptions that underlie most existing computer security implementations. In order to make these new systems secure, we will have to deal with a number of issues that previous systems have been able to ignore or sidestep. This paper surveys the assumptions that mobile code systems violate (including the identification of programs with persons, and other assumptions that follow from that), the new security issues that arise, and some of the ways that these issues will be addressed. 1
21|Management Policy Service for Distributed Systems|Interpreting policy in automated managers facilitates the dynamic change of behaviour of a distributed management system by simply changing policies. This paper describes a management policy notation which can be used to define both authorisation policies (what activities a manager is permitted to do) and obligation policies (the activities a manager must perform). Some example policy specifications are given to demonstrate the notation and the concepts involved. A graphical policy editor is described which permits high level abstract policies to be refined into lower level, implementable policies and maintains derivation and dependency relationships between the different policies. A policy service which stores policies is outlined and its integration within a domain service for grouping policies is explained. Outlines are given of implementations of automated managers for interpreting obligation policies and of an access control mechanism for enforcing authorisation policies. 1 Intro...
22|Towards A Role Based Framework For Distributed Systems Management|Roles have been widely used for modeling the authority, responsibility, functions and interactions associated with manager positions within organizations. In this paper we discuss the issues related to specifying roles for both human and automated managers of distributed computer systems. The starting point is that a role can be defined in terms of the authorization and obligation policies, for a particular manager position, which specify what actions the manager is permitted or is obliged to do on a set of target objects. This permits  individuals to be assigned or removed from positions without respecifying the policies for the  role. However these policies are insufficient for fully specifying relationships between managers and the targets they manage or between different manager roles. There is a need to specify the interaction protocols and how managers coordinate and synchronize their activities.  The role based framework consists of a set of tools enabling the creation of roles ...
23|Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging|this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part of speech tagging
24|Induction of Decision Trees| The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.  
25|Building a Large Annotated Corpus of English: The Penn Treebank|There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.

In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989-1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.
26|WordNet: An on-line lexical database|WordNet is an on-line lexical reference system whose design is inspired by current
27|A statistical approach to machine translation|In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results.
28|A Simple Rule-Based Part of Speech Tagger|Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule- based methods. In this paper, we present a sim- ple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy coinparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a sinall set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, cor- pus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.
29|A Program for Aligning Sentences in Bilingual Corpora|This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths. The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. A probabilistic score is assigned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences. It is remarkable that such a simple approach works as well as it does. An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly aligned all but 4% of the sentences. Moreover, it is possible to extract a large subcorpus that has a much smaller error rate. By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%. There were more errors on the English-French subcorpus than on the English-German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs. To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the align program, an appendix is provided with ...
30|A practical part-of-speech tagger|We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment.
31|Self-organized language modeling for speech recognition|In the case of a trlgr~m language model, the proba-bility of the next word conditioned on the previous two words is estimated from a large corpus of text. The re-sulting static trigram language model (STLM) has fixed probabilities that are independent of the document being dictated. To improve the language mode] (LM), one can adapt the probabilities of the trigram language model to match the current document more closely. The partially dictated document provides significant clues about what words ~re more likely to be used next. Of many meth-ods that can be used to adapt the LM, we describe in this paper a simple model based on the trigram frequencies es-timated from the partially dictated document. We call this model ~ cache trigram language model (CTLM) since we are c~chlng the recent history of words. We have found that the CTLM red,aces the perplexity of a dictated doc-ument by 23%. The error rate of a 20,000-word isolated word recognizer decreases by about 5 % at the beginning of a document and by about 24 % after a few hundred words.
33|Inferring decision trees using the minimum description length principle|We explore the use of Rissanen’s minimum description length principle for the construction of decision trees. Empirical results comparing this approach to other methods are given.  
34|Tagging English Text with a Probabilistic Model|In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main novelty of these experiments is the use of untagged text in the training of the model. We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided. Two approaches in particular are compared and combined: using text that has been tagged by hand and computing relative frequency counts, using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle
35|A method for disambiguating word senses in a large corpus|Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years. Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources, such as semantic networks and annotated corpora. In particular, much of the work on qualitative methods has had to focus on ‘‘toy’’ domains since currently available semantic networks generally lack broad coverage. Similarly, much of the work on quantitative methods has had to depend on small amounts of hand-labeled text for testing and training. We have achieved considerable progress recently by taking advantage of a new source of testing and training materials. Rather than depending on small amounts of hand-labeled text, we have been making use of relatively large amounts of parallel text, text such as the Canadian Hansards, which are available in multiple languages. The translation can often be used in lieu of hand-labeling. For example, consider the polysemous word sentence, which has two major senses: (1) a judicial sentence, and (2), a syntactic sentence. We can collect a number of sense (1) examples by extracting instances that are translated as peine, and we can collect a number of sense (2) examples by extracting instances that are translated as
36|Word-Sense Disambiguation Using Statistical Methods|We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is  constructed to have high mutual information with the translation of that instance in another lan- guage. When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent.
38|Grammatical Category Disambiguation by Statistical Optimization|[This paper focuses on the]... task of [part-of-speech] disambiguation, and particularly on a new algorithm called VOLSUNGA, which avoids syntactic-level analysis, yields about 96% accuracy, and runs in far less time and space than previous attempts. The most recent previous algorithm runs in NP (Non-Polynomial) time, while VOLSUNGA runs in linear time. This is provably optimal; no improvements in the order of its execution time and space are possible. VOLSUNGA is also robust in cases of ungrammaticality. Improvements to this accuracy may be made, perhaps the most potentially significant being to include some higher-level information. With such additions, the accuracy of statistically-based algorithms will approach 100%; and the few remaining cases may be largely those with which humans also find difficulty. In subsequent sections we examine several disambiguation algorithms. Their techniques, accuracies, and efficiencies are analyzed. After presenting the research carried out to date, a discussion of VOLSUNGA&#039;s application to the Brown Corpus...
39|A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation|I.n this paper, we describe a new corpus-based ap  proach to prepositional phrase attachment disambiguation,  and 10resent results comparing perlbrmance  of this algorithm with ol,her corpus-based  approaches to this problem.
40|Towards History-based Grammars: Using Richer Models for Probabilistic Parsing|We describe a generarive probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.
42|Word-Sense Disambiguation Using Decomposable Models|Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features. In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguafion of the noun interest. We describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguafion, without requiring untested assumptions regarding the form of the model. Using this approach, the joint distribution of all variables is described by only the most systematic variable interactions, thereby limiting the number of parameters to be estimated, supporting computational efficiency, and providing an understanding of the data.
43|Automatic Grammar Induction and Parsing Free Text: A Transformation-Based Approach|In this paper we describe a new technique for parsing free text: a transformational grammar  is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.
44|Acquiring Disambiguation Rules From Text|An effective procedure for automatically acquiring a new set of disambiguation rules for an existing deterministic parser on the basis of tagged text is presented. Performance of the automatically acquired rules is much better than the existing handwritten disambiguation rules. The success of the acquired rules depends on using the linguistic information encoded in the parser; enhancements to various components of the parser improves the acquired rule set. This work suggests a path toward more robust and comprehensive syntactic analyz- er8.
45|Parsing The Lob Corpus|This paper s presents a rapid and robust parsing system currently used to learn from large bodies of unedited text. The system contains a multivalued part-of-speech disambiguator and a novel parser employing bottom-up recognition to fred the constituent phrases of larger structures that might be too difficult to analyze. The results of applying the disambiguator and parser to large sections of the Lancaster/ Oslo-Bergen corpus are presented.
46|Claws4: The Tagging Of The British National Corpus|this paper is to describe the CLAWS4 general-purpose grammatical tagger, used for the tagging of the 100-million-word British National Corpus, of which c.70 million words have been tagged at the time of writing (April 1994). 1 We will empbasise the goals of (a) generd-purpose adaptability, (b) incorporation of linguistic knowledge to improve qu,&#039;dity and consistency, and (c) accuracy, measured consistently and in a linguistically informed way
47|Transformation-Based Error-Driven Parsing|In this paper we describe a new technique for parsing free text: a transformational grammar  1  is automatically learned that is capable of accurately parsing text into binarybranching syntactic trees. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce the number of errors. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.  1 INTRODUCTION  There has been a great deal of interest of late in the automatic induction of natural language grammar. Given the difficulty inherent in manually building a robust parser, along with the availability of large amounts of training material, automatic grammar induction seems like a path worth pursuing. A number of syste...
48|Decision Tree Models Applied to the Labeling of Text with|We describe work which uses decision trees to estimate marginal probabilities in a maximum entropy model for predicting the part-of-speech of a word given the context in which it appears. Two experiments are presented which exhibit improvements over the usual hidden Markov model approach. 1.
50|Gapped Blast and PsiBlast: a new generation of protein database search programs|The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSIBLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.
51|An intrusion-detection model|A model of a real-time intrusion-detection expert system capable of detecting break-ins, penetrations, and other forms of computer abuse is described. The model is based on the hypothesis that security violations can be detected by monitoring a system&#039;s audit records for abnormal patterns of system usage. The model includes profiles for representing the behavior of subjects with respect to objects in terms of metrics and statistical models, and rules for acquiring knowledge about this behavior from audit records and for detecting anomalous behavior. The model is independent of any particular system, application environment, system vulnerability, or type of intrusion, thereby providing a framework for a general-purpose intrusion-detection expert system.
52|Modeling and simulation of genetic regulatory systems: A literature review|In order to understand the functioning of organisms on the molecular level, we need to know which genes are expressed, when and where in the organism, and to which extent. The regulation of gene expression is achieved through genetic regulatory systems structured by networks of interactions between DNA, RNA, proteins, and small molecules. As most genetic regulatory networks of interest involve many components connected through interlocking positive and negative feedback loops, an intuitive understanding of their dynamics is hard to obtain. As a consequence, formal methods and computer tools for the modeling and simulation of genetic regulatory networks will be indispensable. This paper reviews formalisms that have been employed in mathematical biology and bioinformatics to describe genetic regulatory systems, in particular directed graphs, Bayesian networks, Boolean networks and their generalizations, ordinary and partial differential equations, qualitative differential equations, stochastic equations, and rule-based formalisms. In addition, the paper discusses how these formalisms have been used in the simulation of the behavior of actual regulatory systems. 
53|Using Bayesian networks to analyze expression data|DNA hybridization arrays simultaneously measure the expression level for thousands of genes. These measurements provide a “snapshot ” of transcription levels within the cell. A major challenge in computational biology is to uncover, from such measurements, gene/protein interactions and key biological features of cellular systems. In this paper, we propose a new framework for discovering interactions between genes based on multiple expression measurements. This framework builds on the use of Bayesian networks for representing statistical dependencies. A Bayesian network is a graph-based model of joint multivariate probability distributions that captures properties of conditional independence between variables. Such models are attractive for their ability to describe complex stochastic processes and because they provide a clear methodology for learning from (noisy) observations. We start by showing how Bayesian networks can describe interactions between genes. We then describe a method for recovering gene interactions from microarray data using tools for learning Bayesian networks. Finally, we demonstrate this method on the S. cerevisiae cell-cycle measurements of Spellman et al. (1998). Key words: gene expression, microarrays, Bayesian methods. 1.
55|Qualitative process theory|Objects move, collide, flow, bend, heat up, cool down, stretch, compress. and boil. These and other things that cause changes in objects over time are intuitively characterized as processes. To understand commonsense physical reasoning and make programs that interact with the physical world as well as people do we must understand qualitative reasoning about processes, when they will occur, their effects, and when they will stop. Qualitative process theory defines a simple notion of physical process that appears useful as a language in which to write dynamical theories. Reasoning about processes also motivates a new qualitative representation for quantity in terms of inequalities, called the quantity space. This paper describes the basic concepts of qualitative process theory, several different kinds of reasoning that can be performed with them, and discusses its implications for causal reasoning. Several extended examples illustrate the utility of the theory, including figuring out that a boiler can blow up, that an oscillator with friction will eventually stop, and how to say that you can pull with a string, but not push with it. 1
56|Knowledge-based Analysis of Microarray Gene Expression Data By Using Support Vector Machines|We introduce a method of functionally classifying genes by using gene expression data from DNA microarray hybridization experiments. The method is based on the theory of support vector machines (SVMs). SVMs are considered a supervised computer learning method because they exploit prior knowledge of gene function to identify unknown genes of similar function from expression data. SVMs avoid several problems associated with unsupervised clustering methods, such as hierarchical clustering and self-organizing maps. SVMs have many mathematical features that make them attractive for gene expression analysis, including their exibility in choosing a similarity function, sparseness of solution when dealing with large data sets, the ability t...
57|Clustering Gene Expression Patterns|Recent advances in biotechnology allow researchers to measure expression levels for thousands of genes simultaneously, across different conditions and over time. Analysis of data produced by such experiments offers potential insight into gene function and regulatory mechanisms. A key step in the analysis of gene expression data is the detection of groups of genes that manifest similar expression patterns. The corresponding algorithmic problem is to cluster multi-condition gene expression patterns. In this paper we describe a novel clustering algorithm that was developed for analysis of gene expression data. We define an appropriate stochastic error model on the input, and prove that under the conditions of the model, the algorithm recovers the cluster structure with high probability. The running time of the algorithm on an n-gene dataset is O(n 2 (log(n)) c ). We also present a practical heuristic based on the same algorithmic ideas. The heuristic was implemented and its p...
58|Genetic Network Inference: From Co-Expression Clustering To Reverse Engineering|motivation: Advances in molecular biological, analytical and computational technologies are enabling us to systematically investigate the complex molecular processes underlying biological systems. In particular, using highthroughput gene expression assays, we are able to measure the output of the gene regulatory network. We aim here to review datamining and modeling approaches for conceptualizing and unraveling the functional relationships implicit in these datasets. Clustering of co-expression profiles allows us to infer shared regulatory inputs and functional pathways. We discuss various aspects of clustering, ranging from distance measures to clustering algorithms and multiple-cluster memberships. More advanced analysis aims to infer causal connections between genes directly, i.e. who is regulating whom and how. We discuss several approaches to the problem of reverse engineering of genetic networks, from discrete Boolean networks, to continuous linear and non-linear models. We conclude that the combination of predictive modeling with systematic experimental verification will be required to gain a deeper insight into living organisms, therapeutic targeting and bioengineering.
60|Qualitative Simulation of Genetic Regulatory Networks Using Piecewise-Linear Models|In order to cope with the large amounts of data that have become available in genomics,  mathematical tools for the analysis of networks of interactions between  genes, proteins, and other molecules are indispensable. We present a method  for the qualitative simulation of genetic regulatory networks, based on a class of  piecewise-linear (PL) differential equations that has been well-studied in mathematical  biology. The simulation method is well-adapted to state-of-the-art measurement  techniques in genomics, which often provide qualitative and coarsegrained  descriptions of genetic regulatory networks. Given a qualitative model  of a genetic regulatory network, consisting of a system of PL differential equations  and inequality constraints on the parameter values, the method produces a  graph of qualitative states and transitions between qualitative states, summarizing  the qualitative dynamics of the system. The qualitative simulation method  has been implemented in Java in the computer tool Genetic Network Analyzer.
61|A rigorous derivation of the chemical master equation|It is widely believed that the chemical master equation has no rigorous microphysical basis, and hence no a priori claim to validity. This view is challenged here through arguments purporting to show that the chemical master equation is exact for any gas-phase chemical system that is kept well stirred and thermally equilibrated. 
62|The Escherichia coli MG1655 in silico metabolic genotype: Its definition, characteristics, and capabilities|The Escherichia coli MG1655 genome has been completely sequenced. The annotated sequence, biochemical information, and other information were used to reconstruct the E. coli metabolic map. The stoichiometric coefficients for each metabolic enzyme in the E. coli metabolic map were assembled to construct a genomespecific stoichiometric matrix. The E. coli stoichiometric matrix was used to define the system’s characteristics and the capabilities of E. coli metabolism. The effects of gene deletions in the central metabolic pathways on the ability of the in silico metabolic network to support growth were assessed, and the in silico predictions were compared with experimental observations. It was shown that based on stoichiometric and capacity constraints the in silico analysis was able to qualitatively predict the growth potential of mutant strains in 86 % of the cases examined. Herein, it is demonstrated that the synthesis of in silico metabolic genotypes based on
63|A Test Case of Correlation Metric Construction of a Reaction Pathway from Measurements|A method for the prediction of the interactions within complex reaction networks from experimentally measured time series of the concentration of the species composing the system has been tested experimentally on the first few steps of the glycolytic pathway. The reconstituted reaction system, containing eight enzymes and 14 metabolic inter-mediates, was kept away from equilibrium in a continuous-flow, stirred-tank reactor. Input concentrations of adenosine monophosphate and citrate were externally varied over time, and their concentrations in the reactor and the response of eight other species were measured. Multidimensional scaling analysis and heuristic algorithms applied to two-species time-lagged correlation functions derived from the time series yielded a diagram from which the interactions among all of the species could be deduced. The diagram predicts essential features of the known reaction network in regard to chemical reactions and interactions among the measured species. The approach is applicable to many complex reaction systems. Traditionally, chemical kinetics reveals the mechanism of a chemical reaction by
64|Kinetic analysis of a molecular model of the budding yeast cell cycle|The molecular machinery of cell cycle control is known in more detail for budding yeast, Saccharomyces cerevisiae, than for any other eukaryotic organism. In recent years, many elegant experiments on budding yeast have dissected the roles of cyclin molecules (Cln1–3 and Clb1–6) in coordinating the events of DNA synthesis, bud emergence, spindle formation, nuclear division, and cell separation. These experimental clues suggest a mechanism for the principal molecular interactions controlling cyclin synthesis and degradation. Using standard techniques of biochemical kinetics, we convert the mechanism into a set of differential equations, which describe the time courses of three major classes of cyclin-dependent kinase activities. Model in hand, we examine the molecular events controlling “Start ” (the commitment step to a new round of chromosome replication, bud formation, and mitosis) and “Finish ” (the transition from metaphase to anaphase, when sister chromatids are pulled apart and the bud separates from the mother cell) in wild-type cells and 50 mutants. The model accounts for many details of the physiology, biochemistry, and genetics of cell cycle control in budding yeast.
65|Hybrid Modeling and Simulation of Biomolecular Networks|In a biological cell, cellular functions and the genetic regulatory  apparatus are implemented and controlled by a network of chemical  reactions in which regulatory proteins can control genes that produce  other regulators, which in turn control other genes. Further, the feedback  pathways appear to incorporate switches that result in changes in  the dynamic behavior of the cell. This paper describes a hybrid systems  approach to modeling the intra-cellular network using continuous di#erential  equations to model the feedback mechanisms and mode-switching  to describe the changes in the underlying dynamics. We use two case  studies to illustrate a modular approach to modeling such networks and  describe the architectural and behavioral hierarchy in the underlying  models. We describe these models using Charon [2], a language that  allows formal description of hybrid systems. We provide preliminary simulation  results that demonstrate how our approach can help biologists  in their analysis of noisy genetic circuits. Finally we describe our agenda  for future work that includes the development of models and simulation  for stochastic hybrid systems.
66|Positive and Negative Circuits in Dynamical Systems|We state precisely and demonstrate two conjectures of R. Thomas following which a) the existence of a positive circuit in the oriented interaction graph of a dioeerential system is a necessary condition for the existence of several steady states, and b) the existence of a negative non-oriented circuit of length at least two is a necessary condition for the existence of a stable periodic orbit.
67|Lateral Inhibition through Delta-Notch Signaling: A Piecewise Affine Hybrid Model|Biological cell networks exhibit complex combinations of both discrete and continuous behaviors: indeed, the dynamics that govern the spatial and temporal increase or decrease of protein concentration inside a single cell are continuous di#erential equations, while the activation or deactivation of these continuous dynamics are triggered by discrete switches which encode protein concentrations reaching given thresholds. In this paper, we model as a hybrid system a striking example of this behavior in a biological mechanism called Delta-Notch signaling, which is thought to be the primary mechanism of cell di#erentiation in a variety of cell networks. We present results in both simulation and reachability analysis of this hybrid system. We emphasize how the hybrid system model is computationally superior (for both simulation and analysis) to other nonlinear models in the literature, without compromising faithful modeling of the biological phenomena. 1 
68|A Class of Piecewise Linear Differential Equations Arising In Biological Models|We investigate the properties of the solutions of a class of piecewise-linear differential equations. The equations are appropriate to model biological systems (e.g., genetic networks) in which there are switch-like interactions between the elements. The analysis uses the concept of Filippov solutions of differential equations with a discontinuous righthand side. It gives an insight into the so-called singular solutions which lie on the surfaces of discontinuity. We show that this notion clarifies the study of several examples studied in the literature.
69|Identifying Gene Regulatory Networks from Experimental Data|this paper, we propose a methodology for making sense of large, multiple time-series data sets arising in expression analysis, and evaluate it both theoretically and through a case study. First, we build a graph representing all putative activation/inhibition relationships by analyzing the expression profiles for all pairs of genes. Second, we prune this graph by solving a combinatorial optimization problem to identify a small set of interesting candidate regulatory elements. We do not assert that we identify &#034;the&#034; regulatory network as a result of this computation. However, we believe that our approach quickly enables biologists to identify and visualize interesting features from raw expression array data sets. We have implemented our methodology and applied it to the analysis of the Saccharomyces cerevisiae data set. In this paper, we report on our implementation and the results of our data analysis. The problem of inducing gene regulation networks has recently come to the computational biology community. Initial attempts at modeling gene expression abd programs to induce regulatory networks from data include [2, 10, 13]. To take fullest advantage of laboratory experiments that can be performed in which a given set of genes can be explicitly expressed or repressed, and the consequences of these genes on expression biologically determined, Akutsu, et.al. [1] considers the problem of designing a minimum-size series of experiments guaranteed to result in the identification of the correct regulatory network. The candidate regulatory network proposed by our system depends upon the specific optimization criteria employed in the second phase of our procedure, although our experiments suggest that the optimal network is surprisingly robust to changes in the objective function...
70|Qualitative and Quantitative Simulation: Bridging the Gap|Shortcomings of qualitative simulation and of quantitative simulation motivate combining them to do simulations exhibiting strengths of both. The resulting class of techniques is called  semi-quantitative simulation. One approach to semi-quantitative simulation is to use numeric intervals to represent incomplete quantitative information. In this research we demonstrate semiquantitative simulation using intervals in an implemented semi-quantitative simulator called Q3. Q3 progressively refines a qualitative simulation, providing increasingly specific quantitative predictions which can converge to a numerical simulation in the limit while retaining important correctness guarantees from qualitative and interval simulation techniques.  Q3&#039;s simulations are based on a technique we call step size refinement. While a pure qualitative simulation has a very coarse step size, representing the state of a system trajectory at relatively few qualitatively distinct states, Q3 interpolates newly expl...
71|A System for Identifying Genetic Networks from Gene Expression Patterns Produced by Gene Disruptions and Overexpressions|A hot research topic in genomics is to analyze the interactions between genes by systematic  gene disruptions and gene overexpressions. Based on a boolean network model without time delay,  we have been investigating e#cient strategies for identifying a genetic network by multiple gene  disruptions and overexpressions. This paper first shows the relationship between our boolean  network model without time delay and the standard synchronous boolean network model. Then we  present a simulator of boolean networks without time delay for multiple gene disruptions and gene  overexpressions, which includes a genetic network identifier with a graphic interface that generates  instructions for experiments of gene disruptions and overexpressions.
72|Knowledge-Based Simulation of DNA Metabolism: Prediction of Enzyme Action|We have developed a knowledge-based simulation of DNA metabolism that accurately predicts the actions of enzymes on DNA under a large number of environmental conditions. Previous simulations of enzyme systems rely predominantly on mathematical models. We use a frame-based representation to model enzymes, substrates, and conditions. Interactions between these objects are expressed using production rules and an underlying truth maintenance system. The system performs rapid inference and can explain its reasoning. A graphical interface provides access to all elements of the simulation, including object representations and explanation graphs. Predicting enzyme action is the first step in the development of a large knowledge base to envision the metabolic pathways of DNA replication and repair.  1. Introduction  Our understanding of any process can be measured by the extent to which a simulation we create mimics the real behavior of that process. Deviations of a simulation indicate either l...
73|BioSim - A New Qualitative Simulation Environment for Molecular Biology|Traditionally, biochemical systems are modelled using kinetics and differential equations in a quantitative simulator. However, for many biological processes detailed quantitative information is not available, only qualitative or fuzzy statements about the nature of interactions. In a previous paper we have shown the applicability of qualitative reasoning methods for molecular biological regulatory processes. Now, we present a newly developed simulation environment, BioSim, that is written in Prolog using constraint logic programming techniques. The simulator combines the basic ideas of two main approaches to qualitative reasoning and integrates the contents of a molecular biology knowledge base, EcoCyc. We show that qualitative reasoning can be combined with automatic transformation of contents of genomic databases into simulation models to give an interactive modelling system that reasons about the relations and interactions of biological entities. This is demonstrated on the glycoly...
74|Modeling the Activity of Single Genes |Introduction  1.1 Motivation -- the challenge of understanding gene regulation  The central dogma of molecular biology states that information is stored in DNA, transcribed to messenger RNA (mRNA) and then translated into proteins. This picture is significantly augmentated when we consider the action of certain proteins in regulating transcription. These transcription factors provide a feedback pathway by which genes can regulate one another&#039;s expression as mRNA and then as protein. To review: DNA, RNA and proteins have different functions. DNA is the molecular storehouse of genetic information. When cells divide, the DNA is replicated, so that each daughter cell maintains the same genetic information as the mother cell. RNA acts as a go-between from DNA to proteins. Only a single copy of DNA is present, but multiple copies of the same piece of RNA may be present, allowing cells to make huge amounts of protein. In eukaryotes (organisms with a nuc
75|Sensitivity of biological models to errors in parameter estimates|Since A. M. Turing’s paper proposing a mathematical basis for pattern formation in developing organisms many mathematical approaches have been proposed to model biological phenomenon. Continued laboratory study and recent improvements in measurement capabilities have provided an immense quantity of raw gene expression data. The level of data now available demands the development of well-characterized and tested computational tools. Thus, we have examined one mathematical model’s sensitivity to errors in estimating its ’ parameters. Errors in parameter estimation can arise from noise in the laboratory measurements and recasting of laboratory data. We elected to examine the rulebased mathematical model of Mjolsness et al for its ’ sensitivity to errors in estimated parameters. We have used the technique of sensitivity equations as generally applied in nonlinear systems analysis.
76|A Knowledge Base for D. melanogaster gene interactions involved in pattern formation |The understanding of pattern formation in Drosophila  requires the handling of the many genetic and molecular  interactions which occur between developmental genes. For  that purpose, a knowledge base (KNIFE) has been developed  in order to structure and manipulate the interaction data.  KNIFE contains data about interactions published in the  literature and gathered from various databases. These data  are structured in an object knowledge representation system  into various interrelated entities. KNIFE can be browsed  through a WWW interface in order to select, classify and  examine the objects and their references in other bases. It  also provides specialised biological tools such as interaction  network manipulation and diagnosis of missing interactions.  We are interested in the biological process of pattern formation in Drosophila and in understanding the basis of specific identity acquisition by the different body parts [Fasano et al. 1991; Rder, Vola and Kerridge 1992; Alexandre...
77|Factor Graphs and the Sum-Product Algorithm|A factor graph is a bipartite graph that expresses how a &#034;global&#034; function of many variables factors into a product of &#034;local&#034; functions. Factor graphs subsume many other graphical models including Bayesian networks, Markov random fields, and Tanner graphs. Following one simple computational rule, the sum-product algorithm operates in factor graphs to compute---either exactly or approximately---various marginal functions by distributed message-passing in the graph. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative &#034;turbo&#034; decoding algorithm, Pearl&#039;s belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform algorithms. 
78|Near Shannon limit error-correcting coding and decoding|Abstract- This paper deals with a new class of convolutional codes called Turbo-codes, whose performances in terms of Bit Error Rate (BER) are close to the SHANNON limit. The Turbo-Code encoder is built using a parallel concatenation of two Recursive Systematic Convolutional codes and the associated decoder, using a feedback decoding rule, is implemented as P pipelined identical elementary decoders. Consider a binary rate R=1/2 convolutional encoder with constraint length K and memory M=K-1. The input to the encoder at time k is a bit dk and the corresponding codeword
79|Low-Density Parity-Check Codes|Preface The Noisy Channel Coding Theorem discovered by C. E. Shannon in 1948 offered communication engineers the possibility of reducing error rates on noisy channels to negligible levels without sacrificing data rates. The primary obstacle to the practical use of this theorem has been the equipment complexity and the computation time required to decode the noisy received data.
80|Typesetting Concrete Mathematics|... tried my best to make the book mathematically interesting, but I also knew that it would be typographically interesting-because it would be the first major use of a new typeface by Hermann Zapf, commissioned by the American Mathematical Society. This typeface, called AMS Euler, had been carefully digitized and put into METAFONT form by Stanford&#039;s digital typography students [a]; but it had not yet been &#034;tuned up &#034; for real applications. My new book served as an ideal test case, because (1) it involved a great variety of mathematical formulas; (2) I was highly motivated to make the book readable and attractive; (3) my experiences with tuning up Computer Modern gave me insights into how to set the mysterious font parameters used by TEX in math mode; and (4) the book was in fact being dedicated to Leonhard Euler, the great mathematician after whom the typeface was named. The underlying philosophy of Zapf&#039;s Euler design was to capture the flavor of mathematics as it might be written by a mathematician with excellent handwriting. For example, one of the earmarks of AMS Euler is its zero, &#039;O&#039;, which is slightly pointed at the top because a handwritten zero rarely closes together smoothly when the curve returns to its starting point. A handwritten rather than mechanical style is appropriate for mathematics because people generally create math with pen, pencil, or chalk. The Euler letters are upright, not italic, so that there is a general consistency with mathematical symbols like plus signs and parentheses, and so that built-up formulas fit together comfortably. AMS Euler includes seven alphabets: Uppercase Roman (ABC through XYZ), lowercase Roman (abc through xyz), uppercase
81|Iterative Decoding of Compound Codes by Probability Propagation in Graphical Models|Abstract—We present a unified graphical model framework for describing compound codes and deriving iterative decoding algorithms. After reviewing a variety of graphical models (Markov random fields, Tanner graphs, and Bayesian networks), we derive a general distributed marginalization algorithm for functions described by factor graphs. From this general algorithm, Pearl’s belief propagation algorithm is easily derived as a special case. We point out that recently developed iterative decoding algorithms for various codes, including “turbo decoding ” of parallelconcatenated convolutional codes, may be viewed as probability propagation in a graphical model of the code. We focus on Bayesian network descriptions of codes, which give a natural input/state/output/channel description of a code and channel, and we indicate how iterative decoders can be developed for paralleland serially concatenated coding systems, product codes, and low-density parity-check codes. Index Terms — Concatenated coding, decoding, graph theory, iterative methods, product codes.
82|Good Codes based on Very Sparse Matrices|. We present a new family of error-correcting codes for the binary symmetric channel. These codes are designed to encode a sparse  source, and are defined in terms of very sparse invertible matrices, in such a way that the decoder can treat the signal and the noise symmetrically. The decoding problem involves only very sparse matrices and sparse vectors, and so is a promising candidate for practical decoding. It can be proved that these codes are `very good&#039;, in that sequences of codes exist which, when optimally decoded, achieve information rates up to the Shannon limit. We give experimental results using a free energy minimization algorithm and a belief propagation algorithm for decoding, demonstrating practical performance superior to that of both Bose-Chaudhury-Hocquenghem codes and Reed-Muller codes over a wide range of noise levels. We regret that lack of space prevents presentation of all our theoretical and experimental results. The full text of this paper may be found elsewher...
83|Probability Propagation and Iterative Decoding|In this paper, we present a unified graphical model framework for describing codes and deriving iterative decoding algorithms. We illustrate how the following systems can be described using graphical models: turbo-codes, serially-concatenated convolutional codes, frame-oriented turbo-codes, low-density parity-check codes, product codes, and convolutional codes on channels with memory. Recently proposed iterative decoding algorithms (e.g., turbo-decoding) can be viewed as a simple message passing procedure on the graphical model. This framework provides the means to derive new iterative decoders for new codes quite easily.
84|Interleaver Properties and Their Applications to the Trellis Complexity Analysis of Turbo Codes|In the first part of this paper, the basic theory of interleavers is revisited in a semi-tutorial manner, and extended to encompass noncausal interleavers. The parameters that characterize the interleaver behavior (like delay, latency, and period) are clearly defined. The input--output interleaver code is introduced and its complexity studied. Connections among various interleaver parameters are explored. The classes of convolutional and block interleavers are considered, and their practical implementation discussed. In the second part, the trellis complexity of turbo codes is tied to the complexity of the constituent interleaver. A procedure of complexity reduction by coordinate permutation is presented, together with some examples of its application.
85|The English noun phrase in its sentential aspect|This dissertation is a defense of the hypothesis that the noun phrase is headed by afunctional element (i.e., \non-lexical &amp;quot; category) D, identi ed with the determiner. In this way, the structure of the noun phrase parallels that of the sentence, which is headed by In (ection), under assumptions now standard within the Government-Binding (GB) framework. The central empirical problem addressed is the question of the proper analysis of the so-called \Poss-ing &amp;quot; gerund in English. This construction possesses simultaneously many properties of sentences, and many properties of noun phrases. The problem of capturing this dual aspect of the Possing construction is heightened by current restrictive views of X-bar theory, which, in particular, rule out the obvious structure for Poss-ing, [NP NP VPing], by virtue of its exocentricity. Consideration of languages in which nouns, even the most basic concrete nouns, show agreement (AGR) with their possessors, points to an analysis
87|AgentSpeak(L): BDI Agents speak out in a logical computable language  (1996) |Belief-Desire-Intention (BDI) agents have been investigated by many researchers from both a theoretical specification perspective and a practical design perspective. However, there still remains a large gap between theory and practice. The main reason for this has been the complexity of theorem-proving or model-checking in these expressive specification logics. Hence, the implemented BDI systems have tended to use the three major attitudes as data structures, rather than as modal operators. In this paper, we provide an alternative formalization of BDI agents by providing an operational and proof-theoretic semantics of a language AgentSpeak(L). This language can be viewed as an abstraction of one of the implemented BDI systems (i.e., PRS) and allows agent programs to be written and interpreted in a manner similar to that of horn-clause logic programs. We show how to perform derivations in this logic using a simple example. These derivations can then be used to prove the properties satis...
88|Planned Team Activity|Agents situated in dynamic environments benefit from having a repertoire of plans, supplied in advance, that permit them to rapidly generate appropriate sequences of actions in response to important events. When agents can form teams, new problems emerge regarding the representation and execution of joint actions. In this paper we introduce a language for representing joint plans for teams of agents, we describe how agents can organize the formation of a suitably skilled team to achieve a joint goal, and we explain how such a team can execute these plans to generate complex, synchronized team activity. The formalism provides a framework for representing and reasoning about joint actions in which various approaches to co-ordination and commitment can be explored.  1 Introduction  A rational agent can be viewed as a system continuously receiving perceptual input from the environment in which it is embedded and responding by taking actions that affect that environment. It can be characte...
89|Foundations of a Logical Approach to Agent Programming|This paper describes a novel approach to high-level agent programming based on a highly  developed logical theory of action. The user provides a specification of the agents&#039; basic actions  (preconditions and effects) as well as of relevant aspects of the environment, in an extended  version of the situation calculus. He can then specify behaviors for the agents in terms of these  actions in a programming language where one can refer to conditions in effect in the environment.  When an implementation of the basic actions is provided, the programs can be executed in a  real environment; otherwise, a simulated execution is still possible. The interpreter automatically  maintains the world model required to execute programs based on the specification. The theoretical  framework includes a solution to the frame problem, allows agents to have incomplete knowledge  of their environment, and handles perceptual actions. The theory can also be used to prove  programs correct. A simple meeting sc...
90|On Being Responsible|this paper is to provide a framework in which one particular class of social activity can be formalised and ultimately analysed: namely that in which a group of autonomous agents (at least two) decides they wish to work together as a team to solve a common problem. A comprehensive theory describing this class of social interaction would need to cover at least the following aspects: when to initiate team activity, how to go about assembling the team, how to plan and distribute work within the team, how to behave once team activity has been initiated and how to complete team activity. The framework described herein defines the prerequisites for such action and also prescribes how agents should behave (both in their own problem solving and with respect to other group members) once the problem solving has been established. Typically in a community of autonomous agents, one of the primary motives for joint action is when no individual is capable of achieving a desired objective alone; only by combining and coordinating with others can the target be reached. Joint action is usually a reciprocal process in which participating agents augment their objectives and problem solving to comply with those of others - hence it is a fairly sophisticated form of cooperation. It requires greater knowledge, awareness and reflection by an agent both with respect to its own problem solving objectives and about their compatibility with the objectives of others, than simpler forms of social interaction (such as task and result sharing [19]). Joint action, by definition, requires an objective the group wishes to achieve - it is the glue which binds the team together. As a consequence of the autonomous nature of the agents, team members will only participate if they can derive some benefit from ...
91|Specification and implementation of a belief desire joint-intention architecture for collaborative problem solving|Systems composed of multiple interacting problem solvers are becoming increasingly pervasive and have been championed in some quarters as the basis of the next generation of intelligent information systems. If this technology is to fulfill its true potential then it is important that the systems which are developed have a sound theoretical grounding. One aspect of this foundation, namely the model of collaborative problem solving, is examined in this paper. A synergistic review of existing models of cooperation is presented, their weaknesses are highlighted and a new model (called joint responsibility) is introduced. Joint responsibility is then used to specify a novel high-level agent architecture for cooperative problem solving in which the mentalistic notions of belief, desire, intention and joint intention play a central role in guiding an individual’s and the group’s problem solving behaviour. An implementation of this high-level architecture is then discussed and its utility is illustrated for the real-world domain of electricity transportation management.
92|How Agents Do It In Stream Logic Programming|The key factor that will determine the speed and depth to which multi-agent systems penetrate the commercial marketplace is the ease with which applications can be developed. One approach is to use general purpose languages to construct layers of agent level constructs. Object-oriented languages have been advocated as appropriate for the complexity of distributed systems. According to Gasser and Briot [1992], the key problem with the common forms of object based concurrent programming is the fixed boundaries they give to agents are too inflexible. They do not reflect either the theoretical positions emerging in Multi-agent systems, MAS, nor the reality of multilevel aggregations of actions and knowledge. This paper advocates the use of a rather different type of object based concurrent language, stream logic programming, SLP, that does not have this drawback. 1 Object Based Concurrent Programming  The key factor that will determine the speed and depth to which multi-agent systems, MAS,...
93|Snort - Lightweight Intrusion Detection for Networks|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
94|Logical foundations of object-oriented and frame-based languages|We propose a novel formalism, called Frame Logic (abbr., F-logic), that accounts in a clean and declarative fashion for most of the structural aspects of object-oriented and frame-based languages. These features include object identity, complex objects, inheritance, polymorphic types, query methods, encapsulation, and others. In a sense, F-logic stands in the same relationship to the objectoriented paradigm as classical predicate calculus stands to relational programming. F-logic has a model-theoretic semantics and a sound and complete resolution-based proof theory. A small number of fundamental concepts that come from object-oriented programming have direct representation in F-logic; other, secondary aspects of this paradigm are easily modeled as well. The paper also discusses semantic issues pertaining to programming with a deductive object-oriented language based on a subset of F-logic.  
95|The Stable Model Semantics For Logic Programming|We propose a new declarative semantics for logic programs with negation. Its formulation is quite simple; at the same time, it is more general than the iterated fixed point semantics for stratied programs, and is applicable to some useful programs that are not stratified.
96|An Overview of the C++ Programming Language|This overview of C++ presents the key design, programming, and language-technical concepts using examples to give the reader a feel for the language. C++ is a general-purpose programming language with a bias towards systems programming that supports efficient low-level computation, data abstraction, object-oriented programming, and generic programming.  
97|A semantics of multiple inheritance|There are two major ways of structuring data in programming languages. The first and common one, used for example in Pascal, can be said to derive from standard branches of mathematics. Data is organized as cartesian products (i.e. record types), disjoint sums (i.e. unions or variant types) and function spaces (i.e. functions and procedures).
98|Querying object-oriented databases|We present a novel language for querying object-oriented databases. The language is built around the idea of extended path expressions that substantially generalize [ZAN83], and on an adaptation of the first-order formalization of object-oriented languages from [KW89, KLW90, KW92]. The language incorporates features not found in earlier proposals; it is easier to use and has greater expressive power. Some of the salient features of our language are: ffl Precise model-theoretic semantics. ffl A very expressive form of path expressions that not only can do joins, selections and unnesting, but can also be used to explore the database schema. ffl Views can be defined and manipulated in a much more uniform way than in other proposals. ffl Database schema can be explored in the very same language that is used to retrieve data. Unlike in relational languages, the user needs not know anything about the system tables that store schema information. ffl The notions of a type and type-correctness have precise meaning. It accommodates a wide variety of queries that might be deemed well- or ill-typed under different circumstances. In particular, we show that there is more than one way of settling the issue of type correctness. For expository purposes and due to space limitation, we chose to make a number of simplifying assumptions and left some features out. A more complete account can be found in [KSK92].
99|The Object-Oriented Database System Manifesto|This paper attempts to define an object-oriented database system. It describes the main features and characteristics that a system must have to qualify as an objectoriented database system. We have separated these characteristics into three groups: ffl Mandatory, the ones the system must satisfy in order to be termed an objectoriented database system. These are complex objects, object identity, encapsulation, types or classes, inheritance, overriding combined with late binding, extensibility, computational completeness, persistence, secondary storage management, concurrency, recovery and an ad hoc query facility. ffl Optional, the ones that can be added to make the system better, but which are not mandatory. These are multiple inheritance, type checking and inferencing, distribution, design transactions and versions. ffl Open, the points where the designer can make a number of choices. These are the programming paradigm, the representation system, the type system, and uniformity. We...
100|On the Power of Magic|This paper considers the efficient evaluation of recursive queries expressed using Horn Clauses. We define sideways information passing formally and show how a query evaluation algorithm may be defined in terms of sideways information passing and control. We then consider a class of information passing strategies that suffices to describe most query evaluation algorithms in the database literature, and show that these strategies may always be implemented by rewriting a given program and evaluating the rewritten program bottom-up. We describe in detail several algorithms for rewriting a program. These algorithms generalize the Counting and Magic Sets algorithms to work with arbitrary programs. Safety and optimality of the algorithms are also considered. 1. Introduction  The evaluation of recursive queries expressed as sets of Horn Clauses over a database has recently received much attention. Consider the following program:  anc (X, Y) :- par (X, Y) anc (X, Y) :- par (X, Z), anc (Z, Y)  ...
101|HiLog: A foundation for higher-order logic programming|We describe a novel logic, called HiLog, and show that it provides a more suitable basis for logic programming than does traditional predicate logic. HiLog has a higher-order syntax and allows arbitrary terms to appear in places where predicates, functions and atomic formulas occur in predicate calculus. But its semantics is first-order and admits a sound and complete proof procedure. Applications of HiLog are discussed, including DCG grammars, higher-order and modular logic programming, and deductive databases.
102|Theory of Generalized Annotated Logic Programming and its Applications|Annotated logics were introduced in [43] and later studied in [5, 7, 31, 32]. In [31], annotations were extended to allow variables and functions, and it was argued that such logics can be used to provide a formal semantics for rule-based expert systems with uncertainty. In this paper we continue to investigate the power of this approach. First, we introduce a new semantics for such programs based on ideals of lattices. Subsequently, some proposals for multivalued logic programming [5, 7, 32, 47, 40, 18] as well as some formalisms for temporal reasoning [1, 3, 42] are shown to fit into this framework. As an interesting by-product of this investigation, we obtain a new result concerning multivalued logic programming: a model theory for Fitting&#039;s bilattice-based logic programming, which until now has not been characterized model-theoretically. This is accompanied by a corresponding proof theory. 1 Introduction  Large knowledge bases can be inconsistent in many ways. Nevertheless, certain...
103|F-Logic: A Higher-Order Language for Reasoning about Objects, Inheritance and Scheme|All in-text	references	underlined	in	blue	are	linked	to	publications	on	ResearchGate, letting you	access	and	read	them	immediately.
104|Every Logic Program Has a Natural Stratification And an Iterated Least Fixed Point Model (Extended Abstract)  (1989) |1 Introduction  The perfect model semantics [ABW88, VG89b, Prz88a, Prz89b] provides an attractive alternative to the traditionally used semantics of logic programs based on Clark&#039;s completion of the program [Cla78, Llo84, Fit85, Kun87]. Perfect models are minimal models of the program, which can be equivalently described as iterated least fixed points of natural operators [ABW88, VG89b], as iterated least models of the program [ABW88, VG89b] or as preferred models with respect to a natural priority relation [Prz88a, Prz89b]. As a result, the perfect model semantics is not only very intuitive, but it also has been proven equivalent to suitable forms of all four major formalizations of non-monotonic reasoning in AI (see [Prz88b]) and is used in existing database [Zan88] and truth maintenance systems. Additionally, the perfect model semantics eliminates some serious drawbacks of Clark&#039;s semantics [Prz89b] and admits a natural sound and complete procedural mechanism, called SLSresolution [...
106|A clash of intuitions: The current state of nonmonotonic multiple inheritance systems|Abstract: Early attempts at combining multiple inheritance with nonmonotonic reasoning were based on straight forward extensions of tree-structured inheritance systems, and were theoretically unsound. In The Mathematics of Inheritance Systems, or TMOIS, Touretzky described two problems these systems cannot handle: reasoning in the presence of true but redundant assertions, and coping with ambiguity. TMOIS provided a definition and analysis of a theoretically sound multiple inheritance system, accompanied by inference algorithms. Other definitions for inheritance have since been proposed that are equally sound and intuitive, but do not always agree with TMOIS. At the heart of the controversy is a clash of intuitions about certain fundamental issues such as skepticism versus credulity, the direction in which inheritance paths are extended, and classical versus intuitive notions of consistency. Just as there are alternative logics, there may be no single &#034;best&#034; approach to nonmonotonic multiple inheritance. 1.
107|On The Power Of Languages For The Manipulation Of Complex Objects|Various models and languages for describing and manipulating hierarchically structured data have been proposed. Algebraic, calculus-based and logic-programming oriented languages have all been considered. This paper presents a general model for complex objects, and languages for it based on the three paradigms. The algebraic language generalizes those presented in the literature; it is shown to be related to the functional style of programming advocated by Backus. The notion of domain independence familiar from relational databases is defined, and syntactic restrictions (referred to as safety conditions) on calculus queries are formulated, that guarantee domain independence. The main results are: The domain-independent calculus, the safe calculus, the algebra, and the logic-programming oriented language have equivalent expressive power. In particular, recursive queries, such as the transitive closure, can be expressed in each of the languages. For this result, the algebra needs the pow...
108|Magic Templates: A Spellbinding Approach to Logic Programs|We consider a bottom-up query-evaluation scheme in which facts of relations are allowed to have nonground terms. The Magic Sets query-rewriting technique is generalized to allow arguments of predicates to be treated as bound even though the rules do not provide ground bindings for those arguments. In particular, we regard as &#034;bound&#034; any argument containing a function symbol or a variable that appears more than once in the argument list. Generalized &#034;magic &#034; predicates are thus defined to compute the set of all goals reached in a top-down exploration of the rules, starting from a given query goal; these goals are not facts of constants as in previous versions of the Magic Sets algorithm. The magic predicates are then used to restrict a bottom-up evaluation of the rules so that there are no redundant actions; that is, every step of the bottom-up computation must be performed by any algorithm that uses the same sideways information passing strategy (sips). The price paid, compared to prev...
109|Transaction Logic Programming|An extension of predicate logic, called Transaction Logic, is proposed, which accounts in a clean and declarative fashion for the phenomenon of state changes in logic programs and databases. Transaction Logic has a natural model theory and a sound and complete proof theory, but unlike many other logics, it allows users to program transactions. The semantics leads naturally to features whose amalgamation in a single logic has proved elusive in the past. These features include both hypothetical and committed updates, dynamic constraints on transaction execution, nondeterminism, and bulk updates. Finally, Transaction Logic holds promise as a logical model of hitherto non-logical phenomena, including so-called procedural knowledge in AI, and the behavior of object-oriented databases, especially methods with side effects. This paper presents the semantics of Transaction Logic coupled with a sound and complete proof theory for a Horn-like subset of the logic. To appear in the Proceedings...
110|Object-Oriented Programming: Themes and Variations|M any of the ideas behind object-oriented programming have roots going back to SIMULA (Dahl &amp; Nygaard, 1966). The first substantial interactive, display-based im-plementation was the SMALLTALK language (Goldberg &amp; Robson, 1983). The object-oriented style has of-ten been advocated for simulation programs, systems pro-gramming, graphics, and AI programming. The history of ideas has some additional threads including work on message passing as in ACTORS (Lieberman, 1981), and multiple inheritance as in FLAVORS (Weinreb &amp; Moon, 1981). It is also related to a line of work in AI on the the-ory of frames (Minsky, 1975) and their implementation in knowledge representation languages such as KRL (Bobrow
111|A Skeptical Theory of Inheritance in Nonmonotonic Semantic Networks|This paper describes a new approach to inheritance reasoning in semantic networks allowing for multiple inheritance with exceptions. The approach leads to an analysis of defeasible inheritance which is both well-defined and intuitively attractive: it yields unambiguous results applied to any acyclic semantic network, and the results conform to our intuitions in those cases in which the intuitions themselves are firm and unambiguous. Since the definition provided here is based on an alternative, skeptical view of inheritance reasoning, however, it does not always agree with previous definitions when it is applied to nets about which our intuitions are unsettled, or in which different reasoning strategies could naturally be expected to yield distinct results. After exploring certain features of the definition presented here, we describe also a hybrid (parallel-serial) algorithm that implements the definition in a parallel marker-passing architecture.  
112|An Overview of Transaction Logic|This paper presents an overview of Transaction Logic---a new formalism recently introduced in [11, 12] and designed to deal with the phenomenon of state changes in logic programming, databases, and AI. Transaction Logic has a natural model theory and a sound and complete proof theory. Unlike many other logics, however, it is suitable for programming procedures that accomplish state transitions in a logically sound manner. Transaction logic amalgamates such features as hypothetical and  committed updates, dynamic constraints on transaction execution, nondeterminism, and bulk updates. Transaction Logic also appears to be suitable as a logical model of hitherto non-logical phenomena, including so-called procedural knowledge in AI, and the behavior of object-oriented databases, especially methods with side effects.   Work supported in part by an Operating Grant from the Natural Sciences and Engineering Research Council of Canada and by a Connaught Grant from the University of Toronto.  y  ...
113|A Logic for Reasoning with Inconsistency|Most known computational approaches to reasoning have problems when facing inconsistency, so they assume that a given logical system is consistent. Unfortunately, the latter is difficult to verify and very often is not true. It may happen that addition of data to a large system makes it inconsistent, and hence destroys the vast amount of meaningful information. We present a logic, called APC (annotated predicate calculus; cf. annotated logic programs of [3], that treats any set of clauses, either consistent or not, in a uniform way. In this logic, consequences of a contradiction are not nearly as damaging as in the standard predicate calculus, and meaningful information can still be extracted from an inconsistent set of formulae. APC has a resolution-based sound and complete proof procedure. We also introduce a novel notion of &#034;epistemic entailment&#034; and show its importance for investigating inconsistency in predicate calculus as well as its application to nonmonotonic reasoning. Most importantly, our claim that a logical theory is an adequate model of human perception of inconsistency, is actually backed by rigorous arguments.
114|ILOG: Declarative Creation and Manipulation of Object Identifiers|yosikawaQkyoto-su.ac.jp Abstract: This paper introduces ILOG ( a declarative language in the style of (stratified) datalog ( which can be used for querying, schema translation, and schema augmentation in the context of object-based data models. The semantics of ILOG is based on the use of Skolem functors, and is closely related to semantics for object-based data manipulation languages which provide mechanisms for explicit creation of object identifiers (OIDs). A normal form is presented for ILOG ’ programs not involving recursion through OID creation, which identifies a precise correspondence between OIDs created in the target, and values and OIDs in the source. The expressive power of various sublanguages of ILOG ’ is shown to range from a natural generalization of the conjunctive queries to the object-based context, to a language which can specify all computable database translat.ions (up to duplicate copies). The issue of testing vuliilityof ILOG programs translat.ing one semantic schema to another is studied: cases are presented for which several-validity issues (e.g., functional and/or subset relationships in the
115|The logic of frames|Minsky introduced the terminology of &#039;frames &#039; to unify and denote a loose collection of related ideas on knowledge representation: a collection which, since the publication of his paper (Minsky, 1975) has become even looser. It is not at all clear now what frames are, or were ever intended to be.
116|Computational Approaches to Analogical Reasoning: A Comparative Analysis|Analogical reasoning has a long history in artificial intelligence research, primarily because of its promise for Ike acquisition unit effective use of knowledge. Defined as a representational mapping from a known &amp;quot;source &amp;quot; domain into a novel &#034;target&#034; domain, analogy provides a basic mechanism for effectively connecting a reasoner&#039;s past and present experience. Using a four-component process model of analogical reasoning, this paper reviews sixteen computational studies of analogy. These studies are organized chronologically within broadly defined task domains of automated deduction, problem solving and planning, natural language comprehension, and machine learning. Drawing on these detailed reviews, a comparative analysis of diverse contributions to basic analogy processes identifies recurrent problems for studies of analogy and common approaches to their solution. The paper concludes by arguing that computational studies of analogy are in a slate of adolescence: looking to more mature research areas in artificial intelligence for robust accounts of basic reasoning processes and drawing upon a long tradition of research in other disciplines.  
117|On inheritance hierarchies with exceptions|Using default logic, we formalize NETL-like inheritance hierarchies with exceptions. This provides a number of benefits: (1) A precise semantics for such hierarchies. (2) A provably correct (with respect to the proof theory of default logic) inference algorithm for acyclic networks. (3) A guarantee that acyclic networks have extensions. (4) A provably correct quasi-parallel inference algorithm for such networks. 1.
118|Using Powerdomains to Generalize Relational Databases|Much of relational algebra and the underlying principles of relational database design have a simple representation in the theory of domains that is traditionally used in the denotational semantics of programming languages. By investigating the possible orderings on powerdomains that are well-known in the study of nondeterminism and concurrency it is possible to show that many of the ideas in relational databases apply to structures that are much more general than relations. This also suggests a method of representing database objects as typed objects in programming languages. In this paper we show how operations such as natural join and projection -- which are fundamental to relational database design -- can be generalized, and we use this generalized framework to give characterizations of several relational database concepts including functional dependencies and universal relations. All of these have a simple-minded semantics in terms of the underlying domains, which can be thought ...
120|Glue-Nail: A Deductive Database System|Glue is a procedural language for deductive databases. It is designed to complement the purely declarative NAIL! language, firstly by performing system functions impossible to write in NAIL!, and secondly by allowing the procedural specification of algorithms for critical code sections. The two languages together are sufficient to write a complete application. Glue was designed to be as close to NAIL! as possible, hence minimizing the impedance mismatch problem. In this paper we concentrate on Glue. Pseudo-higher order programming is used in both languages, in the style of HiLog [1]. In particular Glue-Nail can handle set valued attributes (non1NF schemas) in a clean and efficient manner. NAIL! code is compiled into Glue code, simplifying the system design. An experimental implementation has been written, a more efficient version is under design.
121|Access to objects by path expressions and rules|Object oriented databases provide rich structuring capabilities to organise the objects being relevant for a given application. Due to the possible complexity of object structures, path expressions have become accepted as a concise syntactical means to reference objects. Even though known approaches to path expressions provide quite elegant access to objects, there seems to be still a need for more generality. To this end, the rule-language PathLog is introduced. A first contribution of PathLog is to add a second dimension to path expressions in order to increase conci-seness. In addition, a path expression can also be used to reference virtual objects. Both enhancements give rise to interesting semantic implications. 
122|Relations with Relation Names as Arguments: Algebra and Calculus|We consider a version of the relational model in which relation names may appear as arguments of other relations. Allowing relation names as arguments provides enhanced modelling capabilities, allowing some object-oriented features to be expressed within the relational model. We extend relational algebra with operators for accessing relations, and also define a relational calculus based on the logic HiLog. We prove two equivalence results between extensions of relational algebra and fragments of our HiLog-based calculus. We show that our extensions of relational algebra provide higher expressive power than relational algebra on any given database. Finally, we argue that the extensions proposed here are relatively easy to provide in practice, and should be expressible within modern query languages.
123|On the Declarative and Procedural Semantics of Deductive Object-Oriented Systems|. We present declarative and procedural semantics for a deductive object-oriented language, Gulog. The declarative semantics is based on preferred minimal models. We describe both bottom-up and top-down query evaluation procedures and show that they are sound with respect to the declarative semantics. The results contribute to our understanding of the interaction of inheritance, overriding and deduction in the presence of both functional and set-valued methods, and multiple inheritance.  Keywords: complex objects, deductive databases, fixpoint semantics, procedural semantics, knowledge bases, object-oriented databases, rule based 1. Introduction  The aim of this paper is to propose a simple mathematical foundation for objectoriented systems with deduction. In particular, we present a declarative and procedural semantics for a simple language that includes classes, objects, functional and set-valued methods, (multiple) inheritance, overriding, and derived methods and predicates with the...
124|Semantics of Inheritance in Logical Object Specifications|Our goal is to integrate the paradigms of object-oriented structuring and of rule-based specifications for databases. In this paper, we consider hierarchical specifications of objects and object types with attributes defined by logical rules, and we explain their local (single object) semantics as well as their composite (object society) semantics. In order to allow inheritance with exceptions (overriding) even for rules, the defining formulas are interpreted like defaults of di#erent priorities corresponding to levels in the object type hierarchy. Here, minimal model semantics known from default reasoning in artificial intelligence or from database completions can be utilized, but must be modified to respect object-oriented issues, in particular locality. We prove that intended models exist for object and composite specifications under natural conditions, even if general clauses are used as defaults. 1 Introduction  Much has already been said about the relative merits of the object-or...
125|Towards a Real Horn Clause Language|raviOmcc.com ABSTRACT: Current database languages based on Horn clausea and the bottom-up model of computa-tion, such aa LDL and DataJog, are not as expreesive as Prolog. For example, such languages do not sup-port schema and higher-order predicates in an inte-grated framework but rely on a separate language to specify the schema information and on evaluable pred-icatea for expressing higher-order information. Prolog on the other hand while providing powerful features does so in only a procedural settiag. Caught between a rock and hard place we ask whether a Horn clause language can be designed which provides most if not all of the power of Prolog in a declarative framework. In this paper we start with a simple logic programming language in which the central notion is that of an ob-ject and an expression. We build upon these simple constructs and show that the resulting language has the power of DataIog and a bottom-up semantics. We then successively increase t6e expressive power of the language to subsume LDL in the sense that we can support sets, stratified negation, and updates to base relations. Finally, we show that our language can sup-port meta, schema and higher-order constructs in an integrated, consistent and clean framework. Permission to copy without fee alI or put of this mUcri8l is granted provided that Uu copies UC not mule or distributed for direct wxnmercial uhntage, the VLDB copyri%t notice ud the title of the publication ad its date w, and notice i givea
126|The logic of inheritance in frame systems|This paper shows how the semantics of frames with exceptions can be described logically. We define a simple (purely declarative) frame language allowing for multiple inheritance and meta classes (i.e. the instances of a class may be classes themselves). Expressions of this language are translated into first order formulas. Circumscription of a certain predicate in the resulting theory yields the desired semantics. Our approach allows the intuition that subclasses should override superclasses to be represented in a very natural way. Inheritance systems have a long tradition in AI. They allow the description of hierarchies of objects and
127|BAROQUE: A browser for relational databases|The standard, most efficient method to retrieve information from databases can be described as systematic retrieval: The needs of the user are described in a formal query, and the database management system retrieves the data promptly. There are several situations, however, in which systematic retrieval is difficult or even impossible. In such situations exploratory search (browsing) is a helpful alternative. This paper describes a new user interface, called BAROQUE, that implements exploratory searches in relational databases. BAROQUE requires few formal skills from its users. It does not assume knowledge of the principles of the relational data model or familiarity with the organization of the particular database being accessed. It is especially helpful when retrieval targets are vague or cannot be specified satisfactorily. BAROQUE establishes a view of the relational database that resembles a semantic network, and provides several intuitive functions for scanning it. The network integrates both schema and data, and supports access by value. BAROQUE can be implemented on top of any basic relational database management system but can be modified to take advantage of additional capabilities and enhancements often present in relational systems.
128|A First-Order Theory of Types and Polymorphism in Logic Programming|We describe a new logic called typed predicate calculus (T PC) that gives declarative meaning to logic programs with type declarations and type inference. T PC supports all popular types of polymorphism, such as parametric, inclusion, and ad hoc polymorphism. The proper interaction between parametric and inclusion varieties of polymorphism is achieved through a new construct, called type dependency, which is reminiscent of implication types of [PR89] but yields more natural and succinct specifications. Unlike other proposals where typing has extra-logical status, in T PC the notion of type-correctness has precise model-theoretic meaning that is independent of any specific type-checking or type-inference procedure. Moreover, many different approaches to typing that were proposed in the past can be studied and compared within the framework of our logic. As an illustration, we apply T PC to interpret and compare the results reported in [MO84, Smo88, HT90, Mis84, XW88]. Another novel featu...
129|On the declarative semantics of inheritance networks|Usually, semantics of inheritance networks is specified indirectly through a translation into one of the standard logical formalisms. Since such translation involves an algorithmic aspect, which is usually complex, these approaches to inheritance are not truly declarative. We provide a general framework for specifying a direct semantics of inheritance networks. Because the networks are not expressive enough to capture all intuitions behind inheritance, a number of significantly different semantics have been proposed. Our approach allows us to give direct semantics to a number of different proposals found in the literature, and clarifies the relationships among them. It also provides a yardstick for measuring adequacy of translation into logical formalisms of various intuitions about inheritance. 1
130|A Theory of Nonmonotonic Inheritance Based on Annotated Logic|We propose a logical language for representing networks with nonmonotonic multiple inheritance. The language is based on a variant of annotated logic studied in [5, 6, 17, 18, 19, 20, 21]. The use of annotated logic provides a rich setting that allows to disambiguate networks whose topology does not provide enough information to decide how properties are to be inherited. The proposed formalism handles inheritance via strict as well as defeasible links. We provide a formal account of the language, describe its semantics, and show how a unique intended model can be associated with every inheritance specification written in the language. Finally, we present an algorithm that correctly propagates inherited properties according to the given semantics. The algorithm is also complete in the sense that it computes the set of all properties that must be inherited by any given individual object, and then terminates.   Work supported in part by NSF grant IRI-9009587.  y  Work supported in part by...
131|Polymorphic Types in Higher-Order Logic Programming|This paper analyses the requirements to the notion of type correctness in logic programming and proposes several &#034;adequacy&#034; criteria for such a system. We then present a type theory for a higher-order logic programming language, HiLog [5], which is adequate in that sense. The proposed type system not only captures type errors of syntactic origin, but also ensures that all atoms true in a canonical model (such as the perfect model or the well-founded partial model) of a well-typed program are well-typed. Furthermore, type dependencies among arguments of functions and/or predicates are incorporated into the definition of welltyped terms and atoms, so that the benefits of both parametric and inclusion polymorphism are preserved. Finally types are treated as first-class objects and type declarations can be queried directly by users, making it a suitable framework for schema integration of heterogeneous databases.  Keywords: types, inclusion and parametric polymorphism, type dependency, arg...
133|Statecharts: A Visual Formalism For Complex Systems|We present a broad extension of the conventional formalism of state machines and state diagrams, that is relevant to the specification and design of complex discrete-event systems, such as multi-computer real-time systems, communication protocols and digital control units. Our diagrams, which we call statecharts, extend conventional state-transition diagrams with essentially three olements, dealing, respectively, with the notions of hierarchy, concurrency and communication. These transform the language of state diagrams into a highly structured&#039; and economical description language. Statecharts are thus compact and expressive--small diagrams can express complex behavior--as well as compositional and modular. When coupled with the capabilities of computerized graphics, statecharts enable viewing the description at different levels of detail, and make even very large specifications manageable and comprehensible. In fact, we intend to demonstrate here that statecharts counter many of the objections raised against conventional state diagrams, and thus appear to render specification by diagrams an attractive and plausible approach. Statecharts can be used either as a stand-alone behavioral description or as part of a more general design methodology that deals also with the system&#039;s other aspects, such as functional decomposition and data-flow specification. We also discuss some practical experience that was gained over the last three years in applying the statechart formalism to the specification of a particularly complex system.
134|On the Criteria To Be Used in Decomposing Systems into Modules|This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a “modularization ” is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.
136|Foundations for the Study of Software Architecture|The purpose of this paper is to build the foundation for software architecture. We first develop an intuition for software architecture by appealing to several well-established architectural disciplines. On the basis of this intuition, we present a model of software architec-ture that consists of three components: elements, form, and rationale. Elements are either processing, data, or connecting elements. Form is defined in terms of the properties of, and the relationships among, the elements-- that is, the constraints on the elements. The ratio-nale provides the underlying basis for the architecture in terms of the system constraints, which most often derive from the system:requirements. We discuss the compo-nents of the model in the context of both architectures and architectural styles and present an extended exam-ple to illustrate some important architecture and style considerations. We conclude by presenting some of the benefits of our approach to software architecture, sum-marizing our contributions, and relating our approach to other current work.  
137|The 4+1 view model of architecture|The 4+1 View Model organizes a description of a software architecture using five concurrent views, each of which
addresses a specific set of concerns. Architects capture their design decisions in four views and use the fifth view to illustrate and validate them.
138|Specifying Distributed Software Architectures|There is a real need for clear and sound design specifications of distributed systems at the architectural level. This is the level of the design which deals with the high-level organisation of computational elements and the interactions between those elements. The paper presents the Darwin notation for specifying this high-level organisation. Darwin is in essence a declarative binding language which can be used to define hierarchic compositions of interconnected components. Distribution is dealt with orthogonally to system structuring. The language supports the specification of both static structures and dynamic structures which may evolve during execution. The central abstractions managed by Darwin are components and services. Services are the means by which components interact. In addition to its use in specifying the architecture of a distributed system, Darwin has an operational semantics for the elaboration of specifications such that they may be used at runtime to di...
139|The design and implementation of hierarchical software systems with reusable components|We present a domain-independent model of hierarchical software system design and construction that is based on interchangeable software components and largescale reuse. The model unifies the conceptualizations of two independent projects, Genesis and Avoca, that are successful examples of software component/building-block technologies and domain modeling. Building-block technologies exploit large-scale reuse, rely on open architecture software, and elevate the granularity of programming to the subsystem level. Domain modeling formalizes the similarities and differences among systems of a domain. We believe our model is a blue-print for achieving software component technologies in many domains.
140|The X Window System|The X Window System, Version 11, is the standard window system on Linux and UNIX systems. X11, designed in 1987, was “state of the art ” at that time. From its inception, X has been a network transparent window system in which X client applications can run on any machine in a network using an X server running on any display. While there have been some significant extensions to X over its history (e.g. OpenGL support), X’s design lay fallow over much of the 1990’s. With the increasing interest in open source systems, it was no longer sufficient for modern applications and a significant overhaul is now well underway. This paper describes revisions to the architecture of the window system used in a growing fraction of desktops and embedded systems 1
141|Specification and analysis of system architecture using Rapide|  Rapide is an event-based concurrent, object-oriented language specifically designed for prototyping system architectures. Two principle design goals are (1) to provide constructs for defining executable prototypes of architectures, and (2) to adopt an execution model in which the concurrency, synchronization, dataflow, and timing properties of a prototype are explicitly represented. This paper describes the partially ordered event set (poset) execution model and outlines with examples some of the event-based features for defining communication architectures and relationships between architectures. Various features of Rapide are illustrated by excerpts from a prototype of the X/Open distributed transaction processing reference architecture.
142|A Formal Approach to Software Architecture|As software systems become more complex, the overall system structure---or software architecture---becomes a central design problem. A system&#039;s architecture provides a model of the system that suppresses implementation detail, allowing the architect to concentrate on the analyses and decisions that are most crucial to structuring the system to satisfy its requirements.  Unfortunately, current representations of software architecture are informal and ad hoc. While architectural concepts are often embodied in infrastructure to support specific architectural styles and in the initial conceptualization of a system configuration, the lack of an explicit, independently-characterized architecture or architectural style significantly limits the benefits of software architectural design in current practice.  In this dissertation, I show that an Architecture Description Language based on a formal, abstract model of system behavior can provide a practical means of describing and analyzing softwar...
143|Abstractions for Software Architecture and Tools to Support Them|Architectures for software use rich abstractions and idioms to describe system components, the nature of interactions among the components, and the patterns that guide the composition  of components into systems. These abstractions are higher-level than the elements usually  supported by programming languages and tools. They capture packaging and interaction issues  as well as computational functionality. Well-established (if informal) patterns guide architectural design of systems. We sketch a model for defining architectures and present an implementation of the basic level of that model. Our purpose is to support the abstractions  used in practice by software designers. The implementation provides a testbed for experiments  with a variety of system construction mechanisms. It distinguishes among different  types of components and different ways these components can interact. It supports abstract  interactions such as data flow and scheduling on the same footing as simple procedure...
144|Correct Architecture Refinement|A method is presented for the stepwise refinement of an abstract architecture into a relatively correct lower-level architecture that is intended to implement it. A refinement step involves the application of a predefined refinement pattern that provides a routine solution to a standard architectural design problem. A pattern contains an abstract architecture schema and a more detailed schema intended to implement it. The two schemas usually contain very different architectural concepts (from different architectural styles). Once a refinement pattern is proven correct, instances of it can be used without proof in developing specific architectures. Individual refinements are compositional, permitting incremental development and local reasoning. A special correctness criterion is defined for the domain of software architecture, as well as an accompanying proof technique. A useful syntactic form of correct composition is defined. The main points are illustrated by means of familiar archit...
145|Exploiting Style in Architectural Design Environments|As the design of software architectures emerges as a discipline within software engineering, it will become increasingly important to support architectural description and analysis with tools and environments. In this paper we describe a system for developing architectural design environments that exploit architectural styles to guide software architects in producing specific systems. The primary contributions of this research are: (a) a generic object model for representing architectural designs; (b) the characterization of architectural styles as specializations of this object model; and (c) a toolkit for creating an open architectural design environment from a description of a specific architectural style. We use our experience in implementing these concepts to illustrate how style-oriented architectural design raises new challenges for software support environments. 
146|A Field Guide to Boxology: Preliminary Classification of Architectural Styles for Software Systems|Software architects use a number of commonly-recognized “styles” to guide their design of system structures. Each of these is appropriate for some classes of problems, but none is suitable for all problems. How, then, does a software designer choose an architecture suitable for the problem at hand? Two kinds of information are required: (1) careful discrimination among the candidate architectures and (2) design guidance on how to make appropriate choices. Here we support careful discrimination with a preliminary classification of styles. We use a two-dimensional classification strategy with control and data issues as the dominant organizing axes. We position the major styles within this space and use finer-grained discriminations to elaborate variations on the styles. This provides a framework for organizing design guidance, which we partially flesh out with rules of thumb.
147|Using Style to Understand Descriptions of Software Architecture|The software architecture of most systems is described informally and diagrammatically. In order for these descriptions to be meaningful at all, figures are understood by interpreting the boxes and lines in specific, conventionalized ways [5]. The imprecision of these interpretations has a number of limitations. In this paper we consider these conventionalized interpretations as architectural styles and provide a formal framework for their uniform definition. In addition to providing a template for precisely defining new architectural styles, this framework allows for the proof that the notational constraints on a style are sufficient to guarantee the meanings of all described systems and provides a unified semantic base through which different stylistic interpretations can be compared.
148|Paradigms for process interaction in distributed programs|Distributed computations are concurrent programs in which processes communicate by message passing. Such programs typically execute on network architectures such as networks of workstations ordistributed memory parallel machines (i. e, multicomputers such ashypercubes). Several paradigms—examples or models—for process interaction
149|Formalizing design spaces: Implicit invocation mechanisms |An important goal of software engineering is to exploit commonalities in system design in order to reduce the complexity of building new systems, support largescale reuse, and provide automated assistance for system development. A significant roadblock to accomplishing this goal is that common properties of systems are poorly understood. In this paper we argue that formal specification can help solve this problem. A formal definition of a design framework can identify the common properties of a family of systems and make clear the dimensions of specialization. New designs can then be built out of old ones in a principled way, at reduced cost to designers and implementors. To illustrate these points, we present a formalization of a system integration technique called implicit invocation. We show how many previously unrelated systems can be viewed as instances of the same underlying framework. Then we briefly indicate how the formalization allows us to reason about certain properties of those systems as well as the relationships between different systems. 1
150|Software Requirements Negotiation and Renegotiation Aids: A Theory-W Based Spiral Approach|A major problem in requirements engineering is obtaining requirements that address the concerns of multiple stakeholders. An approach to such a problem is the Theory-W based Spiral Model. One key element of this model is stakeholder collaboration and negotiation to obtain win-win requirements. This paper focuses on the problem of developing a support system for such a model. In particular it identifies needs and capabilities required to address the problem of negotiation and renegotiation that arises when the model is applied to incremental requirements engineering. The paper formulates elements of the support system, called WinWin, for providing such capabilities. These elements were determined by experimenting with versions of WinWin and understanding their merits and deficiencies. The key elements of WinWin are described and their use in incremental requirements engineering are demonstrated, using an example renegotiation scenario from the domain of software engineering environments...
151|Reconciling the Needs of Architectural Description with Object-Modelling Notations|Abstract. Complex software systems require expressive notations for representing their software architectures. Two competing paths have emerged. One is to use a specialized notation for architecture – or architecture description language (ADL). The other is to adapt a general-purpose modeling notation, such as UML. The latter has a number of benefits, including familiarity to developers, close mapping to implementations, and commercial tool support. However, it remains an open question as to how best to use object-oriented notations for architectural description, and, indeed, whether they are sufficiently expressive, as currently defined. In this paper we take a systematic look at these questions, examining the space of possible mappings from ADLs into object notations. Specifically, we describe (a) the principle strategies for representing architectural structure in UML; (b) the benefits and limitations of each strategy; and (c) aspects of architectural description that are intrinsically difficult to model in UML using the strategies. 1
152|Using object-oriented typing to support architectural design in the C2 style|Abstract-- Software architectures enable large-scale software development. Component reuse and substitutability, two key aspects of large-scale development, must be planned for during software design. Object-oriented (OO) type theory supports reuse by structuring inter-component relationships and verifying those relationships through type checking in an architecture definition language (ADL). In this paper, we identify the issues and discuss the ramifications of applying OO type theory to the C2 architectural style. This work stems from a series of experiments that were conducted to investigate component reuse and substitutability in C2. We also discuss the limits of applicability of OO typing to C2 and how we addressed them in the C2 ADL. 1
153|Describing Software Architecture with UML|: This paper describes our experience using UML, the Unified Modeling Language, to describe the software architecture of a system. We found that it works well for communicating the static structure of the architecture: the elements of the architecture, their relations, and the variability of a structure. These static properties are much more readily described with it than the dynamic properties. We could easily describe a particular sequence of activities, but not a general sequence. In addition, the ability to show peer-to-peer communication is missing from UML.  Keywords: software architecture, UML, architecture descriptions, multiple views  1. INTRODUCTION  UML, the Unified Modeling Language, is a standard that has wide acceptance and will likely become even more widely used. Although its original purpose was for detailed design, its ability to describe elements and the relations between them makes it potentially applicable much more broadly. This paper describes our experience usin...
154|Software Interconnection Models|We present a formulation of interconnection models and present the unit and syntactic models --- the primary models used for managing the evolution of large software systems. We discuss various tools that use these models and evaluate how well these models support the management of system evolution. We then introduce the semantic interconnection model. The semantic interconnection model incorporates the advantages of the unit and syntactic interconnection models and provides extremely useful extensions to them. By refining the grain of interconnections to the level of semantics (that is, to the predicates that define aspects of behavior) we provide tools that are better suited to manage the details of evolution in software systems and that provide a better understanding of the implications of changes. We do this by using the semantic interconnection model to formalize the semantics of program construction, the semantics of changes, and the semantics of version equivalence and compatibi...
155|Using tool abstraction to compose systems|paradigms support the evolution of large-scale software systems. Data abstraction eases design changes in the representation of data structures, while tool abstraction does the same with system functions. M anaging complexity and supporting evolution are two fundamental “i, problems with large-scale software systems. ’ Although modularization,. has long been accepted as the basic approach to managing complexity, as David Parnas observed nearly 20 years ago, not all modularizations are equally good at handling evolution.’ Data abstraction is a popular, important style of modularization. In this style, an abstract data type is defined by an explicit interface that specifies operations on
156|Assessing the Suitability of a Standard Design Method for Modeling Software Architectures| Software architecture descriptions are high-level models of software systems.  Most existing special-purpose architectural notations have a great deal of  expressive power but are not well integrated with common development  methods. Conversely, mainstream development methods are accessible to  developers, but lack the semantics needed for extensive analysis. In our  previous work, we described an approach to combining the advantages of  these two ways of modeling architectures. While this approach suggested a  practical strategy for bringing architectural modeling into wider use, it  introduced specialized extensions to a standard modeling notation, which  could also hamper wide adoption of the approach. This paper attempts to  assess the suitability of a standard design method &#034;as is&#034; for modeling  software architectures.  
157|Experience with a course on architectures for software systems|Abstract. As software systems grow in size and complexity their design problem extends beyond algorithms and data structures to issues of system design. This area receives little or no treatment in existing computer science curricula. Although courses about speci c systems are usually available, there is no systematic treatment of the organizations used to assemble components into systems. These issues { the software architecture level of software design { are the subject of a new course that we taught for the rst time in Spring 1992. This paper describes the motivation for the course, the content and structure of the current version, and our plans for improving the next version. 1
158|Aladdin: A Tool for Architecture-Level Dependence Analysis of Software Systems|The emergence of formal architecture description languages provides an opportunity to perform analyses at high levels of abstraction, as well as early in the development process. Previous research has primarily focused on developing techniques such as algebraic and transition-system analysis to detect component mismatches or global behavioral incorrectness. In this paper, we present Aladdin, a tool that implements chaining, a static dependence analysis technique for use with architectural descriptions. Dependence analysis has been used widely at the implementation level to aid program optimization, anomaly checking, program understanding, testing, and debugging. We investigate the definition and application of dependence analysis at the architectural level. We illustrate the utility of chaining, through the use of Aladdin, by showing how the technique can be used to answer various questions one might pose of a Rapide architecture specification. 
159|The impact of Mesa on system design|The Mesa programming language supports program modularity in ways that permit subsystems to be developed separately but to be bound together with complete type safety. Separate and explicit interface definitions provide an effective means of communication, both between programs and between programmers. A configuration language describes the organization of a system and controls the scopes of interfaces. These facilities have had a profound impact on the way we design systems and Organize development projects. This paper reports our recent experience with Mesa, particularly its use in the development of an operating system. It illustrates techniques for designing interfaces, for using the interface language as a specification language, and for organizing a ~ystem to achieve the practical benefits of program modularity without sacrificing strict type-checking.
160|The Structure-Mapping Engine: Algorithm and Examples|This paper describes the Structure-Mapping Engine (SME), a program for studying analogical processing. SME has been built to explore Gentner&#039;s Structure-mapping theory of analogy, and provides a &#034;tool kit&#034; for constructing matching algorithms consistent with this theory. Its flexibility enhances cognitive simulation studies by simplifying experimentation. Furthermore, SME is very efficient, making it a useful component in machine learning systems as well. We review the Structure-mapping theory and describe the design of the engine. We analyze the complexity of the algorithm, and demonstrate that most of the steps are polynomial, typically bounded by O (N 2 ). Next we demonstrate some examples of its operation taken from our cognitive simulation studies and work in machine learning. Finally, we compare SME to other analogy programs and discuss several areas for future work. This paper appeared in Artificial Intelligence, 41, 1989, pp 1-63. For more information, please contact forbu...
161|Why a diagram is (sometimes) worth ten thousand words  (1987) |We distinguish diagrammatic from sentential paper-and-pencil representationsof information by developing alternative models of information-processing systems that are informationally equivalent and that can be characterized as sentential or diagrammatic. Sentential representations are sequential, like the propositions in a text. Dlogrammotlc representations ore indexed by location in a plane. Dio-grommatic representations also typically display information that is only implicit in sententiol representations and that therefore has to be computed, sometimes at great cost, to make it explicit for use. We then contrast the computational efficiency of these representotions for solving several illustrative problems in mothe-matics and physics. When two representotions are informationally equivolent, their computational efficiency depends on the information-processing operators that act on them. Two sets of operators may differ in their copobilities for recognizing patterns, in the inferences they con carry out directly, and in their control strategies (in portitular. the control of search). Diogrommotic ond sentential representations sup
162|Toward a model of text comprehension and production|The semantic structure of texts can be described both at the local microlevel and at a more global macrolevel. A model for text comprehension based on this notion accounts for the formation of a coherent semantic text base in terms of a cyclical process constrained by limitations of working memory. Furthermore, the model includes macro-operators, whose purpose is to reduce the information in a text base to its gist, that is, the theoretical macrostructure. These opera-tions are under the control of a schema, which is a theoretical formulation of the comprehender&#039;s goals. The macroprocesses are predictable only when the control schema can be made explicit. On the production side, the model is con-cerned with the generation of recall and summarization protocols. This process is partly reproductive and partly constructive, involving the inverse operation of the macro-operators. The model is applied to a paragraph from a psycho-logical research report, and methods for the empirical testing of the model are developed. The main goal of this article is to describe the system of mental operations that underlie the processes occurring in text comprehension and in the production of recall and summariza-tion protocols. A processing model will be outlined that specifies three sets of operations. First, the meaning elements of a text become
163|Directed Diffusion for Wireless Sensor Networking|Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is datacentric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network (e.g., data aggregation). We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network analytically and experimentally. Our evaluation indicates that directed diffusion can achieve significant energy savings and can outperform idealized traditional schemes (e.g., omniscient multicast) under the investigated scenarios.
164|Dynamic source routing in ad hoc wireless networks|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any established infrastructure or centralized administration. In such an environment, it may be necessary for one mobile host to enlist the aid of other hosts in forwarding a packet to its destination, due to the limited range of each mobile host’s wireless transmissions. This paper presents a protocol for routing in ad hoc networks that uses dynamic source routing. The protocol adapts quickly to routing changes when host movement is frequent, yet requires little or no overhead during periods in which hosts move less frequently. Based on results from a packet-level simulation of mobile hosts operating in an ad hoc network, the protocol performs well over a variety of environmental conditions such as host density and movement rates. For all but the highest rates of host movement simulated, the overhead of the protocol is quite low, falling to just 1 % of total data packets transmitted for moderate movement rates in a network of 24 mobile hosts. In all cases, the difference in length between the routes used and the optimal route lengths is negligible, and in most cases, route lengths are on average within a factor of 1.01 of optimal. 1.
165|Directed Diffusion: A scalable and robust communication paradigm for sensor networks|Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.
166|Energy-efficient communication protocol for wireless microsensor networks|Wireless distributed microsensor systems will enable the reliable monitoring of a variety of environments for both civil and military applications. In this paper, we look at communication protocols, which can have significant impact on the overall energy dissipation of these networks. Based on our findings that the conventional protocols of direct transmission, minimum-transmission-energy, multihop routing, and static clustering may not be optimal for sensor networks, we propose LEACH (Low-Energy Adaptive Clustering Hierarchy), a clustering-based protocol that utilizes randomized rotation of local cluster base stations (cluster-heads) to evenly distribute the energy load among the sensors in the network. LEACH uses localized coordination to enable scalability and robustness for dynamic networks, and incorporates data fusion into the routing protocol to reduce the amount of information that must be transmitted to the base station. Simulations show that LEACH can achieve as much as a factor of 8 reduction in energy dissipation compared with conventional routing protocols. In addition, LEACH is able to distribute energy dissipation evenly throughout the sensors, doubling the useful system lifetime for the networks we simulated. 
167|A Performance Comparison of Multi-Hop Wireless Ad Hoc Network Routing Protocols|An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Due to the limited transmission range of wireless network interfaces, multiple network &amp;quot;hops &amp;quot; may be needed for one node to exchange data with another across the network. In recent years, a variety of new routing protocols targeted specifically at this environment have been developed, but little performance information on each protocol and no realistic performance comparison between them is available. This paper presents the results of a detailed packet-level simulation comparing four multi-hop wireless ad hoc network routing protocols that cover a range of design choices: DSDV, TORA, DSR, and AODV. We have extended the ns-2 network simulator to accurately model the MAC and physical-layer behavior of the IEEE 802.11 wireless LAN standard, including a realistic wireless transmission channel model, and present the results of simulations of networks of 50 mobile nodes. 1
168|A Highly Adaptive Distributed Routing Algorithm for Mobile Wireless Networks|We present a new distributed routing protocol for mobile, multihop, wireless networks. The protocol is one of a family of protocols which we term &#034;link reversal&#034; algorithms. The protocol&#039;s reaction is structured as a temporally-ordered sequence of diffusing computations; each computation consisting of a sequence of directed l i nk reversals. The protocol is highly adaptive, efficient and scalable; being best-suited for use in large, dense, mobile networks. In these networks, the protocol&#039;s reaction to link failures typically involves only a localized &#034;single pass&#034; of the distributed algorithm. This capability is unique among protocols which are stable in the face of network partitions, and results in the protocol&#039;s high degree of adaptivity. This desirable behavior is achieved through the novel use of a &#034;physical or logical clock&#034; to establish the &#034;temporal order&#034; of topological change events which is used to structure (or order) the algorithm&#039;s reaction to topological changes. We refer to the protocol as the Temporally-Ordered Routing Algorithm (TORA).
169|Location-Aided Routing (LAR) in mobile ad hoc networks  (1998) |A mobile ad hoc network consists of wireless hosts that may move often. Movement of hosts results in a change in routes, requiring some mechanism for determining new routes. Several routing protocols have already been proposed for ad hoc networks. This paper suggests an approach to utilize location information (for instance, obtained using the global positioning system) to improve performance of routing protocols for ad hoc networks. By using location information, the proposed Location-Aided Routing (LAR) protocols limit the search for a new route to a smaller “request zone” of the ad hoc network. This results in a significant reduction in the number of routing messages. We present two algorithms to determine the request zone, and also suggest potential optimizations to our algorithms.
170|Internet time synchronization: The Network Time Protocol|Abstruct- This paper describes the network time protocol (NTP), which is designed to distribute time information in a large, diverse internet system operating at speeds from mundane to lightwave. It uses a symmetric architecture in which a distributed subnet of time servers operating in a self-organizing, hierarchical configuration synchronizes local clocks within the subnet and to national time standards via wire, radio, or calibrated atomic clock. The servers can also redistribute time information within a network via local routing algorithms and time daemons. This paper also discusses the architecture, protocol and algorithms, which were developed over several years of implementation refinement and resulted in the designation of NTP as an Internet Standard protocol. The NTP synchronization system, which has been in regular operation in the Internet for the last several years, is described along with performance data which shows that timekeeping accuracy throughout most portions of the Internet can be ordinarily maintained to within a few milliseconds, even in cases of failure or disruption of clocks, time servers or networks. I.
171| An Architecture for Wide-Area Multicast Routing |Existing multicast routing mechanisms were intended for use within regions where a group is widely represented or bandwidth is universally plentiful. When group members, and senders to those group members, are distributed sparsely across a wide area, these schemes are not efficient; data packets or membership report information are occasionally sent over many links that do not lead to receivers or senders, respectively. Wehave developed a multicast routing architecture that efficiently establishes distribution trees across wide area internets, where many groups will be sparsely represented. Efficiency is measured in terms of the state, control message processing, and data packet processing, required across the entire network in order to deliver data packets to the members of the group. Our Protocol Independent Multicast (PIM) architecture: (a) maintains the traditional IP multicast service model of receiver-initiated membership; (b) can be configured to adapt to different multicast group and network characteristics; (c) is not dependent on a specific unicast routing protocol; and (d) uses soft-state mechanisms to adapt to underlying network conditions and group dynamics. The robustness, flexibility, and scaling properties of this architecture make it well suited to large heterogeneous inter-networks.
172|The design and implementation of an intentional naming system|This paper presents the design and implementation of the Intentional Naming System (INS), a resource discovery and service location system for dynamic and mobile networks of devices and computers. Such environments require a naming system that is (i) expressive, to describe and make requests based on specific properties of services, (ii) responsive, to track changes due to mobility and performance, (iii) robust, to handle failures, and (iv) easily configurable. INS uses a simple language based on attributes and values for its names. Applications use the language to describe what they are looking for (i.e., their intent), not where to find things (i.e., not hostnames). INS implements a late binding mechanism that integrates name resolution and message routing, enabling clients to continue communicating with end-nodes even if the name-to-address mappings change while a session is in progress. INS resolvers self-configure to form an application-level overlay network, which they use to discover new services, perform late binding, and maintain weak consistency of names using soft-state name exchanges and updates. We analyze the performance of the INS algorithms and protocols, present measurements of a Java-based implementation, and describe three applications we have implemented that demonstrate the feasibility and utility of INS.
173|Measuring and Reducing Energy Consumption of Network Interfaces in Hand-Held Devices|Next generation hand-held devices must provide seamless connectivity while obeying stringent power and size constrains. In this paper we examine this issue from the point of view of the Network Interface (NI). We measure the power usage of two PDAs, the Apple Newton Messagepad and Sony Magic Link, and four NIs, the Metricom Ricochet Wireless Modem, the AT&amp;T Wavelan operating at 915 MHz and 2.4 GHz, and the IBM Infrared Wireless LAN Adapter. These measurements clearly indicate that the power drained by the network interface constitutes a large fraction of the total power used by the PDA. We then examine two classes of optimizations that can be used to reduce network interface energy consumption on these devices: transport-level strategies and application-level strategies. Simulation experiments of transportlevel strategies show that the dominant cost comes not from the number of packets sent or received by a particular transport protocol but the amount of time that the NI is in an activ...
174|Impact of network density on Data Aggregation in wireless sensor networks|In-network data aggregation is essential for wireless sensor networks where resources (e.g., bandwidth, energy) are limited. In a previously proposed data dissemination scheme, data is opportunistically aggregated at the intermediate nodes on a low-latency tree which may not necessarily be energy efficient. A more energy-efficient tree is a greedy tree which can be incrementally constructed by connecting each source to the closest point of the existing tree. In this paper, we propose a greedy approach for constructing a greedy aggregation tree to improve path sharing. We evaluated the performance of this greedy approach by comparing it to the prior opportunistic approach. Our preliminary result suggests that although the greedy aggregation and the opportunistic aggregation are roughly equivalent at low-density networks, the greedy aggregation can achieve signficant energy savings at higher densities. In one experiment we found that the greedy aggregation can achieve up to 45 % energy savings over the opportunistic aggregation without an adverse impact on latency or robustness. 
175|Geographical and energy aware routing: A recursive data dissemination protocol for wireless sensor networks|Future sensor networks will be composed of a large number of densely deployed sensors/actuators. A key feature of such networks is that their nodes are untethered and unattended. Consequently, energy efficiency is an important design consideration for these networks. Motivated by the fact that sensor network queries may often be geographical, we design and evaluate an energy efficient routing algorithm that propagates a query to the appropriate geographical region, without flooding. The proposed Geographic and Energy Aware Routing (GEAR) algorithm uses energy aware neighbor selection to route a packet towards the target region and Recursive Geographic Forwarding or Restricted Flooding algorithm to disseminate the packet inside the destination region. We evaluate the GEAR protocol using simulation. We find that, especially for non-uniform traffic distribution, GEAR exhibits noticeably longer network lifetime than non-energyaware geographic routing algorithms. 1
176|Time Synchronization in Wireless Sensor Networks|OF THE DISSERTATION        University of California, Los Angeles, 2003  Professor Deborah L. Estrin, Chair    active research in large-scale networks of small, wireless, low-power sensors and actuators. Time synchronization is a critical piece of infrastructure in any dis- tributed system, but wireless sensor networks make particularly extensive use   physical world. However, while the clock accuracy and precision requirements are often stricter in sensor networks than in traditional distributed systems, energy and channel constraints limit the resources available to meet these goals.
177|Building Efficient Wireless Sensor Networks with Low-Level Naming|In most distributed systems, naming of nodes for low-level communication leverages topological location (such as node addresses)  and is independent of any application. In this paper, we investigate an emerging class of distributed systems where low-level communication does not rely on network topological location. Rather, low-level communication is based on attributes that are external to the network topology and relevant to the application. When combined with dense deployment of nodes, this kind of named data enables in-network processing for data aggregation, collaborative signal processing, and similar problems. These approaches are essential for emerging applications such as sensor networks where resources such as bandwidth and energy are limited. This paper is the first description of the software architecture that supports  named data and in-network processing in an operational, multi-application sensor-network. We show that approaches such as in-network aggregation and nested queries can significantly affect network traffic. In one experiment aggregation reduces traffic by up to 42% and nested queries reduce loss rates by 30%. Although aggregation has been previously studied in simulation, this paper demonstrates nested queries as another form of in-network processing, and it presents the first evaluation of these approaches over an operational testbed.  
178|Compressing TCP/IP headers for low-speed serial links|This RFC is a proposed elective protocol for the Internet community and requests discussion and suggestions for improvement. It describes a method for compressing the headers of TCP/IP datagrams to improve performance over low speed serial links. The motivation, implementation and performance of the method are described. C code for a sample implementation is given for reference. Distribution of this memo is unlimited. NOTE: Both ASCII and Postscript versions of this document are available. The ASCII version, obviously, lacks all the figures and all the information encoded in typographic variation (italics, boldface, etc.). Since this information was, in the author’s opinion, an essential part of the document, the ASCII version is at best incomplete and at worst misleading. Anyone who plans to work with this protocol is strongly encouraged obtain the Postscript version of this RFC.
179|AntNet: A Mobile Agents Approach to Adaptive Routing|This paper introduces AntNet, a new routing algorithm for communications networks. AntNet is an adaptive, distributed, mobile-agents-based algorithm whichwas inspired by recentwork on the ant colony metaphor. We apply AntNet to a datagram network and compare it with both static and adaptive state-of-the-art routing algorithms. We ran experiments for various paradigmatic temporal and spatial traffic distributions. AntNet showed both very good performance and robustness under all the experimental conditions with respect to its competitors.
180|Adaptive web caching: towards a new global caching architecture|An adaptive, highly scalable, and robust web caching system is needed to effectively handle the exponential growth and extreme dynamic environment of the World Wide Web. Our work presented last year sketched out the basic design of such a system. This sequel paper reports our progress over the past year. To assist caches making web query forwarding decisions, we sketch out the basic design of a URL routing framework. To assist fast searching within each cache group, we let neighbor caches share content information. Equipped with the URL routing table and neighbor cache contents, a cache in the revised design can now search the local group, and forward all missing queries quickly and efficiently, thus eliminating both the waiting delay and the overhead associated with multicast queries. The paper also presents a proposal for incremental deployment that provides a smooth transition from the currently deployed cache infrastructure to the new
181|Improving simulation for network research|In recent years, the Internet has grown signicantly in size and scope, and as a result new protocols and algorithms are being developed to meet changing operational
182|Piconet: Embedded Mobile Networking|Piconet is a general-purpose, low-power ad hoc radio network. It provides a base level of connectivity to even the simplest of sensing and  computing objects. It is our intention that a full range of portable and embedded devices may make use of this connectivity. This article outlines  the Piconet system, under development at the Olivetti and Oracle Research Laboratory (ORL). The authors discuss the motivation for providing  this low-level &#034;embedded networking,&#034; and describe their experiences of building such a system. The article concludes with a commentary on  some of the implications that power saving, and other considerations central to Piconet, have on the design of the system.
183|Scalable Timers for Soft State Protocols|Soft state protocols use periodic refresh messages to keep network state alive while adapting to changing network conditions; this has raised concerns regarding the scalability of protocols that use the soft-state approach. In existing soft state protocols, the values of the timers that control the sending of these messages, and the timers for aging out state, are chosen by matching empirical observations with desired recovery and response times. These fixed timer-values fail because they use time as a metric for bandwidth; they adapt neither to (1) the wide range of link speeds that exist in most wide-area internets, nor to (2) fluctuations in the amount of network state over time. We propose and evaluate a new approach in which timervalues adapt dynamically to the volume of control traffic and available bandwidth on the link. The essential mechanisms required to realize this scalable timers approach are: (1) dynamic adjustment of the senders ’ refresh rate so that the bandwidth allocated for control traffic is not exceeded, and (2) estimation of the senders ’ refresh rate at the receiver in order to determine when the state can be timed-out and deleted. The refresh messages are sent in a round robin manner not exceeding the bandwidth allocated to control traffic, and taking into account message priorities. We evaluate two receiver estimation methods for dynamically adjusting network state timeout values: (1) counting of the rounds and (2) exponential weighted moving average. 1
184|A New Proposal for RSVP Refreshes|As a soft-state protocol, RSVP specifies that each RSVP node sends periodic control messages to maintain the state for active RSVP sessions. The protocol overhead due to such periodic messages grows linearly with the number of RSVP sessions. One may reduce the overhead by using a longer refresh period, which unfortunately leads to longer delays in re-synchronizing RSVP state. In this paper we introduce a novel &#034;state-compression&#034; approach to reducing the overhead of periodic refreshes. Instead of per session refresh messages, an RSVP node sends periodically to each of its neighbor node a Digest message that contains a compressed version of the entire RSVP state shared with that particular neighbor. In order to speed up state synchronization in face of message losses we also enhance RSVP with an acknowledgment mechanism. Our mechanisms achieve a constant message transmission overhead and low delay while retaining the soft-state nature of the RSVP protocol. 1. Introduction  RSVP [3] is a...
185|Applying design by contract|Reliability is even more important in object-oriented programming than elsewhere. This article shows how to reduce bugs by building software components on the basis of carefully designed contracts. 40 s object-oriented techniques steadily gain ground in the world of software development. users and prospective users of these techniques are clam-oring more and more loudly for a “methodology ” of object-oriented software construction- or at least for some methodological guidelines. This article presents such guidelines, whose main goal is to help improve the reliability
186|An axiomatic basis for computer programming|In this paper an attempt is made to explore the logical founda-tions of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This in-volves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantages, both theoretical and prac-tical, may follow from a pursuance of these topics.
187|The Application of Petri Nets to Workflow Management|Workflow management promises a new solution to an age-old problem: controlling, monitoring, optimizing and supporting business processes. What is new about workflow management is the explicit representation of the business process logic which allows for computerized support. This paper discusses the use of Petri nets in the context of workflow management. Petri nets are an established tool for modeling and analyzing processes. On the one hand, Petri nets can be used as a design language for the specification of complex workflows. On the other hand, Petri net theory provides for powerful analysis techniques which can be used to verify the correctness of workflow procedures. This paper introduces workflow management as an application domain for Petri nets, presents state-of-the-art results with respect to the verification of workflows, and highlights some Petri-net-based workflow tools.  
188|The Entity-Relationship Model: Toward a Unified View of Data|A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, infor-mation retrieval, and data manipulation are discussed. The entity-relationship model can be used as a basis for unification of different views of data: t,he network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented. Key Words and Phrases: database design, logical view of data, semantics of data, data models, entity-relationship model, relational model, Data Base Task Group, network model, entity set
189|THREE GOOD REASONS FOR USING A PETRI-NET-BASED WORKFLOW MANAGEMENT SYSTEM |Currently, the Dutch Customs Department is building a nationwide information system to handle all kinds of declarations related to the import and export of goods. For this purpose the Petri-net-based Work ow Management System (WFMS) named COSA has been selected. During the selection process, it turned out that there are several reasons for insisting on a Petri-net-based WFMS. The three main reasons for selecting a Petri-net-based WFMS are discussed in this paper. In our opinion these reasons are also relevant for many other projects involved in the selection or implementation of a WFMS. 
190|Complexity Results for 1-safe Nets|We study the complexity of several standard problems for 1-safe Petri nets and some of its subclasses. We prove that reachability, liveness, and deadlock are all PSPACE-complete for 1-safe nets. We also prove that deadlock is NP-complete for free-choice nets and for 1-safe free-choice nets. Finally, we prove that for arbitrary Petri nets, deadlock is equivalent to reachability and liveness.  This paper is to be presented at FST&amp;TCS 13, Foundations of Software Technology &amp; Theoretical Computer Science, to be held 1517 December 1993, in Bombay, India. A version of the paper with most proofs omitted is to appear in the proceedings. 1 Introduction Petri nets are one of the oldest and most studied formalisms for the investigation of concurrency [33]. Shortly after the birth of complexity theory, Jones, Landweber, and Lien studied in their classical paper [24] the complexity of several fundamental problems for Place/Transition nets (called in [24] just Petri nets). Some years later, Howell,...
191|Structural Characterizations of Sound Workflow Nets|this paper we present a method based on Petri nets. This analysis method exploits the structure of the Petri net to find potential errors in the design of the workflow. Moreover, the analysis method allows for the compositional verification of workflows.
192|The Simple Control Property of Business Process Models|this paper we are only concerned with the specification and analysis of process behavior so we do not mention other process&#039; perspectives. In particular, we will specify behavior in a so-called coordination model independently from the organizational context. Kellner [2] defines behavior as &#034;when the process elements are performed (e.g., sequencing), as well as aspects of how they are performed through feedback loops, iteration, complex decision-making conditions, entry and exit criteria, and so forth.&#034; In most workflow systems a user represents control flow graphically usually with a notation that resembles both a control flow diagram and a PERT net; the semantics is usually defined in terms of Petri nets (e.g., ICN [6], Macronets [9]). Complex processes can create behavioral anomalies. Examples of anomalous behavior are processes that deadlock (cannot proceed and have not yielded a result) or activities whose results are not used by other activities. Sequential processes cannot have control anomalies (except for infinite loops). On the other hand, parallel processes without choice---which are basically PERT charts--- cannot have control anomalies [16]. Thus, it is not surprising that the usual way to avoid these anomalies is by defining simple-minded models that inhibit the natural parallelism of activities and/or that abstract away the handling of exceptional cases. This article reports on a study of behavioral properties of process models, part of a larger effort to develop a modelling tool and method for the development of safe process models. Our tool
194|Theory of the Firm: Managerial Behavior, Agency Costs and Ownership Structure|This paper integrates elements from the theory of agency, the theory of property rights and the theory of finance to develop a theory of the ownership structure of the firm. We define the concept of agency costs, show its relationship to the ‘separation and control’ issue, investigate the nature of the agency costs generated by the existence of debt and outside equity, demonstrate who bears costs and why, and investigate the Pareto optimality of their existence. We also provide a new definition of the firm, and show how our analysis of the factors influencing the creation and issuance of debt and equity claims is a special case of the supply side of the completeness of markets problem. 
196|A simple approach to valuing risky fixed and floating rate debt|Your use of the JSTOR archive indicates your acceptance of JSTOR&#039;s Terms and Conditions of Use, available at
198|Design and Valuation of Debt Contracts|This article studies the design and valuation of debt contracts in a general dynamic setting under uncertainty. We incorporate some insights of the recent corporate finance literature into a valuation framework. The basic framework is an extensive form game determined by the terms of a debt contract and applicable bankruptcy laws. Debtholders and equityholders behave noncooperatively. The firm’s reorganization boundary is determined endogenously. Strategic debt service results in significantly higher default premia at even small liquidation costs. Deviations from absolute priority and forced liquidations occur along the equilibrium path. The design tends to stress higher coupons and sinking funds when firms have a higher cash payout ratio. This article studies the design and valuation of debt contracts in a general dynamic setting under uncertainty. In doing so we draw together two strands of the finance literature that have developed significantly in recent years, but have done so in large part indepen-We have benefited from the comments of seminar participants at the
199|Mediators in the architecture of future information systems|The installation of high-speed networks using optical fiber and high bandwidth messsage forwarding gateways is changing the physical capabilities of information systems. These capabilities must be complemented with corresponding software systems advances to obtain a real benefit. Without smart software we will gain access to more data, but not improve access to the type and quality of information needed for decision making. To develop the concepts needed for future information systems we model information processing as an interaction of data and knowledge. This model provides criteria for a high-level functional partitioning. These partitions are mapped into information process-ing modules. The modules are assigned to nodes of the distributed information systems. A central role is assigned to modules that mediate between the users &#039; workstations and data re-sources. Mediators contain the administrative and technical knowledge to create information needed for decision-making. Software which mediates is common today, but the structure, the interfaces, and implementations vary greatly, so that automation of integration is awkward. By formalizing and implementing mediation we establish a partitioned information sys-
200|The emotional dog and its rational tail: a social intuitionist approach to moral judgment|This is the manuscript that was published, with only minor copy-editing alterations, as: Haidt, J. (2001). The emotional dog and its rational tail: A social intuitionist approach to moral judgment. Psychological Review. 108, 814-834 Copyright 2001, American Psychological Association To obtain a reprint of the final type-set article, please go through your library’s journal services, or contact the author directly Research on moral judgment has been dominated by rationalist models, in which moral judgment is thought to be caused by moral reasoning. Four reasons are given for considering the hypothesis that moral reasoning does not cause moral judgment; rather, moral reasoning is usually a post-hoc construction, generated after a judgment has been reached. The social intuitionist model is presented as an alternative to rationalist models. The model is a social model in that it de-emphasizes the private reasoning done by individuals, emphasizing instead the importance of social and cultural influences. The model is an intuitionist model in that it states that moral judgment is generally the result of quick, automatic evaluations (intuitions). The model is more consistent than rationalist models with recent findings in social, cultural, evolutionary, and biological psychology, as well as anthropology and primatology. Author notes
202|Reasoning the fast and frugal way: Models of bounded rationality|Humans and animals make inferences about the world under limited time and knowledge. In contrast, many models of rational inference treat the mind as a Laplacean Demon, equipped with unlimited time, knowledge, and computational might. Following H. Simon’s notion of satisficing, the authors have proposed a family of algorithms based on a simple psychological mechanism: one reason decision making. These fast and frugal algorithms violate fundamental tenets of classical rationality: They neither look up nor integrate all information. By computer simulation, the authors held a competition between the satisficing “Take The Best ” algorithm and various “rational ” inference procedures (e.g., multiple regression). The Take The Best algorithm matched or outperformed all competitors in inferential speed and accuracy. This result is an existence proof that cognitive mechanisms capable of successful performance in the real world do not need to satisfy the classical norms of rational inference. Organisms make inductive inferences. Darwin (1872/1965) observed that people use facial cues, such as eyes that waver and lids that hang low, to infer a person’s guilt. Male toads, roaming through swamps at night, use the pitch of a rival’s croak to infer its size when deciding whether to fight (Krebs &amp; Davies, 1987). Stock brokers must make fast decisions about which of several stocks to trade or invest when only limited information is available. The list goes on. Inductive
203|The unbearable automaticity of being|What was noted by E. J. hanger (1978) remains true today: that much of contemporary psychological research is based on the assumption that people are consciously and systematically processing incoming information in order to construe and interpret their world and to plan and engage in courses of action. As did E. J. hanger, the authors question this assumption. First, they review evidence that the ability to exercise such conscious, intentional control is actually quite limited, so that most of moment-to-moment psychological life must occur through nonconscious means if it is to occur at all. The authors then describe the different possible mechanisms that produce automatic, environmental control over these various phenomena and review evidence establishing both the existence of these mechanisms as well as their consequences for judgments, emotions, and
204|Integration of the cognitive and the psychodynamic unconscious|Cognitive-experiential self-theory integrates the cognitive and the psychodynamic unconscious by assuming the ex-istence of two parallel, interacting modes of information processing: a rational system and an emotionally driven experiential system. Support for the theory is provided by the convergence of a wide variety of theoretical positions on two similar processing modes; by real-life phenom-ena—such as conflicts between the heart and the head; the appeal of concrete, imagistic, and narrative represen-tations; superstitious thinking; and the ubiquity of religion throughout recorded history—and by laboratory research, including the prediction of new phenomena in heuristic reasoning. N early 100 years ago, Freud introduced a dualtheory of information processing that placeddeviant behavior squarely in the realm of the natural sciences and, more particularly, in psychology.
205|Thin slices of expressive behavior as predictors of interpersonal consequences: A meta-analysis|A meta-analysis was conducted on the accuracy of predictions of various objective outcomes in the areas of social and clinical psychology from short observations of expressive behavior (under 5 min). The overall effect size (/) for the accuracy of predictions for 38 different results was.39. Studies using longer periods of behavioral observation did not yield greater predictive accuracy; predictions based on observations under Vi min in length did not differ significantly from predic-tions based on 4- and 5-min observations. The type of behavioral channel (such as the face, speech, the body, tone of voice) on which the ratings were based was not related to the accuracy of predic-tions. Accuracy did not vary significantly between behaviors manipulated in a laboratory and more naturally occurring behavior. Last, effect sizes did not differ significantly for predictions in the areas of clinical psychology, social psychology, and the accuracy of detecting deception. The way in which people move, talk, and gesture—their fa-cial expressions, posture, and speech—all contribute to the for-mation of impressions about them. Many of the judgments we make about others in our everyday lives are based on cues from these expressive behaviors. Gordon Allport (1937) believed that
206|What is Beautiful is Good|A person&#039;s physical appearance, along with his sexual identity, is the personal characteristic that is most obvious and accessible to others in social inter-action. The present experiment was designed to determine whether physically attractive stimulus persons, both male and female, are (a) assumed to possess more socially desirable personality traits than physically unattractive stimulus persons and (6) expected to lead better lives (e.g., be more competent husbands and wives, be more successful occupationally, etc.) than unattrac-tive stimulus persons. Sex of Subject X Sex of Stimulus Person interactions along these dimensions also were investigated. The present results indicate a &#034;what is beautiful is good &#034; stereotype along the physical attractiveness dimen-sion with no Sex of Judge X Sex of Stimulus interaction. The implications of such a stereotype on self-concept development and the course of social inter-action are discussed. A person&#039;s physical appearance, along with his sexual identity, is the personal character-
207|Evidence for terror management theory II: The effects of mortality salience on reactions to those who threaten or bolster the cultural worldview|Three experiments were conducted to test the hypothesis, derived from terror management theory, that reminding people of their mortality increases attraction to those who consensually validate their beliefs and decreases attraction to those who threaten their beliefs. In Study 1, subjects with a Chris-tian religious background were asked to form impressions of Christian and Jewish target persons. Before doing so, mortality was made salient to half of the subjects. In support of predictions, mortal-ity salience led to more positive evaluations of the in-group member (the Christian) and more nega-tive evaluations of the out-group member (the Jew). In Study 2, mortality salience led to especially negative evaluations of an attitudinally dissimilar other, but only among subjects high in authoritari-anism. In Study 3, mortality salience led to especially positive reactions to someone who directly praised subjects &#039; cultural worldviews and especially negative reactions to someone who criticized them. The implications of these findings for understanding in-group favoritism, prejudice, and intol-erance of deviance are discussed. One of the most destructive and perplexing problems facing contemporary society is the pervasive tendency of people to re-spond with hostility and disdain toward those who are different
208|Getting at the truth or getting along: Accuracy- versus impression-motivated heuristic and systematic processing|Two studies examined the heuristic and systematic processing of accuracy- versus impression-motivated individuals expecting a discussion with a partner believed to hold either a favorable or unfavorable opinion on the discussion issue. Given the goal of having a pleasant interaction, impressionmotivated (versus accuracy-motivated) participants in both studies were particularly likely to express attitudes that were evaluatively consistent with the partner&#039;s opinion, reflecting their selective use of a &#034;go along to get along &#034; heuristic. Study 2 yielded stronger evidence for the distinct nature of heuristic and systematic processing in the service of accuracy versus impression goals. In this study, the evaluative implication of impression-motivated participants &#039; low-effort application of a &#034;go along to get along &#034; heuristic biased their more effortful, systematic processing, leading to attitudes consistent with the partner&#039;s views. In contrast, given the goal of determining an accurate issue opinion, accuracy-motivated participants exhibited relatively evenhanded systematic processing, resulting in attitudes unbiased by the partner&#039;s opinion. The results underscore the utility of a dual-process approach to understanding motivated cognition. Intuition and experience suggest that various motives can influence the way in which people process information and the
211|Conditional random fields: Probabilistic models for segmenting and labeling sequence data|We present conditional random fields, a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data. 1.
212|An Empirical Study of Smoothing Techniques for Language Modeling|We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods. 1
213|A Maximum-Entropy-Inspired Parser|We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &#034;stan- dard&#034; sections of the Wall Street Journal tree- bank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innova- tion is the use of a &#034;maximum-entropy-inspired&#034; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head&#039;s pre-terminal before guessing the lexical head.
214|Class-Based n-gram Models of Natural Language|We address the problem of predicting a word from previous words in a sample of text. In particular we discuss n-gram models based on calsses of words. We also discuss several statistical algoirthms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.
215|Generation and Synchronous Tree-Adjoining Grammars|Tree-adjoining grammars (TAG) have been proposed as a formalism for generation based on the intuition that the extended domain of syntactic locality that TAGs provide should aid in localizing semantic dependencies as well, in turn serving as an aid to generation from semantic representations. We demonstrate that this intuition can be made concrete by using the formalism of synchronous tree-adjoining grammars. The use of synchronous TAGs for generation provides solutions to several problems with previous approaches to TAG generation. Furthermore, the semantic monotonicity requirement previously advocated for generation gram- mars as a computational aid is seen to be an inherent property of synchronous TAGs.
216|An Efficient Boosting Algorithm for Combining Preferences|The problem of combining preferences arises in several applications, such as combining the results of different search engines. This work describes an efficient algorithm for combining multiple preferences. We first give a formal framework for the problem. We then describe and analyze a new boosting algorithm for combining preferences called RankBoost. We also describe an efficient implementation of the algorithm for certain natural cases. We discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different WWW search strategies, each of which is a query expansion for a given domain. For this task, we compare the performance of RankBoost to the individual search strategies. The second experiment is a collaborative-filtering task for making movie recommendations. Here, we present results comparing RankBoost to nearest-neighbor and regression algorithms.
217|Lexical-Functional Grammar:  A Formal System for Grammatical Representation|In learning their native language, children develop a remarkable set of capabilities. They acquire knowledge and skills that enable them to produce and comprehend an indefinite number of novel utterances, and to make quite subtle judgments about certain of their properties. The major goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities. In pursuing this goal, we have adopted what we call the Competence Hypothesis as a methodological principle. We assume that an explanatory model of human language performance will incorporate a theoretically justi ed representation of the native speaker&#039;s linguistic knowledge (a grammar) as a component separate both from the computational mechanisms that operate on it (a processor) and from other nongrammatical processing parameters that might influence the processor&#039;s behavior.  To a certain extent the various components that we postulate can be studied independently, guided where appropriate by the well-established methods and evaluation standards of linguistics, computer science, and experimental psychology. However, the requirement that the various components ultimately must fit together in a consistent and coherent model imposes even stronger constraints on their structure and operation.
218|A Maximum Entropy Model for Part-Of-Speech Tagging|This paper presents a statistical model which trains from a corpus annotated with Part-OfSpeech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%). The model can be classified as a Maximum Entropy model and simultaneously uses many contextual &#034;features&#034; to predict the POS tag. Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.
219|Three Generative, Lexicalised Models for Statistical Parsing|In this paper we first propose a new statistical  parsing model, which is a generative  model of lexicalised context-free gram-  mar. We then extend the model to in-  clude a probabilistic treatment of both subcategorisation  and wh~movement. Results  on Wall Street Journal text show that the  parser performs at 88.1/87.5% constituent  precision/recall, an average improvement  of 2.3% over (Collins 96).
220|A New Statistical Parser Based on Bigram Lexical Dependencies|This paper describes a new statistical  parser which is based on probabilities of  dependencies between head-words in the  parse tree. Standard bigram probability estimation  techniques are extended to calculate  probabilities of dependencies between  pairs of words. Tests using Wall Street  Journal data show that the method per-  forms at least as well as SPATTER (Magerman  95; Jelinek et al. 94), which has  the best published results for a statistical  parser on this task. The simplicity of the  approach means the model trains on 40,000  sentences in under 15 minutes. With a  beam search strategy parsing speed can be  improved to over 200 sentences a minute  with negligible loss in accuracy.
221|Statistical Parsing with a Context-free Grammar and Word Statistics|We describe a parsing system based upon a language  model for English that is, in turn, based upon assigning  probabilities to possible parses for a sentence. This  model is used in a parsing system by finding the parse  for the sentence with the highest probability. This system  outperforms previous schemes. As this is the third  in a series of parsers by different authors that are similar  enough to invite detailed comparisons but different  enough to give rise to different levels of performance,  we also report on some experiments designed to identify  what aspects of these systems best explain their  relative performance.  Introduction  We present a statistical parser that induces its grammar and probabilities from a hand-parsed corpus (a tree-bank). Parsers induced from corpora are of interest both as simply exercises in machine learning and also because they are often the best parsers obtainable by any method. That is, if one desires a parser that produces trees in the tree-bank ...
222|Statistical Decision-Tree Models for Parsing|Syntactic natural language parsers have  shown themselves to be inadequate for processing  highly-ambiguous large-vocabulary  text, as is evidenced by their poor per-  formance on domains like the Wall Street  Journal, and by the movement away  from parsing-based approaches to textprocessing  in general. In this paper, I describe  SPATTER, a statistical parser based  on decision-tree learning techniques which  constructs a complete parse for every sentence  and achieves accuracy rates far better  than any published result. This work  is based on the following premises: (1)  grammars are too complex and detailed to  develop manually for most interesting domains;  (2) parsing models must rely heavily  on lexical and contextual information  to analyze sentences accurately; and (3)  existing n-gram modeling techniques are  inadequate for parsing models. In experiments  comparing SPATTER with IBM&#039;s  computer manuals parser, SPATTER significantly  outperforms the grammar-based  parser. Evaluating SPATTER against the  Penn Treebank Wall Street Journal corpus  using the PARSEVAL measures, SPATTER  achieves 86% precision, 86% recall,  and 1.3 crossing brackets per sentence for  sentences of 40 words or less, and 91% precision,  90% recall, and 0.5 crossing brackets  for sentences between 10 and 20 words in  length.
223|The Penn Treebank: Annotating Predicate Argument Structure|The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as &#034;underlying &#034; position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles. 1. INTRODUCTION During the first phase of the The Penn Treebank project [10], ending in December 1992, 4.5 million words of text were tagged for part-of-speech, with about two-thirds of this material also annotated with a skeletal syntactic bracketing. All of this material has been hand corre...
224|Discriminative Reranking for Natural Language Parsing|This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model’s score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.  
225|Three New Probabilistic Models for Dependency Parsing: An Exploration|After presenting a novel O(n³) parsing algorithm  for dependency grammar, we develop  three contrasting ways to stochasticize  it. We propose (a) a lexical affinity model  where words struggle to modify each other,  (b) a sense tagging model where words fluctuate  randomly in their selectional preferences,  and (c) a generative model where  the speaker fleshes out each word&#039;s syntactic  and conceptual structure without regard to  the implications for the hearer. We also give  preliminary empirical results from evaluating  the three models&#039; parsing performance  on annotated Wall Street Journal training  text (derived from the Penn Treebank). In  these results, the generative model performs  significantly better than the others, and  does about equally well at assigning part-of-speech tags.  
226|Treebank Grammars|By a “tree-bank grammar ” we mean a context-free grammar created by reading the production rules directly from hand-parsed sentences in a tree bank. Common wisdom has it that such grammars do not perform we &amp; though we know of no published data on the issue. The primary purpose of this paper is to show that the common wisdom is wrong. In particular, we present results on a tree-bank grammar based on the Penn WaII Street Journal tree bank. To the best of our knowledge, this grammar outperforms ah other non-word-based statistical parsers/grammars on this corpus. That is, it outperforms parsers that consider the input as a string of tags and ignore the actual words of the corpus.
227|Gemini: A Natural Language System For Spoken-Language Understanding|This paper describes the details of the system, and includes relevant measurements of size, efficiency, and performance of each of its components
228|A Linear Observed Time Statistical Parser Based on Maximum Entropy Models|This paper presents a statistical parser for  natural language that obtains a parsing  accuracy--roughly 87% precision and 86%  recall--which surpasses the best previously  published results on the Wall St. Journal  domain. The parser itself requires very little  human intervention, since the information  it uses to make parsing decisions is  specified in a concise and simple manner,  and is combined in a fully automatic way  under the maximum entropy framework.
231|A Statistical Parser for Czech|This paper considers statistical parsing of Czech, which differs radically from English in at least two  respects: (1) it is a highly infiected language, and (2) it has relatively free word order. These dif- ferences are likely to .pose new problems for tech- niques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results - 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.
232|Equations for Part-of-Speech Tagging|We derive from first principles the basic equations for a few of the basic hidden-Markov-model word taggers as well as equations for other models which may be novel (the descriptions in previous papers being too spare to be sure). We give performance results for all of the models. The results from our best model (96.45% on an unused test sample from the Brown corpus with 181 distinct tags) is on the upper edge of reported results. We also hope these results clear up some confusion in the literature about the best equations to use. However, the major purpose of this paper is to show how the equations for a variety of models may be derived and thus encourage future authors to give the equations for their model and the derivations thereof.  Introduction  The last few years have seen a fair number of papers on part-of-speech tagging --- assigning the correct part of speech to each word in a text [1,2,4,5,7,8,9,10]. Most of these systems view the text as having been produced by a hidden Mar...
233|A novel use of statistical parsing to extract information from text|Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.
234|Efficient Parsing for Bilexical Context-Free Grammars and Head Automaton Grammars|Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words. We present O(n^4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of O(n^5). For a common special case that was known to allow O(n³) parsing (Eisner, 1997), we present an O(n³) algorithm with an improved grammar constant.
235|Parsing Inside-Out|Probabilistic Context-Free Grammars (PCFGs) and variations on them have recently become some of the most common formalisms for parsing. It is common with PCFGs to compute the inside and outside probabilities. When these probabilities are multiplied together and normalized, they produce the probability that any given non-terminal covers any piece of the input sentence. The traditional use of these probabilities is to improve the probabilities of grammar rules. In this thesis we show that these values are useful for solving many other problems in Statistical Natural Language Processing. We give a framework for describing parsers. The framework generalizes the inside and outside values to semirings. It makes it easy to describe parsers that compute a wide variety of interesting quantities, including the inside and outside probabilities, as well as related quantities such as Viterbi probabilities and n-best lists. We also present three novel uses for the inside and outside probabilities. T...
236|Learning Parse and Translation Decisions from Examples with Rich Context|We present a knowledge and context-based system for parsing and translating natural language and evaluate it on sentences from the Wall Street Journal. Applying machine learning techniques, the system uses parse action examples acquired under supervision to generate a deterministic shift-reduce parser in the form of a decision structure. It relies heavily on context, as encoded in features which describe the morphological, syntactic, semantic and other aspects of a given parse state.
237|Efficient Algorithms for Parsing the DOP Model|Excellent results have been reported for DataOriented Parsing (DOP) of natural language texts (Bod, 1993c). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo p arsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model toga small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct con- stituents, rather than the probability of a correct parse tree. Using ithe optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is compara- ble to results from a duplication of Pereira and Schabes&#039;s (1992) experiment on the same data. We show that Bod&#039;s results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.
238|Conditional Structure versus Conditional Estimation in NLP Models|This paper separates conditional parameter estimation,  which consistently raises test set accuracy on  statistical NLP tasks, from conditional model structures,  such as the conditional Markov model used  for maximum-entropy tagging, which tend to lower  accuracy. Error analysis on part-of-speech tagging  shows that the actual tagging errors made by the  conditionally structured model derive not only from  label bias, but also from other ways in which the independence  assumptions of the conditional model  structure are unsuited to linguistic sequences. The  paper presents new word-sense disambiguation and  POS tagging experiments, and integrates apparently  conflicting reports from other recent work.
240|Pearl: A Probabilistic Chart Parser|This i)al)cr descrihcs a natural language i)ars - ing algorith,n for unrestricted text whicll uses a i)rol)ability-based scoring fimctiou to select the &#034;}mst&#034; I)arse of a sentence. The parser, earl, is a I. ime-a.synchronous bottom-ul) chart I)arscr with Earicy-type top-down prediction which pursues the highest-scoring theory i} the chart, where the score of a theory represents the cxteut I,o which t. he context of the sentmice predicts that iuterpretation. This parser differs h&#039;om previous attempts at stochastic parsers in thai. it uses a richer form of conditional probalfilitics based on context to l)rcdiet likelihood. Pearl also provides a fralnework for incorporating l.he results of previous work iu Imrt-olLsl)cech assignnmnt, mlknown word models, and ol.her Irol)al)ilistic models of linguistic features iuto one parslug tool, interleaving these techniques instead of using the traditional pipeline archiLecture. In preliminary tests, &#039;Pearl has been successl&#039;ul aL resolving parL-o[-speech and word (in speech processing) ambiguiLy, de[ermiuing categories [or unknown words, and selecLing cotreeL parses first. using a very loosely fiLing covering grammar. 1  
241|Development and Evaluation of a Broad-Coverage Probabilistic Grammar of English-Language Computer Manuals|We present an approach to grammar development where the task is decomposed into two separate subtasks. The first task is linguistic, with the goal of producing a set of rules that have a large coverage (in the sense that the correct parse is among the proposed parses) on a blind test set of sentences. The second task is statistical, with the goal of developing a model of the grammax which assigns maximum probability for the correct parse. We give parsing results on text from computer manuals.
242|Decision tree parsing using a hidden derivation model|Parser development is generally viewed as a primarily linguistic enterprise. A grammarian examines sentences, skillfully extracts the linguistic generalizations evident in the data, and writes grammar rules which cover the language. The grammarian
243|Efficiency, Robustness and Accuracy in Picky Chart Parsing|This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called probabilistic prediction to predict which grammar rules are likely to lead to an acceptable parse of the input. Using a suboptimal search method, Picky significantly reduces the number of edges produced by CKY-like chart parsing algorithms, while maintaining the robustness of pure bottom-up parsers and the accuracy of existing probabilistic parsers. Experiments using Picky demonstrate how probabilistic modelling can impact upon the efficiency, robustness and accuracy of a parser. 
244|Head Automata and Bilingual Tiling: Translation with Minimal Representations|We present a language model consisting of  a collection of costed bidirectional finite  state automata associated with the head  words of phrases. The model is suitable  for incremental application of lexical associations  in a dynamic programming search  for optimal dependency tree derivations. We also
245|Corpus Statistics Meet the Noun Compound: Some Empirical Results|A variety of statistical methods for noun  compound analysis are implemented and  compared. The results support two main  conclusions. First, the use of conceptual  association not only enables a broad coverage,  but also improves the accuracy. Second,  an analysis model based on dependency  grammar is substantially more accurate  than one based on deepest constituents,  even though the latter is more preva-  lent in the literature.
246|Context-Sensitive Statistics for Improved Grammatical Language Models|We develop a language model using probabilistic context-free grammars (PCFGs) that is &#034;pseudo context-sensitive&#034; in that the probability that a non-terminal N expands using a rule r depends on N &#039;s parent. We derive the equations for estimating the necessary probabilities using a variant of the inside-outside algorithm. We give experimental results showing that, beginning with a high-performance PCFG, one can develop a pseudo PCSG that yields significant performance gains. Analysis shows that the benefits from the context-sensitive statistics are localized, suggesting that we can use them to extend the original PCFG. Experimental results confirm that this is both feasible and the resulting grammar retains the performance gains. This implies that our scheme may be useful as a novel method for PCFG induction. 1 Introduction  Like its non-stochastic brethren, probabilistic parsing has been based upon context-free grammars (CFGs), and for similar reasons: CFGs support a simple and efficien...
247|Statistical Parsing of Messages|The recent trend in natural language processing research has been to develop systems that deal with text concerning small, well defined domains. One practical application for such systems is to process messages pertaining to some very specific task or activity [5]. The advantage of dealing with such domains is twofold- firstly, due to the narrowness of the domain, it is possible to encode most of the knowledge related to the domain and to make this knowledge accessible to the natural language processing system, which in turn can use this knowledge to disambiguate the meanings of the messages. Secondly, in such a domain, there is not a great diversity of language constructs and therefore it becomes easier to construct a grammar which will capture all the constructs which exist in this sub-language. However, some practical aspects of such domains tend to make the problem somewhat difficult. Often, the messages tend not to be absolutely grammatically correct. As a result, the grammar designed for such a system needs to be far more forgiving than one designed for the task of parsing edited English. This can result in a proliferation of parses, which in turn makes the disambiguation task more difficult. This problem is further compounded by the telegraphic nature of the discourse, since telegraphic discourse is more prone to be syntactically ambiguous. Statistical Parsing The major objective of the research described in this paper is to use statistical data to evaluate the likelihood of a parse in order to help the parser prune out unlikely parses. Our conjecture- supported by our results and some prior, similar experiments- is that a more probable parse has a greater chance of being the correct one. The related work by the research team at UCREL
248|Global Thresholding and Multiple-Pass Parsing|We present a variation on classic beam  thresholding techniques that is up to an order  of magnitude faster than the traditional  method, at the same performance level. We  also present a new thresholding technique,  global thresholding, which, combined with  the new beam thresholding, gives an additional  factor of two improvement, and a  novel technique, multiple pass parsing, that  can be combined with the others to yield  yet another 50% improvement. We use a  new search algorithm to simultaneously op-  timize the thresholding parameters of the  various algorithms.
249|Probabilistic Feature Grammars|We present a new formalism, probabilistic feature grammar (PFG). PFGs combine most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance. PFGs generate features one at a time, probabilistically, conditioning the probabilities of each feature on other features in a local context. Because the conditioning is local, efficient polynomial time parsing algorithms exist for computing inside, outside, and Viterbi parses. PFGs can produce probabilities of strings, making them potentially useful for language modeling. Precision and recall results are comparable to the state of the art with words, and the best reported without words. 1 Introduction  Recently, many researchers have worked on statistical parsing techniques which try to capture additional context beyond that of simple probabilistic context-free grammars (PCFGs), including Magerman (1995), Charniak (1996), Collins (1996; 1997), ...
250|Stochastic HPSG|In this paper we provide a probabilistic  interpretation for typed feature structures  very similar to those used by Pollard  nd Sag. We begin with a version  of the interpretation which lacks  a treatment of re-entrant feature struc-  tures, then provide an extended interpre-  tation which allows them. We sketch al-  gorithms allowing the numerical parameters  of our probabilistic interpretations  of HPSG to be estimated from corpora.
251|What is the Minimal Set of Fragments that Achieves Maximal Parse Accuracy?|We aim at finding the minimal set of  fragments which achieves maximal parse  accuracy in Data Oriented Parsing. Experiments  with the Penn Wall Street  Journal treebank show that counts of  almost arbitrary fragments within parse  trees are important, leading to improved  parse accuracy over previous models  tested on this treebank (a precision of 90.8% and a recall of 90.6%). We  isolate some dependency relations which  previous models neglect but which  contribute to higher parse accuracy.
252|Automatic Learning for Semantic Collocation|The real difficulty in development of practical NLP systems comes from the fact that we do not have effective means for gathering &#034;knowledge &#034;. In this paper, we propose an algorithm which acquires automatically knowledge of semantic collocations among &#034;words&#034; from sample corpora. The algorithm
253|A Statistical Model for Parsing and Word-Sense Disambiguation|This paper describes a first attempt at a statistical model for simultaneous syntactic parsing and generalized word-sense disambiguation. On a new data set we have constructed for the task, while we were disappointed not to find parsing improvement over a traditional parsing model, our model achieves a recall of 84.0% and a precision of 67.3% of exact synset matches on our test corpus, where the gold standard has a reported inter-annotator agreement of 78.6%.
254|On The Unsupervised Induction Of Phrase-Structure Grammars|This paper examines why some previous approaches have failed to acquire desired grammars without supervision, and proposes that with a different conception of phrase-structure supervision might not be necessary. In particular, it describes in detail some reasons why SCFGs are poor mod-  2 CARL DE MARCKEN els to use for learning human language, especially when combined with the inside-outside algorithm. Following up on these arguments, it proposes that head-driven grammatical formalisms like link grammars (Sleator and Temperley, 1991) are better suited to the task, and introduces a framework for CFG induction that sidesteps many of the search problems that previous schemes have had. In the end, we hope the analysis presented here convinces others to look carefully at their representations and search strategies before blindly applying them to the language learning task. We start the discussion by examining the differences between the linguistic and statistical motivations for phrase structure; this frames our subsequent analysis. Then we introduce a simple extension to stochastic context-free grammars, and use this new class of language models in two experiments that pinpoint specific problems with both SCFGs and the search strategies commonly applied to them. Finally, we explore fixes to these problems.
255|The Effect of Alternative Tree Representations on Tree Bank Grammars|The performance of PCFGs estimated from  tree banks is shown to be sensitive to the particular  way in which linguistic constructions  are represented as trees in the tree bank. This  paper presents a theoretical analysis of the  effect of different tree representations for PP  attachment on PCFG models, and introduces  a new methodology for empirically examining  such effects using tree transformations. It  shows that one transformation, which copies  the label of a parent node onto the labels of  its children, can improve the performance of  a PCFG model in terms of labelled precision  and recall on held out data from 73% (precision)  and 69% (recall) to 80% and 79% respectively.  It also points out that if only maximum  likelihood parses are of interest then  many productions can be ignored, since they  are subsumed by combinations of other productions  in the grammar. In the Penn II tree  bank grammar, almost 9% of productions are  subsumed in this way.  1 
256|A Probabilistic Parser Applied to Software Testing Documents|We describe an approach to training a statistical parser from a bracketed corpus, and demonstrate its use in a software testing application that translates English specifications into an automated testing language. A grammar is not explicitly specified; the rules and contextual probabilities of occurrence are automatically generated from the corpus. The parser is extremely successful at producing and identifying the correct parse, and nearly deterministic in the number of parses that it produces. To compensate for undertraining, the parser also uses general, linguistic subtheories which aid in guessing some types of novel structures.  Introduction  In constrained domains, natural language processing can often provide leverage. In software testing at AT&amp;T, for example, 20,000 English test cases prescribe the behavior of a telephone switching system. A test case consists of about a dozen sentences describing the goal of the test, the actions to perform, and the conditions to verify. Figu...
257|A Probabilistic Parser and Its Application|We describe a general approach to the probabilistic parsing of context-free grammars. The method integrates context-sensitive statistical knowledge of various types (e.g., syntactic and semantic) and can be trained incrementally from a bracketed corpus. We introduce a variant of the GHR contextfree recognition algorithm, and explain how to adapt it for efficient probabilistic parsing. On a real-world corpus of sentences from software testing documents, with 23 possible parses for a sentence of average length, the system accurately finds the correct parse in 99% of cases, while producing only 1.02 parses per sentence. Significantly, the success rate would be only 66% without the semantic statistics.  Introduction  In constrained domains, natural language processing can often provide leverage. At AT&amp;T, for instance, NL technology can potentially help automate many aspects of software development. A typical example occurs in the software testing area. Here 250,000 English sentences specif...
259|Derivatives of regular expressions|Abstract. Kleene&#039;s regular expressions, which can be used for describing sequential circuits, were defined using three operators (union, concatenation and iterate) on sets of sequences. Word descriptions of problems can be more easily put in the regular expression language if the language is enriched by the inclusion of other logical operations. However, in the problem of converting the regular expression description to a state diagram, the existing methods either cannot handle expressions with additional operators, or are made quite complicated by the presence of such operators. In this paper the notion of a derivative of a regular expression is introduced and the properties of derivatives are discussed. This leads, in a very natural way, to the construction of a state diagram from a regular expression containing any number of logical operators.
260|The case for motivated reasoning|It is proposed that motivation may affect reasoning through reliance on a biased set of cognitive processes—that is, strategies for accessing, constructing, and evaluating beliefs. The motivation to be accurate enhances use of those beliefs and strategies that are considered most appropriate, whereas the motivation to arrive at particular conclusions enhances use of those that are considered most likely to yield the desired conclusion. There is considerable evidence that people are more likely to arrive at conclusions that they want to arrive at, but their ability to do so is constrained by their ability to construct seemingly reasonable justifications for these conclusions. These ideas can account for a wide variety of research concerned with motivated reasoning. The notion that goals or motives affect reasoning has a long and controversial history in social psychology. The propositions that motives may affect perceptions (Erdelyi, 1974), attitudes (Festinger, 1957), and attributions (Heider, 1958) have been put forth by some psychologists and challenged by others. Al-though early researchers and theorists took it for granted that motivation may cause people to make self-serving attributions
261|Illusion and well-being: A social psychological perspective on mental health|Many prominent theorists have argued that accurate perceptions of the self, the world, and the future are essential for mental health. Yet considerable research evidence suggests that overly positive selfevaluations, exaggerated perceptions of control or mastery, and unrealistic optimism are characteristic of normal human thought. Moreover, these illusions appear to promote other criteria of mental health, including the ability to care about others, the ability to be happy or contented, and the ability to engage in productive and creative work. These strategies may succeed, in large part, because both the social world and cognitive-processing mechanisms impose filters on incoming information that distort it in a positive direction; negative information may be isolated and represented in as unthreatening a manner as possible. These positive illusions may be especially useful when an individual receives negative feedback or is otherwise threatened and may be especially adaptive under these circumstances. Decades of psychological wisdom have established contact with reality as a hallmark of mental health. In this view, the wcU-adjusted person is thought to engage in accurate reality testing, whereas the individual whose vision is clouded by illusion is regarded as vulnerable to, if not already a victim of, mental illness. Despite its plausibility, this viewpoint is increasingly difficult to maintain (cf. Lazarus, 1983). A substantial amount of research testifies to the prevalence of illusion in normal human
262|Biased assimilation and attitude polarization: The effects of prior theories on subsequently considered evidence|People who hold strong opinions on complex social issues are likely to examine relevant empirical evidence in a biased manner. They are apt to accept &#034;confirming&#034; evidence at face value while subjecting &#034;discontinuing &#034; evidence to critical evaluation, and as a result to draw undue support for their initial positions from mixed or random empirical findings. Thus, the result of exposing contending factions in a social dispute to an identical body of relevant empirical evidence may be not a narrowing of disagreement but rather an increase in polarization. To test these assumptions and predictions, subjects supporting and opposing capital punishment were exposed to two purported studies, one seemingly confirming and one seemingly disconfirming their existing beliefs about the deterrent efficacy of the death penalty. As predicted, both proponents and opponents of capital punishment rated those results and procedures that confirmed their own beliefs to be the more convincing and probative ones, and they reported corresponding shifts in their beliefs as the various results and procedures were presented. The net effect of such evaluations
263|Confirmation, Disconfirmation, and Information in Hypothesis Testing|Strategies for hypothesis testing in scientific investigation and everyday reasoning have interested both psychologists and philosophers. A number of these scholars stress the importance of disconnrmation in reasoning and suggest that people are instead prone to a general deleterious &#034;confirmation bias.&#034; In particular, it is suggested that people tend to test those cases that have the best chance of verifying current beliefs rather than those that have the best chance of falsifying them. We show, however; that many phenomena labeled &#034;confirmation bias&#034; are better understood in terms of a general positive test strategy. With this strategy, there is a tendency to test cases that are expected (or known) to have the property of interest rather than those expected (or known) to lack that property. This strategy is not equivalent to confirmation bias in the first sense; we show that the positive test strategy can be a very good heuristic for determining the truth or falsity of a hypothesis under realistic conditions. It can, however, lead to systematic errors or inefficiencies. The appropriateness of human hypothesis-testing strategies and prescriptions about optimal strategies must be understood in terms of the interaction between the strategy and the task at hand. 
264|Cognitive consequences of forced compliance|WHAT happens to a person&#039;s private opinion if he is forced to do or say something contrary to that opinion? Only recently has there been, any experimental work related to this question. Two studies reported by Janis and King (1954; 1956) clearly showed that, at least under some conditions, the private opinion changes so as to bring it into closer correspondence with the overt behavior the person was forced to perform. Specifically, they showed that if a person is forced to improvise a speech supporting a point of view with which he disagrees, his private opinion moves toward the position advocated in the speech. The observed opinion change is greater than for persons who only hear the speech or for persons who read a prepared speech with emphasis solely on elocution and manner of delivery. The authors of these two studies explain their results mainly in terms of mental rehearsal and thinking up new arguments. Inthisway, they propose, theperson who is forced to improvise a speech convinces himself. They present some evidence, which is not altogether conclusive, in support of this explanation. We will have more to say concerning this explanation in discussing the results of our experiment. Kelrnan (1953) tried to pursue the matter further. He reasoned that if the person is induced to make an overt statement contrary to his private opinion by the offer of some reward, then the greater the reward offered, the greater should be the subsequent opinion change. His data, however, did not support this idea. He found, rather, that a large reward produced less subsequent opinion change than did a smaller reward. Actually, this finding by Kelman is consistent with the theory we will outline below but, for a number of reasons, is
265|The Totalitarian Ego -- Fabrication and Revision of Personal History| This article argues that (a) ego, or self, is an organization of knowledge, (b) ego is characterized by cognitive biases strikingly analogous to totalitarian information-control strategies, and (c) these totalitarian-ego biases junction to preserve organization in cognitive structures. Ego&#039;s cognitive biases are egocentricity (self as the focus of knowledge), &#034;beneffectance&#034; (perception of responsibility for desired, but not undesired, outcomes), and cognitive conservatism (resistance to cognitive change). In addition to being pervasively evident in recent studies of normal human cognition, these three biases are found in actively functioning, higher level organizations of knowledge, perhaps best exemplified by theoretical paradigms in science. The thesis that egocentricity, beneffectance, and
266|Reasons for Confidence|People are often overconfident in evaluating the correctness of their knowl-edge. The present studies investigated the possibility that assessment of con-fidence is biased by attempts to justify one&#039;s chosen answer. These attempts include selectively focusing on evidence supporting the chosen answer and disregarding evidence contradicting it. Experiment 1 presented subjects with two-alternative questions and required them to list reasons for and against each of the alternatives prior to choosing an answer and assessing the prob-ability of its being correct. This procedure produced a marked improvement in the appropriateness of confidence judgments. Experiment 2 simplified the manipulation by asking subjects first to choose an answer and then to list (a) one reason supporting that choice, (b) one reason contradicting it, or (c) one reason supporting and one reason contradicting. Only the listing of contradicting reasons improved the appropriateness of confidence. Correla-tional analyses of the data of Experiment 1 strongly suggested that the con-fidence depends on the amount and strength of the evidence supporting the answer chosen. e remarkable characteristic of human&gt;ry is its knowledge of its own content, nents of confidence in the correctness &#039;all and recognition performance are research was supported by the Advanced ch Projects Agency of the Department of e, and was monitored by Office of Naval
267|Stability and malleability of the self-concept|The self-concept literature is characterized by a continuing controversy over whether the self-concept is stable or malleable. In this article we suggest that it is both but that the stability observed for general descriptions of the self may mask significant local variation. In this study the social environment was varied by creating a situation in which subjects found themselves to be either very unique or very similar to others. Following this manipulation, subjects responded to a series of self-concept mea-sures. Although the uniqueness and similarity subjects did not differ in the trait terms they used to describe themselves, they did differ systematically in their latency for these judgments, in positivity and negativity of their word associations, and in their judgments of similarity to reference groups. These findings imply that subjects made to feel unique recruited conceptions of themselves as similar to others, whereas subjects made to feel similar to others recruited conceptions of themselves as unique. The results suggest that very general self-descriptive measures are inadequate for revealing how the individual adjusts and calibrates the self-concept in response to challenges from the social environment. Two seemingly contradictory aspects of the self have been emphasized in the empirical self-concept literature. The self has been regarded as a stable and enduring structure that pro-tects itself against change (e.g., Greenwald, 1980; Markus,
268|Effects of involvement on persuasion: A meta-analysis|Defines involvement as a motivational state induced by an association between an activated attitude and the self-concept. Integration ofthe available research su~ests hat he effects of involvement on attitude change depended on the aspect of message recipients &#039; elf-concept that was activated to create involvement: (a) their enduring values (value-relevant i volvement), (b) their ability to attain desirable outcomes (outcome-relevant involvement), or (e) the impression they make on others (im-pression-relevant i volvement). Findings howed that (a) with value-relevant i volvement, high-in-volvement subjects were less persuaded than low-involvement subjects; (b) with outcome-relevant involvement, high-involvement subjects were more persuaded than low-involvement subjects by strong arguments and (somewhat inconsistently) less persuaded by weak arguments; and (c) with impression-relevant involvement, high-involvement subjects were slightly less persuaded than low-involvement subjects. To understand the conditions under which people are per-suaded by others, researchers have often invoked the concept of involvement. Although this construct was popular prior to M. Sherifand Cantril&#039;s (1947) work (see A. G. Greenwald&#039;s, 1982,
269|Accountability: A Social Magnifier of the Dilution Effect|This research demonstrated that accountability can not only reduce judgmental bias, but also exac-erbate it—in this case, the dilution effect. Ss made predictions from either diagnostic information alone or diagnostic information plus mixtures of additional data (nondiagnostic information, addi-tional diagnostic data pointing to either the same conclusion or the opposite conclusion). Relative to unaccountable Ss, accountable Ss (a) diluted their predictions in response to nondiagnostic infor-mation and (b) were more responsive to additional diagnostic information. The accountability ma-nipulation motivated subjects to use a wide range of information in making judgments, but did not make them more discriminating judges of the usefulness of that information. Cognitive social psychologists have painted numerous por-traits of the person as information processor. Early work em-phasized the rigorous rationality with which people analyzed and drew inferences from evidence: the correspondent infer-ence model, the causal schemata model, the covariation model, and the Bayesian model. Later work emphasized people&#039;s judg-mental shortcomings. People were depicted as cognitive misers whose preference for simple, easy-to-execute heuristics ren-dered them vulnerable to a variety of errors and biases (Abelson
270|Outcome dependency: Attention, attribution, and attraction|Theoretical and empirical work on the processes by which we attribute dis-positional characteristics to others has focused almost exclusively on how such processes proceed once the perceiver has been motivated to initiate them. The problem of identifying the factors which prompt the perceiver to engage in an attributional analysis in the first place has been relatively ignored, even though the influence of such factors may extend beyond the initiation of the causal analysis to affect the manner in which it unfolds and, ultimately, the form and substance of its conclusion. From the assumption that the function of an attributional analysis is effective control of the social environment, it was hypothesized that high outcome dependency upon another, under conditions of high unfamiliarity, is associated with the initiation of an attributional anal-ysis as evidenced by increased attention to the other, better memory of the other&#039;s characteristics and behavior, more extreme and confidently given evalua-tions of the other on a variety of dispositional trait dimensions, and increased attraction to the other. These hypotheses were tested within the context of a
271|Personal Involvement and Strategies for Making Contingency Judgements: A Stake in the Dating Game Makes a Difference|To examine the relation between degree of involvement in a task and the complexity of strategy a subject applies to the task, we randomly assigned 48 female university volunteers to either a dating condition (high-involvement) or one of two (low-involvement) control conditions. These subjects performed a covariation judgment task for which the likelihood of their using simple or complex strategies was calculated. High-involvement subjects used more complex strategies and tended to be more accurate. These data are discussed in terms of the functionality of human information processing, heuristic analyses of inference strategies, and the importance of considering level of personal involvement in analyses of task performance. There are often many ways to solve a problem. Whether the problem entails choos-ing a mate, buying a car, isolating x on one side of an equation, attributing a cause, de-ciding whether two variables covary, or skin-
272|The TSIMMIS Project: Integration of Heterogeneous Information Sources |The goal of the Tsimmis Project is to develop tools that facilitate the rapid integration of heterogeneous information sources that may include both structured and unstructured data. This paper gives an overview of the project, describing components that extract properties from unstructured objects, that translate information into a common object model, that combine information from several sources, that allow browsing of information, and that manage constraints across heterogeneous sites. Tsimmis is a joint project between Stanford and the IBM Almaden Research Center.
273|A Comparative Analysis of Methodologies for Database Schema Integration| One of the fundamental principles of the database approach is that a database allows a nonredundant, unified representation of all data managed in an organization. This is achieved only when methodologies are available to support integration across organizational and application boundaries. 

Methodologies for database design usually perform the design activity by separately producing several schemas, representing parts of the application, which are subsequently merged. Database schema integration is the activity of integrating the schemas of existing or proposed databases into a global, unified schema. The aim of the paper is to provide first a unifying framework for the problem of schema integration, then a comparative review of the work done thus far in this area. Such a framework, with the associated analysis of the existing approaches, provides a basis for identifying strengths and weaknesses of individual methodologies, as well as general guidelines for future improvements and extensions.
274|Object exchange across heterogeneous information sources|We address the problem of providing integrated access to diverse and dynamic information sources. We explain how this problem differs from the traditional database integration problem and we focus on one aspect of the information integration problem, namely information exchange. We define an object-based information exchange model and a corresponding query language that we believe are well suited for integration of diverse information sources. We describe how, the model and language have been used to integrate heterogeneous bibliographic information sources. We also describe two general-purpose libraries we have implemented for object exchange between clients and servers.
275|Temporal databases|A temporal database (see Temporal Database) contains time-varying data. Time is an important aspect of all real-world phenomena. Events occur at specific points in time; objects and the relationships among objects exist over time. The ability to model this temporal dimension of the real world is essential to many computer applications, such as accounting, banking, econometrics, geographical information systems, inventory control, law, medical records, multi-media, process control, reservation systems, and scientific data analysis. Conventional databases represent the state of an enterprise at a single moment of time. Although the contents of the database continue to change as new information is added, these changes are viewed as modifications to the state, with the old, out-of-date data being deleted from the database. The current contents of the database may be viewed as a snapshot of the enterprise. When a conventional database is used, the attributes involving time are manipulated solely by the application programs, with little help
276|Managing semantic heterogeneity with production rules and persistent queues|Abstract. We show that production rules and persis-tent queues together provide a convenient mechanism for maintaining consistency in semantically heterogeneous multi-database environments. We describe a specification language and methods for automatically deriving production rules that maintain (1) existence dependencies, in which the presence of data in one database implies the presence of related data in another, and (2) value dependencies, in which the value of data in one database is baaed on the value of related data in another. The production rules derived from dependency specifications use persistent queues to monitor and maintain the dependencies automatically, asynchronously, incremen-tally, and correctly. 1
277|Combining Theory and Practice in Integrity Control: A Declarative Approach to the Specification of a Transaction Modification Subsystem|Integrity control is generally considered an important topic in the field of database system research. In the database literature, many proposals for integrity control mechanisms can be found. A large group of proposals has a formal character, and does not cover complete algorithms that can be used in a real-world database system with multi-update transactions. Another group of proposals is system-oriented and often lacks a complete formal background on transactions and integrity control; algorithms are usually described in system terms. This paper combines the essentials of both groups: it presents a declarative specification of a transaction-based integrity control technique that has a solid formal basis and can easily be applied in real-world database systems. The technique, called transaction modification, features simple semantics, full transaction support, and extensibility to parallel data processing. These claims are supported by a prototype implementation of a transaction modi...
278|Constraint Management in Loosely Coupled Distributed Databases|We provide a framework for managing integrity constraints that span multiple databases in loosely coupled, heterogeneous environments. Our framework enables the formal description of (1) interfaces provided by a database for the data items involved in multi-database constraints; (2) strategies for monitoring and maintaining multi-database constraints; (3) guarantees regarding the consistency of multi-database constraints. With our approach one can define &#034;relaxed&#034; constraints that only hold at certain times or under certain conditions. Such constraints appear often in practice and cannot be handled effectively by conventional, transaction-based approaches. We also describe a toolkit, based on our framework, for enforcing constraints over heterogeneous systems. The toolkit includes a general-purpose, distributed constraint manager that can be easily configured to a given environment and constraints. A first version of the toolkit has been implemented and is under evaluation. 1 Introduct...
279|The STATEMATE Semantics of Statecharts|This article describes the semantics of the language of statecharts as implenented in the STATEMATE system [Harel et al. 1990; Harel and Politi 1996]. The initial version of this semantics was developed by a team about.10 years ago. With the added experience of the users of the system it has since been extended and modified. This executable semantics has been in operation in driving the simulation, dynamic tests, and code generation tDols of STATEMATE since 1987, and a technical report describing it has been available from i-Logix, Inc. since 1989. We have now decided to revise and publish the report so as to make it more widely accessible, to alleviate some of the confusion about the &#034;official semantics of the language, and to counter a number of incorrect comments made in the literature about the way statecharts have been implemented. For example, the survey [yon der Beek 1994] does not mention the STATEMATE implementation of statecharts or the semantics adopted for it at all, although this semantics is different from the ones surveyed therein (and was developed earlier than all of them except for Harel et al. [1987]). As another example, Leveson et al. [1995] describe a case that exhibits an unacceptable kind of behavior in a statechart, which they say is what the &#034;semantics of statecharts&#034; leads to (pp. 695-697). Unfortunately, they base their discussion of statechart semantics on one of the many semantics proposed by various authors (that of Pnueli and Shalev [1991]) and give the reader the impression that this is the official semantics of the language
280|Goal-directed Requirements Acquisition|Requirements analysis includes a preliminary acquisition step where a global model for the specification of the system and its environment is elaborated. This model, called requirements model, involves concepts that are currently not supported by existing formal specification languages, such as goals to be achieved, agents to be assigned, alternatives to be negotiated, etc. The paper presents an approach to requirements acquisition which is driven by such higher-level concepts. Requirements models are acquired as instances of a conceptual meta-model. The latter can be represented as a graph where each node captures an abstraction such as, e.g., goal, action, agent, entity, or event, and where the edges capture semantic links between such abstractions. Well-formedness properties on nodes and links constrain their instances - that is, elements of requirements models. Requirements acquisition processes then correspond to particular ways of traversing the meta-model graph to acquire approp...
281|Imagined Communities|This is a field report of a three-week experience in Japan, centered on art education in their cultural and social contexts. Beginning with this overarching focus, the themes and patterns that structure this report were emergent, rising from the experience. Those supporting themes are: being in Japan and in Mino city (setting a context); the culture of handmade Washi paper; the qualities of the Washi paper festival; craft as a way of teaching, being and learning; children and their art at school and through the festival, and the importance of ritual. This report is written in a personal narrative style as suggested in contemporary feminist and transactive ethnographic literature. Key Words:cross-cultural art education, feminist, transactive ethnography, Japanese art education Report from Japan: Art,
282|Representing and Using Non-Functional Requirements: A Process-Oriented Approach|The paper proposes a comprehensive framework for representing and using non-functional requirements during the development process. The framework consists of five basic components which provide for the representation of non-functional requirements in terms of interrelated goals. Such goals can be refined through refinement methods and can be evaluated in order to determine the degree to which a set of non-functional requirements is supported by a particular design. Evidence for the power of the framework is provided through the study of accuracy and performance requirements for information systems.  1 
283|Multi-party Specification|This paper examines a formal model of how specifications can be constructed from multiple viewpoints and presents some tools to support this approach. The development of specifications is presented as a dialogue in which the viewpoints negotiate, establish responsibilities and cooperatively construct a specification. The model is illustrated by means of some small examples. Keywords: formal specification, distributed artificial intelligence, dialogue, logic, tool support 1 Introduction &#034;Specification-in-the-large&#034;, that is the development of requirements specifications for systems of substantial complexity and scale, mirrors &#034;programming-in-thelarge &#034; in raising a variety of difficulties that lie beyond the clerical problems of handling large amounts of information (Cunningham, Finkelstein et al 1985, Finkelstein &amp; Potts 1987). One such difficulty is that of specification from multiple viewpoints (Niskier 1987). Specification-in-the-large is an activity in which there are many particip...
284|Metal: A formalism to specify formalisms|Mentor is a general system for the manipulation of structured information. Its main application is the construction of interactive programming environments. In such an environment, a programmer may design, implement, document, debug, test, analyze, validate, maintain and transport his programs. An environment
285|From data mining to knowledge discovery in databases|¦ Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field. Across a wide variety of fields, data are
287|The Merge/Purge Problem for Large Databases|Many commercial organizations routinely gather large numbers of databases for various marketing and business analysis functions. The task is to correlate information from different databases by identifying distinct individuals that appear in a number of different databases typically in an inconsistent and often incorrect fashion. The problem we study here is the task of merging data from multiple sources in as efficient manner as possible, while maximizing the accuracy of the result. We call this the merge/purge problem. In this paper we detail the sorted neighborhood method that is used by some to solve merge/purge and present experimental results that demonstrates this approach may work well in practice but at great expense. An alternative method based upon clustering is also presented with a comparative evaluation to the sorted neighborhood method. We show a means of improving the accuracy of the results based upon a multi-pass approach that succeeds by computing the Transitive Clos...
288|Unexpectedness as a Measure of Interestingness in Knowledge Discovery|Organizations are taking advantage of &#034;data-mining&#034; techniques to leverage the vast amounts of data captured as they process routine transactions. Data-mining is the process of discovering hidden structure or patterns in data. However several of the pattern discovery methods in datamining systems have the drawbacks that they discover too many obvious or irrelevant patterns and that they do not leverage to a full extent valuable prior domain knowledge that managers have. This research addresses these drawbacks by developing ways to generate interesting patterns by incorporating managers&#039; prior knowledge in the process of searching for patterns in data. Specifically we focus on providing methods that generate unexpected patterns with respect to managerial intuition by eliciting managers&#039; beliefs about the domain and using these beliefs to seed the search for unexpected patterns in data. Our approach should lead to the development of decision support systems that provide managers with mor...
289|The interestingness of deviations|gps~gte.com, matheus~gte.com One of the moet promising areas in Knowledge Discovery in Databases is the automatic analysis of changes and deviations. Several systems have recently been developed for this task. Suc ~ of these systems hinges on their ability to identify s few important and relevant deviations among the multitude of potentially interesting events:  ~ In this paper we argue that related deviations should be grouped togetherin a finding and that the interestingness of a finding is the estimated benefit from a poesible ~tion connected to it. We discuss methods for determining the estimated benefit from the impact of the deviations and the success probability of an action. Our analysis is done in the context of the Key Findings Reporter (KEFIIt), a system for discovering and explaining ~key findings &#034; in large relational databases, currently being applied to the analysis of healthcare information.
290|The World Wide Web: quagmire or gold mine?|This article considers the question: is effective Web mining possible?
291|Discovering Informative Patterns and Data Cleaning|We present a method for discovering informative patterns from data. With this method, large databases can be reduced to only a few representative data entries. Our framework also encompasses methods for cleaning databases containing corrupted data. Both on-line and off-line algorithms are proposed and experimentally checked on databases of handwritten images. The generality of the framework makes it an attractive candidate for new applications in knowledge discovery.  Keywords: knowledge discovery, machine learning, informative patterns, data cleaning, information gain.  4.1 
292|Selecting Among Rules Induced from a Hurricane Database|can achieve orders of magnitude reduction in the volume of data For example, we applied a commercial tool (IXLtin) to a 1,819 record tropical storm database, yielding 161 rules. However, the human comprehension goals of Knowledge Discovery in Databases may require still more orders of magnitude. We present a rule refinement strategy, partly implemented in a Prolog program, that operationalizes &#034;interestingness &#034; into performance, simplicity, novelty, and significance. Applying the strategy to the induced rulebase yielded 10 &#034;genuinely interesting &#034; rules. I. PURPOSE OF THE STUDY At The Travelers Insurance Company, we are involved in applying statistics and artificial intelligence techniques to the solution of business problems. This work is part of an investigation into applications for Natural Hazards Research Services. The purpose of this study is not to deyelop a hurricane model or predictor. It is, rather, to assess the utility of rule induction technology and our particular rule refinement strategy. The object task of the study is to develop rules that
293|KDD for Science Data Analysis: Issues and Examples|Tile analysis of tile massive data sets collected by scientific instruments demands automation as a pre-requisite to analysis. There is an urgent need to cre-ate an intermediate level at which scientists can oper-ate effectively; isolating them from the massive sizes and harnessing human analysis capabilities to focus on tasks in which machines do not even renmtely ap-proach humans--namely, creative data analysis, the-ory and hypothesis formation, and drawing insights into underlying phe,mmena. We give an overview of the main issues in the exploitation of scientific data.sets, present five c,~se studies where KDD tools play important and enabling roles, and conclude with fi,ture challenges for data mining and KDD techniques in science data analysis. 1
294| 	 Active Data Mining   |We introduce an active data mining paradigm that combines the recent work in data mining with the rich literature on active database systems. In this paradigm, data is continuously mined at a desired frequency. As rules are discovered, they are added to a rulebase, and if they already exist, the history of the statistical parameters associated with the rules is updated. When the history starts exhibiting certain trends, specified as shape queries in the user-speci ed triggers, the triggers are red and appropriate actions are initiated. To be able to specify shape queries, we describe the constructs for de ning shapes, and discuss how the shape predicates are used in a query construct to retrieve rules whose histories exhibit the desired trends. We describe how this query capability is integrated into a trigger system to realize an active mining system. The system presented here has been validated using two sets of customer data.
295|Fast Spatio-Temporal Data Mining of Large Geophysical Datasets|The important scientific challenge of understanding global climate change is one that clearly requires the application of knowledge discovery and datamining techniques on a massive scale. Advances in parallel supercomputing technology, enabling high-resolution modeling, as well as in sensor technology, allowing data capture on an unprecedented scale, conspire to overwhelm present-day analysis approaches. We present here early experiences with a prototype exploratory data analysis environment, CONQUEST, designed to provide content-based access to such massive scientific datasets. CONQUEST (CONtent-based Querying in Space and Time) employs a combination of workstations and massively parallel processors (MPP&#039;s) to mine geophysical datasets possessing a prominent temporal component. It is designed to enable complex multi-modal interactive querying and knowledge discovery, while simultaneously coping with the extraordinary computational demands posed by the scope of the datasets involved. A...
296|Predicting Equity Returns from Securities Data|Our experiments with capital markets data suggest that the domain can be effectively modeled by classification rules induced from available historical data for the purpose of making gainful predictions for equityinvestments. New classification techniques developed at IBM Research, including minimal rule generation (R-MINI) and contextual feature analysis, seem robust enough for consistently extracting useful information from noisy domains such as financial markets. We will briefly introduce the rationale for our minimal rule generation technique, and the motivation for the use of contextual information in analyzing features. We will then describe our experience from several experiments with the S&amp;P 500 data, illustrating the general methodology, and the results of correlations and simulated managed investment based on classification rules generated by R-MINI. Wewillsketchhow the rules for classifications can be effectively used for numerical prediction, and eventually to an investment ...
297|Graphical Models for Discovering Knowledge|There are many different ways of representing knowledge, and for each of these ways there are many different discovery algorithms. How can we compare different representations? How can we mix, match and merge representations and algorithms on new problems with their own unique requirements? This chapter introduces probabilistic modeling as a philosophy for addressing these questions and presents graphical models for representing probabilistic models. Probabilistic graphical models are a unified qualitative and quantitative framework for representing and reasoning with probabilities and independencies. 4.1 Introduction Perhaps one common element of the discovery systems described in this and previous books on knowledge discovery is that they are all different. Since the class of discovery problems is a challenging one, we cannot write a single program to address all of knowledge discovery. The KEFIR discovery system applied to health care by Matheus, Piatetsky-Shapiro, and McNeill (199...
298|An Overview of Issues in Developing Industrial Data Mining and Knowledge Discovery Applications|This paper surveys the growing number of indu5 trial applications of data mining and knowledge discovery. We look at the existing tools, describe some representative applications, and discuss the major issues and problems for building and deploying successful applications and their adoption by business users. Finally, we examine how to assess the potential of a knowledge discovery application. 1
299|Analyzing the Benefits of Domain Knowledge in Substructure Discovery|Discovering repetitive, interesting, and functional substructures in a structural  database improves the ability to interpret and compress the data. However, scientists  working with a database in their area of expertise often search for a predetermined  type of structure, or for structures exhibiting characteristics specic to the domain.  This paper presents methods for guiding the discovery process with domain-specic  knowledge. In this paper, the Subdue discovery system is used to evaluate the bene-  ts of using domain knowledge. The domain knowledge is incorporated into Subdue  following a single general methodology to guide the discovery process. Results show  using domain-specic knowledge improves the search for substructures which are useful  to the domain, and leads to greater compression of the data. To illustrate these bene-  ts, examples and experiments from the domain of computer programming, computer  aided design circuit, and a series of articially-generated domains...
300|The Context Toolkit: Aiding the Development of Context-Enabled Applications|Context-enabled applications are just emerging and promise richer interaction by taking environmental context into account. However, they are difficult to build due to their distributed nature and the use of unconventional sensors. The concepts of toolkits and widget libraries in graphical user interfaces has been tremendously successtil, allowing programmers to leverage off existing building blocks to build interactive systems more easily. We introduce the concept of context widgets that mediate between the environment and the application in the same way graphical widgets mediate between the user and the application. We illustrate the concept of context widgets with the beginnings of a widget library we have developed for sensing presence, identity and activity of people and things. We assess the success of our approach with two example context-enabled applications we have built and an existing application to which we have added contextsensing capabilities.
301|The Active Badge Location System|cation is the `pager system&#039;. In order to locate a person a signal is sent out by a central facility that addresses a particular receiver unit (beeper) and produces an audible signal. In addition, it may display a number to which the called-party should phone back (some systems allow a vocal message to be conveyed about the call-back number). It is then up to the recipient to use the conventional telephone system to call-back confirming the signal and determine the required action. Although useful in practice there are still circumstances where it is not ideal. For instance, if the called party does not reply the controller has no idea if they: 1) are in an area where the signal does not penetrate 2) have been completely out of the area for some time 3) have been too busy to reply or 4) have misheard or misread the call-back number. Moreover, in the case where there are a number of people who could respond to a crisis situation, it is not known which one is the nearest to the crisis an
302|Grounding in communication|We give a general analysis of a class of pairs of positive self-adjoint operators A and B for which A + XB has a limit (in strong resolvent sense) as h-10 which is an operator A,  # A! Recently, Klauder [4] has discussed the following example: Let A be the operator-(d2/A2) + x2 on L2(R, dx) and let B = 1 x 1-s. The eigenvectors and eigenvalues of A are, of course, well known to be the Hermite functions, H,(x), n = 0, l,... and E,  = 2n + 1. Klauder then considers the eigenvectors of A + XB (A&gt; 0) by manipulations with the ordinary differential equation (we consider the domain questions, which Klauder ignores, below). He finds that the eigenvalues E,(X) and eigenvectors &amp;(A) do not converge to 8, and H, but rather AO) + (en 4 Ho+, J%(X)-+ gn+1 I n = 0, 2,..., We wish to discuss in detail the general phenomena which Klauder has uncovered. We freely use the techniques of quadratic forms and strong resolvent convergence; see e.g. [3], [5]. Once one decides to analyze Klauder’s phenomenon in the language of quadratic forms, the phenomenon is quite easy to understand and control. In fact, the theory is implicit in Kato’s book [3, VIII.31.
303|ContextAware Computing Applications|This paper describes systems thatel:amine and re-actto an indi7Jidltal&#039;s changing context. Such systems can promote and mediate people&#039;s mleractlOns with de-Vices, computers, and other people, and they can help navigate unfamiliar places. We bel1eve that a lunded amount of information coveTIng a per&#039;son&#039;s proximale environment is most important for this form of com-puting since the interesting part of the world around us is what we can see, hear, and touch. In this paper we define context-aware computing, and describe four cal-egones of conteL·t-aware applications: proximate selec-tion, automatic contextual reconfiguratlOn, contexlual information and commands, and context-triggered ac-tions. fnstances of these application types ha11e been prototyped on the PARCTAB, a wireless, palm-sl.:ed computer. 1
304|The ParcTab Ubiquitous Computing Experiment|... This paper describes the Ubiquitous Computing philosophy, the PARCTAB system, user-interface issues for small devices, and our experience developing and testing a variety of mobile applications.
305|&#034;Forget-me-not&#034;  Intimate Computing in Support of  Human Memory|  At RXRC we have been trying to understand how anticipated developments in mobile computing will impact our customers in the 21st century. One opportunity we can see is to improve computer-based support for human memory --- ironically a problem in office systems research that has almost been forgotten. Considering how often computers are presented as devices capable of &#034;memorising&#034; vast quantities of information, and performing difficult-tomemorise sequences of operations on our behalf, we might be surprised at how often they appear to have increased the load on our own memory. The Forget-me-not project is an attempt to explore new ways in which mobile and ubiquitous technologies might help alleviate the increasing load. Forget-me-not is a memory aid designed to help with everyday memory problems: finding a lost document, remembering somebody &#039;s name, recalling how to operate a piece of machinery. It exploits some well understood features of human episodic memory to provid
306|The Wearable Remembrance Agent: A System for Augmented Memory|This paper describes the wearable Remembrance Agent, a continuously running proactive memory aid that uses the physical context of a wearable computer to provide notes that might be relevant in that context. A currently running prototype is described, along with future directions for research inspired by using the prototype.  1 Introduction  With computer chips getting smaller and cheaper the day will soon come when the desk-top, lap-top, and palm-top computer will all disappear into a vest pocket, wallet, shoe, or anywhere else a spare centimeter or two are available. As the price continues to plummet, these devices will enable all kinds of applications, from consumer electronics to personal communicators to field-operations support. Given that the primary use of today&#039;s palm-top computers is as day-planners, address books, and notebooks, one can expect memory aids will be an important application for wearable computers as well. Current computer-based memory aids are written to make l...
307|The stick-e document: a framework for creating context-aware applications|There is increasing interest in computer applications that are aware of the user’s context. Currently these applications are normally hand-crafted. A lot of them consist of presenting information to users as they enter a given context, for example a tourist nearing a site within a city or a visitor moving round a building. The paper presents a new form of document, and the supporting software, which allows such applications to be created simply by building a new document. The motivation is to make the creation and use of these applications as easy as creating and using web pages. KEY WORDS Document Context-aware PDA HTML Stick-e 1
308|A System Architecture for Context-Aware Mobile Computing|Computer applications traditionally expect a static execution environment. However, this precondition is generally not possible for mobile systems, where the world around an application is constantly changing. This thesis explores how to support and also exploit the dynamic configurations and social settings characteristic of mobile systems. More specifically, it advances the following goals: (1) enabling seamless interaction across devices; (2) creating physical spaces that are responsive to users; and (3) and building applications that are aware of the context of their use. Examples of these goals are: continuing in your office a program started at home; using a PDA to control someone else&#039;s windowing UI; automatically canceling phone forwarding upon return to your office; having an airport overheaddisplay highlight the flight information viewers are likely to be interested in; easily locating and using the nearest printer or fax machine; and automatically turning off a PDA&#039;s audible e-mail notification when in a meeting.
309|The conference assistant: Combining context-awareness with wearable computing|We describe the Conference Assistant, a prototype mobile, context-aware application that assists conference attendees. We discuss the strong relationship between context-awareness and wearable computing and apply this relationship in the Conference Assistant. The application uses a wide variety of context and enhances user interactions with both the environment and other users. We describe how the application is used and the context-aware architecture on which it is based. 1.
310|A Context-Based Infrastructure for Smart Environments|. In order for a smart environment to provide services to its occupants, it must be able to detect its current state or context and determine what actions to take based on the context. We discuss the requirements for dealing with context in a smart environment and present a software infrastructure solution we have designed and implemented to help application designers build intelligent services and applications more easily. We describe the benefits of our infrastructure through applications that we have built. 1 Introduction  One of the goals of a smart environment is that it supports and enhances the abilities of its occupants in executing tasks. These tasks range from navigating through an unfamiliar space, to providing reminders for activities, to moving heavy objects for the elderly or disabled. In order to support the occupants, the smart environment must be able to both detect the current state or context in the environment and determine what actions to take based on this context...
311|A New Model for Handling Input|Although there has been important progress in models and packages for the output of graphics to computer screens, there has been little change in the way that input from the mouse, keyboard, and other input devices is handled. New graphics standards are still using a fifteen-year-old model even though it is widely accepted as inadequate, and most modern window managers simply return a stream of low-level, device-dependent input events. This paper presents a new model that handles input devices for highly interactive, direct manipulation, graphical user interfaces, which could be used in future toolkits, window managers, and graphics standards. This model encapsulates interactive behaviors into a few “Interactor ” object types. Application programs can then create instances of these Interactor objects which hide the details of the underlying window manager events. In addition, Interactors allow a clean separation between the input handling, the graphics, and the application programs. This model has been extensively used as part of the Garnet system and has proven to be convenient, efficient, and easy to learn.
312|Towards Situated Computing|Situated computing concerns the ability of computing devices to detect, interpret and respond to aspects of the user’s local environment. In this paper, we use our recent prototyping experience to identify a number of challenging issues that must be resolved in building wearable computers that support situated applications. The paper is organized into three areas: Sensing the local environment, interpreting sensor data, and realizing the value of situated applications. We conclude that while it is feasible to develop interesting prototypes, there remain many difficulties to overcome before robust systems may be widely deployed.
313|CyberDesk: a framework for providing self-integrating context-aware services|Applications are often designed to take advantage of the potential for integration with each other via shared information. Current approaches for integration are limited, affecting both the programmer and end-user. In this paper, we present CyberDesk, a framework for self-integrating software in which integration is driven by user context. It relieves the burden on programmers by removing the necessity to predict how software should be integrated. It also relieves the burden from users by removing the need to understand how to make different software components work together. # 1998 Elsevier Science B.V. All rights reserved.  Keywords: Context-aware computing; Automated software integration; Dynamic mediation; Ubiquitous computing  1. Introduction  Software applications often work on similar information types such as names, addresses, dates, and locations. Collections of applications are often designed to take advantage of the potential for integration via shared information. As an exa...
314|Supporting Capture and Access Interfaces for Informal and Opportunistic Meetings|Automated support for the capture and access of live experiences is a common theme for ubiquitous computing. For certain capture situations, such as informal or opportunistic gatherings, existing capture frameworks are inadequate for a number of reasons. They require too much time to initiate a capture session and they often are too inflexible to support unstructured and impromptu use. In this paper, we present a whiteboard capture application called DUMMBO, aimed to support opportunistic and serendipitous meeting capture. We emphasize a easy-toinitiate interface that mirrors as much as possible traditional whiteboard functionality. This is accompanied by visualization techniques for accessing captured meetings afterwards. By separating the physical interface for capture from the electronic interface for accessing captured meetings, we demonstrate how a capture and access application can be designed to better support its intended audience. Keywords Ubiquitous computing, automated capture and access
315|Progress of CMAP: A context-aware mobile assistant|An interface agent with a life-like character on a per-sonal mobile computer based guidance system is a plau-sible interface design approach for supporting and medi-ating communication between exhibitors and visitors of museums, laboratory tours and trade-shows. The guide agents should acquire their information ubiquitously and integrate this knowledge to provide context relevant as-sistant to visitors. C-MAP is a context-aware mobile assistant system that provides a mobile guide agent for assisting visitors during exhibitions. The system uses 30 portable computers, two servers, and an active badge location server. The portable computers are networked using a 1Mbit/sec wireless FM connection to the eth-ernet which the servers are connected to. Currently, the guide agent has a life-like animated character with four different behaviours, a physical map and a semantic map. The physical map tracks the visitor’s location rel-ative to the exhibits while the semantic map tracks the visitor’s interests relative to the exhibitions. The guide agent uses the visitor’s interest to plan a tour through the exhibition. A layered infrastructure is used to co-ordinate the guide agents and distribute their compu-tation. This paper presents progress made towards the implementation of C-MAP.
316|A survey of context-aware mobile computing research|Context-aware computing is a mobile computing paradigm in which applications can discover and take advantage of contextual information (such as user location, time of day, nearby people and devices, and user activity). Since it was proposed about a decade ago, many researchers have studied this topic and built several context-aware applications to demonstrate the usefulness of this new technology. Context-aware applications (or the system infrastructure to support them), however, have never been widely available to everyday users. In this survey of research on context-aware systems and applications, we looked in depth at the types of context used and models of context information, at systems that support collecting and disseminating context, and at applications that adapt to the changing context. Through this survey, it is clear that context-aware research is an old but rich area for research. The difficulties and possible solutions we outline serve as guidance for researchers hoping to make context-aware computing a reality. 1.
317|RADAR: an in-building RF-based user location and tracking system|The proliferation of mobile computing devices and local-area wireless networks has fostered a growing interest in location-aware systems and services. In this paper we present RADAR, a radio-frequency (RF) based system for locating and tracking users inside buildings. RADAR operates by recording and processing signal strength information at multiple base stations positioned to provide overlapping coverage in the area of interest. It employs techniques that combine empirical measurements with signal propagation modeling to enable location-aware services and applications. We present concrete experimental results that demonstrate the feasibility of using RADAR to estimate user location with a high degree of accuracy. 1
318|Towards a better understanding of context and context-awareness|Abstract. The use of context is important in interactive applications. It is particularly important for applications where the user’s context is changing rapidly, such as in both handheld and ubiquitous computing. In order to better understand how we can use context and facilitate the building of context-aware applications, we need to more fully understand what constitutes a contextaware application and what context is. Towards this goal, we have surveyed existing work in context-aware computing. In this paper, we provide an overview of the results of this survey and, in particular, definitions and categories of context and context-aware. We conclude with recommendations for how this better understanding of context inform a framework for the development of context-aware applications. 1
319|The Anatomy of a Context-Aware Application|We describe a platform for context-aware computing which enables applications to follow mobile users as they move around a building. The platform is particularly suitable for richly equipped, networked environments. The only item a user is required to carry is a small sensor tag, which identifies them to the system and locates them accurately in three dimensions. The platform builds a dynamic model of the environment using these location sensors and resource information gathered by telemetry software, and presents it in a form suitable for application programmers. Use of the platform is illustrated through a practical example, which allows a user&#039;s current working desktop to follow them as they move around the environment.
320|A New Location Technique for the Active Office|Configuration of the computing and communications systems found at home and in the workplace is a complex task that currently requires the  attention of the user. Recently, researchers have begun to examine computers that would autonomously change their functionality based on  observations of who or what was around them. By determining their context, using input from sensor systems distributed throughout the  environment, computing devices could personalize themselves to their current user, adapt their behavior according to their location, or react  to their surroundings. The authors present a novel sensor system, suitable for large-scale deployment in indoor environments, which allows the  locations of people and equipment to be accurately determined. We also describe some of the context-aware applications that might make use  of this fine-grained location information.
321|Agile Application-Aware Adaptation for Mobility|In this paper we show that application-aware adaptation, a collaborative partnership between the operating system and applications, offers the most general and effective approach to mobile information access. We describe the design of Odyssey, a prototype implementing this approach, and show how it supports concurrent execution of diverse mobile applications. We identify agility as a key attribute of adaptive systems, and describe how to quantify and measure it. We present the results of our evaluation of Odyssey, indicating performance improvements up to a factor of 5 on a benchmark of three applications concurrently using remote services over a network with highly variable bandwidth.  
322|The Smart Floor: A Mechanism for Natural User Identification and Tracking|We have created a system for identifying people based on their footstep force profiles and have tested its accuracy against a large pool of footstep data. This floor system may be used to transparently identify users in their everyday living and working environments. We have created user footstep models based on footstep profile features and have been able to achieve a recognition rate of 93 % using this feature-based approach. We have also shown that the effect of footwear is negligible on recognition accuracy.
323|Location-aware information delivery with comMotion|This paper appears in the HUC 2000 Proceedings, pp.157-171, Springer-Verlag
324|Rapid Prototyping of Mobile Context-Aware Applications: The Cyberguide Case Study|We present the Cyberguide project, in which we are building prototypes of a mobile context-aware tour guide that provide information to a tourist based on knowledge of position and orientation. We describe features of existing Cyberguide prototypes and discuss research issues that have emerged in our context-aware applications development in a mobile environment. Keywords: Mobile computing applications, contextawareness, location-dependent applications, hand-held devices 1 Introduction  The project we report on in this paper, Cyberguide, has as its main focus the rapid prototyping of handheld mobile applications in order to assess the utility of context-awareness in mobile devices. The challenge we are addressing in Cyberguide is how to build mobile applications that make use of the context of the user. Initially, we are concerned with only a small part of the user&#039;s context, specifically location and orientation. The application which drives the development of Cyberguide is a that of ...
326|Supporting Location-Awareness in Open Distributed Systems|Mobile computers and communication devices are establishing themselves as ubiquitous features of daily life. This development is linked to tremendous growth in the number and sophistication of mobile and mobile-aware software applications. Increasingly, such applications need access to information about their own and other objects&#039; physical locations, a requirement known as location-awareness.
327|Experience with Disconnected Operation in a Mobile Computing Environment|In this paper we present qualitative and quantitative data on file access in a mobile computing environment. This information is based on actual usage experience with the Coda File System over a period of about two years. Our experience confirms the viability and effectiveness of disconnected operation. It also exposes certain deficiencies of the current implementation of Coda, and identifies new functionality that would enhance its usefulness for mobile computing. The paper concludes with a description of what we are doing to address these issues. This work was supported by the Advanced Research Projects Agency (Avionics Laboratory, Wright Research and Development Center, Aeronautical Systems Division(AFSC), U.S. Air Force, Wright-Patterson AFB under Contract F33615-90-C-1465, Arpa Order No. 7597), the National Science Foundation (Grant ECD 8907068), IBM, Digital Equipment Corporation, and Bellcore. The views and conclusions contained in this document are those of the authors and sho...
328|The Stick-e Note Architecture: extending the interface beyond the user|This paper proposes a redefinition of the human-computer interface, extending its boundaries to encompass interaction with the user’s physical environment. This extension to the interface enables computers to become aware of their context of use and intelligently adapt their activities and interface to suit their current circumstances. Context-awareness promises to greatly enhance user interfaces, but the complexity of capturing, representing and processing contextual data, presents a major obstacle to its further development. The Stick-e Note Architecture is proposed as a solution to this problem, offering a universal means of providing context-awareness through an easily understood metaphor based on the Post-It note. Keywords context-aware computing, stick-e note architecture, mobile computing, ubiquitous computing, situated information spaces.
329|MOBISAIC: An Information System for A Mobile Wireless Computing Environment|Mobisaic is a World Wide Web information system designed to serve users in a mobile wireless computing environment. Mobisaic extends the Web by allowing documents to both refer and react to potentially changing contextual information, such as current location in the wireless network. Mobisaic relies on clientside processing of HTML documents that support two new concepts: Dynamic Uniform Resource Locators (URLs) and Active Documents. A dynamic URL is one whose results depend upon the state of the user&#039;s mobile context at the time it is resolved. An active document is one that automatically updates its contents in response to changes in a user&#039;s mobile context. This paper describes the design of Mobisaic, the mechanism it uses for representing a user&#039;s mobile context, and the extensions made to the syntax and function of Uniform Resource Locators and HyperText Markup Language documents to support mobility. 
330|An indoor wireless system for personalized shopping assistance|By integrating wireless, video, speech and real-time data access technologies, a unique shopping assistant service can be created that personalizes the attention provided to a customer based on individual needs, without limiting his movement, or causing distractions for others in the shopping center. We have developed this idea into a service based on two products: a very high volume hand-held wireless communications device, the PSA (Personal Shopping Assistant), that the customer owns (or may be provided to a customer by the retailer), and a centralized server located in the shopping center to which the customer communicates using the PSA. The centralized server maintains the customer database, the store database and provides audio/visual responses to inquiries from tens to hundreds of customers in real-time over a small area wireless network. 1.
331|Customizing Mobile Applications|The dynamics of mobile systems require applications to intelligently adapt to changes in system configurations and to their environment. We describe a workplace in which users interact with a number of stationary and mobile systems through the course of a day. The relationship between systems and devices is constantly changing due to user mobility. We present a facility for mobile application customization called &#034;dynamic environment variables.&#034; The facility allows flexible sharing of customization contexts, supports short interactions with long term services, and provides efficient notification of environment changes to applications. A sample application built in PARC&#039;s mobile computing environment and initial performance evaluations are described. 1 Introduction Mobile computing differs from desktop computing because of the dynamic nature of system state. In our research lab mobile users interact with mobile computers as well as a world of stationary and embedded systems [8]. As use...
332|Keeping Up With The Changing Web|Our access to information today is unprecedented in history. However, information depreciates in  value as it gets older, and the problem of updating information to keep it current presents new design  challenges for information providers and consumers. These issues lead to novel concepts and results in  the context of the World Wide Web. We quantify what it means to for search engines to be \up-to-date&#034;  and estimate how often search engines must re-index the web to keep current with it changing pages and  structure.  Three weeks prior to the Soviet invasion of Czechoslovakia, Corona satellite imagery of the area showed no signs of imminent attack. By the time another round of imagery was available, it was too late to react; the invasion had already taken place. In a real sense, the information obtained by the satellite weeks earlier was no longer useful. The fact that information has a useful lifetime is well known in the intelligence community. On the other side of the Iron Curtain,...
334|Providing Architectural Support for Context-Aware applications|Context is an important, yet poorly utilized source of information in interactive computing. It is difficult to use because, unlike other forms of user input, there is no common, reusable way to handle context. Most context-aware applications have been built in an ad hoc manner. We discuss the requirements for dealing with context and present an architectural solution we have designed and implemented to help application designers build context-aware applications more easily. We illustrate the use of the architecture through a context-aware application that assists conference attendees.
335|Teleporting - Making Applications Mobile|The rapid emergence of mobile computers as a popular, and increasingly powerful, computing tool is presenting new challenges. This subject is already being widely addressed within the computing literature. A complementary and relatively unexplored notion of mobility is one in which application interfaces, rather than the computer on which the applications run, are able to move. The Teleporting System developed at the Olivetti Research Laboratory (ORL) is a tool for experiencing such `mobile applications&#039;. It operates within the X Window System  1  , and allows users to interact with their existing X applications at any X display within a building. The process of controlling the interface to the teleporting system is very simple. This simplicity comes from the use of an automatically maintained database of the location of equipment and people within the building. This paper describes the teleporting system, what it does, and how it is used. We outline some of the issues of making applic...
336|A System for Tracking and Recognizing Multiple People with Multiple Cameras|In this paper we present a robust real-time method for tracking and recognizing multiple people with multiple cameras. Our method uses both static and Pan-Tilt-Zoom (PTZ) cameras to provide visual attention. The PTZ camera system uses face recognition to register people in the scene and &#034;lock-on&#034; to those individuals. The static camera system provides a global view of the environment and is used to re-adjust the tracking of the system when the PTZ cameras lose their targets. The system works well even when people occlude one another. The underlying visual processes rely on color segmentation, movement tracking and shape information to locate target candidates. Color indexing and face recognition modules help register these candidates with the system. 1. Introduction  One of the goals of building an intelligent environment is to make it more aware of the user&#039;s presence so that the interface can seek out and serve the user [1]. This work addresses the ability of a system to determine th...
337|Scalable and Flexible Location-Based Services for Ubiquitous Information Access|Abstract. In mobile distributed environments applications often need to dynamically obtain information that is relevant to their current loca-tion. The current design of the Internet does not provide any conceptual models for addressing this issue. As a result, developing a system that requires this functionality becomes a challenging and costly task, lead-ing to individual solutions that only address the requirements of specic application scenarios. In this paper we propose a more generic approach, based on a scalable and 
exible concept of location-based services, and an architectural framework to support its application in the Internet envi-ronment. We describe a case study in which this architectural framework is used for developing a location-sensitive tourist guide. The realisation of this case study demonstrates the applicability of the framework, as well as the overall concept of location-based services, and highlights some of the issues involved. 1
338|Context-aware office assistant|This paper describes the design and implementation of the Office Assistant- an agent that interacts with visitors at the office door and manages the office owner’s schedule. We claim that rich context information about users is key to making a flexible and believable interaction. We also argue that natural face-to-face conversation is an appropriate metaphor for human-computer interaction.
339|Adaptive Support for a Mobile Museum Guide|The paper aims at supporting human activities with
340|Integration of location services in the open distributed office|There has recently been much interest in location systems which enable people and equipment to be tracked as they move within and across buildings  Thus far  such sys tems have been used in isolation with the result that  although there are often several sources of location information available at any one site  users have to consult each sys tem individually  Additionally  it is dicult for services requiring location information to take advantage of all these sources In this paper  we put forward the notion of a master location system to coordinate the location process so that available sources of information are used automatically in as ecient a manner as possible  We discuss a number of factors that are of concern to the design of such a system  and describe a particular implementation which we worked on as part of an oce automation project There has recently been much interest in location systems which enable people and equip ment to be tracked as they move within and across buildings  Perhaps one of the more
341|An Overview of Workflow Management: From Process Modeling to Workflow Automation Infrastructure|  Today’s business enterprises must deal with global competition, reduce the cost of doing business, and rapidly develop new services and products. To address these requirements enterprises must constantly reconsider and optimize the way they do business and change their information systems and applications to support evolving business processes. Workflow technology facilitates these by providing methodologies and software to support (i) business process modeling to capture business processes as workflow specifications, (ii) business process reengineering to optimize specified processes, and (iii) workflow automation to generate workflow implementations from workflow specifications. This paper provides a high-level overview of the current workflow management methodologies and software products. In addition, we discuss the infrastructure technologies that can address the limitations of current commercial workflow technology and extend the scope and mission of workflow management systems to support increased workflow automation in complex real-world environments involving heterogeneous, autonomous, and distributed information systems. In particular, we discuss how distributed object management and customized transaction management can support further advances in the commercial state of the art in this area.
342|The Action Workflow approach to workflow management technology|This paper describes ActionWorkflowTM approach to workflow management technology: a design methodology and associated computer software for the support of work in organizations. The approach is based on theories of communicative activity as language faction and has been developed in a series of systems for coordination among users of networked computers. This paper describes the approach, gives an example of its application, and shows the architecture of a workflow management system based on it.
343|Managing Heterogeneous Multi-system Tasks to Support Enterprise-wide Operations|. The computing environment in most medium-sized and large enterprises involves old main-frame based (legacy) applications and systems as well as new workstation-based distributed computing systems. The objective of the METEOR project is to support multi-system workflow applications that automate enterprise operations. This paper deals with the modeling and specification of workflows in such applications. Tasks in our heterogeneous environment can be submitted through different types of interfaces on different processing entities. We first present a computational model for workflows that captures the behavior of both transactional and nontransactional tasks of different types. We then develop two languages for specifying a workflow at different levels of abstraction: the Workflow Specification Language (WFSL) is a declarative rulebased language used to express the application-level interactions between multiple tasks, while the Task Specification Language (TSL) focuses on the issues re...
344|Specification and Execution of Transactional Workflows|The basic transaction model has evolved over time to incorporate more complex transaction structures and to selectively modify the atomicity and isolation properties. In this chapter we discuss the application of transaction concepts to activities that involve coordinated execution of multiple tasks (possibly of different types) over different processing entities. Such applications are referred to as transactional workflows. In this chapter we discuss the specification of such workflows and the issues involved in their execution.  
345|Merging Application-centric and Data-centric Approaches to Support Transaction-oriented Multi-system Workflows|Workflow management is primarily concerned with dependencies between the tasks of a workflow, to ensure correct control flow and data flow. Transaction management, on the other hand, is concerned with preserving data dependencies by preventing execution of conflicting operations from multiple, concurrently executing tasks or transactions. In this paper we argue that many applications will be served better if the properties of transaction and workflow models are supported by an integrated architecture. We also present preliminary ideas towards such an architecture. 
346|Business process management with FlowMark|From an enterprise point of view the management of business processes is becoming increasingly important: Business processes control which piece of work will be performed by whom and which resources are exploited for this work, i.e. a business process describes how an enterprise will achieve its business goals. In this paper we sketch FlowMark (FlowMark is a trademark of IBM), an IBM program product supporting both, the modeling of business processes and their execution.
347|Using flexible transactions to support multi-system telecommunication applications|Service order provisioning is an important telecommunication application that automates the process of providing telephone services in response to the customer requests. It is an example of a multi-system application that requires access to multiple, independently developed application systems and their databases. In this paper, we describe the design and implementation of a prototype system 1 that supports the execution of the Flexible Transactions and its use to develop the service order provisioning application. We argue that such approach may be used to support the development of multi-system, flow-through processing applications in a systematic and organized manner. Its advantages include fast and easy specification of new services, support for testing of the declaratively specified work-flows, and the specification of potential concurrency among the tasks constituting an application.
348|Distributed Object Management|Future information processing environments will consist of a vast network of heterogeneous, autonomous, and distributed computing resources, including computers (from mainframe to personal), information-intensive applications, and data (files and databases). A key challenge in this environment is providing capabilities for combining this varied collection of resources into an integrated distributed system, allowing resources to be flexibly combined, and their activities coordinated, to address challenging new information processing requirements. In this paper, we describe the concept of distributed object management, and identify its role in the development of these open, interoperable systems. We identify the key aspects of system architectures supporting distributed object management, and describe specific elements of a distributed object management system being developed at GTE Laboratories. 1. Introduction Today, computer usage is expanding into all parts, and all functions, of lar...
349|A Framework For Enforceable Specification Of Extended Transaction Models And Transactional Workflows|A variety of extensions to the traditional (ACID) transaction model have resulted in a plethora of  extended transaction models (ETMs). Many of these ETMs are application-specific, i.e., they are  designed to provide correctness guarantees adequate for a particular application, but not others. Similarly,  an application-specific ETM may impose restrictions that are unacceptable in one application,  yet required in another. To define new ETMs, to determine whether an ETM is appropriate for an  application, and to integrate ETMs to produce new ETMs, we need a framework for ETM specification  and reasoning. In this paper, we describe such a framework. Our framework supports implementation-independent specification of ETMs described in terms of dependencies between transactions.  Dependencies are specified using dependency descriptors. Unlike other transaction specification  frameworks, dependency descriptors use a common set of primitives, and are enforceable, i.e., can be  evaluated at...
350|Chronological Scheduling of Transactions with Temporal Dependencies|Database applications often impose temporal dependencies between transactions that must be satisfied to preserve data consistency. The extant correctness criteria used to schedule the execution of concurrent transactions are either time independent or use strict, difficult to satisfy real-time constraints. On one end of the spectrum, serializability completely ignores time. On the other end, deadline scheduling approaches consider the outcome of each transaction execution correct only if the transaction meets its real-time deadline. In this article, we explore new correctness criteria and scheduling methods that capture temporal transaction dependencies and belong to the broad area between these two extreme approaches. We introduce the concepts of succession dependency and chronological dependency and define correctness criteria under which temporal dependencies between transactions are preserved even if the dependent transactions execute concurrently. We also propose a chronological scheduler that can guarantee that transaction executions satisfy their chronological constraints. The advantages of chronological scheduling over traditional scheduling methods, as well as the main issues in the implementation and performance of the proposed scheduler, are discussed.
351|Modeling Strategic Relationships for Process Reengineering|Existing models for describing a process (such as a business process or a software development process) tend to focus on the \what &#034; or the \how &#034; of the process. For example, a health insurance claim process would typically be described in terms of a number of steps for assessing and approving a claim. In trying to improve orredesign a process, however, one also needs to have an understanding of the \why &#034;  { for example, why dophysicians submit treatment plans to insurance companies before giving treatment? and why do claims managers seek medical opinions when assessing treatment plans? An understanding of the motivations and interests of process participants is often crucial to the successful redesign of processes. This thesis proposes a modelling framework i (pronounced i-star) consisting of two modelling components. The Strategic Dependency (SD) model describes a process in terms of intentional dependency relationships among agents. Agents depend on each other for goals to be achieved, tasks to be performed, and resources to be furnished. Agents are intentional in that they have desires and wants, and strategic in that they are concerned about opportunities and vulnerabilities. The Strategic Rationale (SR) model describes the issues and concerns that agents
352|A Field Study of the Software Design Process for Large Systems|The problems of designing large software systems were studied through interviewing personnel from 17 large projects. A layered behavioral model is used to analyze how three lgf these problems-the thin spread of application domain knowledge, fluctuating and conflicting requirements, and communication bottlenecks and breakdowns-affected software productivity and quality through their impact on cognitive, social, and organizational processes.
353|Value-Based Software Engineering|Abstract—This paper provides a definition of the term “software engineering ” and a survey of the current state of the art and likely future trends in the field. The survey covers the technology available in the various phases of the software life cycle—requirements engineering, design, coding, test, and maintenance—and in the overall area of software management and integrated technology-management approaches. It is oriented primarily toward discussing the domain of applicability of techniques (where and when they work), rather than how they work in detail. To cover the latter, an extensive set of 104 references is provided. Index Terms—Computer software, data systems, information systems,
354|Telos: Representing Knowledge About Information Systems|This paper describes a language that is intended to support software engineers in the development of information systems throughout the software lifecycle. This language is not a programming language. Following the example of a number of other software engineering projects, our work is based on the premise that information system development is knowledge-intensive and that the primary responsibility of any language intended to support this task is to be able to formally represent the relevant knowledge.
355|Knowledge Representation and Reasoning in the Design of Composite Systems|Abstract- Our interest is in the design process that spans the gap between the requirements acquisition process and the implementation process, in which the basic architecture of a system is defined, and functions are allocated to software, hard-ware, and human agents. We call this process composite system design. Our goal is an interactive model of composite system design incorporating deficiency-driven design, formal analysis, incremental design and rationalization, and design reuse. We discuss knowledge representations and reasoning techniques for the product (composite system) that we are designing, and for the design process, which support these goals. To evaluate our model, we report on its use to rationally reconstruct the design of two existing composite systems. Index Terms-Automated analysis, composite systems, knowl-edge-based design, rational reconstruction, software specification. 0
356|Characterizing and Assessing a Large-Scale Software Maintenance Organization|One important component of a software process is the organizational context in which the process is enacted. This component is often missing or incomplete in current process modeling approaches. One technique for modeling this perspective is the Actor-Dependency (AD) Model. This paper reports on a case study which used this approach to analyze and assess a large software maintenance organization. Our goal was to identify the approach&#039;s strengths and weaknesses while providing practical recommendations for improvement. The AD model was found to be very useful in capturing the important properties of the organizational context of the maintenance process, and aided in the understanding of the flaws found in this process. However, a number of opportunities for extending and improving the AD model were identified. Among others, there is a need to incorporate quantitative information to complement the qualitative model. 1. Introduction It has now been recognized that, in order to improve the...
357|Goal-Based Process Analysis: A Method for Systematic Process Redesign|A method is proposed for systematically analyzing and redesigning processes. The method, Goal-based Process Analysis (GPA), helps its user to systematically identify missing objectives, ensure implementation of all the objectives, identify non-functional parts of a process, and explore alternative processes for achieving a given set of objectives. As such, GPA addresses a critical component in process reengineering, that of identifying which part of a given process needs to be improved and what alternatives could be used instead.  Keywords  Process Redesign, Process Analysis, Goal Analysis, Work Flow Design, Organizational Design  1. INTRODUCTION  Critical in process reengineering is some way of identifying what needs to be redesigned as well as understanding what alternatives we have. This paper proposes a method, Goal-based Process Analysis (GPA), that provide a systematic way to: . identify missing goals . ensure implementation of all the goals . identify non-functional parts of a p...
358|Representation and Utilization of Non-Functional Requirements for Information System Design|The complexity and usefulness of large information systems are determined partly by their functionality, i.e., what they do, and partly by global constraints on their accuracy, security, cost, user-friendliness, performance, and the like. Even with the growing interest in developing higher-level models and design paradigms, current technology is inadequate both representationally for expressing such global constraints as formal non-functional requirements and methodologically for utilizing them in generating designs. We propose both a representational and methodological framework for non-functional requirements, focusing on accuracy requirements. With the premise that accuracy is an inherent semantic attribute of information, we take a first step towards establishing a representational basis for accuracy. To guide the design process and justify design decisions, we propose a goal-oriented methodology. In the methodology, accuracy requirements are treated as (potentially conflicting) go...
359|Using Quality Requirements To Systematically Develop Quality Software|. Although quality issues such as accuracy, security, and performance are often crucial to the success of a software system, there has been no systematic way to achieve quality requirements during system development. We offer a framework and an implemented tool which treat quality requirements as goals to be achieved systematically during the system development process. We illustrate the process that a developer would go through, in building quality into a system. We have tested the framework on a number of studies involving a variety of quality requirements, organisational settings, and system types. Keywords: non-functional requirements, accuracy, security, performance, information systems, process, software quality, defect detection, conflicts. 1 Problem  Software development is traditionally driven by functional requirements, i.e., the desired functionality of the system. For example, a credit card system should debit and credit accounts, check credit limits, charge interest, issue...
361|Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features|structure
362|Predicting the secondary structure of globular proteins using neural networks models|We present a new method for predicting the secondary structure of globular proteins based on non-linear neural network models. Network models learn from existing protein structures how to predict the secondary structure of local sequences of amino acids. The average success rate of our method on a testing set of proteins non-homologous with the corresponding training set was 643 % on three types of secondary structure (u-helix, b-sheet, and coil), with correlation coefficients of C,=O-41, C,=O*31 and CcO,, =0*41. These quality indices are all higher than those of previous methods. The prediction accuracy for the first 25 residues of the N-terminal sequence was significantly better. We conclude from computational experiments on real and artificial structures that no method based solely on local information in the protein sequence is likely to produce significantly better results for non-homologous proteins. The performance of our method of homologous proteins is much better than for non-homologous proteins, but is not as good as simply assuming that homologous sequences have identical structures. 1.
363|Conservation analysis and structure prediction of the SH2 family of phosphotyrosine binding domains. FEBS Lett|binding domains
364|Towards flexible teamwork|Many AI researchers are today striving to build agent teams for complex, dynamic multi-agent domains, with intended applications in arenas such as education, training, entertainment, information integration, and collective robotics. Unfortunately, uncertainties in these complex, dynamic domains obstruct coherent teamwork. In particular, team members often encounter differing, incomplete, and possibly inconsistent views of their environment. Furthermore, team members can unexpectedly fail in fulfilling responsibilities or discover unexpected opportunities. Highly flexible coordination and communication is key in addressing such uncertainties. Simply tting individual agents with precomputed coordination plans will not do, for their in flexibility can cause severe failures in teamwork, and their domain-specificity hinders reusability. Our central hypothesis is that the key to such flexibility and reusability isproviding agents with general models of teamwork. Agents exploit such models to autonomously reason about coordination and communication, providing requisite flexibility. Furthermore, the models enable reuse across domains, both saving implementation effort and enforcing consistency. This article presents one general, implemented model of teamwork, called STEAM. The basic building block of teamwork in STEAM is joint intentions (Cohen &amp; Levesque, 1991b); teamwork in STEAM is based on agents&#039; building up a (partial) hierarchy of joint intentions (this hierarchy is seen to parallel Grosz &amp; Kraus&#039;s partial Shared-Plans, 1996). Furthermore, in STEAM, team members monitor the team&#039;s and individual members&#039; performance, reorganizing the team as necessary. Finally, decision-theoretic communication selectivity in STEAM ensures reduction in communication overheads of teamwork, with appropriate sensitivity to the environmental conditions. This article describes STEAM&#039;s application in three different complex domains, and presents detailed empirical results.  
365|Knowledge and Common Knowledge in a Distributed Environment|: Reasoning about knowledge seems to play a fundamental role in distributed systems. Indeed, such reasoning is a central part of the informal intuitive arguments used in the design of distributed protocols. Communication in a distributed system can be viewed as the act of transforming the system&#039;s state of knowledge. This paper presents a general framework for formalizing and reasoning about knowledge in distributed systems. We argue that states of knowledge of groups of processors are useful concepts for the design and analysis of distributed protocols. In particular, distributed knowledge  corresponds to knowledge that is &#034;distributed&#034; among the members of the group, while  common knowledge corresponds to a fact being &#034;publicly known&#034;. The relationship between common knowledge and a variety of desirable actions in a distributed system is illustrated. Furthermore, it is shown that, formally speaking, in practical systems common knowledge cannot be attained. A number of weaker variants...
366|Controlling Cooperative Problem Solving in Industrial Multi-Agent Systems using Joint Intentions|One reason why Distributed AI (DAI) technology has been deployed in relatively few real-size applications is that it lacks a clear and implementable model of cooperative problem solving which specifies how agents should operate and interact in complex, dynamic and unpredictable environments. As a consequence of the experience gained whilst building a number of DAI systems for industrial applications, a new principled model of cooperation has been developed. This model, called Joint Responsibility, has the notion of joint intentions at its core. It specifies pre-conditions which must be attained before collaboration can commence and prescribes how individuals should behave both when joint activity is progressing satisfactorily and also when it runs into difficulty. The theoretical model has been used to guide the implementation of a general-purpose cooperation framework and the qualitative and quantitative benefits of this implementation have been assessed through a series of comparativ...
367|RoboCup: The Robot World Cup Initiative|The Robot World Cup Initiative (RoboCup) is an attempt to foster AI and intelligent robotics research by providing a standard problem where wide range of technologies can be integrated and examined. In order for a robot team to actually perform a soccer game, various technologies must be incorporated including: design principles of autonomous agents, multiagent collaboration, strategy acquisition, realtime reasoning, robotics, and sensor-fusion. Unlike AAAI robot competition, which is tuned for a single heavy-duty slow-moving robot, RoboCup is a task for a team of multiple fastmoving robots under a dynamic environment. Although RoboCup&#039;s final target is a world cup with real robots, RoboCup offers a software platform for research on the software aspects of RoboCup. This paper describes technical challenges involved in RoboCup, rules, and simulation environment.  1 Introduction: RoboCup as a Standard AI Problem  We propose a Robot World Cup (RoboCup), as a new standard problem for AI an...
368|Commitments and conventions: The foundation of coordination in multi-agent systems|Distributed Artificial Intelligence systems, in which multiple agents interact to improve their individual performance and to enhance the system’s overall utility, are becoming an increasingly pervasive means of conceptualising a diverse range of applications. As the discipline matures, researchers are beginning to strive for the underlying theories and principles which guide the central processes of coordination and cooperation. Here agent communities are modelled using a distributed goal search formalism and it is argued that commitments (pledges to undertake a specified course of action) and conventions (means of monitoring commitments in changing circumstances) are the foundation of coordination in multi-agent systems. An analysis of existing coordination models which use concepts akin to commitments and conventions is undertaken before a new unifying framework is presented. Finally a number of prominent coordination techniques which do not explicitly involve commitments or conventions are reformulated in these terms to demonstrate their compliance with the central hypothesis of this paper. 1
369|Designing a Family of Coordination Algorithms|Many researchers have shown that there is no single best organization or coordination mechanism  for all environments. This paper discusses the design and implementation of an extendable  family of coordination mechanisms, called Generalized Partial Global Planning (GPGP). The set  of coordination mechanisms described here assists in scheduling activities for teams of cooperative  computational agents. The GPGP approach has several unique features. First, it is not tied to  a single domain. Each mechanism is defined as a response to certain features in the current task  environment. We show that different combinations of mechanisms are appropriate for different  task environments. Secondly, the approach works in conjunction with an agent&#039;s existing local  planner/scheduler. Finally, the initial set of five mechanisms presented here generalizes and extends  the Partial Global Planning (PGP) algorithm. In comparison to PGP, GPGP allows more  agent heterogeneity, it exchanges less global ...
370|Using Collaborative Plans to Model the Intentional Structure of Discourse|An agent&#039;s ability to understand an utterance depends upon its ability to relate that utterance to the preceding discourse. The agent must determine whether the utterance begins a new segment of the discourse, completes the current segment, or contributes to it. The intentional structure of the discourse, comprised of discourse segment purposes and their interrelationships, plays a central role in this process (Grosz and Sidner, 1986). In this thesis, we provide a computational model for recognizing intentional structure and utilizing it in discourse processing. The model specifies how an agent&#039;s beliefs about the intentions underlying a discourse affects and are affected by its subsequent discourse. We characterize this process for both interpretation and generation and then provide specific algorithms for modeling the interpretation process. The collaborative planning framework of SharedPlans (Lochbaum, Grosz, and Sidner, 1990; Grosz and Kraus, 1993) provides the basis for our model ...
371|Intelligent Agents for Interactive Simulation Environments|cockpit  interface  Abstract  cockpit  interface  Abstract  cockpit  interface  Abstract  cockpit  interface  Figure 1: Human and automated pilots interact with the DIS environment via  distributed simulators.
372|Partial global planning: A coordination framework for distributed hypothesis formation|Abstruct-For distributed sensor network applications, a practical approach to generating complete interpretations from distributed data must coordinate how separate, concurrently running systems form, exchange, and fuse their individual hypotheses to form consistent interpretations. Partial global planning provides a framework for coordinating multiple AI systems that are cooperating in a distributed sensor network. By combining a variety of coordination techniques into a single, unifying framework, partial global planning enables separate AI systems to reason about their roles and responsibilities as part of group problem solving, and to modify their planned processing and communication actions to act as a more coherent team. Partial global planning is uniquely suited for coordinating systems that are working in continuous, dynamic, and unpredictable domains because it interleaves coordination with action and allows systems to make effective decisions despite incomplete and possibly obsolete information about network activity. The authors have implemented and extensively evaluated partial global planning in a simulated vehicle monitoring application, and have identified promising extensions to their framework I.
373|The Uses Of Plans|this paper, I will argue that, contrary to these challenges, planning deserves its central place on the AI map. I will claim that intelligent agents are planning agents, and that philosophical and commonsense psychological theorizing about the process of planning can provide useful insights into the question of agent design. The theories I have in mind are not restricted to  The Uses of Plans 3 how agents can form plans. Much of my research has concerned the ways in which intelligent agents use their plans. I will describe some of that research, and will argue that plans are used not only to guide action, but also to control reasoning and to enable inter-agent coordination. These uses of plans make possible intelligent behavior in complex, dynamic, multiagent environments. 2 Planning We can begin by asking what exactly we mean by &#034;planning&#034;. For many years, planning had a quite specific meaning in AI: it was the process of formulating a program of action to achieve some specified goal. You gave a planning system a description of initial conditions and a goal, and it produced a plan of action whose execution in a state satisfying the initial conditions was guaranteed to result in a state satisfying the goal. These plans were akin to recipes for achieving the goal. Your goal might be to have a chocolate cake. In the initial state, you might have eggs, milk, and chocolate, a pan and a working oven. In these conditions, a valid plan might be to go the store to buy some flour, return home, preheat the oven, mix the ingredients, pour the mixture into the pan, and put it in the oven for 45 minutes. Traditional AI planning systems like STRIPS [22], NOAH [63], and SIPE [71], were designed to construct just this kind of plan---except usually the goal was something like a tower o...
374|An artificial discourse language for collaborative negotiation|Collaborations to accomplish common goals necessi-tate negotiation to share and reach agreement on the beliefs that agents hold as part of the collaboration. Negotiation in communication can be simulated by a series of exchanges in which agents propose, reject, counterpropose or seek supporting information for be-liefs they wish to be held mutually. In an artificial language of negotiation, messages display the state of the agents ’ beliefs. Dialogues consisting of such mes-sages clarify the means by which agents come to agree or fail to agree on mutual beliefs and individual inten-tions.
375|Collagen: When agents collaborate with people|We take the position that autonomous agents, when they interact with people, should be governed by the same principles that underlie human collaboration. These principles come from research in computational linguistics, specifically collaborative discourse theory, which describes how people communicate and coordinate their activities in the context of shared tasks. We have implemented a prototype toolkit, called Collagen, which embodies collaborative discourse principles, and used it to build a collaborative interface agent for a simple air travel application. The potential benefits of this approach include application-independence, naturalness of use, and ease of learning, without requiring natural language understanding by the agent. Superseded by TR97-21.
376|Toward a Semantics for an Agent Communications Language Based on Speech-Acts|Implementations of systems based on distributed agent architectures require an  agent communications language that has a clearly de#ned semantics. Without one,  neither agents nor developers can be sure what another agent&#039;s commitment to perform  a task means #to name just one speech act#. This paper demonstrates that a semantics  for an agent communications language can be founded on the premise that interagent  communications constitute a task-oriented dialogue in which agents are building,  maintaining, and disbanding teams through their performance of communicative acts.  This view requires that de#nitions of basic communicative acts, such as requesting, be  recast in terms of the formation of a jointintention --- a mental state that has been suggested  underlies team behavior. Thus, a model of teamwork based on jointintentions  is shown to provide the coherence for interagent dialogue, and provides motivation for  its step-by-step progression in virtue of the performance of commun...
377|Towards Collaborative and Adversarial Learning: A Case Study in Robotic Soccer|Soccer is a rich domain for the study of multiagent learning issues. Not only must the  players learn low-level skills, but they must also learn to work together and to adapt to the  behaviors of different opponents. We are using a robotic soccer system to study these different  types of multiagent learning: low-level skills, collaborative, and adversarial. Here we describe  in detail our experimental framework. We present a learned, robust, low-level behavior that  is necessitated by the multiagent nature of the domain, namely shooting a moving ball. We  then discuss the issues that arise as we extend the learning scenario to require collaborative  and adversarial learning.
378|Implementing Agent Teams in Dynamic Multi-agent Environments|Teamwork is becoming increasingly critical in multi-agent environments ranging from virtual environments for training and education, to information integration on the internet, to potential multi-robotic space missions. Teamwork in such complex, dynamic environments is more than a simple union of simultaneous individual activity, even if supplemented with preplanned coordination. Indeed in these dynamic environments, unanticipated events can easily cause a breakdown in such preplanned coordination. The central hypothesis in this article is that for effective teamwork, agents should be provided explicit representation of team goals and plans, as well as an explicit representation of a model of teamwork to support the execution of team plans. In our work, this model of teamwork takes the form of a set of domain independent rules that clearly outline an agent&#039;s commitments and responsibilities as a participant in team activities, and thus guide the agent&#039;s social activities while executin...
379|Agent-Oriented Architecture for Air Combat Simulation|Air combat modelling using graphical simulation is a powerful means for development and evaluation of tactics. However, large models are particularly expensive and time-consuming to maintain and modify. Multi-aircraft full mission man-in-the-loop simulators will provide an even more complex programming environment. Agent-oriented architecture provides a suitable software environment for the development of an air combat simulation model based on the concept of rational agents. This approach allows the analyst to work at a high level, formulating concepts and aims, while keeping the detailed computer programming hidden.  1 Introduction  Computer-based air combat modelling is a powerful tool, widely accepted for its usefulness. The extremely high cost of operating aircraft and their weapons has led to a rapid growth in the development and use of computer simulation models as a basis for tactics development, pilot training, and operational evaluation of weapon systems. However, models hav...
380|Recursive Agent and Agent-group Tracking in a Real-time, Dynamic Environment|Agent tracking is an important capability an intelligent agent requires for interacting with other agents. It involves monitoring the observable actions of other agents as well as inferring their unobserved actions or high-level goals and behaviors. This paper focuses on a key challenge for agent tracking: recursive tracking of individuals or groups of agents. The paper first introduces an approach for tracking recursive agent models. To tame the resultant growth in the tracking effort and aid real-time performance, the paper then presents model sharing, an optimization that involves sharing the effort of tracking multiple models. Such shared models are dynamically unshared as needed --- in effect, a model is selectively tracked if it is dissimilar enough to require unsharing. The paper also discusses the application of recursive modeling in service of deception, and the impact of sensor imperfections. This investigation is based on our on-going effort to build intelligent pilot agents...
381|Coordinated Behavior of Computer Generated Forces in TacAir-Soar|The fielding of large numbers of autonomous computer-generated forces requires that these forces be able to coordinate their behaviors. Within the military, there are many levels of coordination, from the high-level management of a theater of war, down to the low-level interactions of individual soldiers. TacAir-Soar represents a data point at this low level, where individual fighter planes must fly together in sections with support from a air intercept controller. In this paper we analyze the types of coordinated behavior required to make TacAir-Soar a realistic model of human behavior, the methods that our agents employ to coordinate their behavior, and finally, the constraints coordination places on the design of computer-generated forces. Introduction One of the ultimate goals of research in computergenerated forces is to populate simulated battlefields with automated intelligent agents 1 which behave as humans would on a real battlefield. Although we can make progress by creat...
382|RESC: An Approach for Real-time, Dynamic Agent Tracking|Agent tracking involves monitoring the observable actions of other agents as well as inferring their unobserved actions, plans, goals and behaviors. In a dynamic, real-time environment, an intelligent agent faces the challenge of tracking other agents&#039; flexible mix of goaldriven and reactive behaviors, and doing so in real-time, despite ambiguities. This paper presents RESC (REal-time Situated Commitments) , an approach that enables an intelligent agent to meet this challenge. RESC&#039;s situatedness derives from its constant uninterrupted attention to the current world situation --- it always tracks other agents&#039; on-going actions in the context of this situation. Despite ambiguities, RESC quickly commits to a single interpretation of the on-going actions (without an extensive examination of the alternatives), and uses that in service of interpretation of future actions. However, should its commitments lead to inconsistencies in tracking, it uses singlestate backtracking to undo some of th...
383|Modelling Teams and Team Tactics in Whole Air Mission Modelling|The problem of whole air mission modelling is part of a larger problem which is the problem of simulating possible war-like scenarios in the air, sea, and on land. In such modelling systems one is required to model the behaviour of various actors and the resources that are available to them. One aspect of this problem is the modelling of a group of actors as a team and then modelling the coordinated behaviour of such a team to achieve a joint goal. In the domain of air mission modelling the actors are pilots that control aircraft and their behaviour is referred to as tactics. In this paper we present the approach we adopted in modelling teams and team tactics as part of the development of the Smart Whole AiR Mission Model (SWARMM) for the DSTO, Air Operations Division.  1 Introduction  Modelling the behaviour of teams is a problem that concerns many analysts who are attempting to model the behaviours of groups of humans and also concerns researchers in Distributed Artificial Intellige...
384|Rapid object detection using a boosted cascade of simple features|This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the &#034;Integral Image&#034; which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers[6]. The third contribution is a method for combining increasingly more complex classifiers in a &#034;cascade&#034; which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.
386|A Model of Saliency-based Visual Attention for Rapid Scene Analysis|A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail. Index terms: Visual attention, scene analysis, feature extraction, target detection, visual search. \Pi I. Introduction Primates have a remarkable ability to interpret complex scenes in real time, despite the limited speed of the neuronal hardware available for such tasks. Intermediate and higher visual processes appear to select a subset of the available sensory information before further processing [1], most likely to reduce the complexity of scene analysis [2]. This selection appears to be implemented in the ...
387|Neural Network-Based Face Detection| We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates. 
388|The Design and Use of Steerable Filters|Oriented filters are useful in many early vision and image processing tasks. One often needs to apply the same filter, rotated to different angles under adaptive control, or wishes to calculate the filter response at various orientations. We present an efficient architecture to synthesize filters of arbitrary orientations from linear combinations of basis filters, allowing one to adaptively &#034;steer&#034; a filter to any orientation, and to determine analytically the filter output as a function of orientation.
389|Boosting the margin: A new explanation for the effectiveness of voting methods|One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik’s support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.  
390|Training Support Vector Machines: an Application to Face Detection|We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&amp;T Bell Labs.) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM&#039;s over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the ...
391|Boosting Image Retrieval|We present an approach for image retrieval using a very large number of highly selective features and efficient online learning. Our approach is predicated on the assumption that each image is generated by a sparse set of visual “causes ” and that images which are visually similar share causes. We propose a mechanism for computing a very large number of highly selective features which capture some aspects of this causal structure (in our implementation there are over 45,000 highly selective features). At query time a user selects a few example images, and a technique known as “boosting ” is used to learn a classification function in this feature space. By construction, the boosting procedure learns a simple classifier which only relies on 20 of the features. As a result a very large database of images can be scanned rapidly, perhaps a million images per second. Finally we will describe a set of experiments performed using our retrieval system on a database of 3000 images.  
392|A SNoW-based face detector|mhyang~vision.ai.uiuc.edu danr~cs.uiuc.edu ahuja~vision.ai.uiuc.edu A novel learning approach for human face detection using a network of linear units is presented. The SNoW learning architecture is a sparse network of linear functions over a pre-defined or incremen-tally learned feature space and is specifically tailored for learning in the presence of a very large number of features. A wide range of face images in different poses, with different expressions and under different lighting conditions are used as a training set to capture the variations of human faces. Experimental results on commonly used benchmark data sets of a wide range of face images show that the SNoW-based approach outperforms methods that use neural networks, Bayesian methods, support vector machines and oth-ers. Furthermore, learning and evaluation using the SNoW-based method are significantly more efficient than with other methods. 1
393|Coarse-to-Fine Face Detection| We study visual selection: Detect and roughly localize all instances of a generic object class, such as a face, in a greyscale scene, measuring performance in terms of computation and false alarms. Our approach is sequential testing which is coarse-to-fine in both in the exploration of poses and the representation of objects. All the tests are binary and indicate the presence or absence of loose spatial arrangements of oriented edge fragments. Starting from training examples, we recursively find larger and larger arrangements which are “decomposable,” which implies the probability of an arrangement appearing on an object decays slowly with its size. Detection means finding a sufficient number of arrangements of each size along a decreasing sequence of pose cells. At the beginning, the tests are simple and universal, accommodating many poses simultaneously, but the false alarm rate is relatively high. Eventually, the tests are more discriminating, but also more complex and dedicated to specific poses. As a result, the spatial distribution of processing is highly skewed and detection is rapid, but at the expense of (isolated) false alarms which, presumably, could be eliminated with localized, more intensive, processing. 
394|Joint Induction of Shape Features and Tree Classifiers|We introduce a very large family of binary features for two-dimensional shapes. The salient ones for separating particular shapes are determined by inductive learning during the construction of classi cation trees. There is a feature for every possible geometric arrangement of local topographic codes. The arrangements express coarse constraints on relative angles and distances among the code locations and are nearly invariant to substantial a ne and non-linear deformations. They are also partially ordered, which makes it possible to narrow the search for informative ones at each node of the tree. Di erent trees correspond to di erent aspects of shape. They are statistically weakly dependent due to randomization and are aggregated in a simple way. Adapting the algorithm to a shape family is then fully automatic once training samples are provided. As an illustration, we classify handwritten digits from the NIST database ? the error rate is:7%.
395|Overcomplete steerable pyramid filters and rotation invariance|A given (overcomplete) discrete oriented pyramid may be converted into a steerable pyramid by interpolation. We present a technique for deriving the optimal interpolation functions (otherwise called steering coefficients). The proposed scheme is demonstrated on a computationally efficient oriented pyramid, which is a variation on the Burt and Adelson pyramid. We apply the generated steerable pyramid to orientation-invarianttexture analysis to demonstrate its excellent rotational isotropy. High classification rates and precise rotation identification are demonstrated. 1
396|Machine Learning in Automated Text Categorization|The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last ten years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely document representation, classifier construction, and classifier evaluation.
397|Text Categorization with Support Vector Machines: Learning with Many Relevant Features|This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies, why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and they behave robustly over a variety of different learning tasks. Furthermore, they are fully automatic, eliminating the need for manual parameter tuning.
398|Transductive Inference for Text Classification using Support Vector Machines|This paper introduces Transductive Support Vector Machines (TSVMs) for text classification.  While regular Support Vector Machines  (SVMs) try to induce a general decision  function for a learning task, Transductive  Support Vector Machines take into account  a particular test set and try to minimize  misclassifications of just those particular  examples. The paper presents an analysis  of why TSVMs are well suited for text  classification. These theoretical findings are  supported by experiments on three test collections.  The experiments show substantial  improvements over inductive methods, especially  for small training sets, cutting the number  of labeled training examples down to a  twentieth on some tasks. This work also proposes  an algorithm for training TSVMs efficiently,  handling 10,000 examples and more. 
399|On the optimality of the simple Bayesian classifier under zero-one loss|The simple Bayesian classifier is known to be optimal when attributes are independent given the class, but the question of whether other sufficient conditions for its optimality exist has so far not been explored. Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive. This article shows that, although the Bayesian classifier’s probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian classifier has a much greater range of applicability than previously thought. For example, in this article it is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption. Further, studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain. This article’s results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier, and this is also verified empirically.
400|Irrelevant Features and the Subset Selection Problem|We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets.
401|A Sequential Algorithm for Training Text Classifiers|The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness. 1 Introduction Text classification is the automated grouping of textual or partially textual entities. Document retrieval, categorization, routing, filtering, and clustering, as well as natural language processing tasks such as tagging, word sense disambiguation, and some aspects of understanding can be formulated as text classification. As the amount of online text increases, the demand for text classification to aid the analysis and mana...
402|NewsWeeder: Learning to Filter Netnews|A significant problem in many information filtering systems is the dependence on the user for the creation and maintenance of a user profile, which describes the user&#039;s interests. NewsWeeder is a netnews-filtering system that addresses this problem by letting the user rate his or her interest level for each article being read (1-5), and then learning a user profile based on these ratings. This paper describes how NewsWeeder accomplishes this task, and examines the alternative learning methods used. The results show that a learning algorithm based on the Minimum Description Length (MDL) principle was able to raise the percentage of interesting articles to be shown to users from 14% to 52% on average. Further, this performance significantly outperformed (by 21%) one of the most successful techniques in Information Retrieval (IR), termfrequency /inverse-document-frequency (tf-idf) weighting. 1
403|Hierarchically Classifying Documents Using Very Few Words|The proliferation of topic hierarchies for text documents has resulted in a need for tools that automatically classify new documents within such hierarchies. Existing classification schemes which ignore the hierarchical structure and treat the topics as separate classes are often inadequate in text classification where the there is a large number of classes and a huge number of relevant features needed to distinguish between them. We propose an approach that utilizes the hierarchical topic structure to decompose the classification task into a set of simpler problems, one at each node in the classification tree. As we show, each of these smaller problems can be solved accurately by focusing only on a very small set of features, those relevant to the task at hand. This set of relevant features varies widely throughout the hierarchy, so that, while the overall relevant feature set may be large, each classifier only examines a small subset. The use of reduced feature sets allows us to util...
404|Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval  (1998) |The naive Bayes classifier, currently experiencing a renaissance  in machine learning, has long been a core technique in information  retrieval. We review some of the variations of naive Bayes models used for  text retrieval and classification, focusing on the distributional assump-  tions made about word occurrences in documents.
405|Enhanced Hypertext Categorization Using Hyperlinks|A major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured search using topic taxonomies, circumvents keyword ambiguity, and improves the quality of search and profile-based routing and filtering. Therefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain highquality semantic clues that are lost upon a purely termbased classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a  relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics. We experimented ...
406|A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization|The Rocchio relevance feedback algorithm is one of the most popular and widely applied learning methods from information retrieval. Here, a probabilistic analysis of this algorithm is presented in a text categorization framework. The analysis gives theoretical insight into the heuristics used in the Rocchio algorithm, particularly the word weighting scheme and the similarity metric. It also suggests improvements which lead to a probabilistic variant of the Rocchio classifier. The Rocchio classifier, its probabilistic variant, and a naive Bayes classifier are compared on six text categorization tasks. The results show that the probabilistic algorithms are preferable to the heuristic Rocchio classifier not only because they are more well-founded, but also because they achieve better performance.
407|N-grambased text categorization|Text categorization is a fundamental task in document processing, allowing the automated handling of enormous streams of documents in electronic form. One difficulty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through OCR. Text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems. We describe here an N-gram-based approach to text categorization that is tolerant of textual errors. The system is small, fast and robust. This system worked very well for language classification, achieving in one test a 99.8 % correct classification rate on Usenet newsgroup articles written in different languages. The system also worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject, achieving as high as an 80 % correct classification rate. There are also several obvious directions for improving the system’s classification performance in those cases where it did not do as well. The system is based on calculating and comparing profiles of N-gram frequencies. First, we use the system to compute profiles on training set data that represent the various categories, e.g., language samples or newsgroup content samples. Then the system computes a profile for a particular document that is to be classified. Finally, the system computes a distance measure between the document’s profile and each of the
408|Information Filtering and Information Retrieval: Two Sides of the Same Coin|Information filtering systems are designed for unstructured or semistructured data, as opposed to database applications, which use very structured data. The systems also deal primarily with textual information, but they may also entail images, voice, video or other data types that are part of multimedia information systems. Information filtering systems also involve a large amount of data and streams of incoming data, whether broadcast from a remote source or sent directly by other sources. Filtering is based on descriptions of individual or group information preferences, or profiles, that typically represent long-term interests. Filtering also implies removal of data from an incoming stream rather than finding data in the stream; users see only the data that is extracted. Models of information retrieval and filtering, and lessons for filtering from retrieval research are presented.
409|Hierarchical classification of Web content|sdumais @ microsoft.com This paper explores the use of hierarchical structure for classifying a large, heterogeneous collection of web content. The hierarchical structure is initially used to train different second-level classifiers. In the hierarchical case, a model is learned to distinguish a second-level category from other categories within the same top level. In the flat non-hierarchical case, a model distinguishes a second-level category from all other second-level categories. Scoring rules can further take advantage of the hierarchy by considering only second-level categories that exceed a threshold at the top level. We use support vector machine (SVM) classifiers, which have been shown to be efficient and effective for classification, but not previously explored in the context of hierarchical classification. We found small advantages in accuracy for hierarchical models over flat models. For the hierarchical approach, we found the same accuracy using a sequential Boolean decision rule and a multiplicative decision rule. Since the sequential approach is much more efficient, requiring only 14%-16 % of the comparisons used in the other approaches, we find it to be a good choice for classifying text into large hierarchical structures.
410|A Comparison of Two Learning Algorithms for Text Categorization|This paper examines the use of inductive learning to categorize natural language documents into predefined content categories. Categorization of text is of increasing importance in information retrieval and natural language processing systems. Previous research on automated text categorization has mixed machine learning and knowledge engineering methods, making it difficult to draw conclusions about the performance of particular methods. In this paper we present empirical results on the performance of a Bayesian classifier and a decision tree learning algorithm on two text categorization data sets. We find that both algorithms achieve reasonable performance and allow controlled tradeoffs between false positives and false negatives. The stepwise feature selection in the decision tree algorithm is particularly effective in dealing with the large feature sets common in text categorization. However, even this algorithm is aided by an initial prefiltering of features, confirming the results...
411|OHSUMED: An interactive retrieval evaluation and new large test collection for research|A series of information retrieval experiments was earned out with a computer installed in a medical practice setting for relatively inexperienced physician end-users. Using a commercial MEDLINE product based on the vector space model, these physicians searched just as effectively as more experienced searchers using Boolean searching. The results of this experiment were subsequently used to create anew large medical test collection, which was used in experiments with the SMART ~trieval system to obtain baseline performance data as well as compare SMART with the other searchers. 1
412|Employing EM in Pool-Based Active Learning for Text Classification|This paper shows how a text classifier&#039;s need for labeled training data can be reduced by a combination of active learning and Expectation Maximization (EM) on a pool of unlabeled data. Query-by-Committee is used to actively select documents for labeling, then EM with a naive Bayes model further improves classification accuracy by concurrently estimating probabilistic labels for the remaining unlabeled documents and using them to improve the model. We also present a metric for better measuring disagreement among committee members; it accounts for the strength of their disagreement and for the distribution of the documents. Experimental results show that our method of combining EM and active learning requires only half as many labeled training examples to achieve the same accuracy as either EM or active learning alone. Keywords:  text classification active learning unsupervised learning information retrieval  1 Introduction  In many settings for learning text classifiers, obtaining lab...
413|Automated Learning of Decision Rules for Text Categorization|We describe the results of extensive experiments on large document collections using optimized  rule-based induction methods. The goal of these methods is to automatically discover  classification patterns that can be used for general document categorization or personalized filtering  of free text. Previous reports indicate that human-engineered rule-based systems, requiring  manymanyears of developmental efforts, have been successfully built to &#034;read&#034; documents and  assign topics to them. In this paper, weshowthatmachine generated decision rules appear  comparable to human performance, while using the identical rule-based representation. In comparison  with other machine learning techniques, results on a key benchmark from the Reuters  collection show a large gain in performance, from a previously reported 65% recall/precision  breakeven point to 80.5%. In the context of a very high dimensional feature space, several  methodological alternatives are examined, including universal versu...
414|Heterogeneous uncertainty sampling for supervised learning|Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger. 1
415|Distributional Clustering of Words for Text Classification|This paper describes the application of Distributional Clustering [20] to document classification. This approach clusters words into groups based on the distribution of class labels associated with each word. Thus, unlike some other unsupervised dimensionalityreduction techniques, such as Latent Semantic Indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy. Experimental results obtained on three real-world data sets show that we can reduce the feature dimensionality by three orders of magnitude and lose only 2% accuracy---significantly better than Latent Semantic Indexing [6], class-based clustering [1], feature selection by mutual information [23], or Markov-blanket-based feature selection [13]. We also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clustering. 1 Introduction The popularity of the Internet has caused an exponent...
416|Improving Text Classification by Shrinkage in a Hierarchy of Classes|When documents are organized in a large number of topic categories, the categories are often arranged in a hierarchy. The U.S. patent database and Yahoo are two examples.
417|Context-Sensitive Learning Methods for Text Categorization|this article, we will investigate the performance of two recently implemented machine-learning algorithms on a number of large text categorization problems. The two algorithms considered are set-valued RIPPER, a recent rule-learning algorithm [Cohen A earlier version of this article appeared in Proceedings of the 19th Annual International ACM Conference on Research and Development in Information Retrieval (SIGIR) pp. 307--315
418|Training Algorithms for Linear Text Classifiers|Systems for text retrieval, routing, categorization and other IR tasks rely heavily on linear classifiers. We propose that two machine learning algorithms, the Widrow-Hoff and EG algorithms, be used in training linear text classifiers. In contrast to most IR methods, theoretical analysis provides performance guarantees and guidance on parameter settings for these algorithms. Experimental data is presented showing Widrow-Hoff and EG to be more effective than the widely used Rocchio algorithm on several categorization and routing tasks.  1 Introduction  Document retrieval, categorization, routing, and filtering systems often are based on classification. That is, the IR system decides for each document which of two or more classes it belongs to, or how strongly it belongs to a class, in order to accomplish the IR task of interest. For instance, the two classes may be the documents relevant to and not relevant to a particular user, and the system may rank documents based on how likely it i...
419|Automatic Detection of Text Genre|As the text databases available to users become larger and more heterogeneous, genre  becomes increasingly important for computational  linguistics as a complement to  topical and structural principles of classification. We propose a th
420|Combining Classifiers in Text Categorization|Three different types of classifiers were investigated in the context of a text categorization problem in the medical domain: the automatic assignment of ICD9 codes to dictated inpatient discharge summaries. K-nearest-neighbor, relevance feedback, and Bayesian independence classifers were applied individually and in combination. A combination of different classifiers produced better results than any single type of classifier. For this specific medical categorization problem, new query formulation and weighting methods used in the k-nearest-neighbor classifier improved performance.   1 Introduction  Past research in information retrieval has shown that one can improve retrieval effectiveness by using multiple representations in indexing and query formulation [27] [19] [3] [11] and by using multiple search strategies [5] [24] [7]. In this work, we investigate whether we can attain similar improvements in the domain of text categorization by combining different representations and classif...
421|Scalable Feature Selection, Classification and Signature Generation for Organizing Large Text Databases Into Hierarchical Topic Taxonomies|We explore how to organize large text databases hierarchically by topic to aid better searching, browsing and filtering. Many corpora, such as internet directories, digital libraries, and patent databases are manually organized into topic hierarchies, also called taxonomies. Similar to indices for relational data, taxonomies make search and access more efficient. However, the exponential growth in the volume of on-line textual information makes it nearly impossible to maintain such taxonomic organization for large, fast-changing corpora by hand. We describe an automatic system that starts with a small sample of the corpus in which topics have been assigned by hand, and then updates the database with new documents as the corpus grows, assigning topics to these new documents with high speed and accuracy. To do this, we use techniques from statistical pattern recognition to efficiently separate the feature words, or...
422|Detecting Concept Drift with Support Vector Machines|For many learning tasks where data is collected over an extended period of time, its underlying distribution is likely to change. A typical example is information filtering, i.e. the adaptive classification of documents with respect to a particular user interest. Both the interest of the user and the document content change over time. A filtering system should be able to adapt to such concept changes. This paper proposes a new method to recognize and handle concept changes with support vector machines. The method maintains a window on the training data. The key idea is to automatically adjust the window size so that the estimated generalization error is minimized. The new approach is both theoretically well-founded as well as effective and efficient in practice. Since it does not require complicated parameterization, it is simpler to use and more robust than comparable heuristics. Experiments with simulated concept drift scenarios based on real-world text data com...
423|Mistake-Driven Learning in Text Categorization|Learning problems in the text processing  domain often map the text to a space  whose dimensions are the measured fea-  tures of the text, e.g., its words. Three  characteristic properties of this domain are  (a) very high dimensionality, (b) both the  learned concepts and the instances reside  very sparsely in the feature space, and (c)  a high variation in the number of active  features in an instance. In this work we  study three mistake-driven learning algo-  rithms for a typical task of this nature -   text categorization. We argue
424|A Probabilistic Learning Approach for Document Indexing|We describe a method for probabilistic document indexing using relevance feedback data  that has been collected from a set of queries. Our approach is based on three new concepts:  (1) Abstraction from specific terms and documents, which overcomes the restriction of limited  relevance information for parameter estimation. (2) Flexibility of the representation, which  allows the integration of new text analysis and knowledge-based methods in our approach as  well as the consideration of document structures or different types of terms. (3) Probabilistic  learning or classification methods for the estimation of the indexing weights making better use  of the available relevance information. Our approach can be applied under restrictions that  hold for real applications. We give experimental results for five test collections which show  improvements over other indexing methods.
425|Tadepalli, Active Learning with Committees for Text Categorization |The availability of vast amounts of information on the World Wide Web has created a big demand for automatic tools to organize and index that information. Unfortunately, the paradigm of supervised machine learning is ill-suited to this task, as it assumes that the training examples are classi-fied by a teacher – usually a human. In this paper, we de-scribe an active learning method based on Query by Com-mittee (QBC) that reduces the number of labeled training examples (text documents) required for learning by 1-2 or-ders of magnitude. 1.
426|Improving text retrieval for the routing problem using latent semantic indexing|Latent Semantic Indexing (LSI) is a novel approach to information retrieval that attempts to model the underlying structure of term associations by transforming the traditional representation of documents as vectors of weighted term frequencies to a new coordinate space where both documents and terms are represented as linear combinations of underlying semantic factors. In previous research, LSI has produced a small improvement in retrieval performance. In this paper, we apply LSI to the routing task, which operates under the assumption that a sample of relevant and non-relevant documents is available to use in constructing the query. Once again, LSI slightly improves performance. However, when LSI is used is conduction with statistical classification, there is a dramatic improvement in performance. 1
427|Models for retrieval with probabilistic indexing|Abstract- in this article three retrieval models for probabilistic indexing are described along with evaluation results for each. First is the binary independence indexing @II) model, which is a generalized version of the Maron and Kuhns indexing model. In this model, the indexing weight of a descriptor in a document is an estimate of the proba-bility of relevance of this document with respect to queries using this descriptor. Sec-ond is the retrieval-with-probabilistic-indexing (RPI) model, which is suited to different kinds of probabilistic indexing. For that we assume that each indexing scheme has its own concept of “correctness ” to which the probabilities relate. In addition to the prob-abilistic indexing weights, the RPI model provides the possibility of reIevance weight-ing of search terms. A third mode1 that is similar was proposed by Croft some years ago as an extension of the binary independence retrieval model but it can be shown that this model is not based on the probabilistic ranking principle. The probabilistic indexing weights required for any of these models can be provided by an application of the Darm-stadt indexing approach (DIA) for indexing with descriptors from a controlled vocabu-Iary. The experimental results show signi~cant improvements over retrieval with binary indexing. Finally, suggestions are made regarding how the DIA can be applied to prob-abilistic indexing with free text terms. 1.
428|Automatic Essay Grading Using Text Categorization Techniques|The commas are the most useful and usable of all the stops. It is highly important to put them in place as you go along. If you try to come back after doing a paragraph and stick them in the various spots that tempt you you will discover that they tend to swarm like minnows into all sorts of crevices whose existence you hadn?t realized and before you know it the whole long sentence becomes immobilized and lashed up squirming in commas. Better to use them sparingly, and with affection precisely when the need for one arises, nicely, by itself.
429|Noun Homograph Disambiguation Using Local Context in Large Text Corpora|This paper describes an accurate, relatively inexpensive method for the disambiguation of noun homographs using large text corpora. The algorithm checks the context surrounding the target noun against that of previously observed instances and chooses the sense for which the most evidence is found, where evidence consists of a set of orthographic, syntactic, and lexical features. Because the sense distinctions made are coarse, the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms. An implementation of the algorithm is described which, starting with a small set of hand-labeled instances, improves its results automatically via unsupervised training. The approach is compared to other attempts at homograph disambiguation using both machine readable dictionaries and unrestricted text and the use of training instances is determined to be a crucial difference. 1 Introduction  Large text corpora and the computational resources to handle them have ...
431|Text categorization of low quality images|Categorization of text images into content-oriented classes would be a useful capability in a variety of document handling systems. Many methods can be usedtocategorize texts once their words are known, but OCR can garble a large proportion of words, particularly when low quality images are used. Despite this, we show for one data set that fax quality images can be categorized with nearly the same accuracy as the original text. Further, the categorization system can be trained on noisy OCR output, without need for the true text of any image, or for editing of OCR output. The useofavector space classi er and training method robust to large feature sets, combined with discarding of low frequency OCR output strings are the key to our approach. 1
432|&#034;Is This Document Relevant? ...Probably&#034;: A Survey of Probabilistic Models in Information Retrieval|This article surveys probabilistic approaches to modeling information retrieval. The  basic concepts of probabilistic approaches to information retrieval are outlined and  the principles and assumptions upon which the approaches are based are  presented. The various models proposed in the development of IR are described,  classified, and compared using a common formalism. New approaches that  constitute the basis of future research are described
433|A Learner-Independent Evaluation of the Usefulness of Statistical Phrases for Automated Text Categorization|In this work we investigate the usefulness of  n-grams for document indexing in text categorization  (TCi  We call-gram a set g k  of n word stems, and we say that g k occurs  in a document d j when a sequence of  words appears in d j that, after stop word removal  and stemming, consists exactly ofthe  n stems in g k , in some order. Previous researches  have investigated the use of n-grams  (or some variant ofthem) in the context of  specific learning algorithms, and thus have  not obtained general answers on their usefulness  for TC In this work we investigate the  usefulness of n-grams  inTC  independently  ofany specific learning algorithm. We do so  by applying feature selection to the pool of  all k-grams (k  #  n), and checking how many  n-grams score high enough to be selected in  the top #k-grams. We report the results of  our experiments, using various feature selection  measures and varying values of #, performed  on  theReuters-21 standardTC  benchmark. We also report resul...
434|A Patent Search and Classification System|We present a system for searching and classifying U.S. patent documents, based on Inquery. Patents are distributed through hundreds of collections, divided up by general area. The system selects the best collections for the query. Users can search for patents or classify patent text. The user interface helps users search in fields without requiring the knowledge of Inquery query operators. The system includes a unique “phrase help ” facility, which helps users find and add phrases and terms related to those in their query.
435|Joins that Generalize: Text Classification Using WHIRL|WHIRL is an extension of relational databases that can perform &#034;soft joins&#034; based on the similarity of textual identifiers; these soft joins extend the traditional operation of joining tables based on the equivalence of atomic values. This paper evaluates WHIRL on a number of inductive classification tasks using data from the World Wide Web. We show that although WHIRL is designed for more general similaritybased reasoning tasks, it is competitive with mature inductive classification systems on these classification tasks. In particular, WHIRL generally achieves lower generalization error than C4.5, RIPPER, and several nearest-neighbor methods. WHIRL is also fast---up to 500 times faster than C4.5 on some benchmark problems. We also show that WHIRL can be efficiently used to select from a large pool of unlabeled items those that can be classified correctly with high confidence. Introduction  Consider the problem of exploratory analysis of data obtained from the Internet. Assuming that o...
436|Text Categorization and Relational Learning|We evaluate the first order learning system FOIL on a series of text categorization problems. It is shown that FOIL usually forms classifiers with lower error rates and higher rates of precision and recall with a relational encoding than with a propositional encoding. We show that FOIL&#039;s performance can be improved by relation selection, a first order analog of feature selection. Relation selection improves FOIL&#039;s performance as measured by any of recall, precision, F-measure, or error rate. With an appropriate level of relation selection, FOIL appears to be competitive with or superior to existing propositional techniques. 1 INTRODUCTION  There is increasing interest in using intelligent systems to perform tasks like e-mail filtering, news filtering, and automatic indexing of documents. Many of these applications require the ability to classify text into one of several predefined categories, and in many of these applications, it would be highly advantageous to automatically learn such...
437|Experiments on the use of feature selection and negative evidence in automated text categorization|In this work we tackle two different problems of text categorization (TC), namely feature selection and classifier induction. Feature selection refers to the activity of selecting, from the set of r distinct features (i.e. words) occurring in the collection, the subset of r '  « r features that are most useful for compactly representing the meaning of the documents. We propose a novel feature selection technique, based on a simplified variant of the ? 2 statistics. Classifier induction refers instead to the problem of automatically building a text classifier by learning from a set of documents pre-classified under the categories of interest. We propose a novel variant, based on the exploitation of negative evidence, of the well-known k-NN method. We report the results of systematic experimentation of these two methods performed on the standard Reuters-21578 benchmark.
438|Method Combination For Document Filtering|There is strong empirical and theoretic evidence that combination of retrieval methods can improve performance. In this paper, we systematically compare combination strategies in the context of document filtering, using queries from the Tipster reference corpus. We find that simple averaging strategies do indeed improve performance, but that direct averaging of probability estimates is not the correct approach. Instead, the probability estimates must be renormalized using logistic regression on the known relevance judgements. We examine more complex combination strategies but find them less successful due to the high correlations among our filtering methods which are optimized over the same training data and employ similar document representations. 1 Introduction A text filtering system monitors an incoming document stream and selects documents identified as relevant to one or more of its query profiles. If profile interactions are ignored, this reduces to a number of independent bina...
439|The TREC-6 Filtering Track: Description and Analysis|This article details the experiments conducted in the TREC-6 filtering track. The filtering track is an extension of the routing track which adds time sequencing of the document stream and set-based evaluation strategies which simulate immediate distribution of the retrieved documents. It also introduces an adaptive filtering subtrack which is designed to simulate on-line or sequential filtering of documents. In addition to motivating the task and describing the practical details of participating in the track, this document includes a detailed graphical presentation of the experimental results and attempts to analyze and explain the observed patterns. The final section suggests some ways to extend the current research in future experiments. 1 Introduction  There is increasing evidence that text filtering will become a critical tool in searching and managing the flow of data in the information age. New companies are appearing daily which offer push services or intelligent agents centere...
440|Automatic Text Categorization and Its Application to Text Retrieval|We develop an automatic text categorization approach and investigate its application to text retrieval. The categorization approach is derived from a combination of a learning paradigm known as instancebased learning and an advanced document retrieval technique known as retrieval feedback. We demonstrate the effectiveness of our categorization approach using two real-world document collections from the MEDLINE database. Next we investigate the application of automatic categorization to text retrieval. Our experiments clearly indicate that automatic categorization improves the retrieval performance compared with no categorization. We also demonstrate that the retrieval performance using automatic categorization achieves the same retrieval quality as the performance using manual categorization. Furthermore, detailed analysis of the retrieval performance on each individual test query is provided. Index Terms: Text Categorization, Automatic Classification, Text Retrieval, Instance-Based Le...
441|Using WordNet to complement training information in text categorization|Automatic Text Categorization (TC) is a complex and useful task for many natural language applications, and is usually performed through the use of a set of manually classified documents, a training collection. We suggest the utilization of additional resources like lexical databases to increase the amount of information that TC systems make use of, and thus, to improve their performance. Our approach integrates WordNet information with two training approaches through the Vector Space Model. The training approaches we test are the Rocchio (relevance feedback) and the Widrow-Hoff (machine learning) algorithms. Results obtained from evaluation show that the integration of WordNet clearly outperforms training approaches, and that an integrated technique can effectively address the classification of low frequency categories. 1
442|A probabilistic description-oriented approach for categorising Web documents|The automatic categorisation of web documents is becoming crucial for organising the huge amount of information available in the Internet. We are facing a new challenge due to the fact that web documents have a rich structure and are highly heterogeneous. Two ways to respond to this challenge are (1) using a representation of the content of web documents that captures these two characteristics and (2) using more eective classiers.  Our categorisation approach is based on a probabilistic description-oriented representation of web documents, and a probabilistic interpretation of the k-nearest neighbour classifier. With the former, we provide an enhanced document representation that incorporates the structural and heterogeneous nature of web documents. With the latter, we provide a theoretical sound justification for the various parameters of the k-nearest neighbour classifier.  Experimental results show that (1) using an enhanced representation of web documents is crucial for an effective categorisation of web documents, and (2) a theoretical interpretation of the k-nearest neighbour classifier gives us improvement over the standard k-nearest neighbour classifier.  
443|Probabilistic Information Retrieval as Combination of Abstraction, Inductive Learning and Probabilistic Assumptions|   We show that former approaches in probabilistic information retrieval are based on one or two of the three concepts abstraction, inductive learning and probabilistic assumptions, and we propose a new approach which combines all three concepts. This approach is illustrated for the case of indexing with a controlled ...
444|Feature Reduction for Neural Network Based Text Categorization|In a text categorization model using an artificial neural network as the text classifier, scalability is poor if the neural network is trained using the raw feature space since textural data has a very high-dimension feature space. We proposed and compared four dimensionality reduction techniques to reduce the feature space into an input space of much lower dimension for the neural network classifier. To test the effectiveness of the proposed model, experiments were conducted using a subset of the Reuters-22173 test collection for text categorization. The results showed that the proposed model was able to achieve high categorization effectiveness as measured by precision and recall. Among the four dimensionality reduction techniques proposed, Principal Component Analysis was found to be the most effective in reducing the dimensionality of the feature space. 1. Introduction  Text categorization is the classification of text documents into a set of one or more categories. In this paper, ...
445|Text Classification Using ESC-based Stochastic Decision Lists|We propose a new method of text classification using stochastic decision lists. A stochastic decision list is an ordered sequence of IF-THEN rules, and our method can be viewed as a rule-based method for text classification having advantages of readability and refinability of acquired knowledge. Our method is unique in that decision lists are automatically constructed on the basis of the principle of minimizing Extended Stochastic Complexity (ESC), and with it we are able to construct decision lists that have fewer errors in classification. The accuracy of classification achieved with our method appears better than or comparable to those of existing rule-based methods. We have empirically demonstrated that rule-based methods like ours result in high classification accuracy when the categories to which texts are to be assigned are relatively specific ones and when the texts tend to be short. We have also empirically verified the advantages of rule-based methods over non-rule-based ones.
447|Probabilistic learning for selective dissemination of information|New methods and new systems are needed to filter or to selectively distribute the increasing volume of electronic information being produced nowadays. An effective information filtering system is one that provides the exact information that fulfills user&#039;s interests with the minimum effort by the user to describe it. Such a system will have to be adaptive to the user changing interest. In this paper we describe and evaluate a learning model for information filtering which is an adaptation of the generalized probabilistic model of Information Retrieval. The model is based on the concept of `uncertainty sampling&#039;, a technique that allows for relevance feedback both on relevant and nonrelevant documents. The proposed learning model is the core of a prototype information filtering system called ProFile.
448|Autonomous Document Classification for Business|With the continuing exponential growth of the Internet  and the more recent growth of business Intranets, the  commercial world is becoming increasingly aware of  the problem of electronic information overload. This  has encouraged interest in developing agents/softbots  that can act as electronic personal assistants and can  develop and adapt representations of users information  needs, commonly known as profiles. As the
449|Exploiting Thesaurus Knowledge in Rule Induction for Text Classification|Systems for learning text classifiers recently gained considerable interest. One technique to implement such systems is rule induction. While most other approaches rely on a relatively simple document representation and do not make use of any background knowledge, rule induction algorithms offer a good potential for improvements in both of these areas. In this paper, we show how an operator-based view of rule induction enables the easy integration of a thesaurus as background knowledge. Results with an algorithm extended by thesaurus knowledge are presented and interpreted. The interpretation shows the strengths and weaknesses of using thesaurus knowledge and gives hints for future research. 1 Introduction Text classification deals with the task of assigning a label out of a set of predefined classes to a given text document. Example applications include classifying technical reports according to their subject research area for archiving, or analyzing incoming newswire articles wrt. ...
450|The process group approach to reliable distributed computing|The difficulty of developing reliable distributed softwme is an impediment to applying distributed computing technology in many settings. Expeti _ with the Isis system suggests that a structured approach based on virtually synchronous _ groups yields systems that are substantially easier to develop, exploit sophisticated forms of cooperative computation, and achieve high reliability. This paper reviews six years of resemr,.hon Isis, describing the model, its impl_nentation challenges, and the types of applicatiom to which Isis has been appfied. 1 In oducfion One might expect the reliability of a distributed system to follow directly from the reliability of its con-stituents, but this is not always the case. The mechanisms used to structure a distributed system and to implement cooperation between components play a vital role in determining how reliable the system will be. Many contemporary distributed operating systems have placed emphasis on communication performance, overlooking the need for tools to integrate components into a reliable whole. The communication primitives supported give generally reliable behavior, but exhibit problematic semantics when transient failures or system configuration changes occur. The resulting building blocks are, therefore, unsuitable for facilitating the construction of systems where reliability is impo/tant. This paper reviews six years of research on Isis, a syg_,,m that provides tools _ support the construction of reliable distributed software. The thesis underlying l._lS is that development of reliable distributed software can be simplified using process groups and group programming too/_. This paper motivates the approach taken, surveys the system, and discusses our experience with real applications.
452|Transis: A Communication Sub-System for High Availability|This paper describes Transis, a communication sub-system for high availability. Transis is a transport layer package that supports a variety of reliable multicast message passing services between processors. It provides highly tuned multicast and control services for scalable systems with arbitrary topology. The communication domain comprises of a set of processors that can initiate multicast messages to a chosen subset. Transis delivers them reliably and maintains the  membership of connected processors automatically, in the presence of arbitrary communication delays, of message losses and of processor failures and joins. The contribution of this paper is in providing an aggregate definition of communication and control services over broadcast domains. The main benefit is the efficient implementation of these services using the broadcast capability. In addition, the membership algorithm has a novel approach in handling partitions and remerging; in allowing the regular flow of messages...
453|Fail-Stop Processors: An Approach to Designing Fault-Tolerant Computing Systems|This paper was originally submitted to ACM Transactions on Programming Languages and Systems. The responsible editor was Susan L. Graham. The authors and editor kindly agreed to transfer the paper to the ACM Transactions on Computer Systems
454|Memory access buffering in multiprocessors|In highly-pipelined machines, instructions and data are prefetched and buffered in both the processor and the cache. This is done to reduce the average memory access la-tency and to take advantage of memory interleaving. Lock-up free caches are designed to avoid processor blocking on a cache miss. Write buffers are often included in a pipelined machine to avoid processor waiting on writes. In a shared memory multiprocessor, there are more advantages in buffering memory requests, since each memory access has to traverse the memory- processor interconnection and has to compete with memory requests issued by different processors. Buffering, however, can cause logical problems in multipro-cessors. These problems are aggravated if each processor has a private memory in which shared writable data may be present, such as in a cache-based system or in a system with a distributed global memory. In this paper, we analyze the benefits and problems associated with the buffering of memory requests in shared memory multiprocessors. We show that the logical problem of buffering is directly related to the problem of synchronization. A simple model is presented to evaluate the performance improvement result-ing from buffering. 1.
455|Using Process Groups to Implement Failure Detection in Asynchronous Environments|Agreement on the membership of a group of processes in a distributed system is a basic problem that arises in a wide range of applications. Such groups occur when a set of processes co-operate to perform some task, share memory, monitor one another, subdivide a computation, and so forth. In this paper we discuss the Group Membership Problem as it relates to failure detection in asynchronous, distributed systems. We present a rigorous, formal specification for group membership under this interpretation. We then present a solution for this problem that improves upon previous work.
456|An Efficient Reliable Broadcast Protocol|Many distributed and parallel applications can make good use of  broadcast communication. In this paper we present a (software) protocol  that simulates reliable broadcast, even on an unreliable network. Using  this protocol, application programs need not worry about lost messages.  Recovery of communication failures is handled automatically and transparently  by the protocol. In normal operation, our protocol is more  efficient than previously published reliable broadcast protocols. An initial  implementation of the protocol on 10 MC68020 CPUs connected by a 10  Mbit/sec Ethernet performs a reliable broadcast in 1.5 msec.   
457|The Distributed V Kernel and its Performance for Diskless Workstations|The distributed V kernel is a message-oriented kernel that provides uniform local and network interprocess communication. It is primarily being used in an environment of diskless workstations connected by a high-speed local network to a set of file servers. We describe a performance evaluation of the kernel, with particular emphasis on the cost of network file access. Our results show that over a local network: 1. Diskless workstations can access remote files with minimal performance penalty. 2. The V message facility can be used to access remote files at comparable cost to any well-tuned specialized file access protocol. We conclude that it is feasible to build a distributed system with all network communication using the V message facility even when most of the network nodes have no secondary storage. 1.
458|Highly-available distributed services and fault-tolerant distributed garbage collection|This paper describes two techniques that are only loosely related. The first is a method for constructing a highly available service for use in a distributed system. The service presents its clients with a consistent view of its state, but the view may contain old information. Clients can indicate how recent the information must be. The method can be used in any application in which the property of interest is stable: once the property becomes true, it remains true forever. The paper also describes a fault-tolerant garbage collection method for a distributed heap. The method is practical and efficient. Each computer that contains part of the heap does local garbage collection independently, using whatever algorithm it chooses, and without needing to communicate with the other computers that contain parts of the heap. The highly available central service is used to store information about inter.computer references. 1.
459|Designing Application Software in Wide Area Network Settings|T(&#039;VED FOR PUBLIC RELEASE
460|The Architecture of Cognition|Spanning seven orders of magnitude: a challenge for
461|Toward an instance theory of automatization|This article presents a theory in which automatization is construed as the acquisition of a domain-specific knowledge base, formed of separate representations, instances, of each exposure to the task. Processing is considered automatic if it relies on retrieval of stored instances, which will occur only after practice in a consistent environment. Practice is important because it increases the amount retrieved and the speed of retrieval; consistency is important because it ensures that the retrieved instances will be useful. The theory accounts quantitatively for the power-function speed-up and predicts a power-function reduction in the standard deviation that is constrained to have the same exponent as the power function for the speed-up. The theory accounts for qualitative properties as well, explaining how some may disappear and others appear with practice. More generally, it provides an alternative to the modal view of automaticity, arguing that novice performance is limited by a lack of knowledge rather than a scarcity of resources. The focus on learning avoids many problems with the modal view that stem from its focus on resource limitations. Automaticity is an important phenomenon in everyday men-tal life. Most of us recognize that we perform routine activities quickly and effortlessly, with little thought and conscious aware-ness--in short, automatically (James, 1890). As a result, we of-ten perform those activities on &amp;quot;automatic pilot &amp;quot; and turn our minds to other things. For example, we can drive to dinner while conversing in depth with a visiting scholar, or we can make coffee while planning dessert. However, these benefits may be offset by costs. The automatic pilot can lead us astray, caus-ing errors and sometimes catastrophes (Reason &amp; Myceilska, 1982). If the conversation is deep enough, we may find ourselves and the scholar arriving at the office rather than the restaurant, or we may discover that we aren&#039;t sure whether we put two or three scoops of coffee into the pot. Automaticity is also an important phenomenon in skill acqui-sition (e.g., Bryan &amp; Harter, 1899). Skills are thought to consist largely of collections of automatic processes and procedures
463|Intelligent Tutoring Goes to School in the Big City|Abstract. This paper reports on a large-scale experiment introducing and evaluating intelligent tutoring in an urban High School setting. Critical to the success of this project has been a client-centered design approach that has matched our client&#039;s expertise in curricular objectives and classroom teaching with our expertise in artificial intelligence and cognitive psychology. The Pittsburgh Urban Mathematics Project (PUMP) has produced an algebra curriculum that is centrally focused on mathematical analysis of real world situations and the use of computational tools. We have built an intelligent tutor, called PAT, that supports this curriculum and has been made a regular part of 9th grade Algebra in 3 Pittsburgh schools. In the 1993-94 school year, we evaluated the effect of the PUMP curriculum and PAT tutor use. On average the 470 students in experimental classes outperformed students in comparison classes by 15% on standardized tests and 100 % on tests targeting the PUMP objectives. This study provides further evidence that laboratory tutoring systems can be scaled up and made to work, both technically and pedagogically, in real and unforgiving settings like urban high schools. 1.
464|Cognitive tutors: Lessons learned|This article reviews the 10-year history of tutor development based on the
465|Judgments of frequency and recognition memory in a multiple-trace memory model (Tech  (1986) |The multiple-trace simulation model, MINERVA 2, was applied to a number of phenomena found in experiments on relative and absolute judgments of frequency, and forced-choice and yes-no recognition memory. How the basic model deals with effects of repetition, forgetting, list length, orientation task, selective retrieval, and similarity and how a slightly modified version accounts for effects of contextual variability on frequency judgments were shown. Two new experiments on similarity and recognition memory were presented, together with appropriate simulations; attempts to modify the model to deal with additional phenomena were also described. Questions related to the representation of frequency are addressed, and the model is evaluated and compared with related models of frequency judgments and recognition memory. Although memory for specific events (episodic memory) and memory for abstract concepts (generic memory) seem quite different intuitively, experimental evidence for different underlying systems is sparse (see McKoon, Ratcliff,  &amp; Dell, 1986; Ratcliff &amp; McKoon, 1986; Tulving, 1986). One suggestion has been that the two systems are affected differently by repetition,
466|Automated Intelligent Pilots for Combat Flight Simulation|TacAir-Soar is an intelligent, rule-based system that generates believable &#034;human-like&#034; behavior for large scale, distributed military simulations. The innovation of the application is primarily a matter of scale and integration. The system is capable of executing most of the airborne missions that the United States military flies in fixed-wing aircraft. It accomplishes this by integrating a wide variety of intelligent capabilities, including real-time hierarchical execution of complex goals and plans, communication and coordination with humans and simulated entities, maintenance of situational awareness, and the ability to accept and respond to new orders while in flight. The system is currently deployed at the Oceana Naval Air Station WISSARD Lab and the Air Force Research Laboratory in Mesa, AZ. Its most dramatic use was in the Synthetic Theater of War 1997, which was an operational training exercise that ran for 48 continuous hours during which TacAir-Soar flew all U.S. fixed-wing aircraft.
467|Retrieval of propositional information from long-term memory|Three experiments are reported in which subjects learn propositions like A hippie is in the park. The experiments manipulate the number of such propositions involving a particular person (e.g., hippie) or a par-ticular location (e.g., park). After learning the material, subjects are asked to judge whether particular probe propositions are from the study set. Times to make these judgments about probe propositions increase with the number of study propositions involving the person or location used in the probe proposition. A model is presented which assumes a subject simul-taneously accesses memory from all concepts in a probe proposition and serially searches through all study propositions involving each concept. Search of memory terminates as soon as one search process from a concept finds the probe proposition or exhausts the study propositions attached to that concept. This paper is concerned with how propositional information is re-trieved from long-term memory. Three variations will be reported on a
468|The Fan Effect: New Results and New Theories|The fan effect (Anderson, 1974) has been attributed to interference among competing associations to a concept. Recently, it has been suggested that such effects might be due to multiple mental models (Radvansky, Spieler,  &amp; Zacks, 1993) or suppression of concepts (Anderson &amp; Spellman, 1995; Conway &amp; Engle, 1994). We show that the ACT-R (Adaptive Control of Thought-Rational) theory, which embodies associative interference, is consistent with the Radvansky et al results and we fail to find any evidence for concept suppression in a new fan experiment. The ACT-R model provides good quantitative fits to the results from a variety of experiments. The three key concepts in these fits are (a) the associative strength between two concepts reflect the degree to which one concept predicts the other; (b) foils are rejected by retrieving mismatching facts; and (c) subjects can adjust the relative weights they give to various cues in retrieval. 
469|Locus of feedback control in computer-based tutoring: Impact on learning rate, achievement and attitudes|The advent of second-generation intelligent computer tutors raises an important instructional design question:  when should tutorial advice be presented in problem solving? This paper examines four feedback conditions in the ACT Programming Tutor. Three versions offer the student different levels of control over error feedback and correction: (a) immediate feedback and immediate error correction; (b) immediate error flagging and student control of error correction; (c) feedback on demand and student control of error correction. A fourth, No-tutor condition offers no step-by-step problem solving support. The immediate feedback group with greatest tutor control of problem solving yielded the most efficient learning. These students completed the tutor problems fastest, and the three tutor-
