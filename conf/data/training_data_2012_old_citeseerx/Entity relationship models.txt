ID|Title|Summary
1|The Entity-Relationship Model: Toward a Unified View of Data|A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, infor-mation retrieval, and data manipulation are discussed. The entity-relationship model can be used as a basis for unification of different views of data: t,he network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented. Key Words and Phrases: database design, logical view of data, semantics of data, data models, entity-relationship model, relational model, Data Base Task Group, network model, entity set
2|Data structure diagrams|Successful communication of ideas has been and will continue to be a limiting factor in man&#039;s endeavors to survive and to better his life. The invention of algebra, essentially a graphic technique for communicating truths with respect to classes of arithmetic statements, broke the bond that slowed the development of mathematics. Whereas &#034;12+ 13=25 &#039; &#039; and &#034;3+7 = 10 &#034; and &#034;14+(-2)  = 12&#034; are arithmetic statements, &#034;a+b=c &#039; &#039; is an algebraic statement. In particular, it is an algebraic statement controlling an entire class of arithmetic statements such as those listed. Data Structure Diagrams The Data Structure Diagram is also a graphic technique. It is based on a type of notation dealing with classes--specifically, with classes of entities and the classes of sets that relate them. For example, individual people and automobiles
3|Foundations of Entity-Relationship Modeling|Database design methodologies should facilitate database modeling, effectively support database processing and transform a conceptual schema of the database to a high-performance database schema in the model of the corresponding DBMS. The Entity-Relationship Model is extended to the Higher-order Entity-Relationship Model (HERM) which can be used as a high-level, simple and comprehensive database design model for the complete database information on the structure, operations, static and dynamic semantics. The model has the expressive power of semantic models and possesses the simplicity of the entity-relationship model. The paper shows that the model has a well-founded semantics. Several semantical constraints are considered for this model. 1 Introduction  The problem of database design can be stated as follows: Design the logical and physical structure of a database in a given database management system to contain all the information required by the user and required for an efficient b...
4|On Line Processing of Compacted Relations|Most data base machines use some kind of &amp;quot;filter&amp;quot; that performs unary relational operators (selec-tion and projection) on relations Cl to 71. These filters operate &amp;quot;on the fly &amp;quot; that is, at the speed of the disk, while the relation is being transferred into main memory, Processing time being proportional to relation size, it is therefore important to represent data in the most compacted way. In this paper we address the problem of satisfying the two seemingly contra-dictory requirements: i) finding an &amp;quot;optimal &amp;quot; compaction scheme ii) processing optimally compacted relations on
5|An Integrated Approach to Logical Design of Relational Database Schemes|We propose a new approach to the design of relational database schemes. The main features of the approach are the following: (a) A combination of the traditional decomposition and synthesis approaches, thus allowing the use of both functional and multivalued dependencies. lb) Separation of structural dependencies relevant for the design process from integrity constraints, that is, constraints that do not bear any structural information about the data and which should therefore be discarded at the design stage. This separation is supported hy a simple syntactic test filtering out nonstructural dependencies. (cl Automatic correction of schemes which lack certain desirable properties.
6|Entity-Relationship Modeling Revisited|In this position paper, we argue the modern applications require databases to capture and enforce more domain semantics than traditional applications. We also argue that the best way to incorporate additional semantics into database systems is by capturing the added information in conceptual models and then using it for database design. In this light, we revisit Entity-Relationship models and investigate ways in which such models could be extended to play a role in the process. Inspired by a paper by Rafael Camps Pare ([2]), we suggest avenues of research in the issue.
7|Semantic and schematic similarities between database objects: A context-based approach|Inamultidatabase system, schematic con icts between two objects are usually of interest only when the objects have some semantic similarity. We use the concept of semantic proximity, which is essentially an abstraction/mapping between the domains of the two objects associated with the context of comparison. An explicit though partial context representation is proposed and the speci city relationship between contexts is de ned. The contexts are organized as a meet semi-lattice and associated operations like the greatest lower bound (glb) are de ned. The context of comparison and the type of abstractions used to relate the two objects form the basis of a semantic taxonomy. Atthesemantic level, the intensional description of database objects provided by the context is expressed in a description logic language. Schema correspondences are used to store mappings from the semantic level to the data level and are associated with the respective contexts. Inferences about database content at the federation level are modeled as changes in the context and the associated schema correspondences. We try to reconcile the dual (schematic and semantic) perspecitves by: enumerating possible semantic similarities between objects having schema and data conicts, and modeling schema correspondences as the projection of semantic proximity wrt context. 1
8|View integration: A step forward in solving structural conflicts| Thanks to the development of the federated systems approach on the one hand and the emphasis on user involvement in database design on the other, the interest in schema integration techniques is significantly increasing. Theories, methods and sometime tools have been proposed. Conflict resolution is the key issue. Different perceptions by schema designers may lead to different representations. A way must be found to support these different representations within a single system. Most current integration methodologies rely on modification of initial schemas,to solve the conflicts. This approach needs a strong interaction with the database administrator, who has authority to modify the initial schemas. This paper presents an approach to view integration specifi-cally intended to support the coexistence of different representa-tions of the same real-world objects. The main characteristics of
9|Schema Equivalence in Heterogeneous Systems: Bridging Theory and Practice|Current theoretical work offers measures of schema equivalence based on the information capacity of schemas. This work is based on the existence of abstract functions satisfying various restrictions between the sets of all instances of two schemas. In considering schemas that arise in practice, however, it is not clear how to reason about the existence of such abstract functions. Further, these notions of equivalence tend to be too liberal in that schemas are often considered equivalent when a practitioner would consider them to be different. As a result, practical integration methodologies have not utilized this theoretical foundation and most of them have relied on ad-hoc approaches. We present results that seek to bridge this gap. First, we consider the problem of deciding information capacity equivalence and dominance of schemas that occur in practice, i.e., those that can express inheritance and simple integrity constraints. We show that this problem is undecidable. This undecidab...
10|Using Schematically Heterogeneous Structures|Schematic heterogeneity arises when information that is represented as data under one schema, is represented within the schema (as metadata) in another. Schematic heterogeneity is an important class of heterogeneity that arises frequently in integrating legacy data in federated or data warehousing applications. Traditional query languages and view mechanisms are insufficient for reconciling and translating data between schematically heterogeneous schemas. Higher order query languages, that permit quantification over schema labels, have been proposed to permit querying and restructuring of data between schematically disparate schemas. We extend this work by considering how these languages can be used in practice. Specifically, we consider a restricted class of higher order views and show the power of these views in integrating legacy structures. Our results provide insights into the properties of restructuring transformations required to resolve schematic discrepancies. In addition, we ...
11|Semantic vs. Structural Resemblance of Classes|We present an approach to determine the similarity of classes which utilizes fuzzy and incomplete terminological knowledge together with schema knowledge. We clearly distinguish between semantic similarity determining the degree of resemblance according to real world semantics, and structural correspondence explaining  how classes can actually be interrelated. To compute the semantic similarity we introduce the notion of semantic relevance and apply fuzzy set theory to reason about both terminological knowledge and schema knowledge. 1 Introduction  The identification of similar or corresponding concepts forms one of the main steps when investigating different world models and relating them to each other. Apart from its long tradition in document retrieval, this issue has also been investigated in more structured frameworks such as schema independent query formulation, e.g., [Mot90], or database integration, where for a survey you may look at [SL90]. As argued in [GPN91], there should b...
12|Entity-Relationship Models and Sketches|Entity-Relationship models are a common tool for specification and design of information systems. They use a graphical technique for displaying the objects of the system and relatioships among them. The design process can be enhanced by specifying constraints of the system and the natural environment for these is the categorical notion of sketch. Here we argue that the finite-limit, finite-coproduct sketches with a terminal node are the appropriate class and call them ER sketches. A model for an ER sketch is a `snapshot&#039; of a database. The models of an ER sketch in a lextensive category are a category with determined values on the entities and additional properties in case the entities have key attributes. Among the models is the query language associated to the ER sketch. We also show that database updates are simply spans of models. Moreover the category of updates of models of an ER sketch is the category of models of the sketch in the 2-category of spans.    Research partially supp...
14|A 2-Categorical Approach To Change Of Base And Geometric Morphisms II|We introduce a notion of equipment which generalizes the earlier notion of pro-arrow equipment and includes such familiar constructs as relK, spnK, parK, and proK for a suitable category K, along with related constructs such as the V-pro arising from a suitable monoidal category V. We further exhibit the equipments as the objects of a 2-category, in such a way that arbitrary functors F: L ? K induce equipment arrows relF: relL ? relK, spnF: spnL ? spnK, and so on, and similarly for arbitrary monoidal functors V ? W. The article I with the title above dealt with those equipments M having each M(A, B) only an ordered set, and contained a detailed analysis of the case M = relK; in the present article we allow the M(A, B) to be general categories, and illustrate our results by a detailed study of the case M = spnK. We show in particular that spn is a locally-fully-faithful 2-functor to the 2-category of equipments, and determine its image on arrows. After analyzing the nature of adjunctions in the 2-category of equipments, we are able to give a simple characterization of those spnG which arise from a geometric morphism G. 
16|Relational Databases and Indexed Categories|. A description of relational databases in categorical terminology given here has as intended application the study of database dynamics, in particular we view (i) updates as database objects in a suitable category indexed by a topos; (ii) L-fuzzy databases as database objects in sheaves. Indexed categories are constructed to model the databases on a fixed family of domains and also all databases for a varying family of domains. Further, we show that the process of constructing the relational completion of a relational database is a monad in a 2-category of functors. Introduction  We use the term relation for a subobject of a finite product of objects in a category. Following the relational database literature, we use the term domain  for an object of the ambient category (and warn readers that these are not  the ordered objects which go by the name &#034;domain&#034; elsewhere in theoretical Computer Science.) A relational database, as defined by E. F. Codd [3], is first of all a family of rela...
17|Leveled Entity Relationship Model|The Entity-Relationship formalism, introduced in the mid-seventies, is an extensively used tool for database design. The database community is now involved in building the next generation of database systems. However, there is no effective formalism similar to ER for modeling the complex data in these systems. We propose the Leveled Entity Relationship (LER) formalism as a step towards fulfilling such a need. An essential characteristic of these next-generation systems is that a data element is no longer assumed to be atomic, instead it could have a complex internal structure. The LER formalism models structured data by leveling ER diagrams such that deeper layers provide greater structural detail. Moreover, elements deep inside one structure reference elements deep inside another structure. LER cleanly formalizes the relation between such deep structural elements. 
19|Extending the Database Relational Model to Capture More Meaning|During the last three or four years several investigators have been exploring “semantic models ” for formatted databases. The intent is to capture (in a more or less formal way) more of the meaning of the data so that database design can become more systematic and the database system itself can behave more intelligently. Two major thrusts are clear: (I) the search for meaningful units that are as small as possible--atomic semantics; (2) the search for meaningful units that are larger than the usual n-ary relation-molecular semantics. In this paper we propose extensions to the relational model to support certain atomic and molecular semantics. These extensions represent a synthesis of many ideas from the published work in semantic modeling plus the introduction of new rules for insertion, update, and deletion, as well as new algebraic operators.
20|Distilled Entity Relationship Model |We formalize the syntax of ER-diagrams, trying to expli-cate details left unclear in the literature either by neglect or by a lack of consensus with regard to the basic elements of ER-modelling and the restrictions on their use. Some of these restrictions and the consequences of relaxing them are discussed.
22|Database Abstractions|We present a method for deriving shape space parameters that are consistent with immunological data, and illustrate the method by deriving shape space parameters for a model of cross-reactive memory. Cross-reactive memory responses occur when the immune system is primed by one strain of a pathogen and challenged with a related, but different, strain. Much of the nature of a cross-reactive response is determined by the quantity and distribution of the memory cells, raised to the primary antigen, that cross-react with the secondary antigen. B cells with above threshold affinity for an antigen lie in a region of shape space that we call a ball of stimulation. In a cross-reactive response, the intersection of the balls of stimulation of the primary and secondary antigens contains the cross-reactive B cells and thus determines the degree of cross-reactivity between the antigens. We derive formulas for the volume of intersection of balls of stimulation in different shape spaces and show that the parameters of shape space, such as its dimensionality, have a large impact on the number of B cells in the intersection. The application of our method for deriving shape space parameters indicates that, for Hamming shape spaces, twenty to twenty-five dimensions, a three or four letter alphabet, and balls of stimulation of radius five or six, are choices that match the experimental data. For Euclidean shape spaces, five to eight dimensions and balls of stimulation with radius about twenty percent of the radius of the whole space, match the experimental data.
23|XER — Extensible Entity Relationship Modeling|XML is one of the premier formats for data representation and interchange. Many organizations are starting to store data in XML and using XML as an intermediate format for publication and use of these documents. Most database systems have support for storing data in XML or internally representing XML data for storage. However, XML does not have a suitable mechanism for intuitively creating a conceptual model for the data and cannot automatically or semi-automatically generate the schema for the actual data storage. The area of designing conceptual modeling techniques for XML is still not adequately explored in literature. In this paper we describe the XER (Extensible Entity-Relationship) model, a conceptual modeling approach that can describe XML document structures in a simple visual form reminiscent of the ER model, and has the capability to automatically generate XML document type definitions and schema from such structures.
24|ABSTRACT Event-Entity-Relationship Modeling |In Data Warehouse Environments1 We use the event-entity-relationship model (EVER) to illustrate the use of entity-based modeling languages for conceptual schema design in data warehouse environments. EVER is a general-purpose information modeling language that supports the specification of both general schema structures and multi-dimensional schemes that are customized to serve specific information needs. EVER is based on an event concept that is very well suited for multidimensional modeling because measurement data often represent events in multi-dimensional databases.
25|Temporal databases|A temporal database (see Temporal Database) contains time-varying data. Time is an important aspect of all real-world phenomena. Events occur at specific points in time; objects and the relationships among objects exist over time. The ability to model this temporal dimension of the real world is essential to many computer applications, such as accounting, banking, econometrics, geographical information systems, inventory control, law, medical records, multi-media, process control, reservation systems, and scientific data analysis. Conventional databases represent the state of an enterprise at a single moment of time. Although the contents of the database continue to change as new information is added, these changes are viewed as modifications to the state, with the old, out-of-date data being deleted from the database. The current contents of the database may be viewed as a snapshot of the enterprise. When a conventional database is used, the attributes involving time are manipulated solely by the application programs, with little help
26|Database Description with SDM: A Semantic Database Model|SDM is a high-level semantics-based database description and structuring formalism (database model) for databases. This database model is designed to capture more of the meaning of an application environment than is possible with contemporary database models. An SDM specification describes a database in terms of the kinds of entities that exist in the application environment, the classifications and groupings of those entities, and the structural interconnections among them. SDM provides a collection of high-level modeling primitives to capture the semantics of an application environment. By accommodating derived information in a database structural specification, SDM allows the same information to be viewed in several ways; this makes it possible to directly accommodate the variety of needs and processing requirements typically present in database applications. The design of the present SDM is based on our experience in using a preliminary version of it. SDM is designed to enhance the effectiveness and usability of database systems. An SDM database description can serve as a formal specification and documentation tool for a database; it can provide a basis for supporting a variety of powerful user interface facilities, it can serve as a conceptual database model in the database design process; and, it can be used as the database model for a new kind of database management system.
27|A Methodological Framework for Data Warehouse Design|Though designing a data warehouse requires techniques completely different from those adopted for operational systems, no significant effort has been made so far to develop a complete and consistent design methodology for data warehouses. In this paper we outline a general methodological framework for data warehouse design, based on our Dimensional Fact Model (DFM). After analyzing the existing information system and collecting the user requirements, conceptual design is carried out semi-automatically starting from the operational database scheme. A workload is then characterized in terms of data volumes and expected queries, to be used as the input of the logical and physical design phases whose output is the final scheme for the data warehouse. Keywords Data warehouse, design methodology, conceptual model. 1. INTRODUCTION The database community is devoting increasing attention ...
28|Queries and query processing in object-oriented database systems|One of the basic functionalities of database management systems (DBMSs) is to be able to process declarative user queries. The first generation of object-oriented DBMSs did not provide declarative query capabilities. However, the last decade has seen significant research in defining query models (including calculi, algebra and user languages) and in techniques for processing and optimizing them. Many of the current commercial systems provide at least rudimentary query capabilities. In this chapter we discuss the techniques that have been developed for processing object-oriented queries. Our particular emphasis is on extensible query processing architectures and techniques. The other chapters in this book on query languages and optimization techniques complement this chapter.  
29|Multidimensional Data Modeling  for Complex Data|Systems for On-Line Analytical Processing (OLAP) considerably ease the process of analyzing business  data and have become widely used in industry. OLAP systems primarily employ multidimensional  data models to structure their data. However, current multidimensional data models fall short in their  ability to model the complex data found in some real-world application domains. The paper presents  nine requirements to multidimensional data models, each of which is exemplified by a real-world, clinical  case study. A survey of the existing models reveals that the requirements not currently met include  support for many-to-many relationships between facts and dimensions, built-in support for handling  change and time, and support for uncertainty as well as different levels of granularity in the data. The  paper defines an extended multidimensional data model, which addresses all nine requirements. Along  with the model, we present an associated algebra, and outline how to implement the model using relational  databases.
30|A Data Warehouse Conceptual Data Model for Multidimensional Aggregation: a preliminary report|This paper presents a proposal for a Data Warehouse Conceptual Data Model which allows for the description of both the relevant aggregated entities of the domain--- together with their properties and their relationships with other relevant entities---and the relevant dimensions involved in building the aggregated entities. The proposed Data Warehouse Conceptual Data Model is able to capture the database schemata expressed in the most interesting traditional Semantic Data Models and Object-Oriented Data Models; it is able to introduce complex descriptions of the structure of aggregated entities and multiply hierarchically organised dimensions; it is based on Description Logics, a class of formalisms for which it is possible to study the expressivity in relation with decidability of reasoning problems and completeness of algorithms; it supports the most important reasoning services for the basic Data Warehouse operations. 1 Introduction  Data Warehouse---and especially OLAP---application...
31|A principled approach to data integration and reconciliation|Integration is one of the most important aspects of a Data Warehouse. When data passes from the sources of the application-oriented operational environment to the Data Warehouse, possible inconsistencies and redundancies should be resolved, so that the warehouse is able to provide an integrated and reconciled view of data of the organization. We describe a novel approach to data integration and reconciliation, based on a conceptual representation of the Data Warehouse application domain. The main idea is to declaratively specify suitable matching, conversion, and reconciliation operations to be used in order to solve possibile conflicts among data in different sources. Such a specification is provided in terms of the conceptual model of the application, and is effectively used during the design of the software modules that load the data from the sources into the Data Warehouse. 1
32|The Entity-Relationship Model for Multilevel Security|A design environment for security critical database applications that  should be implemented by using multilevel technology is proposed. For this  purpose, the Entity-Relationship model is extended to capture security  semantics. Important security semantics are defined and a language to express  them in an ER model by means of security constraints is developed. The main  contribution consists of the development and implementation of a rule-based  system with which security semantics specified may be checked for conflicting  constraints. The check involves application independent as well as application  dependent integrity constraints and leads to a non conflicting conceptual  representation of the security semantics of a multilevel secure database  application.  1 
33|Toward a Multilevel Secure Relational Data Model|Although there are several e#orts underway to build multilevel secure relational database management systems, there is no clear consensus regarding what a multilevel secure relational data model exactly is. In part this lack of consensus on fundamental issues reflects the subtleties involved in extending the classical (single-level) relational model to a multilevel environment. Our aim in this paper is to discuss the most fundamental aspects of the multilevel secure relational model. Specifically, we consider two requirements: entity integrity and update semantics. Our overall goal is to preserve as much as possible the simplicity and flexibility of the relational model without sacrificing security in the process. 
34|Entity-Relationship Models of Information Artefacts |Data modelling reveals the internal structure of an information system, abstracting away  from details of the physical representation. We show that entity-relationship modelling, a  well-tried example of a data-modelling technique, can be applied to both interactive and  non-interactive information artefacts in the domain of HCI. By extending the conventional  ER notation slightly (to give ERMIA, Entity-Relationship Modelling for Information  Artefacts) it can be used to describe differences between different representations of the same  information, differences between usersconceptual models of the same device, and the  structure and update requirements of distributed information in a worksystem. It also yields  symbolic-level estimates of Card et al.s (1994) index of cost-of-knowledge in an information  structure, plus a novel index, the cost-of-update; these symbolic estimates offer a useful  complement to the highly detailed analyses of time costs obtainable from GOMS-like  models. We conclude that, as a cheap, coarse-grained, and easy-to-learn modelling  technique, ERMIA usefully fills a gap in the range of available HCI analysis techniques.  
35|Beyond Interface Builders: Model-Based Interface Tools|Interface builders only support the construction of the menus and dialogue boxes of an application. They do not support the construction of interfaces of many application classes (visualization, simulation, command and control, domain-specific editors) because of the dynamic and complex information that these applications process. HUMANOID is a model-based interface design and construction tool where interfaces are specified by building a declarative description (model) of their presentation and behavior. HUMANOID&#039; s modeling language provides simple abstraction, iteration and conditional constructs to model the interface features of these application classes. HUMANOID provides an easy-touse designer&#039;s interface that lets designers build complex interfaces without programming.
36|Formal Methods for Interactive Systems|This material is copyright. You must include this page with any portion of the book. Please refer to book web site for distribution conditions. Please note that as the book has been re-typeset for electronic distribution, page numbers may difer slightly from the original. CONTENTS Preface
37|Artifact as theory-nexus: hermeneutics meets theory-based design|We suggest hat HCI designs characteristically em-body multiple, distinct psychological c aims, that vir-tually every aspect of a system&#039;s usability is overdetermined by independent psychological ration-ales inherent in its design. These myriad claims cohere in being implemented together in a running system. Thus, ItCI artifacts themselves are perhaps the most effective medium for theory development in ItCI. We advance a framework for articulating the psychological claims embodied by artifacts. This proposal reconciles the contrasting perspectives of theory-based esign and hermeneutics, and clarifies the apparent paradox of ItCI application leading HCI theory.
38|Relational database: A practical foundation for productivity|It is the Association&#039;s foremost award for technical contributions to the com-puting community. (2odd was selected by the A(2M General Technical Achievement Award (2ommittee for his &#034;fundamental and continuing contributions to the theory and practice of database management systems. &#034; The originator of the relational model for databases, (2odd has made further important contributions in the development of relational algebra, relational calculus, and normalization of relations. Edgar F. (2odd joined IBM in 1949 to prepare programs for the Selective Sequence Electronic Calculator. Since then, his work in computing has encom-passed logical design of computers (IBM 701 and Stretch), managing a computer center in Canada, heading the development ofone of the first operating systems with a general multiprogramming capability, contributing to the logic of self-reproducing automata, developing high level techniques for software specifica-tion, creating and extending the relational approach to database management, and developing an English analyzing and synthesizing subsystem for casual users of relational databases. He is also the author of Cellular Automata, n early volume in the A(2M Monograph Series.
39|Deliberated Evolution: Stalking the View Matcher in Design Space|Technology development in HCI can be interpreted as a co-evolution of tasks and artifacts. The tasks people actually engage in (successfully or problematically) and those they wish to engage in (or perhaps merely to imagine) define requirements for future technology, and specifically for new HCI artifacts. These artifacts, in turn, open up new possibilities for human tasks, new ways to do familiar things, entirely new kinds of things to do. In this paper we describe psychological design rationale as an approach to augmenting HCI technology development and to clarifying the sense in which HCI artifacts embody psychological theory. A psychological design rationale is an enumeration of the psychological claims embodied by an artifact for the situations in which it is used. As an example, we present our design work with the View Matcher, a Smalltalk programming environment for coordinating multiple views of an example application. In particular, we show how psychological design rationale was used to develop a view matcher for code reuse from prior design rationales for related programming tasks and environments. 1. TASKS AND ARTIFACTS  In 1605, Sir Francis Bacon called for a &#034;natural history of trades.&#034; He urged that technical tools, techniques and processes be made more public and explicit. This was one element in his broader project of developing practical science, and hinged on the assumption that if such knowledge could be more systematically considered and integrated, human progress would necessarily result. Thus, Bacon suggested that new concepts and inventions would result &#034;by a connexion and transferring of the observations of one Arte, to the use of another, when the experiences of several misteries shall fall under the consideration of one man&#039;s minde.&#034;(1970: Book...
40|Knowledgeable Development Environments Using Shared Design Models|We describe MASTERMIND, a step toward our vision of a knowledge-based design-time and run-time environment where human-computer interfaces development is centered around an all-encompassing design model. The MASTERMIND approach is intended to provide integration and continuity across the entire life cycle of the user interface. In addition it facilitates higher quality work within each phase of the life cycle. MASTERMIND is an open framework, in which the design knowledge base allows multiple tools to come into play and makes knowledge created by each tool accessible to the others.  KEYWORDS: models, collaboration, design, development  INTRODUCTION  The challenge facing the research community is to provide the bass for an effective, integrated suite of tools to support the entire lifecycle of an interface. This means that the tools must be given a great deal more knowledge than they currently have about the product they are intended to construct. It means that this knowledge must be pr...
41|Task Analysis and System Design: The Discipline of Data|this paper, Diaper and Addison (Diaper and Addison, 1991) defended TA in general and TAKD (e.g. Diaper, 1989b) in particular against the criticisms and made a number of assertions about my stance regarding humans and computers. In this reply, I intend to clarify firstly what I mean by a data-centred view, secondly what I mean by device independence and finally why the datacentred view is a vital part of systems development. In doing this I hope to deal with many of the specific points raised by Diaper and Addison. Underlying this discussion is a view of how system designers should undertake their designs and a theoretical, or philosophical basis of human-computer interaction (HCI) provided by the data-centred view. Whilst both of these are important subjects, there is neither the space nor the time to give adequate coverage here. However, I am sure that both the theory and the practice of HCI will benefit from this continuing debate on the preferred role of task analysis in system design. My original concern about TA stemmed from the claim made by its advocates that task analysis is potentially the most powerful method in HCI...for producing requirements specifications (Diaper 1989a, preface). I believe this view is unsatisfactory because TA does not focus sufficiently on the data in the system. This paper is aimed at explicating this position. The central part of this paper is an exposition of data models as conceptual models which capture both a human and a computer view of systems. This discussion is necessary in order to understand the notion of device independence which is presented in the following section. Before turning to the details of data, it is important to obtain a perspective on system development
42|An Entity-Relationship Model for Forest Inventory |This paper presents a general data model for forest inventory and management. The data model is based on the entity-relationship model and it can be implemented by relational database management systems. The data model can be used for inventories based on various kinds of sampling designs, and for different forest resource management systems from a forest stand to a national level. The data model supports data needs of complex forest surveys and integration of modules of forest information systems (geographic information systems, database management systems, computation, graphical user interface). The data model can be also applied to other natural resource data management cases.
43|A Comparative Analysis of Methodologies for Database Schema Integration| One of the fundamental principles of the database approach is that a database allows a nonredundant, unified representation of all data managed in an organization. This is achieved only when methodologies are available to support integration across organizational and application boundaries. 

Methodologies for database design usually perform the design activity by separately producing several schemas, representing parts of the application, which are subsequently merged. Database schema integration is the activity of integrating the schemas of existing or proposed databases into a global, unified schema. The aim of the paper is to provide first a unifying framework for the problem of schema integration, then a comparative review of the work done thus far in this area. Such a framework, with the associated analysis of the existing approaches, provides a basis for identifying strengths and weaknesses of individual methodologies, as well as general guidelines for future improvements and extensions.
44|Semantic database modeling: Survey, applications, and research issues|Most common database management systems represent information in a simple record-based format. Semantic modeling provides richer data structuring capabilities for database applications. In particular, research in this area has articulated a number of constructs that provide mechanisms for representing structurally complex interrelations among data typically arising in commercial applications. In general terms, semantic modeling complements work on knowledge representation (in artificial intelligence) and on the new generation of database models based on the object-oriented paradigm of programming languages. This paper presents an in-depth discussion of semantic data modeling. It reviews the philosophical motivations of semantic models, including the need for high-level modeling abstractions and the reduction of semantic overloading of data type constructors. It then provides a tutorial introduction to the primary components of semantic models, which are the explicit representation of objects, attributes of and relationships among objects, type constructors for building complex types, ISA relationships, and derived schema components. Next, a survey of the prominent semantic models in the literature is presented. Further, since a broad area of research has developed around semantic modeling, a number of related topics based on these models are discussed, including data languages, graphical interfaces, theoretical investigations, and physical implementation strategies.
45|A Logical Design Methodology for Relational Databases Using the Extended Entity-Relationship Model|A database design methodology is defined for the design of large relational databases. First, the data requirements are conceptualized using an extended entity-relationship model, with the extensions being additional semantics such as ternary relationships, optional relationships, and the generalization abstraction. The extended entity-relationship model is then decomposed according to a set of basic entity-relationship constructs, and these are transformed into candidate relations. A set of basic transformations has been developed for the three types of relations: entity relations, extended entity relations, and relationship relations. Candidate relations are further analyzed and modified to attain the highest degree of normalization desired. The methodology produces database designs that are not only accurate representations of reality, but flexible enough to accommodate future processing requirements. It also reduces the number of data dependencies that must be analyzed, using the extended ER model conceptualization, and maintains data integrity through normalization. This approach can be implemented manually or in a simple software package as long as a “good ” solution is acceptable and absolute optimality is not required.
46|A Generic Entity-Relationship Model|this paper will try to justify it, these models can be considered as variants of a more general information model
47|On The Power Of Languages For The Manipulation Of Complex Objects|Various models and languages for describing and manipulating hierarchically structured data have been proposed. Algebraic, calculus-based and logic-programming oriented languages have all been considered. This paper presents a general model for complex objects, and languages for it based on the three paradigms. The algebraic language generalizes those presented in the literature; it is shown to be related to the functional style of programming advocated by Backus. The notion of domain independence familiar from relational databases is defined, and syntactic restrictions (referred to as safety conditions) on calculus queries are formulated, that guarantee domain independence. The main results are: The domain-independent calculus, the safe calculus, the algebra, and the logic-programming oriented language have equivalent expressive power. In particular, recursive queries, such as the transitive closure, can be expressed in each of the languages. For this result, the algebra needs the pow...
48|Mining Association Rules in Entity-Relationship Modeled Databases|. Current data mining algorithms handle databases consisting  of a single table. This paper addresses the problem of mining association  rules in databases consisting of multiple tables and designed using the  entity-relationship model. We discuss previous approaches to this problem  and point out some unaddressed issues, and we present a couple of  algorithms to address these issues and experimental results showing the  scalability of these algorithms with respect to the increase in size of the  database. The paper concludes with a discussion of the possibility of extending  our algorithms to database schemas more complex than a star  schema.  Keywords: entity-relationship model, itemset, association rule, entity support, join support  1 
49|Mining Association Rules between Sets of Items in Large Databases|We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm. 
50|Mining Frequent Patterns  without Candidate Generation: A Frequent-Pattern Tree Approach|Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist a large number of patterns and/or long patterns. In this study,  we propose a novel
frequent-pattern tree
(FP-tree) structure, which is an extended prefix-tree
structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-
based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth.
Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a condensed,
smaller data structure, FP-tree which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts
a pattern-fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a
partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for
mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance
study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns,
and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported
new frequent-pattern mining methods
51|Dynamic Itemset Counting and Implication Rules for Market Basket Data|We consider the problem of analyzing market-basket data and present several important contributions. First, we present a new algorithm for finding large itemsets which uses fewer passes over the data than classic algorithms, and yet uses fewer candidate itemsets than methods based on sampling. We investigate the idea of item reordering, which can improve the low-level efficiency of the algorithm. Second, we present a new way of generating &#034;implication rules,&#034; which are normalized based on both the antecedent and the consequent and are truly implications (not simply a measure of co-occurrence), and we show how they produce more intuitive results than other methods. Finally, we show how different characteristics of real data, as opposed to synthetic data, can dramatically affect the performance of the system and the form of the results.  1 Introduction  Within the area of data mining, the problem of deriving associations from data has recently received a great deal of attention. The prob...
52|A tree projection algorithm for generation of frequent itemsets|In this paper we propose algorithms for generation of frequent itemsets by successive construction of the nodes of a lexicographic tree of itemsets. We discuss di erent strategies in generation and traversal of the lexicographic tree such as breadth- rst search, depth- rst search or a combination of the two. These techniques provide di erent trade-o s in terms of the I/O, memory and computational time requirements. We use the hierarchical structure of the lexicographic tree to successively project transactions at each node of the lexicographic tree, and use matrix counting on this reduced set of transactions for nding frequent itemsets. We tested our algorithm on both real and synthetic data. We provide an implementation of the tree projection method which is up to one order of magnitude faster than other recent techniques in the literature. The algorithm has a well structured data access pattern which provides data locality and reuse of data for multiple levels of the cache. We also discuss methods for parallelization of the
53|Galois Connections and Data Mining|: We investigate the application of Galois connections to the identication of frequent item sets, a central problem in data mining. Starting from the notion of closure generated by a Galois connection, we dene the notion of extended closure, and we use these notions to improve the classical Apriori algorithm. Our experimental study shows that in certain situations, the algorithms that we describe outperform the  Apriori algorithm. Also, these algorithms scale up linearly.  Key Words: Galois connection, closure, extended closure, support, frequent set of items  Category: H.2.0, E.5 1 
54|Software Cost Estimation through Entity Relationship Model |Abstract: Software Cost Estimation is essential for efficient control and management of the whole software development process. Today, Constructive Cost Model (COCOMO 11) is very popular for estimating software cost. In Constructive Cost Model lines of code and function, points are used to calculate the software size. Actually, this work represents the implementation stages but in early stages in software development, it was not easy to estimate software cost. The entity relationship model (ER Model) is very useful in requirement analysis for data concentrated systems. This paper highlights the use of Entity Relationship Model for software cost estimation. Pathway Density is ushered in. By using the Pathway Density and other factors, many regression models are built for estimating the software cost. So in this paper, Entity Relationship Model is based on estimated cost of software. [Journal of
55|Using Rough Sets with Heuristics for Feature Selection|Practical machine learning algorithms are known to degrade in performance (prediction accuracy) when faced with many features (sometimes attribute is used instead of feature) that are not necessary for rule discovery. To cope with this problem, many methods for selecting a subset of features have been proposed. Among such methods, the filter approach that selects a feature subset using a preprocessing step, and the wrapper approach that selects an optimal feature subset from the space of possible subsets of features using the induction algorithm itself as a part of the evaluation function, are two typical ones. Although the filter approach is a faster one, it has some blindness and the performance of induction is not considered. On the other hand, the optimal feature subsets can be obtained by using the wrapper approach, but it is not easy to use because of the complexity of time and space. In this paper, we propose an algorithm which is using rough set theory with greedy heuristics for feature selection. Selecting features is similar to the filter approach, but the evaluation criterion is related to the performance of induction. That is, we select the features that do not damage the performance of induction.
56|An Investigation of Machine Learning Based Prediction Systems|Traditionally, researchers have used either off-the-shelf models such as COCOMO, or developed local models using statistical techniques such as stepwise regression, to obtain software effort estimates. More recently, attention has turned to a variety of machine learning methods such as artificial neural networks (ANNs), case-based reasoning (CBR) and rule induction (RI). This paper outlines some comparative research into the use of these three machine learning methods to build software effort prediction systems. We briefly describe each method and then apply the techniques to a dataset of 81 software projects derived from a Canadian software house in the late 1980s. We compare the 1  Chris Schofield is now with Nortel (cscho@nortelnetworks.com).  --- 2 ---  prediction systems in terms of three factors: accuracy, explanatory value and configurability. We show that ANN methods have superior accuracy and that RI methods are least accurate. However, this view is somewhat counteracted by pr...
57|Entity-Relationship Modeling Re-revisited |ABSTRACT. Since its introduction, the Entity-Relationship (ER) model has been the vehicle of choice in communicating the structure of a database schema in an implementation-independent fashion. Part of its popularity has no doubt been due to the clarity and simplicity of the associated pictorial Entity-Relationship Diagrams (“ERD’s”) and to the dependable mapping it affords to a relational database schema. Although the model has been extended in different ways over the years, its basic properties have been remarkably stable. Even though the ER model has been seen as pretty well “settled, ”  some recent papers, notably [4] and [2 (from whose paper our title is derived)], have enumerated what their authors consider serious shortcomings of the ER model. They illustrate these by some interesting examples. We believe, however, that those examples are themselves flawed. In fact, while not claiming that the ER model is perfect, we do believe that the overhauls hinted at are probably not necessary and possibly counterproductive.
58|Analysis of Binary/Ternary Cardinality Combinations in Entity-Relationship Modeling|In this paper, we discuss the simultaneous existence, and relationships, between binary and ternary relationships in entity-relationship (ER) modeling. We define the various interpretations that can be applied to the simultaneous existence of ternary and binary relationships having the same participating entities. We have identified that only certain cardinalities are permitted to exist simultaneously in such ER structures. We demonstrate which binary relationship cardinalities are permitted within ternary relationships, during ER modeling. We develop an Implicit Binary Cardinality (IBC) rule, which states that, in any ternary relationship, the cardinality of any binary relationship embedded in the ternary, is many-to-many when there are no explicit constraints on the data instances. We then present an Explicit Binary Permission (EBP) rule, which explains and enumerates all permitted binary relationships for various cardinalities of ternary relationships. Finally we present an Implicit Binary Override (IBO) rule, which states that the implicit binary cardinalities can be constrained in a ternary relationship by an explicitly imposed binary relationship. We then use these rules to consider the further implicit dynamics of ternary relationships when multiple binary relationships are imposed.
59|Binary Equivalents of Ternary Relationships in Entity-Relationship Modeling: a Logical Decomposition Approach|Little work has been completed which addresses the logical composition and use of
60|An Analysis of Cardinality Constraints in Redundant Relationships|In this paper, we present a complete analysis of redundant
62|Entity Relationship Model…………………..Page 153 |For my senior project, I will create a website for my hometown church. The project will entail constructing several active server pages, a database, and communicating with the client as the project progresses. At this point, I have conversed with the client about preliminary website specifications. The site will require the implementation of three types of user roles: administrators, site members, and general users. Administrators will have complete access to the website, and will have unrestricted permission to update, delete, and add content. The member user will be able to read church news, access Sunday school class information, join/participate in discussion groups, download files, and view the online church directory. A general user is defined as someone who comes across the site on the web and has no affiliation with the church. This user will have access to general church information (i.e. address, contact information, etc.). Each of these user roles will be further elaborated upon in the functional specification. Another large feature of this website is that it will be mostly updateable via active server pages. Administrators will have the ability to update web pages/database
63|Rapid Prototyping of Extended Entity-Relationship Models |Abstract: The entity-relationship model is considered to be the standard for conceptual design of data in information systems and relational databases. Extended entity-relationship models provide extra concepts such as generalization, dependency and classification. In the context of objectoriented software development, such models are able to represent part of the functionality whereas more operational functionality is implemented in the object-oriented language. In the resulting object-oriented program, however, the original data model is often lost and the relationship and dependency constraints are not enforced. We propose an approach which consists of an active modeling phase and an interactive prototyping phase. The overall result is a model in which the elements have active links to fully operational implementation objects, which in turn are actively constrained by the model. We have developed a system that supports the manual steps in these two phases and fully implements the (semi-)automatic steps.
64|SELF: The power of simplicity| SELF is an object-oriented language for exploratory programming based on a small number of simple and concrete ideas: prototypes, slots, and behavior. Prototypes combine inheritance and instantiation to provide a framework that is simpler and more flexible than most object-oriented languages. Slots unite variables and procedures into a single construct. This permits the inheritance hierarchy to take over the function of lexical scoping in conventional languages. Finally, because SELF does not distinguish state from behavior, it narrows the gaps between ordinary objects, procedures, and closures. SELF’s simplicity and expressiveness offer new insights into objectoriented computation.  
65|Using Prototypical Objects to Implement Shared Behavior in Object Oriented Systems|A traditional philosophical controversy between representing general concepts as abstract sets or classes and representing concepts as concrete prototypes is reflected in a controversy between two mechanisms for sharing behavior between objects in object oriented programming languages. Inheritance splits the object world into classes, which encode behavior shared among a group of instances, which represent individual members of these sets. The class/instance distinction is not needed if the alternative of using prototypes is adopted. A prototype represents the default behavior for a concept, and new objects can re-use part of the knowledge stored in the prototype by saying how the new object differs from the prototype. The prototype approach seems to hold some advantages for representing default knowledge, and incrementally and dynamically modifying concepts. Delegation is the mechanism for implementing this in object oriented languages. After checking its idiosyncratic behavior, an ob...
66|OdeView: The Graphical Interface to Ode|OdeView is the graphical front end for Ode, an object-oriented database system and environment. Ode&#039;s data model supports data encapsulation, type inheritance, and complex objects. OdeView provides facilities for examining the database schema (i.e., the object type or class hierarchy), examining class definitions, browsing objects, following chains of references starting from an object, synchronized browsing, displaying selected portions of objects (projection), and retrieving objects with specific characteristics (selection).  OdeView does not need to know about the internals of Ode objects. Consequently, the internals of specific classes are not hardwired into OdeView and new classes can be added to the Ode database without requiring any changes to or recompilation of OdeView. Just as OdeView does not know about the object internals, class functions (methods) for displaying objects are written without knowing about the specifics of the windowing software used by OdeView or the graphi...
67|Mapping UML Associations into Java Code|Object-oriented programming languages do not contain syntax or semantics to express associations directly. Therefore, UML associations have to be implemented by an adequate combination of classes, attributes and methods. This paper presents some principles for the implementation of UML binary associations in Java, paying special attention to multiplicity, navigability and visibility. Our analysis has encountered some paradoxes in the specification of visibility for bidirectional associations. These principles have been used to write a series of code patterns that we use in combination with a tool that generates code for associations, which are read from a model stored in XMI format.
68|Super - Visual Interfaces For Object+Relationship Data Models|SUPER is an exploratory project into the next generation of user-DBMS interfaces. Its main objective is to demonstrate that a visual paradigm can lead to powerful and user-friendly interfaces supporting all phases of the database life cycle (i.e., creation, manipulation and evolution). Visual interaction in SUPER is based on direct manipulation of objects and functions, with a special focus on providing users with maximum flexibility and independence from database technicalities. The set of tools offers facilities to meet the varied demands from categories of users with different levels of skill. Diagrammatic representations and a basic set of functions are better suited for novice and occasional users, while menus and dialog boxes speed up the dialog for expert users. At the same time, a consistent interaction style over the various functions and tools has been emphasized. SUPER has been designed as a front-end to a relational or an object-oriented DBMS, i.e., the persistence of data,...
69|Integrating the ER Approach in an OO Environment|We translate Entity-Relationship (ER) schemas into the object-oriented specification language TROLL light . This language describes the Universe of Discourse (UoD) as a system of concurrently existing and interacting objects, i.e., an object community. Thereby two essential aspects, structure and behavior, are integrated in one formalism. By doing the translation from ER to TROLL light we preserve the visual advantages of the former and receive a formalism through the latter which can be mapped to an adequate object-oriented database system. Proceeding this way we hope our proposal for transforming ER schemas into TROLL light specifications provides a valuable link between structural and dynamic modeling. 1 Introduction  Proc. 12th Int. Conf. on Entity-Relationship Approach (ER&#039;93) R. Elmasri, V. Kouramajian, B. Thalheim (Eds.), Springer, Berlin, LNCS 823, pp. 376--389 (1994) Nowadays the Entity-Relationship approach [Che76] has been accepted as a quasi standard [Teo90, BCN92] for the ...
70|Transforming Conceptual Data Models into an Object Model|. In this paper a conceptually simple structural object model focusing on object types, attributes and ISA relationships is introduced. The model is derived mainly from an extended Entity-Relationship approach, but concepts from other semantic and object-oriented models have influenced its features. It is shown how high-level conceptual data models can be mapped to this model, and to what extent the object model subsumes classical modeling paradigms. 1 Introduction  In recent years numerous data models for the conceptual modeling of information systems have been proposed. Among them there are semantic data models like SDM [13], IFO [3] and (extended) ER models [8, 9, 10, 24, 31, 34], complex object models like [1, 18, 26], purely object-based models like FDM [28] as well as object-based models with complex values such as [22]. All the approaches have different motivation, terminology, and aims. Here we aim to show you how some of these models can be translated into a conceptually simpl...
71|Database Schema Evolution using EVER Diagrams|We present an approach to schema evolution through changes to the ER diagram representing the schema of a database. In order to facilitate changes to the ER schema we enhance the graphical constructs used in ER diagrams, and develop EVER, an EVolutionary ER diagram for specifying the derivation relationships between schema versions, relationships among attributes, and the conditions for maintaining consistent views of programs. In this paper, we demonstrate the mapping of the EVER diagram into an underlying database and the construction of database views for schema versions. Through the reconstruction of views after database reorganization, changes to an ER diagram can be made transparent to the application programs while all objects in the database remain accessible to the application programs. The EVER system can serve as a front-end for object-oriented databases. 1 Introduction  In this paper we present an approach to schema evolution through changes to the ER diagram representing t...
72|A Definition of Round-Trip Engineering|Abstract. This paper provides a definition of Round-trip Engineering (RE) and introduces RE-systems. Where previous definitions of RE are abstract in the sense that they treat views as black boxes this paper provides a more detailed definition in which views are transparent. Having transparent views enables us to reason about what happens to views during transformations. This will enable studies on the requirements on views, view transformations and view operations and how these relate to each other. RE-systems may include multiple domains and transformations between those domains. This will enable studies on the requirements on complete RE-systems. 1
73|Mapping an Extended Entity-Relationship Schema into a Schema of Complex Objects|. With the advent of object-oriented database systems, there is an urgent need to define a methodology for mapping a conceptual schema into an object-oriented one, and migrating from a conventional database to an object-oriented database containing complex objects. This paper deals with an important step of the migration process by describing a technique for complex entity formation which involves recursively grouping entities and relationships from an extended entityrelationship schema, using semantic abstractions such as aggregation, generalization and association. The abstract schema produced by the clustering technique at a given level of grouping can then be converted into a structurally object-oriented schema allowing the explicit expression of complex entity types, relationships and integrity constraints. The overall methodology is implemented within the environment of INTERSEM, a prototype for semantic object-oriented modelling.  1 Introduction  Most of present database (DB) ap...
74|Some Patterns for Relationships|Relationships between objects are almost as important to designs as objects themselves. Most programming languages do not support relationships well, so programmers must implement relationships in terms of more primitive constructs. This collection of patterns describe how objects can be used to model relationships within programs. By using these patterns, programs and designs can be smaller, more flexible, and easier to understand and maintain.
75|Stepwise Re-Engineering and Development of Object-Oriented Database Schemata|We present a general approach for re-engineering of object-oriented database schemata. The approach consists of four dependent steps: (1) description of the data within the underlying basic data format, (2) application of a powerful semantic data model in order to construct a semantic database schema, (3) translation of the achieved schema into a general object model, and (4) implementation of the object schema in a concrete object-oriented database system. As an instantiation of this general procedure we report on a case study carried out in an industrial context where an Extended Entity-Relationship model was used as the semantic data model and ObjectStore as the implementation platform. 1. Introduction  From-scratch database schema development and reengineering of database schemata are special cases of general software development activities. Thus well-developed techniques known from the software engineering world should be applied here as well. This means, that one should develop s...
76|Event-Entity-Relationship Modeling in Data Warehouse Environments|We use the event-entity-relationship model (EVER) to illustrate the use of entity-based modeling languages for conceptual schema design in data warehouse environments. EVER is a general-purpose information modeling language that supports the specification of both general schema structures and multi-dimensional schemes that are customized to serve specific information needs. EVER is based on an event concept that is very well suited for multi-dimensional modeling because measurement data often represent events in multi-dimensional databases.
77|Probabilistic Entity-Relationship Models, PRMs, and Plate|In this chapter, we introduce a graphical language for relational data called the probabilistic entity-relationship (PER) model. The model is an extension of the entity-relationship model, a common model for the abstract representation of database structure. We concentrate on the directed version of this model—the directed acyclic probabilistic entity-relationship (DAPER) model. The DAPER model is closely related to the plate model and the probabilistic relational model (PRM), existing models for relational data. The DAPER model is more expressive than either existing model, and also helps to demonstrate their similarity. In addition to describing the new language, we discuss important facets of modeling relational data, including the use of restricted relationships, self relationships, and probabilistic relationships. Many examples are provided.
78|Bayesian Data Analysis|I actually own a copy of Harold Jeffreys’s Theory of Probability but have only read small bits of it, most recently over a decade ago to confirm that, indeed, Jeffreys was not too proud to use a classical chi-squared p-value when he wanted to check the misfit of a model to data (Gelman, Meng and Stern, 2006). I do, however, feel that it is important to understand where our probability models come from, and I welcome the opportunity to use the present article by Robert, Chopin and Rousseau as a platform for further discussion of foundational issues. 2 In this brief discussion I will argue the following: (1) in thinking about prior distributions, we should go beyond Jeffreys’s principles and move toward weakly informative priors; (2) it is natural for those of us who work in social and computational sciences to favor complex models, contra Jeffreys’s preference for simplicity; and (3) a key generalization of Jeffreys’s ideas is to explicitly include model checking in the process of data analysis.
79|Factor Graphs and the Sum-Product Algorithm|A factor graph is a bipartite graph that expresses how a &#034;global&#034; function of many variables factors into a product of &#034;local&#034; functions. Factor graphs subsume many other graphical models including Bayesian networks, Markov random fields, and Tanner graphs. Following one simple computational rule, the sum-product algorithm operates in factor graphs to compute---either exactly or approximately---various marginal functions by distributed message-passing in the graph. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative &#034;turbo&#034; decoding algorithm, Pearl&#039;s belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform algorithms. 
80|Learning probabilistic relational models|A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with &amp;quot;flat &amp;quot; data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning — the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases. 1
81|Operations for Learning with Graphical Models|This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, and the manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximization algorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper conclu...
82|Object-Oriented Bayesian Networks|Bayesian networks provide a modeling language and associated inference algorithm for stochastic domains. They have been successfully applied in a variety of medium-scale applications. However, when faced with a large complex domain, the task of modeling using Bayesian networks begins to resemble the task of programming using logical circuits. In this paper, we describe an object-oriented Bayesian network (OOBN) language, which allows complex domains to be described in terms of inter-related objects. We use a Bayesian network fragment to describe the probabilistic relations between the attributes of an object. These attributes can themselves be objects, providing a natural framework for encoding part-of hierarchies. Classes are used to provide a reusable probabilistic model which can be applied to multiple similar objects. Classes also support inheritance of model fragments from a class to a subclass, allowing the common aspects of related classes to be defined only once. Our language h...
83|Dependency networks for inference, collaborative filtering, and data visualization |We describe a graphical model for probabilistic relationships|an alternative tothe Bayesian network|called a dependency network. The graph of a dependency network, unlike aBayesian network, is potentially cyclic. The probability component of a dependency network, like aBayesian network, is a set of conditional distributions, one for each nodegiven its parents. We identify several basic properties of this representation and describe a computationally e cient procedure for learning the graph and probability components from data. We describe the application of this representation to probabilistic inference, collaborative ltering (the task of predicting preferences), and the visualization of acausal predictive relationships.
85|Learning Probabilistic Models of Link Structure|Most real-world data is heterogeneous and richly interconnected. Examples include the Web, hypertext,  bibliometric data and social networks. In contrast, most statistical learning methods work with &#034;flat&#034; data  representations, forcing us to convert our data into a form that loses much of the link structure. The recently  introduced framework of probabilistic relational models (PRMs) embraces the object-relational nature  of structured data by capturing probabilistic interactions between attributes of related entities. In this paper,  we extend this framework by modeling interactions between the attributes and the link structure itself. An  advantage of our approach is a unified generarive model for both content and relational structure. We propose  two mechanisms for representing a probabilistic distribution over link structures: reference uncertainty and  existence uncertainty. We describe the appropriate conditions for using each model and present learning algorithms  for each. We present experimental results showing that the learned models can be used to predict link  structure and, moreover, the observed link structure can be used to provide better predictions for the attributes  in the model.
86|The estimation of probabilities|By way of introduction, a classification of kinds of probability is given in the form of a tree which also forms an approximate hierarchy: psychological, subjective, logical, physical, and tautological. Various relationships between these kinds of probability are mentioned. Methods, all more or less Bayesian, for the estimation of physical prob-abilities are then described. Binomial and multinomial probabilities are estimated by means of a three-tiered hierarchical Bayesian method. The method can also be regarded, in some of its aspects, as Bayesian in the ordinary sense, wherein the initial distribution for the physical probabilities is a weighted sum of symmetrical Dirichlet distributions. It can be proved that this is equivalent to the use of a single symmetrical Dirichlet distribution whose parameter is selected after sampling. Thus, in this problem, an ordinary Bayesian method implies an empirical Bayesian method. Next the species-sampling or vocabulary-sampling problem is considered wherein there is a multinomial population having a very large number of categories. The theory derives from a suggestion of Turing&#039;s, which in part anticipates the empirical Bayesian method. Among other things, it leads to estimates of population coverage for an enlarged sample. Finally the estimation of probabilities in multidimensional contingency tables is considered. The main method here is that of maximum entropy, but it is shown that this can be subsumed under a more general method of minimum discriminability for the formulation of hypotheses. Entropy is best regarded as a special case of the older and more obviously Bayesian concept of &#034;expected weight of evidence&#034;. 1.
87|Approximate Inference for First-Order Probabilistic Languages|A new, general approach is described for approximate  inference in first-order probabilistic languages,  using Markov chain Monte Carlo (MCMC)  techniques in the space of concrete possible worlds  underlying any given knowledge base. The simplicity  of the approach and its lazy construction of  possible worlds make it possible to consider quite  expressive languages. In particular, we consider  two extensions to the basic relational probability  models (RPMs) defined by Koller and Pfeffer, both  of which have caused difficulties for exact algorithms.  The first extension deals with uncertainty  about relations among objects, where MCMC samples  over relational structures. The second extension  deals with uncertainty about the identity of  individuals, where MCMC samples over sets of  equivalence classes of objects. In both cases, we  identify types of probability distributions that allow  local decomposition of inference while encoding  possible domains in a plausible way. We apply  our algorithms to simple examples and show that  the MCMC approach scales well.  1 
88|An Extended Entity-Relationship Model for Geographic Applications|. A special-purpose extension of the EntityRelationship model for the needs of conceptual modeling of geographic applications, called the Geo-ER Model, is presented. Handling properties associated to objects not because of the objects&#039; nature but because of the objects&#039; position, calls for dealing-at the semantic modeling levelwith space, location and dimensionality of objects, spatial relationships, space-depending attributes, and scale and generalization of representations. In order to accomplish this in the framework of ER and its derivatives, we introduce special entity sets, relationships, and add new constructs. The rationale as well as examples of usage of the Geo-ER model from actual projects are presented.  1. Introduction  Is everybody special or are we all alike? Should we develop applications according to a special methodology for each class of applications, such as medical, business process and geographic, or should we use a single blanket approach for all? Personal prefer...
89|Querying object-oriented databases|We present a novel language for querying object-oriented databases. The language is built around the idea of extended path expressions that substantially generalize [ZAN83], and on an adaptation of the first-order formalization of object-oriented languages from [KW89, KLW90, KW92]. The language incorporates features not found in earlier proposals; it is easier to use and has greater expressive power. Some of the salient features of our language are: ffl Precise model-theoretic semantics. ffl A very expressive form of path expressions that not only can do joins, selections and unnesting, but can also be used to explore the database schema. ffl Views can be defined and manipulated in a much more uniform way than in other proposals. ffl Database schema can be explored in the very same language that is used to retrieve data. Unlike in relational languages, the user needs not know anything about the system tables that store schema information. ffl The notions of a type and type-correctness have precise meaning. It accommodates a wide variety of queries that might be deemed well- or ill-typed under different circumstances. In particular, we show that there is more than one way of settling the issue of type correctness. For expository purposes and due to space limitation, we chose to make a number of simplifying assumptions and left some features out. A more complete account can be found in [KSK92].
90|Conceptual Modelling and Telos  |We review basic premises underlying the application of conceptual modelling to the development of information systems and point out a fundamental problem arising from the broad range of concepts that need to be modelled. We then argue that conventional conceptual models are weak for such broad domains of discourse because they come with built-in collections of primitive notions in terms of which conceptual modelling is to be done. Telos is then introduced as a conceptual modelling language designed for capturing knowledge about information systems and it is argued that, unlike its peers, it offers facilities not only for modelling an application but also the notions used to model an application. The presentation of features of the language is eclectic and generally non-technical. Details about Telos can be found in [Mylopoulos90] and [Koubarakis89].
91|Modeling Behavior of Geographic Objects: An Experience with the Object Modeling Technique|Behavior of geographic objects holds a critical role in spatial databases. This, along with objects&#039; position and space-varying attributes, form a minimal set of concepts sufficient to capture spatial peculiarities in terms of the object-oriented rational. We present the semantics and the graphical notation of a prototypical object-oriented model for the conceptual design of spatial databases: by extending the Object Model of the Object Modeling Technique to the Geographic Object Model, we show how the above three concepts fit naturally into any object-oriented tool. We augment this model with the constructs of spatial aggregation and spatial grouping to express the critical aspects of space-varying attributes, object  boundary fuzziness and uncertainty, spatial relationships, and attribute generalization. Our proposal integrates the field- and object-based geographic views in one model. The principal idea behind this effort is the incorporation of a set of concepts into any semantic or object-oriented model, to make them communicate at the conceptual level (semantic interoperability).
92|On Mapping between UML and Entity-Relationship Model|: Nowadays, the Entity-Relationship Model (ERM) is the most important and widely used method for modeling data and designing databases. On the other hand, the Unified Modeling Language (UML) is expected to become more and more popular in object-oriented analysis and design (OOA/OOD). As a byproduct of OOA/OOD, a database design can be derived by mapping of objects to entities. The purpose of this paper is to define a mapping between UML and ERM. The translation of a UML class diagram to and from an ER diagram is also elaborated. This work is part of a multi-model multi-tool database application engineering framework that is being designed and prototyped at University Konstanz.  1 Motivation  Since its introduction by Chen (1976), the Entity-Relationship Model (ERM) has found its wide acceptance in the area of database design and related fields. In the ER model, all data are viewed as stating facts about entities and relationships, i.e., connections or associations between entities. The...
93|Object-oriented Analysis and Design with Applications|My friend, my lover, my wife
94|Database Evolution: the DB-MAIN Approach| The paper analyses some of the practical problems that arise when the requirements of  an information system evolve, and when the database and its application programs are to be  modified accordingly. It presents four important strategies to cope with this evolution, namely  forward maintenance, backward maintenance, reverse engineering and anticipating design. A  common, generic, framework that can support these strategies is described. It is based on a generic  data structure model, on a transformational approach to database engineering, and on a design  process model. The paper discusses how this framework allows formalizing these evolution  strategies, and describes a generic CASE tool that supports database applications maintenance. 
95|An Entity-Relationship Model Extended to Describe Historical Information|&lt;F1.409e+04&gt;  The entity-relationship (ER) model has proved a successful conceptual capture and modelling tool for the relational data model. Much effort has recently been made to extend the relational data model to describe historical information, but as yet little corresponding development in ER modelling has been made. We describe in detail the various temporal behaviours that entities and relationships may exhibit, and apply the results to enhance a binary ER model with temporal semantics. The resultant ERT may be used to fully model historical relational database schemata.&lt;F2.323e+04&gt;  Keywords:&lt;F1.409e+04&gt; Semantic Data Models, Entity-Relationship Models, Temporal Databases, Historical Databases.&lt;F2.323e+04&gt; Introduction&lt;F1.409e+04&gt;  The semantical data models of the past have focused on data representation without considering in depth the temporal and behavioural aspects of information modelling. A large proportion of research into temporal data models and query languages has be...
96|Collection Type Constructors in Entity-Relationship Modeling |Abstract. Collections play an important part in everyday life. There-fore, conceptual data models should support collection types to make data modeling as natural as possible for its users. Based on the funda-mental properties of endorsing order and multiplicity of its elements we introduce the collection types of rankings, lists, sets and bags into the framework of Entity-Relationship modeling. This provides users with easy-to-use constructors that naturally model different kinds of collec-tions. Moreover, we propose a transformation of extended ER schemata into relational database schemata. The transformation is intuitive and invertable introducing surrogate attributes that preserve the semantics of the collection. Furthermore, it is a proper extension to previous trans-formations, and results in a relational database schema that is in Inclu-sion Dependency Normal Form. In addition, we introduce a uniqueness constraint that identifies collections uniquely and guarantees referential integrity at the same time. 1
97|The Object-Oriented Database System Manifesto|This paper attempts to define an object-oriented database system. It describes the main features and characteristics that a system must have to qualify as an objectoriented database system. We have separated these characteristics into three groups: ffl Mandatory, the ones the system must satisfy in order to be termed an objectoriented database system. These are complex objects, object identity, encapsulation, types or classes, inheritance, overriding combined with late binding, extensibility, computational completeness, persistence, secondary storage management, concurrency, recovery and an ad hoc query facility. ffl Optional, the ones that can be added to make the system better, but which are not mandatory. These are multiple inheritance, type checking and inferencing, distribution, design transactions and versions. ffl Open, the points where the designer can make a number of choices. These are the programming paradigm, the representation system, the type system, and uniformity. We...
98|Spatial-Temporal Conceptual Models: Data Structures + Space + Time, Conference on information and knowledge management proceeding s of the 7 th international symposium on advances in geographic information systems|Nowadays, many applications need data modeling facilities for the description of complex objects with spatial and/or temporal facilities. Responses to such requirements may be found in Geographic Information Systems (GIS), in some DBMS, or in the research literature. However, most of existing models cover only partly the requirements (they address either spatial or temporal modeling), and most are at the logical level, hence not well suited for database design. This paper proposes a spatiotemporal modeling approach at the conceptual level, called MADS. The proposal stems from the identification of the criteria to be met for a conceptual model. It is advocated that orthogonality is the key issue for achieving a powerful and intuitive conceptual model. Thus, the proposal focuses on highlighting similarities in the modeling of space and time, which enhance readability and understandability of the model.
99|Justification for inclusion dependency normal form|permission from the publisher. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE. © 2000 IEEE. Copyright and all rights therein are retained by authors or by other copyright holders. All persons downloading this information are expected to adhere to the terms and constraints invoked by copyright. This document or any part thereof may not be reposted without the explicit permission of the copyright holder. Citation for this copy: Levene, Mark and Vincent, Millist W. (2000) Justification for inclusion dependency normal form. London: Birkbeck ePrints. Available at:
100|Abstract English Sentence Structures and EER Modeling |An input to the conceptual database design phase contains a description of the target database. This description is usually given in some natural language, for instance in English. Conceptual design aims at transforming English sentences into a conceptual database schema. A conceptual data model should therefore possess modeling features that can represent English sentence structures. Since Chen himself has argued that the basic ER model has such constructs many extensions of this basic ER model have been proposed. Based on these new features we revise the several correspondences between English sentence structures and concepts of ER modeling. It turns out that EER modeling can provide both a well-defined semantics and improved modeling elements that naturally reflect English language sentence structures. 1
101|Deriving Identity from Extensionality|In recent years, a number of proposals have been made to extend conventional conceptual data modeling techniques with concepts for modeling complex object structures. Among the most prominent proposed concepts is the concept of collection type. A collection type is an object type of which the instances are sets of instances of an other object type. A drawback of the introduction of such a new concept is that the formal definition of the technique involved becomes considerably more complex. This is a result of the fact that collection types are populatable types and such types tend to complicate updates. In this paper it is shown how a new kind of constraint, the extensional uniqueness constraint, allows for an alternative treatment of collection types avoiding update problems. The formal definition of this constraint type is presented, other advantages of its introduction are discussed, and its consequences for, among others, identification schemes are elaborated.  Keywords--- Conceptu...
102|Bioinformatics Adventures in Database Research|Informatics has helped launch molecular biology into the genomic  era. It appears certain that informatics will remain a major contributor to molecular  biology in the post-genome era. We discuss here data integration and datamining  in bioinformatics, as well as the role that database theory played in these topics.
103|Temporal Entity-Relationship Models - A Survey|The Entity-Relationship (ER) model, using varying notations and with some semantic variations, is enjoying a  remarkable, and increasing, popularity in both the research community#the computer science curriculum#and in industry. In step  with the increasing diffusion of relational platforms, ER modeling is growing in popularity. It has been widely recognized that  temporal aspects of database schemas are prevalent and difficult to model using the ER model. As a result, how to enable the ER  model to properly capture time-varying information has, for a decade and a half, been an active area in the database-research  community. This has led to the proposal of close to a dozen temporally enhanced ER models. This paper surveys all temporally  enhanced ER models known to the authors. It is the first paper to provide a comprehensive overview of temporal ER modeling and  it, thus, meets a need for consolidating and providing easy access to the research in temporal ER modeling. In the presentation of  each model, the paper examines how the time-varying information is captured in the model and presents the new concepts and  modeling constructs of the model. A total of 19 different design properties for temporally enhanced ER models are defined, and each  model is characterized according the these properties.
104|The Temporal Query Language TQuel|This paper defines aggregates in the temporal query language TQuel and provides their rormal semantics in the tuple relational calculus. A rormal semantics (or Que! aggregates is defined in the process. Multiple aggregates; aggregates appearing in the where, when, valid, and as-or clauses; nested aggregation; and instantaneous, cumulative, and unique variants are supported. These aggregates give the user a rich set or statistical functions that range over time, while requiring minimal additions to TQuel and its semantics..:&#039;l&#039;bi1 work wu nppolied bJ NSF (l&#039;&amp;lli DCR·8402330 and by a Junior Faculty Denlopmnt Awud from the UNC. CH FoUD.datioa. The &amp;nt aat.hor wu npport.ed ia pan by u IBM Faculty Developmnt. Award..,!;
105|A Glossary of Temporal Database Concepts|This glossary contains concepts specific to temporal databases that are well-defined, well understood, and widely used. In addition to defining and naming the concepts, the glossary also explains the decisions made. It lists competing alternatives and discusses the pros and cons of these. It also includes evaluation criteria for the naming of concepts. This paper is a structured presentation of the results of e-mail discussions initiated during the preparation of the first book on temporal databases, Temporal Databases: Theory, Design, and Implementation, published by Benjamin/Cummings, to appear January 1993. Independently of the book, an initiative aimed at designing a consensus Temporal SQL is under way. The paper is a contribution towards establishing common terminology, an initial subtask of this initiative. This paper appeared in SIGMOD Record, Vol. 21, No. 3, September 1992. 1 Introduction Maintaining a precise, well-defined, and intuitive technical language is important to t...
107|Evaluation of Relational Algebras Incorporating the Time Dimension in Databases|The relational algebra is a procedural query language for relational databases. In this paper we survey extensions of the relational algebra that can query databases recording time-varying data. Such an algebra is a critical part of a temporal DBMS. We identify 26 criteria that provide an objective basis for evaluating temporal algebras, Seven of the criteria are shown to be mutually unsatisfiable, implying there can be no perfect temporal algebra, Choices made as to which of the incompatible criteria are satisfied characterize existing algebras Twelve time-oriented algebras are summarized and then evaluated against the criteria. We demonstrate that the design space has in some sense been explored in that all combinations of basic design decisions have at least one representative algebra. Coverage of the remaining criteria provides one measure of the quality of each algebra We argue that all of the criteria are independent and that the criteria identified as compatible are indeed so, Finally, we list plausible properties proposed by others that are either subsumed by other criteria, are not well defined, or have no objective basis for being evaluated. The algebras realize many different approaches to what appears initially to be a straightforward design task.
108|A Conceptual Modelling Formalism for Temporal Database Applications|Arguably the most critical of all activities in the development of an information system is that of requirements modelling. The effectiveness of such a specification depends largely on the ability of the chosen conceptual model to represent the problem domain in such a way so as to permit natural and rigorous descriptions within a methodological framework. Recent years have witnessed an increased demand for information systems which cover a wide spectrum of application domains. This, inevitably, has had the effect of demanding conceptual models of enhanced functionality and expressive power than is currently possible in practice. This paper introduces the TEMPORA  modelling paradigm for developing information system applications from a unified perspective which deals with definitional, intentional and constrain knowledge. The paper discusses in detail one of the components of the TEMPORA  conceptual model, the Entity-Relationship-Time (ERT) model, which deals with structural aspects in...
109|Semantics of Time-Varying Information|This paper provides a systematic and comprehensive study of the underlying semantics of temporal databases, summarizing the results of an intensive collaboration between the two authors over the last five years. We first examine how facts may be associated with time, most prominently with one or more dimensions of valid time and transaction time. One common case is that of a bitemporal relation, in which facts are associated with exactly one valid time and one transaction time. These two times may be related in various ways, yielding temporal specialization. Multiple transaction times arise when a fact is stored in one database, then later replicated or transferred to another database. By retaining the transaction times, termed temporal generalization, the original relation can be effectively queried by referencing only the final relation. We attempt to capture the essence of time-varying information via a very simple data model, the bitemporal conceptual data model. Emphasis is placed...
110|The Time Dimension in Conceptual Modelling|In recent years there has been a growing interest in the explicit introduction of time modelling in a conceptual schema. This has come about as as a result of the relaisation that realisation that the development of large information systems is becoming increasingly more difficult as user requirements become broader and more sophisticated. Arguably the most critical activity in the development of a large data-intensive information system is that of requirements capture and specification. The effectiveness of such a specification depends largely on the ability of the chosen conceptual model to represent the problem domain in such a way so as to permit natural and rigorous descriptions within a methodological framework. The explicit representation of time in a conceptual model plays a major role in achieving this effectiveness. This paper examines the ontology and properties of time in the context of information systems and conceptual modelling. In particular, a critical set of ontologic...
111|A Visual Entity-Relationship Model for Constraint-Based |Abstract. University timetabling (UTT) is a complex problem due to its combinatorial nature but also the type of constraints involved. The holy grail of (constraint) programming: ”the user states the problem the program solves it ” remains a challenge since solution quality is tightly coupled with deriving ”effective models”, best handled by technology experts. In this paper, focusing on the field of university timetabling, we introduce a visual graphic communication tool that lets the user specify her problem in an abstract manner, using a visual entity-relationship model. The entities are nodes of mainly two types: resource nodes (lecturers, assistants, student groups) and events nodes (lectures, lab sessions, tutorials). The links between the nodes signify a desired relationship between them. The visual modeling abstraction focuses on the nature of the entities and their relationships and abstracts from an actual constraint model. 1
112|Adaptive Constraint Satisfaction|Many different approaches have been applied to constraint satisfaction. These range from complete backtracking algorithms to sophisticated distributed configurations. However, most research effort in the field of constraint satisfaction algorithms has concentrated on the use of a single algorithm for solving all problems. At the same time, a consensus appears to have developed to the effect that it is unlikely that any single algorithm is always the best choice for all classes of problem. In this paper we argue that an adaptive approach should play an important part in constraint satisfaction. This approach relaxes the commitment to using a single algorithm once search commences. As a result, we claim that it is possible to undertake a more focused approach to problem solving, allowing for the correction of bad algorithm choices and for capitalising on opportunities for gain by dynamically changing to more suitable candidates.
113|The essence of ESSENCE: A constraint language for specifying combinatorial problems|Abstract. Essence is a new language for specifying combinatorial (decision or optimisation) problems at a high level of abstraction. The key feature enabling this abstraction is the provision of decision variables whose values can be combinatorial objects, such as tuples, sets, multisets, relations, partitions and functions. Essence also allows these combinatorial objects to be nested to arbitrary depth, thus providing, for example, sets of partitions, sets of sets of partitions, and so forth. 1
114|Entity-Generating schema Transformation for Entity-Relationship Models|The paper is a contribution to the problem of semantics-preserving schema restructuring.
115|From Entity-Relationship Models to Role-Attribute Models|. This paper is a short presentation of the SOM (Semantic Object Model) approach. SOM was created to fulfill two main objectives. The first objective is the revision of the traditional data modeling techniques in order to integrate them within an object oriented framework, without sacrificing the main object-oriented principles, namely encapsulation, extendibility and reusability. The paper advocates that the way data modeling concepts have been combined with object-oriented concepts does not reach that goal. The second objective is the improvement of data modeling techniques in order to make them able to model roles. Roles are an important real world aspect we think has not been suitably dealt with. This paper describes attributes and phases, the two main concepts of SOM. Attribute is the single concept used to model all static relationships and phases are thought to model roles. The paper also outlines the textual language (T-SOM) and the graphical language (G-SOM) used in SOM to des...
116|Semantic data models|Semantic data models have emerged from a requirement for more expressive conceptual data models. Current generation data models lack direct support for relationships, data abstraction, inheritance, constraints, unstructured objects, and the dynamic properties of an application. Although the need for data models with richer semantics is widely recognized, no single approach has won general acceptance. This paper describes the generic properties of semantic data models and presents a representative selection of models that have been proposed since the mid-1970s. In addition to explaining the features of the individual models, guidelines are offered for the comparison of models. The paper concludes with a discussion of future directions in the area of conceptual data modeling.
117|Predicate Classes|. Predicate classes are a new linguistic construct designed to complement normal classes in objectoriented languages. Like a normal class, a predicate class has a set of superclasses, methods, and instance variables. However, unlike a normal class, an object is automatically an instance of a predicate class whenever it satisfies a predicate expression associated with the predicate class. The predicate expression can test the value or state of the object, thus supporting a form of implicit property-based classification that augments the explicit type-based classification provided by normal classes. By associating methods with predicate classes, method lookup can depend not only on the dynamic class of an argument but also on its dynamic value or state. If an object is modified, the property-based classification of an object can change over time, implementing shifts in major behavior modes of the object. A version of predicate classes has been designed and implemented in the context of t...
118|Galileo, a strongly-typed, interactive conceptual language|Galileo, a programming language for database applications, is presented. Galileo is a strongly-typed, interactive programming language designed specifically to support semantic data model features (classification, aggregation, and specialization), as well as the abstraction mechanisms of modern programming languages (types, abstract types, and modularization). The main contributions of Galileo are (a) a flexible type system to model database structure and semantic integrity constraints; (b) the inclusion of type hierarchies to support the specialization abstraction mechanisms of semantic data models; (c) a modularization mechanism to structure data and operations into interrelated units (d) the integration of abstraction mechanisms into an expression-based language that allows interactive use of the database without resorting to a new stand-alone query language. Galileo will be used in the immediate future as a tool for database design and, in the long term, as a high-level interface for DBMSs.
119|Limitations of Record-Based Information Models|Record structures are generally efficient, familiar, and easy to use for most current data processing applications. But they are not complete in their ability to represent information, nor are they fully self-describing.
120|What is an Object, After All?|The envisaged notion of object is presented as corresponding to the basic, universal building block of (information) systems. A simple mathematical model for fully concurrent objects (actors) is adopted that extends a suitable model for sequential processes. An object is defined as a process possibly endowed with initiative and tracedependent attributes. Transactional requirements are analysed within this framework as liveness requirements. Object aggregation is explained using the general notion of object morphism. The basic inheritance, overriding and reification mechanisms are also presented, as well as a suitable notion of object-type. The computational model is shown through examples to provide a sound basis for (information) systems design, including abstract conceptual modeling and layered implementation of both passive (record-like) and active (procedure-like) objects. The model establishes a suitable semantic domain for the envisaged broad spectrum specification/design languag...
121|Clovers: The Dynamic Behavior of Types and Instances|Clovers are a novel mechanism for object-oriented languages that relax the constraints of the conventional type/instance distinction. Clovers provide a new definition of objecthood, in which a single object may consist of multiple overlapping representations, sharing aspects of both behavior and identity. We show how clovers can be used to implement multiple views, changes to the type of an object, and expanded type notions such as minimal template. We argue that clovers provide a useful unification of the type/instance relaxations that have been presented in the literature, such as versioning, prototypes, and boolean classes. 1 Introduction  In most object-oriented languages, an object has a single defining &#034;type,&#034; often corresponding to its class. This type determines the complete behavior of an object. It is typically not possible to modify this behavior after the object has been created. This view of types and objects simplifies the type systems of object-oriented languages, making...
122|A Calculus for Fuzzy Queries on Fuzzy Entity-Relationship Model |Most query languages are designed to retrieve information from databases containing precise and certain data using precisely specified commands. Application of fuzzy set theory to relational data models has been studied extensively in recent years. This paper presents a calculus for fuzzy queries on a fuzzy entity-relationship model. The paper, first, defines a fuzzy entity-relationship model capable of representing imprecision and uncertainty in entities, attributes, and relationships. Then, it describes a calculus for fuzzy queries along with operational semantics. Some of the key aspects of this calculus are the provision of multiple terms, aggregate functions, and various forms of quantification. 1. 
123|Fuzzy Functional Dependencies and Lossless Join Decomposition of Fuzzy Relational Database Systems|This paper deals with the application of fuzzy logic in a relational database environment with the objective of capturing more meaning of the data. It is shown that with suitable interpretations for the fuzzy membership functions, a fuzzy relational data model can be used to represent ambiguities in data values as well as impreciseness in the association among them. Relational operators for fuzzy relations have been studied, and applicability of fuzzy logic in capturing integrity constraints has been investigated. By introducing a fuzzy resemblance measure EQUAL for comparing domain values, the definition of classical functional dependency has been generalized to fuzzy functional dependency (ffd). The implication problem of ffds has been examined and a set of sound and complete inference axioms has been proposed. Next, the problem of lossless join decomposition of fuzzy relations for a given set of fuzzy functional dependencies is investigated. It is proved that with a suitable restriction on EQUAL, the design theory of a classical relational database with functional dependencies can be extended to fuzzy relations satisfying fuzzy functional dependencies.
124|An ER Calculus For The Entity-Relationship Complex Model|The entity-relationship (ER) approach still lacks sound and complete theoretical foundations for modern, more sophisticated, ER models. We already proposed an algebra as a basis for data manipulation languages for such an advanced ER model, called ERC. The aim of this paper is to formally define a companion ERC calculus. We first investigate a general calculus, which is more natural (straight derivation from the model), but too powerful as it also allows expressions which are semantically inconsistent with the existing database. We then define what restrictions should be imposed, and how, to lead to a calculus whose expressive power is equivalent to the one of the ERC algebra.  Keywords: entity-relationship, database theory, data manipulation languages, query languages, algebra, calculus  _________________________________________  This work was supported by a research contract with INRIA as part of the &#034;Programme de Recherches Coordonnes Bases de donnes de troisime gnration&#034; (PRC BD3)....
125|Reasoning with enhanced Temporal Entity-Relationship Models|Recent efforts in the Conceptual Modelling community have been devoted to properly capturing time-varying information, and several proposals of temporally enhanced Entity-Relationship (ER) exist. This work gives a logical formalisation of the various properties that characterise and extend different temporal ER models which are found in literature. The formalisation we propose is based on Description Logics (DL), which have been proved useful for a logical reconstruction of the most popular conceptual data modelling formalisms. The proposed DL has the ability to express both enhanced temporal ER schemas and integrity constraints in the form of complex inclusion dependencies. Reasoning in the devised logic is decidable, thus allowing for automated deductions over the whole conceptual representation, which includes both the ER schema and the integrity constraints over it.  1 Introduction  In the temporal ER community two different main modelling approaches have been devised to provide su...
126|Description Logics For Conceptual Data Modeling|The article aims at establishing a logical approach to class-based  data modeling. After a discussion on class-based formalisms for data modeling,  we introduce a family of logics, called Description Logics, which stem from  research on Knowledge Representation in Arti  cial Intelligence. The logics of  this family are particularly well suited for specifying data classes and relationships  among classes, and are equipped with both formal semantics and inference  mechanisms. We demonstrate that several popular data modeling formalisms,  including the Entity-Relationship Model, and the most common variants of  object-oriented data models, can be expressed in terms of speci  c logics of the  family. For this purpose we use a unifying Description Logic, which incorporates  all the features needed for the logical reformulation of the data models  used in the various contexts. We also discuss the problem of devising reasoning  procedures for the unifying formalism, and show that they provide valuable  supports for several important data modeling activities.
127|A description logic with transitive and inverse roles and role hierarchies|transitive roles play an important rôle in the adequate representation of aggregated objects: they allow these objects to be described by referring to their parts without specifying a level of decomposition. In [Horrocks &amp; Gough, 1997], the Description Logic (DL) ALCH R + is presented, which extends ALC with transitive roles and a role hierarchy. It is argued in [Sattler, 1998] that ALCH R + is well-suited to the representation of aggregated objects in applications that require various part-whole relations to be distinguished, some of which are transitive. For example, a medical knowledge base could contain the following entries defining two different parts of the brain, namely the gyrus and the cerebellum. In contrast to a gyrus, a cerebellum is an integral organ and, furthermore, a functional component of the brain. Hence the role is component (which is a non-transitive sub-role of is part) is used to describe the relation between the brain and the cerebellum: is component ? is part gyrus:= (?consists.brain mass)  ? (?is part.brain) cerebellum:= organ ? (?is component.brain) However, ALCH R + does not allow the simultaneous description of parts by means of the whole to which they belong and of wholes by means of their constituent parts: one or other is possible, but not both. To overcome this limitation, we present the DL ALCHI R + which extends ALCH R + with inverse (converse) roles, allowing, for example, the use of has part as well as is part. 1 Using ALCHIR +, we can define a tumorous brain as: tumorous brain:= brain ? (tumorous ? (?has part.tumorous)) Part of this work was carried out while being a guest at IRST,
129|Temporalizing description logics|Traditional rst order predicate logic is known to be designed for representing and manipulating static knowledge (e.g. mathematical theories). So are manyof its applications. Knowledge representation systems based on concept description logics are not exceptions.
130|Temporal ER Modelling with Description Logics|Recent efforts in the Conceptual Modelling community have  been devoted to properly capturing time-varying information. Various  temporally enhanced Entity-Relationship (ER) models have been proposed  that are intended to model the temporal aspects of database conceptual  schemas. This work gives a logical formalisation of the various  properties that characterise and extend different temporal ER models  which are found in literature. The formalisation we propose is based on  Description Logics (DL), which have been proved useful for a logical reconstruction  of the most popular conceptual data modelling formalisms.
131|Complexity of Reasoning over Entity-Relationship Models|We investigate the complexity of reasoning over various fragments of the Extended Entity-Relationship (EER) language, which include different combinations of the constructors for isa between concepts and relationships, disjointness, covering, cardinality constraints and their refinement. Specifically, we show that reasoning over EER diagrams with isa between relationships is ExpTime-complete even when we drop both covering and disjointness for relationships. Surprisingly, when we also drop isa between relations, reasoning becomes NP-complete. If we further remove the possibility to express covering between entities, reasoning becomes polynomial. Our lower bound results are established by direct reductions, while the upper bounds follow from correspondences with expressive variants of the description logic DL-Lite. The established correspondence shows also the usefulness of DL-Lite as a language for reasoning over conceptual models and ontologies.
132|Data complexity of query answering in description logics|In this paper we study data complexity of answering conjunctive queries over Description Logic knowledge bases constituted by an ABox and a TBox. In particular, we are interested in characterizing the FOL-reducibility and the polynomial tractability boundaries of conjunctive query answering, depending on the expressive power of the Description Logic used to specify the knowledge base. FOL-reducibility means that query answering can be reduced to evaluating queries over the database corresponding to the ABox. Since firstorder queries can be expressed in SQL, the importance of FOL-reducibility is that, when query answering enjoys this property, we can take advantage of Data Base Management System (DBMS) techniques for both representing data, i.e., ABox assertions, and answering queries via reformulation into SQL. What emerges from our complexity analysis is that the Description Logics of the DL-Lite family are the maximal logics allowing conjunctive query answering through standard database technology. In this sense, they are the first Description Logics specifically tailored for effective query answering over very large ABoxes.
133|DL-Lite: Tractable description logics for ontologies|We propose a new Description Logic, called DL-Lite, specifically tailored to capture basic ontology languages, while keeping low complexity of reasoning. Reasoning here means not only computing subsumption between concepts, and checking satisfiability of the whole knowledge base, but also answering complex queries (in particular, conjunctive queries) over the set of instances maintained in secondary storage. We show that in DL-Lite the usual DL reasoning tasks are polynomial in the size of the TBox, and query answering is polynomial in the size of the ABox (i.e., in data complexity). To the best of our knowledge, this is the first result of polynomial data complexity for query answering over DL knowledge bases. A notable feature of our logic is to allow for a separation between TBox and ABox reasoning during query evaluation: the part of the process requiring TBox reasoning is independent of the ABox, and the part of the process requiring access to the ABox can be carried out by an SQL engine, thus taking advantage of the query optimization strategies provided by current DBMSs.
134|Reasoning on UML Class Diagrams|UML is the de-facto standard formalism for software design and analysis. To support the design of  large-scale industrial applications, sophisticated CASE tools are available on the market, that provide  a user-friendly environment for editing, storing, and accessing multiple UML diagrams. It would be  highly desirable to equip such CASE tools with automated reasoning capabilities in order to detect  relevant formal properties of UML diagrams, such as inconsistencies or redundancies. With regard to  this issue, we consider UML class diagrams, which are one of the most important components of UML,  and we address the problem of reasoning on such diagrams. We resort to several results developed  in the  eld of Description Logics (DLs), a family of logics that admit decidable reasoning procedures.
135|Unifying class-based representation formalisms|The notion of class is ubiquitous in computer science and is central in many formalisms for the representation of structured knowledge used both in knowledge representation and in databases. In this paper we study the basic issues underlying such representation formalisms and single out both their common characteristics and their distinguishing features. Such investigation leads us to propose a unifying framework in which we are able to capture the fundamental aspects of several representation languages used in different contexts. The proposed formalism is expressed in the style of description logics, which have been introduced in knowledge representation as a means to provide a semantically well-founded basis for the structural aspects of knowledge representation systems. The description logic considered in this paper is a subset of first order logic with nice computational characteristics. It is quite expressive and features a novel combination of constructs that has not been studied before. The distinguishing constructs are number restrictions, which generalize existence and functional dependencies, inverse roles, which allow one to refer to the inverse of a relationship, and possibly cyclic assertions, which are necessary for capturing real world
136|DL-Lite in the light of first-order logic|The use of ontologies in various application domains, such as Data Integration, the Semantic Web, or ontology-based data management, where ontologies provide the access to large amounts of data, is posing challenging requirements w.r.t. a trade-off between expressive power of a DL and efficiency of reasoning. The logics of the DL-Lite family were specifically designed to meet such requirements and optimized w.r.t. the data complexity of answering complex types of queries. In this paper we propose DL-Litebool, an extension of DL-Lite with full Booleans and number restrictions, and study the complexity of reasoning in DL-Litebool and its significant sub-logics. We obtain our results, together with useful insights into the properties of the studied logics, by a novel reduction to the one-variable fragment of first-order logic. We study the computational complexity of satisfiability and subsumption, and the data complexity of answering positive existential queries (which extend unions of conjunctive queries). Notably, we extend the LOGSPACE upper bound for the data complexity of answering unions of conjunctive queries in DL-Lite to positive queries and to the possibility of expressing also number restrictions, and hence local functionality in the TBox.
137|On the Interaction Between ISA and Cardinality Constraints|ISA and cardinality constraints are among the most interesting types of constraints in data models. ISA constraints are used to establish several forms of containment among classes, and are receiving great attention in moving to object-oriented data models, where classes are organized in hierarchies based on a generalization /specialization principle. Cardinality constraints impose restrictions on the number of links of a certain type involving every instance of a given class, and can be used for representing several forms of dependencies beteewn classes, including functional and existence dependencies. While the formal properties of each type of constraints are now well understood, little is known of their interaction. The goal of this paper is to present an effective method for reasoning about a set of ISA and cardinality constraints in the context of a simple data model based on the notions of classes and relationships. In particular, the method allows one both to verify the satisfi...
138|On the Ontological Expressiveness of Temporal Extensions to the Entity-Relationship Model|It is widely recognized that temporal aspects of database schemas are prevalent, but also difficult to  capture using the ER model. The database research community&#039;s response has been to develop temporally  enhanced ER models. However, these models have not been subjected to systematic evaluation.
139|Conceptual Modeling of Time-Varying Information|A wide range of database applications manage information that varies over time. Many of the underlying  database schemas of these were designed using one of the several versions, with varying syntax and  semantics, of the Entity-Relationship (ER) model. In the research community as well as in industry, it is  common knowledge that the temporal aspects of the mini-world are pervasive and important, but are also  difficult to capture using the ER model. Not surprisingly, several enhancements to the ER model have  been proposed in an attempt to more naturally and elegantly support the modeling of temporal aspects of  information. Common to the existing temporally extended ER models, few or no specific requirements  to the models were given by their designers. With the
140|Towards a theory of the deep structure of information systems|The deep structure of an information system comprises those properties that manifest he meaning of the real-world system that the information system is intended to model. In this paper we describe three models that we have developed of information systems deep-structure properties. The first, the representational model, proposes a set of constructs that enable the ontological completeness of an information systems grammar to be evaluated. The second, the state-tracking model, proposes four requirements hat information systems must satisfy if they are to faithfully track the real-world system they are intended to model. The third, the good-decomposition model, proposes a set of necessary conditions that an information system must meet if it is to be well decomposed. The three models facilitate the evaluation of grammars used to analyze, design, and implement information systems and specific scripts that represent implemented information systems. 1.
141|Software Engineering Economics|Abstract—This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation. Index Terms—Computer programming costs, cost models, management decision
142|Value-Based Software Engineering|Abstract—This paper provides a definition of the term “software engineering ” and a survey of the current state of the art and likely future trends in the field. The survey covers the technology available in the various phases of the software life cycle—requirements engineering, design, coding, test, and maintenance—and in the overall area of software management and integrated technology-management approaches. It is oriented primarily toward discussing the domain of applicability of techniques (where and when they work), rather than how they work in detail. To cover the latter, an extensive set of 104 references is provided. Index Terms—Computer software, data systems, information systems,
143|An Analysis of the Structural Validity of Unary and Binary Relationships in Entity Relationship Modeling|We explore the criteria that contribute to the validity of
144|A Taxonomy of Recursive Relationships and Their Structural Validity in ER Modeling| In this paper, we present the complete classification of recursive relationships
145|The Skull beneath the Skin: Entity-Relationship Models of Information Artefacts|Data modelling reveals the internal structure of an information system, abstracting away  from details of the physical representation. We show that entity-relationship modelling, a  well-tried example of a data-modelling technique, can be applied to both interactive and  non-interactive information artefacts in the domain of HCI. By extending the conventional  ER notation slightly (to give ERMIA, Entity-Relationship Modelling for Information  Artefacts) it can be used to describe differences between different representations of the same  information, differences between user&#039;s conceptual models of the same device, and the  structure and update requirements of distributed information in a worksystem. It also yields  symbolic-level estimates of Card et al.&#039;s (1994) index of `cost-of-knowledge&#039; in an informa-  tion structure, plus a novel index, the `cost-of-update&#039;; these symbolic estimates offer a useful  complement to the highly detailed analyses of time costs obtainable from GOMS-like...
146|Cognitive dimensions of notations|‘Cognitive dimensions ’ are features of computer languages considered purely as information structures or notations. They therefore apply to many types of language—interactive or programming, high or low level, procedural or declarative, special purpose or general purpose. They are ‘cognitive ’ dimensions because they control how (or whether) the preferred cognitive strategy for design-like tasks can be adopted: it has repeatedly been shown that users prefer opportunistic planning rather than any fixed strategy such as top-down development. The dimension analysis makes it easier to compare dissimilar interfaces or languages, and also helps to identify the relationship between support tools and programming languages: the support tools make it possible to use opportunistic planning with notations that would otherwise inhibit it. Keywords: Computer Languages; Opportunistic Planning, Cognitive Dimensions.
147|Oriented and Entity Relationship Modeling, Fourteenth International|Any software made available via TimeCenter is provided \as is &amp;quot; and without any express or implied warranties, including, without limitation, the implied warranty of merchantability and tness for a particular purpose. The TimeCenter icon on the cover combines two \arrows. &amp;quot; These \arrows &amp;quot; are letters in the so-called Rune alphabet used one millennium ago by theVikings, as well as by their precedessors and successors, The Rune alphabet (second phase) has 16 letters. They all have angular shapes and lack horizontal lines because the primary storage medium was wood. However, runes may also be found on jewelry, tools, and weapons. Runes were perceived by manyashaving magic, hidden powers. The two Rune arrows in the icon denote \T &amp;quot; and \C, &amp;quot; respectively. This paper concerns the design of temporal relational database schemas. Normal forms play a central role during the design of conventional relational databases, and we have previously extended all existing relational normal forms to apply to temporal relations. However, these normal forms are all atemporal in nature and do not fully take into account the
149|Temporal Specialization and Generalization| A standard relation has two dimensions: attributes and tuples. A temporal relation contains two additional orthogonal time dimensions, namely, valid time and transaction time. Valid time records when facts are true in the modeled reality, and transaction time records when facts are stored in the temporal relation. Although, in general, there are no restrictions between the valid time and transaction time associated with each fact, in many practical applications, the valid and transaction times exhibit more or less restricted interrelationships that define several types of specialized temporal relations. The paper examines five different areas where a variety of types of specialized temporal relations are present. In application systems with multiple, interconnected temporal relations, multiple time dimensions may be associated with facts as they flow from one temporal relation to another. For example, a fact may have an associated transaction time indicating when it was stored in a previous temporal relation. The paper investigates several aspects of the resulting generalized temporal relations, including the ability to query a predecessor relation from a successor relation. The presented framework for generalization and specialization allows researchers as well as database and system designers to precisely characterize, compare, and thus better understand temporal relations and the application systems in which they are embedded. The framework’s comprehensiveness and its use in understanding temporal relations are demonstrated by placing previously proposed temporal data models within the framework. The practical relevance of the defined specializations and gener-alizations is illustrated by sample realistic applications in which they occur. The additional semantics of specialized relations are especially useful for improving the performance of query processing.  
150|Valid Time Integrity Constraints|This paper investigates temporal integrity constraints in valid time databases, i.e. databases  that capture the time-varying nature of the part of reality being modeled. We  first provide a taxonomy of integrity constraints in (temporal) databases in order to  establish a common terminology. The taxonomy identifies two classes of valid time integrity  constraints: intrastate and interstate integrity constraints. Intrastate integrity  constraints result from generalizing nontemporal integrity constraints. They guarantee  the consistency of every snapshot of a valid time database. Interstate integrity  constraints relate and constrain different valid time snapshots and, therefore, they are  unique to the temporal dimension. ChronoLog, a query language based on first order  predicate logic, can express both types of integrity constraints. Furthermore, timerestricted  integrity constraints may be expressed in ChronoLog. Finally, we discuss  the efficient checking of valid time integrity c...
151|An Algebra for TSQL2|Introduction  TSQL2 is a declarative query language, and as such, requires a procedural (algebraic) equivalent for implementation. In this chapter, we describe such an algebraic language. We undertook this design in order to show that TSQL2 can be implemented efficiently, with minimal extension of existing techniques. As TSQL2 provides a consistent extension of SQL-92, we had a parallel goal in the construction of this algebra. Namely, whenever possible, we extend, rather than modify, the snapshot relational algebra to accommodate the TSQL2 data model. This extension is performed to allow the use of established optimization strategies and evaluation algorithms. In addition, we have the somewhat conflicting goal of completeness, i.e., any query expressible in TSQL2 should be implementable as an algebraic expression. We informally demonstrate how TSQL2 language clauses are supported in the algebra. We first describe an algebra for the conceptual data model underlying TSQL2 [5]. A
152|On Time-Invariance and Synchronism in Valid-Time Relational Databases|. We propose new temporal dependencies for valid-time relations. The proposed dependencies can be thought of as generalizations of the temporal database concept of time-invariant attribute. They are natural extensions of the conventional construct of functional dependency. A complete axiomatization is obtained by extending Armstrong&#039;s axioms. The new dependencies can be expressed in a first-order temporal predicate calculus with equality. We argue that temporal dependency preservation may constitute a valuable design guide-line during normalization.  Keywords. temporal databases, relational data model, functional dependency, temporal logic 1. Reserach Assistant of the National Fund for Scientific Research (Belgium).  1193  1. Introduction  Temporal database constraints, in general, can be expressed as formulas of temporal logic that must be satisfied by any &#034;legal&#034; database. Several temporal logics can be found in literature [1,4,8,14]. Certain temporal constraints are more significan...
153|A Decomposition Method for Entity Relationship Models: A Systems Theoretic Approach |ABSTRACT. This paper defines a method for decomposing a large Entity Relationship model into a hierarchy of models of manageable size. The purpose of this is to improve user understanding and simplify documentation and maintenance. We define the problem as an instance of the general systems decomposition problem or systems simplification problem. We first define a set of principles for decomposing Entity Relationship models based on systems theory and human information processing. These define the desirable characteristics of a decomposition, and may be used to evaluate the quality of a decomposition and to choose between alternatives. We then define a procedure which can be used by humans to develop a relatively optimal (“good”) decomposition. Finally, we define a genetic algorithm which automatically finds an optimal decomposition based on the principles defined.
154|The magical number seven, plus or minus two: Some limits on our capacity for processing information|z Information measurement z Absolute judgments of unidimensional stimuli z Absolute judgments of multidimensional stimuli z Subitizing
155|KERMIT: A Knowledge-based Entity Relationship Modelling Intelligent Tutor|. Database (DB) modelling is the foundation of an efficient database. Similar to other design tasks, database modelling requires extensive practice to excel in it. Conventionally, DB modelling is taught in classrooms where the task of modelling a typical database is demonstrated and students practice in tutorials. Even though one-to-one human tutoring is the most effective teaching method, there will never be enough resources to provide individual human tutoring to each student. Intelligent Teaching Systems offer bright prospects for providing individualised pedagogical sessions to a much larger population of students. We present KERMIT, the Knowledge-based Entity Relationship Modelling Intelligent Tutor. KERMIT is developed as a problem-solving environment for Entity Relationship modelling, a popular high-level conceptual data model. The main aim of the system is to individualise pedagogical instructions towards each student. This paper presents the current state of the implementation of KERMIT.
156|A Coached Collaborative Learning Environment for Entity-Relationship Modeling|. We discuss the design of an agent for coaching collaborative learning  in a distance learning context. The learning domain is entity-relationship  modeling, a domain in which collaborative problem solving is regularly practiced,  and for which there exist formally interpretable representations of problem  solutions known as entity-relationship diagrams. The design of the coach  was based on socio-cognitive conflict theory, which states that collaborative  learning is effective to the extent that learners identify and discuss conflicts in  their beliefs. Students begin by constructing individual entity-relationship diagrams  expressing their solution to a database modeling problem, and then work  in small groups to agree upon a group solution. The coaching agent leverages  learning opportunities by encouraging students to share and discuss solution  components that conflict with components of the group solution. Our work  shows one way to utilize domain specific knowledge in order to ...
157|An analysis of SQL integrity constraints from an entity-relationship model perspective, in preparation|Abstract- This paper presents an analysis of the integrity constraints defined in the SQL IS0 stan-dard disclosure in the light of the entity-relationship model. It points out what features of integrity constraints in SQL support which features of the entity-relationship model by discussing how to map an entity-relationship schema into a SQL schema. In order to organize the analysis, the paper distinguishes three levels of the entity-relationship model. The first level corresponds to the basic model, augmented with simple specializations. The second level considers totality and more complex specializations, and allows deferred and immediate propagation of deletions. Finally, the third level introduces generaliza-tion hierarchies and considers some types of inter-relationship constraints. For the first and second levels, the analysis indicates that the more complex features of the SQL referential integrity construct are necessary only when optimization is considered, but one of the features- propagation of nulls-should be slightly enhanced to easy the optimization task. However, for the third level, the analysis shows that, in most cases, the SQL referential integrity construct can efficiently model subsetting, but that the direct use of SQL integrity constraints to account for mutual exclusion may be very expensive. 1.
158|Optimization of relational schemas containing inclusion dependencies|A two-step optimization strategy for relational schemas that contains a class of inclusion dependencies is described. Both steps take into account additional information that indicates how to preserve each inclu-sion dependency in the presence of insertions an‘d deletioy. The first step eliminates inclusion dependen-cies which are redundant with respect to both the semantics of the data and the behavior of the trans-actions. The second step discards dependencies through a structural transformation that again preserves the semantics of the data and of the transactions and that applies both to ihrF and to iV # relational schemas. 1.
159|An Architecture for Intelligent Collaborative Educational Systems|Abstract: A major technological concern of our work is to improve the cost effectiveness, reusability, and interoperability of advanced educational software. To make these technologies viable, we must be able to add component functionality incrementally, and enable systems to interoperate with commercial software and internet resources. We have designing and implemented an architecture that places shared resources and “heavyweight ” functionality on servers, and uses Java and Netscape to deliver student interfaces on a wide variety of client platforms at any location with internet access. This paper describes the architecture at five levels of description. Its strengths and weaknesses provide a case study in how to improve the deployability and interoperability of knowledge-based educational software without sacrificing advanced functionality. 1.
160|BetterBlether: The design and evaluation of a discussion tool for education|Abstract. Communication skills play a prominent role in the primary school curriculum. Children are first expected to acquire these skills within a supervised group environment in which the teacher guides and sets the pace for the discussion, and later transfer them to a less dependent setting. This paper describes BetterBlether 1, a computer mediated educational communication tool designed to facilitate and promote effective group interaction skills. BetterBlether uses a sentence opener approach (McManus &amp; Aiken, 1995) in order to scaffold the use of a range of discussion skills. In so doing, it provides support for the move from teacher dominated discussions to ones in which the pupils play a more active part. We first provide an overview of BetterBlether before going on to describe an empirical evaluation which was carried out in a local primary school. Finally, we compare these results with outcomes of research on both supervised and unsupervised group discussions (Harwood, 1995). FOSTERING GROUP DISCUSSION SKILLS In recent years the Scottish National Curriculum has highlighted the important role which talking and listening skills play in the teaching of English Language. Communication skills are essential for effective learning, cultivating an awareness and knowledge of language, and play
161|Missed Opportunities for Learning in Collaborative Problem-solving Interactions|Abstract: When students attempt to solve problems collaboratively in learning environments they may miss opportunities to use available resources for achieving learning goals. We present an approach to qualitative analysis of such &amp;quot;missed opportunities &amp;quot; (&amp;quot;MOs&amp;quot;) in collaborative problem-solving interactions, and discuss how the analysis can contribute to the design of the &amp;quot;CHENE &amp;quot; Computer Supported Collaborative Learning (&amp;quot;CSCL&amp;quot;) system, that is used to support physics modelling tasks. Since benefits of collaboration require involvement of both partners, we concentrate on MOs to use one&#039;s partner as a resource in achieving goals of coconstructing domain concepts. After presenting analyses of different cases of MOs of this type, we discuss why MOs occur and how they may be identified. In conclusion, we propose a &amp;quot;minimal graded intervention &amp;quot; approach to guidance in CSCL environments that is intended to address the problem of MOs for learning.
162|Preferences for Model Selection in Explanation|The range of possible domain models on which an  explanation can be based is often large, yet human  explainers are able to choose models that address a  questioner&#039;s informative needs without undue obscurity.
163|ABSTRACT Event-Entity-Relationship Modeling In Data Warehouse Environments |We use the event-entity-relationship model (EVER) to illustrate the use of entity-based modeling languages for conceptual schema design in data warehouse environments. EVER is a general-purpose information modeling language that supports the specification of both general schema structures and multi-dimensional schemes that are customized to serve specific information needs. EVER is based on an event concept that is very well suited for multi-dimensional modeling because measurement data often represent events in multi-dimensional databases.
164|A Formal Software Specification Tool Using The Entity/Relationship Model|: Software engineering, like any other engineering field, needs to use formal methods to prove the reliability of its products and optimise their production and maintenance. In order to do that, software specification needs to be expressed in a language whose vocabulary, syntax and semantics are formally defined. These languages can be a federating formalism in information systems for data structures and dynamics. Formal specifications may then be automatically processed and software tools can be built to assist their development. Nevertheless, little effort has been devoted by research to tool support, yet such support is essential if specifications are to be used in industry. Within this perspective, we propose a support tool that uses the Entity-Relationship model and a rule language as an interface for developing formal software specifications.  Keywords : E/R model, formal software specification , formal specification language, CASE tools, consistency proof, integrity constraints,...
165|Semantic modeling of object oriented databases|This paper describes a design methodology for an object oriented database, based on a semantic network. This approach is based on the assumption that Yemantic data models are more powerful and more easy to use than current proposed object oriented data models. They are especially more poweful in representing integrity constraints and various relationships, Object oriented data models are generally based only on class hierarchies and inheritance, plus their ability to represent the behavior of objects, But this latter capability is generally provided through an algorithmic language which cannot be considered as a conceptual language. This paper describes a design procedure which generates an object oriented database schema (both the structural aspect and the dynamic aspect) from an abstract specification giveri in a high level language. This specification language is built upon a semantic network and allows to define integrity constraints and behavior rules. This approach is presented through a CASE tool environment. 1.
166|A Hybrid Query Language for an Extended Entity-Relationship Model|We present the hybrid query language HQL/EER for an Extended Entity-Relationship model. As its main characteristic, this language allows a user to use both graphical and textual elements in the formulation of one and the same query. We demonstrate the lookand -feel of this query language by means of examples, and show how syntax and semantics of this language are formally defined using programmed graph rewriting systems. Although we present the language in the context of the EER model, the concept of hybrid languages is applicable in the context of other database models as well. We illustrate this claim by discussing a prototype implementation of a Hybrid Query Tool based on an object-oriented approach, namely the Object Modeling Technique (OMT). 1 Introduction The database research efforts of the past decade have provided us with a wide range of both database models and systems, allowing the user to perform complex manipulations on data structures of high modeling power. Th...
167|GraphLog: a Visual Formalism for Real Life Recursion|We present a query language called GraphLog, based on a graph representation of both  data and queries. Queries are graph patterns. Edges in queries represent edges or paths in the  database. Regular expressions are used to qualify these paths. We characterize the expressive  power of the language and show that it is equivalent to stratified linear Datalog, first order  logic with transitive closure, and non-deterministic logarithmic space (assuming ordering on  the domain). The fact that the latter three classes coincide was not previously known. We  show how GraphLog can be extended to incorporate aggregates and path summarization, and  describe briefly our current prototype implementation.  1 Introduction  The literature on theoretical and computational aspects of deductive databases, and the additional power they provide in defining and querying data, has grown rapidly in recent years. Much less work has gone into the design of languages and interfaces that make this additional pow...
168|A Graph-Oriented Object Database Model|A graph-oriented object database model (GOOD) is introduced as a theoretical basis for database systems in which manipulation as well as conceptual representation of data is transparently graph-based. In the GOOD model, the scheme as well as the instance of an object database is represented by a graph, and the data manipulation is expressed by graph transformations. These graph transformations are described using five basic operations and a method construct, all with a natural semantics. The basic operations add and delete objects and edges in function of the matchings of a pattern. The expressiveness of the model in terms of object-oriented modeling and data manipulation power is investigated. Index terms: Database models, query languages, graph transformations, objectoriented databases, user interfaces. Preliminary versions of this paper were presented at the 9th ACM Symposium on Principles of Database Systems [16] and the 1990 ACM SIGMOD International Conference on Management of D...
169|QBD*: a Graphical Query Language with Recursion|One of the main problems in the database area is to define query languages characterized by both high expressive power and ease of use. In this paper, we propose a system to query databases, using diagrams as a standard user interface. The system, called Query by Diagram* (QBD*), makes use of a conceptual data model, a query language on this model and a graphical user interface. The conceptual model is the Entity-Relationship Model; the query language, whose expressive power allows recursive queries, supports visual interaction. The main characteristics of the interface are the ease of use, and the availability of a rich set of primitives for schema selection and query formulation. Furthermore, we compare the expressive power of QBD* and G+, which are the only languages allowing recursive queries to be expressed graphically. 
170|Conceptual Modelling of Database Applications Using an Extended ER Model|In this paper, we motivate and present a data model for conceptual design of structural and behavioural aspects of databases. We follow an object centered design paradigm in the spirit of semantic data models. The specification of structural aspects is divided into modelling of object structures and modelling of data types used for describing object properties. The specification of object structures is based on an Extended Entity--Relationship (EER) model. The specification of behavioural aspects is divided into the modelling of admissible database state evolutions by means of temporal integrity constraints and the formulation of database (trans)actions. The central link for integrating these design components is a descriptive logic-- based query language for the EER model. The logic part of this language is the basis for static constraints and descriptive action specifications by means of pre- and postconditions. A temporal extension of this logic is the specification language for tem...
171|A Visual Database Management Interface Based on GOOD|We present various tools that are part of a visual interface for a database management system. All tools are based on the GOOD language, a graphical data definition and manipulation language. The language allows us to build a consistent set of visual tools for both naive and experienced users. In this paper we mainly focus on the programbuilder, which is a tool for writing GOOD programs and can be used to perform a wide range of database tasks, such as querying, updating and schema manipulation. The main principles of this tool are, direct manipulation of objects; syntax support through conditioned graphical editor facilities, although not limiting flexibility of specification; reusability of programs; allowing the use of views; backtracking facilities and support of the conceptual effect of the operation by showing the result on the schema. We also briefly introduce the view and browse tool. Finally, we give some technical aspects of the visual interface. 1 Introduction  One of the ma...
172|Nondeterministic Control Structures for Graph Rewriting Systems|:  The work reported here is part of the IPSEN  3  project whose very goal is the development of an Integrated Project Support ENvironment. Within this project directed, attributed, node- and edge- labeled graphs (diane graphs) are used to model the internal structure of software documents and PROgrammed Graph REwriting SyStems are used to specify the operational behavior of document processing tools like syntax-directed editors, static analyzers, or incremental compilers and interpreters. Recently a very high-level language, named PROGRESS, has been developed to support these activities. This language offers its users a convenient, partly textual, partly graphical concrete syntax and a rich system of consistency checking rules (mainly type compatibility rules) for the underlying calculus of programmed diane-graph rewriting systems. This paper presents a partly imperative, partly rule-oriented sublanguage of PROGRESS for composing complex graph queries and graph transformations (transa...
173|Two-dimensional specification of universal quantification in a graphical database query language|Abstract-We propose a technique for specifying universal quantification and existential quantification (combined with negation) in a two-dimensional (graphical) database query language. Unlike other approaches that provide set operators to simulate universal quantification, this technique allows a direct representation of universal quantification. We present syntactic constructs for specifying universal and existential quantifications, two-dimensional translation of universal quantification to existential quantification (with negation), and translation of existentially quantified two-dimensional queries to relational queries. The resulting relational queries can be processed directly by many existing database systems. Traditionally, universal quantification has been considered a difficult concept for typical database programmers. We claim that this technique renders universal quantification easy to understand. To substantiate this claim, we provide a simple, easy-to-follow guideline for constructing universally quantified queries. We believe that the direct representation of universal quantification in a two-dimensional language is new and that our technique contributes significantly to the understanding of universal quantification in the context of database query languages. Index Terms- Universal quantification, existential quantification, graphical query languages, databases, relational calculus, entity-relationship model. I.
174|Activity Recognition in Wide Aerial Video Surveillance Using Entity Relationship Models |We present the design and implementation of an activity recognition system in wide area aerial video surveillance using Entity Relationship Models (ERM). In this approach, finding an activity is equivalent to sending a query to a Relational DataBase Management System (RDBMS). By incorporating reference imagery and Geographic Information System (GIS) data, tracked objects can be associated with physical meanings, and several high levels of reasoning, such as traffic patterns or abnormal activity detection, can be performed. We demonstrate that different types of activities, with hierarchical structure, multiple actors, and context information, are effectively and efficiently defined and inferred using the ERM framework. We also show how visual tracks can be better interpreted as activities by using geo information. Experimental results on both real visual tracks and GPS traces validate our approach. 1.
175|Detection and Tracking of Large Number of Targets in Wide Area Surveillance |Abstract. In this paper, we tackle the problem of object detection and tracking in a new and challenging domain of wide area surveillance. This problem poses several challenges: large camera motion, strong parallax, large number of moving objects, small number of pixels on target, single channel data and low framerate of video. We propose a method that overcomes these challenges and evaluate it on CLIF dataset. We use median background modeling which requires few frames to obtain a workable model. We remove false detections due to parallax and registration errors using gradient information of the background image. In order to keep complexity of the tracking problem manageable, we divide the scene into grid cells, solve the tracking problem optimally within each cell using bipartite graph matching and then link tracks across cells. Besides tractability, grid cells allow us to define a set of local scene constraints such as road orientation and object context. We use these constraints as part of cost function to solve the tracking problem which allows us to track fast-moving objects in low framerate videos. In addition to that, we manually generated groundtruth for four sequences and performed quantitative evaluation of the proposed algorithm.
176|Inferring tracklets for multi-object tracking|Recent work on multi-object tracking has shown the promise of tracklet-based methods. In this work we present a method which infers tracklets then groups them into tracks. It overcomes some of the disadvantages of existing methods, such as the use of heuristics or non-realistic constraints. The main idea is to formulate the data association problem as inference in a set of Bayesian networks. This avoids exhaustive evaluation of data association hypotheses, provides a confidence estimate of the solution, and handles split-merge observations. Consistency of motion and appearance is the driving force behind finding the MAP data association estimate. The computed tracklets are then used in a complete multi-object tracking algorithm, which is evaluated on a vehicle tracking task in an aerial surveillance context. Very good performance is achieved on challenging video sequences. Track fragmentation is nearly non-existent, and false alarm rates are low. 1.
177|Dynamic network data exploration through semi-supervised functional embedding|The paper presents a framework for semi-supervised non-linear embedding methods useful for exploratory analysis and visualization of spatio-temporal network data. The method provides a functional embedding based on a neural net-work optimizing the graph-based cost function. It exploits an online stochastic gradient descent which, avoiding the costly matrix computations and the out-of-sample problem, makes it naturally applicable for large-scale dynamic spatio-temporal problems. The semi-supervised schemes are intro-duced to guide the method with precisely defined locations, pairwise distances or norms of the selected data samples in the embedded space. The method is useful for exploring the complex fully dynamic networks with a variable number of geo-referenced nodes and evolving edges. The approach is illustrated with a case study devoted to the real-time em-bedding of the geo-referenced data on instant messaging on the internet.
178|In this issue: Self-paced Learning and On-line Teaching of Entity-Relationship Modeling|Abstract: We present a detailed plan to teach Entity-Relationship modeling in an undergraduate course in Database Systems. The course syllabus fulfills the requirements of a first database course in the IS 2002 model curriculum. It is designed to be offered on-line for self-paced learning within the 15 weeks of a regular term. Our goal is to ensure the scope of coverage and the quality of delivery, and at the same time provide sufficient flexibility for student learning. Our teaching plan comprises 19 modules: from the introduction to Extended ER modeling covering specialization and generalization, including two optional modules for alternative ER diagram notations. The teaching plan is complete and yet very compact. The dependency graph governs the flow of the learning modules. Students will then have the flexibility to work out their schedules for their effective use of time and materials on-line. The critical design factor in each module is in the selected ER modeling features to cover, and very specific learning objectives. We believe our design will engender effective self-paced learning. The plan also prepares very well for on-line delivery. We are eager to further develop the plan by gathering a very large collection of illustrative examples, together with exercises
179|A First Course in Database Management|This paper describes a course that provides Computer Information System students with an introduction to Database Management in which they choose their own semester database project. The course was designed to allow the student to work on his project while learning the database theory in a concurrent manner. The organization of the course is presented, including a database written to track the progress of the students with their work. This database is available for download. The results of the student projects as well as student attitudes are discussed and recommendations for course structure and content are presented.
180|The Pros and Cons of Using a Comprehensive Final Case Project in a Database Management Systems Course: Marvin&#039;s Magnificent Magazine Publishing House|There are many challenges in providing a curriculum with a solid Information Systems founda-tion that meshes with the rapid changes in technology and its use within organizations. Edu-cators must struggle to fit all the necessary information into a limited number of credits while continuing to add skills including soft-skills. One particular area of pressure is the need to ex-pand topics in the typical database management course due to the increased importance of databases in organizations, the tremendous volume of data that must be handled, non-traditional types of data (e.g., multimedia, web-based) and the expanding array of database-related tools. This paper discusses the pros and cons of using a comprehensive database pro-ject as the culmination of an introductory course in database theory and design. An instruc-tor-created, team database project is described. Marvin’s Magnificent Magazine Publishing House database has provided students with a valuable experience on the four most popular database topics: the relational data model, SQL, the entity relational model, and normaliza-tion.
181|Simplicity First: Use of Tools in Undergraduate Computer Science and Information Systems Teaching|Use of tools – either home grown or industry supported- is inevitable in teaching CS/IS courses. The authors first examine the pros and cons of using tools in Computer Science and Information Systems courses. They briefly discuss the side effects of using tools on learning. In light of these discussions, they then focus on the impact of using tools in database man-agement, and systems analysis and design on the students ’ overall learning by analyzing stu-dent feedback in these courses and student performance in the capstone project course in which knowledge gained in these two are applied. Based on their observations, the authors make a few suggestions for the appropriate use of tools and conclude that more care is re-quired in using tools in lower-level courses.
182|Self-paced Learning and On-line Teaching of Entity-Relationship Modeling |We present a detailed plan to teach Entity-Relationship modeling in an undergraduate course in Database Systems. The course syllabus fulfills the requirements of a first database course in the IS 2002 model curriculum. It is designed to be offered on-line for self-paced learning within the 15 weeks of a regular term. Our goal is to ensure the scope of coverage and the quality of delivery, and at the same time provide sufficient flexibility for student learning. Our teaching plan comprises 19 modules: from the introduction to Extended ER modeling covering specialization and generalization, including two optional modules for alternative ER diagram notations. The teaching plan is complete and yet very compact. The dependency graph governs the flow of the learning modules. Students will then have the flexibility to work out their schedules for their effective use of time and materials on-line. The critical design factor in each module is in the selected ER modeling features to cover, and very specific learning objectives. We believe our design will engender effective self-paced learning. The plan also prepares very well for on-line delivery. We are eager to further develop the plan by gathering a very large collection of illustrative examples, together with exercises and test questions.
183|Mapping OWL to the Entity Relationship and Extended Entity Relationship models |Abstract: This paper presents mapping rules to conceptually model an Entity Relationship (ER) diagram and Extended Entity Relationship (EER) diagram from OWL by identifying ER and EER constructs in OWL. OWL has been designed for the semantic web, but data in OWL format is not easy to manipulate or query. The conceptual view of OWL presented in this paper is necessary to understand OWL and OWL data, and will be used to eventually map OWL data to the relational model. Once in the relational model, OWL data can avail of mature relational database technology.
184|On Deep Annotation|The success of the Semantic Web crucially depends on the easy creation, integration and use of semantic data. For this purpose, we consider an integration scenario that defies core assumptions of current metadata construction methods. We describe a framework of metadata creation when web pages are generated from a database and the database owner is cooperatively participating in the Semantic Web. This leads us to the definition of ontology mapping rules by manual semantic annotation and the usage of the mapping rules and of web services for semantic queries. In order to create metadata, the framework combines the presentation layer with the data description layer in contrast to &#034;conventional&#034; annotation, which remains at the presentation layer. Therefore, we refer to the framework as deep annotation. t  We consider deep annotation as particularly valid because, (/), web pages generated from databases outnumber static web pages, (ii), annotation of web pages may be a very intuitive way to create semantic data from a database and, (iii), data from databases should not be materialized as RDF files, it should remain where it can be handled most efficiently in its databases.
185|Relational.OWL - A Data and Schema Representation Format Based on OWL|One of the research fields which has recently gained much scientific interest within the database community are Peer-to-Peer databases, where peers have the autonomy to decide whether to join or to leave an information sharing environment at any time. Such volatile data nodes may appear shortly, collect or deliver some data, and disappear again. It even can not be assured that a peer joins the network ever again.
186|Learning Ontology From Relational Database|Ontology provides a shared and reusable piece of knowledge about a specific domain, and has been applied in many fields, such as Semantic Web, e-commerce and information retrieval, etc. However, building ontology by hand is a very hard and error-prone task. Learning ontology from existing resources is a good solution. Because relational database is widely used for storing data and OWL is the latest standard recommended by W3C, this paper proposes an approach of learning OWL ontology from data in relational database. Compared with existing methods, the approach can acquire ontology from relational database automatically by using a group of learning rules instead of using a middle model. In addition, it can obtain OWL ontology, including the classes, properties, properties characteristics, cardinality and instances, while none of existing methods can acquire all of them. The proposed learning rules have been proven to be correct by practice.
187|Unveiling the hidden bride: deep annotation for mapping and migrating legacy data to the Semantic Web|The success of the Semantic Web crucially depends on the easy creation, integration, and use of semantic data. For this purpose, we consider an integration scenario that defies core assumptions of current metadata construction methods. We describe a framework of metadata creation where Web pages are generated from a database and the database owner is cooperatively participating in the Semantic Web. This leads us to the deep annotation of the database---directly by annotation of the logical database schema or indirectly by annotation of the Web presentation generated from the database contents. From this annotation, one may execute data mapping and/or migration steps, and thus prepare the data for use in the Semantic Web. We consider deep annotation as particularly valid because: (i) dynamic Web pages generated from databases outnumber static Web pages, (ii) deep annotation may be a very intuitive way to create semantic data from a database, and (iii) data from databases should remain where it can be handled most efficiently---in its databases. Interested users can then query this data directly or choose to materialize the data as RDF files.
188|XML: Current developments and future challenges for the database community|Abstract. While we can take as a fact \the Web changes everything&#034;, we argue that \XML is the means &#034; for such a change to make a sig-ni cant step forward. We therefore regard XML-related research as the most promising and challenging direction for the community of database researchers. In this paper, we approach XML-related research by taking three progressive perspectives. We rst consider XML as a data represen-tation standard (in the small), then as a data interchange standard (in the large), and nally as a basis for building a new repository technology. After a broad and necessarily coarse-grain analysis, we turn our focus to three specic research projects which are currently ongoing at the Po-litecnico di Milano, concerned with XML query languages, with active document management, and with XML-based specications of Web sites. 1
189|Future Directions of Conceptual Modeling|This paper discusses several important future research directions in  conceptual modeling. Since this field is very broad, the list of topics discussed  here is not an exhaustive list but rather the &#034;partial&#034; composite views of the coauthors  on the important issues from their own perspectives. Some of the  important issues are omitted because they were covered extensively in [1], and  the readers should also read the papers in that volume to get a more  comprehensive view of where the field is going. Also, the selection of research  topics is independent of their difficulties. Some of the research problems  discussed here could be solved in the next decades while the others may take  much longer to develop a reasonable solution.
190|Architectural Issues for Integrating XML and Relational Database Systems – The X-Ray Approach|Relational databases get more and more employed in order to store the content of a web site. At the same time, XML is fast emerging as the dominant standard at the hypertext level of web site management describing pages and links between them. Thus, the integration of XML with relational database systems to enable the storage, retrieval, and update of XML documents is of major importance. Data model heterogeneity and schema heterogeneity, however, makes this a challenging task. This paper presents design alternatives for integrating XML and relational database systems and discusses the architecture of a generic integration approach called X-Ray.
191|Research on the Rules of Mapping from Relational Model to OWL, OWLED|Abstract. Data integration provides the user with a unified view of legacy data, and the semantic mapping from relational database (local sources) to global ontologies is one of key aspects for building data integration system. The manual mapping is time-consuming and error-prone. In this paper, groups of semantic mapping rules from relational database to global OWL ontologies are proposed in detail, including rules for the classes, properties, restriction and instances, which avoid migrating large amounts of data. The rules are demonstrated with examples. They are practical for semantic mapping or ontologies learning (semi-)automatically.
192|Relaxing Constraints in Enhanced Entity-Relationship Models Using Fuzzy Quantifiers |Abstract—While various articles about fuzzy entity relationship (ER) and enhanced entity relationship (EER) models have recently been published, not all examine how the constraints expressed in the model may be relaxed. In this paper, our aim is to relax the constraints which can be expressed in a conceptual model using the modeling tool, so that these constraints can be made more flex-ible. We will also study new constraints that are not considered in classic EER models. We use the fuzzy quantifiers which have been widely studied in the context of fuzzy sets and fuzzy query systems for databases. In addition, we shall examine the repre-sentation of these constraints in an EER model and their prac-tical repercussions. The following constraints are studied: the fuzzy participation constraint, the fuzzy cardinality constraint, the fuzzy completeness constraint to represent classes and subclasses, the fuzzy cardinality constraint on overlapping specializations, fuzzy disjoint and fuzzy overlapping constraints on specializations, fuzzy attribute-defined specializations, fuzzy constraints in union types or categories and fuzzy constraints in shared subclasses. We shall also demonstrate how fuzzy (min, max) notation can substitute the fuzzy cardinality constraint but not the fuzzy participation con-straint. All these fuzzy constraints have a new meaning, they offer greater expressiveness in conceptual design, and are included in the so-called fuzzy EER model. Index Terms—Conceptual database design, extended (or enhanced) entity-relationship model (EER), fuzzy conceptual modeling, fuzzy constraints, fuzzy databases, fuzzy quantifiers. I.
194|GEFRED. A Generalized Model of Fuzzy Relational Data Bases. Ver. 1.1|In this paper, we present a Fuzzy Relational Databases model whose main characteristics are: the integration of previous models in the same framework, representation capabilities for a wide series of fuzzy information and a coherent and flexible handling of it. This model aims to solve each problem of representation and handling of fuzzy information taking into account its specific nature, and hence it allows the user to choose the comparison operator and the fuzzy compatibility measure to be used in a query. Besides, it permits the user to specify the precision with which the conditions involved in a query are satisfied. Keywords: Fuzzy Relational Database, Database, Fuzzy Sets, Relational Model 1 Introduction The aim of this paper is to present a fuzzy extension to the Databases Relational Model. This extension will try to solve the problems related to the representation and handling of imprecise information. Because of this, we will have to incorporate some new elements into the Rel...
195|Accommodating Imprecision in Database Systems: Issues and Solutions|Most database systems are designed under assumptions of precision of both the data stored in their databases, and the requests to retrieve data. In reality, however, these assumptions are often invalid, and in recent years considerable attention has been given to issues of imprecision i database systems. In this paper we review the major solutions for accommodating imprecision, and we describe issues that have yet to addressed, offering possible research directions. 1 In t roduct ion Information stored in a database is precise if it is assured to be identical to the &#034;real world&#034; information which it represents. When precise information is unavailable, it is often the case that some relevant information is nonetheless available. In these cases, it may be advantageous to design methods by which this information, termed imprecise information, can be stored, manipulated and retrieved. Imprecision may also be present in requests to retrieve data, when users, either intentionally or by necessity, formulate their queries in imprecise terms. Depending on the data model used, the information stored in a database may take different forms, and imprecision could affect each and every one of them. For example, in the entity-
196|Fuzzy types: A new concept of type for managing vague structures|The requirements of complex applications in the world of object-oriented databases have motivated the study of the addition of vagueness to the existing models, giving rise to different approaches. The presence of vagueness can be considered in the type associated to a class, parallel to and independently from a fuzzy view of the set of objects that belong to the class Ž extent of the class.. This paper offers a new perspective for representing the type associated to a class, tackling the problem of vagueness in the database schema and defining the concept of fuzzy type. Two different components of these types are defined: the structural component and the behavior component. An adaptation of the mechanism of instantiation and inheritance is presented by considering adequate union operators. A new criterion for handling the redefinition capability of the object-oriented data model is explained. The description is accompanied by some illustrative examples. ? 2000 John Wiley &amp; Sons, Inc. I.
197|A Design Methodology for Databases with Uncertain Data|this paper, we demonstrate the utility of our methods by applying our methodology to a system which must cope with imprecise information. This system is a run-to-run (R2R) process control
198|Relaxing the Universal Quantifier of the Division in Fuzzy Relational Databases |In a previous paper, we presented an approach to calculate relational division in fuzzy databases, starting with the GEFRED model. This work centered on dealing with fuzzy attributes and fuzzy values and only the universal quantifier was taken into account since it is the inherent quantifier in classical relational division. In this paper, we present an extension of that division to relax the universal quantifier. With this new system we can use both absolute quantifiers and relative quantifiers irrespective of how the function of the fuzzy quantifier is defined. We also include a comparison with other fuzzy division approaches to relax the universal quantifier that have been published. Furthermore, in this paper we have extended the fuzzy SQL language to express any kind of fuzzy division. ? 2001 John Wiley &amp; Sons, Inc. 1.
199|Uncertainty models in information and database systems|Information systems have evolved to the point where it is desirable to capture the vagueness and uncertainty of data that occurs in actuality. Approaches have been taken using various fuzzy set concepts such as degree of membership, similarity relations and possibility distributions. This leads to the concept of generalized information systems which are typically char-acterized by heterogeneous data representations, weakly typed data domains and the requirement for semantic knowledge during query interpretation. A generalized information system is more likely to have a direct representation for larger classes of information at the cost of more complex data management and query processing. In general the various fuzzy database approaches that have been developed are overviewed in the paper and characterized with respect to the concept of a generalized information system.
200|Extending Entity-Relationship Modeling Notation To Manage Fuzzy Datasets |Current work in modeling has focused on fuzziness as it applies to single entities. An application of fuzzy theory may be made to managing data sets and collections, thus treating the data sets as collection of fuzzy objects. The application of fuzzy theory to management of sets creates has not been fully explored. In the area of Geospatial Information Science (GIS), one can find many types of sets, representing ambiguous overlapping spatial extents. As part of our research, a current ERD data model notation is being populated and extended with a notation representing fuzzy theory as it applies to the problem of set management. One part of this research is in the process of developing a discretinzing function D() for fuzzy problems defined by continuous field data. D() is a specialized member of the class of functions M(), where the values it selects on are spatial definitions and temporally continuous fields. We have also developed a function M spatial (), where M spatial () is a specialized member of M() pertaining to noncontinuous spatial entities.  Background  The University of Idaho College of Forestry manages a research project referred to as the Experimental Forest. The forest encompasses 7300 acres in multiple non-contiguous tracts[1]. Because of the high usage of this laboratory, the university began to seek ways to track and annotate developments in the forest. GIS data has become more prevalent in management of the Experimental Forest. Students use the data to learn principles of GIS and to maintain forest data. Because of the many ways that GIS data about the forest is utilized, this data has become a critical information resource for the forest&#039;s management. Students are continually updating information about the Experimental Forest. Adding good data to bad...
201|Management of an estate agency allowing fuzzy data and flexible queries |As main result of all our previous work, we have now available a FSQL Server for Oracle Databases, programmed in PL/SQL. Thisserver allows us to query either a Fuzzy or Classic Relational Database with the FSQL language ( Fuzzy SQL). The FSQL language is an extension of the SQL language which permits us to write exible (or fuzzy) conditions in our queries to a fuzzy or traditional database. In this work we present a management system for a real estate agency in which some attributes of the landed properties may be fuzzy, i.e., we can store imprecise information about it. Besides, our system allows the user to make exible or fuzzy queries in order to retrieve the most relevant properties of our database, starting with the customer information. The goal is to retrieve the most interesting landed properties according to the initial customer preferences. Of course, we can obtain a membership degree for each landed property in the fuzzy query result.
202|An Analysis of Structural Validity of Ternary Relationships in Entity Relationship Modeling|____________________________________________________________________________________________________
203|Decomposition of a Data Base and the Theory of Boolean Switching Functions|Abstract: The notion of a functional relation among the attributes of a data set can be fruitfully applied in the structuring of an information system. These relations are meaningful both to the user of the system in his semantic understanding of the data, and to the designer in implementing the system. An important equivalence between operations with functional relations and operations with analogous Boolean functions is demonstrated in this paper. The equivalence is computationally helpful in exploring the properties of a given set of functional relations, as well as in the task of partitioning a data set into subfiles for efficient implementation. 1.
205|The management of changing types in an object-oriented database|We examine the problem of type evolution in an objectoriented database environment. Type definitions are persistent objects in the database and as such may be modified and shared. The effeets of changing a type extend to objects of the type and to programs that use objeets of the type. We propese a solution to the problem through an extension of the semantic data model. A ehange in the interfaee defined by a type may result in errors when programs use new or old objects of the type. Through the use of an abstraction of the type over time, timestamping and error handling mechanisms provide support for the type designer in creating compatible versions of the type. The mechanisms are incorporated Into the behavior defined by the type and are inherited via the type-lattice. 1.
206|Beyond schema evolution to database reorganization|While the c.ontents of databases can be easily changed, their organization is typically extremely rigid. Some databases relax the rigidity of database organization somewhat by supporting simpIe changes to individual
207|Bibliography: Temporal Databases|17. COSATI CODES j18. SUBJECT TERMS (Continue an revetse If necensaiy and Identify by block number&#039;
208|Specifications for automatic relational database system conversion |Changes in requirements for database systems necessitate schema restructuring, database translation, and application or query program conversion. An alternative to the lengthy manual revision process is proposed by offering a set of 15 transformations keyed to the relational model of data and the relational algebra. Motivations, examples, and detailed descriptions are provided.
209|Auguston, Marshaling and Unmarshaling Models Using Entity-Relationship Model|Software systems are usually designed and documented with the aid of visual modeling notations. Visual modeling notations keep evolving over the years in tandem with visual modeling tools, and the tight binding in between impedes the exchanging of modeling assets, which causes a spatial isolation of the models. Another problem with legacy software models is that they are isolated temporally in the early phases of the software engineering life cycle without reaching out to the later phases. This paper presents an approach for breaking both spatial and temporal isolation of software models by marshaling and unmarshaling models using the Entity-Relationship (ER) model, thus providing a promising way for evolving model-driven software development.
210|E-R Modeler: A Database Modeling Toolkit for Eclipse |Eclipse is a Java integrated development environment (IDE) and tool integration platform that offers numerous extension points for customization through a plug-in architecture. This paper describes the design of an Eclipse plug-in called E-R Modeler. As a database design toolkit, the E-R Modeler provides an E-R (Entity-Relationship) diagram development environment that supports XML and DDL (Database Definition Language) generation tools. It also provides database connection and schema creation capabilities. As an Eclipse plug-In, it offers multiple extension points for other applications to build upon.
212|Entity-Relationship Modeling Of Spatial Data For Geographic Information Systems |This article presents an extension to the basic Entity-Relationship (E-R) database modeling approach for use in designing geographic databases. This extension handles the standard spatial objects found in geographic information systems, multiple representations of the spatial objects, temporal representations, and the traditional coordinate and topological attributes associated with spatial objects and relationships. Spatial operations are also included in this extended E-R modeling approach to represent the instances where relationships between spatial data objects are only implicit in the database but are made explicit through spatial operations. The extended E-R modeling methodology maps directly into a detailed database design and a set of GIS function.
213|Language Features for Flexible Handling of Exceptions in Information Systems|We present an exception handling facility suitable for languages used to implement database-intensive Information Systems. Such a mechanism facilitates the development and maintenance of more flexible software systems by supporting the abstraction of details concerning special or abnormal occurrences. We consider the type constraints imposed by the schema as well as various semantic integrity assertions to be normalcy conditions, and the key contribution of this work is to allow exceptions to these constraints to  persist. To achieve this, we propose solutions to a range of problems, including sharing and computing with exceptional information, exception handling by users, the logic of constraints with exceptions, and implementation issues. We also illustrate the use of exception handling in dealing with null values, estimates, and measurements. Keywords and phrases: semantic integrity, violations of type constraints, exception handling, accommodating exceptions, conceptual models  CR ...
214|S.: Context-Adaptive Approach for Automated Entity Relationship Modeling|Even a smart data modeler may not be an expert in terms of job knowledge. Hence, the design of a database model is limited by the data modeler’s resolution and subjectivity. Because the data modeler transforms a domain user’s representations into a database model on the basis of arbitrary decisions, the data modeler may distort or lose information. The best way of designing a database is for a domain user to lay out a database, though this approach might impose a heavy modeling burden on the user. Many traditional automated design systems have failed to become widely used. We propose a new model, the association-based conceptual model (ABCM), for an ordinary field worker. The ABCM does not require a user to have expert knowledge to discriminate entities from attributes and relies solely on business descriptions to generate an appropriate ERD. We devise a context-adaptive approach to automate the creation of ERD, which means that ER modeling depends on the context of a business description. Accordingly, this approach performs modeling by analyzing contexts in a business description that the user creates and then utilizing associations among the various contexts. We introduce the scope of the proposed system and present the detailed logic of the system. Finally, we perform a case study to evaluate the devised system’s applicability to practical business fields.
215|Data Mining: Concepts and Techniques|Our capabilities of both generating and collecting data have been increasing rapidly in the last several decades. Contributing factors include the widespread use of bar codes for most commercial products, the computerization of many business, scientific and government transactions and managements, and advances in data collection tools ranging from scanned texture and image platforms, to on-line instrumentation in manufacturing and shopping, and to satellite remote sensing systems. In addition, popular use of the World Wide Web as a global information system has flooded us with a tremendous amount of data and information. This explosive growth in stored data has generated an urgent need for new techniques and automated tools that can intelligently assist us in transforming the vast amounts of data into useful information and knowledge. This book explores the concepts and techniques of data mining, a promising and flourishing frontier in database systems and new database applications. Data mining, also popularly referred to as knowledge discovery in databases (KDD), is the automated or convenient extraction of patterns representing knowledge implicitly stored in large databases, data warehouses, and other massive information repositories. Data mining is a multidisciplinary field, drawing work from areas including database technology, artificial intelligence, machine learning, neural networks, statistics, pattern recognition, knowledge based systems, knowledge acquisition, information retrieval, high performance computing, and data visualization. We present the material in
216|The architecture of complexity|A number of proposals have been advanced in recent years for the development of “general systems theory ” that, abstracting from properties peculiar to physical, biological, or social systems, would be applicable to all of them. 1 We might well feel that, while the goal is laudable, systems of such diverse kinds could hardly be expected to have any nontrivial properties in common. Metaphor and analogy can be helpful, or they can be misleading. All depends on whether the similarities the metaphor captures are significant or superficial. It may not be entirely vain, however, to search for common properties among diverse kinds of complex systems. The ideas that go by the name of cybernetics constitute, if not a theory, at least a point of view that has been proving fruitful over a wide range of applications. 2 It has been useful to look at the behavior of adaptive systems in terms of the concepts of feedback and homeostasis, and to analyze adaptiveness in terms of the theory of selective information. 3 The ideas of feedback and information provide a frame of reference for viewing a wide range of situations, just as do the ideas of evolution, of relativism, of axiomatic method, and of
217|An ontological analysis of the relationship construct in conceptual modeling|Conceptual models or semantic data models were developed to capture the meaning of an application domain as perceived by someone. Moreover, concepts employed in semantic data models have recently been adopted in object-oriented approaches to systems analysis and design. To employ conceptual modeling constructs effectively, their meanings have to be defined rigorously. Often, however, rigorous definitions of these constructs are missing. This situation occurs especially in the case of the relationship construct. Empirical evidence shows that use of relationships is often problematical as a way of communicating the meaning of an application domain. For example, users of conceptual modeling methodologies are frequently confused about whether to show an association between things via a relationship, an entity, or an attribute. Because conceptual models are intended to capture knowledge about a real-world domain, we take the view that the meaning of modeling constructs should be sought in models of reality. Accordingly, we use ontology, which is the branch of philosophy dealing with models of reality, to analyze the meaning of common conceptual modeling constructs. Our analysis provides a precise definition of several conceptual modeling constructs. Based on our analysis, we derive rules for the use of relationships in entity-relationship conceptual modeling. Moreover, we show how the rules resolve ambiguities that exist in current practice and how they can enrich the capacity of an entity-relationship conceptual model to capture knowledge about an application domain.
218|Knowledge-Based Automation of a Design Method For Concurrent Systems|This paper describes a knowledge-based approach to automate a software design method for concurrent systems. The  approach uses multiple paradigms to represent knowledge embedded in the design method. Semantic data modeling provides the  means to represent concepts from a behavioral modeling technique, called Concurrent Object-Based Real-time Analysis (COBRA),  which defines system behavior using data/control flow diagrams. Entity-Relationship modeling is used to represent a design  metamodel based on a design method, called COncurrent Design Approach for Real-Time Systems (CODARTS), which represents  concurrent designs as software architecture diagrams, task behavior specifications, and module specifications. Production rules  provide the mechanism for codifying a set of CODARTS heuristics that can generate concurrent designs based on semantic concepts  included in COBRA behavioral models and on entities and relationships included in CODARTS design metamodels. Together, the  semantic data model, the entity-relationship model, and the production rules, when encoded using an expert-system shell, compose  CODA, an automated designer&#039;s assistant. Other forms of automated reasoning, such as knowledge-based queries, can be used to  check the correctness and completeness of generated designs with respect to properties defined in the CODARTS design metamodei.
219|Acquisition of Entity Relationship Models for Maitenance - Dealing with Data Intensive Programs in A Transformation System|This paper presents results of a research programme on reverse engineering using a transformation system for maintenance and focuses on dealing with data-intensive programs such as those written in COBOL. Problems with data-intensive programs are addressed, our solutions to these problems are discussed and the results of experiments are presented. It is concluded that formal transformations provide a way of combining design decisions which have become instantiated in both the code and the data structures. We describe a solution to the problem of acquisition Entity Relationship (ER) diagrams from data-intensive source code. In such programs, the relationships between data items are often represented within imperative code as well as within data structures, and we show that reverse engineering can be improved if both are used. This distinguishes our work from other works in the field. Our method is based on formal transformations. We identify imperative constructs which improve the high-level ER models that can be captured. Suitable transformations are then briefly summarised. A series of experiments with industrial COBOL programs is described. Our results show that code-embedded relations can be usefully incorporated into data intensive reverse engineering, and that they enhance the designs extracted.
220|Formalized Entity Extraction Methodology for Changeable Business Requirements |Without a formal methodology extracting entities from business descriptions, a business requirement in the real world cannot be abstracted correctly into an entity-relationship schema. Once core entities are discovered, we can obtain an Entity-Relationship Diagram (ERD) by inserting relationships between/among the relevant entities and by aggregating some attributes into one of the entities or relationships. There have been so many studies on formal guidelines for extracting entities from business descriptions. Most of them adopt a knowledge-based approach which consults a knowledge base to recommend entity candidates. However, the knowledge-based approach usually fails to construct the most appropriate ERD for a given business domain. The approach performs the entity extraction on the stiff premise that an object would be classified as an entity if it happen to be classified as an entity once or more in past applications. The previous studies did not consider the flexibility in the object classification that even the same object could be regarded as either an entity or an attribute according to the various concerns of field workers. In this paper, we discuss some limitations of the previous researches on object classification and propose a new methodology for flexible entity extraction. To evaluate the practicality of the devised methodology, we developed a tool for the methodology and performed a case study on option trading applications with the tool.
221|Prism: A Case Study in Behavioral Entity-Relationship Modeling and Design|We describe the collaboratively developed Prism, a tightly integrated but architecturally flexible environment for modeling, visualizing, and simulating radiation treatment plans for cancer patients. Despite significant advances in function, we built Prism at low cost in effort and code size. The key was using behavioral entity-relationship (ER) modeling to craft an architecture, and objects representing abstract behavioral types (ABTs) to implement it. In more detail, we model a system as a set of independently defined entities used directly by clients but made to work together by behavioral relationships connecting them. We implement this model in an imperative programming framework by representing both entities and relationships as instances of classes that define, announce, and register with events in addition to defining and calling operations. Prism provided a realistic test of the behavioral ER modeling and design method. 3  This work was supported in part by grant number R01 LM...
222|An Overview of the C++ Programming Language|This overview of C++ presents the key design, programming, and language-technical concepts using examples to give the reader a feel for the language. C++ is a general-purpose programming language with a bias towards systems programming that supports efficient low-level computation, data abstraction, object-oriented programming, and generic programming.  
223|On the Criteria To Be Used in Decomposing Systems into Modules|This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a “modularization ” is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.
224|The X Window System|The X Window System, Version 11, is the standard window system on Linux and UNIX systems. X11, designed in 1987, was “state of the art ” at that time. From its inception, X has been a network transparent window system in which X client applications can run on any machine in a network using an X server running on any display. While there have been some significant extensions to X over its history (e.g. OpenGL support), X’s design lay fallow over much of the 1990’s. With the increasing interest in open source systems, it was no longer sufficient for modern applications and a significant overhaul is now well underway. This paper describes revisions to the architecture of the window system used in a growing fraction of desktops and embedded systems 1
225|Program Restructuring as an Aid to Software Maintenance|  Maintenance tends to degrade the structure of software, ultimately making maintenance more costly. At times, then, it is worthwhile to manipulate the structure of a system to make changes easier. However, it is shown that manual restructuring is an error-prone and expensive activity. By separating structural manipulations from other maintenance activities, the semantics of a system can be held constant by a tool, assuring that no errors are introduced by restructuring. To allow the maintenance team to focus on the aspects of restructuring and maintenance requiring human judgment, a transformation-based tool can be provided---based on a model that exploits preserving data flow-dependence and control flow-dependence---to automate the repetitive, errorprone, and computationally demanding aspects of re...
226|Adding implicit invocation to languages: Three approaches|Implicit invocation based on event announcement is an increasingly impor-tant technique for integrating systems. However, the use of this technique has largely been conned to tool integration systems|in which tools exist as indepen-dent processes|and special-purpose languages|in which specialized forms of event broadcast are designed into the language from the start. This paper broadens the class of systems that can benet from this approach by showing how to augment general-purpose programming languages with facilities for implicit invocation. We illustrate the approach in the context of three dierent languages, Ada, C++, and Common Lisp. The intent is to highlight the key design considerations that arise in extending such languages with implicit invocation. 1
227|Entity-Relationship Modeling: Historical Events, Future Trends, and Lessons Learned|This paper describes the historial developments of the ER model from the 70&#039;s to recent years. It starts with a discussion of the motivations and the environmental factors in the early days. Then, the paper points out the role of the ER model in the Computer-Aided Software Engineering (CASE) movement in the late 80&#039;s and early 90&#039;s. It also describes the possibility of the role of author&#039;s Chinese culture heritage in the development of the ER model. In that context, the relationships between natural languages (including Ancient Egyptian hieroglyphs) and ER concepts are explored. Finally, the lessons learned and future directions are presented. 1 
228|Ancient Egyptian Language to Future Conceptual Modeling,” in: Conceptual Modeling: Current Issues and Future|Abstract. This paper discusses the construction principles of ancient Egyptian hieroglyphs from the point of view of conceptual modeling. The paper starts with a summary of author’s previous work on the correspondence between the Entity-Relationship diagrammatic (ERD) technique and two natural languages: English and Chinese. In one previous work, the similarity between the English sentence structure/grammar and the construction blocks and rules of ERD was discovered. In another work, the similarity between the Chinese character construction methods and the ER modeling principles was also discovered. In this paper, construction methods of the ancient Egyptian hieroglyph are analyzed with respect to the construction methods of Chinese characters and the conceptual modeling principles. At the end, possible applications and extensions of this research work are discussed. 1
229|Efficient data retrieval and manipulation using Boolean entity lattice|This paper proposes a technique to organize the database such that retrievals and manipulations will be operated on a subset instead of the whole entity set. The conceptual partitioning lattice and the Boolean partitioning lattice of an entity set are introduced as basic concepts. A technique is presented for determining all descendant atoms (subsets) of a given entity set. It is demonstrated that using the Boolean lattice to describe the structure between entity subsets is better than using an ordinary lattice. It is also demonstrated that using the Boolean lattice to organize a database can make data manipulation and retrieval operations more efficient.
230|TOPO-NET SPATIAL ENTITY RELATIONSHIP MODEL FOR GEOGRAPHIC INFORMATION SYSTEM APPLICATIONS |Rapid development of Geographic Information System (GIS) has given rise to great amount of spatial database, around which a large number of GIS applications are being created. A GIS based network application for planning and management being developed in the country has typical requirement for representation of datasets at primary level.This paper examines the representation of such datasets through ER and EER models and summarizes their limitation. In order to represent the evolving database design for GIS based network application Topo-Net Spatial ER was evolved and proposed with focus on representation of spatial, topological,network and generalization aspects and has led to successful conceptual database design. The representation have been achieved through moderating EER Model with topological and network concepts and hence termed as Topo-Net Spatial ER model. The approach and representations of the same are described in the present paper. The approach and representations of the same are described in the present paper and is being demonstrated through a case study for Optical Fiber Cable Network.
231|GeoOOA: Object-Oriented Analysis for Geographic Information Systems|We stress the need of a domain-tailored requirements engineering method for the development of GIS-applications. To this end, we discuss three GIS-application scenarios, the so-called topological scenario, the network scenario and the (spatio-) temporal scenario, we frequently encountered in real life projects. Our investigations reveal that conventional RE-methods like e.g. Coad/Yourdon&#039;s Object-Oriented Analysis (OOA) are not capable to model GIS-specific requirements appropriately. To overcome the deficiencies of conventional OOA we introduce GeoOOA which complements OOA by suitable domain-tailored primitives and allows for a more adequate treatment of GIS-specific requirements. We provide GeoOOA-models of a  broadcasting corporation, an electric power supply system and a land registry as first validation steps for the usefulness of GeoOOA.
232|Unsupervised Models for Named Entity Classification|This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple “seed ” rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98). 1
234|A Theory of the Learnable|  Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems.
235|Combining labeled and unlabeled data with co-training|We consider the problem of using a large unlabeled sample to boost performance of a learning algorithm when only a small set of labeled examples is available. In particular, we consider a setting in which the description of each example can be partitioned into two distinct views, motivated by the task of learning to classify web pages. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks that point to that page. We assume that either view of the example would be su cient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment amuch smaller set of labeled examples. Speci cally, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm&#039;s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to signi cant improvement of hypotheses in practice. As part of our analysis, we provide new re-
236|Automatic Acquisition of Hyponyms from Large Text Corpora|We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidante of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also he acquirable iu this way. A subset of the acquisitiou algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.
237|Improved Boosting Algorithms Using Confidence-rated Predictions| We describe several improvements to Freund and Schapire’s AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper.
238|The Strength of Weak Learnability|Abstract. This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a Source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent. A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error e.
239|A New Statistical Parser Based on Bigram Lexical Dependencies|This paper describes a new statistical  parser which is based on probabilities of  dependencies between head-words in the  parse tree. Standard bigram probability estimation  techniques are extended to calculate  probabilities of dependencies between  pairs of words. Tests using Wall Street  Journal data show that the method per-  forms at least as well as SPATTER (Magerman  95; Jelinek et al. 94), which has  the best published results for a statistical  parser on this task. The simplicity of the  approach means the model trains on 40,000  sentences in under 15 minutes. With a  beam search strategy parsing speed can be  improved to over 200 sentences a minute  with negligible loss in accuracy.
240|Extracting patterns and relations from the world wide web|Abstract. The World Wide Web is a vast resource for information. At the same time it is extremely distributed. A particular type of data such as restaurant lists may be scattered across thousands of independent information sources in many di erent formats. In this paper, we consider the problem of extracting a relation for such a data type from all of these sources automatically. We present a technique which exploits the duality between sets of patterns and relations to grow the target relation starting from a small sample. To test our technique we use it to extract a relation of (author,title) pairs from the World Wide Web. 1
241|Finding Parts in Very Large Corpora|We present a method for extracting parts of objects from wholes (e.g. &#034;speedometer&#034; from &#034;car&#034;). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon.
242|Unsupervised Learning of Disambiguation Rules for Part of Speech Tagging|In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus. We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers. Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text.
243|Additive Models, Boosting, and Inference for Generalized Divergences|We present a framework for designing incremental learning algorithms derived from generalized entropy functionals. Our approach is based on the use of Bregman divergences together with the associated class of additive models constructed using the Legendre transform. A particular one-parameter family of Bregman divergences is shown to yield a family of loss functions that includes the log-likelihood criterion of logistic regression as a special case, and that closely approximates the exponential loss criterion used in the AdaBoost algorithms of Schapire et al., as the natural parameter of the family varies. We also show how the quadratic approximation of the gain in Bregman divergence results in a weighted least-squares criterion. This leads to a family of incremental learning algorithms that builds upon and extends the recent interpretation of boosting in terms of additive models proposed by Friedman, Hastie, and Tibshirani. 1 Introduction  Logistic regression is a widely used statisti...
244|Technical Report: Activity Recognition in Wide Aerial Video Surveillance Using Entity Relationship Models|We present the design and implementation of an activity recognition system for wide area aerial video surveillance using Entity Relationship Models (ERM). In this approach, finding an activity is equivalent to sending a query to the Relational DataBase Management System (RDBMS). By incorporating reference imagery and Geographic Information System (GIS) data, tracked objects can be associated with physical meanings, and several high levels of reasoning, such as traffic patterns or abnormal activity detection, can be performed. We demonstrate that different types of activities, with hierarchical structure, multiple actors, and context information, are effectively and efficiently defined and inferred using the ERM framework. We also show how visual tracks can be better interpreted as activities by using geo information. Experimental results on both real visual tracks and GPS traces validate our approach. 1.
245|Recognition of visual activities and interactions by stochastic parsing|This paper describes a probabilistic syntactic approach to the detection and recognition of temporally extended activities and interactions between multiple agents. The fundamental idea is to divide the recognition problem into two levels. The lower level detections are performed using standard independent probabilistic event detectors to propose candidate detections of low-level features. The outputs of these detectors provide the input stream for a stochastic context-free grammar parsing mechanism. The grammar and parser provide longer range temporal constraints, disambiguate uncertain low-level detections, and allow the inclusion of a priori knowledge about the structure of temporal events in a given domain. To achieve such a system we: 1) provide techniques for generating a discrete symbol stream from continuous low-level detectors; 2) extend stochastic context-free parsing to handle uncertainty in the input symbol stream; 3) augment a run-time parsing algorithm to enforce intersymbol constraints such as requiring temporal consistency between primitives; and 4) extend the consistency filtering to maintain consistent multiobject interactions. We develop a real-time system and demonstrate the approach in several experiments on gesture recognition and in video surveillance. In the surveillance application, we show how the system correctly interprets activities of multiple, interacting objects.
246|Machine recognition of human activities: A survey| The past decade has witnessed a rapid proliferation of video cameras in all walks of life and has resulted in a tremendous explosion of video content. Several applications such as content-based video annotation and retrieval, highlight extraction and video summarization require recognition of the activities occurring in the video. The analysis of human activities in videos is an area with increasingly important consequences from security and surveillance to entertainment and personal archiving. Several challenges at various levels of processing—robustness against errors in low-level processing, view and rate-invariant representations at midlevel processing and semantic representation of human activities at higher level processing—make this problem hard to solve. In this review paper, we present a comprehensive survey of efforts in the past couple of decades to address the problems of representation, recognition, and learning of human activities from video and related applications. We discuss the problem at two major levels of complexity: 1) “actions ” and 2) “activities. ” “Actions ” are characterized by simple motion patterns typically executed by a single human. “Activities ” are more complex and involve coordinated actions among a small number of humans. We will discuss several approaches and classify them according to their ability to handle varying degrees of complexity as interpreted above. We begin with a discussion of approaches to model the simplest of action classes known as atomic or primitive actions that do not require sophisticated dynamical modeling. Then, methods to model actions with more complex dynamics are discussed. The discussion then leads naturally to methods for higher level representation of complex activities.  
247|Objects in action: an approach for combining action understanding and object perception|Analysis of videos of human-object interactions involves understanding human movements, locating and recogniz-ing objects and observing the effects of human movements on those objects. While each of these can be conducted independently, recognition improves when interactions be-tween these elements are considered. Motivated by psycho-logical studies of human perception, we present a Bayesian approach which unifies the inference processes involved in object classification and localization, action understanding and perception of object reaction. Traditional approaches for object classification and ac-tion understanding have relied on shape features and move-ment analysis respectively. By placing object classification and localization in a video interpretation framework, we can localize and classify objects which are either hard to localize due to clutter or hard to recognize due to lack of discriminative features. Similarly, by applying context on human movements from the objects on which these move-ments impinge and the effects of these movements, we can segment and recognize actions which are either too subtle to perceive or too hard to recognize using motion features alone. 1.
248|A ‘‘string of feature graphs’’ model for recognition of complex activities|Videos usually consist of activities involving interactions between multiple actors, sometimes referred to as complex activities. Recognition of such activities requires modeling the spatio-temporal relationships between the actors and their individual variabilities. In this paper, we consider the problem of recognition of complex activities in a video given a query example. We propose a new feature model based on a string representation of the video which respects the spatio-temporal ordering. This ordered arrangement of local collections of features (e.g., cuboids, STIP), which are the characters in the string, are initially matched using graph-based spectral techniques. Final recognition is obtained by matching the string representations of the query and the test videos in a dynamic programming framework which allows for variability in sampling rates and speed of activity execution. The method does not require tracking or recognition of body parts, is able to identify the region of interest in a cluttered scene, and gives reasonable performance with even a single query example. We test our approach in an example-based video retrieval framework with two publicly available complex activity datasets and provide comparisons against other methods that have studied this problem. 1.
249|Structure from statistics - unsupervised activity analysis using suffix trees|Models of activity structure for unconstrained environments are generally not available a priori. Recent representational approaches to this end are limited by their computational complexity, and ability to capture activity structure only up to some fixed temporal scale. In this work, we propose Suffix Trees as an activity representation to efficiently extract structure of activities by analyzing their constituent event-subsequences over multiple temporal scales. We empirically compare Suffix Trees with some of the previous approaches in terms of feature cardinality, discriminative prowess, noise sensitivity and activity-class discovery. Finally, exploiting properties of Suffix Trees, we present a novel perspective on anomalous subsequences of activities, and propose an algorithm to detect them in linear-time. We present comparative results over experimental data, collected from a kitchen environment to demonstrate the competence of our proposed framework. 1. Introduction &amp; Previous
250|PLASMA: Combining Predicate Logic and Probability for Information Fusion and Decision Support. Paper presented at the AAAI Spring Symposium|Decision support and information fusion in complex domains requires reasoning about inherently uncertain properties of and relationships among varied and often unknown number of entities interacting in differing and often unspecified ways. Tractable probabilistic reasoning about such complex situations requires combining efficient inference with logical reasoning about which variables to include in a model and what the appropriate probability distributions are. This paper describes the PLASMA architecture for predicate logic based assembly of situationspecific probabilistic models. PLASMA maintains a declarative representation of a decision theoretically coherent first-order probabilistic domain theory. As evidence about a situation is absorbed and queries are processed, PLASMA uses logical inference to reason about which known and/or hypothetical entities to represent explicitly in the situation model, which known and/or uncertain relationships to represent, what functional forms and parameters to specify for the local distributions, and which exact or approximate inference and/or optimization techniques to apply. We report on a prototype implementation of the PLASMA architecture within IET’s Quiddity*Suite, a knowledge-based probabilistic reasoning toolkit. Examples from our application experience are discussed.
251|Recognizing linked events: Searching the space of feasible explanations|The ambiguity inherent in a localized analysis of events from video can be resolved by exploiting constraints between events and examining only feasible global explanations. We show how jointly recognizing and linking events can be formulated as labeling of a Bayesian network. The framework can be extended to multiple linking layers, expressing explanations as compositional hierarchies. The best global explanation is the Maximum a Posteriori (MAP) solution over a set of feasible explanations. The search space is sampled using Reversible Jump Markov Chain Monte Carlo (RJMCMC). We propose a set of general move types that is extensible to multiple layers of linkage, and use simulated annealing to find the MAP solution given all observations. We provide experimental results for a challenging two-layer linkage problem, demonstrating the ability to recognise and link drop and pick events of bicycles in a rack over five days. 1.
252|Modeling Strategic Relationships for Process Reengineering|Existing models for describing a process (such as a business process or a software development process) tend to focus on the \what &#034; or the \how &#034; of the process. For example, a health insurance claim process would typically be described in terms of a number of steps for assessing and approving a claim. In trying to improve orredesign a process, however, one also needs to have an understanding of the \why &#034;  { for example, why dophysicians submit treatment plans to insurance companies before giving treatment? and why do claims managers seek medical opinions when assessing treatment plans? An understanding of the motivations and interests of process participants is often crucial to the successful redesign of processes. This thesis proposes a modelling framework i (pronounced i-star) consisting of two modelling components. The Strategic Dependency (SD) model describes a process in terms of intentional dependency relationships among agents. Agents depend on each other for goals to be achieved, tasks to be performed, and resources to be furnished. Agents are intentional in that they have desires and wants, and strategic in that they are concerned about opportunities and vulnerabilities. The Strategic Rationale (SR) model describes the issues and concerns that agents
253|A Field Study of the Software Design Process for Large Systems|The problems of designing large software systems were studied through interviewing personnel from 17 large projects. A layered behavioral model is used to analyze how three lgf these problems-the thin spread of application domain knowledge, fluctuating and conflicting requirements, and communication bottlenecks and breakdowns-affected software productivity and quality through their impact on cognitive, social, and organizational processes.
254|Representing and Using Non-Functional Requirements: A Process-Oriented Approach|The paper proposes a comprehensive framework for representing and using non-functional requirements during the development process. The framework consists of five basic components which provide for the representation of non-functional requirements in terms of interrelated goals. Such goals can be refined through refinement methods and can be evaluated in order to determine the degree to which a set of non-functional requirements is supported by a particular design. Evidence for the power of the framework is provided through the study of accuracy and performance requirements for information systems.  1 
255|Telos: Representing Knowledge About Information Systems|This paper describes a language that is intended to support software engineers in the development of information systems throughout the software lifecycle. This language is not a programming language. Following the example of a number of other software engineering projects, our work is based on the premise that information system development is knowledge-intensive and that the primary responsibility of any language intended to support this task is to be able to formally represent the relevant knowledge.
256|Knowledge Representation and Reasoning in the Design of Composite Systems|Abstract- Our interest is in the design process that spans the gap between the requirements acquisition process and the implementation process, in which the basic architecture of a system is defined, and functions are allocated to software, hard-ware, and human agents. We call this process composite system design. Our goal is an interactive model of composite system design incorporating deficiency-driven design, formal analysis, incremental design and rationalization, and design reuse. We discuss knowledge representations and reasoning techniques for the product (composite system) that we are designing, and for the design process, which support these goals. To evaluate our model, we report on its use to rationally reconstruct the design of two existing composite systems. Index Terms-Automated analysis, composite systems, knowl-edge-based design, rational reconstruction, software specification. 0
257|Characterizing and Assessing a Large-Scale Software Maintenance Organization|One important component of a software process is the organizational context in which the process is enacted. This component is often missing or incomplete in current process modeling approaches. One technique for modeling this perspective is the Actor-Dependency (AD) Model. This paper reports on a case study which used this approach to analyze and assess a large software maintenance organization. Our goal was to identify the approach&#039;s strengths and weaknesses while providing practical recommendations for improvement. The AD model was found to be very useful in capturing the important properties of the organizational context of the maintenance process, and aided in the understanding of the flaws found in this process. However, a number of opportunities for extending and improving the AD model were identified. Among others, there is a need to incorporate quantitative information to complement the qualitative model. 1. Introduction It has now been recognized that, in order to improve the...
258|Goal-Based Process Analysis: A Method for Systematic Process Redesign|A method is proposed for systematically analyzing and redesigning processes. The method, Goal-based Process Analysis (GPA), helps its user to systematically identify missing objectives, ensure implementation of all the objectives, identify non-functional parts of a process, and explore alternative processes for achieving a given set of objectives. As such, GPA addresses a critical component in process reengineering, that of identifying which part of a given process needs to be improved and what alternatives could be used instead.  Keywords  Process Redesign, Process Analysis, Goal Analysis, Work Flow Design, Organizational Design  1. INTRODUCTION  Critical in process reengineering is some way of identifying what needs to be redesigned as well as understanding what alternatives we have. This paper proposes a method, Goal-based Process Analysis (GPA), that provide a systematic way to: . identify missing goals . ensure implementation of all the goals . identify non-functional parts of a p...
259|Representation and Utilization of Non-Functional Requirements for Information System Design|The complexity and usefulness of large information systems are determined partly by their functionality, i.e., what they do, and partly by global constraints on their accuracy, security, cost, user-friendliness, performance, and the like. Even with the growing interest in developing higher-level models and design paradigms, current technology is inadequate both representationally for expressing such global constraints as formal non-functional requirements and methodologically for utilizing them in generating designs. We propose both a representational and methodological framework for non-functional requirements, focusing on accuracy requirements. With the premise that accuracy is an inherent semantic attribute of information, we take a first step towards establishing a representational basis for accuracy. To guide the design process and justify design decisions, we propose a goal-oriented methodology. In the methodology, accuracy requirements are treated as (potentially conflicting) go...
260|Using Quality Requirements To Systematically Develop Quality Software|. Although quality issues such as accuracy, security, and performance are often crucial to the success of a software system, there has been no systematic way to achieve quality requirements during system development. We offer a framework and an implemented tool which treat quality requirements as goals to be achieved systematically during the system development process. We illustrate the process that a developer would go through, in building quality into a system. We have tested the framework on a number of studies involving a variety of quality requirements, organisational settings, and system types. Keywords: non-functional requirements, accuracy, security, performance, information systems, process, software quality, defect detection, conflicts. 1 Problem  Software development is traditionally driven by functional requirements, i.e., the desired functionality of the system. For example, a credit card system should debit and credit accounts, check credit limits, charge interest, issue...
261|Multi-Valued Relationship Attributes in Extended Entity Relationship Model and Their Mapping to Relational Schema|Conceptual modeling is one of the most important phases in designing database applications. The success of this design relies heavily on how clearly the real world requirements are represented in the conceptual model. To date, the Extended Entity Relationship (EER) model extended from the traditional Entity Relationship (ER) model is a widely used modeling technique during the phase of conceptual modeling. This paper identifies semantic ambiguities that are still present in the EER model leading to incorrect knowledge representation and eventually to incorrect design of relational database schema. These ambiguities are identified in case of many-to-many relationships which have their own attributes. This paper shows that mapping such relationships to a relational database schema generates relations having primary keys which cannot guarantee unique tuples for real world data thus violating the definition of a primary key. In addition, it shows that these relations may not satisfy second normal form. A number of such cases are elaborated and a new concept of multi-valued relationship attribute is introduced that can successfully represent these real world constraints. For this concept, a diagrammatic notation to use in ER diagram is introduced. A mapping algorithm to transform the corresponding EER model to a relational database schema is also defined. This concept of multi-valued relationship attribute and its mapping to relational schema generate relations which satisfy higher normal forms.
262|A Compositional Approach to Performance Modelling|Performance modelling is concerned with the capture and analysis of the dynamic behaviour of computer and communication systems. The size and complexity of many modern systems result in large, complex models. A compositional approach decomposes the system into subsystems that are smaller and more easily modelled. In this thesis a novel compositional approach to performance modelling is presented. This approach is based on a suitably enhanced process algebra, PEPA (Performance Evaluation Process Algebra). The compositional nature of the language provides benefits for model solution as well as model construction. An operational semantics is provided for PEPA and its use to generate an underlying Markov process for any PEPA model is explained and demonstrated. Model simplification and state space aggregation have been proposed as means to tackle the problems of large performance models. These techniques are presented in terms of notions of equivalence between modelling entities. A framewo...
263|Attention, similarity, and the identification-Categorization Relationship|A unified quantitative approach to modeling subjects &#039; identification and categorization of multidimensional perceptual stimuli is proposed and tested. Two subjects identified and categorized the same set of perceptually confusable stimuli varying on separable dimensions. The identification data were modeled using Sbepard&#039;s (1957) multidimensional scaling-choice framework. This framework was then extended to model the subjects &#039; categorization performance. The categorization model, which generalizes the context theory of classification developed by Medin and Schaffer (1978), assumes that subjects store category exemplars in memory. Classification decisions are based on the similarity of stimuli to the stored exemplars. It is assumed that the same multidimensional perceptual representation underlies performance in both the identification and Categorization paradigms. However, because of the influence of selective attention, similarity relationships change systematically across the two paradigms. Some support was gained for the hypothesis that subjects distribute attention among component dimensions so as to optimize categorization performance. Evidence was also obtained that subjects may have augmented their category representations with inferred exemplars. Implications of the results for theories of multidimensional scaling and categorization are discussed. 
264|Attention and learning processes in the identification and categorization of integral stimuli|The relationship between subjects &#039; identification and categorization learning of integral-dimension stimuli was studied within the framework of an exemplar-based generalization model. The model was used to predict subjects &#039; learning in six different categorization conditions on the basis of data obtained in a single identification learning condition. A crucial assumption in the model is that because of selective attention to component dimensions, similarity relations may change in systematic ways across different experimental contexts. The theoretical analysis provided evidence that, at least under unspeeded conditions, selective attention may play a critical role in determining the identification-categorization relationship for integral stimuli. Evidence was also provided that similarity among exemplars decreased as a function of identification learning. Various alternative classification models, including prototype, multiple-prototype, average distance, and &#034;value-on-dimensions&#034; models, were unable to account for the results. This article seeks to characterize performance relations between the two fundamental classification paradigms of identification and categorization. Whereas in an identification paradigm people identify stimuli as unique items (a one-to-one
265|Strategies and classification learning|How do strategies affect the learning of categories that lack necessary and suf-ficient attributes? The usual answer is that different strategies correspond to different models. In this article we provide evidence for an alternative view— Strategy variations induced by instructions affect only the amount of information represented about attributes, not the process operating on these representations. The experiment required subjects to classify schematic faces into two categories. Three groups of subjects worked with different sets of instructions: roughly, form
266|An evaluation of the identification|ollections of tiny, inexpensive wire-less sensor nodes capable of contin-uous, detailed, and unobtrusive measurement have attracted much attention in the past few years.1 Prototypes exist for applications such as early detection of factory equipment failure, opti-mization of building energy use, habitat mon-itoring, microclimate monitoring, and moni-toring structural integrity against earthquakes. Unfortunately, the very prop-erties that make sensor nodes attractive for these applica-tions—low cost, small size, wireless functioning, and timely,
267|An experimental and theoretical investigation of the constant-ratio rule and other models of visual letter confusion|The constant-ratio rule (CRR) and four interpretations of R. D. Lute’s (In R. D. Lute,
268|Classification in well-defined and ill-defined categories: Evidence for common processing strategies|had criterial features and that category membership could be determined by logical rules for the combination of features. More recent theories have assumed that categories have an ill-defined structure and have proposed probabilistic or global similarity models for the verification of category membership. In the experiments reported here, several models of categorization were compared, using one set of categories having criterial features and another set having an ill-defined structure. Schematic faces were used as exemplars in both cases. Because many models depend on distance in a multidimensional space for their predictions, in Experiment 1 a multidimensional scaling study was performed using the faces of both sets as stimuli. In Experiment 2, subjects learned the category membership of faces for the cate-gories having criterial features. After learning, reaction times for category verification and typicality judgments were obtained. Subjects also judged the similarity of pairs of faces. Since these categories had characteristic as well as defining features, it was possible to test the predictions of the feature comparison model (Smith et al.), which
269|A psychophysical approach to dimensional separability|Combinations of some physically independent dimensions appear to fuse into a single perceptual attribute, whereas combinations of other dimensions leave the dimensions perceptually distinct. This apparent difference in the perceived dis-tinctiveness of visual dimensions has previously been explained by the postulation of two types of internal representations-integral and separable. It is argued that apparent integrality, as well as its intermediate forms, can result from a single type of representation (the separable type), due to various degrees of correspondence between physical and separable psychological dimensions. Three experiments tested predictions of this new conceptualization of dimensional separability. Ex-periment 1 demonstrated that a physical dimension corresponding to a separable psychological dimension did not produce interference, whereas a physical di-mension not corresponding to a separable psychological dimension did produce interference. Experiment 2 showed that the pattern of results obtained in Exper-iment 1 could not be accounted for by similarity relations between stimuli. Ex-periment 3 showed that degrees of correspondence could account for different
270|Empirical Analysis of Predictive Algorithm for Collaborative Filtering|1
271|Learning Bayesian networks: The combination of knowledge and statistical data|We describe scoring metrics for learning Bayesian networks from a combination of user knowledge and statistical data. We identify two important properties of metrics, which we call event equivalence and parameter modularity. These properties have been mostly ignored, but when combined, greatly simplify the encoding of a user’s prior knowledge. In particular, a user can express his knowledge—for the most part—as a single prior Bayesian network for the domain. 1
273|Loopy Belief Propagation for Approximate Inference: An Empirical Study|Recently, researchers have demonstrated that  &#034;loopy belief propagation&#034; --- the use of  Pearl&#039;s polytree algorithm in a Bayesian  network with loops --- can perform well  in the context of error-correcting codes.  The most dramatic instance of this is the  near Shannon-limit performance of &#034;Turbo  Codes&#034; --- codes whose decoding algorithm  is equivalent to loopy belief propagation in a  chain-structured Bayesian network.  In this paper we ask: is there something special  about the error-correcting code context,  or does loopy propagation work as an approximate  inference scheme in a more general  setting? We compare the marginals computed  using loopy propagation to the exact  ones in four Bayesian network architectures,  including two real-world networks: ALARM  and QMR. We find that the loopy beliefs often  converge and when they do, they give a  good approximation to the correct marginals.  However, on the QMR network, the loopy beliefs  oscillated and had no obvious relationship  ...
274|Learning to Extract Symbolic Knowledge from the World Wide Web|The World Wide Web is a vast source of information  accessible to computers, but understandable  only to humans. The goal of the research described  here is to automatically create a computer  understandable world wide knowledge base whose  content mirrors that of the World Wide Web. Such a 
275|Context-Specific Independence in Bayesian Networks|Bayesiannetworks provide a languagefor qualitatively  representing the conditional independence properties  of a distribution. This allows a natural and compact  representation of the distribution, eases knowledge acquisition,  and supports effective inference algorithms.
276|Learning Bayesian network structure from massive datasets: the “sparse candidate” algorithm|Learning Bayesian networks is often cast as an optimization problem, where the computational task is to find a structure that maximizes a sta-tistically motivated score. By and large, existing learning tools address this optimization problem using standard heuristic search techniques. Since the search space is extremely large, such search procedures can spend most of the time examining candidates that are extremely unreasonable. This problem becomes critical when we deal with data sets that are large either in the number of in-stances, or the number of attributes. In this paper, we introduce an algorithm that achieves faster learning by restricting the search space. This iterative algorithm restricts the par-ents of each variable to belong to a small sub-set of candidates. We then search for a network that satisfies these constraints. The learned net-work is then used for selecting better candidates for the next iteration. We evaluate this algorithm both on synthetic and real-life data. Our results show that it is significantly faster than alternative search procedures without loss of quality in the learned structures. 1
277|Correctness of Local Probability Propagation in Graphical Models with Loops|This article analyzes the behavior of local propagation rules in graphical models with a loop.
278|Learning Bayesian Networks is NP-Complete|Algorithms for learning Bayesian networks from data havetwo components: a scoring metric and a  search procedure. The scoring metric computes a score reflecting the goodness-of-fit of the structure  to the data. The search procedure tries to identify network structures with high scores. Heckerman  et al. (1995) introduce a Bayesian metric, called the BDe metric, that computes the relative posterior  probabilityofanetwork structure given data. In this paper, we show that the search problem of  identifying a Bayesian network---among those where each node has at most K parents---that has a  relative posterior probability greater than a given constant is NP-complete, when the BDe metric  is used.  12.1 
279|Probabilistic Frame-Based Systems|Two of the most important threads of work in knowledge representation today are frame-based representation systems (FRS&#039;s) and Bayesian networks (BNs). FRS&#039;s provide an excellent representation for the organizational structure of large complex domains, but their applicability is limited because of their inability to deal with uncertainty and noise. BNs provide an intuitive and coherent probabilistic representation of our uncertainty, but are very limited in their ability to handle complex structured domains. In this paper, we provide a language that cleanly integrates these approaches, preserving the advantages of both. Our approach allows us to provide natural and compact definitions of probability models for a class, in a way that is local to the class frame. These models can be instantiated for any set of interconnected instances, resulting in a coherent probability distribution over the instance properties. Our language also allows us to represent important types of uncertainty tha...
280|A Bayesian approach to learning Bayesian networks with local structure|Recently several researchers have investigated techniques for using data to learn Bayesian networks containing compact representations for the conditional probability distributions (CPDs) stored at each node. The majority of this work has concentrated on using decision-tree representations for the CPDs. In addition, researchers typically apply non-Bayesian (or asymptotically Bayesian) scoring functions such as MDL to evaluate the goodness-of-fit of networks to the data. In this paper we investigate a Bayesian approach to learning Bayesian networks that contain the more general decision-graph representations of the CPDs. First, we describe how to evaluate the posterior probability— that is, the Bayesian score—of such a network, given a database of observed cases. Second, we describe various search spaces that can be used, in conjunction with a scoring function and a search procedure, to identify one or more high-scoring networks. Finally, we present an experimental evaluation of the search spaces, using a greedy algorithm and a Bayesian scoring function. 1
281|Learning Belief Networks in the Presence of Missing Values and Hidden Variables|In recent years there has been a flurry of works on learning probabilistic belief networks. Current state of the art methods have been shown to be successful for two learning scenarios: learning both network structure and parameters from  complete data, and learning parameters for a fixed network from incomplete data---that is, in the presence of missing values or hidden variables. However, no method has yet been demonstrated to effectively learn network structure from incomplete data. In this paper, we propose a new method for learning network structure from incomplete data. This method is based on an extension of the  Expectation-Maximization (EM) algorithm for model selection problems that performs search for the best structure inside the EM procedure. We prove the convergence of this algorithm, and adapt it for learning belief networks. We then describe how to learn networks in two scenarios: when the data contains missing values, and in the presence of hidden variables. We provide...
282|Unsupervised Learning from Dyadic Data|Dyadic data refers to a domain with two finite sets of objects in which observations are made for dyads, i.e., pairs with one element from either set. This includes event co-occurrences, histogram data, and single stimulus preference data as special cases. Dyadic data arises naturally in many applications ranging from computational linguistics and information retrieval to preference analysis and computer vision. In this paper, we present a systematic, domain-independent framework for unsupervised learning from dyadic data by statistical mixture models. Our approach covers different models with flat and hierarchical latent class structures and unifies probabilistic modeling and structure discovery. Mixture models provide both, a parsimonious yet flexible parameterization of probability distributions with good generalization performance on sparse data, as well as structural information about data-inherent grouping structure. We propose an annealed version of the standard Expectation Maximization algorithm for model fitting which is empirically evaluated on a variety of data sets from different domains.
283|Answering Queries from Context-Sensitive Probabilistic Knowledge Bases|We define a language for representing context-sensitive probabilistic knowledge. A knowledge base consists of a set of universally quantified probability sentences that include context constraints, which allow inference to be focused on only the relevant portions of the probabilistic knowledge. We provide a declarative semantics for our language. We present a query answering procedure which takes a query Q and a set of evidence E and constructs a Bayesian network to compute P (QjE). The posterior probability is then computed using any of a number of Bayesian network inference algorithms. We use the declarative semantics to prove the query procedure sound and complete. We use concepts from logic programming to justify our approach. Keywords: reasoning under uncertainty, Bayesian networks, Probability model construction, logic programming Submitted to Theoretical Computer Science special issue on Uncertainty in Databases and Deductive Systems. This work was partially supported by NSF g...
284|Probabilistic Reasoning for Complex Systems|ii
285|Learning Statistical Models from Relational Data|This workshop is the second in a series of workshops held in conjunction with AAAI and IJCAI. The first workshop was held in July, 2000 at AAAI. Notes from that workshop are available at
286|Learning Probabilities for Noisy First-Order Rules|First-order logic is the traditional basis for knowledge representation languages. However, its applicability to many real-world tasks is limited by its inability to represent uncertainty. Bayesian belief networks, on the other hand, are inadequate for complex KR tasks due to the limited expressivity of the underlying (propositional) language. The need to incorporate uncertainty into an expressive language has led to a resurgence of work on first-order probabilistic logic. This paper addresses one of the main objections to the incorporation of probabilities into the language: &#034;Where do the numbers come from?&#034; We present an approach that takes a knowledge base in an expressive rule-based first-order language, and learns the probabilistic parameters associated with those rules from data cases. Our approach, which is based on algorithms for learning in traditional Bayesian networks, can handle data cases where many of the relevant aspects of the situation are unobserved. It is also capabl...
287|Prospective assessment of ai technologies for fraud detection: A case study|In September 1995, the Congressional O ce of Technology Assessment completed a study of the potential for AI technologies to detect money laundering by screening wire transfers. The study, conducted at the request of the Senate Permanent Subcommittee on Investigations, evaluates the technical and public policy implications of widespread use of AI technologies by the Federal government for fraud detection. Its conclusions are relevant tomanyother uses of AI technologies for fraud detection in both the public and private sectors.
288|PRL: A probabilistic relational language|In this paper, we describe the syntax and semantics for a probabilistic relational language (PRL). PRL is a recasting of recent work in Probabilistic Relational Models (PRMs) into a logic programming framework. We show how to represent varying degrees of complexity in the semantics including attribute uncertainty, structural uncertainty and identity uncertainty. Our approach is similar in spirit to the work in Bayesian Logic Programs (BLPs), and Logical Bayesian Networks (LBNs). However, surprisingly, there are still some important differences in the resulting formalism; for example, we introduce a general notion of aggregates based on the PRM approaches. One of our contributions is that we show how to support richer forms of structural uncertainty in a probabilistic logical language than have been previously described. Our goal in this work is to present a unifying framework that supports all of the types of relational uncertainty yet is based on logic programming formalisms. We also believe that it facilitates understanding the relationship between the frame-based approaches and alternate logic programming approaches, and allows greater
289|User Modelling for Conceptual Database Design Based on an Extended Entity Relationship Model: A Preliminary Study |©Copyright in this paper belongs to the author(s) Published in collaboration with the
290|Lattice-Based Access Control Models|The objective of this article is to give a tutorial on lattice-based  access control models for computer security. The paper begins with a review  of Denning&#039;s axioms for information flow policies, which provide a theoretical  foundation for these models. The structure of security labels in the military and  government sectors, and the resulting lattice is discussed. This is followed by a  review of the Bell-LaPadula model, which enforces information flow policies by  means of its simple-security and *-properties. It is noted that information flow  through covert channels is beyond the scope of such access controls. Variations  of the Bell-LaPadula model are considered. The paper next discusses the Biba  integrity model, examining its relationship to the Bell-LaPadula model. The  paper then reviews the Chinese Wall policy, which arises in a segment of the  commercial sector. It is shown how this policy can be enforced in a lattice  framework.
291|A Lattice Model of Secure Information Flow|This paper investigates mechanisms that guarantee secure information flow in a computer system. These mechanisms are examined within a mathematical framework suitable for formulating the requirements of secure information flow among security classes. The central component of the model is a lattice structure derived from the security classes and justified by the semantics of information flow. The lattice properties permit concise formulations of the security requirements of different existing systems and facilitate the construction of mechanisms that enforce security. The model provides a unifying view of all systems that restrict information flow, enables a classification of them according to security objectives, and suggests some new approaches. It also leads to the construction of automatic program certification mechanisms for verifying the secure flow of information through a program.
292|A Note on the Confinement Problem|This not explores the problem of confining a program during its execution so that it cannot transmit information to any other program except its caller. A set of examples attempts to stake out the boundaries of the problem. Necessary conditions for a solution are stated and informally justified.
293|Role-Based Access Control|While Mandatory Access Controls (MAC) are appropriate for multilevel secure military applications, Discretionary Access Controls (DAC) are often perceived as meeting the security processing needs of industry and civilian government. This paper argues that reliance on DAC as the principal method of access control is unfounded and inappropriate for many commercial and civilian government organizations. The paper describes a type of non-discretionary access control - role-based access control (RBAC) - that is more central to the secure processing needs of non-military systems then DAC. 1 Introduction  The U.S. government has been involved in developing security technology for computer and communications systems for some time. Although advances have been great, it is generally perceived that the current state of security technology has, to some extent failed to address the needs of all. [1], [2] This is especially true of organizations outside the Department of Defense (DoD). [3] The curre...
294|The Typed Access Matrix Model|The access matrix model as formalized by Harrison, Ruzzo, and Ullman (HRU) has broad expressive power. Unfortunately, HRU has weak safety properties (i.e., the determination of whether or not a given subject can ever acquire access to a given object). Most security policies of practical interest fall into the undecidable cases of HRU. This is true even for monotonic policies (i.e., where access rights can be deleted only if the deletion is itself reversible). In this paper we define the typed access matrix (TAM) model by introducing strong typing into HRU (i.e., each subject or object is created to be of a particular type which thereafter does not change). We prove that monotonic TAM (MTAM) has strong safety properties similar to Sandhu&#039;s Schematic Protection Model. Safety in MTAM&#039;s decidable case is, however, NP-hard. We develop a model called ternary MTAM which has polynomial safety for its decidable case, and which nevertheless retains the full expressive power of MTAM. There is compelling evidence that the decidable safety cases of ternary MTAM are quite adequate for modeling practial monotonic security policies.
295|Access Rights Administration in Role-Based Security Systems|This paper examines the concept of role-based protection and, in particular, role  organization. From basic role relationships, a model for role organization is developed.  The role graph model, its operator semantics based on graph theory and algorithms for  role administration are proposed. The role graph model, in our view, presents a very  generalized form of role organization for access rights administration. It is shown how  the model simulates other organizational structures such as hierarchies [TDH92] and  privilege graphs [Bal90]. 
296|Conceptual Foundations for a Model of Task-based Authorizations|In this paper we describe conceptual foundations to address integrity issues in computerized information systems from the enterprise perspective. Our motivation for this effort stems from the recognition that existing models are formulated at too low a level of abstraction, to be useful for modeling organizational requirements, policy aspects, and internal controls, pertaining to maintenance of integrity in information systems. In particular, these models are primarily concerned with the integrity of internal data components within computer systems, and thus lack the constructs necessary to model enterprise level integrity principles. The starting point in our investigation is the notion of authorization functions and tasks associated with business activities carried out in the enterprise. These functions identify the authorization requirements while the authorization tasks embody the concepts required to carry out such authorizations. We believe a model of task-based autho...
297|A Lattice Interpretation Of The Chinese Wall Policy|The Chinese Wall policy was identi#ed and so named by Brewer and Nash #2#.  This policy arises in the segment of the commercial sector which provides consulting  services to other companies. Consultants naturally have to deal with con#dential company  information for their clients. The objective of the Chinese Wall policy is to prevent  information #ows which cause con#ict of interest for individual consultants. Brewer and  Nash develop a mathematical model of the Chinese Wall policy, on the basis of which  they claim that this policy #cannot be correctly represented by a Bell-LaPadula model.&#034;  In this paper we demonstrate that the Brewer-Nash model is too restrictivetobeemployed  in a practical system. This is due to their treatment of users and subjects as  synonymous concepts, with the consequence that they do not distinguish security policy  as applied to human users versus security policy as applied to computer subjects. By  maintaining a careful distinction between users, princip...
298|Delegation Of Authority|This paper is concerned with the specification of discretionary access control policy for commercial security and the delegation of access control authority in a way which gives flexibility while retaining management control. Large distributed processing systems have very large numbers of users and resource objects so that it is impractical to specify access control policy in terms of individual objects or individual users. We need to be able to specify it as relationships between groups of users and groups of objects. The systems typically consist of multiple interconnected networks and span a number of different organisations. Authority cannot be delegated or imposed from one central point, but has to be negotiated between independent managers who wish to cooperate but who may have a very limited trust in each other. The paper proposes the use of access rules to specify, in terms of their domain memberships, what operations a user can perform on a target object. The delegation of aut...
299|An intrusion-detection model|A model of a real-time intrusion-detection expert system capable of detecting break-ins, penetrations, and other forms of computer abuse is described. The model is based on the hypothesis that security violations can be detected by monitoring a system&#039;s audit records for abnormal patterns of system usage. The model includes profiles for representing the behavior of subjects with respect to objects in terms of metrics and statistical models, and rules for acquiring knowledge about this behavior from audit records and for detecting anomalous behavior. The model is independent of any particular system, application environment, system vulnerability, or type of intrusion, thereby providing a framework for a general-purpose intrusion-detection expert system.
300|A reverse engineering method for identifying reusable abstract data types|This paper presents results from an experiment in reuse within the RE2 project. It shows how a particular candidature criterion for identifying abstract data types in txisting software systems can be applied both ar the theoretical and practical level. The RE2 project is concerned with the exploration of I everse engineering and re-engineering techniques to facilitate reuse re-engineering by the identification and I dassijication of appropriate candidature criteria. 1.
301|The relationship between return and market value of common stocks|This study examines the empirical relattonship between the return and the total market value of NYSE common stocks. It is found that smaller firms have had htgher risk adjusted returns, on average, than larger lirms. This ‘size effect ’ has been in existence for at least forty years and is evidence that the capital asset pricing model is misspecttied. The size elfect is not linear in the market value; the main effect occurs for very small tirms while there is little difference m return between average sized and large firms. It IS not known whether size per se is responsible for the effect or whether size IS just a proxy for one or more true unknown factors correlated with size. 1.
302|Validation of a Method for Representing Large Entity Relationship Models: An Action Research Study|One of the most serious limitations of the Entity Relationship (ER) Model in practice is its inability to cope with complexity. Once data models exceed a certain threshold size, they become difficult to understand (end user’s viewpoint) and also to document and maintain (analyst’s viewpoint). A number of approaches have been proposed in the literature to address this problem, but so far there has been no systematic empirical research into the effectiveness of these methods. This paper describes an action research study in which a method for representing large ER models was tested in a large application development project in one of Australia’s largest commercial organisations. The research was successful in achieving both practical and research outcomes?it resulted in change of data modelling practices in the organisation, and the method was refined significantly as a result of experiences in practice. However a major problem experienced in this study was that the size of the project imposed constraints on the evolution of the method. Because of the number of people involved, it was difficult to make changes to the method “on the fly ” and to experiment with variations of the method, as is customary
303|Softsystems methodology |The only man-made object on our planet which is visible to astronauts in space is the Great Wall of China. It’s creations over several thousand years, or, to take a more recent and less awe-inspiring example, the creators of the American telephone network in the early years of this century, must have been engineers and managers of considerable skill. In both cases they successfully accomplished what in today’s language would be called major “projects”, though that word has become popular only in recent times. The notion of a project implies bringing together the materials and skills necessary to create both some complex object and the way it will be used. A project implies the exercise of a combination of engineering and management skills. In the case of the latter, not only does the project itself have to be managed, but also the project content must include creating a way of using (managing) the physical object or objects. In the case of the Anglo-French Concorde project, for example, the overall task was to create both the world’s first supersonic passenger aircraft and ways in which it could be manned, flown, serviced, and fitted into airline operations.
304|Building Links Between Is Research And Professional Practice: Improving The Relevance And Impact Of Is Research|There has been a great deal of debate about the status of information systems (IS) as an academic discipline,  its progress, and continued survival. Most of these critiques have been rather inward-looking, and have  focused either on research methodology or the need to develop theoretical foundations. This paper argues that  as an applied discipline, IS will not achieve legitimacy by the rigor of its methods or by its theoretical base,  but by being practically useful. Its success will be measured by its contribution to the IS profession, and  ultimately to society. We argue that to be effective, research must be both (1) relevant to the needs of practice  and (2) disseminated and used by practitioners. We use medicine, a discipline which has a high level of  integration between research and practice, as a model for radically changing IS research so that it can become  more relevant and have a genuine impact in practice.   
305|Current interests: databases, software engineering, Internet security, XML Entity-Relationship Modeling |This paper describes the historical developments of the Entity-Relationship (ER) model from the 1970s to recent years. It starts with a discussion of the motivations and environmental factors in the early days. Then, the paper points out the role of the ER model in the Computer-Aided Software Engineering (CASE) movement in the late 1980s and early 1990s. It also describes the possible role of the author’s Chinese cultural heritage in the development of the ER model. In that context, the relationships between natural languages (including Ancient Egyptian hieroglyphics) and ER concepts are explored. Finally, the lessons learned and future directions are presented.
306|Modifying Entity Relationship Models for Collaborative Fiction Planning and its Impact on Potential Authors* |We propose a modified Entity Relationship (E-R) model, traditionally used for software en-gineering, to structure, store and share plot data. The flexibility of E-R modelling has been demonstrated by its decades of usage in a wide variety of situations. The success of the E-R model suggests that it could be useful for collaborating fiction authors, adding a certain degree of computational power to their process. We changed the E-R model syntax to better suit the story plans, switching the emphasis from generic types to instanced story entities, but preserving relationships and attributes. We conducted a small-scale basic experiment to study the impact of using our modified E-R model on authors when understanding and contributing into a pre-existing fiction story plan. The results analysis revealed that the E-R model supports authors as effectively as written text in reading comprehension, memory, and contributing. In addition, the results show that, when combined together, the written text and the E-R model help participants achieve better comprehension – always within the frame of our experiment. We discuss potential applications of these findings.
307|Artificial Intelligence and Literary Creativity: Inside the Mind of BRUTUS, a Storytelling Machine|Professor Hart, and Hart had often said--to others and to himself--that he was honored to help Dave secure his well-earned  dream.  Well before the defense, Striver gave Hart a penultimate copy of his thesis. Hart read it and  told Dave that it was absolutely first-rate, and that he would gladly sign it at the defense. They  even shook hands in Hart&#039;s book-lined office. Dave noticed that Hart&#039;s eyes were bright and trustful, and his bearing paternal.  At the defense, Dave thought that he eloquently summarized Chapter 3 of his dissertation. There were two quest2ons, one from Professor Rodman and one from Dr. Teer; Dave answered both, apparently to everyone&#039;s satisfaction. There were no further objections.  Professor Rodman signed. He slid the tome to Teer; she too signed, and then slid it in front of Hart. Hart didn&#039;t move.  &#034;Ed?&#034; Rodman said.  Hart still sat motionless. Dave felt slightly dizzy.  &#034;Edward, are you going to sign?&#034;  Later, Hart sat alone in his office, in his big leather 
308|The 4+1 view model of architecture|The 4+1 View Model organizes a description of a software architecture using five concurrent views, each of which
addresses a specific set of concerns. Architects capture their design decisions in four views and use the fifth view to illustrate and validate them.
309|A Study|on the rubrene emission sensitized by a phosphorescent Ir compound in the host of CBP
310|From Domain Model to Architectures|A software system can be evaluated against criteria in two broad categories: • functional and performance attributes: how well does the system, during execution, satisfy its behavioral, functional, and performance requirements? Does it provide the required results? Does it provide them in a timely enough manner? Are the results correct, or within specified accuracy and stability tolerances? • non-functional attributes: how easy is the system to integrate, test, and modify? How expensive was it to develop? These two categories are orthogonal; systems that unfailingly meet all of their requirements may or may not have been prohibitively expensive to develop, and may or may not be impossible to modify. Highly modifiable systems may or may not produce correct results. Given a set of requirements for a system, the developer must choose an architecture that will allow the implementation of the system to proceed in a straightforward manner, producing a product that meets its functional and non-functional requirements. How is that done? 1.1 Producing architectures to meet functional requirements There is, unfortunately, no reliable automatic or semi-automatic technology that will produce
311|Active Appearance Models|AbstractÐWe describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors. Index TermsÐAppearance models, deformable templates, model matching. 1
312|Efficient region tracking with parametric models of geometry and illumination|Abstract—As an object moves through the field of view of a camera, the images of the object may change dramatically. This is not simply due to the translation of the object across the image plane. Rather, complications arise due to the fact that the object undergoes changes in pose relative to the viewing camera, changes in illumination relative to light sources, and may even become partially or fully occluded. In this paper, we develop an efficient, general framework for object tracking—one which addresses each of these complications. We first develop a computationally efficient method for handling the geometric distortions produced by changes in pose. We then combine geometry and illumination into an algorithm that tracks large image regions using no more computation than would be required to track with no accommodation for illumination changes. Finally, we augment these methods with techniques from robust statistics and treat occluded regions on the object as statistical outliers. Throughout, we present experimental results performed on live video sequences demonstrating the effectiveness and efficiency of our methods. Index Terms—Visual tracking, real-time vision, illumination, motion estimation, robust statistics.
313|XM2VTSDB: The Extended M2VTS Database|In this paper we describe the acquisition and content of a large multi-modal database intended for training and testing of multi-modal verification systems. The XM2VTSDB database offers synchronised video and speech data as well as image sequences allowing multiple views of the face. It consists of digital video recordings taken of 295 hundred subjects at one month intervals taken over a period of five months. We also describe a protocol for evaluating verification algorithms on the database. The database has been made available to anyone on request to the University of Surrey through http://www.ee.surrey.ac.uk/Research/VSSP/xm2vtsdb.
314|Automatic interpretation and coding of face images using flexible models|Abstract—Face images are difficult to interpret because they are highly variable. Sources of variability include individual appearance, 3D pose, facial expression, and lighting. We describe a compact parametrized model of facial appearance which takes into account all these sources of variability. The model represents both shape and gray-level appearance, and is created by performing a statistical analysis over a training set of face images. A robust multiresolution search algorithm is used to fit the model to faces in new images. This allows the main facial features to be located, and a set of shape, and gray-level appearance parameters to be recovered. A good approximation to a given face can be reconstructed using less than 100 of these parameters. This representation can be used for tasks such as image coding, person identification, 3D pose recovery, gender recognition, and expression recognition. Experimental results are presented for a database of 690 face images obtained under widely varying conditions of 3D pose, lighting, and facial expression. The system performs well on all the tasks listed above.
315|A Minimum Description Length Approach to Statistical Shape Modelling|We describe a method for automatically building statistical shape models from a training set of exam- ple boundaries / surfaces. These models show considerable promise as a basis for segmenting and interpreting images. One of the drawbacks of the approach is, however, the need to establish a set of dense correspondences between all members of a set of training shapes. Often this is achieved by locating a set of qandmarks manually on each training image, which is time-consuming and subjective in 2D, and almost impossible in 3D. We describe how shape models can be built automatically by posing the correspondence problem as one of finding the parameterization for each shape in the training set. We select the set of parameterizations that build the best model. We define best as that which min- imizes the description length of the training set, arguing that this leads to models with good compactness, specificity and generalization ability. We show how a set of shape parameterizations can be represented and manipulated in order to build a minimum description length model. Results are given for several different training sets of 2D boundaries, showing that the proposed method constructs better models than other approaches including manual landmarking - the current gold standard. We also show that the method can be extended straightforwardly to 3D.
316|Interpreting Face Images using Active Appearance Models|We demonstrate a fast, robust method of interpreting face images using an Active Appearance Model (AAM). An AAM contains a statistical model of shape and grey-level appearance which can generalise to almost any face. Matching to an image involves finding model parameters which minimise the difference between the image and a synthesised face. We observe that displacing each model parameter from the correct value induces a particular pattern in the residuals. In a training phase, the AAM learns a linear model of the correlation between parameter displacements and the induced residuals. During search it measures the residuals and uses this model to correct the current parameters, leading to a better fit. A good overall match is obtained in a few iterations, even from poor starting estimates. We describe the technique in detail and show it matching to new face images.  1 Introduction  There is currently a great deal of interest in model-based approaches to the interpretation of face images...
317|Face Recognition using View-Based and Modular Eigenspaces|In this paper we describe experiments using eigenfaces for recognition and interactive search in the FERET face database. A recognition accuracy of 99.35% is obtained using frontal views of 155 individuals. This figure is consistent with the 95% recognition rate obtained previously on a much larger database of 7,562 &#034;mugshots&#034; of approximately 3,000 individuals, consisting of a mix of all age and ethnic groups. We also demonstrate that we can automatically determine head pose without significantly lowering recognition accuracy; this is accomplished by use of a viewbased multiple-observer eigenspace technique. In addition, a modular eigenspace description is used which incorporates salient facial features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields slightly higher recognition rates as well as a more robust framework for face recognition. In addition, a robust and automatic feature detection technique using eigentemplates is demonstra...
318|Resynthesizing Facial Animation through 3D Model-Based Tracking|Given video footage of a person&#039;s face, we present new techniques to automatically recover the face position and the facial expression from each frame in the video sequence. A 3D face model is fitted to each frame using a continuous optimization technique. Our model is based on a set of 3D face models that are linearly combined using 3D morphing. Our method has the advantages over previous techniques of fitting directly a realistic 3-dimensional face model and of recovering parameters that can be used directly in an animation system. We also explore many applications, including performance-driven animation (applying the recovered position and expression of the face to a synthetic character to produce an animation that mimics the input video), relighting the face, varying the camera position, and adding facial ornaments such as tattoos and scars. 1 Introduction There are many techniques and tools that can be used to create facial animations. These tools can be as simple as a pencil an...
319|A Multi-View Nonlinear Active Shape Model Using Kernel PCA|Recovering the shape of any 3D object using multiple 2D views requires  establishing correspondence between feature points at different views. However  changes in viewpoint introduce self-occlusions, resulting nonlinear variations  in the shape and inconsistent 2D features between views. Here we  introduce a multi-view nonlinear shape model utilising 2D view-dependent  constraint without explicit reference to 3D structures. For nonlinear model  transformation, we adopt Kernel PCA based on Support Vector Machines.
320|Multidimensional morphable models: A framework for representing and matching object classes|This thesis describes a flexible model for representing images of objects of a certain class, such as faces, and introduces a new algorithm for matching the model to novel images from the class. The model and matching algorithm are very general and can be used for many image analysis tasks. The flexible model, called a multidimensional morphable model, is learned from example images (called prototypes) of objects of a class. In the learning phase, pixelwise correspon-dences between a reference prototype and each of the other prototypes are first computed and then used to obtain shape and texture vectors associated with each prototype. The mor-phable model is then synthesized as a linear combination that spans the linear vector space determined by the prototypical shapes and textures. We next introduce an effective stochastic gradient descent algorithm that automatically matches a model to a novel image by finding the parameters that minimize the error between the image generated by the model and the novel image. Several experiments demonstrate the robustness and the broad range of applicability of the matching algorithm and the underlying
321|Multidimensional Morphable Models|We describe a flexible model for representing images of objects of a certain class, known a priori, such as faces, and introduce a new algorithm for matching it to a novel image and thereby performing image analysis. We call this model a multidimensional morphable model or just a morphable model. The morphable model is learned from example images (called prototypes) of objects of a class. In this paper we introduce an effective stochastic gradient descent algorithm that automatically matches a model to a novel image by finding the parameters that minimize the error between the image generated by the model and the novel image. Two examples demonstrate the robustness and the broad range of applicability of the matching algorithm and the underlying morphable model. Our approach can provide novel solutions to several vision tasks, including the computation of image correspondence, object verification, image synthesis and image compression.  1 Introduction  An important problem in computer ...
322|Learning to Identify and Track Faces in Image Sequences|We address the problem of robust face identification in the presence of pose, lighting, and expression variation. Previous approaches to the problem have assumed similar models of variation for each individual, estimated from pooled training data. We describe a method of updating a first order global estimate of identity by learning the classspecific correlation between the estimate and the residual variation during a sequence. This is integrated with an optimal tracking scheme, in which identity variation is decoupled from pose, lighting and expression variation. The method results in robust tracking and a more stable estimate of facial identity under changing conditions.  1 Introduction  Locating and interpreting faces in images and image sequences is a difficult problem in machine vision, due to the inherent variability between and within individuals. The appearance of a face in an image varies with the identity of the individual, pose, lighting conditions, and deformations due to e...
323|Facial Analysis and Synthesis Using Image-Based Models|In this paper, we describe image-based modeling techniques that make possible the creation of photo-realistic computer models of real human faces. The image-based model is built using example views of the face, bypassing the need for any three-dimensional computer graphics models. A learning network is trained to associate each of the example images with a set of pose and expression parameters. For a novel set of parameters, the network synthesizes a novel, intermediate view using a morphing approach. This image-based synthesis paradigm can adequately model both rigid and non-rigid facial movements. We also describe an analysis-by-synthesis algorithm, which is capable of extracting a set of high-level parameters from an image sequence involving facial movement using embedded image-based models. The parameters of the models are perturbed in a local and independent manner for each image until a correspondence-based error metric is minimized. A small sample of experimental results is pres...
324|Generalized Image Matching: Statistical Learning of Physically-Based Deformations|We describe a novel approach for image matching based on deformable intensity surfaces. In this approach, the intensity surface of the image is modeled as a deformable 3D mesh in the (x; y; I(x;y)) space. Each surface point has 3 degrees of freedom, thus capturing fine surface changes. A set of representative deformations within a class of objects (e.g. faces) are statistically learned through a Principal Components Analysis, thus providing a priori knowledge about object-specific deformations. We demonstrate the power of the approach by examples such as image matching and interpolation of missing data. Moreover this approach dramatically reduces the computational cost of solving the governing equation for the physically based system by approximately three orders of magnitude.
325|Face Recognition from Unfamiliar Views: Subspace Methods and Pose Dependency|A framework for recognising human faces from unfamiliar views is described and a simple implementation of this framework evaluated. The interaction between training view and testing view is shown to compare with observations in human face recognition experiments. The ability of the system to learn from several training views, as available in video footage, is shown to improve the overall performance of the system as is the use of multiple testing images. 1 Introduction Recognising faces from previously unseen viewpoints is inherently more difficult than matching faces at the same view. Simple image comparisons such as correlation demonstrate that there is a greater difference between different viewpoints of the same subject than between different subjects at the same view which means that the recognition method used must take into account the non-linear variations of faces with viewpoint. In order to achieve recognition of previously unseen views, we require a method of relating the...
326|Estimating Coloured 3D Face Models from Single Images: An Example Based Approach|Abstract. In this paper we present a method to derive 3D shape and surface texture of a human face from a single image. The method draws on a general flexible 3D face model which is “learned ” from examples of individual 3D-face data (Cyberware-scans). In an analysis-by-synthesis loop, the flexible model is matched to the novel face image. From the coloured 3D model obtained by this procedure, we can generate new images of the face across changes in viewpoint and illumination. Moreover, nonrigid transformations which are represented within the flexible model can be applied, for example changes in facial expression. The key problem for generating a flexible face model is the computation of dense correspondence between all given 3D example faces. A new correspondence algorithm is described which is a generalization of common algorithms for optic flow computation to 3D-face data. 1
327|An Algorithm for the Learning of Weights in Discrimination Functions using a priori Constraints|We introduce a learning algorithm for the weights in a very common class of discrimination functions usually called &#034;weighted average&#034;. The learning algorithm can reduce the number of free variables by simple but effective a priori criteria about significant features. Here we apply our algorithm to three tasks of different dimensionality all concerned with face recognition. 1 Introduction  Many pattern recognition systems can be roughly divided into two parts, feature extraction and pattern discrimination. In feature extraction an input I is transformed into a vector I k 2 IR  N  . (In speech recognition I k can, for example, represent the Fourier transformation in a certain time interval in a specific frequency band [14]; in image processing I k could be the filter response of a wavelet-like filter at a certain position in the grey-level picture [11, 15]). In discrimination the input I has to be assigned to a specific class c. The extracted features are used to evaluate certain simila...
328|From Regular Images to Animated Heads: A Least Squares Approach|We show that we can effectively fit arbitrarily complex animation models to noisy image data. Our approach is based on least-squares adjustment using of a set of progressively finer control triangulations and takes advantage of three complementary sources of information: stereo data, silhouette edges and 2-D feature points.
329|Understanding pose discrimination in similarity space|Identity-independent estimation of head pose from prototype im-ages is a perplexing task, requiring pose-invariant face detection. The problem is exacerbated by changes in illumination, identity and facial position. Facial images must be transformed in such a way as to em-phasise dierences in pose, while suppressing dierences in identity. We investigate appropriate transformations for use with a similarity-to-prototypes philosophy. The results show that orientation-selective Gabor lters enhance dierences in pose, and that dierent lter ori-entations are optimal at dierent poses. In contrast, PCA was found to provide an identity-invariant representation in which similarities can be calculated more robustly. We also investigate the angular resolution at which pose changes can be resolved using our methods. An angular resolution of 10 was found to be suciently discriminable at some poses but not at others, while 20 is quite acceptable at most poses. 1
330|On utilising template and feature-based correspondence in multi-view appearance models|Abstract. In principle, the recovery and reconstruction of a 3D object from its 2D view projections require the parameterisation of its shape structure and surface re ectance properties. Explicit representation and recovery of such 3D information is notoriously di cult to achieve. Alternatively, a linear combination of 2D views can be used which requires the establishment of dense correspondence between views. This in general, is di cult to compute and necessarily expensive. In this paper we examine the use of a ne and local feature-based transformations in establishing correspondences between very large pose variations. In doing so, we utilise a generic-view template, a generic 3D surface model and Kernel PCA for modelling shape and texture nonlinearities across views. The abilities of both approaches to reconstruct and recover faces from any 2D image are evaluated and compared. 1
331|Model-based Initialisation for Segmentation|The initialisation of segmentation methods aiming at the localisation of biological structures in medical imagery is frequently regarded as a given precondition. In practice, however, initialisation is usually performed manually or by some heuristic preprocessing steps. Moreover, the same framework is often employed to recover from imperfect results of the subsequent segmentation. Therefore, it is of crucial importance for everyday application to have a simple and effective initialisation method at one&#039;s disposal. This paper proposes a new model-based framework to synthesise sound initialisations by calculating the most probable shape given a minimal set of statistical landmarks and the applied shape model. Shape information coded by particular points is first iteratively removed from a statistical shape description that is based on the principal component analysis of a collection of shape instances. By using the inverse of the resulting operation, it is subsequently poss...
333|Oligomorphic permutation groups|A permutation group G (acting on a set ?, usually infinite) is said to be oligomorphic if G has only finitely many orbits on ? n (the set of n-tuples of elements of ?). Such groups have traditionally been linked with model theory and combinatorial enumeration; more recently their group-theoretic properties have been studied, and links with graded algebras, Ramsey theory, topological dynamics, and other areas have emerged. This paper is a short summary of the subject, concentrating on the enumerative and algebraic aspects but with an account of grouptheoretic properties. The first section gives an introduction to permutation groups and to some of the more specific topics we require, and the second describes the links to model theory and enumeration. We give a spread of examples, describe results on the growth rate of the counting functions, discuss a graded algebra associated with an oligomorphic group, and finally discuss group-theoretic properties such as simplicity, the small index property, and “almost-freeness”.
334|Structure and Complexity of Relational Queries|This paper is an attempt at laying the foundations for the classification of queries on  relational data bases according to their structure and their computational complexity. Using  the operations of composition and fixpoints, a Z--// hierarchy of height w 2, called the  fixpoint query hierarchy, is defined, and its properties investigated. The hierarchy includes  most of the queries considered in the literathre including those of Codd and Aho and Ullman
335|Toward Logic Tailored for Computational Complexity|Whereas first-order logic was developed to confront the infinite it is often used in computer science in such a way that infinite models are meaningless. We discuss the first-order theory of finite structures and alternatives to first-order logic, especially polynomial time logic. 
336|The Mordell-Lang conjecture for function fields|In [La65], Lang formulated a hypothesis including as special cases the Mordell conjecture concerning rational points on curves, and the Manin-Mumford conjecture on torsion points of Abelian varieties. Sometimes generalized to semi-Abelian varieties, and to positive characteristic, this has been called the Mordell-Lang conjecture;
337|Extending partial automorphisms and the profinite topology on free groups|Abstract. A class of structures C is said to have the extension property for partial automorphisms (EPPA) if, whenever C1 and C2 are structures in C, C1 finite, C1 ? C2, and p1,p2,...,pn are partial automorphisms of C1 extending to automorphisms of C2, then there exist a finite structure C3 in C and automorphisms a1,a2,...,an of C3 extending the pi. We will prove that some classes of structures have the EPPA and show the equivalence of these kinds of results with problems related with the profinite topology on free groups. In particular, we will give a generalisation of the theorem, due to Ribes and Zalesskii stating that a finite product of finitely generated subgroups is closed for this topology. 1.
338|Logarithmic-Exponential Power Series|. We use generalized power series to construct algebraically a nonstandard model of the theory of the real field with exponentiation. This model enables us to show the undefinability of the zeta function and certain non-elementary and improper integrals. We also use this model to answer a question of Hardy by showing that the compositional inverse to the function (log x)(log log x) is not asymptotic as x ! +1 to a composition of semialgebraic functions, log and exp. x1 Introduction and preliminaries  Let RfX 1 ; : : : ; Xm  g denote the ring of all real power series in X 1 ; : : : ; Xm that converge in a neighborhood of I  m  , where I = [\Gamma1; 1]. For f 2 RfX 1 ; : : : ; Xm  g we let  e f : R  m  ! R be given by:  e f(x) =  ae  f(x); for x 2 I  m  , 0; x 62 I  m  . We call the e f &#039;s restricted analytic functions. Let L an be the language of ordered rings  f!; 0; 1; +; \Gamma; \Deltag augmented by a new function symbol for each function e f . We let R an be the reals with its natur...
339|A zero-one law for logic with a fixed-point operator |The logic obtained by adding the least-fixed-point operator to first-order logic was proposed as a query language by Aho and Ullman (in &#034;Proc. 6th ACM Sympos. on Principles of Programming Languages, &#034; 1979, pp. 110-120) and has been studied, particularly in connection with finite models, by numerous authors. We extend to this logic, and to the logic containing the more powerful iterative-fixedpoint operator, the zero-one law proved for first-order logic in (Glebskii, Kogan, Liogonki, and Talanov (1969), Kibernetika 2, 31-42; Fagin (1976), J. Symbolic Logic 41, 50-58). For any sentence q ~ of the extended logic, the proportion of models of q ~ among all structures with universe {1, 2,..., n} approaches 0 or 1 as n tends to infinity. We also show that the problem of deciding, for any cp, whether this proportion approaches 1 is complete for exponential time, if we consider only q)&#039;s with a fixed finite vocabulary (or vocabularies of bounded arity) and complete for double-exponential time if ~0 is unrestricted. In addition, we establish some related results. © 1985 Academic Press, Inc.
340|Elastically deformable models|The goal of visual modeling research is to develop mathematical models and associated algorithms for the analysis and synthesis of visual information. Image analysis and synthesis characterize the domains of computer vision and computer graphics, respectively. For nearly three decades, the vision and graphics fields have been developing almost entirely independently—this despite the fact that, at least conceptually, the two disciplines are bound in a mutually converse relationship. Graphics, the direct problem, involves the synthesis of images from object models, whereas vision, the inverse problem, involves the analysis of images to infer object models. Visual modeling takes a unified approach to vision and graphics via modeling that exploits computational physics. In addition to geometry, physics-based modeling employs forces, torques, internal strain energies, and other physical quantities to control the creation and evolution of models. Mathematically, the approach prescribes systems of dynamic (ordinary and partial) differential equations to govern model behavior. These equations of motion may be
341|Snakes: Active contour models|A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the cap-ture region surrounding a feature. Snakes provide a unified account of a number of visual problems, in-cluding detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.
342|Symmetry-seeking models and 3D object reconstruction|We propose models of 3D shape which may be viewed as deformable bodies composed of simulated elastic material. In contrast to traditional, purely geometric models of shape, deformable models are active--their shapes change in response to externally applied forces. We develop a deformable model for 3D shape which has a preference for axial symmetry. Symmetry is represented even though the model does not belong to a parametric shape family such as (generalized) cylinders. Rather, a symmetry-seeking property is designed into internal forces that constrain the deformations of the model. We develop a framework for 3D object reconstruction based on symmetry-seeking models. Instances of these models are formed from monocular image data through the action of external forces derived from the data. The forces proposed in this paper deform the model in space so that the shape of its projection into the image plane is consistent with the 2D silhouette of an object of interest. The effectiveness of our approach is demonstrated using natural images. 1
343|Signal Matching Through Scale Space|Given a collection of similar signals that have been deformed with respect to each other, the general signal-matching problem is to recover the deformation. We formulate the problem as the minimization of an energy measure that combines a smoothness term and a similarity term. The minimization reduces to a dynamic system governed by a set of coupled, first-order differential equations. The dynamic system finds an optimal solution at a coarse scale and then tracks it continuously to a fine scale. Among the major themes in recent work on visual signal matching have been the notions of matching as constrained opti-mization, of variational surface reconstruction, and of coarse-to-fine matching. Our solution captures these in a precise, succinct, and unified form. Results are presented for one-dimensional signals, a motion sequence, and a stereo pair. 1
344|Sampling and Reconstruction with Adaptive Meshes|This paper introduces an approach to visual sampling and reconstruction motivated by concepts from numerical grid generation. We develop adaptive meshes that can nonuniformly sample and reconstruct intensity and range data. Adaptive meshes are dynamic models which are assembled by interconnecting nodal masses with adjustable springs. Acting as mobile sampling sites, the nodes observe interesting properties of the input data, such as intensities, depths, gradients, and curvatures. Based on these nodal observations, the springs automatically adjust their stiffnesses so as to distribute the available degrees of freedom of the reconstructed model in accordance with the local complexity of the input data. The adaptive mesh algorithm runs at interactive rates with continuous 3D display on a graphics workstation. We apply it to the adaptive sampling and reconstruction of images and surfaces.
345|Resource Description Framework (RDF) Model and Syntax Specification  (1998) |This document is a revision of the public working draft dated 1998-08-19 incorporating suggestions received in review comments and further deliberations of the W3C RDF Model and Syntax Working Group. With the publication of this draft, the RDF Model and Syntax Specification enters &#034;last call.&#034; The last call period will end on October 23, 1998. Comments on this specification may be sent to www-rdf-comments@w3.org. The archive of public comments is available at http://www.w3.org/Archives/Public/www-rdf-comments. Significant changes from the previous draft are highlighted in Appendix E. While we do not anticipate substantial changes, we still caution that further changes are possible. Therefore while we encourage active implementation to test this specification we also recommend that only software that can be easily field-upgraded be implemented to this specification at this time. This is a W3C Working Draft for review by W3C members and other interested parties. Publication as a working draft does not imply endorsement by the W3C membership. The RDF Model and Syntax  Working Group will not allow early implementation to constrain their ability to make changes to this specification prior to final release. This is a draft document and may be updated, replaced or obsoleted by other documents at any time. It is inappropriate to cite W3C Working Drafts as other than &#034;work in progress&#034;. This work is part of the W3C Metadata Activity.
346|Model-Based Clustering, Discriminant Analysis, and Density Estimation|Cluster analysis is the automated search for groups of related observations in a data set. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as \How many clusters are there?&#034;, &#034;Which clustering method should be used?&#034; and \How should outliers be handled?&#034;. We outline a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, mineeld detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology, a...
347|Bayes Factors|In a 1935 paper, and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P -values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this paper we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology and psychology.
349|Bayesian Density Estimation and Inference Using Mixtures|We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation, and are exemplified by special cases where data are modelled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models. Keywords: Kernel estimation; Mixtures of Dirichlet processes; Multimodality; Normal mixtures; Posterior sampling; Smoothing parameter estimation * Michael D. Escobar is Assistant Professor, Department of Statistics and Department of Preventive Medicine and Biostatistics, University ...
350|Bayesian Model Selection in Social Research (with Discussion by Andrew Gelman &amp; Donald B. Rubin, and Robert M. Hauser, and a Rejoinder)  (1995) |It is argued that P-values and the tests based upon them give unsatisfactory results, especially in large samples. It is shown that, in regression, when there are many candidate independent variables, standard variable selection procedures can give very misleading results. Also, by selecting a single model, they ignore model uncertainty and so underestimate the uncertainty about quantities of interest. The Bayesian approach to hypothesis testing, model selection and accounting for model uncertainty is presented. Implementing this is straightforward using the simple and accurate BIC approximation, and can be done using the output from standard software. Specific results are presented for most of the types of model commonly used in sociology. It is shown that this approach overcomes the difficulties with P values and standard model selection procedures based on them. It also allows easy comparison of non-nested models, and permits the quantification of the evidence for a null hypothesis...
352|Regularized discriminant analysis|Linear and quadratic discriminant analysis are considered in the small sample high-dimensional setting. Alternatives to the usual maximum likelihood (plug-in) estimates for the covariance matrices are proposed. These alternatives are characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample based estimate of future misclassification risk. Computationally fast implementations are presented, and the efficacy of the approach is examined through simulation studies and application to data. These studies indicate that in many circumstances dramatic gains in classification accuracy can be achieved. Submitted to Journal of the American Statistical Association
354|Discriminant Analysis by Gaussian Mixtures|Fisher-Rao linear discriminant analysis (LDA) is a valuable tool for multigroup classification. LDA is equivalent to maximum likelihood classification assuming Gaussian distributions for each class. In this paper, we fit Gaussian mixtures to each class to facilitate effective classification in non-normal settings, especially when the classes are clustered. Low dimensional views are an important by-product of LDA---our new techniques inherit this feature. We are able to control the within-class spread of the subclass centers relative to the between-class spread. Our technique for fitting these models permits a natural blend with nonparametric versions of LDA.  Keywords: Classification, Pattern Recognition, Clustering, Nonparametric, Penalized. 1 Introduction  In the generic classification or discrimination problem, the outcome of interest  G falls into J unordered classes, which for convenience we denote by the set J =  f1; 2; 3; \Delta \Delta \Delta Jg. We wish to build a rule for pred...
355|Practical Bayesian Density Estimation Using Mixtures Of Normals|this paper, we propose some solutions to these problems. Our goal is to come up with a simple, practical method for estimating the density. This is an interesting problem in its own right, as well as a first step towards solving other inference problems, such as providing more flexible distributions in hierarchical models. To see why the posterior is improper under the usual reference prior, we write the model in the following way. Let Z = (Z 1 ; : : : ; Z n ) and X = (X 1 ; : : : ; X n ). The Z
357|Detecting Features in  Spatial Point Processes with . . .|We consider the problem of detecting features in spatial point processes in the presence of substantial clutter. One example is the detection of mine elds using reconnaissance aircraft images that erroneously identify many objects that are not mines. Another is the detection of seismic faults on the basis of earthquake catalogs: earthquakes tend to be clustered close to the faults, but there are many that are farther away. Our solution uses model-based clustering based on a mixture model for the process, in which features are assumed to generate points according to highly linear multivariate normal densities, and the clutter arises according to a spatial Poisson process. Very nonlinear features are represented by several highly linear multivariate normal densities, giving a piecewise linear representation. The model is estimated in two stages. In the rst stage, hierarchical model-based clustering is used to provide a rst estimate of the features. In the second stage, this clustering is re ned using the EM algorithm. The number of features is found using an approximation to the posterior probability of each number of features. For the minefield
358|MCLUST: Software for Model-based Cluster Analysis|MCLUST is a software package for cluster analysis written in Fortran and interfaced to the S-PLUS commercial software package1. It implements parameterized Gaussian hierarchical clustering algorithms [16, 1, 7] and the EM algorithm for parameterized Gaussian mixture models [5, 13, 3, 14] with the possible addition of a Poisson noise term. MCLUST also includes functions that combine hierarchical clustering, EM and the Bayesian Information Criterion (BIC) in a comprehensive clustering strategy [4, 8]. Methods of this type have shown promise in a number of practical applications, including character recognition [16], tissue segmentation [1], mine eld and seismic fault detection [4], identi cation of textile aws from images [2], and classi cation of astronomical data [3, 15]. Aweb page with related links can be found at
359|Algorithms for model-based Gaussian hierarchical clustering|1 Funded by the O ce of Naval Research under contracts N00014-96-1-0192 and N00014-96-1-
360|Regularized Gaussian Discriminant Analysis Through Eigenvalue Decomposition|Friedman (1989) has proposed a regularization technique (RDA) of discriminant analysis in the Gaussian framework. RDA makes use of two regularization parameters to design an intermediate classi cation rule between linear and quadratic discriminant analysis. In this paper, we propose an alternative approach to design classi cation rules which have also a median position between linear and quadratic discriminant analysis. Our approach is based on the reparametrization of the covariance matrix k of a group Gk in terms of its eigenvalue decomposition, k = kDkAkD 0 k where k speci es the volume of Gk, Ak its shape, and Dk its orientation. Variations on constraints concerning k?Ak and Dk lead to 14 discrimination models of interest. For each model, we derived the maximum likelihood parameter estimates and our approach consists in selecting the model among the 14 possible models by minimizing the sample-based estimate of future misclassi cation risk by cross-validation. Numerical experiments show favorable behavior of this approach as compared to RDA.
361|Scaling EM (Expectation-Maximization) Clustering to Large Databases  (1999) |Practical statistical clustering algorithms typically center upon an iterative refinement optimization procedure to compute a locally optimal clustering solution that maximizes the fit to data. These algorithms typically require many database scans to converge, and within each scan they require the access to every record in the data table. For large databases, the scans become prohibitively expensive. We present a scalable implementation of the Expectation-Maximization (EM) algorithm. The database community has focused on distance-based clustering schemes and methods have been developed to cluster either numerical or categorical data. Unlike distancebased algorithms (such as K-Means), EM constructs proper statistical models of the underlying data source and naturally generalizes to cluster databases containing both discrete-valued and continuous-valued data. The scalable method is based on a decomposition of the basic statistics the algorithm needs: identifying regions of the data that...
362|Averaging, Maximum Penalized Likelihood and Bayesian Estimation for Improving Gaussian Mixture Probability Density Estimates|We apply the idea of averaging ensembles of estimators to probability density estimation.
363|Non Parametric Maximum Likelihood Estimation of Features in . . .|This paper addresses the problem of estimating the support domain of a bounded point process in presence of background noise. This situation occurs for example in the detection of a mine field from aerial observations. A Maximum Likelihood Estimator for a mixture of uniform point processes is derived using a natural partition of the space defined by the data themselves: the Voronoï tessellation. The methodology is tested on simulations and compared to a model-based clustering technique.
364|Inference in model-based cluster analysis|A new approach to cluster analysis has been introduced based on parsimonious geometric modelling of the within-group covariance matrices in a mixture of multivariate normal distributions, using hierarchical agglomeration and iterative relocation. It works well and is widely used via the MCLUST software available in S-PLUS and StatLib. However, it has several limitations: there is no assessment of the uncertainty about the classification, the partition can be suboptimal, parameter estimates are biased, the shape matrix has to be specified by the user, prior group probabilities are assumed to be equal, the method for choosing the number of groups is based on a crude approximation, and no formal way of choosing between the various possible models is included. Here, we propose a new approach which overcomes all these difficulties. It consists of exact Bayesian inference via Gibbs sampling, and the calculation of Bayes factors (for choosing the model and the number of groups) from the output using the Laplace-Metropolis estimator. It works well in several real and simulated examples.  
365|Principal curve clustering with noise|was supported by ONR grants N00014-96-1-0192 and N00014-96-1-0330. The authors are Clustering on principal curves combines parametric modeling of noise with nonparametric modeling of feature shape. This is useful for detecting curvilinear features in spatial point patterns, with or without background noise. Applications of this include the detection of curvilinear mine elds from reconnaissance images, some of the points in which represent false detections, and the detection of seismic faults from earthquake catalogs. Our algorithm for principal curve clustering is in two steps: the rst is hierarchical and agglomerative (HPCC), and the second consists of iterative relocation based on the Classi cation EM algorithm (CEM-PCC). HPCC is used to combine potential feature clusters, while CEM-PCC re nes the results and deals with background noise. It is importanttohave a good starting point for the algorithm: this can be found manually or automatically using, for example, nearest neighbor clutter removal or model-based clustering. We choose the number of features and the amount of smoothing simultaneously using approximate Bayes
366|Fitting mixtures of Kent distributions to aid in joint set identification|When examining a rock mass, joint sets and their orientations can play a significant role with regard to how the rock mass will behave. To identify joint sets present in the rock mass, the orientation of individual fracture planes can be measured on exposed rock faces and the resulting data can be examined for heterogeneity. In this article, the expectation–maximization algorithm is used to fit mixtures of Kent component distributions to the fracture data to aid in the identification of joint sets. An additional uniform component is also included in the model to accommodate the noise present in the data.
367|From Massive Data Sets to Science Catalogs: Applications and Challenges|With hardware advances in scientific instruments and data gathering techniques comes the  inevitable flood of data that can render traditional approaches to science data analysis severely  inadequate. The traditional approach of manual and exhaustive analysis of a data set is no longer  feasible for many tasks ranging from remote sensing, astronomy, and atmospherics to medicine,  molecular biology, and biochemistry. In this paper we present our views as practitioners engaged  in building computational systems to help scientists deal with large data sets. We focus on  what we view as challenges and shortcomings of the current state-of-the-art in data analysis in  view of the massive data sets that are still awaiting analysis. The presentation is grounded in  applications in astronomy, planetary sciences, solar physics, and atmospherics that are currently  driving much of our work at JPL.  keywords: science data analysis, limitations of current methods, challenges for massive data sets, ...
368|Modeling Rational Agents within a BDI-Architecture|Intentions, an integral part of the mental state of an agent, play an important role in 
369|Plans And Resource-Bounded Practical Reasoning|An architecture for a rational agent must allow for means-end reasoning, for the weighing of competing alternatives, and for interactions between these two forms of reasoning. Such an architecture must also address the problem of resource boundedness. We sketch a solution of the first problem that points the way to a solution of the second. In particular, we present a high-level specification of the practical-reasoning component of an architecture for a resource-bounded rational agent. In this architecture, a major role of the agent&#039;s plans is to constrain the amount of further practical reasoning she must perform.
370|A temporal logic for reasoning about processes and plans|Much previous work in artificial intelligence has neglected representing time in all its complexity. In particular, it has neglected continuous change and the indeterminacy of the future. To rectify this, I have developed a first-order tem-poral logic, in which it is possible to name and prove things about facts, events, plans, and world histories. In particular, the logic provides analyses of causality, continuous change in quantities, the persistence of facts (the frame problem), and the relationship between tasks and actions. It may be possible to implement a temporal-inference machine based on this logic, which keeps track of several &#034;maps &#034; of a time line, one per possible history. I.
371|Decision-Making in an Embedded Reasoning System |The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate e ectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability toreact rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.  
372|Principles of Metareasoning|In this paper we outline a general approach to the study of metareasoning, not in the sense of explicating the semantics of explicitly specified meta-level control policies, but in the sense of providing a basis for selecting and justifying computational actions. This research contributes to a developing attack on the problem of resource-bounded rationality, by providing a means for analysing and generating optimal computational strategies. Because reasoning about a computation without doing it necessarily involves uncertainty as to its outcome, probability and decision theory will be our main tools. We develop a general formula for the utility of computations, this utility being derived directly from the ability of computations to affect an agent&#039;s external actions. We address some philosophical difficulties that arise in specifying this formula, given our assumption of limited rationality. We also describe a methodology for applying the theory to particular problem-solving systems, a...
373|Asymmetry Thesis and Side-Effect Problems in Linear-Time and Branching-Time Intention Logics|In this paper, we examine the relationships between beliefs, goals, and intentions. In particular, we consider the formalization of the Asymmetry Thesis as proposed by Bratman [ 1987 ] . We argue that the semantic characterization of this principle determines if the resulting logic is capable of handling other important problems, such as the side-effect problem of belief-goal-intention interaction. While Cohen and Levesque&#039;s [ 1990 ] formalization faithfully models some aspects of the asymmetry thesis, it does not solve all the side-effect problems; on the other hand the formalization provided by Rao and Georgeff [ 1991 ] solves all the side-effect problems, but only models a weak form of the asymmetry thesis. In this paper, we combine the intuition behind both these approaches and provide a semantic account of the asymmetry thesis, in both linear-time and branching-time logics, for solving many of these problems.  1 Introduction  Formalizations of intentions and their relationships w...
374|Minimal change and maximal coherence: a basis for belief revision and reasoning about actions|The study of belief revision and reasoning about actions have been two of the most active areas of research in AI. Both these areas involve reasoning about change. However very little work has been done in analyzing the principles common to both these areas. This paper presents a formal characterization of belief revision, based on the principles of minimal change and maximal coherence. This formal theory is then used to reason about actions. The resulting theory provides an elegant solution to the conceptual frame and ramification problems. It also facilitates reasoning in dynamic situations where the world changes during the execution of an action. The principles of minimal change and maximal coherence seem to unify belief revision and reasoning about actions and may form a fundamental core for reasoning about other dynamic processes that involve change. 1
375|The Nash Bargaining Solution in Economic Modeling|This article establishes the relationship between the static axiomatic theory of bargaining and the sequential strategic approach to bargaining. We consider two strategic models of alternating offers. The models differ in the source of the incentive of the bargaining parties to reach agreement: the bargainers &#039; time preference and the risk of breakdown of negotiation. Each of the models has a unique perfect equilibrium. When the motivation to reach agreement is made negligible, in each model the unique perfect equilibrium outcome approaches the Nash bargaining solution, with utilities that reflect the incentive to settle and with the proper disagreement jfoint chosen. The results provide a guide for the application of the Nash bar-gaining solution in economic modelling. 1.
378|Conditional random fields: Probabilistic models for segmenting and labeling sequence data|We present conditional random fields, a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data. 1.
379|An Empirical Study of Smoothing Techniques for Language Modeling|We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods. 1
380|A Maximum-Entropy-Inspired Parser|We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &#034;stan- dard&#034; sections of the Wall Street Journal tree- bank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innova- tion is the use of a &#034;maximum-entropy-inspired&#034; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head&#039;s pre-terminal before guessing the lexical head.
381|Class-Based n-gram Models of Natural Language|We address the problem of predicting a word from previous words in a sample of text. In particular we discuss n-gram models based on calsses of words. We also discuss several statistical algoirthms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.
382|Generation and Synchronous Tree-Adjoining Grammars|Tree-adjoining grammars (TAG) have been proposed as a formalism for generation based on the intuition that the extended domain of syntactic locality that TAGs provide should aid in localizing semantic dependencies as well, in turn serving as an aid to generation from semantic representations. We demonstrate that this intuition can be made concrete by using the formalism of synchronous tree-adjoining grammars. The use of synchronous TAGs for generation provides solutions to several problems with previous approaches to TAG generation. Furthermore, the semantic monotonicity requirement previously advocated for generation gram- mars as a computational aid is seen to be an inherent property of synchronous TAGs.
383|An Efficient Boosting Algorithm for Combining Preferences|The problem of combining preferences arises in several applications, such as combining the results of different search engines. This work describes an efficient algorithm for combining multiple preferences. We first give a formal framework for the problem. We then describe and analyze a new boosting algorithm for combining preferences called RankBoost. We also describe an efficient implementation of the algorithm for certain natural cases. We discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different WWW search strategies, each of which is a query expansion for a given domain. For this task, we compare the performance of RankBoost to the individual search strategies. The second experiment is a collaborative-filtering task for making movie recommendations. Here, we present results comparing RankBoost to nearest-neighbor and regression algorithms.
384|Lexical-Functional Grammar:  A Formal System for Grammatical Representation|In learning their native language, children develop a remarkable set of capabilities. They acquire knowledge and skills that enable them to produce and comprehend an indefinite number of novel utterances, and to make quite subtle judgments about certain of their properties. The major goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities. In pursuing this goal, we have adopted what we call the Competence Hypothesis as a methodological principle. We assume that an explanatory model of human language performance will incorporate a theoretically justi ed representation of the native speaker&#039;s linguistic knowledge (a grammar) as a component separate both from the computational mechanisms that operate on it (a processor) and from other nongrammatical processing parameters that might influence the processor&#039;s behavior.  To a certain extent the various components that we postulate can be studied independently, guided where appropriate by the well-established methods and evaluation standards of linguistics, computer science, and experimental psychology. However, the requirement that the various components ultimately must fit together in a consistent and coherent model imposes even stronger constraints on their structure and operation.
385|A Maximum Entropy Model for Part-Of-Speech Tagging|This paper presents a statistical model which trains from a corpus annotated with Part-OfSpeech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%). The model can be classified as a Maximum Entropy model and simultaneously uses many contextual &#034;features&#034; to predict the POS tag. Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.
386|Three Generative, Lexicalised Models for Statistical Parsing|In this paper we first propose a new statistical  parsing model, which is a generative  model of lexicalised context-free gram-  mar. We then extend the model to in-  clude a probabilistic treatment of both subcategorisation  and wh~movement. Results  on Wall Street Journal text show that the  parser performs at 88.1/87.5% constituent  precision/recall, an average improvement  of 2.3% over (Collins 96).
387|Statistical Parsing with a Context-free Grammar and Word Statistics|We describe a parsing system based upon a language  model for English that is, in turn, based upon assigning  probabilities to possible parses for a sentence. This  model is used in a parsing system by finding the parse  for the sentence with the highest probability. This system  outperforms previous schemes. As this is the third  in a series of parsers by different authors that are similar  enough to invite detailed comparisons but different  enough to give rise to different levels of performance,  we also report on some experiments designed to identify  what aspects of these systems best explain their  relative performance.  Introduction  We present a statistical parser that induces its grammar and probabilities from a hand-parsed corpus (a tree-bank). Parsers induced from corpora are of interest both as simply exercises in machine learning and also because they are often the best parsers obtainable by any method. That is, if one desires a parser that produces trees in the tree-bank ...
388|Statistical Decision-Tree Models for Parsing|Syntactic natural language parsers have  shown themselves to be inadequate for processing  highly-ambiguous large-vocabulary  text, as is evidenced by their poor per-  formance on domains like the Wall Street  Journal, and by the movement away  from parsing-based approaches to textprocessing  in general. In this paper, I describe  SPATTER, a statistical parser based  on decision-tree learning techniques which  constructs a complete parse for every sentence  and achieves accuracy rates far better  than any published result. This work  is based on the following premises: (1)  grammars are too complex and detailed to  develop manually for most interesting domains;  (2) parsing models must rely heavily  on lexical and contextual information  to analyze sentences accurately; and (3)  existing n-gram modeling techniques are  inadequate for parsing models. In experiments  comparing SPATTER with IBM&#039;s  computer manuals parser, SPATTER significantly  outperforms the grammar-based  parser. Evaluating SPATTER against the  Penn Treebank Wall Street Journal corpus  using the PARSEVAL measures, SPATTER  achieves 86% precision, 86% recall,  and 1.3 crossing brackets per sentence for  sentences of 40 words or less, and 91% precision,  90% recall, and 0.5 crossing brackets  for sentences between 10 and 20 words in  length.
389|The Penn Treebank: Annotating Predicate Argument Structure|The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as &#034;underlying &#034; position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles. 1. INTRODUCTION During the first phase of the The Penn Treebank project [10], ending in December 1992, 4.5 million words of text were tagged for part-of-speech, with about two-thirds of this material also annotated with a skeletal syntactic bracketing. All of this material has been hand corre...
391|Discriminative Reranking for Natural Language Parsing|This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model’s score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.  
392|Three New Probabilistic Models for Dependency Parsing: An Exploration|After presenting a novel O(n³) parsing algorithm  for dependency grammar, we develop  three contrasting ways to stochasticize  it. We propose (a) a lexical affinity model  where words struggle to modify each other,  (b) a sense tagging model where words fluctuate  randomly in their selectional preferences,  and (c) a generative model where  the speaker fleshes out each word&#039;s syntactic  and conceptual structure without regard to  the implications for the hearer. We also give  preliminary empirical results from evaluating  the three models&#039; parsing performance  on annotated Wall Street Journal training  text (derived from the Penn Treebank). In  these results, the generative model performs  significantly better than the others, and  does about equally well at assigning part-of-speech tags.  
393|Treebank Grammars|By a “tree-bank grammar ” we mean a context-free grammar created by reading the production rules directly from hand-parsed sentences in a tree bank. Common wisdom has it that such grammars do not perform we &amp; though we know of no published data on the issue. The primary purpose of this paper is to show that the common wisdom is wrong. In particular, we present results on a tree-bank grammar based on the Penn WaII Street Journal tree bank. To the best of our knowledge, this grammar outperforms ah other non-word-based statistical parsers/grammars on this corpus. That is, it outperforms parsers that consider the input as a string of tags and ignore the actual words of the corpus.
394|Gemini: A Natural Language System For Spoken-Language Understanding|This paper describes the details of the system, and includes relevant measurements of size, efficiency, and performance of each of its components
395|A Linear Observed Time Statistical Parser Based on Maximum Entropy Models|This paper presents a statistical parser for  natural language that obtains a parsing  accuracy--roughly 87% precision and 86%  recall--which surpasses the best previously  published results on the Wall St. Journal  domain. The parser itself requires very little  human intervention, since the information  it uses to make parsing decisions is  specified in a concise and simple manner,  and is combined in a fully automatic way  under the maximum entropy framework.
396|A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation|I.n this paper, we describe a new corpus-based ap  proach to prepositional phrase attachment disambiguation,  and 10resent results comparing perlbrmance  of this algorithm with ol,her corpus-based  approaches to this problem.
397|Towards History-based Grammars: Using Richer Models for Probabilistic Parsing|We describe a generarive probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.
400|A Statistical Parser for Czech|This paper considers statistical parsing of Czech, which differs radically from English in at least two  respects: (1) it is a highly infiected language, and (2) it has relatively free word order. These dif- ferences are likely to .pose new problems for tech- niques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results - 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.
401|Automatic Grammar Induction and Parsing Free Text: A Transformation-Based Approach|In this paper we describe a new technique for parsing free text: a transformational grammar  is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.
402|Equations for Part-of-Speech Tagging|We derive from first principles the basic equations for a few of the basic hidden-Markov-model word taggers as well as equations for other models which may be novel (the descriptions in previous papers being too spare to be sure). We give performance results for all of the models. The results from our best model (96.45% on an unused test sample from the Brown corpus with 181 distinct tags) is on the upper edge of reported results. We also hope these results clear up some confusion in the literature about the best equations to use. However, the major purpose of this paper is to show how the equations for a variety of models may be derived and thus encourage future authors to give the equations for their model and the derivations thereof.  Introduction  The last few years have seen a fair number of papers on part-of-speech tagging --- assigning the correct part of speech to each word in a text [1,2,4,5,7,8,9,10]. Most of these systems view the text as having been produced by a hidden Mar...
403|A novel use of statistical parsing to extract information from text|Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.
404|Efficient Parsing for Bilexical Context-Free Grammars and Head Automaton Grammars|Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words. We present O(n^4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of O(n^5). For a common special case that was known to allow O(n³) parsing (Eisner, 1997), we present an O(n³) algorithm with an improved grammar constant.
405|Parsing Inside-Out|Probabilistic Context-Free Grammars (PCFGs) and variations on them have recently become some of the most common formalisms for parsing. It is common with PCFGs to compute the inside and outside probabilities. When these probabilities are multiplied together and normalized, they produce the probability that any given non-terminal covers any piece of the input sentence. The traditional use of these probabilities is to improve the probabilities of grammar rules. In this thesis we show that these values are useful for solving many other problems in Statistical Natural Language Processing. We give a framework for describing parsers. The framework generalizes the inside and outside values to semirings. It makes it easy to describe parsers that compute a wide variety of interesting quantities, including the inside and outside probabilities, as well as related quantities such as Viterbi probabilities and n-best lists. We also present three novel uses for the inside and outside probabilities. T...
406|Learning Parse and Translation Decisions from Examples with Rich Context|We present a knowledge and context-based system for parsing and translating natural language and evaluate it on sentences from the Wall Street Journal. Applying machine learning techniques, the system uses parse action examples acquired under supervision to generate a deterministic shift-reduce parser in the form of a decision structure. It relies heavily on context, as encoded in features which describe the morphological, syntactic, semantic and other aspects of a given parse state.
407|Efficient Algorithms for Parsing the DOP Model|Excellent results have been reported for DataOriented Parsing (DOP) of natural language texts (Bod, 1993c). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo p arsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model toga small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct con- stituents, rather than the probability of a correct parse tree. Using ithe optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is compara- ble to results from a duplication of Pereira and Schabes&#039;s (1992) experiment on the same data. We show that Bod&#039;s results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.
408|Conditional Structure versus Conditional Estimation in NLP Models|This paper separates conditional parameter estimation,  which consistently raises test set accuracy on  statistical NLP tasks, from conditional model structures,  such as the conditional Markov model used  for maximum-entropy tagging, which tend to lower  accuracy. Error analysis on part-of-speech tagging  shows that the actual tagging errors made by the  conditionally structured model derive not only from  label bias, but also from other ways in which the independence  assumptions of the conditional model  structure are unsuited to linguistic sequences. The  paper presents new word-sense disambiguation and  POS tagging experiments, and integrates apparently  conflicting reports from other recent work.
410|Pearl: A Probabilistic Chart Parser|This i)al)cr descrihcs a natural language i)ars - ing algorith,n for unrestricted text whicll uses a i)rol)ability-based scoring fimctiou to select the &#034;}mst&#034; I)arse of a sentence. The parser, earl, is a I. ime-a.synchronous bottom-ul) chart I)arscr with Earicy-type top-down prediction which pursues the highest-scoring theory i} the chart, where the score of a theory represents the cxteut I,o which t. he context of the sentmice predicts that iuterpretation. This parser differs h&#039;om previous attempts at stochastic parsers in thai. it uses a richer form of conditional probalfilitics based on context to l)rcdiet likelihood. Pearl also provides a fralnework for incorporating l.he results of previous work iu Imrt-olLsl)cech assignnmnt, mlknown word models, and ol.her Irol)al)ilistic models of linguistic features iuto one parslug tool, interleaving these techniques instead of using the traditional pipeline archiLecture. In preliminary tests, &#039;Pearl has been successl&#039;ul aL resolving parL-o[-speech and word (in speech processing) ambiguiLy, de[ermiuing categories [or unknown words, and selecLing cotreeL parses first. using a very loosely fiLing covering grammar. 1  
411|Development and Evaluation of a Broad-Coverage Probabilistic Grammar of English-Language Computer Manuals|We present an approach to grammar development where the task is decomposed into two separate subtasks. The first task is linguistic, with the goal of producing a set of rules that have a large coverage (in the sense that the correct parse is among the proposed parses) on a blind test set of sentences. The second task is statistical, with the goal of developing a model of the grammax which assigns maximum probability for the correct parse. We give parsing results on text from computer manuals.
412|Decision tree parsing using a hidden derivation model|Parser development is generally viewed as a primarily linguistic enterprise. A grammarian examines sentences, skillfully extracts the linguistic generalizations evident in the data, and writes grammar rules which cover the language. The grammarian
413|Efficiency, Robustness and Accuracy in Picky Chart Parsing|This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called probabilistic prediction to predict which grammar rules are likely to lead to an acceptable parse of the input. Using a suboptimal search method, Picky significantly reduces the number of edges produced by CKY-like chart parsing algorithms, while maintaining the robustness of pure bottom-up parsers and the accuracy of existing probabilistic parsers. Experiments using Picky demonstrate how probabilistic modelling can impact upon the efficiency, robustness and accuracy of a parser. 
414|Head Automata and Bilingual Tiling: Translation with Minimal Representations|We present a language model consisting of  a collection of costed bidirectional finite  state automata associated with the head  words of phrases. The model is suitable  for incremental application of lexical associations  in a dynamic programming search  for optimal dependency tree derivations. We also
415|Corpus Statistics Meet the Noun Compound: Some Empirical Results|A variety of statistical methods for noun  compound analysis are implemented and  compared. The results support two main  conclusions. First, the use of conceptual  association not only enables a broad coverage,  but also improves the accuracy. Second,  an analysis model based on dependency  grammar is substantially more accurate  than one based on deepest constituents,  even though the latter is more preva-  lent in the literature.
416|Context-Sensitive Statistics for Improved Grammatical Language Models|We develop a language model using probabilistic context-free grammars (PCFGs) that is &#034;pseudo context-sensitive&#034; in that the probability that a non-terminal N expands using a rule r depends on N &#039;s parent. We derive the equations for estimating the necessary probabilities using a variant of the inside-outside algorithm. We give experimental results showing that, beginning with a high-performance PCFG, one can develop a pseudo PCSG that yields significant performance gains. Analysis shows that the benefits from the context-sensitive statistics are localized, suggesting that we can use them to extend the original PCFG. Experimental results confirm that this is both feasible and the resulting grammar retains the performance gains. This implies that our scheme may be useful as a novel method for PCFG induction. 1 Introduction  Like its non-stochastic brethren, probabilistic parsing has been based upon context-free grammars (CFGs), and for similar reasons: CFGs support a simple and efficien...
417|Statistical Parsing of Messages|The recent trend in natural language processing research has been to develop systems that deal with text concerning small, well defined domains. One practical application for such systems is to process messages pertaining to some very specific task or activity [5]. The advantage of dealing with such domains is twofold- firstly, due to the narrowness of the domain, it is possible to encode most of the knowledge related to the domain and to make this knowledge accessible to the natural language processing system, which in turn can use this knowledge to disambiguate the meanings of the messages. Secondly, in such a domain, there is not a great diversity of language constructs and therefore it becomes easier to construct a grammar which will capture all the constructs which exist in this sub-language. However, some practical aspects of such domains tend to make the problem somewhat difficult. Often, the messages tend not to be absolutely grammatically correct. As a result, the grammar designed for such a system needs to be far more forgiving than one designed for the task of parsing edited English. This can result in a proliferation of parses, which in turn makes the disambiguation task more difficult. This problem is further compounded by the telegraphic nature of the discourse, since telegraphic discourse is more prone to be syntactically ambiguous. Statistical Parsing The major objective of the research described in this paper is to use statistical data to evaluate the likelihood of a parse in order to help the parser prune out unlikely parses. Our conjecture- supported by our results and some prior, similar experiments- is that a more probable parse has a greater chance of being the correct one. The related work by the research team at UCREL
418|Global Thresholding and Multiple-Pass Parsing|We present a variation on classic beam  thresholding techniques that is up to an order  of magnitude faster than the traditional  method, at the same performance level. We  also present a new thresholding technique,  global thresholding, which, combined with  the new beam thresholding, gives an additional  factor of two improvement, and a  novel technique, multiple pass parsing, that  can be combined with the others to yield  yet another 50% improvement. We use a  new search algorithm to simultaneously op-  timize the thresholding parameters of the  various algorithms.
419|Probabilistic Feature Grammars|We present a new formalism, probabilistic feature grammar (PFG). PFGs combine most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance. PFGs generate features one at a time, probabilistically, conditioning the probabilities of each feature on other features in a local context. Because the conditioning is local, efficient polynomial time parsing algorithms exist for computing inside, outside, and Viterbi parses. PFGs can produce probabilities of strings, making them potentially useful for language modeling. Precision and recall results are comparable to the state of the art with words, and the best reported without words. 1 Introduction  Recently, many researchers have worked on statistical parsing techniques which try to capture additional context beyond that of simple probabilistic context-free grammars (PCFGs), including Magerman (1995), Charniak (1996), Collins (1996; 1997), ...
420|Stochastic HPSG|In this paper we provide a probabilistic  interpretation for typed feature structures  very similar to those used by Pollard  nd Sag. We begin with a version  of the interpretation which lacks  a treatment of re-entrant feature struc-  tures, then provide an extended interpre-  tation which allows them. We sketch al-  gorithms allowing the numerical parameters  of our probabilistic interpretations  of HPSG to be estimated from corpora.
421|What is the Minimal Set of Fragments that Achieves Maximal Parse Accuracy?|We aim at finding the minimal set of  fragments which achieves maximal parse  accuracy in Data Oriented Parsing. Experiments  with the Penn Wall Street  Journal treebank show that counts of  almost arbitrary fragments within parse  trees are important, leading to improved  parse accuracy over previous models  tested on this treebank (a precision of 90.8% and a recall of 90.6%). We  isolate some dependency relations which  previous models neglect but which  contribute to higher parse accuracy.
422|Automatic Learning for Semantic Collocation|The real difficulty in development of practical NLP systems comes from the fact that we do not have effective means for gathering &#034;knowledge &#034;. In this paper, we propose an algorithm which acquires automatically knowledge of semantic collocations among &#034;words&#034; from sample corpora. The algorithm
423|A Statistical Model for Parsing and Word-Sense Disambiguation|This paper describes a first attempt at a statistical model for simultaneous syntactic parsing and generalized word-sense disambiguation. On a new data set we have constructed for the task, while we were disappointed not to find parsing improvement over a traditional parsing model, our model achieves a recall of 84.0% and a precision of 67.3% of exact synset matches on our test corpus, where the gold standard has a reported inter-annotator agreement of 78.6%.
424|On The Unsupervised Induction Of Phrase-Structure Grammars|This paper examines why some previous approaches have failed to acquire desired grammars without supervision, and proposes that with a different conception of phrase-structure supervision might not be necessary. In particular, it describes in detail some reasons why SCFGs are poor mod-  2 CARL DE MARCKEN els to use for learning human language, especially when combined with the inside-outside algorithm. Following up on these arguments, it proposes that head-driven grammatical formalisms like link grammars (Sleator and Temperley, 1991) are better suited to the task, and introduces a framework for CFG induction that sidesteps many of the search problems that previous schemes have had. In the end, we hope the analysis presented here convinces others to look carefully at their representations and search strategies before blindly applying them to the language learning task. We start the discussion by examining the differences between the linguistic and statistical motivations for phrase structure; this frames our subsequent analysis. Then we introduce a simple extension to stochastic context-free grammars, and use this new class of language models in two experiments that pinpoint specific problems with both SCFGs and the search strategies commonly applied to them. Finally, we explore fixes to these problems.
425|The Effect of Alternative Tree Representations on Tree Bank Grammars|The performance of PCFGs estimated from  tree banks is shown to be sensitive to the particular  way in which linguistic constructions  are represented as trees in the tree bank. This  paper presents a theoretical analysis of the  effect of different tree representations for PP  attachment on PCFG models, and introduces  a new methodology for empirically examining  such effects using tree transformations. It  shows that one transformation, which copies  the label of a parent node onto the labels of  its children, can improve the performance of  a PCFG model in terms of labelled precision  and recall on held out data from 73% (precision)  and 69% (recall) to 80% and 79% respectively.  It also points out that if only maximum  likelihood parses are of interest then  many productions can be ignored, since they  are subsumed by combinations of other productions  in the grammar. In the Penn II tree  bank grammar, almost 9% of productions are  subsumed in this way.  1 
426|A Probabilistic Parser Applied to Software Testing Documents|We describe an approach to training a statistical parser from a bracketed corpus, and demonstrate its use in a software testing application that translates English specifications into an automated testing language. A grammar is not explicitly specified; the rules and contextual probabilities of occurrence are automatically generated from the corpus. The parser is extremely successful at producing and identifying the correct parse, and nearly deterministic in the number of parses that it produces. To compensate for undertraining, the parser also uses general, linguistic subtheories which aid in guessing some types of novel structures.  Introduction  In constrained domains, natural language processing can often provide leverage. In software testing at AT&amp;T, for example, 20,000 English test cases prescribe the behavior of a telephone switching system. A test case consists of about a dozen sentences describing the goal of the test, the actions to perform, and the conditions to verify. Figu...
427|A Probabilistic Parser and Its Application|We describe a general approach to the probabilistic parsing of context-free grammars. The method integrates context-sensitive statistical knowledge of various types (e.g., syntactic and semantic) and can be trained incrementally from a bracketed corpus. We introduce a variant of the GHR contextfree recognition algorithm, and explain how to adapt it for efficient probabilistic parsing. On a real-world corpus of sentences from software testing documents, with 23 possible parses for a sentence of average length, the system accurately finds the correct parse in 99% of cases, while producing only 1.02 parses per sentence. Significantly, the success rate would be only 66% without the semantic statistics.  Introduction  In constrained domains, natural language processing can often provide leverage. At AT&amp;T, for instance, NL technology can potentially help automate many aspects of software development. A typical example occurs in the software testing area. Here 250,000 English sentences specif...
428|FRBR: Functional Requirements for Bibliographic Records 150 Application of the Entity-Relationship Model to Humphry Clinker |Visit LRTS online at www.ala.org/alcts/lrts. For current news and reports on ALCTS activi-ties, see the ALCTS Newsletter Online at www. ala.org/ alcts/alcts_news.
429|Hidden Markov models in computational biology: applications to protein modeling|Hidden.Markov Models (HMMs) are applied t.0 the problems of statistical modeling, database searching and multiple sequence alignment of protein families and protein domains. These methods are demonstrated the on globin family, the protein kinase catalytic domain, and the EF-hand calcium binding motif. In each case the parameters of an HMM are estimated from a training set of unaligned sequences. After the HMM is built, it is used to obtain a multiple alignment of all the training sequences. It is also used to search the. SWISS-PROT 22 database for other sequences. that are members of the given protein family, or contain the given domain. The Hi &#034; produces multiple alignments of good quality that agree closely with the alignments produced by programs that incorporate threedimensional structural information. When employed in discrimination tests (by examining how closely the sequences in a database fit the globin, kinase and EF-hand HMMs), the &#039;\ HMM is able to distinguish members of these families from non-members with a high degree of accuracy. Both the HMM and PROFILESEARCH (a technique used to search for relationships between a protein sequence and multiply aligned sequences) perform better in these tests than PROSITE (a dictionary of sites and patterns in proteins). The HMM appecvs to have a slight advantage over PROFILESEARCH in terms of lower rates of false
430|A tutorial on hidden Markov models and selected applications in speech recognition|Although initially introduced and studied in the late 1960s and early 1970s, statistical methods of Markov source or hidden Markov modeling have become increasingly popular in the last several years. There are two strong reasons why this has occurred. First the models are very rich in mathematical structure and hence can form the theoretical basis for use in a wide range of applications. Sec-ond the models, when applied properly, work very well in practice for several important applications. In this paper we attempt to care-fully and methodically review the theoretical aspects of this type of statistical modeling and show how they have been applied to selected problems in machine recognition of speech.  
431|On the Computational Complexity of Approximating Distributions by Probabilistic Automata|We introduce a rigorous performance criterion for training algorithms for probabilistic automata (PAs) and hidden Markov models (HMMs), used extensively for speech recognition, and analyze the complexity of the training problem as a computational problem. The PA training problem is the problem of approximating an arbitrary, unknown source distribution by distributions generated by a PA. We investigate the following question about this important, well-studied problem: Does there exist an efficient training algorithm such that the trained PAs provably converge to a model close to an optimum one with high confidence, after only a feasibly small set of training data? We model this problem in the framework of computational learning theory and analyze the sample as well as computational complexity. We show that the number of examples required for training PAs is moderate -- essentially linear in the number of transition probabilities to be trained and a low-degree polynomial in the example l...
432|Using Dirichlet Mixture Priors to Derive Hidden Markov Models for Protein Families|A Bayesian method for estimating the amino acid  distributions in the states of a hidden Markov  model (HMM) for a protein family or the columns  of a multiple alignment of that family is introduced.  This method uses Dirichlet mixture densities  as priors over amino acid distributions. These  mixture densities are determined from examination  of previously constructed HMMs or multiple  alignments. It is shown that this Bayesian method  can improve the quality of HMMs produced from  small training sets. Specific experiments on the  EF-hand motif are reported, for which these priors  are shown to produce HMMs with higher likelihood  on unseen data, and fewer false positives  and false negatives in a database search task.  
433|Maximum likelihood competitive learning|One popular class of unsupervised algorithms are competitive algo-rithms. In the traditional view of competition, only one competitor, the winner, adapts for any given case. I propose to view compet-itive adaptation as attempting to fit a blend of simple probability generators (such as gaussians) to a set of data-points. The maxi-mum likelihood fit of a model of this type suggests a &#034;softer &#034; form of competition, in which all competitors adapt in proportion to the relative probability that the input came from each competitor. I investigate one application of the soft competitive model, place-ment of radial basis function centers for function interpolation, and show that the soft model can give better performance with little additional computational cost. 1
434|Protein Modeling using Hidden Markov Models: Analysis of Globins|We apply Hidden Markov Models (HMMs) to the problem of statistical modeling and multiple alignment of protein families. A variant of the Expectation Maximization (EM) algorithm known as the Viterbi algorithm is used to obtain the statistical model from the unaligned sequences. In a detailed series of experiments, we have taken 400 unaligned globin sequences, and produced a statistical model entirely automatically from the primary (unaligned) sequences using no prior knowledge of globin structure. The produced model includes amino acid distributions for all the known positions in the 7 major alpha-helices, as well as the probability of and average length of insertions between these positions, and the probability that each position is not present at all. Using this model, we obtained a multiple alignment of the 400 sequences and 225 other globin sequences, that agrees almost perfectly with a structural alignment by Bashford et al. This model can also discriminate all these 625 globins fr...
435|Stochastic Context-Free Grammars for Modeling RNA|this paper, we apply stochastic context-free grammars (SCFGs) to the problems of statistical modeling, database searching, multiple alignment, and prediction of the secondary structure of RNA families. This approach is highly related to our previous work on modeling protein families with HMMs [HKMS93, KBM
436|Hidden Markov Models in Molecular Biology: New Algorithms and Applications|Hidden Markov Models (HMMs) can be applied to several important  problems in molecular biology. We introduce a new convergent  learning algorithm for HMMs that, unlike the classical Baum-Welch  algorithm is smooth and can be applied on-line or in batch mode,  with or without the usual Viterbi most likely path approximation.  Left-right HMMs with insertion and deletion states are then trained  to represent several protein families including immunoglobulins and  kinases. In all cases, the models derived capture all the important  statistical properties of the families and can be used efficiently in  a number of important tasks such as multiple alignment, motif detection,  and classification.  3  and Division of Biology, California Institute of Technology.  y  and Department of Psychology, Stanford University.  1 INTRODUCTION  Hidden Markov Models (e.g., Rabiner, 1989) and the more general EM algorithm in statistics can be applied to the modeling and analysis of biological primary sequenc...
437|Improved Statistical Alignment Models|In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications.
439|Improved Alignment Models for Statistical Machine Translation|In this paper, we describe improved alignment  models for statistical machine translation. The  statistical translation approach uses two types  of information: a translation model and a lan-  guage model. The language model used is a  bigram or general m-gram model. The translation  model is decomposed into a lexical and an  alignment model. We describe two different approaches  for statistical translation and present  experimental results. The first approach is  based on dependencies between single words,  the second approach explicitly takes shallow  phrase structures into account, using two different  alignment levels: a phrase level alignment  between phrases and a word level alignment  between single words. We present results us-  ing the Verbmobil task (German-English, 6000word  vocabulary) which is a limited-domain  spoken-language task. The experimental tests  were performed on both the text transcription  and the speech recognizer output.
440|Manual annotation of translational equivalence: The Blinker project|Bilingual annotators were paid to link roughly sixteen thousand corresponding words between on-line versions of the Bible in modern French and modern English. These annotations are freely available to the researchcommunity from
441|But dictionaries are data too|Although empiricist approaches to machine translation depend vitally on data in the form of large bilingual corpora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iikefihood estimates of the parameters of a probabilistic model from this combined data and we show how these parameters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between rationalist
442|Evaluation of Word Alignment Systems|Recent years have seen a few serious attempts to develop methods and measures for the evaluation of word alignment systems, notably the Blinker project (Melamed, 1998) and the ARCADE project (Vronis and Langlais, forthcoming). In this paper we discuss different approaches to the problem and report on results from a project where two word alignment systems have been evaluated. These results include methods and tools for the generation of reference data and a set of measures for system performance. We note that the selection and sampling of reference data can have a great impact on scoring results.
443|Goal-directed Requirements Acquisition|Requirements analysis includes a preliminary acquisition step where a global model for the specification of the system and its environment is elaborated. This model, called requirements model, involves concepts that are currently not supported by existing formal specification languages, such as goals to be achieved, agents to be assigned, alternatives to be negotiated, etc. The paper presents an approach to requirements acquisition which is driven by such higher-level concepts. Requirements models are acquired as instances of a conceptual meta-model. The latter can be represented as a graph where each node captures an abstraction such as, e.g., goal, action, agent, entity, or event, and where the edges capture semantic links between such abstractions. Well-formedness properties on nodes and links constrain their instances - that is, elements of requirements models. Requirements acquisition processes then correspond to particular ways of traversing the meta-model graph to acquire approp...
444|Statecharts: A Visual Formalism For Complex Systems|We present a broad extension of the conventional formalism of state machines and state diagrams, that is relevant to the specification and design of complex discrete-event systems, such as multi-computer real-time systems, communication protocols and digital control units. Our diagrams, which we call statecharts, extend conventional state-transition diagrams with essentially three olements, dealing, respectively, with the notions of hierarchy, concurrency and communication. These transform the language of state diagrams into a highly structured&#039; and economical description language. Statecharts are thus compact and expressive--small diagrams can express complex behavior--as well as compositional and modular. When coupled with the capabilities of computerized graphics, statecharts enable viewing the description at different levels of detail, and make even very large specifications manageable and comprehensible. In fact, we intend to demonstrate here that statecharts counter many of the objections raised against conventional state diagrams, and thus appear to render specification by diagrams an attractive and plausible approach. Statecharts can be used either as a stand-alone behavioral description or as part of a more general design methodology that deals also with the system&#039;s other aspects, such as functional decomposition and data-flow specification. We also discuss some practical experience that was gained over the last three years in applying the statechart formalism to the specification of a particularly complex system.
445|Imagined Communities|This is a field report of a three-week experience in Japan, centered on art education in their cultural and social contexts. Beginning with this overarching focus, the themes and patterns that structure this report were emergent, rising from the experience. Those supporting themes are: being in Japan and in Mino city (setting a context); the culture of handmade Washi paper; the qualities of the Washi paper festival; craft as a way of teaching, being and learning; children and their art at school and through the festival, and the importance of ritual. This report is written in a personal narrative style as suggested in contemporary feminist and transactive ethnographic literature. Key Words:cross-cultural art education, feminist, transactive ethnography, Japanese art education Report from Japan: Art,
446|Computational Approaches to Analogical Reasoning: A Comparative Analysis|Analogical reasoning has a long history in artificial intelligence research, primarily because of its promise for Ike acquisition unit effective use of knowledge. Defined as a representational mapping from a known &amp;quot;source &amp;quot; domain into a novel &#034;target&#034; domain, analogy provides a basic mechanism for effectively connecting a reasoner&#039;s past and present experience. Using a four-component process model of analogical reasoning, this paper reviews sixteen computational studies of analogy. These studies are organized chronologically within broadly defined task domains of automated deduction, problem solving and planning, natural language comprehension, and machine learning. Drawing on these detailed reviews, a comparative analysis of diverse contributions to basic analogy processes identifies recurrent problems for studies of analogy and common approaches to their solution. The paper concludes by arguing that computational studies of analogy are in a slate of adolescence: looking to more mature research areas in artificial intelligence for robust accounts of basic reasoning processes and drawing upon a long tradition of research in other disciplines.  
447|Multi-party Specification|This paper examines a formal model of how specifications can be constructed from multiple viewpoints and presents some tools to support this approach. The development of specifications is presented as a dialogue in which the viewpoints negotiate, establish responsibilities and cooperatively construct a specification. The model is illustrated by means of some small examples. Keywords: formal specification, distributed artificial intelligence, dialogue, logic, tool support 1 Introduction &#034;Specification-in-the-large&#034;, that is the development of requirements specifications for systems of substantial complexity and scale, mirrors &#034;programming-in-thelarge &#034; in raising a variety of difficulties that lie beyond the clerical problems of handling large amounts of information (Cunningham, Finkelstein et al 1985, Finkelstein &amp; Potts 1987). One such difficulty is that of specification from multiple viewpoints (Niskier 1987). Specification-in-the-large is an activity in which there are many particip...
448|Metal: A formalism to specify formalisms|Mentor is a general system for the manipulation of structured information. Its main application is the construction of interactive programming environments. In such an environment, a programmer may design, implement, document, debug, test, analyze, validate, maintain and transport his programs. An environment
449|Centering: A Framework for Modeling the Local Coherence Of Discourse|This paper concerns relationships among focus of attention, choice of referring expression, and perceived coherence of utterances within a discourse segment. It presents a framework and initial theory of centering intended to model the local component of attentional state. The paper  examines interactions between local coherence and choice of referring expressions; it argues that  differences in coherence correspond in part to the inference demands made by different types of referring expressions, given a particular attentional state. It demonstrates that the attentional state properties modeled by centering can account for these differences
450|ATTENTION,  INTENTIONS,  AND THE STRUCTURE OF DISCOURSE|In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse. In this theory, discourse structure is composed of three separate but interre-lated components: the structure of the sequence of utterances (called the linguistic structure), a struc-ture of purposes (called the intentional structure), and the state of focus of attention (called the attentional state). The linguistic structure consists of segments of the discourse into which the utter-ances naturally aggregate. The intentional structure captures the discourse-relevant purposes, expressed in each of the linguistic segments as well as relationships among them. The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds. The attentional state, being dynamic, records the objects, properties, and relations that are salient at each point of the discourse. The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases, referring expressions, and interruptions. The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses. Various properties of discourse are described, and explanations for the behavior of cue phrases, referring expressions, and interruptions are explored. This theory provides a framework for describing the processing of utterances in a discourse. Discourse processing requires recognizing how the utterances of the discourse aggregate into segments, recognizing the intentions expressed in the discourse and the relationships among intentions, and track-ing the discourse through the operation of the mechanisms associated with attentional state. This processing description specifies in these recognition tasks the role of information from the discourse and from the participants &#039; knowledge of the domain. 1
452|Mixed Initiative in Dialogue: An Investigation into Discourse Segmentation|Conversation between two people is usually of MIXED-INITIATIVE, with CONTROL over the conversation being transferred from one person to another. We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns. The application of the control rules lets us derive domain-independent discourse structures. The derived structures indicate that initiative plays a role in the structuring of discourse. In order to explore the relationship of control and initiative to discourse processes like centering, we analyze the distribution of four different classes of anaphora for two data sets. This distribution indicates that some control segments are hierarchically related to others. The analysis suggests that discourse participants often mutually agree to a change of topic. We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types. These differences can be explained in terms of collaborative planning principles. 
453|Some Intonational Characteristics Of Discourse Structure|This paper reports on a study of the relationship between acoustic-prosodic variation and discourse structure, as determined from an independent model of discourse. We present results of two pilot studies. Our corpus consisted of three AP news stories recorded by a professional speaker. Discourse structure was labeled by subjects either from text alone or from text (with all orthographic markings except sentence-final punctuation removed) and speech, following Grosz &amp; Sidner 1986; average inter-labeler agreement for structural elements varied from 74.3%-95.1%, depending upon feature. These elements of global structure, together with elements of local structure such as parentheticals and attributive tags, were correlated with variation in intonational and acoustic features such as pitch range, contour, timing, and amplitude. We found statistically significant associations between aspects of pitch range, amplitude, and timing with features of global and local structure both for labelings...
454|Collaborative plans for group activities|The original formulation of SharedPlans [Grosz and Sidner, 1990] was developed to provide a model of collaborative planning in which it was not necessary for one agent to have intentions toward an act of a different agent. This formulation provided for two agents to coordinate their activities without introducing any notion of jointly held intentions (or, &#039;weintentions&#039;). However, it only treated activities that directly decomposed into single agents actions. In this paper we provide a revised and expanded version of SharedPlans that accommodates actions involving groups of agents as well as complex actions that decompose into multi-agent actions. The new definitions also allow for contracting out certain actions, and provide a model with the features required in Bratman&#039;s account of shared cooperative activity [Bratman, 1992]. A reformulation of the model of individual plans that meshes with the definition of SharedPlans is also provided. 1
455|Japanese Discourse and the Process of Centering|This paper has three aims: (1) to generalize a computational account of the discourse process called CENTERING, (2) to apply this account to discourse processing in Japanese so that it can be used in computational systems for machine translation or language understanding, and (3) to provide some insights on the effect of syntactic factors in Japanese on discourse interpretation. We argue that while discourse interpretation is an inferential process, syntactic cues constrain this process, and demonstrate this argument with respect to the interpretation of ZEROS, unexpressed arguments of the verb, in Japanese. The syntactic cues in Japanese discourse that we investigate are the morphological markers for grammatical TOPIC, the postposition wa, as well as those for grammatical functions such as SUBJECT, ga, OBJECT, o and OBJECT2, ni. In addition, we investigate the role of speaker&#039;s EMPATHY, which is the viewpoint from which an event is described. This is syntactically indicated through the use of verbal compounding, i.e. the auxiliary use of verbs such as kureta, kita. Our results are based on a survey of native speakers of their interpretation of short discourses, consisting of minimal pairs, varied by one of the above factors. We demonstrate that these syntactic cues do indeed affect the interpretation of ZEROS, but that having previously been the TOPIC and being realized as a ZERO also contributes to the salience of a discourse entity. We propose a discourse rule of ZERO TOPIC ASSIGNMENT, and show that CENTERING provides constraints on when a ZERO can be interpreted as the ZERO TOPIC
456|CONTROL OF INFERENCE: ROLE OF SOME ASPECTS OF DISCOURSE STRUCTURE- CENTERING|The purpose of this communication is to examine one particular aspect of discourse structure, namely, a discourse construct called center of a sentence (utterance) in discourse and its relation to the larger issue of control of inference. We have described very briefly the notion of center(s) of a sentence in discourse and discussed how the centering phenomenon might be incorporated in a formal model of inference and its relation to the intrinsic complexity of certain inferences. 
457|Evaluating Discourse Processing Algorithms|In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study. We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues. We present the quantitative results of handsimulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general. We illustrate the general difficulties encountered with quantitative evaluation. These are prob- lems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining.
458|Temporal Centering|We present a semantic and pragmatic account of the anaphoric properties of past and perfect that improves on previous work by integrating discourse structure, aspectual type, surface structure and commonsense knowledge. A novel aspect of our account is that we distinguish between two kinds of temporal intervals in the interpretation of temporal operators -- discourse reference intervals and event intervals. This distinction makes it possible to develop an analogy between centering and temporal centering, which operates on discourse reference intervals. Our temporal property-sharing principle is a defeasible inference rule on the logical form. Along with lexical and causal reasoning, it plays a role in incrementally resolving underspecified aspects of the event structure representation of an utterance against the current context.
459|Centering in Japanese Discourse|In this paper we propose a computational treatment of the resolution of zero pronouns in Japanese discourse, using an adaptation of the centering algorithm. We are able to factor language-specific dependencies into one parameter of the centering algorithm. Previous analyses have stipulated that a zero pronoun and its cospecifier must share a grammatical function property such as Subject or NonSubject.  We show that this property-sharing stipulation is unneeded. In addition we propose the notion of topic ambiguity within the centering framework, which predicts some ambiguities that occur in Japanese discourse. This analysis has implications for the design of language-independent discourse modules for Natural Language systems. The centering algorithm has been implemented in an HPSG Natural Language system with both English and Japanese grammars. 1 Introduction  Japanese is a language well-known for grammaticization of discourse function. It is rich with ways for speakers to indicate the ...
460|Pragmatic Aspects of Scrambling and Topicalization in German: A Centering Approach|This paper will attempt to elucidate the pragmatic conditions on these movement types. It is structured as follows: the next two sections discuss pragmatic constraints on topicalization and scrambling, respectively. A prediction is discussed in Section 4. 2 Scrambling
461|Centering theory and the italian pronominal system|In this paper, I give an account, in terms of centering theory [GJW86], of some phenomena of pronominalization in Italian, in particular the use of the null or the overt pronoun in subject position. After a general introduction to the Italian pronominal system, I will review centering, and then show how the original rules given in [GJW86] have to be extended or modified. Finally, I will show that centering does not account for two phenomena: first, the functional role of an utterance may override the predictions of centering; second, a null subject can be used to refer to a whole discourse segment. This latter phenomenon should ideally be explained in the same terms that the other phenomena involving null subject are. 1 The Italian pronominal system In Italian, there are two pronominM systems, characterized by a different syntactic distribution: weak pronouns, that must always be cliffcized to the verb (e.g.]a,]o, li, le- respectively her, accusative; him, accusative; them, masculine, accusative; them, feminine, accusative or her, dative), and strong pronouns (lui, lei, Ioro-respectively he or him; she or her; they or them). The null subject can be considered as belonging to the system of weak pronouns. Notice that in Italian there is no neuter gender: nouns referring to inanimate objects are masculine or feminine. The weak pronouns used in this case are those of the corresponding gender, while, when a strong pronoun has to be used, paraphrase or deictics *This research was supported by DARPA grant no. N0014-85--K0018. are preferred. A strong pronoun for inanimate objects does exist- esso for masculine, essa for feminine, but it is not much used in current Italian. Weak and strong pronouns are often in complementary distribution, as the following example shows- the contrast is between the use of the null or overt pronoun in subject position 1: Ex. 1 a) b)
462|The Effect Of Establishing Coherence In Ellipsis And Anaphora Resolution|This paper presents a new model of anaphoric processing that utilizes the establishment of coherence relations between clauses in a discourse. We survey data that comprises a currently stalemated argument over whether VP-ellipsis is an inherently syntactic or inherently semantic phenomenon, and show that the data can be handled within a uniform discourse processing architecture. This architecture, which revises the dichotomy between ellipsis  vs. Model Interpretive Anaphora given by Sag and Hankamer (1984) , is also able to accommodate divergent theories and data for pronominal reference resolution. The resulting architecture serves as a baseline system for modeling the role of cohesive devices in natural language.  1 Introduction  There has been much debate concerning the appropriate level of language processing at which to treat VP-ellipsis resolution. Syntactic accounts (Fiengo and May, 1990; Haik, 1987; Hellan, 1988; Hestvik, 1993; Lappin, 1993; Lappin and McCord, 1990) claim that ...
463|Subject-Prodrop in Yiddish|this paper, I shall present a corpus-based analysis of Subject-Prodrop in Yiddish, a language in which Subject-Prodrop has not yet been analyzed, and I shall show that, at least in this corpus of this language, the term &#039;Subject-Prodrop&#039; is a rubric covering phenomena that have diverse syntactic and discourse constraints. In what follows, I shall first describe the corpus and present the syntactic constraints found. Then I shall present the results of a discourse analysis of Subject-Prodrop in Yiddish with respect to Centering Theory. Finally, I shall discuss the implications of these findings. 1. Yiddish Subject-Prodrop: the facts.
464|Some facts about centers, indexicals and demonstratives|ABSTRACT 1 Certain pronoun contexts are argued to establish a local center (LC), i.e., a conventionalized indexical similar to lst/2nd pers. pronouns. Demonstrative pronouns, also indexicals, are shown to access entities that are not LCs because they lack discourse relevance or because they are not yet in the universe of discourse. 1
465|A Model of Investor Sentiment|Recent empirical research in finance has uncovered two families of pervasive regularities: underreaction of stock prices to news such as earnings announcements, and overreaction of stock prices to a series of good or bad news. In this paper, we present a parsimonious model of investor sentiment, or of how investors form beliefs, which is consistent with the empirical findings. The model is based on psychological evidence and produces both underreaction and overreaction for a wide range of parameter values. ? 1998 Elsevier Science S.A. All rights reserved. JEL classification: G12; G14
466|The cross-section of expected stock returns|Your use of the JSTOR archive indicates your acceptance of JSTOR &#039; s Terms and Conditions of Use, available at
470|Market Efficiency, Long-Term Returns, and Behavioral Finance|Market efficiency survives the challenge from the literature on long-term return anomalies. Consistent with the market efficiency hypothesis that the anomalies are chance results, apparent overreaction to information is about as common as underreaction, and post-event continuation of pre-event abnormal returns is about as frequent as post-event reversal. Most important, consistent with the market efficiency prediction that apparent anomalies can be due to methodology, most long-term return anomalies tend to disappear with reasonable changes in technique.  
472|Market underreaction to open market share repurchases|We examine long-run firm performance following open market share repurchase announcements, 1980-1990. We find that the average abnormal four-year buy-and-hold return measured after the initial announcement is 12.1%. For ‘value ’ stocks, companies more likely to be repurchasing shares because of undervaluation, the average abnormal return is 45.3%. For repurchases announced by ‘glamour ’ stocks, where undervaluation is less likely to be an important motive, no positive drift in abnormal returns is observed. Thus, at least with respect to value stocks, the market errs in its initial response and appears to ignore much of the information conveyed through repurchase announcements.
474|Momentum strategies|We examine whether the predictability of future returns from past returns is due to the market&#039;s underreaction to information, in particular to past earnings news. Past return and past earnings surprise each predict large drifts in future returns after controlling for the other. Market risk, size, and book-to-market effects do not explain the drifts. There is little evidence of subsequent reversals in the returns of stocks with high price and earnings momentum. Security analysts &#039; earnings forecasts also respond sluggishly to past news, especially in the case of stocks with the worst past performance. The results suggest a market that responds only gradually to new information. AN EXTENSIVE BODY OF RECENT finance literature documents that the crosssection of stock returns is predictable based on past returns. For example, DeBondt and Thaler (1985, 1987)report that long-term past losers outperform long-term past winners over the subsequent three to five years. Jegadeesh (1990) and Lehmann (1990) find short-term return reversals. Jegadeesh and
476|Evidence that stock prices do not fully reflect the implications of current earnings for future earnings|Evidence presented here is consistent with a failure of stock prices to reflect fully the implications of current earnings for future earnings. Specifically, the three-day price reactions to announcements of earnings for quarters t + 1 through I + 4 are predictable, based on earnings of quarter r. Even more surprisingly, the signs and magnitudes of the three-day reactions are related to the autocorrelation structure of earnings, as if stock prices fail to reflect the extent to which each firm’s earnings series differs from a seasonal random walk. 1.
477|Value versus growth: The international evidence|Value stocks have higher returns than growth stocks in markets around the world. For the period 1975 through 1995, the difference between the average returns on global portfolios of high and low book-to-market stocks is 7.68 percent per year, and value stocks outperform growth stocks in twelve of thirteen major markets. An international capital asset pricing model cannot explain the value premium, but a two-factor model that includes a risk factor for relative distress captures the value premium in international returns. 
478|The weighting of evidence and the determinants of confidence|The pattern of overconfidence and underconfidence observed in studies of in-tuitive judgment is explained by the hypothesis that people focus on the strength or extremeness of the available evidence (e.g., the warmth of a letter or the size of an effect) with insufficient regard for its weight or credence (e.g., the credibility of the writer or the size of the sample). This mode of judgment yields overconfi-dence when strength is high and weight is low, and underconfidence when strength is low and weight is high. We first demonstrate this phenomenon in a chance setup where strength is defined by sample proportion and weight is defined by sample size, and then extend the analysis to more complex evidential prob-lems, including general knowledge questions and predicting the behavior of self and of others. We propose that people’s confidence is determined by the balance of arguments for and against the competing hypotheses, with insufficient regard for the weight of the evidence. We show that this account can explain the effect of item difficulty on overconfidence, and we relate the observed discrepancy between confidence judgments and frequency estimates to the illusion of validity.
479|Fads, martingales, and market efficiency|for helpful coments. They share no responsibiTfty for any remaining errors.
480|An introduction to hidden Markov models|The basic theory of Markov chains has been known to
481|Dynamic topic models|Scientists need new tools to explore and browse large collections of scholarly literature. Thanks to organizations such as JSTOR, which scan and index the original bound archives of many journals, modern scientists can search digital libraries spanning hundreds of years. A scientist, suddenly
482|Latent dirichlet allocation|We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model. 1.
483|A bayesian hierarchical model for learning natural scene categories|We propose a novel approach to learn and recognize natural scene categories. Unlike previous work [9, 17], it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a “theme”. In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes. 1.
484|The Author-Topic Model for Authors and Documents |We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, &amp; Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics
that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact
inference is intractable for these datasets and
we use Gibbs sampling to estimate the topic
and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model)
and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications
to computing similarity between authors and
entropy of author output.
485|Sparse Gaussian processes using pseudo-inputs|We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M « N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M 2 N) training cost and O(M 2) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime. 1
486|Discovering object categories in image collections|Given a set of images containing multiple object categories, we seek to discover those categories and their image locations without supervision. We achieve this using generative models from the statistical text literature: probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA). In text analysis these are used to discover topics in a corpus using the bag-of-words document representation. Here we discover topics as object categories, so that an image containing instances of several categories is modelled as a mixture of topics. The models are applied to images by using a visual analogue of a word, formed by vector quantizing SIFT like region descriptors. We investigate a set of increasingly demanding scenarios, starting with image sets containing only two object categories through to sets containing multiple categories (including airplanes, cars, faces, motorbikes, spotted cats) and background clutter. The object categories sample both intra-class and scale variation, and both the categories and their approximate spatial layout are found without supervision. We also demonstrate classification of unseen images and images containing multiple objects. Performance of the proposed unsupervised method is compared to the semi-supervised approach of [7].  
487|Integrating topics and syntax|Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words. We present a generative model that uses both kinds of dependencies, and can be used to simultaneously find syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency. This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively. 1
488|A Generalized Mean Field Algorithm for Variational Inference in Exponential Families|We present a class of generalized mean field (GMF) algorithms for approximate inference in exponential family graphical models which is analogous to the generalized belief propagation (GBP) or cluster variational methods. While those methods are based on...
489|Collaborative Filtering: A Machine Learning Perspective|Collaborative filtering was initially proposed as a framework for filtering information based on the preferences of users, and has since been refined in many different ways. This thesis is a comprehensive study of rating-based, pure, non-sequential collaborative filtering. We analyze existing methods for the task of rating prediction from a machine learning perspective. We show that many existing methods proposed for this task are simple applications or modi  cations of one or more standard machine learning methods for classifi cation, regression, clustering, dimensionality reduction, and density estimation. We introduce new prediction methods in all of these classes. We introduce a new experimental procedure for testing stronger forms of generalization than has been used previously. We implement a total of nine prediction methods, and conduct large scale prediction accuracy experiments. We show interesting new results on the relative performance of these methods.
490|Applying Discrete PCA in Data Analysis|Methods for analysis of principal components in discrete data have existed for some time under various names such as grade of membership modelling, probabilistic latent semantic analysis, and genotype inference with admixture. In this paper we explore a number of extensions to the common theory, and present some application of these methods to some common statistical tasks. We show that these methods can be interpreted as a discrete version of ICA. We develop a hierarchical version yielding components at different levels of detail, and additional techniques for Gibbs sampling. We compare the algorithms on a text prediction task using support vector machines, and to information retrieval.  
491|The authorrecipienttopic model for topic and role discovery in social networks: Experiments with Enron and academic email|Previous work in social network analysis (SNA) has modeled the existence of links from one entity to another, but not the language content or topics on those links. We present the Author-Recipient-Topic (ART) model for social network analysis, which learns topic distributions based on the the directionsensitive messages sent between entities. The model builds on Latent Dirichlet Allocation and the Author-Topic (AT) model, adding the key attribute that distribution over topics is conditioned distinctly on both the sender and recipient—steering the discovery of topics according to the relationships between people. We give results on both the Enron email corpus and a researcher’s email archive, providing evidence not only that clearly relevant topics are discovered, but that the ART model better predicts people’s roles. 1
493|Simplicial mixtures of Markov chains: Distributed modelling of dynamic user profiles|To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the definition of individual specific and global models. This paper proposes a lineartime distributed model for finite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be ‘explained ’ by a relatively small number of structurally simple common behavioral patterns which may interleave randomly in a user-specific proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efficient representation scheme, reflected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations. 1
494|An Overview of Workflow Management: From Process Modeling to Workflow Automation Infrastructure|  Today’s business enterprises must deal with global competition, reduce the cost of doing business, and rapidly develop new services and products. To address these requirements enterprises must constantly reconsider and optimize the way they do business and change their information systems and applications to support evolving business processes. Workflow technology facilitates these by providing methodologies and software to support (i) business process modeling to capture business processes as workflow specifications, (ii) business process reengineering to optimize specified processes, and (iii) workflow automation to generate workflow implementations from workflow specifications. This paper provides a high-level overview of the current workflow management methodologies and software products. In addition, we discuss the infrastructure technologies that can address the limitations of current commercial workflow technology and extend the scope and mission of workflow management systems to support increased workflow automation in complex real-world environments involving heterogeneous, autonomous, and distributed information systems. In particular, we discuss how distributed object management and customized transaction management can support further advances in the commercial state of the art in this area.
495|The Action Workflow approach to workflow management technology|This paper describes ActionWorkflowTM approach to workflow management technology: a design methodology and associated computer software for the support of work in organizations. The approach is based on theories of communicative activity as language faction and has been developed in a series of systems for coordination among users of networked computers. This paper describes the approach, gives an example of its application, and shows the architecture of a workflow management system based on it.
496|Managing Heterogeneous Multi-system Tasks to Support Enterprise-wide Operations|. The computing environment in most medium-sized and large enterprises involves old main-frame based (legacy) applications and systems as well as new workstation-based distributed computing systems. The objective of the METEOR project is to support multi-system workflow applications that automate enterprise operations. This paper deals with the modeling and specification of workflows in such applications. Tasks in our heterogeneous environment can be submitted through different types of interfaces on different processing entities. We first present a computational model for workflows that captures the behavior of both transactional and nontransactional tasks of different types. We then develop two languages for specifying a workflow at different levels of abstraction: the Workflow Specification Language (WFSL) is a declarative rulebased language used to express the application-level interactions between multiple tasks, while the Task Specification Language (TSL) focuses on the issues re...
497|Specification and Execution of Transactional Workflows|The basic transaction model has evolved over time to incorporate more complex transaction structures and to selectively modify the atomicity and isolation properties. In this chapter we discuss the application of transaction concepts to activities that involve coordinated execution of multiple tasks (possibly of different types) over different processing entities. Such applications are referred to as transactional workflows. In this chapter we discuss the specification of such workflows and the issues involved in their execution.  
498|Merging Application-centric and Data-centric Approaches to Support Transaction-oriented Multi-system Workflows|Workflow management is primarily concerned with dependencies between the tasks of a workflow, to ensure correct control flow and data flow. Transaction management, on the other hand, is concerned with preserving data dependencies by preventing execution of conflicting operations from multiple, concurrently executing tasks or transactions. In this paper we argue that many applications will be served better if the properties of transaction and workflow models are supported by an integrated architecture. We also present preliminary ideas towards such an architecture. 
499|Business process management with FlowMark|From an enterprise point of view the management of business processes is becoming increasingly important: Business processes control which piece of work will be performed by whom and which resources are exploited for this work, i.e. a business process describes how an enterprise will achieve its business goals. In this paper we sketch FlowMark (FlowMark is a trademark of IBM), an IBM program product supporting both, the modeling of business processes and their execution.
500|Using flexible transactions to support multi-system telecommunication applications|Service order provisioning is an important telecommunication application that automates the process of providing telephone services in response to the customer requests. It is an example of a multi-system application that requires access to multiple, independently developed application systems and their databases. In this paper, we describe the design and implementation of a prototype system 1 that supports the execution of the Flexible Transactions and its use to develop the service order provisioning application. We argue that such approach may be used to support the development of multi-system, flow-through processing applications in a systematic and organized manner. Its advantages include fast and easy specification of new services, support for testing of the declaratively specified work-flows, and the specification of potential concurrency among the tasks constituting an application.
501|Distributed Object Management|Future information processing environments will consist of a vast network of heterogeneous, autonomous, and distributed computing resources, including computers (from mainframe to personal), information-intensive applications, and data (files and databases). A key challenge in this environment is providing capabilities for combining this varied collection of resources into an integrated distributed system, allowing resources to be flexibly combined, and their activities coordinated, to address challenging new information processing requirements. In this paper, we describe the concept of distributed object management, and identify its role in the development of these open, interoperable systems. We identify the key aspects of system architectures supporting distributed object management, and describe specific elements of a distributed object management system being developed at GTE Laboratories. 1. Introduction Today, computer usage is expanding into all parts, and all functions, of lar...
502|A Framework For Enforceable Specification Of Extended Transaction Models And Transactional Workflows|A variety of extensions to the traditional (ACID) transaction model have resulted in a plethora of  extended transaction models (ETMs). Many of these ETMs are application-specific, i.e., they are  designed to provide correctness guarantees adequate for a particular application, but not others. Similarly,  an application-specific ETM may impose restrictions that are unacceptable in one application,  yet required in another. To define new ETMs, to determine whether an ETM is appropriate for an  application, and to integrate ETMs to produce new ETMs, we need a framework for ETM specification  and reasoning. In this paper, we describe such a framework. Our framework supports implementation-independent specification of ETMs described in terms of dependencies between transactions.  Dependencies are specified using dependency descriptors. Unlike other transaction specification  frameworks, dependency descriptors use a common set of primitives, and are enforceable, i.e., can be  evaluated at...
503|Chronological Scheduling of Transactions with Temporal Dependencies|Database applications often impose temporal dependencies between transactions that must be satisfied to preserve data consistency. The extant correctness criteria used to schedule the execution of concurrent transactions are either time independent or use strict, difficult to satisfy real-time constraints. On one end of the spectrum, serializability completely ignores time. On the other end, deadline scheduling approaches consider the outcome of each transaction execution correct only if the transaction meets its real-time deadline. In this article, we explore new correctness criteria and scheduling methods that capture temporal transaction dependencies and belong to the broad area between these two extreme approaches. We introduce the concepts of succession dependency and chronological dependency and define correctness criteria under which temporal dependencies between transactions are preserved even if the dependent transactions execute concurrently. We also propose a chronological scheduler that can guarantee that transaction executions satisfy their chronological constraints. The advantages of chronological scheduling over traditional scheduling methods, as well as the main issues in the implementation and performance of the proposed scheduler, are discussed.
504|New empirical relationships among magnitude, rupture length, rupture width, rupture area, and surface|Abstract Source parameters for historical earthquakes worldwide are compiled to develop a series of empirical relationships among moment magnitude (M), surface rupture length, subsurface rupture length, downdip rupture width, rupture area, and maximum and average displacement per event. The resulting data base is a significant update of previous compilations and includes the additional source parameters of seismic moment, moment magnitude, subsurface rupture length, downdip rupture width, and average surface displacement. Each source parameter is classified as reliable or unreliable, based on our evaluation of the accuracy of individual values. Only the reliable source parameters are used in the final analyses. In comparing source parameters, we note the following trends: (1) Generally, the length of rupture at the surface is equal to 75% of the subsurface rupture length; however, the ratio of surface rupture length to subsurface rupture length increases with magnitude; (2) the average surface displacement per event is about one-half the maximum surface displacement per event; and (3) the average subsurface displacement on the fault plane is less
505|A moment-magnitude scale|ABSTRACT-The equating of scores on alternate forms of differdnt achievement tests through the use of the three-parameter latent trait model, item-rbsponse theory (IRT) equating, was compared with the results of score equatings basedon conventional linear and curvilinear equating models. Ten equatings&#039;were completed for pairs: of alternate,forms of the Advanced Placement Program, whiCh measures different content areas and traits in each subject area. It was found that deipite thsapArent violation of the, unidimentionality. assumption, the/Nquating results obtainedithrough the IRT equating&#039;&#039; modest wake found to be in agreement with those of the conventional equating models. By demonstrating that the IRT equating results parallel, those of the simpler, less costly;°.conventional methods, it has Been&#039;&#039;shown.that it ip still possible, to equate scores on non-parallel tests under Condiqons which &#039; make:conventional equating inapplicable. (Author/PN) *******************************11******4****4*************************.** * Reproductions iuppliedbx;EDRS are the-best that can be made.* *- from the original document.. * *****************************************************,***********4****&#034;**
506|Theoretical basis of some empirical relations in seismology|Empirical relations involving seismic moment Mo,  magnitude Ms,  energy Es and fault dimension L (or area S) are discussed on the basis of an extensive set of earthquake data (M s&gt; = 6) and simple crack and dynamic dislocation models. The relation between log S and log M o is remarkably linear (slope ~ 2/3) indicating a constant stress drop Aa; Atr = 30, 100 and 60 bars are obtained for inter-plate, intra-plate and &#034;average &#034; earthquakes, respectively. Except for very large earthquakes, the relation M s ~ (2/3) log M o ~ 2 log L is established by the data. This is consistent with the dynamic dislocation model for point dis-location rise times and rupture times of most earthquakes. For very large earth-quakes M s ~ (1/3) log M o,, ~ log L ~ (1/3) log E s. For very small earthquakes M s ~ log M o, ~ 3 log L ~ log E s. Scaling rules are assumed and justified. This model predicts log E s ~ 1.5 M s,- ~ 3 log L which is consistent with the Gutenberg-Richter elation. Since the static energy is proportional to 0L 3, where ~ is the average stress, this relation suggests a constant apparent stress ~/¢i where r / is the efficiency. The earthquake data suggest r/0 ~   Atr. These relations lead to log S,, ~ M s consistent with the empirical relation. This relation together with a simple geometrical argument explains the magnitude-frequency relation log N N- Ms.
507|Scaling laws for large earthquakes: consequences for physical models|It is observed that the mean slip in large earthquakes is linearly proportional to fault length and does not correlate with fault width. This observation is interpreted in the light of the two possible classes of models for large earth-quakes: W models, in which stress drop and slip are determined by fault width, and L models, in which these parameters are fundamentally determined by fault length. In the W model interpretation, stress drop systematically increases with L/W, the aspect ratio, and, as a consequence, seismic moment. The correlation of slip with length means that the rupture length is determined by the dynamic stress drop. This conflicts with the observation that the length of large earth-quakes is often controlled by adjacent rupture zones of previous earthquakes or by tectonic obstacles. It also conflicts with the observations for small earth-quakes that stress drop is nearly constant and does not correlate with source radius over a broad range. In the L model interpretation, the correlation between slip and length means that stress drop is constant, namely about 7.5, 12, and 60 bars for interplate strike-slip, thrust, and Japanese intraplate earthquakes, respectively. L models require that the fault be mechanically unconstrained at the base. W models predict that mean particle velocity increases with fault length, but rise time is constant. / _ models predict the opposite.
508|Earthquakes, Quaternary faults, and seismic hazard in California|Data describing the locations, slip rates, and lengths of Quaternary faults are the primary basis in this work for constructing maps that characterize seismic hazard in California. The expected seismic moment Mo e and the strength of ground shaking resulting from the entire rupture of each mapped fault (or fault segment) are estimated using empirical relations between seismic moment Mo, rupture length, source to site distance, and strong ground motion•s. Assuming a fault model. whereby the repeat time T of earthquakes on each fault equals Moe/M•o (where the moment rate Mo gis propor-tional to fault slip rate), it is observed that the moment-frequency distribution of earthquakes predicted from the geologic data agrees well with the distribution determined from a 150-year histori-cal record. The agreement is consistent with the argument that the geologic record of Quaternary fault offsets contains information sufficient to predict the average spatial and size distribution of earth-quakes through time in California. The estimates of r for each fault are the foundation for con-structing maps that depict he average return period of&gt; • 0. lg peak horizontal ground accelerations, and the horizontal components of peak acceleration, peak velocity, and the pseudovelocity response (at 1-period and 5 % damping) expected to occur at the level of 0.1 probability during a 50-year period of time. A map is also formulated to show the probability that&gt; • 0. lg horizontal ground accelera-tions will occur during the next 50 years. The maps serve to illustrate the potential value of Quater-nary fault studies for assessing seismic hazard. Interpretation of available slip rates indicates that the largest and most frequent occurrence of potentially destructive strong ground motions are associated
509|Minimum Earthquake Magnitude Associated with |Rupture of the ground surface by faulting associated with shallow earthquakes is an important element to consider in the evaluation of fault activity. Observational data compiled in this report indicate that the minimum earthquake magnitude associa ted with reported sudden surface faulting is about M, 5. Considering that the epicentral areas of many earthquakes of M, 5 or less were not searched for evidence of surface faulting, the actual minimum magnitude may be smaller. A combined empirical and theoretical analysis suggests that under ideal conditions, coseismic surface faulting of a few millimeters associated with earthquakes having moment magnitudes as small as 3 could be recognized by simple field methods. Several factors such as dimensions, depth, and orientation of the rupture surface together with observational conditions affect the development and subsequent recognition of surface faulting. Surface displacements ranging from a few millimeters to several decimeters have accompanied earthquakes having magnitudes between 5 and 6. The larger fault displacements and the earthquakes can damage structures, and that possibility should be considered in regions where shallow earthquakes of that size can occur. The generally small and short surface ruptures associated with such earthquakes may leave very little evidence in the topography, stratigraphy, or near-surface structure, especially if the displacements are consistently small and the recurrence intervals for earthquakes are long. Such conditions may explain why so few active faults have been recognized in some regions of infrequent shallow earthquakes, such as eastern North America.
510|A Survey of Mobility Models for Ad Hoc Network Research|In the performance evaluation of a protocol for an ad hoc network, the protocol should be tested under realistic conditions including, but not limited to, a sensible transmission range, limited buffer space for the storage of messages, representative data traffic models, and realistic movements of the mobile users (i.e., a mobility model). This paper is a survey of mobility models that are used in the simulations of ad hoc networks. We describe several mobility models that represent mobile nodes whose movements are independent of each other (i.e., entity mobility models) and several mobility models that represent mobile nodes whose movements are dependent on each other (i.e., group mobility models). The goal of this paper is to present a number of mobility models in order to offer researchers more informed choices when they are deciding upon a mobility model to use in their performance evaluations. Lastly, we present simulation results that illustrate the importance of choosing a mobility model in the simulation of an ad hoc network protocol. Specifically, we illustrate how the performance results of an ad hoc network protocol drastically change as a result of changing the mobility model simulated.
511|Dynamic source routing in ad hoc wireless networks|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any established infrastructure or centralized administration. In such an environment, it may be necessary for one mobile host to enlist the aid of other hosts in forwarding a packet to its destination, due to the limited range of each mobile host’s wireless transmissions. This paper presents a protocol for routing in ad hoc networks that uses dynamic source routing. The protocol adapts quickly to routing changes when host movement is frequent, yet requires little or no overhead during periods in which hosts move less frequently. Based on results from a packet-level simulation of mobile hosts operating in an ad hoc network, the protocol performs well over a variety of environmental conditions such as host density and movement rates. For all but the highest rates of host movement simulated, the overhead of the protocol is quite low, falling to just 1 % of total data packets transmitted for moderate movement rates in a network of 24 mobile hosts. In all cases, the difference in length between the routes used and the optimal route lengths is negligible, and in most cases, route lengths are on average within a factor of 1.01 of optimal. 1.
512|Location-Aided Routing (LAR) in mobile ad hoc networks  (1998) |A mobile ad hoc network consists of wireless hosts that may move often. Movement of hosts results in a change in routes, requiring some mechanism for determining new routes. Several routing protocols have already been proposed for ad hoc networks. This paper suggests an approach to utilize location information (for instance, obtained using the global positioning system) to improve performance of routing protocols for ad hoc networks. By using location information, the proposed Location-Aided Routing (LAR) protocols limit the search for a new route to a smaller “request zone” of the ad hoc network. This results in a significant reduction in the number of routing messages. We present two algorithms to determine the request zone, and also suggest potential optimizations to our algorithms.
513|A group mobility model for ad hoc wireless networks|In this paper, we present a survey of various mobility models in both cellular networks and multi-hop networks. We show that group motion occurs frequently in ad hoc networks, and introduce a novel group mobility model- Reference Point Group Mobility (RPGM)- to represent the relationship among mobile hosts. RPGM can be readily applied to many existing applications. Moreover, by proper choice of parameters, RPGM can be used to model several mobility models which were previously proposed. One of the main themes of this paper is to investigate the impact of the mobility model on the performance of a specific network protocol or application. To this end, we have applied our RPGM model to two different network protocol scenarios, clustering and routing, and have evaluated network performance under different mobility patterns and for different protocol implementations. As expected, the results indicate that different mobility patterns affect the various protocols in different ways. In particular, the ranking of routing algorithms is influenced by the choice of mobility pattern. 1
514|Multicast Operation of the Ad-hoc On-Demand Distance Vector Routing Protocol|An ad-hoc network is the cooperative engagement of a  collection of (typically wireless) mobile nodes without the required intervention of any centralized access point or existing infrastructure. To provide optimal communication ability, a routing protocol for such a dynamic self-starting network must be capable of unicast, broadcast, and multicast. In this paper we extend Ad-hoc On-Demand Distance Vector Routing (AODV), an algorithm for the operation of such ad-hoc networks, to offer novel multicast capabilities which follow naturally from the way AODV establishes unicast routes. AODV builds  multicast trees as needed (i.e., on-demand) to connect multicast group members. Control of the multicast tree is distributed so that there is no single point of failure. AODV provides loop-free routes for both unicast and multicast, even while repairing broken links. We include an evaluation methodology and simulation results to validate the correct and efficient operation of the AODV algorithm.   
515|User mobility modeling and characterization of mobility patterns|Abstract—A mathematical formulation is developed for systematic tracking of the random movement of a mobile station in a cellular environment. It incorporates mobility parameters under the most generalized conditions, so that the model can be tailored to be applicable in most cellular environments. This mobility model is used to characterize different mobility-related traffic parameters in cellular systems. These include the distribution of the cell residence time of both new and handover calls, channel holding time, and the average number of handovers. It is shown that the cell resistance time can be described by the generalized gamma distribution. It is also shown that the negative exponential distribution is a good approximation for describing the channel holding time. Index Terms—Mobile communication. I.
516|An analysis of the optimum node density for ad hoc mobile networks|An ad hoc mobile network is a collection of nodes, each of which communicates over wireless channels and is capable of movement. Wireless nodes have the unique capability of transmission at different power levels. As the transmission power is varied, a tradeoff exists between the number of hops from source to destination and the overall bandwidth available to individual nodes. Because both battery life and channel bandwidth are limited resources in mobile networks, it is important to ascertain the effects different transmission powers have on the overall performance of the network. This paper explores the nature of this transmission power tradeoff in mobile networks to determine the optimum node density for delivering the maximum number of data packets. It is shown that there does not exist a global optimum density, but rather that, to achieve this maximum, the node density should increase as the rate of node movement increases.
517|Mobile Users: To Update or not to Update?|This paper focuses on three natural strategies in which the mobile users make the decisions when and where to update: the time-based strategy, the number of movements-based strategy, and the distance-based strategy. We consider both memoryless movement patterns and movements with Markovian memory along a topology of cells arranged as a ring. We analyze the performance of each one of the three strategies under such movements, and show the performance differences between the strategies.
518|Predictive Distance-Based Mobility Management for PCS Networks|This paper presents a mobile tracking scheme that exploits the predictability of user mobility patterns in wireless PCS networks. Instead of the constant velocity fluid-flow or the random-walk mobility model, a more realistic Gauss-Markov model is introduced, where a mobile&#039;s velocity is correlated in time to a various degree. Based on the Gauss-Markov model, a mobile&#039;s future location is predicted by the network based on the information gathered from the mobile&#039;s last report of location and velocity. When a call is made, the network pages the destination mobile at and around the predicted location of the mobile and in the order of descending probability until the mobile is found. A mobile shares the same prediction information with the network and reports its new location whenever it reaches some threshold distance away from the predicted location. We describe an analytical framework to evaluate the cost of mobility management for the proposed predictive distance-based scheme. We then...
519|A Multicast Routing Protocol for Ad-Hoc Networks|The Core-Assisted Mesh Protocol (CAMP) is introduced for multicast routing in ad-hoc networks. CAMP generalizes the notion of core-based trees introduced for internet multicasting into multicast meshes that have much richer connectivity than trees. A shared multicast mesh is defined for each multicast group; the main goal of using such meshes is to maintain the connectivity of multicast groups even while network routers move frequently. CAMP consists of the maintenance of multicast meshes and loop-free packet forwarding over such meshes. Within the multicast mesh of a group, packets from any source in the group are forwarded along the reverse shortest path to the source, just as in traditional multicast protocols based on source-based trees. CAMP guarantees that, within a finite time, every receiver of a multicast group has a reverse shortest path to each source of the multicast group. Multicast packets for a group are forwarded along the shortest paths from sources to receivers define...
520|Wireless Hierarchical Routing Protocol with Group Mobility (WHIRL)  (1999) |In this paper we address the problem of routing in a large wireless, mobile network such as found in the automated battle field or in extensive disaster recovery operations. Conventional routing does not scale well to network size. Likewise, conventional hierarchical routing cannot handle mobility efficiently. In this paper, we propose a novel soft state Wireless HIerarchical Routing protocoL (WHIRL). We distinguish between the &#034;physical&#034; routing hierarchy (dictated by geographical relationships between nodes) and &#034;logical&#034; hierarchy of subnets in which the members move as a group (e.g., company, brigade, battalion in the battlefield). WHIRL keeps track of logical subnet movements using Home Agent concepts akin to Mobile IP. A group mobility model is introduced and the performance of the WHIRL is evaluated through a detailed wireless simulation model.
521|Source-Tree Routing in Wireless Networks|We present the source-tree adaptive routing (STAR) protocol and analyze its performance in wireless networks with broadcast radio links. Routers in STAR communicate to its neighbors their source routing trees either incrementally or in atomic updates. Source routing trees are specified by stating the link parameters of each link belonging to the paths used to reach every destination. Hence, a router disseminates link-state updates to its neighbors for only those links along paths used to reach destinations. Simulation results show that STAR is an order of magnitude more efficient than any topology-broadcast protocol, and four times more efficient than ALP, which was the most efficient table-driven routing protocol based on partial link-state information reported to date. The results also show that STAR is even more efficient than the Dynamic Source Routing (DSR) protocol, which has been shown to be one of the best performing on-demand routing protocols.
522|Geographic Routing for Wireless Networks|Distributed shortest-path routing protocols for wired networks either describe the entire topology of a network or provide a digest of the topology to every router. They continu-ally update the state describing the topology at all routers as the topology changes to find correct routes for all destinations. Hence, to find routes robustly, they generate routing pro-tocol message traffic proportional to the product of the number of routers in the network and the rate of topological change in the network. Current ad-hoc routing protocols, de-signed specifically for mobile, wireless networks, exhibit similar scaling properties. It is the reliance of these routing protocols on state concerning all links in the network, or all links on a path between a source and destination, that is responsible for their poor scaling. We present Greedy Perimeter Stateless Routing (GPSR), a novel routing protocol for wireless datagram networks that uses the positions of routers and a packet’s destination to make packet forwarding decisions. GPSR makes greedy forwarding decisions using only information about a router’s immediate neighbors in the network topology. When a packet reaches a region where greedy forwarding is impossible, the algorithm recovers by routing around the perimeter of the region. By keeping state only about the local topology, GPSR
523|Evaluating Mobility Models Within An Ad Hoc Network|With current advances in technology, wireless networks are increasing in popularity. Wireless networks allow users the freedom to travel from one location to another without interruption of their computing services. However, wireless networks require the existence of a wired base station (BS) in order for the wireless user to send/receive messages. Ad hoc networks, a subset of wireless networks, allow the formation of a wireless network without the need for a BS. All participating users in an ad hoc network agree to accept and forward messages, to and from each other. With this flexibility, wireless networks have the ability to form anywhere, at any time, as long as two or more wireless users are willing to communicate. In an ad hoc network, the ability to send a message to a group of users, based solely on their geographic location, is desirable. A geocast protocol serves this purpose. Rescue missions, military scenarios, and even advertising schemes benefit from this type of message delivery service. However, before implementation occurs, an ad hoc network protocol such as a geocast protocol must be tested under realistic conditions including, but not limited to, a sensible transmission range, limited buffer for storage of messages, and realistic movements of the wireless users (i.e., a mobility model). The results presented in this thesis focus on several mobility models in an attempt to compare iv the effects that different mobility models have on an ad hoc network protocol. It is obvious that wireless users will travel from one location to another. However, representing their exact movements is not so simple. The results presented in this thesis illustrate the importance in carefully evaluating and implementing multiple mobility models when evaluating an ad hoc net...
524|On-Demand Multicast in Mobile Wireless Networks|In this paper we propose an &#034;on demand&#034; multicast routing protocol for a wireless, mobile, multihop network. The proposed scheme has two key features: (a) it is based on the forwarding group concept (i.e., a subset of nodes is in charge of forwarding the multicast packets via scoped flooding) rather than on the conventional multicast tree scheme; (b) it dynamically refreshes the forward group members using a procedure akin to on demand routing (hence the name). &#034;On Demand&#034; multicast is well suited to operate in an On Demand routing environment where routes are selectively computed as needed between communicating node pairs instead of being maintained and updated globally by a routing &#034;infrastructure&#034; (like in Distance Vector or Link State, for example). On Demand Multicast is particularly attractive in mobile, rapidly changing networks, where the traffic overhead caused by routing updates and tree reconfigurations may become prohibitive beyond a critical speed; and, in large networks w...
525|Wireless Network Multicasting| Wireless networks provide mobile users with ubiquitous communicating capability and information access regardless of location. Conventional ground radio networks are the &#034;last hop&#034; extension of a wireline network, thus supporting only single hop communications within a &#034;cell&#034;. In this dissertation we address a novel type of wireless networks called &#034;multihop&#034; networks. As a difference from &#034;single hop&#034; (i.e., cellular) networks which require fixed base stations interconnected by a wired backbone, multihop networks have no fixed based stations nor a wired backbone. The main application for mobile wireless multihopping is rapid deployment and dynamic reconfiguration. When the wireline network is not available, as in battlefield communications and search and rescue operations, multihop wireless network...
526|Load Reduction in Ad Hoc Networks Using Mobile Servers|In this thesis, we propose two algorithms for network load reduction in ad hoc networks. With the increasing popularity of mobile computing and Internet based client-server applications, such algorithms should become popular.
527|Atmospheric Modeling, Data Assimilation and Predictability|Numerical weather prediction (NWP) now provides major guidance in our daily weather forecast. The accuracy of NWP models has improved steadily since the first successful experiment made by Charney, Fj!rtoft and von Neuman (1950). During the past 50 years, a large number of technical papers and reports have been devoted to NWP, but the number of textbooks dealing with the subject has been very small, the latest being the 1980 book by Haltiner &amp; Williams, which was dedicated to descriptions of the atmospheric dynamics and numerical methods for atmospheric modeling. However, in the intervening years much impressive progress has been made in all aspects of NWP, including the success in model initialization and ensemble forecasts. Eugenia Kalnay’s recent book covers for the first time in the long history of NWP, not only methods for numerical modeling, but also the important related areas of data assimilation and predictability. It incorporates all aspects of environmental computer modeling including an historical overview of NWP, equations of motion and their approximations, a modern description of the methods to determine the initial conditions using weather observations and a clear discussion of chaos in dynamic systems and how these concepts can be
528|Models and issues in data stream systems|In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues. 
529|Randomized Algorithms|Randomized algorithms, once viewed as a tool in computational number theory, have by now found widespread application. Growth has been fueled by the two major benefits of randomization: simplicity and speed. For many applications a randomized algorithm is the fastest algorithm available, or the simplest, or both. A randomized algorithm is an algorithm that uses random numbers to influence the choices it makes in the course of its computation. Thus its behavior (typically quantified as running time or quality of output) varies from
530|The space complexity of approximating the frequency moments|The frequency moments of a sequence containing mi elements of type i, for 1 = i = n, are the numbers Fk = ?n i=1 mki. We consider the space complexity of randomized algorithms that approximate the numbers Fk, when the elements of the sequence are given one by one and cannot be stored. Surprisingly, it turns out that the numbers F0, F1 and F2 can be approximated in logarithmic space, whereas the approximation of Fk for k = 6 requires n?(1) space. Applications to data bases are mentioned as well. 
531|Multiparty Communication Complexity|A given Boolean function has its input distributed among many parties. The aim is to determine which parties to tMk to and what information to exchange with each of them in order to evaluate the function while minimizing the total communication. This paper shows that it is possible to obtain the Boolean answer deterministically with only a polynomial increase in communication with respect to the information lower bound given by the nondeterministic communication complexity of the function.
532|NiagaraCQ: A Scalable Continuous Query System for Internet Databases|Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system’s performance and scalability. 1.
533|Fast subsequence matching in time-series databases|We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an ecient and eective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.  
534|Eddies: Continuously Adaptive Query Processing|In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.  In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry  during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates prom...
535|Mining high-speed data streams|Categories and Subject ?????????? ? ?¨?????????????????????????¦???¦??????????¡¤?? ¡  ? ¡????????????????¦¡¤????§?£????
536|Online Aggregation|Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in postgres. 1 Introduction  Aggregation is an incre...
537|Efficient Filtering of XML Documents for Selective Dissemination of Information|Information Dissemination applications are gaining increasing  popularity due to dramatic improvements in  communications bandwidth and ubiquity. The sheer  volume of data available necessitates the use of selective  approaches to dissemination in order to avoid overwhelming  users with unnecessaryinformation. Existing  mechanisms for selective dissemination typically rely  on simple keyword matching or &#034;bag of words&#034; information  retrieval techniques. The advent of XML as a  standard for information exchangeand the development  of query languages for XML data enables the development  of more sophisticated filtering mechanisms that  take structure information into account. We have developed  several index organizations and search algorithms  for performing efficient filtering of XML documents for  large-scale information dissemination systems. In this  paper we describe these techniques and examine their  performance across a range of document, workload, and  scale scenarios.  1 
538|External Memory Algorithms and Data Structures|Data sets in large applications are often too massive to fit completely inside the computer&#039;s internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this paper, we survey the state of the art in the design and analysis of external memory algorithms and data structures (which are sometimes referred to as &#034;EM&#034; or &#034;I/O&#034; or &#034;out-of-core&#034; algorithms and data structures). EM algorithms and data structures are often designed and analyzed using the parallel disk model (PDM). The three machine-independent measures of performance in PDM are the number of I/O operations, the CPU time, and the amount of disk space. PDM allows for multiple disks (or disk arrays) and parallel CPUs, and it can be generalized to handle tertiary storage and hierarchical memory. We discuss several important paradigms for how to solve batched and online problems efficiently in external memory. Programming tools and environments are available for simplifying the programming task. The TPIE system (Transparent Parallel I/O programming Environment) is both easy to use and efficient in terms of execution speed. We report on some experiments using TPIE in the domain of spatial databases. The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.  
539|Random sampling with a reservoir|We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O(n(1 + log(N/n))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.
540|Stable Distributions, Pseudorandom Generators, Embeddings and Data Stream Computation|In this paper we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular: ffl we show how to maintain (using only O(log n=ffl  2  ) words of storage) a sketch C(p) of a point p 2 l  n  1 under dynamic updates of its coordinates, such that given sketches C(p) and C(q) one can estimate jp \Gamma qj 1 up to a factor of (1 + ffl) with large probability. This solves the main open problem of [10].  ffl we obtain another sketch function C  0  which maps l  n  1 into a normed space l  m  1 (as opposed to C), such that m = m(n) is much smaller than n; to our knowledge this is the first dimensionality reduction lemma for l 1 norm  ffl we give an explicit embedding of l  n  2 into l  n  O(log n) 1  with distortion (1 + 1=n  \Theta(1)  ) and a nonconstructive embedding of l  n  2 into l  O(n)  1 with distortion  (1 + ffl) such that the embedding can be represented using only O(n log  2  n) bits (as opposed to at least...
541|Mining time-changing data streams|Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a station-ary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying pro-cesses generating them changed during this time, sometimes radically. Although a number of algorithms have been pro-posed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes ques-tionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.
542|Continuous Queries over Data Streams|In many recent applications, data may take the form of continuous data streams, rather than finite stored data sets. Several aspects of data management need to be re-considered in the presence of data streams, offering a new research direction for the database community. In this pa-per we focus primarily on the problem of query processing, specifically on how to define and evaluate continuous queries over data streams. We address semantic issues as well as efficiency concerns. Our main contributions are threefold. First, we specify a general and flexible architecture for query processing in the presence of data streams. Second, we use our basic architecture as a tool to clarify alternative semantics and processing techniques for continuous queries. The architecture also captures most previous work on continuous queries and data streams, as
544|Fjording the Stream: An Architecture for Queries over Streaming Sensor Data|If industry visionaries are correct, our lives will soon be full of sensors, connected together in loose conglomerations via wireless networks, each monitoring and collecting data about the environment at large. These sensors behave very differently from traditional database sources: they have intermittent connectivity, are limited by severe power constraints, and typically sample periodically and push immediately, keeping no record of historical information. These limitations make traditional database systems inappropriate for queries over sensors. We present the Fjords architecture for managing multiple queries over many sensors, and show how it can be used to limit sensor resource demands while maintaining high query throughput. We evaluate our architecture using traces from a network of traffic sensors deployed on Interstate 80 near Berkeley and present performance results that show how query throughput, communication costs, and power consumption are necessarily coupled in sensor environments.
545|Maintaining Stream Statistics over Sliding Windows (Extended Abstract)  (2002) |Mayur Datar  Aristides Gionis  y  Piotr Indyk  z  Rajeev Motwani  x  Abstract  We consider the problem of maintaining aggregates and statistics over data streams, with respect to the last N data elements seen so far. We refer to this model as the sliding window model. We consider the following basic problem: Given a stream of bits, maintain a count of the number of 1&#039;s in the last N elements seen from the stream. We show that using O(  1  ffl log  2  N) bits of memory, we can estimate the number of 1&#039;s to within a factor of 1 + ffl. We also give a matching lower bound of \Omega\Gamma  1  ffl log  2  N) memory bits for any deterministic or randomized algorithms. We extend our scheme to maintain the sum of the last N positive integers. We provide matching upper and lower bounds for this more general problem as well. We apply our techniques to obtain efficient algorithms for the Lp norms (for p 2 [1; 2]) of vectors under the sliding window model. Using the algorithm for the basic counting problem, one can adapt many other techniques to work for the sliding window model, with a multiplicative overhead of O(  1  ffl log N) in memory and a 1 + ffl factor loss in accuracy. These include maintaining approximate histograms, hash tables, and statistics or aggregates such as sum and averages.
546|Continuously Adaptive Continuous Queries over Streams|We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive crossquery sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators -- both selections and join state -- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.
547|Multiple-Query Optimization|Some recently proposed extensions to relational database systems, as well as to deductive database systems, require support for multiple-query processing. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions may expand to more than one actual query that has to be run over the database. It is an interesting problem then to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of the paper is a systematic look at the problem, along with the presentation and analysis of algorithms that can be used for multiple-query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably.
548|Trajectory Sampling for Direct Traffic Observation|Traffic measurement is a critical component for the control and engineering of communication networks. We argue that traffic measurement should make it possible to obtain the spatial flow of traffic through the domain, i.e., the paths followed by packets between any ingress and egress point of the domain. Most resource allocation and capacity planning tasks can benefit from such information. Also, traffic measurements should be obtained without a routing model and without knowledge of network state. This allows the traffic measurement process to be resilient to network failures and state uncertainty.  We propose a method that allows the direct inference of traffic flows through a domain by observing the trajectories of a subset of all packets traversing the network. The key advantages of the method are that (i) it does not rely on routing state, (ii) its implementation cost is small, and (iii) the measurement reporting traffic is modest and can be controlled precisely. The key idea of the method is to sample packets based on a hash function computed over the packet content. Using the same hash function will yield the same sample set of packets in the entire domain, and enables us to reconstruct packet trajectories.  I. 
549|  Wavelet-Based Histograms for Selectivity Estimation |Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations. In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data values give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using
550|An Adaptive Query Execution System for Data Integration|Query processing in data integration occurs over networkbound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the  Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila  to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that the Tukwila architecture extends previous innovations in adaptive execution (such as...
551|Surfing wavelets on streams: One-pass summaries for approximate aggregate queries|Abstract We present techniques for computing small spacerepresentations of massive data streams. These are inspired by traditional wavelet-based approx-imations that consist of specific linear projections of the underlying data. We present general&amp;quot;sketch &amp;quot; based methods for capturing various linear projections of the data and use them to pro-vide pointwise and rangesum estimation of data streams. These methods use small amounts ofspace and per-item time while streaming through the data, and provide accurate representation asour experiments with real data streams show.
552|Approximate Query Processing Using Wavelets|Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today’s decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing
553|Space-Efficient Online Computation of Quantile Summaries|An &amp;epsilon;-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of &amp;epsilon;N . We present a new online...
554| Approximate Computation of Multidimensional Aggregates of Sparse Data Using Wavelets |Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries. In this paper, we present anovel method that provides approximate answers to high-dimensional OLAP aggregation queries in massive sparse data sets in a time-efficient and space-efficient manner. We construct a compact data cube, which is an approximate and space-efficient representation of the underlying multidimensional array, based upon a multiresolution wavelet decomposition. In the on-line phase, each aggregation query can generally be answered using the compact data cube in one I/O or a small number of I/Os, depending upon the desired accuracy. We present two I/O-efficient algorithms to construct the compact data cube for the important case of sparse high-dimensional arrays, which often arise in practice. The traditional histogram methods are infeasible for the massive high-dimensional data sets in OLAP applications. Previously developed wavelet techniques are efficient only for dense data. Our on-line query processing algorithm is very fast and capable of refining answers as the user demands more accuracy. Experiments on real data show that our method provides significantly more accurate results for typical OLAP aggregation queries than other efficient approximation techniques such as random sampling.
555|Computing on Data Streams|In this paper we study the space requirement of algorithms that make  only one (or a small number of) pass(es) over the input data. We study such  algorithms under a model of data streams that we introduce here. We give  a number of upper and lower bounds for problems stemming from queryprocessing,  invoking in the process tools from the area of communication  complexity.
556|Processing Complex Aggregate Queries over Data Streams|Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (i.e., data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed.
557|Continual Queries for Internet Scale Event-Driven Information Delivery|In this paper we introduce the concept of continual queries, describe the design of a distributed  event-driven continual query system -- OpenCQ, and outline the initial implementation of OpenCQ  on top of the distributed interoperable information mediation system DIOM [21, 19]. Continual  queries are standing queries that monitor update of interest and return results whenever the update  reaches specified thresholds. In OpenCQ, users may specify to the system the information they would  like to monitor (such as the events or the update thresholds they are interested in). Whenever the  information of interest becomes available, the system immediately delivers it to the relevant users;  otherwise, the system continually monitors the arrival of the desired information and pushes it to the  relevant users as it meets the specified update thresholds. In contrast to conventional pull-based data  management systems such as DBMSs and Web search engines, OpenCQ exhibits two important featu...
558|Join synopses for approximate query answering|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses (join samples) as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known. We also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations. One of our key contributions is a detailed analysis of the error bounds obtained for approximate answers that demonstrates the trade-offs in various methods, as well as the advantages in certain scenarios of a new subsampling method we propose. Our extensive set of experiments on the TPC-D benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper. 1
559|An efficient cost-driven index selection tool for Microsoft SQL Server|In this paper we describe novel techniques that make it possible to build an industrial-strength tool for automating the choice of indexes in the physical design of a SQL database. The tool takes as input a workload of SQL queries, and suggests a set of suitable indexes. We ensure that the indexes chosen are effective in reducing the cost of the workload by keeping the index selection tool and the query optimizer &amp;quot;in step&amp;quot;. The number of index sets that must be evaluated to find the optimal configuration is very large. We reduce the complexity of this problem using three techniques. First, we remove a large number of spurious indexes from consideration by taking into account both query syntax and cost information. Second, we introduce optimizations that make it possible to cheaply evaluate the “goodness ” of an index set. Third, we describe an iterative approach to handle the complexity arising from multicolumn indexes. The tool has been implemented on Microsoft SQL Server 7.0. We performed extensive experiments over a range of workloads, including TPC-D. The results indicate that the tool is efficient and its choices are close to optimal. 1.
560|Reductions in Streaming Algorithms, with an Application to Counting Triangles in Graphs |We introduce reductions in the streaming model as a tool in the design of streaming algorithms. We develop
561|Computing iceberg queries efficiently|Many applications compute aggregate functions...
562|Data-Streams and Histograms|Histograms have been used widely to capture data distribution, to represent the data by a small number of step functions. Dynamic programming algorithms which provide optimal construction of these histograms exist, albeit running in quadratic time and linear space. In this paper we provide linear time construction of 1 + epsilon approximation of optimal histograms, running in polylogarithmic space. Our results extend to the context of data-streams, and in fact generalize to give 1 + epsilon approximation of several problems in data-streams which require partitioning the index set into intervals. The only assumptions required are that the cost of an interval is monotonic under inclusion (larger interval has larger cost) and that the cost can be computed or approximated in small space. This exhibits a nice class of problems for which we can have near optimal data-stream algorithms.
563|XJoin: A Reactively-Scheduled Pipelined Join Operator|Wide-area distribution raises significant performance problems for traditional query processing techniques as data access becomes less predictable due to link congestion, load imbalances, and temporary outages. Pipelined query execution is a promising approach to coping with unpredictability in such environments as it allows scheduling to adjust to the arrival properties of the data. We have developed a non-blocking join operator, called XJoin, which has a small memory footprint, allowing many such operators to be active in parallel. XJoin is optimized to produce initial results quickly and can hide intermittent delays in data arrival by reactively scheduling background processing. We show that XJoin is an effective solution for providing fast query responses to users even in the presence of slow and bursty remote sources.  
564|Rate-Based Query Optimization for Streaming Information Sources|Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences. We present a preliminary validation of our rate-based optimization framework on a prototype XML query engine, though it is generic enough to be used in other database contexts. The results show that rate-based optimization is feasible and can indeed yield correct decisions.
565|Making Views Self-Maintainable for Data Warehousing|A data warehouse stores materialized views over data from one or more sources in order to provide fast access to the integrated data, regardless of the availability of the data sources. Warehouse views need to be maintained inresponse to changes to the base data in the sources. Except for very simple views, maintaining a warehouse view requires access to data that is not available in the view itself. Hence, to maintain the view, one either has to query the data sources or store auxiliary data in the warehouse. We show that by using key and referential integrity constraints, we often can maintain a select-project-join view without going to the data sources or replicating the base relations in their entirety in the warehouse. We derive a set of auxiliary views such that the warehouse view and the auxiliary views together are self-maintainable|they can be maintained without going to the data sources or replicating all base data. In addition, our technique can be applied to simplify traditional materialized view maintenance by exploiting key and referential integrity constraints. 1
566|Approximate Medians and other Quantiles in One Pass and with Limited Memory|We present new algorithms for computing approximate quantiles of large datasets in a single pass. The approximation guarantees are explicit, and apply without regard to the value distribution or the arrival distributions of the dataset. The main memory requirements are smaller than those reported earlier by an order of magnitude.  We also discuss methods that couple the approximation algorithms with random sampling to further reduce memory requirements. With sampling, the approximation guarantees are explicit but probabilistic, i.e., they apply with respect to a (user controlled) confidence parameter.  We present the algorithms, their theoretical analysis and simulation results.  1 Introduction  This article studies the problem of computing order statistics  of large sequences of online or disk-resident data using as little main memory as possible. We focus on computing  quantiles, which are elements at specific positions in the sorted order of the input. The OE-quantile, for OE 2 [0; ...
567|Random Sampling for Histogram Construction: How much is enough?|Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining &#034;How much sampling is enough?&#034; We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a  conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose ...
568|Sampling From a Moving Window Over Streaming Data|We introduce the problem of sampling from a moving window of recent items from a data stream and develop the &#034;chain-sample&#034; and &#034;priority-sample&#034; algorithms for this problem.
569|Tracking join and self-join sizes in limited storage|This paper presents algorithms for tracking (approximate) join and self-join sizes in limited storage, in the presence of insertions and deletions to the data set(s). Such algorithms detect changes in join and self-join sizes without an expensive recomputation from the base data, and without the large space overhead required to maintain such sizes exactly. Query optimizers rely on fast, high-quality estimates of join sizes in order to select between various join plans, and estimates of self-join sizes are used to indicate the degree of skew in the data. For self-joins, we considertwo approaches proposed in [Alon, Matias, and Szegedy. The Space Complexity of Approximating the Frequency Moments. JCSS, vol. 58, 1999, p.137-147], which we denote tug-of-war and sample-count. Wepresent fast algorithms for implementing these approaches, and extensions to handle deletions as well as insertions. We also report on the rst experimental study of the two approaches, on a range of synthetic and real-world data sets. Our study shows that tug-of-war provides more accurate estimates for a given storage limit than sample-count, which in turn is far more accurate than a standard sampling-based approach. For example, tug-of-war needed only 4{256 memory words, depending on the data set, in order to estimate the self-join size
570|Data Cube Approximation and Histograms via Wavelets (Extended Abstract)  (1998) |) Jeffrey Scott Vitter   Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  jsv@cs.duke.edu Min Wang  y Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  minw@cs.duke.edu Bala Iyer  Database Technology Institute IBM Santa Teresa Laboratory P.O. Box 49023 San Jose, CA 95161 USA  balaiyer@vnet.ibm.com Abstract  There has recently been an explosion of interest in the analysis of data in data warehouses in the field of On-Line Analytical Processing (OLAP). Data warehouses can be extremely large, yet obtaining quick answers to queries is important. In many situations, obtaining the exact answer to an OLAP query is prohibitively expensive in terms of time and/or storage space. It can be advantageous to have fast, approximate answers to queries. In this paper, we present an I/O-efficient technique based upon a multiresolution wavelet decomposition that yields an approximate a...
571|The design and implementation of a sequence database system|This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. There are three distinct contributions made in this paper. (1) We describe the specification of sequence queries using the SEQUIN query language. (2) We quantitatively demonstrate the importance of various storage and optimization techniques by studying their effect on performance. (3) We present a novel nested design paradigm used in PREDATOR to combine sequence ‘and relational data.
572|Random sampling techniques for space efficient online computation of order statistics of large datasets|In a recent paper [MRL98], we had described a general framework for single pass approximate quantile nding algorithms. This framework included several known algorithms as special cases. We had identi ed a new algorithm, within the framework, which had a signi cantly smaller requirement for main memory than other known algorithms. In this paper, we address two issues left open in our earlier paper. First, all known and space e cient algorithms for approximate quantile nding require advance knowledge of the length of the input sequence. Many important database applications employing quantiles cannot provide this information. In this paper, we present anovel non-uniform random sampling scheme and an extension of our framework. Together, they form the basis of a new algorithm which computes approximate quantiles without knowing the input sequence length. Second, if the desired quantile is an extreme value (e.g., within the top 1 % of the elements), the space requirements of currently known algorithms are overly pessimistic. We provide a simple algorithm which estimates extreme values using less space than required by the earlier more general technique for computing all quantiles. Our principal observation here is that random sampling is quanti ably better when estimating extreme values than is the case with the median.  
573|Characterizing Memory Requirements for Queries over Continuous Data|This paper deals with continuous conjunctive queries with arithmetic comparisons and optional aggregation over multiple data streams. An algorithm is presented for determining whether or not any given query can be evaluated using a bounded amount of memory for all possible instances of the data streams. For queries that can be evaluated using bounded memory, an execution strategy based on constant-sized synopses of the data streams is proposed. For queries that cannot be evaluated using bounded memory, data stream scenarios are identified in which evaluating the queries requires memory linear in the size of the unbounded streams
574|Congressional samples for approximate answering of group-by queries|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex decision support queries using precomputed summary statistics, such as samples. Decision support queries routinely segment the data into groups and then aggregate the information in each group (group-by queries). Depending on the data, there can be a wide disparity between the number of data items in each group. As a result, approximate answers based on uniform random samples of the data can result in poor accuracy for groups with very few data items, since such groups will be represented in the sample by very few (often zero) tuples. In this paper, we propose a general class of techniques for obtaining fast, highly-accurate answers for group-by queries. These techniques rely on precomputed non-uniform (biased) samples of the data. In particular, we proposecongressional samples, ahybrid union of uniform and biased samples. Given a xed amount of space, congressional samples seek to maximize the accuracy for all possible group-by queries on a set of columns. We present a one pass algorithm for constructing a congressional sample and use this technique to also incrementally maintain the sample up-to-date without accessing the base relation. We also evaluate query rewriting strategies for providing approximate answers from congressional samples. Finally, we conduct an extensive set of experiments on the TPC-D database, which demonstrates the e cacy of the techniques proposed. 1
575|Histogram-Based Approximation of Set-Valued Query Answers|Answering queries approximately has recently  been proposed as a way to reduce query response  times in on-line decision support systems,  when the precise answer is not necessary  or early feedback is helpful. Most of the  work in this area uses sampling-based techniques  and handles aggregate queries, ignoring  queries that return relations as answers. In  this paper, we extend the scope of approximate  query answering to general queries. We propose  a novel and intuitive error measure for  quantifying the error in an approximate query  answer, which can be a multiset in general.
576|Data Integration using Self-Maintainable Views|.  In this paper we de#ne the concept of self-maintainable views  # these are views that can be maintained using only the contents of  the view and the database modi#cations, without accessing any of the  underlying databases. We derive tight conditions under which several  types of select-project-join are self-maintainable upon insertions, deletions  and updates. Self-Maintainability is a desirable property for e#-  ciently maintaining large views in applications where fast response and  high availability are important. One example of suchanenvironment  is data warehousing wherein views are used for integrating data from  multiple databases.  1 Introduction  Most large organizations have related data in distinct databases. Many of these databases may be legacy systems, or systems separated for organizational reasons like funding and ownership. Integrating data from such distinct databases is a pressing business need. A common approach for integration is to de#ne an integrated view and...
577|Monitoring XML Data on the Web|We consider the monitoring of a flow of incoming documents. More precisely, we present here the monitoring used in a very large warehouse built from XML documents found on the web. The flow of documents consists in XML pages (that are warehoused) and HTML pages (that are not). Our contributions are the following:  ffl a subscription language which specifies the monitoring of pages when fetched, the periodical evaluation of continuous queries and the production of XML reports. ffl the description of the architecture of the system we implemented that makes it possible to monitor a flow of millions of pages per day with millions of subscriptions on a single PC, and scales up by using more machines. ffl a new algorithm for processing alerts that can be used in a wider context. We support
579|Expiring data in a warehouse|Data warehouses collect data into materi-alized views for analysis. After some time, some of the data may no longer be needed or may not be of interest. In this pa-per, we handle this by expiring or remov-ing unneeded materialized view tuples. A framework supporting such expiration is presented. Within it, a user or adminis-trator can declaratively request expirations and can specify what type of modifications are expected from external sources. The lat-ter can significantly increase the amount of data that can be expired. We present effi-cient algorithms for determining what data can be expired (data not needed for main-tenance of other views), taking into account the types of updates that may occur. 1
581|Sequence Query Processing|Many applications require the ability to manipulate sequences of data. We motivate the importance of sequence query processing, and present a framework for the optimization of sequence queries based on several novel techniques. These include query transformations, optimizations that utilize meta--data, and caching of intermediate results. We present a bottom-up algorithm that generates an efficient query evaluation plan based on cost estimates. This work also identifies a number of directions in which future research can be directed.  1 Introduction  Many real life applications manipulate data that is inherently sequential. Such data is logically viewed and queried in terms of a sequence abstraction and is often physically stored as a sequence. Databases should (a) allow sequences to be queried in a declarative manner, utilizing the ordered semantics of the data, and (b) take advantage of the opportunities available for query optimization. Relational databases are inadequate in this re...
582|SEQ: A Model for Sequence Databases|This paper presents the model which is the basis for a system to manage various kinds of sequence data. The model separates the data from the ordering information, and includes operators based on two distinct abstractions of a sequence. The main contributions of the model are: (a) it can deal with different types of sequence data, (b) it supports an expressive range of sequence queries, (c) it draws from many of the diverse existing approaches to modeling sequence data. 1
583|Sampling Algorithms: Lower Bounds and Applications (Extended Abstract)  (2001) |]  Ziv Bar-Yossef  y  Computer Science Division  U. C. Berkeley  Berkeley, CA 94720  zivi@cs.berkeley.edu  Ravi Kumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  ravi@almaden.ibm.com  D. Sivakumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  siva@almaden.ibm.com  ABSTRACT  We develop a framework to study probabilistic sampling algorithms that approximate general functions of the form f : A  n  ! B, where A and B are arbitrary sets. Our goal is to obtain lower bounds on the query complexity of functions, namely the number of input variables x i that any sampling algorithm needs to query to approximate f(x1 ; : : : ; xn ).  We define two quantitative properties of functions --- the block sensitivity and the minimum Hellinger distance --- that give us techniques to prove lower bounds on the query complexity. These techniques are quite general, easy to use, yet powerful enough to yield tight results. Our applications include the mean and higher statistical moments, the median and other selection functions, and the frequency moments, where we obtain lower bounds that are close to the corresponding upper bounds.  We also point out some connections between sampling and streaming algorithms and lossy compression schemes.  1. 
584|Approximating a Data Stream for Querying and Estimation: Algorithms and Performance Evaluation|Obtaining fast and good quality approximations to data distributions is a problem of central interest to database management. A variety of popular database applications including, approximate querying, similarity searching and data mining in most application domains, rely on such good quality approximations. Histogram based approximation is a very popular method in database theory and practice to succinctly represent a data distribution in a space efficient manner. In this paper, we place the problem of histogram construction into perspective and we generalize it by raising the requirement of a finite data set and/or known data set size. We consider the case of an infinite data set on which data arrive continuously forming an infinite data stream. In this context, we present the first single pass algorithms capable of constructing histograms of provable good quality. We present algorithms for the fixed window variant of the basic histogram construction problem, supporting incremental maintenance of the histograms. The proposed algorithms trade accuracy for speed and allow for a graceful tradeoff between the two, based on application requirements. In the case of approximate queries on infinite data streams, we present a detailed experimental evaluation comparing our algorithms with other applicable techniques using real data sets, demonstrating the superiority of our proposal. 1
585|A robust, optimization-based approach for approximate answering of aggregate queries|The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are “similar ” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work. 1
586|Online Dynamic Reordering for Interactive Data Processing|Abstract We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing.
588|On Sampling and Relational Operators|A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. We highlight the primary difficulties, summarize the results of some recent work in this area, and indicate directions for future work.   
590|Convex Analysis|In this book we aim to present, in a unified framework, a broad spectrum of mathematical theory that has grown in connection with the study of problems of optimization, equilibrium, control, and stability of linear and nonlinear systems. The title Variational Analysis reflects this breadth. For a long time, ‘variational ’ problems have been identified mostly with the ‘calculus of variations’. In that venerable subject, built around the minimization of integral functionals, constraints were relatively simple and much of the focus was on infinite-dimensional function spaces. A major theme was the exploration of variations around a point, within the bounds imposed by the constraints, in order to help characterize solutions and portray them in terms of ‘variational principles’. Notions of perturbation, approximation and even generalized differentiability were extensively investigated. Variational theory progressed also to the study of so-called stationary points, critical points, and other indications of singularity that a point might have relative to its neighbors, especially in association with existence theorems for differential equations.
591|A View Of The Em Algorithm That Justifies Incremental, Sparse, And Other Variants|. The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible. 1. Introduction The Expectation-Maximization (EM) algorithm finds maximum likelihood parameter estimates in problems where some variables were unobserved. Special cases of the algorithm date back several dec...
592|Probabilistic Inference Using Markov Chain Monte Carlo Methods|Probabilistic inference is an attractive approach to uncertain reasoning and empirical learning in artificial intelligence. Computational difficulties arise, however, because probabilistic models with the necessary realism and flexibility lead to complex distributions over high-dimensional spaces. Related problems in other fields have been tackled using Monte Carlo methods based on sampling using Markov chains, providing a rich array of techniques that can be applied to problems in artificial intelligence. The &#034;Metropolis algorithm&#034; has been used to solve difficult problems in statistical physics for over forty years, and, in the last few years, the related method of &#034;Gibbs sampling&#034; has been applied to problems of statistical inference. Concurrently, an alternative method for solving problems in statistical physics by means of dynamical simulation has been developed as well, and has recently been unified with the Metropolis algorithm to produce the &#034;hybrid Monte Carlo&#034; method. In computer science, Markov chain sampling is the basis of the heuristic optimization technique of &#034;simulated annealing&#034;, and has recently been used in randomized algorithms for approximate counting of large sets. In this review, I outline the role of probabilistic inference in artificial intelligence, present the theory of Markov chains, and describe various Markov chain Monte Carlo algorithms, along with a number of supporting techniques. I try to present a comprehensive picture of the range of methods that have been developed, including techniques from the varied literature that have not yet seen wide application in artificial intelligence, but which appear relevant. As illustrative examples, I use the problems of probabilistic inference in expert systems, discovery of latent classes from data, and Bayesian learning for neural networks.
593|Bucket Elimination: A Unifying Framework for Probabilistic Inference| Probabilistic inference algorithms for belief updating, finding the most probable explanation, the maximum a posteriori hypothesis, and the maximum expected utility are reformulated within the bucket elimination framework. This emphasizes the principles common to many of the algorithms appearing in the probabilistic inference literature and clarifies the relationship of such algorithms to nonserial dynamic programming algorithms. A general method for combining conditioning and bucket elimination is also presented. For all the algorithms, bounds on complexity are given as a function of the problem&#039;s structure.  
594|Approximating probabilistic inference in Bayesian belief networks is NP-hard|Abstract- A belief network comprises a graphical representation of dependencies between variables of a domain and a set of conditional probabilities associated with each dependency. Unless P=NP, an efficient, exact algorithm does not exist to compute probabilistic inference in belief networks. Stochastic simulation methods, which often improve run times, provide an alternative to exact inference algorithms. We present such a stochastic simulation algorithm 2)-BNRAS that is a randomized approximation scheme. To analyze the run time, we parameterize belief networks by the dependence value PE, which is a measure of the cumulative strengths of the belief network dependencies given background evidence E. This parameterization defines the class of f-dependence networks. The run time of 2)-BNRAS is polynomial when f is a polynomial function. Thus, the results of this paper prove the existence of a class of belief networks for which inference approximation is polynomial and, hence, provably faster than any exact algorithm. I.
595|The Wake-Sleep Algorithm for Unsupervised Neural Networks|We describe an unsupervised learning algorithm for a multilayer network of stochastic neurons. Bottom-up &#034;recognition&#034; connections convert the input into representations in successive hidden layers and top-down &#034;generative&#034; connections reconstruct the representation in one layer from the representation in the layer above. In the &#034;wake&#034; phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the &#034;sleep&#034; phase, neurons are driven by generative connections and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above. Supervised learning algorithms for multilayer neural networks face two problems: They require a teacher to specify the desired output of the network and they require some method of communicating error information to all of the connections. The wake-sleep alg...
596|Probabilistic independence networks for hidden Markov probability models|Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper contains a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach. 
597|Stochastic simulation algorithms for dynamic probabilistic networks|Stochastic simulation algorithms such as likelihood weighting often give fast, accurate approximations to posterior probabilities in probabilistic networks, and are the methods of choice for very large networks. Unfortunately, the special characteristics of dynamic probabilistic networks (DPNs), which are used to represent stochastic temporal processes, mean that standard simulation algorithms perform very poorly. In essence, the simulation trials diverge further and further from reality as the process is observed over time. In this paper, we present simulation algorithms that use the evidence observed at each time step to push the set of trials back towards reality. The first algorithm, &amp;quot;evidence reversal &amp;quot; (ER) restructures each time slice of the DPN so that the evidence nodes for the slice become ancestors of the state variables. The second algorithm, called &amp;quot;survival of the fittest &amp;quot; sampling (SOF), &amp;quot;repopulates &amp;quot; the set of trials at each time step using a stochastic reproduction rate weighted by the likelihood of the evidence according to each trial. We compare the performance of each algorithm with likelihood weighting on the original network, and also investigate the benefits of combining the ER and SOF methods. The ER/SOF combination appears to maintain bounded error independent of the number of time steps in the simulation.
598|Keeping Neural Networks Simple by Minimizing the Description Length of the Weights |Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-off between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed efficiently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights.  
599|Mean Field Theory for Sigmoid Belief Networks|We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics.
600|Probabilistic Diagnosis Using a Reformulation of the INTERNIST-1/QMR Knowledge Base - II. Evaluation of Diagnostic Performance|We have developed a probabilistic reformulation of the Quick Medical Reference (QMR) system. In Part I of this two-part series, we described a two-level, multiply connected belief-network representation of the QMR knowledge base and a simulation algorithm to perform probabilistic inference on the reformulated knowledge base. In Part II of this series, we report on an evaluation of the probabilistic QMR, in which we compare the performance of QMR to that of our probabilistic system on cases abstracted from continuing medical education materials from Scientific American Medicine. In addition, we analyze empirically several components of the probabilistic model and simulation algorithm.
601|Exploiting Tractable Substructures in Intractable Networks|We develop a refined mean field approximation for inference and learning in probabilistic neural networks. Our mean field theory, unlike most, does not assume that the units behave as independent degrees of freedom; instead, it exploits in a principled way the existence of large substructures that are computationally tractable. To illustrate the advantages of this framework, we show how to incorporate weak higher order interactions into a first-order hidden Markov model, treating the corrections (but not the first order structure) within mean field theory. 1 INTRODUCTION  Learning the parameters in a probabilistic neural network may be viewed as a problem in statistical estimation. In networks with sparse connectivity (e.g. trees and chains), there exist efficient algorithms for the exact probabilistic calculations that support inference and learning. In general, however, these calculations are intractable, and approximations are required. Mean field theory provides a framework for app...
602|Bayesian Methods for Mixtures of Experts|We present a Bayesian framework for inferring the parameters of a mixture of experts model based on ensemble learning by variational free energy minimisation. The Bayesian approach avoids the over-fitting and noise level under-estimation problems of traditional maximum likelihood inference. We demonstrate these methods on artificial problems and sunspot time series prediction. INTRODUCTION The task of estimating the parameters of adaptive models such as artificial neural networks using Maximum Likelihood (ML) is well documented eg. Geman, Bienenstock
603|Variational Methods for Inference and Estimation in Graphical Models|Graphical models enhance the representational power of probability models through qualitative characterization of their properties. This also leads to greater efficiency in terms of the computational algorithms that empower such representations. The increasing complexity of these models, however, quickly renders exact probabilistic calculations infeasible. We propose a principled framework for approximating graphical models based on variational methods. We develop variational techniques from the perspective that unifies and expands their applicability to graphical models. These methods allow the (recursive) computation of upper and lower bounds on the quantities of interest. Such bounds yield considerably more information than mere approximations and provide an inherent error metric for tailoring the approximations individually to the cases considered. These desirable properties, concomitant to the variational methods, are unlikely to arise as a result of other deterministic or stochas...
604|Hidden Markov decision trees|We study a time series model that can be viewed as a decision tree with Markov temporal structure. The model is intractable for exact calculations, thus we utilize variational approximations. We consider three different distributions for the approximation: one in which the Markov calculations are performed exactly and the layers of the decision tree are decoupled, one in which the decision tree calculations are performed exactly and the time steps of the Markov chain are decoupled, and one in which a Viterbi-like assumption is made to pick out a single most likely state sequence. We present simulation results for artificial data and the Bach chorales. Accepted for oral presentation at NIPS*96. 1 Introduction  Decision trees are regression or classification models that are based on a nested decomposition of the input space. An input vector x is classified recursively by a set of &#034;decisions&#034; at the nonterminal nodes of a tree, resulting in the choice of a terminal node at which an output...
605|Improving the Mean Field Approximation via the Use of Mixture Distributions|Introduction  Graphical models provide a formalism in which to express and manipulate conditional independence statements. Inference algorithms for graphical models exploit these independence statements, using them to compute conditional probabilities while avoiding brute force marginalization over the joint probability table. Many inference algorithms, in particular the clustering algorithms, make explicit their usage of conditional independence by constructing a data structure that captures the essential Markov properties underlying the graph. That is, the algorithm groups interacting variables into clusters, such that the hypergraph of clusters has Markov properties that allow simple local algorithms to be employed for inference. In the best case, in which the original graph is sparse and without long cycles, the clusters are small and inference is efficient. In the worst case, such as the case of a dense graph, the clusters are large and inference is inefficient (complexity
606|Switching State-Space Models|We introduce a statistical model for times series data with nonlinear dynamics  which iteratively segments the data into regimes with approximately linear dynamics  and learns the parameters of each of those regimes. This model combines and generalizes  two of the most widely used stochastic time series models---the hidden Markov  model and the linear dynamical system---and is related to models that are widely used  in the control and econometrics literatures. It can also be derived by extending the  mixture of experts neural network model (Jacobs et al., 1991) to its fully dynamical  version, in which both expert and gating networks are recurrent. Inferring the posterior  probabilities of the hidden states of this model is computationally intractable,  and therefore the exact Expectation Maximization (EM) alogithm cannot be applied.  However, we present a variational approximation which maximizes a lower bound on  the log likelihood and makes use of both the forward--backward recursio...
607|Localized Partial Evaluation of Belief Networks|Most algorithms for propagating evidence through belief networks have been exact and  exhaustive: they produce an exact (pointvalued) marginal probability for every node in the network. Often, however, an application will not need information about every node in the network nor will it need exact probabilities. We present the localized partial evaluation (LPE) propagation algorithm, which computes interval bounds on the marginal probability of a specified query node by examining a subset of the nodes in the entire network. Conceptually, LPE ignores parts of the network that are &#034;too far away&#034; from the queried node to have much impact on its value. LPE has the &#034;anytime&#034; property of being able to produce better solutions (tighter intervals) given more time to consider more of the network. 1 Introduction  Belief networks provide a way of encoding knowledge about the probabilistic dependencies and independencies of a set of variables in some domain. Variables are encoded as nodes in the ne...
608|Computing Upper and Lower Bounds on Likelihoods in Intractable Networks|We present techniques for computing upper and lower bounds on the likelihoods of partial instantiations  of variables in sigmoid and noisy-OR networks. The bounds determine confidence intervals for the desired  likelihoods and become useful when the size of the network (or clique size) precludes exact computations.
609|A hierarchical community of experts|We describe a hierarchical generative model that selects from a large collection of available linear units an appropriate subset to model each observation. The selection mechanism is a corresponding network of binary units each of which gates the output of a linear unit. Inference in the binary network is intractable, but the statistics required to learn maximum-likelihood model parameters can be approximated with Gibbs sampling, even if the sampling is so brief that the Markov chain is far from equilibrium. 1 Multilayer networks of linear-Gaussian units We consider directed acyclic networks of simple stochastic units, where the units are arranged in layers. The input to a unit is the weighted sum of the activities of units in the layer above, plus a bias. In the generative model, the joint probability of all of the units in the network taking on a particular set of values, or configuration, can be factored into a product of probabilities of individual units, conditioned on the units in the layer above. The simplest unit we will consider is a linear-Gaussian unit. The probability that a linear-Gaussian unit takes on a particular value is given by a Gaussian distribution centered at the top-down prediction of the unit’s parents. The top-down prediction for unit i, denoted ?yi, is the weighted sum of its parents ’ outputs, plus a bias: ?yi = ? j?P a(i)
610|Approximating Posterior Distributions in Belief Networks using Mixtures|Exact inference in densely connected Bayesian networks is computationally intractable, and so there is considerable interest in developing effective approximation schemes. One approach which has been adopted is to bound the log likelihood using a mean-field approximating distribution. While this leads to a tractable algorithm, the mean field distribution is assumed to be factorial and hence unimodal. In this paper we demonstrate the feasibility of using a richer class of approximating distributions based on mixtures of mean field distributions. We derive an efficient algorithm for updating the mixture parameters and apply it to the problem of learning in sigmoid belief networks. Our results demonstrate a systematic improvement over simple mean field theory as the number of mixture components is increased. 1 Introduction Bayesian belief networks can be regarded as a fully probabilistic interpretation of feedforward neural networks. Maximum likelihood learning for Bayesian n...
611|Recursive Algorithms for Approximating Probabilities in Graphical Models|We develop a recursive node-elimination formalism for efficiently approximating large probabilistic networks. No constraints are set on the network topologies. Yet the formalism can be straightforwardly integrated with exact methods whenever they are/become applicable. The approximations we use are controlled: they maintain consistently upper and lower bounds on the desired quantities at all times. We show that Boltzmann machines, sigmoid belief networks, or any combination (i.e., chain graphs) can be handled within the same framework. The accuracy of the methods is verified experimentally.   1 Introduction  Graphical models (see, e.g., Lauritzen 1996) provide a medium for rigorously embedding domain knowledge into network models. The structure in these graphical models embodies the qualitative assumptions about the independence relationships in the domain while the probability model attached to the graph permits a consistent computation of belief (or uncertainty) about the values of t...
612|A Statistical Approach to Decision Tree Modeling|A statistical approach to decision tree modeling is described. In this approach, each decision in the tree is modeled parametrically as is the process by which an output is generated from an input and a sequence of decisions. The resulting model yields a likelihood measure of goodness of fit, allowing ML and MAP estimation techniques to be utilized. An efficient algorithm is presented to estimate the parameters in the tree. The model selection problem is presented and several alternative proposals are considered. A hidden Markov version of the tree is described for data sequences that have temporal dependencies.
613|Learning in Boltzmann Trees|We introduce a large family of Boltzmann machines that can be trained using standard gradient descent. The networks can have one or more layers of hidden units, with tree-like connectivity. We show how to implement the supervised learning algorithm for these Boltzmann machines exactly, without resort to simulated or mean-field annealing. The stochastic averages that yield the gradients in weight space are computed by the technique of decimation. We present results on the problems of N-bit  parity and the detection of hidden symmetries. 1 Introduction  Boltzmann machines (Ackley, Hinton, &amp; Sejnowski, 1985) have several compelling virtues. Unlike simple perceptrons, they can solve problems that are not linearly separable. The learning rule, simple and locally based, lends itself to massive parallelism. The theory of Boltzmann learning, moreover, has a solid foundation in statistical mechanics. Unfortunately, Boltzmann machines--- as originally conceived---also have some serious drawbacks...
614|Reduction of Computational Complexity in Bayesian Networks through Removal of Weak Dependences|The paper presents a method for reducing the computational complexity of Bayesian networks through identification and removal of weak dependences (removal of links from the (moralized) independence graph). The removal of a small number of links may reduce the computational complexity dramatically, since several fill-ins and moral links may be rendered superfluous by the removal. The method is described in terms of impact on the independence graph, the junction tree, and the potential functions associated with these. An empirical evaluation of the method using large real-world networks demonstrates the applicability of the method. Further, the method, which has been implemented in Hugin, complements the approximation method suggested by Jensen &amp; Andersen (1990). 
615|Variational methods and the QMR-DT database|We describe variational approximation methods for e cient probabilistic reasoning, applying these methods to the problem of diagnostic inference in the QMR-DT database. The QMR-DT database is a large-scale belief network based on statistical and expert knowledge in internal medicine. The size and complexity of this network render exact probabilistic diagnosis infeasible for all but a small set of cases. This has hindered the development of the QMR-DT network as a practical diagnostic tool and has hindered researchers from exploring and critiquing the diagnostic behavior of QMR. In this paper we describe how variational approximation methods can be applied to the QMR network, resulting in fast diagnostic inference. We evaluate the accuracy of our methods on a set of standard diagnostic cases and compare to stochastic sampling methods. 1
616|A Mean Field Learning Algorithm For Unsupervised Neural Networks|. We introduce a learning algorithm for unsupervised neural networks based on ideas from statistical mechanics. The algorithm is derived from a mean field approximation for large, layered sigmoid belief networks. We show how to (approximately) infer the statistics of these networks without resort to sampling. This is done by solving the mean field equations, which relate the statistics of each unit to those of its Markov blanket. Using these statistics as target values, the weights in the network are adapted by a local delta rule. We evaluate the strengths and weaknesses of these networks for problems in statistical pattern recognition. 1. Introduction  Multilayer neural networks trained by backpropagation provide a versatile framework for statistical pattern recognition. They are popular for many reasons, including the simplicity of the learning rule and the potential for discovering hidden, distributed representations of the problem space. Nevertheless, there are many issues that are...
617|Annealed Theories of Learning|We study annealed theories of learning boolean functions using a concept class of finite cardinality. The naive annealed theory can be used to derive a universal learning curve bound for zero temperature learning, similar to the inverse square root bound from the Vapnik-Chervonenkis theory. Tighter, nonuniversal learning curve bounds are also derived. A more refined annealed theory leads to still tighter bounds, which in some cases are very similar to results previously obtained using one-step replica symmetry breaking. 1. Introduction  The annealed approximation  1  has proven to be an invaluable tool for studying the statistical mechanics of learning from examples. Previously it was found that the annealed approximation gave qualitatively correct results for several models of perceptrons learning realizable rules.  2  Because of its simplicity relative to the full quenched theory, the annealed approximation has since been used in studies of more complicated multilayer architectures. ...
618|Consumers and Their Brands: Developing Relationship Theory|Although the relationship metaphor dominates contemporary marketing thought and practice, surprisingly little empirical work has been conducted on relational phenomena in the consumer products domain, particularly at the level of the brand. In this article, the author: (1) argues for the validity of the relationship proposition in the consumer-brand context, including a debate as to the legiti-macy of the brand as an active relationship partner and empirical support for the phenomenological significance of consumer-brand bonds; (2) provides a framework for characterizing and better understanding the types of relationships consumers form with brands; and (3) inducts from the data the concept of brand relationship quality, a diagnostic tool for conceptualizing and evaluating relation-ship strength. Three in-depth case studies inform this agenda, their interpretation guided by an integrative review of the literature on person-to-person relationships. Insights offered through application of inducted concepts to two relevant research domains—brand loyalty and brand personality—are advanced in closing. The exercise is intended to urge fellow researchers to refine, test, and augment the
619|Maximum entropy markov models for information extraction and segmentation|Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ’s.  
620|Inducing Features of Random Fields|We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classifica...
621|The Infinite Hidden Markov Model|We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters define a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be infinite---consider, for example, symbols being possible words appearing in English text.
622|An Algorithm that Learns What&#039;s in a Name|In this paper, we present IdentiFinder^TM, a hidden Markov model that learns to recognize and classify names, dates, times, and numerical quantities. We have evaluated the model in English (based on data from the Sixth and Seventh Message Understanding Conferences [MUC-6, MUC-7] and broadcast news) and in Spanish (based on data distributed through the First Multilingual Entity Task [MET-1]), and on speech input (based on broadcast news). We report results here on standard materials only to quantify performance on data available to the community, namely, MUC-6 and MET-1. Results have been consistently better than reported by any other learning algorithm. IdentiFinder&#039;s performance is competitive with approaches based on handcrafted rules on mixed case text and superior on text where case information is not available. We also present a controlled experiment showing the effect of training set size on performance, demonstrating that as little as 100,000 words of training data is adequate to get performance around 90% on newswire. Although we present our understanding of why this algorithm performs so well on this class of problems, we believe that significant improvement in performance may still be possible.
623|A Gaussian prior for smoothing maximum entropy models|In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood train-ing for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods com-pare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty [1] performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parame-ters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance.
624|Maximum Entropy Models for Natural Language Ambiguity Resolution|The best aspect of a research environment, in my opinion, is the abundance of bright people with whom you argue, discuss, and nurture your ideas. I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas. I hope that Ihave kept the good ideas in this thesis, and left the bad ideas out! Iwould like toacknowledge the following people for their contribution to my education: I thank my advisor Mitch Marcus, who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing, and also gave me direction when necessary. I also thank Mitch for many fascinating conversations, both personal and professional, over the last four years at Penn. I thank all of my thesis committee members: John La erty from Carnegie Mellon University, Aravind Joshi, Lyle Ungar, and Mark Liberman, for their extremely valuable suggestions and comments about my thesis research. I thank Mike Collins, Jason Eisner, and Dan Melamed, with whom I&#039;ve had many stimulating and impromptu discussions in the LINC lab. Iowe them much gratitude for their valuable feedback onnumerous rough drafts of papers and thesis chapters.
625|Adaptive Statistical Language Modeling: A Maximum Entropy Approach|Language modeling is the attempt to characterize, capture and exploit regularities in natural language. In statistical language modeling, large amounts of text are used to automatically determine the model&#039;s parameters. Language modeling is useful in automatic speech recognition, machine translation, and any other application that processes natural language with incomplete knowledge.  In this thesis, I view language as an information source which emits a stream of symbols from a finite alphabet (the vocabulary). The goal of language modeling is then to identify and exploit sources of information in the language stream, so as to minimize its perceived entropy.  Most existing statistical language models exploit the immediate past only. To extract information from further back in the document&#039;s history, I use trigger pairs as the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse.  Next, statistical evidence from many sources must...
626|Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition|This paper describes a novel statistical namedentity (i.e. &#034;proper name&#034;) recognition system built around a maximum enti W framework. By working within the framework of maximum entropy. theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. These knowledge sources include capitalization features, lexical features, features in- dicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-wtrd terms. The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems. However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published.
627|Information Extraction Using Hidden Markov Models|This thesis shows how to design and tune a hidden Markov model to extract factual information from a corpus of machine-readable English prose. In particular, the thesis presents a HMM that classifies and parses natural language assertions about genes being located at particular positions on chromosomes. The facts extracted by this HMM can be inserted into biological databases. The HMM is trained on a small set of sentence fragments chosen from the collected scientific abstracts in the OMIM (On-Line Mendelian Inheritance in Man) database and judged to contain the target binary relationship between gene names and gene locations. Given a novel sentence, all contiguous fragments are ranked by log-odds score, i.e. the log of the ratio of the probability of the fragment according to the target HMM to that according to a &#034;null&#034; HMM trained on all OMIM sentences. The most probable path through the HMM gives bindings for the annotations with precision as high as 80%. In contrast with traditional natural language processing methods, this stochastic approach makes no use either of part-of-speech taggers or dictionaries, instead employing non-emitting states to assemble modules roughly corresponding to noun, verb, and prepostional phrases. Algorithms for reestimating parameters for HMMs with non-emitting states are presented in detail. The ability to tolerate new words and recognize a wide variety of syntactic forms arises from the judicious use of &#034;gap&#034; states.
628|Question Answering from Frequently-Asked Question Files: Experiences with the FAQ Finder System|This paper describes FAQ Finder, a natural language question-answering system that uses files of frequently-asked questions as its knowledge base. Unlike AI question-answering systems that focus on the generation of new answers, FAQ Finder retrieves existing ones found in frequently-asked question files. Unlike information retrieval approaches that rely on a purely lexical metric of similarity between query and document, FAQ Finder uses a semantic knowledge base (WordNet) to improve its ability to match question and answer. We describe the design considerations that have entered into the system and various experiments that influence the system&#039;s current implementation. We include results from an evaluation of the system&#039;s performance against a corpus of user questions, and show that a combination of semantic and statistical techniques works better than any single approach. Introduction  In the vast information space of the Internet, individuals and groups have created small pockets of ...
629|Efficient Sampling and Feature Selection in Whole Sentence Maximum Entropy Language Models  |Conditional Maximum Entropy models have been successfully
630|Markov Processes on Curves for Automatic Speech Recognition|We investigate a probabilistic framework for automatic speech recognition based on the intrinsic geometric properties of curves. In particular, we analyze the setting in which two variables---one continuous (x), one discrete (s)---evolve jointly in time. We suppose that the vector x traces out a smooth multidimensional curve and that the variable s evolves stochastically as a function of the  arc length traversed along this curve. Since arc length does not depend on the rate at which a curve is traversed, this gives rise to a family of Markov processes whose predictions, Pr[sjx], are invariant to nonlinear warpings of time. We describe the use of such models, known as Markov processes on curves (MPCs), for automatic speech recognition, where x are acoustic feature trajectories and s are phonetic transcriptions. On two tasks---recognizing New Jersey town names and connected alpha-digits---we find that MPCs yield lower word error rates than comparably trained hidden Markov models. 1 Intr...
631|Impulses and Physiological States in Theoretical Models of Nerve Membrane|ABSTRACT Van der Pol&#039;s equation for a relaxation oscillator is generalized by the addition of terms to produce a pair of non-linear differential equations with either a stable singular point or a limit cycle. The resulting &#034;BVP model &#034; has two variables of state, representing excitability and refractoriness, and qualitatively resembles Bonhoeffer&#039;s theoretical model for the iron wire model of nerve. This BVP model serves as a simple representative of a class of excitable-oscillatory systems including the Hodgkin-Huxley (HH) model of the squid giant axon. The BVP phase plane can be divided into regions corresponding to the physio-logical states of nerve fiber (resting, active, refractory, enhanced, depressed, etc.) to form a &#034;physiological state diagram, &#034; with the help of which many physiological phenomena can be summarized. A properly chosen projection from the 4-dimensional HH phase space onto a plane produces a similar diagram which shows the underlying relationship between the two models. Impulse trains occur in the BVP and HH models for a range of constant applied currents which make the singular point representing the resting state unstable.
632|A quantitative description of membrane currents and its application to conduction and excitation in nerve|This article concludes a series of papers concerned with the flow of electric current through the surface membrane of a giant nerve fibre (Hodgkin, Huxley &amp; Katz, 1952; Hodgkin &amp; Huxley, 1952 a-c). Its general object is to discu the results of the preceding papers (Part I), to put them into mathematical form (Part II) and to show that they will account for con-duction and excitation in quantitative terms (Part III). PART I. DISCUSSION OF EXPERIMENTAL RESULTS The results described in the preceding papers suggest that the electrical behaviour of the membrane may be represented by the network shown in Fig. 1. Current can be carried through the membrane either by charging the membrane capacity or by movement of ion-s through the resistances in parallel with the capacity. The ionic current is divided into components carried by sodium and potassium ions (INa and IK), and a small &#039;leakage current &#039; (I,) made up by chloride and other ions. Each component of the ionic current is determined by a driving force which may conveniently be measured as an
633|Activation of passive iron as a model for the excitation of nerve|If a piece of passive iron in concentrated nitric acid is touched momentarily with a zinc rod, the iron may become active and the activation may spread from the point of contact over the whole piece of iron. Whether the iron will become active, and if it becomes active, whether it will return to its former passive state
634|ELECTROKINETIC MEMBRANE PROCESSES IN RELATION TO PROPERTIES OF EXCITABLE TISSUES II. Sou- ~ &amp;quot;I&#039;m~Om~TXCAL CONSIDERATIONS|A quantitative theory is presented /or the behavior of a membrane-electrolyte system subject to an electric current flow (the &amp;quot;membrane oscillator&amp;quot;). If the mem-brane is porous, carries &amp;quot;fixed charges, &amp;quot; and separates electrolyte solutious of differ-ent conductances, it can be the site of repetitive osculatory changes in the mefiabrane potential, the membrane resistance, and the hydrostatic pressure difference across the membrane. These events are accompanied by a pulsating transport of bulk solutions. The theory assumes the superposition of electrochemical and hydrostatic gradients and centers round the kinetics of resistance changes within the membrane, as caused by effects from diffusion and electro-osmotic fluid streaming. The results are laid down in a set of five simple, basic expressions, which can be transformed into a pair of non-llnear differential equations yielding oscillatory solutions. A graphical inte-gration method is also outlined (Appendix II). The agreement between the theory and previous experimental observations is satisfactory. The applied electrokinetic concepts may have importance in relation to
636|AN n 5/2 ALGORITHM FOR MAXIMUM MATCHINGS IN BIPARTITE GRAPHS|The present paper shows how to construct a maximum matching in a bipartite graph with n vertices and m edges in a number of computation steps proportional to (m + n)x/.
637|E-Cell : Software environment for whole cell simulation|We present E-CELL, a generic computer software environment for modeling a cell and conducting experiments in silico. The E-CELL system allows a user to de ne functions of proteins, protein-protein interactions, protein-DNA interactions, regulation of gene expression and other features of cellular metabolism, in terms of a set of reaction rules. The system then executes those reactions iteratively, and the user can observe, through a computer display, dynamic changes in concentrations of proteins, protein complexes and other chemical compounds in the cell. Using this software, we constructed a model of a hypothetical cell with only 127 genes su cient for transcription, translation, energy production and phospholipid synthesis. Most of the genes are taken from Mycoplasma genitalium, the organism having the smallest known chromosome, whose complete 580kb genome sequence was determined at TIGR in 1995. We discuss future applications of the E-CELL system with special respect to genome engineering.
638|Conservation analysis in biochemical networks: computational issues for software writers|Large scale genomic studies are generating significant amounts of data on the structure of cellular networks. This is in contrast to kinetic data which is frequently absent, unreliable or fragmentary. There is therefore a desire by many in the community to investigate the potential rewards of analyzing the more readily available topological data. This brief review is concerned with a particular property of biological networks, namely structural conservations (e.g. moiety conserved cycles). There has been much discussion in the literature on these cycles but a review on the computational issues related to conserved cycles has been missing 1. This review is concerned with the detection and characterization of conservation relations in arbitrary networks and related issues which impinge on simulation simulation software writers. This review will not address flux balance constraints or small-world type analyses in any significant detail. Contact:
639|Markov Random Field Models in Computer Vision|. A variety of computer vision problems can be optimally posed as Bayesian labeling in which the solution of a problem is defined as the maximum a posteriori (MAP) probability estimate of the true labeling. The posterior probability is usually derived from a prior model and a likelihood model. The latter relates to how data is observed and is problem domain dependent. The former depends on how various prior constraints are expressed. Markov Random Field Models (MRF) theory is a tool to encode contextual constraints into the prior probability. This paper presents a unified approach for MRF modeling in low and high level computer vision. The unification is made possible due to a recent advance in MRF modeling for high level object recognition. Such unification provides a systematic approach for vision modeling based on sound mathematical principles. 1 Introduction  Since its beginning in early 1960&#039;s, computer vision research has been evolving from heuristic design of algorithms to syste...
641|Constructing Simple Stable Descriptions for Image Partitioning|A new formulation of the image partitioning problem is presented: construct a complete and stable description of an image, in terms of a specified descriptive language, that is simplest in the sense of being shortest. We show that a descriptive language limited to a low-order polynomial description of the intensity variation within each region and a chain-code-like description of the region boundaries yields intuitively satisfying partitions for a wide class of images. The advantage of this formulation is that it can be extended to deal with subsequent steps of the image-understanding problem (or to deal with other image attributes, such as texture) in a natural way by augmenting the descriptive language. Experiments performed on a variety of both real and synthetic images demonstrate the superior performance of this approach over partitioning techniques based on clustering vectors of local image attributes and standard edge-detection techniques. 1 Introduction  The partitioning proble...
642|A Markov Random Field Model for Object Matching under Contextual Constraints|This paper presents a Markov random field (MRF) model for object recognition in high level vision. The labeling state of a scene in terms of a model object is considered as an MRF or couples MRFs. Within the Bayesian framework, the optimal solution is defined as the maximum a posteriori (MAP) estimate of the MRF. The posterior distribution is derived based on sound mathematical principles from theories of MRF and probability, which is in contrast to heuristic formulations. An experimental result is presented. 1 Introduction  In object recognition, an object is usually represented by a set of primitives or features. These features are attributed by their properties and are constrained to one another by contextual inter-relations. Two issues must be addressed for successful recognition: how to use contextual constraints effectively and how to deal with uncertainties. Markov random field (MRF) theory provides a way of encoding contextual constraints. Since 1980&#039;s, there has been considera...
643|Toward 3D Vision from Range Images: An Optimization Framework and Parallel Networks |We propose a unified approach to solve low, intermediate and high level computer vision problems for 3D object recognition from range images. All three levels of computation are cast in an optimization framework and can be implemented on neural network style architecture. In the low level computation, the tasks are to estimate curvature images from the input range data. Subsequent processing at the intermediate level is concerned with segmenting these curvature images into coherent curvature sign maps. In the high level, image features are matched against model features based on an object description called  attributed relational graph (ARG). We show that the above computational tasks at each of the three different levels can all be formulated as optimizing a two-term energy function. The first term encodes unary constraints while the second term binary ones. These energy functions are minimized using parallel and distributed relaxation-based algorithms which are well suited for neural...
644|Toward a model of text comprehension and production|The semantic structure of texts can be described both at the local microlevel and at a more global macrolevel. A model for text comprehension based on this notion accounts for the formation of a coherent semantic text base in terms of a cyclical process constrained by limitations of working memory. Furthermore, the model includes macro-operators, whose purpose is to reduce the information in a text base to its gist, that is, the theoretical macrostructure. These opera-tions are under the control of a schema, which is a theoretical formulation of the comprehender&#039;s goals. The macroprocesses are predictable only when the control schema can be made explicit. On the production side, the model is con-cerned with the generation of recall and summarization protocols. This process is partly reproductive and partly constructive, involving the inverse operation of the macro-operators. The model is applied to a paragraph from a psycho-logical research report, and methods for the empirical testing of the model are developed. The main goal of this article is to describe the system of mental operations that underlie the processes occurring in text comprehension and in the production of recall and summariza-tion protocols. A processing model will be outlined that specifies three sets of operations. First, the meaning elements of a text become
