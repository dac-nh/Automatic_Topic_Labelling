ID|Title|Summary
1|Querying Semi-Structured Data|

2|The Lorel Query Language for Semistructured Data|We present the Lorel language, designed for querying semistructured data. Semistructured data is becoming more and more prevalent, e.g., in structured documents such as HTML and when performing simple integration of data from multiple sources. Traditional data models and query languages are inappropriate, since semistructured data often is irregular, some data is missing, similar concepts are represented using different types, heterogeneous sets are present, or object structure is not fully known. Lorel is a user-friendly language in the SQL/OQL style for querying such data effectively. For wide applicability, the simple object model underlying Lorel can be viewed as an extension of ODMG and the language as an extension of OQL. The main novelties of the Lorel language are: (i) extensive use of coercion to relieve the user from the strict typing of OQL, which is inappropriate for semistructured data
3|DataGuides: Enabling Query Formulation and Optimization in Semistructured Databases|In semistructured databases there is no schema fixed  in advance. To provide the benefits of a schema in  such environments, we introduce DataGuides:  concise and accurate structural summaries of  semistructured databases. DataGuides serve as  dynamic schemas, generated from the database; they  are useful for browsing database structure,  formulating queries, storing information such as  statistics and sample values, and enabling query  optimization. This paper presents the theoretical  foundations of DataGuides along with an algorithm  for their creation and an overview of incremental  maintenance. We provide performance results based  on our implementation of DataGuides in the Lore  DBMS for semistructured data. We also describe the  use of DataGuides in Lore, both in the user interface  to enable structure browsing and query formulation,  and as a means of guiding the query processor and  optimizing query execution.
4|Object exchange across heterogeneous information sources|We address the problem of providing integrated access to diverse and dynamic information sources. We explain how this problem differs from the traditional database integration problem and we focus on one aspect of the information integration problem, namely information exchange. We define an object-based information exchange model and a corresponding query language that we believe are well suited for integration of diverse information sources. We describe how, the model and language have been used to integrate heterogeneous bibliographic information sources. We also describe two general-purpose libraries we have implemented for object exchange between clients and servers.
5|Answering queries using views|We consider the problem of computing answers to queries by using materialized views. Aside from its potential in optimizing query evaluation, the problem also arises in applications such as Global Information Systems, Mobile Computing and maintaining physical data independence. We consider the problem of nding a rewriting of a query that uses the materialized views, the problem of nding minimal rewritings, and nding complete rewritings (i.e., rewritings that use only the views). We show that all the possible rewritings can be obtained by considering containment mappings from the views to the query, and that the problems we consider are NP-complete when both the query and the views are conjunctive and don&#039;t involve built-in comparison predicates. We show that the problem has two independent sources of complexity (the number of possible containment mappings, and the complexity of deciding which literals from the original query can be deleted). We describe a polynomial time algorithm for nding rewritings, and show that under certain conditions, it will nd the minimal rewriting. Finally, we analyze the complexity of the problems when the queries and views may be disjunctive and involve built-in comparison predicates. 1
6|A Query Language and Optimization Techniques for Unstructured Data|A new kind of data model has recently emerged in which the database is not constrained by a conventional schema. Systems like ACeDB, which has become very popular with biologists, and the recent Tsimmis proposal for data integration organize data in tree-like structures whose components can be used equally well to represent sets and tuples. Such structures allow great flexibility in data representation  What query language is appropriate for such structures? Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or &#034;vertical&#034; dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the &#034;horizontal&#034; dimension of UnQL.  
7|Lore: A database management system for semistructured data|Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources. 
8|Maintenance of Materialized Views: Problems, Techniques, and Applications|In this paper we motivate and describe materialized views, their applications, and the problems  and techniques for their maintenance. We present a taxonomy of view maintenanceproblems  basedupon the class of views considered, upon the resources used to maintain the view, upon the types of modi#cations to the base data that areconsidered during maintenance, and whether the technique works for all instances of databases and modi#cations. We describe some of the view maintenancetechniques proposed in the literature in terms of our taxonomy. Finally, we consider new and promising application domains that are likely to drive work in materialized  views and view maintenance.  1 Introduction  What is a view? A view is a derived relation de#ned in terms of base #stored# relations. A view thus de#nes a function from a set of base tables to a derived table; this function is typically recomputed every time the view is referenced.  What is a materialized view? A view can be materialized by storin...
9|Objects and Views|Object-oriented databases have been introduced primarily to ease the development of database applications. However, the difficulties encountered when, for instance, trying to restructure data or integrate databases demonstrate that the models being used still lack flexibility. We claim that the natural way to overcome these shortcomings is to introduce a sophisticated view mechanism. This paper presents such a mechanism, one which allows a programmer to restructure the class hierarchy and modify the behavior and structure of objects. The mechanism allows a programmer to specify attribute values implicitly, rather than storing them. It also allows him to introduce new classes into the class hierarchy. These virtual classes are populated by selecting existing objects from other classes and by creating new objects. Fixing the identify of new objects during database updates introduces subtle issues into view design. Our presentation, mostly informal, leans on a number of illustrative examples meant to emphasize the simplicity of our mechanism. 1
10|Object fusion in mediator systems|One of the main tasks of mediators is to fuse information from heterogeneous information sources. This may involve, for example, removing redundancies, and resolving inconsistencies in favor of the most reliable source. The problem becomes harder when the sources are unstructured/semistructured and we do not have complete knowledge of their contents and structure. In this paper we show how many common fusion operations can be specified non-procedurally and succinctly. The key to our approach is to assign semantically meaningful object ids to objects as they are &amp;quot;imported &amp;quot; into the mediator.
11|Regular Path Queries with Constraints|The evaluation of path expression queries on semistructured data in a distributed asynchronous environment is considered. The focus is on the use of local information expressed in the form of path constraints in the optimization of path expression queries. In particular, decidability and complexity results on the implication problem for path constraints are established.
12|Supporting Views in Object-Oriented Databases|  Relational database systems provide a powerful abstraction mechanism: any query, since it returns a relation, can be used to define a view, that becomes a derived (or virtual) relation. Views are defined by a statement such as &#034;define view &lt;name&gt; as &lt;query&gt;.&#034; Views can be used to tailor the global database schema.  . Query formulation is simplified, if frequent subexpressions are predefined.  . Application programs are insulated from changes to the underlying schema.  . Information can be restructured to better suit an application programs&#039; requirements.  . Derived information is kept consistent with base data.  . Access restrictions (for authorization) can be enforced by hiding data.  In contrast to base relations, views are typically not stored permanently, but rather computed on demand. Queries to view relations are modified by query substitution so as to operate on the underlying b
13|MultiView: A Methodology for Supporting Multiple Views in Object-Oriented Databases|A view in object-oriented databases (OODB) corresponds to virtual schema graph with possibly restructured generalization and decomposition hierarchies. We propose a methodology, called MultiView, for supporting multiple such view schemata. MultiView represents a simple yet powerful approach achieved by breaking view specification into independent tasks: class derivation, global schema integration, view class selection, and view schema generation. Novel features of MultiView include an object algebra for class customization; an algorithm for the integration of virtual classes into the global schema; a view definition language for view class selection, and the automatic generation of a view class hierarchy. In addition, we present algorithms that verify the closure property of a view and, if found to be incomplete, transform it into a closed, yet minimal, view. Lastly, we introduce the fundamental concept of view independence and show  MultiView to be view independent.  
14|The GMAP: a versatile tool for physical data independence|.&lt;F3.733e+05&gt; Physical data independence is touted as a central feature of modern database systems. It allows users to frame queries in terms of the logical structure of the data, letting a query processor automatically translate them into optimal plans that access physical storage structures. Both relational and object-oriented systems, however, force users to frame their queries in terms of a logical schema that is directly tied to physical structures. We present an approach that eliminates this dependence. All storage structures are defined in a declarative language based on relational algebra as functions of a logical schema. We present an algorithm, integrated with a conventional query optimizer, that translates queries over this logical schema into plans that access the storage structures. We also show how to compile update requests into plans that update all relevant storage structures consistently and optimally. Finally, we report on experiments with a prototype implementation ...
15|Virtual Schemas and Bases|We propose the notions of virtual schemas and virtual bases as a coherent way of integrating various features in OODB views. A virtual schema is defined based on some existing (real) schema. A virtual base is obtained when a (real) base is attached to a virtual schema. We study the consequences of this simple assumption. In particular, we observe the differences between a real schema and a virtual one. We also consider an extension (that we call generic schemas) where it is necessary to specify several real bases to attach data to a virtual schema. We show how the flexibility provided by virtual schemas can be used to cope with various dynamic features of database systems. 1 Introduction Views are intended to increase the flexibility of database systems and their definition in the object-oriented database (OODB) context comes as a natural extension of the original paradigm. The yet relatively young research on this topic has introduced a large variety of indispensable new features. H...
16|Querying object-oriented databases|We present a novel language for querying object-oriented databases. The language is built around the idea of extended path expressions that substantially generalize [ZAN83], and on an adaptation of the first-order formalization of object-oriented languages from [KW89, KLW90, KW92]. The language incorporates features not found in earlier proposals; it is easier to use and has greater expressive power. Some of the salient features of our language are: ffl Precise model-theoretic semantics. ffl A very expressive form of path expressions that not only can do joins, selections and unnesting, but can also be used to explore the database schema. ffl Views can be defined and manipulated in a much more uniform way than in other proposals. ffl Database schema can be explored in the very same language that is used to retrieve data. Unlike in relational languages, the user needs not know anything about the system tables that store schema information. ffl The notions of a type and type-correctness have precise meaning. It accommodates a wide variety of queries that might be deemed well- or ill-typed under different circumstances. In particular, we show that there is more than one way of settling the issue of type correctness. For expository purposes and due to space limitation, we chose to make a number of simplifying assumptions and left some features out. A more complete account can be found in [KSK92].
17|Logical foundations of object-oriented and frame-based languages|We propose a novel formalism, called Frame Logic (abbr., F-logic), that accounts in a clean and declarative fashion for most of the structural aspects of object-oriented and frame-based languages. These features include object identity, complex objects, inheritance, polymorphic types, query methods, encapsulation, and others. In a sense, F-logic stands in the same relationship to the objectoriented paradigm as classical predicate calculus stands to relational programming. F-logic has a model-theoretic semantics and a sound and complete resolution-based proof theory. A small number of fundamental concepts that come from object-oriented programming have direct representation in F-logic; other, secondary aspects of this paradigm are easily modeled as well. The paper also discusses semantic issues pertaining to programming with a deductive object-oriented language based on a subset of F-logic.  
18|F-Logic: A Higher-Order Language for Reasoning about Objects, Inheritance and Scheme|All in-text	references	underlined	in	blue	are	linked	to	publications	on	ResearchGate, letting you	access	and	read	them	immediately.
19|A clash of intuitions: The current state of nonmonotonic multiple inheritance systems|Abstract: Early attempts at combining multiple inheritance with nonmonotonic reasoning were based on straight forward extensions of tree-structured inheritance systems, and were theoretically unsound. In The Mathematics of Inheritance Systems, or TMOIS, Touretzky described two problems these systems cannot handle: reasoning in the presence of true but redundant assertions, and coping with ambiguity. TMOIS provided a definition and analysis of a theoretically sound multiple inheritance system, accompanied by inference algorithms. Other definitions for inheritance have since been proposed that are equally sound and intuitive, but do not always agree with TMOIS. At the heart of the controversy is a clash of intuitions about certain fundamental issues such as skepticism versus credulity, the direction in which inheritance paths are extended, and classical versus intuitive notions of consistency. Just as there are alternative logics, there may be no single &#034;best&#034; approach to nonmonotonic multiple inheritance. 1.
20|The logic of inheritance in frame systems|This paper shows how the semantics of frames with exceptions can be described logically. We define a simple (purely declarative) frame language allowing for multiple inheritance and meta classes (i.e. the instances of a class may be classes themselves). Expressions of this language are translated into first order formulas. Circumscription of a certain predicate in the resulting theory yields the desired semantics. Our approach allows the intuition that subclasses should override superclasses to be represented in a very natural way. Inheritance systems have a long tradition in AI. They allow the description of hierarchies of objects and
21|A First-Order Theory of Types and Polymorphism in Logic Programming|We describe a new logic called typed predicate calculus (T PC) that gives declarative meaning to logic programs with type declarations and type inference. T PC supports all popular types of polymorphism, such as parametric, inclusion, and ad hoc polymorphism. The proper interaction between parametric and inclusion varieties of polymorphism is achieved through a new construct, called type dependency, which is reminiscent of implication types of [PR89] but yields more natural and succinct specifications. Unlike other proposals where typing has extra-logical status, in T PC the notion of type-correctness has precise model-theoretic meaning that is independent of any specific type-checking or type-inference procedure. Moreover, many different approaches to typing that were proposed in the past can be studied and compared within the framework of our logic. As an illustration, we apply T PC to interpret and compare the results reported in [MO84, Smo88, HT90, Mis84, XW88]. Another novel featu...
22|Query evaluation techniques for large databases|Database management systems will continue to manage large data volumes. Thus, efficient algorithms for accessing and manipulating large sets and sequences will be required to provide acceptable performance. The advent of object-oriented and extensible database systems will not solve this problem. On the contrary, modern data models exacerbate it: In order to manipulate large sets of complex objects as efficiently as today’s database systems manipulate simple records, query processing algorithms and software will become more complex, and a solid understanding of algorithm and architectural issues is essential for the designer of database management software. This survey provides a foundation for the design and implementation of query execution facilities in new database management systems. It describes a wide array of practical query evaluation techniques for both relational and post-relational database systems, including iterative execution of complex query evaluation plans, the duality of sort- and hash-based set matching algorithms, types of parallel query execution and their implementation, and special operators for emerging database application domains.
23|Space/Time Trade-offs in Hash Coding with Allowable Errors|this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash- coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency
24|The ubiquitous B-tree|B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and nnplemented using B-trees This paper reviews B-trees and shows why they have been so successful It discusses the major variations of the B-tree, especially the B+-tree,
25|Parallel database systems: the future of high performance database systems|Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1.
26|Security-control methods for statistical databases: a comparative study|This paper considers the problem of providing security to statistical databases against disclosure of confidential information. Security-control methods suggested in the literature are classified into four general approaches: conceptual, query restriction, data perturbation, and output perturbation. Criteria for evaluating the performance of the various security-control methods are identified. Security-control methods that are based on each of the four approaches are discussed, together with their performance with respect to the identified evaluation criteria. A detailed comparative analysis of the most promising methods for protecting dynamic-online statistical databases is also presented. To date no single security-control method prevents both exact and partial disclosures. There are, however, a few perturbation-based methods that prevent exact disclosure and enable the database administrator to exercise “statistical disclosure control. ” Some of these methods, however introduce bias into query responses or suffer from the O/l query-set-size problem (i.e., partial disclosure is possible in case of null query set or a query set of size 1). We recommend directing future research efforts toward developing new methods that prevent exact disclosure and provide statistical-disclosure control, while at the same time do not suffer from the bias problem and the O/l query-set-size problem. Furthermore, efforts directed toward developing a bias-correction mechanism and solving the general problem of small query-set-size would help salvage a few of the current perturbation-based methods.
27|The OO7 benchmark|The OO7 Benchmark represents a comprehensive test of OODBMS performance. In this report we describe the benchmark and present performance results from its implementation in four OODB systems. It is our hope that the OO7 Benchmark will provide useful insight for end-users evaluating the performance of OODB systems; we also hope that the research community will nd that OO7 provides a database schema, instance, and workload that is useful for evaluating new techniques and algorithms for OODBMS implementation.
28|The Gamma database machine project|This paper describes the design of the Gamma database machine and the techniques employed in its implementation. Gamma is a relational database machine currently operating on an Intel iPSC/2 hypercube with 32 processors and 32 disk drives. Gamma employs three key technical ideas which enable the architecture to be scaled to 100s of processors. First, all relations are horizontally partitioned across multiple disk drives enabling relations to be scanned in parallel. Second, novel parallel algorithms based on hashing are used to implement the complex relational operators such as join and aggregate functions. Third, dataflow scheduling techniques are used to coordinate multioperator queries. By using these techniques it is possible to control the execution of very complex queries with minimal coordination- a necessity for configurations involving a very large number of processors. In addition to describing the design of the Gamma software, a thorough performance evaluation of the iPSC/2 hypercube version of Gamma is also presented. In addition to measuring the effect of relation size and indices on the response time for selection, join, aggregation, and update queries, we also analyze the performance of Gamma relative to the number of processors employed when the sizes of the input relations are kept constant (speedup) and when the sizes of the input relations are increased proportionally to the number of processors (scaleup). The speedup results obtained for both selection and join queries are linear; thus, doubling the number of processors
29|System R: Relational Approach to Database Management|System R is a database management system which provides a high level relational data interface. The system provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment. This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product.
30|Physical database design for relational databases|This paper describes the concepts used in the implementation of DBDSGN, an experimental physical design tool for relational databases developed at the IBM San Jose Research Laboratory. Given a workload for System R (consisting of a set of SQL statements and their execution frequencies), DBDSGN suggests physical configurations for efficient performance. Each configuration consists of a set of indices and an ordering for each table. Workload statements are evaluated only for atomic configurations of indices, which have only one index per table. Costs for any configuration can be obtained from those of the atomic configurations. DBDSGN uses information supplied by the System R optimizer both to determine which columns might be worth indexing and to obtain estimates of the cost of executing statements in different configurations. The tool finds efficient solutions to the index-selection problem; if we assume the cost estimates supplied by the optimizer are the actual execution costs, it finds the optimal solution. Optionally, heuristics can be used to reduce execution time. The approach taken by DBDSGN in solving the index-selection problem for multiple-table statements significantly reduces the complexity of the problem. DBDSGN’s principles were used in the Relational Design Tool (RDT), an IBM product based on DBDSGN, which performs design for SQL/DS, a relational system based on System R. System R actually uses DBDSGN’s suggested solutions as the tool expects because cost estimates and other necessary information can be obtained from System R using a new SQL statement, the EXPLAIN statement. This illustrates how a system can export a model of its internal assumptions and behavior so that other systems (such as tools) can share this model.
31|Parallel sorting on a shared-nothing architecture using probabilistic splitting|We consider the problem of external sorting in a shared-nothing multiprocessor. A critical step in the algorithms we consider is to determine the range of sort keys to be handled by each processor. We consider two techniques for determining these ranges of sort keys: exact splitting, using a parallel version of the algorithm proposed by Iyer, Ricard, and Varman; and probabilistic splitting, which uses sampling to estimate quantiles. We present analytic results showing that probabilistic splitting performs better than exact splitting. Finally, we present experimental results from an implementation of sorting via probabilistic splitting in the Gamma parallel database machine. 
32|An Evaluation of Non-Equijoin Algorithms|A non-equijoin of relations R and S is a band join if the join predicate requires values in the join attribute of R to fall within a speci ed band about the values in the join attribute of S. We propose a new algorithm, termed a partitioned band join, for evaluating band joins. We present a comparison between the partitioned band join algorithm and the classical sort-merge join algorithm (optimized for band joins) using both an analytical model and an implementation on top of the WiSS storage system. The results show that the partitioned band join algorithm outperforms sortmerge unless memory is scarce and the operands of the join are of equal size. We also describe a parallel implementation of the partitioned band join on the Gamma database machine, and present data from speedup and scaleup experiments demonstrating that the partitioned band join is efficiently parallelizable. 
33|Parallel Database Systems: The Future of Database Processing or a Passing Fad?|Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems.
34|Dynamic Hashing Schemes|A new type of dynamic file access called dynamic hushing has recently emerged. It promises the flexibility of handling dynamic tiles while preserving the fast access times expected from hashing. Such a fast, dynamic file access scheme is needed to support modern database systems. This paper surveys dynamic hashing schemes and examines
35|ParaDiGM: A Highly Scalable Shared-Memory Multi-Computer Architecture|ParaDiGM is a highly scalable shared-memory multi-computer architecture. It is being developed to demonstrate the feasibility of building a relatively low-cost shared-memory parallel computer that scales to large configurations, and yet provides sequential programs with performance comparable to a high-end microprocessor. A key problem is building a scalable memory hierarchy. In this paper we describe the ParaDiGM architecture, highlighting the innovations of our approach and presenting results of our evaluation of the design. We envision that scalable shared-memory multiprocessors like ParaDiGM will soon become the dominant form of parallel processing, even for very large-scale computation, providing a uniform platform for parallel programming systems and applications. 1 Introduction ParaDiGM (PARAllel DIstributed Global Memory) 1 is a highly scalable shared-memory multi-computer architecture. By multi-computer, we mean a system interconnected by both bus and network technology. B...
36|An indexing technique for object-oriented databases|We present an indexing technique providing efficient support for object-oriented queries. We first briefly discuss object-oriented query languages. Then, we de-scribe the indexing technique and the associated op-erations. A preliminary comparison of our technique with previously defined access mechanisms is also pre-sented. 1
37|A survey of parallel execution strategies for transitive closure and logic programs|An important feature of database technology of the nineties is the use of parallelism for speeding up the execution of complex queries. This technology is being tested in several experimental database architectures and a few commercial systems for conventional select-project-join queries. In particular, hash-based fragmentation is used to distribute data to disks under the control of different processors in order to perform selections and joins in parallel. With the development of new query languages, and in particular with the definition of transitive closure queries and of more general logic programming queries, the new dimension of recursion has been added to query processing. Recursive queries are complex; at the same time, their regular structure is particularly suited for parallel execution, and parallelism may give a high efficiency gain. We survey the approaches to parallel execution of recursive queries that have been presented in the recent literature. We observe that research on parallel execution of recursive queries is separated into two distinct subareas, one focused on the transitive closure of Relational Algebra expressions, the other one focused on optimization of more general Datalog queries. Though the subareas seem radically different because of the approach and formalism used, they have many common features. This is not surprising, because most typical Datalog queries can be solved by means of the transitive closure of simple
38|Spatial Priority Search: An Access Technique for Scaleless Maps|In geographic information systems, an important goal is the maintenance of seamless, scaleless maps. The amount of detail desired on a map decreases with decreasing scale. Cartographic techniques called generalization  define the representations of geographic objects, depending on the scale. While generalization as a whole is considered an art, simple automatic generalization techniques exist for simple geometric objects. For polygonal lines and polygons, simplification techniques assign priorities to points. A map at a desired scale is then obtained by ignoring all points of sufficiently low priority. This implies that a geometric object appears on a map only if its priority is high enough, and also that an object is represented only by those of its defining points that have sufficiently high priority. The efficiency of retrieving a map of some area at a certain scale ideally should only depend on the amount of data retrieved.  In this paper, we present algorithms and a fully adaptive...
39|An Implementation for Nested Relational Databases|We propose an architecture for implementing nested re- In this paper we propose an implementation, ANDA’ lational databases. In particular, we discuss the storage for the Nested Relational Data Model(NRJ3M). In parstructures, their organization and an access language titular, we diicuaa the storage structures, their orgafor specifying access plans. nization, and an access language for specifying access The featurw of our implementation are: plans. The motivation for our design comes from these observations: A notation for hierarchical tuple identification. One value-driven indexing structure (VALTREE) for the entire database. A main-memory based component (CACHE) for ma-nipulating hierarchical tuple-identifiers. A hashing scheme (RECLISTS) for fast access to data specified by tuple-identifiers. An access language based on the VALTREE, the RECLIST and the CACHE to define access plans for execution of queries. Pemusaon to copy without fee all ar put of this granted pvided that the oopies are not made or disrritutcd for direct commercial advantage, the VLDB cop&amp;ht notice md the title of the public&amp;m and its date s~~eer, and notice L given
40|Database Operations in a Cube-Connected Multicomputer System|Parallel architectures for database processing should pro-  vide parallel I/O as well as parallel CPU capability. Two issues that arise in such systems are data combination and nonuniform data distribution.
41|From Structured Documents to Novel Query Facilities|Structured documents (e.g., SGML) can benefit a lot from database support and more specifically from object-oriented database (OODB) management systems. This paper describes a natural mapping from SGML documents into OODB&#039;s and a formal extension of two OODB query languages (one SQL-like and the other calculus) in order to deal with SGML document retrieval. Although motivated by structured documents, the extensions of query languages that we present are general and useful for a variety of other OODB applications. A key element is the introduction of paths as first class citizens. The new features allow to query data (and to some extent schema) without exact knowledge of the schema in a simple and homogeneous fashion. 1 Introduction  Structured documents are central to a wide class of applications such as software engineering, libraries, technical documentation, etc. They are often stored in file systems and document access tools are somewhat limited. We believe that (object-oriented) d...
42|Finding Regular Simple Paths In Graph Databases|We consider the following problem: given a labelled directed graph G and a regular expression R, find all pairs of nodes connected by a simple path such that the concatenation of the labels along the path satisfies R. The problem is motivated by the observation that many recursive queries in relational databases can be expressed in this form, and by the implementation of a query language, G+ , based on this observation. We show that the problem is in general intractable, but present an algorithm than runs in polynomial time in the size of the graph when the regular expression and the graph are free of conflicts. We also present a class of languages whose expressions can always be evaluated in time polynomial in the size of both the graph and the expression, and characterize syntactically the expressions for such languages. 
43|MedMaker: A Mediation System Based on Declarative Specifications|Mediators are used for integration of heterogeneous information sources. We present a system for declaratively specifying mediators. It is targeted for integration of sources with unstructured or semi-structured data and/or sources with changing schemas. We illustrate the main features of the Mediator Specification Language (MSL), show how they facilitate integration, and describe the implementation of the system that interprets the MSL specifications. 
44|Representative Objects: Concise Representations of Semistructured Hierarchical Data|In this paper we introduce the representative object, which uncovers the inherent schema(s) in semistructured, hierarchical data sources and provides a concise description of the structure of the data. Semistructured data, unlike data stored in typical relational or object-oriented databases, does not have fixed schema that is known in advance and stored separately from the data. Withthe rapid growth of the World Wide Web, semistructured hierarchical data sources are becoming widely available to the casual user. The lack of external schema information currently makes browsing and querying these data sources inefficient at best, and impossible at worst. We show how representative objects make schema discovery efficient and facilitate the generation of meaningful queries over the data. 1.
46|Evaluating Queries with Generalized Path Expressions|In the past few years, query languages featuring generalized path expressions have been proposed. These languages allow the interrogation of both data and structure. They are powerful and essential for a number of applications. However, until now, their evaluation has relied on a rather naive and inefficient algorithm. In this paper, we extend an object algebra with two new operators and present some interesting rewriting techniques for queries featuring generalized path expressions. We also show how a query optimizer can integrate the new techniques.  1 Introduction  In the past few years there has been a growing interest in query languages featuring generalized path expressions (GPE) [BRG88, KKS92, CACS94, QRS  +  95]. With these languages, one may issue queries on data without exact knowledge of its structure. A GPE queries data and structure at the same time. Although very useful for standard database applications, these languages are vital for new applications dedicated, for insta...
47|Integrating a Structured-Text Retrieval System with an Object-Oriented Database System|We describe the integration of a structured-text retrieval system (TextMachine) into an  object-oriented database system (OpenODB). Our approach is a light-weight one, using the external  function capability of the database system to encapsulate the text retrieval system as an  external information source. Yet, we are able to provide a tight integration in the query language  and processing; the user can access the text retrieval system using a standard database query  language. The efficient and effective retrieval of structured text performed by the text retrieval  system is seamlessly combined with the rich modeling and general-purpose querying capabilities  of the database system, resulting in an integrated system with querying power beyond those of  the underlying systems. The integrated system also provides uniform access to textual data in  the text retrieval system and structured data in the database system, thereby achieving information  fusion. We discuss the design and imple...
48|Efficient and Effective Querying by Image Content|In the QBIC (Query By Image Content) project we are studying methods to query large  on-line image databases using the images&#039; content as the basis of the queries. Examples of  the content we use include color, texture, and shape of image objects and regions. Potential  applications include medical (&#034;Give me other images that contain a tumor with a texture like this  one&#034;), photo-journalism (&#034;Give me images that have blue at the top and red at the bottom&#034;),  and many others in art, fashion, cataloging, retailing, and industry.  We describe a set of novel features and similarity measures allowing query by color, texture,  and shape of image object. We demonstrate the effectiveness of the QBIC system with normalized  precision and recall experiments on test databases containing over 1000 images and 1000  objects populated from commercially available photo clip art images, and of images of airplane  silhouettes. We also consider the efficient indexing of these features, specifically addre...
49|Color indexing|Computer vision is embracing a new research focus in which the aim is to develop visual skills for robots that allow them to interact with a dynamic, realistic environment. To achieve this aim, new kinds of vision algorithms need to be developed which run in real time and subserve the robot&#039;s goals. Two fundamental goals are determin-ing the location of a known object. Color can be successfully used for both tasks. This article demonstrates that color histograms of multicolored objects provide a robust, efficient cue for index-ing into a large database of models. It shows that color histograms are stable object representations in the presence of occlusion and over change in view, and that they can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and im-age histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm called Histogram Backprojection, which performs this task efficiently in crowded scenes. 1
50|Computer Vision|Driver inattention is one of the main causes of traffic accidents. Monitoring a driver to detect inattention is a complex problem that involves physiological and behavioral elements. Different approaches have been made, and among them Computer Vision has the potential of monitoring the person behind the wheel without interfering with his driving. In this paper I have developed a system that can monitor the alertness of drivers in order to prevent people from falling asleep at the wheel. The other main aim of this algorithm is to have efficient performance on low quality webcam and without the use of infrared light which is harmful for the human eye. Motor vehicle accidents cause injury and death, and this system will help to decrease the amount of crashes due to fatigued drivers. The proposed algorithm will work in three main stages. In first stage the face of the driver is detected and tracked. In the second stage the facial features are extracted for further processing. In last stage the most crucial parameter is monitored which is eye’s status. In the last stage it is determined that whether the eyes are closed or open. On the basis of this result the warning is issued to the driver to take a break.
51|Voronoi diagrams -- a survey of a fundamental geometric data structure|This paper presents a survey of the Voronoi diagram, one of the most fundamental data structures in computational geometry. It demonstrates the importance and usefulness of the Voronoi diagram in a wide variety of fields inside and outside computer science and surveys the history of its development. The paper puts particular emphasis on the unified exposition of its mathematical and algorithmic properties. Finally, the paper provides the first comprehensive bibliography on Voronoi diagrams and related structures.
52|The grid file: an adaptable, symmetric multikey file structure|Traditional file structures that provide multikey access to records, for example, inverted files, are extensions of file structures originally designed for single-key access. They manifest various deficiencies in particular for multikey access to highly dynamic files. We study the dynamic aspects of tile structures that treat all keys symmetrically, that is, file structures which avoid the distinction between primary and secondary keys. We start from a bitmap approach and treat the problem of file design as one of data compression of a large sparse matrix. This leads to the notions of a grid partition of the search space and of a grid directory, which are the keys to a dynamic file structure called the grid file. This tile system adapts gracefully to its contents under insertions and deletions, and thus achieves an upper hound of two disk accesses for single record retrieval; it also handles range queries and partially specified queries efficiently. We discuss in detail the design decisions that led to the grid file, present simulation results of its behavior, and compare it to other multikey access file structures.
53|New Techniques for Best-Match Retrieval|A scheme to answer best-match queries from a file containing a collection of objects is described. A best-match query is to find the objects in the file that are closest (according to some (dis)similarity measure) to a given target. Previous work [5, 331 suggests that one can reduce the number of comparisons required to achieve the desired results using the triangle inequality, starting with a data structure for the file that reflects some precomputed intrafile distances. We generalize the technique to allow the optimum use of any given set of precomputed intrafile distances. Some empirical results are presented which illustrate the effectiveness of our scheme, and its performance relative to previous algorithms.
54|The Skyline Operator|We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SQL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations (e.g., join and Top N).
55|Nearest Neighbor Queries|A frequently encountered type of query in Geographic Information Systems is to find the k nearest neighbor objects to a given point in space. Processing such queries requires substantially different search algorithms than those for location or range queries. In this paper we present an efficient branch-and-bound R-tree traversal algorithm to find the nearest neighbor object to a point, and then generalize it to finding the k nearest neighbors. We also discuss metrics for an optimistic and a pessimistic search ordering strategy as well as for pruning. Finally, we present the results of several experiments obtained using the implementation of our algorithm and examine the behavior of the metrics and the scalability of the algorithm. 
56|Combining fuzzy information from multiple systems (Extended Abstract)  (1996) |In a traditional database system, the result of a query is a set of values (those values that satisfy the query). In other data servers, such as a system with queries baaed on image content, or many text retrieval systems, the result of a query is a sorted list. For example, in the case of a system with queries based on image content, the query might aak for objects that are a particular shade of red, and the result of the query would be a sorted list of objects in the database, sorted by how well the color of the object matches that given in the query. A multimedia system must somehow synthesize both types of queries (those whose result is a set, and those whose result is a sorted list) in a consistent manner. In this paper we discuss the solution adopted by Garlic, a multimedia information system being developed at
57|On finding the maxima of a set of vectors|ASSTRACT. Let U1, U2,..., Ud be totally ordered sets and let V be a set of n d-dimensional vectors In U ~ X Us.. X Ud. A partial ordering is defined on V in a natural way The problem of finding all maximal elements of V with respect to the partial ordering ~s considered The computational com-plexity of the problem is defined to be the number of required comparisons of two components and is denoted by Cd(n). It is tnwal that C~(n)  = n- 1 and C,~(n) &lt; O(n 2) for d _ ~ 2 In this paper we show: (1) C2(n)  = O(n logan) for d = 2, 3 and Cd(n)  ~ O(n(log2n) ~-~) for d ~ 4, (2) C,t(n)&gt; _ flog2 n!l for d _&gt; 2 KEY WORDS AND PHRASES: maxima of a set of vectors, computattonal complexity, number of com-parisons, algorithm, recurrence CR CATEaOmES. 5.25, 5,31, 5.39 1.
58|Reducing the braking distance of an SQL query engine|In a recent paper, we proposed adding a STOP AFTER clause to SQL to permit the cardinality of a query result to be explicitly limited by query writers and query tools. We demonstrated the usefulness of having this clause, showed how to extend a traditional cost-based query optimizer to accommodate it, and demonstrated via DB2-based simulations that large performance gains are possible when STOP AFTER queries are explicitly supported by the database engine. In this paper, we present several new strategies for efficiently processing STOP AFTER queries. These strategies, based largely on the use of range partitioning techniques, offer significant additional savings for handling STOP AFTER queries that yield sizeable result sets. We describe classes of queries where such savings would indeed arise and present experimental measurements that show the benefits and tradeoffs associated with the new processing strategies. 1
59|Duplicate record elimination in large data files |The issue of duplicate elimination for large data files in which many occurrences of the same record may appear is addressed. A comprehensive cost analysis of the duplicate elimination operation is presented. This analysis is based on a combinatorial model developed for estimating the size of intermediate runs produced by a modified merge-sort procedure. The performance of this modified merge-sort procedure is demonstrated to be significantly superior to the standard duplicate elimination technique of sorting followed by a sequential pass to locate duplicate records. The results can also be used to provide critical input to a query optimizer in a relational database system.
60|SEEKing the Truth about Ad Hoc Join Costs|In this paper, we reexamine the results of prior work on methods for computing ad hoc joins. We  develop a detailed cost model for predicting join algorithm performance, and we use the model to develop  cost formulas for the major ad hoc join methods found in the relational database literature. We show  that various pieces of &#034;common wisdom&#034; about join algorithm performance fail to hold up when analyzed  carefully, and we use our detailed cost model to derive optimal buffer allocation schemes for each of  the join methods examined here. We show that optimizing their buffer allocations can lead to large  performance improvements, e.g., as much as a 400% improvement in some cases. We also validate our  cost model&#039;s predictions by measuring an actual implementation of each join algorithm considered. The  results of this work should be directly useful to implementors of relational query optimizers and query  processing systems.  1 Introduction  The join of two sets of tuples is a fundament...
61|Grouping and Duplicate Elimination: Benefits of Early Aggregation|Early aggregation is a technique for speeding up the processing of GROUP BY  queries by reducing the amount of intermediate data transferred between main memory and disk. It can also be applied to duplicate elimination because duplicate elimination is equivalent to grouping with no aggregation functions. This paper describes six different algorithms for grouping and aggregation, shows how to incorporate early aggregation in each of them, and analyzes the resulting reduction in intermediate data. In addition to the grouping algorithm used, the reduction depends on several factors: the number of groups, the skew in group size distribution, the input size, and the amount of main memory available. All six algorithms considered benefit from early aggregation with grouping by hash partitioning producing the least amount of intermediate data. If the group size distribution is skewed, the overall reduction can be very significant, even with a modest amount of additional main memory. 
62|Query Evaluation in CROQUE  -- Calculus and Algebra Coincide  | With the substantial change of declarative query languages from plain SQL to the so-called &#034;object SQLs&#034;, in particular OQL, there has surprisingly been not much change in the way problems of query representation and optimization for such languages are tackled. We identify some of the difficulties pure algebraic approaches experience when facing object models and the operations defined for them. Calculus-style formalisms suite this challenge better, but are said not to be efficiently implementable in the database context. This paper proposes a hybrid query representation and optimization approach, combining the strengths of a many-sorted query algebra and the monoid comprehension calculus. We show that efficient execution plans beyond nested-loop processing can be derived -- not only for oe-ß-1 queries -- in such a framework. The translation process accounts for queries manipulating bulk-typed values by employing various join methods of the database engine, as well as queries that us...
63|Tinydb: An acquisitional query processing system for sensor networks|We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. Categories and Subject Descriptors: H.2.3 [Database Management]: Languages—Query languages; H.2.4 [Database Management]: Systems—Distributed databases; query processing
64|Directed Diffusion: A scalable and robust communication paradigm for sensor networks|Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.
65|System architecture directions for networked sensors|Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world or spread throughout our environment. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances. 
66|ANALYSIS OF WIRELESS SENSOR NETWORKS FOR HABITAT MONITORING|  We provide an in-depth study of applying wireless sensor networks (WSNs) to real-world habitat monitoring. A set of system design requirements were developed that cover the hardware design of the nodes, the sensor network software, protective enclosures, and system architecture to meet the requirements of biologists. In the summer of 2002, 43 nodes were deployed on a small island off the coast of Maine streaming useful live data onto the web. Although researchers anticipate some challenges arising in real-world deployments of WSNs, many problems can only be discovered through experience. We present a set of experiences from a four month long deployment on a remote island. We analyze the environmental and node health data to evaluate system performance. The close integration of WSNs with their environment provides environmental data at densities previously impossible. We show that the sensor data is also useful for predicting system operation and network failures. Based on over one million 2 Polastre et. al. data readings, we analyze the node and network design and develop network reliability profiles and failure models.
68|The Cricket Location-Support System|This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, locationdependent applications. It allows applications running on mobile and static nodes to learn their physical location by using listeners that hear and analyze information from beacons spread throughout the building. Cricket is the result of several design goals, including user privacy, decentralized administration, network heterogeneity, and low cost. Rather than explicitly tracking user location, Cricket helps devices learn where they are and lets them decide whom to advertise this information to; it does not rely on any centralized management or control and there is no explicit coordination between beacons; it provides information to devices regardless of their type of network connectivity; and each Cricket device is made from off-the-shelf components and costs less than U.S. $10. We describe the randomized algorithm used by beacons to transmit information, the use of concurrent radio and ultrasonic signals to infer distance, the listener inference algorithms to overcome multipath and interference, and practical beacon configuration and positioning techniques that improve accuracy. Our experience with Cricket shows that several location-dependent applications such as in-building active maps and device control can be developed with little effort or manual configuration.  1 
69|The nesC language: A holistic approach to networked embedded systems|We present nesC, a programming language for networked embedded systems that represent a new design space for application developers. An example of a networked embedded system is a sensor network, which consists of (potentially) thousands of tiny, lowpower “motes, ” each of which execute concurrent, reactive programs that must operate with severe memory and power constraints. nesC’s contribution is to support the special needs of this domain by exposing a programming model that incorporates event-driven execution, a flexible concurrency model, and component-oriented application design. Restrictions on the programming model allow the nesC compiler to perform whole-program analyses, including data-race detection (which improves reliability) and aggressive function inlining (which reduces resource consumption). nesC has been used to implement TinyOS, a small operating system for sensor networks, as well as several significant sensor applications. nesC and TinyOS have been adopted by a large number of sensor network research groups, and our experience and evaluation of the language shows that it is effective at supporting the complex, concurrent programming style demanded by this new class of deeply networked systems.
70|NiagaraCQ: A Scalable Continuous Query System for Internet Databases|Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system’s performance and scalability. 1.
71|Timing-Sync Protocol for Sensor Networks|Wireless ad-hoc sensor networks have emerged as an interesting and important research area in the last few years. The applications envisioned for such networks require collaborative execution of a distributed task amongst a large set of sensor nodes. This is realized by exchanging messages that are timestamped using the local clocks on the nodes. Therefore, time synchronization becomes an indispensable piece of infrastructure in such systems. For years, protocols such as NTP have kept the clocks of networked systems in perfect synchrony. However, this new class of networks has a large density of nodes and very limited energy resource at every node; this leads to scalability requirements while limiting the resources that can be used to achieve them. A new approach to time synchronization is needed for sensor networks.
72|TelegraphCQ: Continuous Dataflow Processing for an Uncertan World|Increasingly pervasive networks are leading towards a world where data is constantly in motion. In such a world, conventional techniques for query processing, which were developed under the assumption of a far more static and predictable computational environment, will not be sufficient. Instead, query processors based on adaptive dataflow will be necessary. The Telegraph project has developed a suite of novel technologies for continuously adaptive query processing. The next generation Telegraph system, called TelegraphCQ, is focused on meeting the challenges that arise in handling large streams of continuous queries over high-volume, highly-variable data streams. In this paper, we describe the system architecture and its underlying technology, and report on our ongoing implementation effort, which leverages the PostgreSQL open source code base. We also discuss open issues and our research agenda.
73|The Cougar Approach to In-Network Query Processing in Sensor Networks|The widespread distribution and availability of smallscale sensors, actuators, and embedded processors is transforming the physical world into a computing platform. One such example is a sensor network consisting of a large number of sensor nodes that combine physical sensing capabilities such as temperature, light, or seismic sensors with networking and computation capabilities. Applications range from environmental control, warehouse inventory, and health care to military environments. Existing sensor networks assume that the sensors are preprogrammed and send data to a central frontend where the data is aggregated and stored for offline querying and analysis. This approach has two major drawbacks. First, the user cannot change the behavior of the system on the fly. Second, conservation of battery power is a major design factor, but a central system cannot make use of in-network programming, which trades costly communication for cheap local computation.
74|A Transmission Control Scheme for Media Access in Sensor Networks|We study the problem of media access control in the novel regime of sensor networks, where unique application behavior and tight constraints in computation power, storage, energy resources, and radio technology have shaped this design space to be very different from that found in traditional mobile computing regime. Media access control in sensor networks must not only be energy efficient but should also allow fair bandwidth allocation to the infrastructure for all nodes in a multihop network. We propose an adaptive rate control mechanism aiming to support these two goals and find that such a scheme is most effective in achieving our fairness goal while being energy effcient for both low and high duty cycle of network traffic.
75|Routing indices for peer-to-peer systems|Finding information in a peer-to-peer system currently requires either a costly and vulnerable central index, or ooding the network with queries. In this paper we introduce the concept of Routing Indices (RIs), which allow nodes to forward queries to neighbors that are more likely to have answers. If a node cannot answer a query, it forwards the query to a subset of its neighbors, based on its local RI, rather than by selecting neighbors at random or by ooding the network by forwarding the query to all neighbors. We present three RI schemes: the compound, the hop-count, and the exponential routing indices. We evaluate their performance via simulations, and nd that RIs can improve performance by one or two orders of magnitude vs. a ooding-based system, and by up to 100 % vs. a random forwarding system. We also discuss the tradeo s between the di erent RIschemes and highlight the e ects of key design variables on system performance.
76|Eddies: Continuously Adaptive Query Processing|In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.  In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry  during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates prom...
78|An Adaptive Query Execution System for Data Integration|Query processing in data integration occurs over networkbound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the  Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila  to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that the Tukwila architecture extends previous innovations in adaptive execution (such as...
79|Approximate Query Processing Using Wavelets|Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today’s decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing
80|Optimization of nonrecursive queries|State-of-the-art optimization approaches for relational database systems, e.g., those used in systems such as OBE, SQL/DS, and commercial INGRES. when used for queries in non-traditional database applications, suffer from two problems. First, the time complexity of their optimization algorithms, being combinatoric, is exponential in the number of relations to be joined in the query. Their cost is therefore prohibitive in situa-tions such as deductive databases and logic oriented languages for knowledge bases, where hundreds of joins may be required. The second problem with the traditional approaches is that, al-beit effective in their specific domain, it is not clear whether they can be generalized to different scenarios (e.g. parallel evalua-tion) since they lack a formal model to define the assumptions and critical factors on which their valiclity depends. This paper
81|Extensible/Rule Based Query Rewrite Optimization in Starburst|This paper describes the Query Rewrite facility of the Starburst extensible database system, a novel phase of query optimization. We present a suite of rewrite rules used in Starburst to transform queries into equivalent queries for faster execution, and also describe the production rule engine which is used by Starburst to choose and execute these rules. Examples are provided demonstrating that these Query Rewrite transformations lead to query execution time improvements of orders of magnitude, suggesting that Query Rewrite in general --- and these rewrite rules in particular --- are an essential step in query optimization for modern database systems.  1 Introduction  In traditional database systems, query optimization typically consists of a single phase of processing in which access methods, join orders and join methods are chosen to provide an efficient plan for executing a user&#039;s declarative query. We refer to this phase as plan optimization. In this paper we present a distinct ph...
82|Query Processing, Approximation, and Resource Management In a Data Stream Management System|This paper describes our ongoing work developing the Stanford Stream Data Manager (STREAM), a system for executing continuous queries over multiple continuous data streams. The STREAM system supports a declarative query language, and it copes with high data rates and query workloads by providing approximate answers when resources are limited. This paper describes specific contributions made so far and enumerates our next steps in developing a general-purpose Data Stream Management System.
83|Querying in highly mobile distributed environments|New challenges to the database field brought about by the rapidly growing area of wireless personal communication are presented. Preliminary solutions to two of the major problems, control of update volume and integration of query processing and data acquisition, are discussed along with the simulation results. 1
84|Adaptive Query Processing: Technology in Evolution|As query engines are scaled and federated, they must cope with highly unpredictable and changeable environments. In the Telegraph project, we are attempting to architect and implement a continuously adaptive query engine suitable for global-area systems, massive parallelism, and sensor networks. To set the stage for our research, we present a survey of prior work on adaptive query processing, focusing on three characterizations of adaptivity: the frequency of adaptivity, the effects of adaptivity, and the extent of adaptivity. Given this survey, we sketch directions for research in the Telegraph project.  
85|Database System Issues in Nomadic Computing|Mobile computers and wireless networks are emerging technologies that will soon be available to a  wide variety of computer users. Unlike earlier generations of laptop computers, the new generation  of mobile computers can be an integrated part of a distributed computing environment, one in  which users change physical location frequently. The result is a new computing paradigm, nomadic  computing. This paradigm will affect the design of much of our current systems software, including  that of database systems.  This paper discusses in some detail the impact of nomadic computing on a number of traditional  database system concepts. In particular, we point out how the reliance on short-lived batteries  changes the cost assumptions underlying query processing. In these systems, power consumption  competes with resource utilization in the definition of cost metrics. We also discuss how the likelihood  of temporary disconnection forces consideration of alternative transaction processing pr...
86|Optimization techniques for queries with expensive methods|Object-Relational database management systems allow knowledgeable users to de ne new data types, as well as new methods (operators) for the types. This exibility produces an attendant complexity, which must be handled in new ways for an Object-Relational database management system to be e cient. In this paper we study techniques for optimizing queries that contain time-consuming methods. The focus of traditional query optimizers has been on the choice of join methods and orders; selections have been handled by \pushdown &#034; rules. These rules apply selections in an arbitrary order before as many joins as possible, using the assumption that selection takes no time. However, users of Object-Relational systems can embed complex methods in selections. Thus selections may take signi cant amounts of time, and the query optimization model must be enhanced. In this paper, we carefully de ne a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial Object-Relational database management system Illustra, and discuss practical issues that a ect our earlier assumptions. We compare Predicate Migration to a variety of simpler optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we presentmaybe useful for constrained workloads.
87|Cost-based Query Scrambling for Initial Delays|Remote data access from disparate sources across a wide-area network such as the Internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites. Traditional, static, query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays. Query scrambling has been proposed to address this problem. Scrambling modifies query execution plans on-the-fly when delays are encountered during runtime. In its original formulation, scrambling was based on simple heuristics, which although providing good performance in many cases, were also shown to be susceptible to problems resulting from bad scrambling decisions. In this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices. We propose three different approaches to using query optimization for scramblin...
88|Adaptive Parallel Aggregation Algorithms|Aggregation and duplicate removal are common in SQL queries. However, in the parallel query processing literature, aggregate processing has received surprisingly little attention; furthermore, for each of the traditional parallel aggregation algorithms, there is a range of grouping selectivities where the algorithm performs poorly. In this work, we propose new algorithms that dynamically adapt, at query evaluation time, in response to observed grouping selectivities. Performance analysis via analytical modeling and an implementation on a workstation-cluster shows that the proposed algorithms are able to perform well for all grouping selectivities. Finally, we study the effect of data skew and show that for certain data sets the proposed algorithms can even outperform the best of traditional approaches. 1 Introduction  SQL queries are replete with aggregate and duplicate elimination operations. One measure of the perceived importance of aggregation is that in the proposed TPCD benchmark...
90|Bluetooth and Sensor Networks: A Reality Check|The current generation of sensor nodes rely on commodity components. The choice of the radio is particularly important as it impacts not only energy consumption but also software design (e.g., network self-assembly, multihop routing and in-network processing). Bluetooth is one of the most popular commodity radios for wireless devices. As a representative of the frequency hopping spread spectrum radios, it is a natural alternative to broadcast radios in the context of sensor networks. The question is whether Bluetooth can be a viable alternative in practice. In this paper, we report our experience using Bluetooth for the sensor network regime. We describe our tiny Bluetooth stack that allows TinyOS applications to run on Bluetooth-based sensor nodes, we present a multihop network assembly procedure that leverages Bluetooth&#039;s device discovery protocol, and we discuss how Bluetooth favorably impacts in-network query processing. Our results show that despite obvious limitations the Bluetooth sensor nodes we studied exhibit interesting properties, such as a good energy per bit sent ratio. This reality check underlies the limitations and some promises of Bluetooth for the sensor network regime.
91|Aggregation and Relevance in Deductive Databases|In this paper we present a technique to optimize queries on deductive databases that use aggregate operations such as min, max, and “largest Ic values. ” Our approach is based on an extended notion of relevance of facts to queries that takes aggregate operations into account. The approach has two parts: a rewriting part that labels predica.tes with “ag-gregate selections, ” and an evaluat,ion part. t.hat, makes use of “aggregate selections ” to detect that facts are irrelevant and discards them. The rewriting complements standard rewrit-ing algorithms like Magic sets, and the evaluation essentially refines Semi-Naive evaluation. 1
92|Query Optimization In Compressed Database Systems|Over the lastd ecad es, improvements in CPU speed have outpaced improvements in main memory and d isk access rates by ord ers of magnitud , enabling the use ofd ata compression techniques to improve the performance ofd atabase systems. Previous work d scribes the benefits of compression for numerical attributes, whered8 a is stored in compressed  format ond isk. Despite the abund3&amp; e of stringvalued  attributes in relational schemas there is little work on compression for string attributes in ad atabase context. Moreover, none of the previous work suitablyad2 esses the role of the query optimizer: During query execution, dD a is either eagerly d compressed when it is read into main memory, or dD a lazily stays compressed in main memory and is d compressed ond emand only. In this paper, we present an e#ective approach for dD abase compression based on lightweight, attribute-level compression techniques. We propose a Hierarchical ictionary Encod  ing strategy that intelligently selects the most e#ective compression method for string-valued attributes. We show that eager and lazy d compression strategies prod1 e suboptimal plans for queries involving compressed string attributes. We then formalize the problem of compressionaware query optimizationand propose one provably optimal  and two fast heuristic algorithms for selecting a query plan for relational schemas with compressed attributes; our algorithms can easily be integrated into existing cost-based query optimizers. Experiments using TPC-Hd atad emonstrate the impact of our string compression method s and show the importance of compression-aware query optimization. Our approach results in up to an or d r speed up over existing approaches.  1. 
93|The Design and Implementation of the Ariel Active Database Rule System|This paper describes the design and implementation of the Ariel DBMS and it&#039;s tightlycoupled forward-chaining rule system. The query language of Ariel is a subset of POSTQUEL, extended with a new production-rule sublanguage. Ariel supports traditional relational database query and update operations efficiently, using a System Rlike query processing strategy. In addition, the Ariel rule system is tightly coupled with query and update processing. Ariel rules can have conditions based on a mix of patterns, events, and transitions. For testing rule conditions, Ariel makes use of a discrimination network composed of a special data structure for testing single-relation selection conditions efficiently, and a modified version of the TREAT algorithm, called A-TREAT, for testing join conditions. The key modification to TREAT (which could also be used in the Rete algorithm) is the use of virtual ff-memory nodes which save storage since they contain only the predicate associated with the memory n...
94|DOMINO: Databases fOr MovINg Objects tracking|Consider a database that represents information about moving objects and their location. For example, for a database representing the location of taxi-cabs a typical query may be: retrieve the free cabs that are currently within 1 mile of 33 N. Michigan Ave., Chicago (to pick-up a customer); or for a trucking company database a typical query may be: retrieve the trucks that are currently within 1 mile of truck ABT312 (which needs assistance); or for a database representing the current location of objects in a battlefield a typical query may be: retrieve the friendly helicopters that are in a given region, or, retrieve the friendly helicopters that are expected to enter the region within the next 10 minutes. The queries may originate from the moving objects, or from stationary users. We will refer to applications with the above characteristics as moving-objects-database (MOD) applications, and to queries as the ones mentioned above as MOD queries.
95|From ethnography to design in a vineyard, in|This paper summarizes the process from ethnographic study of a vineyard to concept development and interaction design for a ubiquitous computing solution. It provides examples of vineyard interfaces and the lessons learned that could be generally applied to the interaction design of ubiquitous systems. These are: design for multiple perspectives on data, design for multiple access points, and design for varying levels of attention.
96|Query Optimization in Mobile Environments|We consider the issue of optimizing queries for a distributed processing in mobile environment. An interesting characteristic of mobile machines is that they depend on battery as a source of energy which may not be substantial enough. Hence, the appropriate optimization criterion in a mobile environment considers both resource utilization and energy consumption at the mobile client. In this scenario, the optimal plan for a query depends on the residual battery level of the mobile client and the load at the server. We approach this problem by compiling a query into a sequence of candidate plans, such that for any state of the client-server system, the optimal plan is one of the candidate plans. A general solution is proposed by adapting the partial order dynamic programming search algorithm [17, 18] (p.o dp) such that the coverset of the query is the set of candidate plans. We propose two novel algorithms, namely, the linear combinations algorithm and the linearset algorithm (referred t...
97|Online Dynamic Reordering|We present a mechanism for providing dynamic user control during long running, data-intensive operations. Users  can see partial results and dynamically direct the processing to suit their interests, thereby enhancing the interactivity  in several contexts such as online aggregation and large-scale spreadsheets.  In this paper, we develop a pipelining, dynamically tunable reorder operator for providing user control. Users  specify preferences for different data items based on prior results, so that data of interest is prioritized for early  processing. The reordering mechanism is efficient and non-blocking, and can be used over arbitrary data streams  from files and indexes, as well as continuous data feeds. We also investigate several policies for the reordering  based on the performance goals of various typical applications. We present performance results for reordering in the  context of an Online Aggregation implementation in Informix, and in the context of sorting and scrolling in...
98|The design of an acquisitional query processor for sensor networks|We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. 1.
99|Fine-grained network time synchronization using reference broadcasts|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
100|Continuously Adaptive Continuous Queries over Streams|We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive crossquery sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators -- both selections and join state -- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.
101|Visual Query Operators for Temporal Databases |Many recent proposals in the literature present easy-to-use query languages, based on visual representations of the database and direct manipulation mechanisms. Such proposals essentially deals with conventional query operations, whereas little effort is devoted to investigate friendly environments for querying temporal databases. Nevertheless, temporal aspects are generally considered extremely relevant in several database applications. Thus, the various users should be provided with powerful and simple query systems also exhibiting temporal features. While attempting to meet this need, we propose a visual query system for temporal databses. It adopts a diagrammatic representation of the database schema (including temporal classes and relationships), on which conventional as well as temporal visual query operators can be applied. In this paper we concentrate on the temporal operators, showing both examples of usage and formalization. 1.
102|Maintaining knowledge about temporal intervals|The problem of representing temporal knowledge arises in many areas of computer science. In applications in which such knowledge is imprecise or relative, current representations based on date lines or time instants are inadequate. An interval-based temporal logic is introduced, together WiUl a computationally effective reasoning algorithm based on constraint- propagation. This system is notable in offering a delicate balance between expressive power and the efficiency of its deductive engine. A notion of reference intervals is introduced which captures the temporal hierarchy implicit in many domains, and which can be used to precisely control the amount of deduction performed automatically by the system. Examples.are provided for a data base containing historical data, a d&lt;lta base. used for modeling processes and process interaction, and a data base for an interactive system where the present moment is continually being updated.
103|Direct manipulation: a step beyond programming languages|Direct manipulation systems offer the satisfying experience of operating on visible objects. The computer becomes transparent, and users can concentrate on their tasks.
104|The Temporal Query Language TQuel|This paper defines aggregates in the temporal query language TQuel and provides their rormal semantics in the tuple relational calculus. A rormal semantics (or Que! aggregates is defined in the process. Multiple aggregates; aggregates appearing in the where, when, valid, and as-or clauses; nested aggregation; and instantaneous, cumulative, and unique variants are supported. These aggregates give the user a rich set or statistical functions that range over time, while requiring minimal additions to TQuel and its semantics..:&#039;l&#039;bi1 work wu nppolied bJ NSF (l&#039;&amp;lli DCR·8402330 and by a Junior Faculty Denlopmnt Awud from the UNC. CH FoUD.datioa. The &amp;nt aat.hor wu npport.ed ia pan by u IBM Faculty Developmnt. Award..,!;
105|Temporal Reasoning Based on Semi-Intervals|A generalization of Allen&#039;s interval-based approach to temporal reasoning is presented. The notion of `conceptual neighborhood&#039; of qualitative relations between events is central to the presented approach. Relations between semi-intervals rather than intervals are used as the basic units of knowledge. Semi-intervals correspond to temporal beginnings or endings of events. We demonstrate the advantages of reasoning on the basis of semi-intervals: 1) semi-intervals are rather natural entities both from a cognitive and from a computational point of view; 2) coarse knowledge can be processed directly; computational effort is saved; 3) incomplete knowledge about events can be fully exploited; 4) incomplete inferences made on the basis of complete knowledge can be used directly for further inference steps; 5) there is no trade-off in computational strength for the added flexibility and efficiency; 6) for a natural subset of Allen&#039;s algebra, global consistency can be guaranteed in polynomial time; 7) knowledge about relations between events can be represented much more compactly.
106|Fundamental Graphical Primitives for Visual Query Languages|The need of a friendly man-machine interaction is becoming crucial for a large variety of applications. In order to reach such a friendliness a new class of languages has been proposed (Visual Languages), based on the extensive use of graphical and iconic mechanisms. We are interested in a particular subclass of Visual Languages, called Visual Query Languages (VQLs), devoted to the extraction of information from databases. VQLs are mainly based on the idea of applying new interaction mechanisms, based on the &#034;direct manipulation&#034; paradigm, on visually represented databases. Various VQLs have been proposed, but only a few of them are provided with a formal definition and, also when such a formal definition exists, it does not give the semantics of the graphical operations performed by the user. In this paper we aim to provide such a semantics by proposing a graphical data model, the graph model, in which the visual representation is part of the model itself, and a minimal set of Graphic...
107|A temporal knowledge representation model OSAM*/T and its query language OQL/T|The importance of temporal database management and temporal reasoning has long been recognized by the database and AI communities. Many techniques for modeling and managing temporal databases have been introduced. Most of the existing research on temporal databases have been based on the relational data model which has limited capabilities in modeling complex objects, constraints, and behavioral properties. In this paper, we present an object-oriented knowledge-based approach to model and process temporal information. A temporal knowledge representation model OSAM*/T is described. It is an extension of the object-oriented semantic association model OSAM * to capture the concepts of time and history. Object time-stamping is used to record the evolutions of objects, and knowlcdgc rules instead of extra time notions are used to capture special temporal requirements. The temporal model is also featured by its separation of historical data and current data and its simple and maintainable representation of complex objects. In addition to the temporal data model, this paper also introduces a temporal query language OQL/T which is an extension of the object-oriented query language OQL for processing temporal information. Several temporal functions, interval comparison operators, and set operators are described. 1
108|A Graph-based Framework for Multiparadigmatic Visual Access to Databases|We describe an approach for multiparadigmatic visual access to databases, which is proposed to achieve seamless integration of different interaction paradigms. The user is provided with an adaptive interface augmented by a user model, supporting different visual representations of both data and queries. The visual representations are characterized on the basis of the chosen visual formalisms, namely forms, diagrams, and icons. To access different databases, a unified data model, the Graph Model, is used as a common underlying formalism to which databases, expressed in the most popular data models, can be mapped. Graph Model databases are queried through the adaptive interface. The semantics of the query operations is formally defined in terms of graphical primitives. Such a formal approach permits us to define the concept of &#034;atomic query&#034;, which is the minimal portion of a query that can be transferred from one interaction paradigm to another and processed by the system. Since certain...
109|Graphical Interaction with Heterogeneous Databases|During the last years our research efforts were inspired by two different needs. On  one side the number of non-expert users accessing databases is largely growing. On  the other side, information systems will be no longer composed by a single  centralised architecture, but rather by several heterogeneous component systems.  In order to address such needs we have designed a new query system, having both  user-oriented and multidatabase features. The system main components are an  adaptive visual interface, providing the user with different and interchangeable  interaction modalities, and a &#034;translation layer&#034;, which creates and offers to the user  the illusion of a single homogeneous schema out of several heterogeneous  components. Both components are founded on a common ground, i.e. a formally  defined and semantically rich data model, the Graph Model, and a minimal set of  Graphical Primitives, in terms of which general query operations may be visually  expressed. The Graph Model has...
110|Querying Heterogeneous Information Sources Using Source Descriptions|We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can ask). (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an imp...
111|Retrieving And Integrating Datafrom Multiple Information Sources|With the current explosion of data, retrieving and integrating information  from various sources is a critical problem. Work in multidatabase systems  has begun to address this problem, but it has primarily focused on methods  for communicating between databases and requires significant effort for each  new database added to the system. This paper describes a more general  approach that exploits a semantic model of a problem domain to integrate  the information from various information sources. The information sources  handled include both databases and knowledge bases, and other information  sources (e.g., programs) could potentially be incorporated into the system.  This paper describes how both the domain and the information sources are  modeled, shows how a query at the domain level is mapped into a set of  queries to individual information sources, and presents algorithms for automatically  improving the efficiency of queries using knowledge about both the  domain and the informat...
112|A Softbot-Based Interface to the Internet|this article, we focus on the ideas underlying the softbot-based interface.
113|Optimizing Queries with Materialized Views|While much work has addressed the problem of maintaining materialized views, the important question of optimizing queries in the presence of materialized views has not been resolved. In this paper, we analyze the optimization question and provide a comprehensive and efficient solution. Our solution has the desirable property that it is a simple generalization of the traditional query optimization algorithm. 1 Introduction  The idea of using materialized views for the benefit of improved query processing has been proposed in the literature more than a decade ago. In this context, problems such as definition of views, composition of views, maintenance of views [BC79, KP81, SI84, BLT86, CW91, Rou91, GMS93] have been researched but one topic has been conspicuous by its absence. This concerns the problem of the judicious use of materialized views in answering a query. It may seem that materialized views should be used to evaluate a query whenever they are applicable. In fact, blind applicat...
114|Data Model and Query Evaluation in Global Information Systems|. Global information systems involve a large number of information sources distributed over computer networks. The variety of information sources and disparity of interfaces makes the task of easily locating and efficiently accessing information over the network very cumbersome. We describe an architecture for global information systems that is especially tailored to address the challenges raised in such an environment, and distinguish our architecture from architectures of multidatabase and distributed database systems. Our architecture is based on presenting a conceptually unified view of the information space to a user, specifying rich descriptions of the contents of the information sources, and using these descriptions for optimizing queries posed in the unified view. The contributions of this paper include: (1) we identify aspects of site descriptions that are useful in query optimization; (2) we describe query optimization techniques that minimize the number of information source...
115|Answering queries using templates with binding patterns|When integrating heterogeneous information re-sources, it is often the case that the source is rather limited in the kinds of queries it can answer. If a query is asked of the entire system, we have a new kind of optimization problem, in which we must try to express the given query in terms of the lim-ited query templates that this source can answer. For the case of conjunctive queries, we show how to decide with a nondeterministic polynomial-time al-gorithm whether the given query can be answered. We then extend our results to allow arithmetic comparisons in the given query and in the tem-plates.
116|Query Caching and Optimization in Distributed Mediator Systems|Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source&#039;s performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a costbased optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants  mechanism, which s...
117|A Query Translation Scheme for Rapid Implementation of Wrappers|Wrappers provide access to heterogeneous information sources by converting application queries into source specific queries or commands. In this paper we present a wrapper implementation toolkit that facilitates rapid development of wrappers. We focus on the query translation component of the toolkit, called the converter. The converter takes as input a Query Description and Translation Language (QDTL) description of the queries that can be processed by the underlying source. Based on this description the converter decides if an application query is (a) directly supported, i.e., it can be translated to a query of the underlying system following instructions in the QDTL description; (b) logically supported, i.e., logically equivalent to a directly supported query; (c) indirectly supported, i.e., it can be computed by applying a filter,  automatically generated by the converter, to the result of a directly supported query. 1 Introduction  A wrapper or translator [C  +  94, PGMW95] is a s...
118|Distributed Active Catalogs and Meta-Data Caching in Descriptive Name Services|Today&#039;s global internetworks challenge the ability of name services and other information services to locate data quickly. We introduce a distributed active catalog and meta-data caching for optimizing queries in this environment. Our active catalog constrains the search space for a query by returning a list of data repositories where the answer to the query is likely to be found. Meta-data caching improves performance by keeping frequently used characterizations of the search space close to the user, and eliminating active catalog communication and processing costs. When searching for query responses, our techniques contact only the small percentage of the data repositories with actual responses, resulting in search times of a few seconds. We implemented a distributed active catalog and meta-data caching in a prototype descriptive name service called &#034;Nomenclator. &#034; We present performance results for Nomenclator in a search space of 1000 data repositories. 1. Introduction  Users canno...
119|Using Heterogeneous Equivalences for Query Rewriting in Multidatabase Systems|In order to have significant practical impact on future information systems, multidatabase management systems (MDBMS) must be both flexible and efficient. We consider a MDBMS with a common object-oriented model, based on the ODMG standard, and local databases that may be relational or object-oriented. In this context, query rewriting (for optimization) is made difficult by schematic discrepancy, and the need to model mapping information between the multidatabase and local schemas. We address the flexibility issue by representing the mappings from a local schema to the multidatabase schema, as a set of heterogeneous object equivalences, in a declarative language. Efficiency is obtained by exploiting these equivalences to rewrite multidatabase OQL queries into equivalent, simplified queries on the local schemas. 1 Introduction  The advent of open systems is increasingly stimulating the development of information systems which can provide high-level integration of heterogeneous informatio...
120|The Identification and Resolution of Semantic Heterogeneity in Multidatabase Systems|This paper describes several aspects of the  Remote--Exchange project at USC, which focuses on the controlled sharing and exchange of information among autonomous, heterogeneous database systems. The spectrum of heterogeneity which may exist among the components in a federation of database systems is examined, and an approach to accommodating such heterogeneity is described. An overview of the Remote--Exchange experimental system is provided. 1 Introduction  Consider an environment consisting of a collection of data/knowledge bases and their supporting systems, and in which it is desired to accommodate the controlled sharing and exchange of information among the collection. We shall refer to this as the (interconnected) autonomous heterogeneous database environment. Such environments are extremely common in various application domains, including office information systems, computer-integrated manufacturing systems (with computer-aided design as a subset), personal computing, business a...
121|Relational Databases for Querying XML Documents:  Limitations and Opportunities|XML is fast emerging as the dominant standard for representing data in the World Wide Web. Sophisticated query engines that allow users to effectively tap the data stored in XML documents will be crucial to exploiting the full power of XML. While there has been a great deal of activity recently proposing new semistructured data models and query languages for this purpose, this paper explores the more conservative approach of using traditional relational database engines for processing XML documents conforming to Document Type Descriptors (DTDs). To this end, we have developed algorithms and implemented a prototype system that converts XML documents to relational tuples, translates semi-structured queries over XML documents to SQL queries over tables, and converts the results to XML. We have qualitatively evaluated this approach using several real DTDs drawn from diverse domains. It turns out that the relational approach can handle most (but not all) of the semantics of semi-structured queries over XML data, but is likely to be effective only in some cases. We identify the causes for these limitations and propose certain extensions to the relational 
122|Multiple-Query Optimization|Some recently proposed extensions to relational database systems, as well as to deductive database systems, require support for multiple-query processing. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions may expand to more than one actual query that has to be run over the database. It is an interesting problem then to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of the paper is a systematic look at the problem, along with the presentation and analysis of algorithms that can be used for multiple-query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably.
123|Optimizing Regular Path Expressions Using Graph Schemas|Query languages for data with irregular structure use regular path expressions for navigation. This feature is useful for querying data where parts of the structure is either unknown, unavailable to the user, or changes frequently. Naive execution of regular path expressions is inefficient, however, because it ignores any structure in the data. We describe two optimization techniques for queries with regular path expressions. Both rely on graph schemas for specifying partial knowledge about the data&#039;s structure. Query pruning uses this structure to restrict navigation to only a fragment of the data; we give an efficient algorithm for rewriting any regular path expression query into a pruned one. Query rewriting using state extents  can eliminate or reduce navigation altogether; it is reminiscent of optimizing relational queries using indices. There may be several ways to optimize a query using state extents; we give a polynomial-space algorithm that finds all such optimizations. For re...
124|Multivalued Dependencies and a New Normal Form for Relational Databases|A new type of dependency, which includes the well-known functional dependencies as a special case, is defined for relational databases. By using this concept, a new (“fourth”) normal form for relation schemata is defined. This fourth normal form is strictly stronger than Codd’s “im-proved third normal form ” (or “Boyce-Codd normal form”). It is shown that, every relation schema can be decomposed into a family of relation schemata in fourth normal form without loss of information (that is, the original relation can be obtained from the new relations by taking joins). Key words and phrases: database design, multivalued dependency, functional dependency, fourth normal form, 4NF, third normal form, 3NF, Boyce-Codd normal form, normalization, decomposition, relational database
125|Sesame: A Generic Architecture for Storing and Querying RDF and RDF Schema|RDF and RDF Schema are two W3C standards aimed at  enriching the Web with machine-processable semantic data.
126|Resource Description Framework (RDF) Model and Syntax Specification  (1998) |This document is a revision of the public working draft dated 1998-08-19 incorporating suggestions received in review comments and further deliberations of the W3C RDF Model and Syntax Working Group. With the publication of this draft, the RDF Model and Syntax Specification enters &#034;last call.&#034; The last call period will end on October 23, 1998. Comments on this specification may be sent to www-rdf-comments@w3.org. The archive of public comments is available at http://www.w3.org/Archives/Public/www-rdf-comments. Significant changes from the previous draft are highlighted in Appendix E. While we do not anticipate substantial changes, we still caution that further changes are possible. Therefore while we encourage active implementation to test this specification we also recommend that only software that can be easily field-upgraded be implemented to this specification at this time. This is a W3C Working Draft for review by W3C members and other interested parties. Publication as a working draft does not imply endorsement by the W3C membership. The RDF Model and Syntax  Working Group will not allow early implementation to constrain their ability to make changes to this specification prior to final release. This is a draft document and may be updated, replaced or obsoleted by other documents at any time. It is inappropriate to cite W3C Working Drafts as other than &#034;work in progress&#034;. This work is part of the W3C Metadata Activity.
127|The RDFSuite: Managing Voluminous RDF Description Bases|Metadata are widely used in order to fully exploit information resources available  on corporate intranets or the Internet. The Resource Description Framework (RDF)  aims at facilitating the creation and exchange of metadata as any other Web data. The  growing number of available information resources and the proliferation of description  services in various user communities, lead nowadays to large volumes of RDF metadata.  Managing such RDF resource descriptions and schemas with existing low-level APIs and  file-based implementations does not ensure fast deployment and easy maintenance of realscale  RDF applications. In this paper, we advocate the use of database technology to  support declarative access, as well as, logical and physical independence for voluminous  RDF description bases.  We present RDFSuite, a suite of tools for RDF validation, storage and querying.  Specifically, weintroduce a formal data model for RDF description bases created using  multiple schemas. Compared to ...
128|Querying Community Web Portals|Anewgeneration of information systems suchasorganizational memories, vertical aggregators,  infomediaries, etc. is emerging nowadays. Such systems, termed CommunityWeb  Portals, intend to support specific communities of interest (e.g., enterprise, professional, trading)  on corporate intranets or the Web. More precisely, Portal Catalogs, organize and describe  various information resources (e.g., sites, documents, data) for diverse target audiences  (corporate, inter-enterprise, e-marketplace, etc.), in a multitude of ways, which are far more  flexible and complex than those provided by standard (relational or object) databases. Yet, in  commercial software for deploying CommunityPortals, querying is still limited to full-text (or  attribute-value) retrieval and more advanced information-seeking needs implies navigational  access. Furthermore, recentWeb standards for describing resources are completely ignored.
129|Algorithms for Temporal Query Operators in XML Databases|We have in a previous paper introduced the new query operators that are needed in order to support  an XML query language that supports temporal operations. The query operators make it possible to  query historical versions, retrieve documents valid at a certain time, query changes to documents, etc. In  this paper, we describe algorithms for execution of the query operators, and we also discuss document  content indexing for more efficient execution of the operators.
130|Detecting Changes in XML Documents|We present a diff algorithm for XML data. This work is motivated by the support for change control in the context of the Xyleme project that is investigating dynamic warehouses capable of storing massive volume of XML data. Because of the context, our algorithm has to be very efficient in terms of speed and memory space even at the cost of some loss of &#034;quality&#034;. Also, it considers, besides insertions, deletions and updates (standard in diffs), a move operation on subtrees that is essential in the context of XML. Intuitively, our diff algorithm uses signatures to match (large) subtrees that were left unchanged between the old and new versions. Such exact matchings are then possibly propagated to ancestors and descendants to obtain more matchings. It also uses XML specific information such as ID attributes. We provide a performance analysis of the algorithm. We show that it runs in average in linear time vs. quadratic time for previous algorithms. We present experiments on synthetic data that confirm the analysis. Since this problem is NPhard, the linear time is obtained by trading some quality. We present experiments (again on synthetic data) that show that the output of our algorithm is reasonably close to the &#034;optimal&#034; in terms of quality. Finally we present experiments on a small sample of XML pages found on the Web. 1 
131|Change-Centric Management of Versions in an XML Warehouse|We present a change-centric method to manage versions in a Web WareHouse of XML data. The starting points is a sequence of snapshots of XML documents we obtain from the web. By running a diff algorithm, we compute the changes between two consecutive versions. We then represent the sequence using a novel representation of changes based on completed deltas and persistent identifiers. We present the foundations of the logical representation and some aspects of the physical storage policy. The work presented here was developed in the context of the Xyleme project of massive XML warehouse for XML data from the Web. It has been implemented and tested. We briefly discuss the implementation.
132|Representing and querying changes in semistructured data|Semistructured data may be irregular and incomplete and does not necessarily conform to a fixed schema. As with structured data, it is often desirable to maintain a history of changes to data, and to query over both the data and the changes. Representing and querying changes in semistructured data is more difficult than in structured data due to the irregularity and lack of schema. We present a model for representing changes in semistructured data and a language for querying over these changes. An important feature of our approach is that we represent and query changes directly as annotationson the affected data, instead of indirectly as the difference between database states. We describe the implementation of our model and query language. We also describe the design and implementation of a query subscription service that permits users to subscribe to changes in semistructured information sources. 1
133|Efficient Management of Multiversion Documents by Object Referencing|Traditional approaches to versioning semistructured information are edit-based, i.e., subsequent document versions are represented by using edit scripts. This paper proposes a reference-based version management scheme that preserves the logical structure of the evolving document through the use of object references. By preserving the document structure among versions the new scheme facilitates more efficient query support. In particular, we examine queries involving projections and selections on the document versions, as well as queries on the document evolution history. Moreover, we show that the proposed scheme provides an effective representation of multiversioned XML documents, both at the transport and exchange levels. In fact, with the reference-based scheme, a document&#039;s history can also be viewed and processed as yet another XML document. Furthermore, we demonstrate the effectiveness of the new scheme at the storage level. In particular, the scheme is enhanced with a usefulness-based page management policy that extends and adapts techniques used in transaction-time databases to ensure efficient clustering of information among versions. An extensive comparison of the reference-based versioning against representations used in temporal databases and persistent object managers depicts the performance advantages of the new approach. Finally it should be noted that reference-based versioning is applicable to other kinds of semistructured information (besides XML documents), and can be used to replace traditional version control schemes, such as the edit-based RCS and the timestamp-based SCCS. 
134|The Valid Web: an XML/XSL Infrastructure for Temporal Management of Web Documents|In this paper we present a temporal extension of the World Wide Web based on a complete XML/XSL infrastructure to support valid time. The proposed technique enables the explicit definition of temporal information within HTML/XML documents, whose contents can then be selectively accessed according to their valid time. By acting on a navigation validity context, the proposed solution makes it possible to &#034;travel in time&#034; in a given virtual environment with any XML-compliant browser; this allows, for instance, to cut personalized visit routes for a specific epoch in a virtual museum or a digital historical library, to visualize the evolution of an archaeological site through successives ages, to selectively access past issues of magazines, to browse historical time series (e.g. stock quote archives), etc. The proposed Web extensions have been tested on a demo prototype showing, as application example, the functionalities of a temporal Web museum.
136|Managing Historical Semistructured Data|this article appeared in [6].
137|A Comparative Study of Version Management Schemes for XML Documents|The problem of managing multiple versions for XML documents and semistructured data is of significant interest in many DB applications and web-related services. Traditional document version control schemes, such as RCS, suffer from the following two problems. At the logical level, they conceal the structure of the documents by modeling them as sequences of text lines, and storing a document&#039;s evolution as a line-edit script. At the physical level, they can incur in severe storage or processing costs because of their inability to trade-off storage with computation. To solve these problems, we propose version management strategies that preserve the structure of the original document, and apply and extend DB techniques to minimize storage and processing costs. Therefore, we propose and compare three schemes for XML version management, namely, the Usefulness-Based Copy Control, the Multiversion B-Tree, and the Partially Persistent List Method. A common characteristic of these schemes is that they cluster data using the notion of page usefulness, which by selectively copying current information from obsolete pages provides for fast version reconstruction with minimal storage overhead. The cost and performance of these version management schemes are evaluated and compared through extensive analysis and experimentation.
138|Temporal Aspects of Semistructured Data|In many applications, information about the history of data and their dynamic aspects are just as important as static information. During the last years the increasing amount of information accessible through the Web has presented new challenges to academic and industrial research on database. In this context, data are either structured, when coming from relational or object-oriented databases, or partially or completely unstructured, when they consist of simple collections of text or image files. In the context of semistructured data, model and query languages must be extended in order to consider dynamic aspects. We present a model based on labeled graphs for representing changes in semistructured data and a SQL-like query language for querying it.
139|The Valid Web: it&#039;s Time to Go...|The development of the World Wide Web technology on the Internet is a major achievement of computer  research in recent years. The great availability of easy-to-access on-line hypermedial documents  has been a real revolution in the information world, also for its important social, cultural and economical  consequences. However, a scarce attention has been devoted so far to the temporal aspects of the World  Wide Web definition and technology, although time undoubtedly plays a pervasive and fundamental role  in the reality and in its information counterpart. Moreover, although the management of time-varying  information is a consolidated research issue in the database field, no integrations between the World  Wide Web and database worlds has been attempted in this respect yet. Notwithstanding, this happens  while there is a substantial convergence between the two worlds, with a renewed interest on the XML  emergence as the upcoming standard for the representation and exchange of semistructured data on the  Internet.  In this work, we present a temporal extension of the World Wide Web by defining a complete  XML/XSL infrastructure to support valid-time. The proposed valid-time versioning scheme allows the  explicit definition of temporal information within HTML/XML documents, whose contents can then be  selectively accessed on the basis of the validity coded within them. By acting on a navigation validity  context, the proposed solution makes it possible to navigate through time in a given virtual environment  with any XML-compliant browser; this allows, for instance, to cut personalized visit routes for a specific  epoch in a virtual museum or a digital historical library, or to visualize the evolution of an archaeological  site through successives ages, or to selective...
140|A Temporal Object Query Language|One of the main reasons that current commercial DBMSs provide only minimal support for temporal concepts is the size and complexity involved. Object-oriented databases, on the other hand, were developed to deal with complex database applications. Because temporal concepts require complex type support and advanced modeling concepts, object-oriented databases are excellent candidates for realization of temporal databases without requiring fundamental extensions to the basic data model. The ODMG (Object Data Management Group) has proposed a standard for object-oriented databases, including a standard object model, an object query language (OQL), and an object definition language (ODL). These do not include temporal support except at the data type level, as in SQL2. In this paper, we present a language extension to OQL to accommodate time information. Our goal is not to propose a new temporal query language, but to incorporate temporal features into the existing OQL framework.
141|Mathematical Control Theory: Deterministic Finite Dimensional Systems|The title of this book gives a very good description of its contents and style, although I might have added “Introduction to ” at the beginning. The style is mathematical: precise, clear statements (i.e., theorems) are asserted, then carefully proved. The book covers many of the key topics in control theory, except — as the subtitle has warned us — those involving stochastic processes or infinite-dimensional systems. The level is appropriate for a senior
142|Lottery Scheduling: Flexible Proportional-Share Resource Management|This paper presents lottery scheduling, a novel randomized resource allocation mechanism. Lottery scheduling provides efficient, responsive control over the relative execution rates of computations. Such control is beyond the capabilities of conventional schedulers, and is desirable in systems that service requests of varying importance, such as databases, media-based applications, and networks. Lottery scheduling also supports modular resource management by enabling concurrent modules to insulate their resource allocation policies from one another. A currency abstraction is introduced to flexibly name, share, and protect resource rights. We also show that lottery scheduling can be generalized to manage many diverse resources, such as I/O bandwidth, memory, and access to locks. We have implemented a prototype lottery scheduler for the Mach 3.0 microkernel, and found that it provides flexible and responsive control over the relative execution rates of a wide range of applications. The overhead imposed by our unoptimized prototype is comparable to that of the standard Mach timesharing policy. 
143|Cluster-Based Scalable Network Services|This paper has benefited from the detailed and perceptive comments of our reviewers, especially our shepherd Hank Levy. We thank Randy Katz and Eric Anderson for their detailed readings of early drafts of this paper, and David Culler for his ideas on TACC&#039;s potential as a model for cluster programming. Ken Lutz and Eric Fraser configured and administered the test network on which the TranSend scaling experiments were performed. Cliff Frost of the UC Berkeley Data Communications and Networks Services group allowed us to collect traces on the Berkeley dialup IP network and has worked with us to deploy and promote TranSend within Berkeley. Undergraduate researchers Anthony Polito, Benjamin Ling, and Andrew Huang implemented various parts of TranSend&#039;s user profile database and user interface. Ian Goldberg and David Wagner helped us debug TranSend, especially through their implementation of the rewebber
144|Optimizing Queries across Diverse Data Sources|Businesses today need to interrelate data stored in diverse systems with differing capabilities, ideally via a single high-level query interface. We present the design of a query optimizer for Gar- lic [C+95], a middleware system designed to integrate data from a broad range of data sources with very different query capabilities. Garlic&#039;s optimizer extends the rule-based approach of [Loh88 ] to work in a heterogeneous environment, by defining generic rules for the middleware and using wrapper-provided rules to encapsulate the capabilities of each data source. This approach offers great advantages in terms of plan quality, extensibility to new sources, incremental implementation of rules for new sources, and the ability to express the capabilities of a diverse set of sources. We describe the design and implementation of this optimizer, and illustrate its actions through an example.
145|Dataflow query execution in a parallel main-memory environment|Abstract. In this paper, the performance and characteristics of the execution of various join-trees on a parallel DBMS are studied. The results of this study are a step into the direction of the design of a query optimization strategy that is fit for parallel execution of complex queries. Among others, synchronization issues are identified to limit the performance gain from parallelism. A new hash-join algorithm is introduced that has fewer synchronization constraints han the known hash-join algorithms. Also, the behavior of individual join operations in a join-tree is studied in a simulation experiment. The results how that the introduced Pipelining hash-join algorithm yields a better performance for multi-join queries. The format of the optimal join-tree appears to depend on the size of the operands of the join: A multi-join between small operands performs best with a bushy schedule; larger operands are better off with a linear schedule. The results from the simulation study are confirmed with an analytic model for dataflow query execution. Ke~,ords: parallel query processing, multi-join queries, simulation, analytical modeling 1.
146| Ripple Joins for Online Aggregation |We present a new family of join algorithms, called ripple joins, for online processing of multi-table aggregation queries in a relational database management system (dbms). Such queries arise naturally in interactive exploratory decision-support applications. Traditional offline join algorithms are designed to minimize the time to completion of the query. In contrast, ripple joins are designed to minimize the time until an acceptably precise estimate of the query result is available, as measured by the length of a confidence interval. Ripple joins are adaptive, adjusting their behavior during processing in accordance with the statistical properties of the data. Ripple joins also permit the user to dynamically trade off the two key performance factors of online aggregation: the time between successive updates of the running aggregate, and the amount by which the confidence-interval length decreases at each update. We show how ripple joins can be implemented in an existing dbms using iterators, and we give an overview of the methods used to compute confidence intervals and to adaptively optimize the ripple join &#034;aspect-ratio&#034; parameters. In experiments with an initial implementation of our algorithms in the postgres dbms, the time required to produce reasonably precise online estimates was up to two orders of magnitude smaller than the time required for the best offline join algorithms to produce exact answers. 
147|The Design and Implementation of INGRES|The currently operational (March 1976) version of the INGRES database management system is described. This multiuser system gives a relational view of data, supports two high level nonprocedural data sublanguages, and runs as a collection of user processes on top of the UNIX operating system for Digital Equipment Corporation PDP 11/40, 11/45, and 11/70 computers. Emphasis is on the design decisions and tradeoffs related to (1) structuring the system into processes, (2) embedding one command language in a general purpose programming language, (3) the algorithms implemented to process interactions, (4) the access methods implemented, (5) the concurrency and recovery control currently provided, and (6) the data structures used for system catalogs and the role of the database administrator. 
Also discussed are (1) support for integrity constraints (which is only partly operational), (2) the not yet supported features concerning views and protection, and (3) future plans concerning the system.
149|Dynamic Query Evaluation Plans|Traditional query optimizers assume accurate knowledge of run-time parameters such as selectivities and resource availability during plan optimization, i.e., at compile-time. In reality, however, this assumption is often not justified. Therefore, the “static ” plans produced by traditional optimizers may not be optimal for many of their actual run-time invocations. Instead, we propose a novel optimization model that assigns the bulk of the optimization effort to compile-time and delays carefully selected optimization decisions until run-time. Our previous work defined the run-time primitives, “dynamic plans ” using “choose-plan” operators, for executing such delayed decisions, but did not solve the problem of constructing dynamic plans at compile-time. The present paper introduces techniques that solve this problem. Experience with a working prototype optimizer demonstrates (i) that the additional optimization and start-up overhead of dynamic plans compared to static plans is dominated by their advantage at run-time, (ii) that dynamic plans are as robust as the “brute-force” remedy of run-time optimization, i.e., dynamic plans maintain their optimality even if parameters change between compile-time and run-time, and (iii) that the start-up overhead of dynamic plans is signifmantly less than the time required for complete optimization at run-time. In other words, our proposed techniques are superior to both techniques considered to-date, namely compile-time optimization into a single static plan as well as run-time optimization. Finally, we believe that the concepts and technology described can be transferred to commercial query optimizers in order to improve the performance of embedded queries with host variables in the query predicate and to adapt to run-time system loads unpredictable at compile-time. 1.
150|Query optimization in the presence of limited access patterns|We consider the problem of query optimization in the presence of limitations on access patterns to the data (i.e., when one must provide values for one of the attributes of a relation in order to obtain tuples). We show that in the presence of limited access patterns we must search a space of annotated query plans, where the annotations describe the inputs that must be given to the plan. We describe a theoretical and experimental analysis of the resulting search space and a novel query optimization algorithm that is designed to perform well under the di erent conditions that may arise. The algorithm searches the set of annotated query plans, pruning invalid and non-viable plans as early as possible in the search space, and it also uses a best- rst search strategy in order to produce a rst complete plan early in the search. We describe experiments to illustrate the performance of our algorithm. 1
151|High-Performance Sorting on Networks of Workstations|We report the performance of NOW-Sort, a collection of sorting implementations on a Network of Workstations (NOW). We find that paraflel sorting on a NOW is competitive to sorting on the large-scale SMPS that have traditionally held the performance records. On a 64-node cluster, we sort 6.0 GB in just under one minute, while a 32-node cluster finishes the Datamation benchmark in 2.41 seconds. Our implementations can be applied to a variety of disk, memory, and processor configurations; we highlight salient issues for tuning each component of the system. We evaluate the use of commodity operating systems and hardware for parallel sorting. We find existing OS primitives for memory management and file access adequate. Due to aggregate communication and disk bandwidth requirements, the bottleneck of our system is the workstation I/O bus.  
152|The MultiSpace: an Evolutionary Platform for Infrastructural Services|This paper presents the architecture for a Base, a clustered environment for building and executing highly available, scalable, but exible and adaptable infrastructure services. Our architecture has three organizing principles: addressing all of the dicult service faulttolerance, availability, and consistency problems in a carefully controlled environment, building that environment out of a collection of execution environments that are receptive to mobile code, and using dynamically generated code to introduce run-time-generated levels of indirection separating clients from services. We present a prototype Java implementation of a Base called the MultiSpace, and talk about two applications written on this prototype: the Ninja Jukebox (a cluster based music warehouse), and Keiretsu (an instant messaging service that supports heterogeneous clients). We show that the MultiSpace implementation successfully reduces the complexity of implementing services, and that the platform is conducive...
153|Online Dynamic Reordering for Interactive Data Processing|Abstract We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing.
154|Dynamic Query Re-Optimization|Very long-running queries in database systems are not uncommon in non-traditional application domains such as image processing or data warehousing analysis. Query optimization, therefore, is important. However, estimates of the query characteristics before query execution are usually inaccurate. Further, system configuration and resource availability may change during long evaluation period. As a result, queries are often evaluated with sub-optimal plan configurations. To remedy this situation, we have designed a novel approach to re-optimize suboptimal query plan configurations onthe -fly with Conquest --- an extensible and distributed query processing system. A dynamic optimizer considers reconfiguration cost as well as execution cost in determining the best query plan configuration. Experimental results are presented. 1 Introduction Parallelism is important in today&#039;s database query processing. Very long-running queries require parallel processing to deliver reasonable performance ...
155|How to Avoid Building DataBlades That Know the Value of Everything and the Cost of Nothing|The object-relational database management system (ORDBMS) offers many potential benefits for scientific, multimedia and financial applications. However, work remains in the integration of domain-specific class libraries into ORDBMS query processing. A major problem is that the standard mechanisms for query selectivity estimation, taken from relational database systems, rely on properties specific to the standard data types; creation of new mechanisms remains extremely difficult because the software interfaces provided by vendors are relatively low-level. In this paper, we discuss extensions of the generalized search tree, or GiST, to support a higher-level but less type-specific approach. Specifically, we discuss the computation of selectivity estimates with confidence intervals using a variety of index-based approaches and present results from an experimental comparison of these methods with several estimators from the literature.  1. 
156|Dynamic source routing in ad hoc wireless networks|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any established infrastructure or centralized administration. In such an environment, it may be necessary for one mobile host to enlist the aid of other hosts in forwarding a packet to its destination, due to the limited range of each mobile host’s wireless transmissions. This paper presents a protocol for routing in ad hoc networks that uses dynamic source routing. The protocol adapts quickly to routing changes when host movement is frequent, yet requires little or no overhead during periods in which hosts move less frequently. Based on results from a packet-level simulation of mobile hosts operating in an ad hoc network, the protocol performs well over a variety of environmental conditions such as host density and movement rates. For all but the highest rates of host movement simulated, the overhead of the protocol is quite low, falling to just 1 % of total data packets transmitted for moderate movement rates in a network of 24 mobile hosts. In all cases, the difference in length between the routes used and the optimal route lengths is negligible, and in most cases, route lengths are on average within a factor of 1.01 of optimal. 1.
157|Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers  (1994) |An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize adhoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.  
158|A Performance Comparison of Multi-Hop Wireless Ad Hoc Network Routing Protocols|An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Due to the limited transmission range of wireless network interfaces, multiple network &amp;quot;hops &amp;quot; may be needed for one node to exchange data with another across the network. In recent years, a variety of new routing protocols targeted specifically at this environment have been developed, but little performance information on each protocol and no realistic performance comparison between them is available. This paper presents the results of a detailed packet-level simulation comparing four multi-hop wireless ad hoc network routing protocols that cover a range of design choices: DSDV, TORA, DSR, and AODV. We have extended the ns-2 network simulator to accurately model the MAC and physical-layer behavior of the IEEE 802.11 wireless LAN standard, including a realistic wireless transmission channel model, and present the results of simulations of networks of 50 mobile nodes. 1
159|Next century challenges: Scalable coordination in sensor networks|Networked sensors-those that coordinate amongst them-selves to achieve a larger sensing task-will revolutionize information gathering and processing both in urban envi-ronments and in inhospitable terrain. The sheer numbers of these sensors and the expected dynamics in these environ-ments present unique challenges in the design of unattended autonomous sensor networks. These challenges lead us to hypothesize that sensor network coordination applications may need to be structured differently from traditional net-work applications. In particular, we believe that localized algorithms (in which simple local node behavior achieves a desired global objective) may be necessary for sensor net-work coordination. In this paper, we describe localized al-gorithms, and then discuss directed diffusion, a simple com-munication model for describing localized algorithms. 1
160|Power-Aware Routing in Mobile Ad Hoc Networks|In this paper we present a case for using new power-aware metrics for determining routes in wireless  ad hoc networks. We present five different metrics based on battery power consumption at nodes. We  show that using these metrics in a shortest-cost routing algorithm reduces the cost/packet of routing  packets by 5-30% over shortest-hop routing (this cost reduction is on top of a 40-70% reduction in energy  consumption obtained by using PAMAS, our MAC layer protocol). Furthermore, using these new metrics  ensures that the mean time to node failure is increased significantly. An interesting property of using  shortest-cost routing is that packet delays do not increase. Finally, we note that our new metrics can  be used in most traditional routing protocols for ad hoc networks.  1 
161|Topology Control of Multihop Wireless Networks using Transmit Power Adjustment| We consider the problem of adjusting the transmit powers of nodes in a multihop wireless network (also called an ad hoc network) to create a desired topology. We formulate it as a constrained optimization problem with two constraints- connectivity and biconnectivity, and one optimization objective- maximum power used. We present two centralized algorithms for use in static networks, and prove their optimality. For mobile networks, we present two distributed heuristics that adaptively adjust node transmit powers in response to topological changes and attempt to maintain a connected topology using minimum power. We analyze the throughput, delay, and power consumption of our algorithms using a prototype software implementation, an emulation of a power-controllable radio, and a detailed channel model. Our results show that the performance of multihop wireless networks in practice can be substantially increased with topology control.  
162|Next Century Challenges: Mobile Networking for “Smart Dust”|Large-scale networks of wireless sensors are becoming an active topic of research. Advances in hardware technology and engineering design have led to dramatic reductions in size, power consumption and cost for digital circuitry, wire-less communications and Micro ElectroMechanical Systems (MEMS). This has enabled very compact, autonomous and mobile nodes, each containing one or more sensors, computation and communication capabilities, and a power supply. The missing ingredient is the networking and applications layers needed to harness this revolutionary capability into a complete system. We review the key elements of the emergent technology of “Smart Dust ” and outline the research challenges they present to the mobile networking and systems community, which must provide coherent connectivity to large numbers of mobile network nodes co-located within a small volume. 
163|Energy Conserving Routing in Wireless Ad-hoc Networks| An ad-hoc network of wireless static nodes is considered as it arises in a rapidly deployed, sensor based, monitoring system. Information is generated in certain nodes and needs to reach a set of designated gateway nodes. Each node may adjust its power within a certain range that determines the set of possible one hop away neighbors. Traffic forwarding through multiple hops is employed when the intended destination is not within immediate reach. The nodes have limited initial amounts of energy that is consumed in different rates depending on the power level and the intended receiver. We propose algorithms to select the routes and the corresponding power levels such that the time until the batteries of the nodes drain-out is maximized. The algorithms are local and amenable to distributed implementation. When there is a single power level, the problem is reduced to a maximum flow problem with node capacities and the algorithms converge to the optimal solution. When there are multiple power levels then the achievable lifetime is close to the optimal (that is computed by linear programming) most of the time. It turns out that in order to maximize the lifetime, the traffic should be routed such that the energy consumption is balanced among the nodes in proportion to their energy reserves, instead of routing to minimize the absolute consumed power.
164|Distributed Database Systems |this article, we discuss the fundamentals of distributed DBMS technology. We address the data distribution and architectural design issues as well as the algorithms that need to be implemented to provide the basic DBMS functions such as query processing, concurrency control, reliability, and replication control.
165|Performance comparison of two on-demand routing protocols for ad hoc networks|Abstract — Ad hoc networks are characterized by multihop wireless connectivity, frequently changing network topology and the need for efficient dynamic routing protocols. We compare the performance of two prominent ondemand routing protocols for mobile ad hoc networks — Dynamic Source Routing (DSR) and Ad Hoc On-Demand Distance Vector Routing (AODV). A detailed simulation model with MAC and physical layer models is used to study interlayer interactions and their performance implications. We demonstrate that even though DSR and AODV share a similar on-demand behavior, the differences in the protocol mechanics can lead to significant performance differentials. The performance differentials are analyzed using varying network load, mobility and network size. Based on the observations, we make recommendations about how the performance of either protocol can be improved.
166|Online Aggregation|Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in postgres. 1 Introduction  Aggregation is an incre...
167|The state of the art in distributed query processing|Distributed data processing is fast becoming a reality. Businesses want to have it for many reasons, and they often must have it in order to stay competitive. While much of the infrastructure for distributed data processing is already in place (e.g., modern network technology), there are a number of issues which still make distributed data processing a complex undertaking: (1) distributed systems can become very large involving thousands of heterogeneous sites including PCs and mainframe server machines ? (2) the state of a distributed system changes rapidly because the load of sites varies over time and new sites are added to the system? (3) legacy systems need to be integrated|such legacy systems usually have not been designed for distributed data processing and now need to interact with other (modern) systems in a distributed environment. This paper presents the state of the art of query processing for distributed database and information systems. The paper presents the \textbook &amp;quot; architecture for distributed query processing and a series of techniques that are particularly useful for distributed database systems. These techniques include special join techniques, techniques to exploit intra-query parallelism, techniques to reduce communication costs, and techniques to exploit caching and replication of data. Furthermore, the paper discusses di erent kinds of distributed systems such as client-server, middleware (multi-tier), and heterogeneous database systems and shows how query processing works in these systems. Categories and subject descriptors: E.5 [Data]:Files ? H.2.4 [Database Management Systems]: distributed databases, query processing ? H.2.5 [Heterogeneous Databases]: data translation General terms: algorithms ? performance Additional key words and phrases: query optimization ? query execution ? client-server databases ? middleware ? multi-tier architectures ? database application systems ? wrappers? replication ? caching ? economic models for query processing ? dissemination-based information systems 1
168|Fjording the Stream: An Architecture for Queries over Streaming Sensor Data|If industry visionaries are correct, our lives will soon be full of sensors, connected together in loose conglomerations via wireless networks, each monitoring and collecting data about the environment at large. These sensors behave very differently from traditional database sources: they have intermittent connectivity, are limited by severe power constraints, and typically sample periodically and push immediately, keeping no record of historical information. These limitations make traditional database systems inappropriate for queries over sensors. We present the Fjords architecture for managing multiple queries over many sensors, and show how it can be used to limit sensor resource demands while maintaining high query throughput. We evaluate our architecture using traces from a network of traffic sensors deployed on Interstate 80 near Berkeley and present performance results that show how query throughput, communication costs, and power consumption are necessarily coupled in sensor environments.
169|XJoin: A Reactively-Scheduled Pipelined Join Operator|Wide-area distribution raises significant performance problems for traditional query processing techniques as data access becomes less predictable due to link congestion, load imbalances, and temporary outages. Pipelined query execution is a promising approach to coping with unpredictability in such environments as it allows scheduling to adjust to the arrival properties of the data. We have developed a non-blocking join operator, called XJoin, which has a small memory footprint, allowing many such operators to be active in parallel. XJoin is optimized to produce initial results quickly and can hide intermittent delays in data arrival by reactively scheduling background processing. We show that XJoin is an effective solution for providing fast query responses to users even in the presence of slow and bursty remote sources.  
170|Multimedia Database Systems|Though there are now numerous examples of multimedia systems in the commercial market, these systems have been developed primarily on a case-by-case basis. The largescale development of such systems requires a principled characterization of multimedia systems which is independent of any single application. It requires a unified query language framework to access these different structures in a variety of ways. It requires algorithms that are provably correct in processing such queries and whose efficiency can be appropriately evaluated. In this paper, we develop a framework for characterizing multimedia information systems which builds on top of the implementations of individual media, and provides a logical query language that integrates such diverse media. We develop indexing structures and algorithms to process such queries and show that these algorithms are sound and complete and relatively efficient (polynomial-time). We show that the generation of media-events (i.e. generating di...
171|Adaptive Selectivity Estimation Using Query Feedback|In this paper, we propose a novel approach for estimating the record selectivities of database queries. The real attribute value distribution is adaptively approximated by a curve-fitting function using a query feedback mechanism. This approach has the advantages of requiring no extra database access overhead for gathering statistics and of being able to continuously adapt the value distribution through queries and updates. Experimental results show that the estimation accuracy of this approach is comparable to traditional methods based on statistics gathering. 1 Introduction  In most database systems, the task of query optimization is to choose an efficient execution plan. Best plan selection requires accurate estimates of the costs of alternative plans. One of the most important factors that affects plan cost is selectivity, which is the number of tuples satisfying a given predicate. Therefore, in most cases, the accuracy of selectivity estimates directly affects the choice of best p...
172|The design and implementation of a sequence database system|This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. There are three distinct contributions made in this paper. (1) We describe the specification of sequence queries using the SEQUIN query language. (2) We quantitatively demonstrate the importance of various storage and optimization techniques by studying their effect on performance. (3) We present a novel nested design paradigm used in PREDATOR to combine sequence ‘and relational data.
173|Piconet: Embedded Mobile Networking|Piconet is a general-purpose, low-power ad hoc radio network. It provides a base level of connectivity to even the simplest of sensing and  computing objects. It is our intention that a full range of portable and embedded devices may make use of this connectivity. This article outlines  the Piconet system, under development at the Olivetti and Oracle Research Laboratory (ORL). The authors discuss the motivation for providing  this low-level &#034;embedded networking,&#034; and describe their experiences of building such a system. The article concludes with a commentary on  some of the implications that power saving, and other considerations central to Piconet, have on the design of the system.
174|Parallel Query Processing|With relations growing larger and queries becoming more complex, parallel query processing is an increasingly attractive option for improving the performance of database systems. The objective of this paper is to examine the various issues encountered in parallel query processing and the techniques available for addressing these issues. The focus of the paper is on the join operation with both sort-merge join and hash joins being considered. Three types of parallelism can be exploited, namely intra-operator, inter-operator, and inter-query parallelism. In intra-operator parallelism the major issue is task creation, and the objective is to split a join operation into tasks in a manner such that the load can be spread evenly across a given number of processors. This is a challenge when the values on the join attribute are not uniformly distributed. Inter-operator parallelism can be achieved either through parallel execution of independent operations or through pipelining. In either case,...
175|SEQ: A Model for Sequence Databases|This paper presents the model which is the basis for a system to manage various kinds of sequence data. The model separates the data from the ordering information, and includes operators based on two distinct abstractions of a sequence. The main contributions of the model are: (a) it can deal with different types of sequence data, (b) it supports an expressive range of sequence queries, (c) it draws from many of the diverse existing approaches to modeling sequence data. 1
176|GADT: A Probability Space ADT for Representing and Querying the Physical World|Large sensor networks are being widely deployed for measurement, detection, and monitoring applications. Many of these applications involve database systems to store and process data from the physical world. This data has inherent measurement uncertainties that are properly represented by continuous probability distribution functions (pdf&#039;s). We introduce a new object-relational data type, the Gaussian ADT GADT, that models physical data as gaussian pdf&#039;s, and we show that existing index structures can be used as fast access methods for GADT data. We also present a measure-theoretic model of probabilistic data and evaluate GADT in its light.
177|Dynamic Query Optimization on a Distributed Object Management Platform|A Distributed Object Management (DOM) architecture, when used as the infrastructure of a multidatabase system, not only enables easy and flexible interoperation of DBMSs, but also facilitates interoperation of the multidatabase system with other repositories that do not have DBMS capabilities. This is an important advantage, since most of data still resides on repositories that do not have DBMS capabilities. In this paper, we describe a dynamic query optimization technique for a multidatabase system, namely MIND, implemented on a DOM environment. Dynamic query optimization, which schedules intersite operations at runtime, fits better to such an environment since it benefits from location transparency provided by the DOM framework. In this way, the dynamic changes in the configuration of system resources such as a relocated DBMS or a new mirror to an existing DBMS, do not affect the optimized query execution in the system. Furthermore, the uncertainty in estimating the appearance times ...
178|Memory Management during Run Generation in External Sorting|If replacement selection is used in an external mergesort to generate initial runs, individual records are deleted and in-  serted in the sort operation&#039;s workspace. Variable-length records introduce the need for possibly complex memory management and extra copying of records. As a result, few systems employ replacement selection, even though it produces longer runs than commonly used algorithms. We experimentally compared several algorithms and variants for managing this workspace. We found that the simple best fit algorithm achieves memory utilization of 90% or better and run lengths over 1.8 times workspace size, with no extra copying of records and very little other overhead, for widely varying record sizes and for a wide range of memory sizes. Thus, replacement selection is a viable algorithm for commercial database systems, even for variable-length records.  Efficient memory management also enables an external sort algorithm that degrades gracefully when its input is only slightl...
179|Exokernel: An Operating System Architecture for Application-Level Resource Management|We describe an operating system architecture that securely multiplexes machine resources while permitting an unprecedented degree of application-specific customization of traditional operating system abstractions. By abstracting physical hardware resources, traditional operating systems have significantly limited the performance, flexibility, and functionality of applications. The exokernel architecture removes these limitations by allowing untrusted software to implement traditional operating system abstractions entirely at application-level. We have implemented a prototype exokernel-based system that includes Aegis, an exokernel, and ExOS, an untrusted application-level operating system. Aegis defines the low-level interface to machine resources. Applications can allocate and use machine resources, efficiently handle events, and participate in resource revocation. Measurements show that most primitive Aegis operations are 10–100 times faster than Ultrix,a mature monolithic UNIX operating system. ExOS implements processes, virtual memory, and inter-process communication abstractions entirely within a library. Measurements show that ExOS’s application-level virtual memory and IPC primitives are 5–50 times faster than Ultrix’s primitives. These results demonstrate that the exokernel operating system design is practical and offers an excellent combination of performance and flexibility. 1
180|Active Messages: a Mechanism for Integrated Communication and Computation|The design challenge for large-scale multiprocessors is (1) to minimize communication overhead, (2) allow communication to overlap computation, and (3) coordinate the two without sacrificing processor cost/performance. We show that existing message passing multiprocessors have unnecessarily high communication costs. Research prototypes of message driven machines demonstrate low communication overhead, but poor processor cost/performance. We introduce a simple communication mechanism, Active Messages, show that it is intrinsic to both architectures, allows cost effective use of the hardware, and offers tremendous flexibility. Implementations on nCUBE/2 and CM-5 are described and evaluated using a split-phase shared-memory extension to C, Split-C. We further show that active messages are sufficient to implement the dynamically scheduled languages for which message driven machines were designed. With this mechanism, latency tolerance becomes a programming/compiling concern. Hardware support for active messages is desirable and we outline a range of enhancements to mainstream processors.  
181|Efficient Software-Based Fault Isolation|One way to provide fault isolation among cooperating software modules is to place each in its own address space. However, for tightly-coupled modules, this solution incurs prohibitive context switch overhead. In this paper, we present a software approach to implementing fault isolation within a single address space. Our approach has two parts. First, we load the code and data for a distrusted module into its own fault domain, a logically separate portion of the application&#039;s address space. Second, we modify the object code of a distrusted module to prevent it from writing or jumping to an address outside its fault domain. Both these software operations are portable and programming language independent. Our approach poses a tradeo relative to hardware fault isolation: substantially faster communication between fault domains, at a cost of slightly increased execution time for distrusted modules. We demonstrate that for frequently communicating modules, implementing fault isolation in software rather than hardware can substantially improve end-to-end application performance.
182|Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism|Threads are the vehicle,for concurrency in many approaches to parallel programming. Threads separate the notion of a sequential execution stream from the other aspects of traditional UNIX-like processes, such as address spaces and I/O descriptors. The objective of this separation is to make the expression and control of parallelism sufficiently cheap that the programmer or compiler can exploit even fine-grained parallelism with acceptable overhead. Threads can be supported either by the operating system kernel or by user-level library code in the application address space, but neither approach has been fully satisfactory. This paper addresses this dilemma. First, we argue that the performance of kernel threads is inherently worse than that of user-level threads, rather than this being an artifact of existing implementations; we thus argue that managing par- allelism at the user level is essential to high-performance parallel computing. Next, we argue that the lack of system integration exhibited by user-level threads is a consequence of the lack of kernel support for user-level threads provided by contemporary multiprocessor operating systems; we thus argue that kernel threads or processes, as currently conceived, are the wrong abstraction on which to support user- level management of parallelism. Finally, we describe the design, implementation, and performance of a new kernel interface and user-level thread package that together provide the same functionality as kernel threads without compromis- ing the performance and flexibility advantages of user-level management of parallelism.
183|Extensibility, safety and performance in the SPIN operating system|This paper describes the motivation, architecture and performance of SPIN, an extensible operating system. SPIN provides an extension infrastructure, together with a core set of extensible services, that allow applications to safely change the operating system&#039;s interface and implementation. Extensions allow an application to specialize the underlying operating system in order to achieve a particular level of performance and functionality. SPIN uses language and link-time mechanisms to inexpensively export ne-grained interfaces to operating system services. Extensions are written in a type safe language, and are dynamically linked into the operating system kernel. This approach o ers extensions rapid access to system services, while protecting the operating system code executing within the kernel address space. SPIN and its extensions are written in Modula-3 and run on DEC Alpha workstations. 1
184|Lightweight remote procedure call|Lightweight Remote Procedure Call (LRPC) is a communication facility designed and optimized for communication between protection domains on the same machine. In contemporary small-kernel operating systems, existing RPC systems incur an unnecessarily high cost when used for the type of communication that predominates-between protection domains on the same machine. This cost leads system designers to coalesce weakly related subsystems into the same protection domain, trading safety for performance. By reducing the overhead of same-machine communication, LRPC encourages both safety and performance. LRPC combines the control transfer and communication model of capability systems with the programming semantics and large-grained protection model of RPC. LRPC achieves a factor-of-three performance improvement over more traditional approaches based on independent threads exchanging messages, reducing the cost of same-machine communication to nearly the lower bound imposed by conventional hardware. LRPC has been integrated into the Taos operating system of the DEC SRC Firefly multiprocessor workstation.
185|Experiences with the amoeba distributed operating system|The Amoeba distributed operating system has been in development and use for over eight years now. In this paper we describe the present system and our experience with it—what we did right, but also what we did wrong. Among the things done right were basing the system on objects, using a single uniform mechanism (capabilities) for naming and protecting them in a location independent way, and designing a completely new, and very fast file system. Among the things done wrong were having threads not be pre-emptable, initially building our own homebrew window system, and not having a multicast facility at the outset.
186|The Packet Filter: An Efficient Mechanism for User-level Network Code|Code to implement network protocols can be either inside the kernel of an operating system or in user-level processes. Kernel-resident code is hard to develop, debug, and maintain, but user-level implementations typically incur significant overhead and perform poorly.  The performance of user-level network code depends on the mechanism used to demultiplex received packets. Demultiplexing in a user-level process increases the rate of context switches and system calls, resulting in poor performance. Demultiplexing in the kernel eliminates unnecessary overhead.  This paper describes the packet filter, a kernel-resident, protocolindependent packet demultiplexer. Individual user processes have great flexibility in selecting which packets they will receive. Protocol implementations using the packet filter perform quite well, and have been in production use for several years.  
187|Hints for Computer Systems Design|Studying the design and implementation of a number of computer has led to some general hints for system design. They are described here and illustrated by many examples, ranging from hardware such as the Alto and the Dorado to application programs such as Bravo and Star. 1.
188|Overview of the CHORUS Distributed Operating Systems|The CHORUS technology has been designed for building new generations of open, distributed, scalable operating systems. CHORUS has the following main characteristics:  # a communication-based architecture, relying on a minimal Nucleus which integrates distributed processing and communication at the lowest level, and which implements generic services used by a set of subsystem servers to extend standard operating system interfaces. A UNIX subsystem has been developed; other subsystems such as objectoriented systems are planned;  # a real-time Nucleus providing real-time services which are accessible to system programmers;   # a modular architecture providing scalability, and allowing, in particular, dynamic configuration of the system and its applications over a wide range of hardware and network configurations, including parallel and multiprocessor systems. CHORUS-V3 is the current version of the CHORUS Distributed Operating System, developed by Chorus systemes. Earlier versions were st...
189|Virtual Memory Primitives for User Programs|Memory Management Units (MMUs) are traditionally used by operating systems to implement disk-paged virtual memory. Some operating systems allow user programs to specify the protection level (inaccessible, readonly. read-write) of pages, and allow user programs t.o handle protection violations. bur. these mechanisms are not. always robust, efficient, or well-mat. ched to the needs of applications.
190|Improving IPC by kernel design|Inter-process communication (ipc) has to be fast and e ective, otherwise programmers will not use remote procedure calls (RPC), multithreading and multitasking adequately. Thus ipc performance is vital for modern operating systems, especially µ-kernel based ones. Surprisingly, most µ-kernels exhibit poor ipc performance, typically requiring 100 µs for a short message transfer on a modern processor, running with 50 MHz clock rate. In contrast, we achieve 5 µs; a twentyfold improvement. This paper describes the methods and principles used, starting from the architectural design and going down to the coding level. There is no single trick to obtaining this high performance; rather, a synergetic approach in design and implementation on all levels is needed. The methods and their synergy are illustrated by applying them to a concrete example, the L3-kernel (an industrial-quality operating system in daily use at several hundred sites). The main ideas are to guide the complete kernel design by the ipc requirements, and to make heavy use of the concept of virtual address space inside the-kernel itself. As the L3 experiment shows, significant performance gains are possible: compared with Mach, they range from a factor of 22 (8-byte messages) to 3 (4-Kbyte messages). Although hardware specific details in uence both the design and implementation, these techniques are applicable to the whole class of conventional general
191|Experiences with a High-Speed Network Adaptor: A Software Perspective|This paper describes our experiences, from a software perspective, with the OSIRIS network adaptor. It first identifies the problems we encountered while programming OSIRIS and optimizing network performance, and outlines how we either addressed them in the software, or had to modify the hardware. It then describes the opportunities provided by OSIRIS that we were able to exploit in the host operating system (OS); opportunities that suggested techniques for making the OS more effective in delivering network data to application programs. The most novel of these techniques, called application device channels, gives application programs running in user space direct access to the adaptor. The paper concludes with the lessons drawn from this work, which we believe will benefit the designers of future network adaptors. 1 Introduction  With the emergence of high-speed network facilities, several research efforts are focusing on the design and implementation of network adaptors [5, 2, 3, 16, 2...
192|Stride Scheduling: Deterministic Proportional-Share Resource Management|This paper presents stride scheduling, a deterministic scheduling technique that efficiently supports the same flexible resource management abstractions introduced by lottery scheduling. Compared to lottery scheduling, stride scheduling achieves significantly improved accuracy over relative throughput rates, with significantly lower response time variability. Stride scheduling implements proportional-share control over processor time and other resources by cross-applying elements of rate-based flow control algorithms designed for networks. We introduce new techniques to support dynamic changes and higher-level resource management abstractions. We also introduce a novel hierarchical stride scheduling algorithm that achieves better throughput accuracy and lower response time variability than prior schemes. Stride scheduling is evaluated using both simulations and prototypes implemented for the Linux kernel.
193|Accent: A Communication Oriented Network Operating System Kernel|Accent is a communication oriented operating system kernel being built at Carnegie-Mellon University to support the distributed personal computing project, Spice, and the development of a fault-tolerant distributed sensor network (DSN). Accent is. built around a single, powerful abstraction of communication between processes, with all kernel functions, such as device access and virtual memory management accessible through messages and distributable throughout a network. In this paper, specific attention is given to system supplied facilities which support transparent network access and fault-tolerant behavior. Many of these facilities are already being provided under a modified version of VAX/UNIX. The Accent system itself is currently being implemented on the Three Rivers Corp. PERQ. Keywords: Inter-process communication, networking,
194|Implementation and Performance of Application-Controlled File Caching|Traditional file system implementations do not allow applications to control file caching replacement decisions.  We have implemented two-level replacement, a scheme that allows applications to control their own cache replacement, while letting the kernel control the allocation of cache space among processes. We designed an interface to let applications exert control on replacement via a set of directives to the kernel. This is effective and requires low overhead.  We demonstrate that for applications that do not perform well under traditional caching policies, the combination of good application-chosen replacement strategies, and our kernel allocation policy LRU-SP, can reduce the number of block I/Os by up to 80%, and can reduce the elapsed time by up to 45%. We also show that LRU-SP is crucial to the performance improvement for multiple concurrent applications: LRUSP fairly distributes cache blocks and offers protection against foolish applications.   
195|PATHFINDER: A Pattern-Based Packet Classifier|This paper describes a pattern-based approach to building packet classifiers. One novelty of the approach is that it can be implemented efficiently in both software and hardware. A performance study shows that the software implementation is about twice as fast as existing mechanisms, and that the hardware implementation is currently able to keep up with OC-12 (622Mbps) network links and is likely to operate at gigabit speeds in the near future. 1 Introduction A packet classifier is a mechanism that inspects incoming network packets, and based on the values found in select header fields, determines how each is to be processed. A classifier can be thought of as assigning a tag (type) to each packet, or alternatively, identifying the flow or path to which the packet belongs. We call this mechanism a packet classifier rather than a packet filter because it more accurately describes the function being performed---it classifies all packets, rather than filtering out select packets. Packet ...
196|Efficient Packet Demultiplexing for Multiple Endpoints and Large Messages|This paper describes a new packet filter mechanism that efficiently dispatches incoming network packets to one of multiple endpoints, for example address spaces. Earlier packet filter systems iteratively applied each installed filter against every incoming packet, resulting in high processing overhead whenever multiple filters existed. Our new packet filter provides an associative match function that enables similar but not identical filters to be combined together into a single filter. The filter mechanism, which we call the Mach Packet Filter (MPF), has been implemented for the Mach 3.0 operating system and is being used to support endpoint-based protocol processing, whereby each address space implements its own suite of network protocols. With large numbers of registered endpoints, MPF outperforms the earlier BSD Packet Filter (BPF) by over a factor of four. MPF also allows a filter program to dispatch fragmented packets, which was quite difficult with previous filter mechanisms.  
197|Sharing and protection in a single-address-space operating system|This article explores memory sharing and protection support in Opal, a single-address-space operating system designed for wide-address (64-bit) architectures. Opal threads execute within protection domains in a single shared virtual address space. Sharing is simplified, because addresses are context independent. There is no loss of protection, because addressability and access are independent; the right to access a segment is determined by the protection domain in which a thread executes. This model enables beneficial code- and data-sharing patterns that are currently prohibitive, due in part to the inherent restrictions of multiple address spaces, and in part to Unix programming style. We have designed and implemented an Opal prototype using the Mach 3.0 microkernel as a base. Our implementation demonstrates how a single-address-space structure can be supported alongside of other environments on a modern microkernel operating system, using modern wide-address architectures. This article justifies the opal model and its goals for sharing and protection, presents the system and its abstractions, describes the prototype implementation,
198|Threads and Input/Output in the Synthesis kernel|The Synthesis operating system kernel combines several techniques to provide high performance, including kernel code synthesis, ne-grain scheduling, and optimistic synchronization. Kernel code synthesis reduces the execution path for frequently used kernel calls. Optimistic synchronization increases concurrency within the kernel. Their combination results in signi cant performance improvement over traditional operating system implementations. Using hardware and software emulating a SUN 3/160 running SUNOS, Synthesis achieves several times to several dozen times speedup for Unix kernel calls and context switch times of 21 microseconds or faster. 1
199|Limits to Low-Latency Communication on High-Speed Networks|The throughput of local area networks is rapidly increasing. For example, the bandwidth of new ATM networks and FDDI token rings is an order of magnitude greater than that of Ethernets. Other network technologies promise a bandwidth increase of yet another order of magnitude in a few years. However, in distributed systems, lowered latency rather than increased throughput is often of primary concern. This paper examines the system-level effects of newer high-speed network technologies on low-latency, cross-machine communications. To evaluate a number of influences, both hardware and software, we designed and imple-mented a new remote procedure call system targeted at providing low latency. We then ported this system to several hardware platforms (DECstation and SPARCstation) with several differ-ent networks and controllers (ATM, FDDI, and Ethernet). Comparing these systems allows us to explore the performance impact of alternative designs in the communication system with respect to achieving low latency, e.g., the network, the network controller, the host architecture and cache system, and the kernel and user-level runtime software. Our RPC system, which achieves substantially reduced call times (170 pseconds on an ATM network using DECstation 5000/200 hosts), allow us to isolate those components of next-
200|Pilot: An Operating System for a Personal Computer|this paper was presented at the 7th ACM Symposium on Operating Systems Principles, Pacific Grove, Calif., Dec. 10-12, 1979
201|Synthesis: An Efficient Implementation of Fundamental Operating System Services|This dissertation shows that operating systems can provide fundamental services an order of magnitude more efficiently than traditional implementations. It describes the implementation of a new operating system kernel, Synthesis, that achieves this level of performance. The Synthesis kernel combines several new techniques to provide high performance without sacrificing the expressive power or security of the system. The new ideas include: ffl Run-time code synthesis --- a systematic way of creating executable machine code at runtime to optimize frequently-used kernel routines --- queues, buffers, context switchers, interrupt handlers, and system call dispatchers --- for specific situations, greatly reducing their execution time. ffl Fine-grain scheduling --- a new process-scheduling technique based on the idea of feedback that performs frequent scheduling actions and policy adjustments (at submillisecond intervals) resulting in an adaptive, self-tuning system that can support real-ti...
202|Architectural support for translation table management in large address space machines|Virtual memoy page translation tables provide mappings from virtual to physical addresses. When the hardware controlled Tratmlation L.ookaside Buffers (TLBs) do not contain a translation, these tables provide the translation. Approaches to the structure and management of these tables vary from full hardware implementations to complete software based algon”thms. The size of the virtual aaliress space used by processes is rapidly growing beyond 32 bits of address. As the utilized address space increases, new problems and issues surjace. Traditional methoak for managing the page translation tables are inappropriate for large address space architectures. The Hashed Page Table (HPI’), described here, provides a very fast and space ejicient translation table that reduces ovdwad by splitting TLB management responsibilities between hardware and software. Measurements demonstrate its applicability to a diverse range of operating systems and workloads and, in particular, to large virtual address space machines. In simulations of over 4 billion instructions, improvements of 5 to IO % were observed. 1.
203|Design Tradeoffs for Software-Managed TLBs|this paper appeared in the Proceedings of the 20th Annual International Symposium on Computer Architecture, San Diego, May 1993. Authors&#039; address: Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109-2122 This work was supported by Defense Advanced Research Projects Agency under DARPA/ARO Contract Number DAAL03-90-C-0028 and a National Science Foundation Graduate Fellowship.  Uhlig et al. . 2 within the kernel. These and related operating system trends place greater stress upon the TLB by increasing miss rates and, hence, decreasing overall system performance. This paper explores these issues by examining design trade-offs for software-managed TLBs and their impact, in conjunction with various operating systems, on overall system performance. To examine issues which cannot be adequately modeled with simulation, we have developed a system analysis tool called Monster, which enables us to monitor actual systems. We have also developed a novel TLB simulator called Tapeworm, which is compiled directly into the operating system so that it can intercept all TLB misses caused by both user process and OS kernel memory references. The information that Tapeworm extracts from the running system is used to obtain TLB miss counts and to simulate different TLB configurations. The remainder of this paper is organized as follows: Section 2 examines previous TLB and OS research related to this work. Section 3 describes our analysis tools, Monster and Tapeworm. The MIPS R2000 TLB structure and its performance under Ultrix, OSF/1 and Mach 3.0 are explored in Section 4. Hardware- and software-based performance improvements are presented in Section 5. Section 6 summarizes our conclusions. 2 RELATED WORK By caching page table entries, TLBs g...
204|SPIN - an extensible microkernel for application-specific operating system services|Application domains, such as multimedia, databases, and parallel computing, require operating system services with high performance and high functionality. Existing operating systems provide fixed interfaces and implementations to system services and resources. This makes them inappropriate for applications whose resource demands and usage patterns are poorly matched by the services provided. The SPIN operating system enables system services to be defined in an application-specific fashion through an extensible microkernel. It offers fine-grained control over a machine&#039;s logical and physical resources to applications through run-time adaptation of the system to application requirements. 1
205|Hardware and Software Support for Efficient Exception Handling|Program-synchronous exceptions, for example, breakpoints, watchpoints, illegal opcodes, and memory access violations, provide information about exceptional conditions, interrupting the program and vectoring to an operating system handler. Over the last decade, however, programs and run-time systems have increasingly employed these mechanisms as a performance optimization to detect normal and expected conditions. Unfortunately, current architecture and operating system structures are designed for exceptional or erroneous conditions, where performance is of secondary importance, rather than normal conditions. Consequently, this has limited the practicality of such hardware-based detection mechanisms. We propose both hardware and software structures that permit efficient handling of synchronous exceptions by user-level code. We demonstrate a software implementation that reduces exceptiondelivery cost by an order-of-magnitude on current RISC processors, and show the performance benefits o...
206|Fast Mutual Exclusion for Uniprocessors|In this paper we describe restartable atomic sequences, an optimistic mechanism for implementing simple atomic operations (such as Test-And-Set) on a uniprocessor. A thread that is suspended within a restartable atomic sequence is resumed by the operating system at the beginning of the sequence, rather than at the point of suspension. This guarantees that the thread eventually executes the sequence atomically. A restartable atomic sequence has signi cantly less overhead than other software-based synchronization mechanisms, such askernel emulation or software reservation. Consequently, it is an attractive alternative for use on uniprocessors that do not support atomic operations. Even on processors that do support atomic operations in hardware, restartable atomic sequences can have lower overhead. We describe di erent implementations of restartable atomic sequences for the Mach 3.0 and Taos operating systems. These systems &#039; thread management packages
207|Software Prefetching and Caching for Translation Lookaside Buffers|A number of interacting trends in operating system structure, processor architecture, and memory systems are increasing both the rate of translation lookaside buffer (TLB) misses and the cost of servicing a miss. This paper presents two novel software schemes, implemented under Mach 3.0, to decrease both the number and the cost of kernel TLB misses (i.e., misses on kernel data structures, including user page tables). The first scheme is a new use of prefetching for TLB entries on the IPC path, and the second scheme is a new use of software caching of TLB entries for hierarchical page table organizations.  For a range of applications, prefetching decreases the number of kernel TLB misses by 40% to 50%, and caching decreases TLB penalties by providing a fast path for over 90% of the misses. Our caching scheme also decreases the number of nested TLB traps due to the page table hierarchy, reducing the number of kernel TLB miss traps for applications by 20% to 40%. Prefetching and caching, ...
208|Tools for the Development of Application-Specific Virtual Memory Management|While many applications incur few page faults, some scientific and database applications perform poorly when running on top of a traditional virtual memory implementation. To help address this problem, several systems have been built to allow each program the flexibility to use its own application-specific page replacement policy, in place of the generic policy provided by the operating system. This has the potential to improve performance for the class of applications limited by virtual memory behavior; however, to realize this performance gain, application developers must re-implement much of the virtual memory system, a non-trivial programming task. Our goal is to make it easy for programmers to develop new application-specific page replacement policies. To do this, we have implemented (i) an extensible object-oriented user-level virtual memory system and (ii) a graphical performance monitor for virtual memory behavior. Together, these help the user to identify problems with an appl...
210|The Operating System Kernel as a Secure Programmable Machine|To provide modularity and performance, operating system kernels should have only minimal embedded functionality. Today&#039;s operating systems are large, inefficient and, most importantly, inflexible. In our view, most operating system performance and flexibility problems can be eliminated simply by pushing the operating system interface lower. Our goal is to put abstractions traditionally implemented by the kernel out into user-space, where user-level libraries and servers abstract the exposed hardware resources. To achieve this goal, we have defined a new operating system structure, exokernel, that safely exports the resources defined by the underlying hardware. To enable applications to benefit from full hardware functionality and performance, they are allowed to download additions to the supervisor-mode execution environment. To guarantee that these extensions are safe, techniques such as code inspection, inlined cross-domain procedure calls, and secure languages are used. To test and ...
211|An open operating system for a single-user machine|The file system and modularization of a single-user operating system are described. The main points of interest are the openness of the system, which establishes no sharp boundary between itself and the user&#039;s programs, and the techniques used to make the system robust. 1.
212|How to Use a 64-Bit Virtual Address Space|Most operating systems execute programs in private address spaces communicating through messages or files. The traditional private address space model was developed for 16- and 32-bit architectures, on which virtual addresses are a scarce resource. The recent appearance of architectures with flat 64-bit virtual addressing opens an opportunity to reconsider our use of virtual address spaces. In this paper we argue for an alternative addressing model, in which all programs and data reside in a single global virtual address space shared by multiple protection domains. Hardware-based memory protection exists within the single address space, providing firewalls as strong as in conventional systems. We explore the tradeoffs in the use of a global virtual address space relative to the private address space model. We contend that a shared address space can eliminate obstacles to efficient sharing and exchange of data structures that are inherent in private address space systems. The shared add...
213|Vino: an integrated platform for operating systems and database research|In 1981, Stonebraker wrote: Operating system services in many existing systems are either too slow or inappropriate. Current DBMSs usually provide their own and make little or no use of those o ered by the operating system. [STON81] The standard operating system model has changed little since that time, and we believe that, at its core, it is the wrong model for DBMS and other resource-intensive applications. The standard model is in exible, uncooperative, and irregular in its treatment of resources. We describe the design of a new system, the VINO kernel, which addresses the limitations of standard operating systems. It focuses on three key ideas: Applications direct policy. Kernel mechanisms are reusable by applications. All resources share a common extensible interface. VINO&#039;s power and exibility make it an ideal platform for the design and implementation of traditional and modern database management systems. 1
214|SPACE: A New Approach to Operating System Abstraction|Object-oriented operating systems, as well as conventional O/S designs, present an overly restrictive level of abstraction to the programmer. Models of objects, processes, concurrency, etc., are embedded within the system in such a way that they are difficult to extend or replace. SPACE is an extensible operating system being developed for research into object-oriented and distributed systems design. SPACE uses capability mechanisms based on the manipulation of address spaces to provide low-level kernel primitives from which higherlevel abstractions can be constructed. Standard micro-kernel abstractions such as processes, virtual memory, interprocess communication, and object models are built outside the kernel in SPACE, using the SPACE-kernel primitives: spaces, domains, and portals. Multiple versions of the standard O/S abstractions can coexist and interact.
216|FUGU: Implementing Translation and Protection in a  Multiuser Multimodel Multiprocessor|Multimodel multiprocessors provide both shared memory and message passing primitives to the  user for efficient communication. In a multiuser machine, translation permits machine resources to be  virtualized and protection permits users to be isolated. The challenge in a multiuser multiprocessor is  to provide translation and protection sufficient for general-purpose computing without compromising  communication performance, particularly the performance of communication between parallel threads  belonging to the same computation. FUGU is a proposed architecture that integrates translation and  protection with a set of communication mechanisms originally designed for high performance on a  single-user, physically-addressed, large-scale, multimodel multiprocessor. Communication
217|A complete universal query operator. |We argue that the division operator in relational algebra is insufficiently equipped to handle all kinds of universal queries. This is a problem since division is widely regarded as the operational counterpart of universal quantification. Accordingly, research related to universal queries, for instance on query-optimization, is based on division. We propose generalized division operators to provide better approximations of the concept of universal quantification. 1
218|TreadMarks: Distributed Shared Memory on Standard Workstations and Operating Systems|TreadMarks is a distributed shared memory (DSM) system for standard Unix systems such as  SunOS and Ultrix. This paper presents a performance evaluation of TreadMarks running on  Ultrix using DECstation-5000/240&#039;s that are connected by a 100-Mbps switch-based ATM LAN  and a 10-Mbps Ethernet. Our objective is to determine the efficiency of a user-level DSM implementation  on commercially available workstations and operating systems.  We achieved good speedups on the 8-processor ATM network for Jacobi (7.4), TSP (7.2), Quicksort  (6.3), and ILINK (5.7). For a slightly modified version of Water from the SPLASH benchmark  suite, we achieved only moderate speedups (4.0) due to the high communication and synchronization  rate. Speedups decline on the 10-Mbps Ethernet (5.5 for Jacobi, 6.5 for TSP, 4.2 for  Quicksort, 5.1 for ILINK, and 2.1 for Water), reflecting the bandwidth limitations of the Ethernet.  These results support the contention that, with suitable networking technology, DSM is a...
219|Memory Coherence in Shared Virtual Memory Systems|This paper studies the memory coherence problem in designing  said inaplementing a shared virtual memory on looselycoupled  multiprocessors. Two classes of aIgoritb. ms for solving  the problem are presented. A prototype shared virtual  memory on an Apollo ring has been implemented based  on these algorithms. Both theoretical and practical results  show tkat the mentory coherence problem cast indeed be  solved efficiently on a loosely-coupled multiprocessor.
220|PVM: A Framework for Parallel Distributed Computing|The PVM system is a programming environment for the development and execution of large concurrent or parallel applications that consist of many interacting, but relatively independent, components. It is intended to operate on a collection of heterogeneous computing elements interconnected by one or more networks. The participating processors may be scalar machines, multiprocessors, or special-purpose computers, enabling application components to execute on the architecture most appropriate to the algorithm. PVM provides a straightforward and general interface that permits the description of various types of algorithms (and their interactions), while the underlying infrastructure permits the execution of applications on a virtual computing environment that supports multiple parallel computation models. PVM contains facilities for concurrent, sequential, or conditional execution of application components, is portable to a variety of architectures, and supports certain forms of error dete...
221|Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors|Scalable shared-memory multiprocessors distribute memory among the processors and use scalable interconnection networks to provide high bandwidth and low latency communication. In addition, memory accesses are cached, buffered, and pipelined to bridge the gap between the slow shared memory and the fast processors. Unless carefully controlled, such architectural optimizations can cause memory accesses to be executed in an order different from what the programmer expects. The set of allowable memory access orderings forms the memory consistency model or event ordering model for an architecture.
222|Implementation and performance of Munin|Munin is a distributed shared memory (DSM) system that allows shared memory parallel programs to be executed efficiently on distributed memory multiprocessors. Munin is unique among existing DSM systems in its use of multiple consistency protocols and in its use of release consistency. In Munin, shared program variables are annotated with their expected access pattern, and these annotations are then used by the runtime system to choose a consistency protocol best suited to that access pattern. Release consistency allows Munin to mask network latency and reduce the number of messages required to keep memory consistent. Munin&#039;s multiprotocol release consistency is implemented in software using a delayed update queue that buffers and merges pending outgoing writes. A sixteen-processor prototype of Munin is currently operational. We evaluate its implementation and describe the execution of two Munin programs that achieve performance within ten percent of message passing implementations of the same programs. Munin achieves this level of performance with only minor annotations to the shared memory programs.  
223|Evaluation of Release Consistent Software Distributed Shared Memory on Emerging Network Technology |We evaluate the effect of processor speed, network characteristics, and software overhead on the performance of release-consistent software distributed shared memory. We examine five different protocols for implementing release consistency: eager update, eager invalidate, lazy update, lazy invalidate, and a new protocol called lazy hybrid. This lazy hybrid protocol combines the benefits of both lazy update and lazy invalidate. Our simulations indicate that with the processors and networks that are becoming available, coarse-grained applications such as Jacobi and TSP perform well, more or less independent of the protocol used. Medium-grained applications, such as Water, can achieve good performance, but the choice of protocol is critical. For sixteen processors, the best protocol, lazy hybrid, performed more than three times better than the worst, the eager update. Fine-grained applications such as Cholesky achieve little speedup regardless of the protocol used because of the frequency of synchronization operations and the high latency involved. While the use of relaxed memory models, lazy implementations, and multiple-writer protocols has reduced the impact of false sharing, synchronization latency remains a serious problem for software distributed shared memory systems. These results suggest that future work on software DSMs should concentrate on reducing the amount ofsynchronization or its effect.  
224|Weak Ordering -- A New Definition|A memory model for a shared memory, multiprocessor commonly and often implicitly assumed by programmers is that of sequential consistency. This model guarantees that all memory accesses will appear to execute atomically and in program order. An alternative model, weak ordering, offers greater performance potential. Weak ordering was first defined by Dubois, Scheurich and Briggs in terms of a set of rules for hardware that have to be made visible to software. The central hypothesis of this work is that programmers prefer to reason about sequentially consistent memory, rather than having to think about weaker memory, or even write buffers. Following this hypothesis, we re-define weak ordering as a contract between software and hardware. By this contract, software agrees to some formally specified constraints, and hardware agrees to appear sequentially consistent to at least the software that obeys those constraints. We illustrate the power of the new definition with a set of software constraints that forbid data races and an imple-mentation for cache-coherent systems chat is not allowed by the old definition. 
225|The Duality of Memory and Communication in the Implementation of a Multiprocessor Operating System|Mach is a multiprocessor operating system being implemented at Carnegie-Mellon University. An important component of the Mach design is the use of memory objects which can be managed either by the kernel or by user programs through a message interface. This feature allows applications such as transaction management systems to participate in decisions regarding secondary storage management and page replacement. This paper explores the goals, design and implementation of Mach and its external memory management facility. The relationship between memory and communication in Mach is examined as it relates to overall performance, applicability of Mach to new multiprocessor architectures, and the structure of application programs. This research was sponsored by the Defense Advanced Research Projects Agency (DOD), ARPA Order No. 4864, monitored by the Space and Naval Warfare Systems Command under contract N00039-85-C-1034. The views expressed are those of the authors alone. Permission to copy...
226|An Implementation of Distributed Shared Memory|Shared memory is a simple yet powerful paradigm for structuring systems. Recently, there has been an interest in extending this paradigm to non-shared memory architectures as well. For example, the virtual address spaces for all objects in a distributed object-based system could be viewed as constituting a global distributed shared memory. We propose a set of primitives for managing distributed shared memory. We present an implementation of these primitives in the context of an object-based operating system as well as on top of Unix.

The purpose of this paper is to present a set of mechanisms for DSM and an implementation of these mechanisms. All the resources of the system are viewed as potentially shared objects. The name space of these objects constitute a distributed shared memory. The objects are composed of segments, where a segment is a logical entity that has attributes such as read-only, and read-write. There is a concept of ownership and the node where a segment is created (the owner node) is responsible for guaranteeing the consistency of the segment. The distributed shared memory
controller (DSMC) to be described next is the entity that provides the mechanisms for managing these segments.
228|A Distributed Implementation Of The Shared Data-Object Model|The shared data-object model is designed to ease the implementation of parallel applications on loosely coupled distributed systems. Unlike most other models for distributed programming (e.g., RPC), the shared data-object model allows processes on different machines to share data. Such data are encapsulated in data-objects, which are instances of user-defined abstract data types. The shared data-object model forms the basis of a new language for distributed programming, Orca, which gives linguistic support for parallelism and data-objects. A distributed implementation of the shared data-object model should take care of the physical distribution of objects among the local memories of the processors. In particular, an implementation may replicate objects in order to decrease access times to objects and increase parallelism.  The intent of this paper is to show that, for several applications, the proposed model is both easy to use and efficient. We first give a brief description of the sh...
229|Rijke. Term selection and query operations for video retrieval|Abstract. We investigate the influence of term selection and query operations on the text retrieval component of video search. Our main finding is that the greatest gain is to be found in the combination of character n-grams, stemmed text, and proximity terms. 1
230|Adding semantics to detectors for video retrieval|Abstract — In this paper, we propose an automatic video retrieval method based on high-level concept detectors. Research in video analysis has reached the point where over 100 concept detectors can be learned in a generic fashion, albeit with mixed performance. Such a set of detectors is very small still compared to ontologies aiming to capture the full vocabulary a user has. We aim to throw a bridge between the two fields by building a multimedia thesaurus, i.e. a set of machine learned concept detectors that is enriched with semantic descriptions and semantic structure obtained from WordNet. Given a multimodal user query, we identify three strategies to select a relevant detector from this thesaurus, namely: text matching, ontology querying, and semantic visual querying. We evaluate the methods against the automatic search task of the TRECVID 2005 video retrieval benchmark, using a news video archive of 85 hours in combination with a thesaurus of 363 machine learned concept detectors. We assess the influence of thesaurus size on video search performance, evaluate and compare the multimodal selection strategies for concept detectors, and finally discuss their combined potential using oracle fusion. The set of queries in the TRECVID 2005 corpus is too small to be definite in our conclusions, but the results suggest promising new lines of research. Index Terms — Video retrieval, concept learning, knowledge modeling, content analysis and indexing, multimedia information systems I.
231|Boosting web retrieval through query operations| We explore the use of phrase and proximity terms in the context of web retrieval, which is different from traditional ad-hoc retrieval both in document structure and in query characteristics. We show that for this type of task, the usage of both phrase and proximity terms is highly beneficial for early precision as well as for overall retrieval effectiveness. We also analyze why phrase and proximity terms are far more effective for web retrieval than for ad-hoc retrieval.
232|The effectiveness of combining information retrieval strategies for European languages|Building an effective Information Retrieval system requires various design choices, ranging from the weighting scheme to the type of morphological normalization. The combination of runs has become a standard technique to reap the benefits of different run types. Until now, systematic studies of the effectiveness of combination strategies have only been carried out for English. This paper provides an exploratory overview of the effectiveness of combination methods in nine European languages. We demonstrate that the combination of effective information retrieval strategies can lead to significant improvements of retrieval effectiveness. Furthermore, we analyze the relative impact of retrieving more relevant documents and of improved ranking of relevant documents. The experimental evidence is obtained using the 2003 testsuite of the cross-language evaluation forum (CLEF).  
233|Completeness of Query Operations on Resource Spaces* |A great variety of languages can be designed by different people for different purposes to operate resource spaces. Two fundamental issues are: can we design more operations in addition to existing operations? and, how many operations are sufficient or necessary? This paper solves these problems by investigating the theoretical basis for determining how complete a selection capability is provided in a resource operation sublanguage independent of any host language. The result is very useful to the design and analysis of operating languages.
234|Relational completeness of data base sublanguages|ABSTRACT: In the near future, we can expect a great variety of languages to be proposed for interrogating and updating data bases. This paper attempts to provide a theoretical basis which may be used to determine how complete a selection capability is provided in a proposed data sublanguage independently of any host language in which the sublanguage may be embedded. A relational algebra and a relational calculus are defined. Then, an algorithm is presented for reducing an arbitrary relation-defining expression (based on the calculus) into a semantically equivalent expression of the relational algebra. Finally, some opinions are stated regarding the relative merits of calculus-oriented versus algebra-oriented data sublanguages from the standpoint of optimal search and highly discriminating authorization schemes.
235|Contiki - a Lightweight and Flexible Operating System for Tiny Networked Sensors|of tiny networked devices that communicate untethered. For large scale networks it is important to be able to dynamically download code into the network. In this paper we present Contiki, a lightweight operating system with support for dynamic loading and replacement of individual programs and services. Contiki is built around an event-driven kernel but provides optional preemptive multithreading that can be applied to individual processes. We show that dynamic loading and unloading is feasible in a resource constrained environment, while keeping the base system lightweight and compact.
236|Maté: A Tiny Virtual Machine for Sensor Networks|Composed of tens of thousands of tiny devices with very limited resources (&#034;motes&#034;), sensor networks are subject to novel systems problems and constraints. The large number of motes in a sensor network means that there will often be some failing nodes; networks must be easy to repopu-late. Often there is no feasible method to recharge motes, so energy is a precious resource. Once deployed, a network must be reprogrammable although physically unreachable, and this reprogramming can be a significant energy cost. We present Maté, a tiny communication-centric virtual machine designed for sensor networks. Mat~&#039;s high-level in-terface allows complex programs to be very short (under 100 bytes), reducing the energy cost of transmitting new programs. Code is broken up into small capsules of 24 instructions, which can self-replicate through the network. Packet sending and reception capsules enable the deploy-ment of ad-hoc routing and data aggregation algorithms. Maté&#039;s concise, high-level program representation simplifies programming and allows large networks to be frequently re-programmed in an energy-efficient manner; in addition, its safe execution environment suggests a use of virtual ma-chines to provide the user/kernel boundary on motes that have no hardware protection mechanisms. 
237|Trickle: A Self-Regulating Algorithm for Code Propagation and Maintenance in Wireless Sensor Networks|We present Trickle, an algorithm for propagating and maintaining code updates in wireless sensor networks. Borrowing techniques from the epidemic/gossip, scalable multicast, and wireless broadcast literature, Trickle uses a &#034;polite gossip&#034; policy, where motes periodically broadcast a code summary to local neighbors but stay quiet if they have recently heard a summary identical to theirs. When a mote hears an older summary than its own, it broadcasts an update. Instead of flooding a network with packets, the algorithm controls the send rate so each mote hears a small trickle of packets, just enough to stay up to date. We show that with this simple mechanism, Trickle can scale to thousand-fold changes in network density, propagate new code in the order of seconds, and impose a maintenance cost on the order of a few sends an hour.
238|The Design and Implementation of an Operating System to Support Distributed Multimedia Applications|Support for multimedia applications by general purpose computing platforms has been the subject of considerable research. Much of this work is based on an evolutionary strategy in which small changes to existing systems are made. The approach adopted here is to start ab initio with no backward compatibility constraints. This leads to a novel structure for an operating system. The structure aims to decouple applications from one another and to provide multiplexing of all resources, not just the CPU, at a low level. The motivation for this structure, a design based on the structure, and its implementation on a number of hardware platforms is described. I. Introduction G ENERAL purpose multimedia computing platforms should endow text, images, audio and video with equal status: interpreting an audio or video stream should not be a privileged task of special functions provided by the operating system, but one of ordinary user programs. Support for such processing on a platform on which ot...
239|Emstar: a software environment for developing and deploying wireless sensor networks|Recent work in wireless embedded networked systems has followed heterogeneous designs, incorporating a mixture of elements from extremely constrained 8- or 16-bit “Motes ” to less resourceconstrained 32-bit embedded “Microservers.” Emstar is a software environment for developing and deploying complex applications on such heterogeneous networks. Emstar is designed to leverage the additional resources of Microservers by trading off some performance for system robustness in sensor network applications. It enables fault isolation, fault tolerance, system visiblity, in-field debugging, and resource sharing across multiple applications. In order to accomplish these objectives, Emstar is designed to run as a multiprocess system and consists of libraries that implement message-passing IPC primitives, services that support networking, sensing, and time synchronization, and tools that support simulation, emulation, and visualization of live systems, both real and simulated. We evaluate this work by discussing the Acoustic ENSBox, a platform for distributed acoustic sensing that we built using Emstar. We show that by leveraging existing Emstar services, we are able to significantly reduce development time This work was made possible with support from The Center for Embedded Networked Sensing (CENS) under the NSF Cooperative Agreement CCR-0120778, and the UC MICRO program (grant
240|The Spring nucleus: A microkernel for objects|The Spring system is a distributed operating system that supports a distributed, object-oriented application framework. Each individual Spring system is based around a microkernel known as the nucleus, which is structured to support fast cross-address-space object invocations. This paper discusses the design rationale for the nucleus&#039;s IPC facilities and how they fit into the overall Spring programming model. We then describe how the internal structure of the nucleus is organized to support fast crossaddress -space calls, including some specific details and performance information on the current implementation.  
241|Design and Implementation of a Framework for Efficient and Programmable Sensor Networks|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
242|The emergence of networking abstractions and techniques in tinyos|The constraints of sensor networks, an emerging area of network research, require new approaches in system design. We study the evolution of abstractions and techniques in TinyOS, a popular sensor network operating system. Examining CVS repositories of several research institutions that use TinyOS, we trace three areas of development: single-hop networking, multi-hop networking, and network services. We note common techniques and draw conclusions on the emerging abstractions as well as the novel constraints that have shaped them. 1.
243|Efficient Code Distribution in Wireless Sensor Networks|The need to reprogramme a wireless sensor network may arise from changing application requirements, bug fixes, or during the application development cycle. Once deployed, it will be impractical at best to reach each individual node. Thus, a scheme is required to wirelessly reprogramme the nodes. We present an energy-efficient code distribution scheme to wirelessly update the code running in a sensor network. Energy is saved by distributing only the changes to the currently running code. The new code image is built using an edit script of commands that are easy to process by the nodes. A small change to the programme code can cause many changes to the binary code because the addresses of functions and data change. A naive approach to building the edit script string would result in a large script. We describe a number of optimisations and present experimental results showing that these significantly reduce the edit script size.
244|MANTIS: System Support For Multimodal Networks Of . . .|Technology trends are extending the flexibility and convenience of today&#039;s sensor networks through the introduction of new capabilities such as multi-frequency radio transceivers, GPS, and innovative system management tools. This paper expands upon these motivations to present an integrated general-purpose MultimodAl system for NeTworks of In-situ wireless Sensors (MANTIS). The MANTIS system promotes multimodal flexibility and ease of use through its support for multimodal sensing including GPS-enabled location and time, multi-frequency communication, multitasking sensor nodes, and a new multi-platform operating system called MANTIS OS (MOS). For the novice, MANTIS provides convenient tools such as a simple cross-platform API, a remote shell for debugging and logging into MOS nodes, fine-grained dynamic reprogramming via the radio, and in-board sensor placement. For the expert, MANTIS supports true networked emulation of MOS sensor nodes as X86 processes, as well as seamless bridging between an X86-based network of MOS virtual sensors and an actual sensor network of active physical MOS nodes, called nymphs.
245|Exterminate All Operating System Abstractions |The defining tragedy of the operating systems community has been the definition of an operating system as software that both multiplexes and abstracts the hardware is based on the assumption that it is possible both to define abstractions that are appropriate for all areas and to implement them to perform efficiently in all situations. We believe that the fallacy of this quixotic goal is self-evident, and that the operating system problems of the last two decades (poor performance, poor reliability, poor adaptability, and inflexibility) can be traced back to it. The solution we propose is simple: complete elimination of operating system abstractions by lowering the operating system interface to the hardware level.  
246|On the Need for System-Level Support for Ad hoc and Sensor Networks|Ad hoc and sensor networks are an important, emerging niche that is poorly supported by existing operating systems. In this paper, we argue that network-wide energy management is a primary concern in ad hoc networks, and that this functionality is best provided by a systems layer. We are currently designing and implementing a distributed, power-aware, adaptive operating system, called MagnetOS, specifically targeting ad hoc and sensor networks. MagnetOS provides a single system image of a unified Java virtual machine across the nodes that comprise an ad hoc network. By automatically and transparently partitioning applications into components and dynamically placing these components on nodes within the ad hoc network, our system reduces energy consumption, avoids hotspots and increases system longevity. We show that a systems approach to automatic object placement in an ad hoc network can increase system longevity by a factor of four to five.
247|A Linux-based Real-Time Operating System|This work describes the design, implementation, and possible applications of Real-Time Linux --- a hard real-time version of the Linux operating system. In this system, a standard time-sharing OS and a real-time executive run on the same computer. Interrupt controller emulation is used to guarantee a low maximum interrupt latency independently of the base system. The use of a one-shot timer makes it possible to achieve a low task release jitter without compromising throughput. Lock-free FIFO buffers are employed for communication between real-time tasks and Linux processes. User-defined schedulers are allowed as are run-time changes in the scheduling policy. The system is in active use for real-time data acquisition, control, and communications. The software is free and can be obtained by FTP at luz.cs.nmt.edu:/pub/rtlinux or via the WWW at http://luz.cs.nmt.edu/~rtlinux. Acknowledgements I would like to express my deep appreciation to my advisor, Victor Yodaiken, for his support, en...
248|Multidimensional Access Methods|Search operations in databases require special support at the physical level. This is true for conventional databases as well as spatial databases, where typical search operations include the point query (find all objects that contain a given search point) and the region query (find all objects that overlap a given search region).
249|On visible surface generation by a priori tree structures|This paper describes a new algorithm for solving the hidden surface (or line) problem, to more rapidly generate realistic images of 3-D scenes composed of polygons, and presents the development of theoretical foundations in the area as well as additional related algorithms. As in many applications the environment to be displayed consists of polygons many of whose relative geometric relations are static, we attempt to capitalize on this by pre processing tile environment,s database so as to decrease the run-time computations required to generate a scene. This preprocessing is based on generating a &amp;quot;ninary space partitioning &amp;quot; tree whose inorder traversal of visibility priority at run-time will produce a lineaL &amp;quot; order, dependent upon the viewing position, on (parts of) the polygons, which can then be used to easily solve the hidden surfac6 problem. In the application where the entire environment is static with only the viewing-position changing, as is common in simulation, the results presented will be safficient to solve completely tlae llidden surface proulem.;_N~a~OUCZZON One of the long-term goals of computer graphics has been, and continues to be, the rapid, possibly real-time generation of £ealistic images of simulated 3-D environments. &amp;quot;Real-time,&amp;quot; in current practice, has come to mean creating an image in 1/30 of a second--fast enough to continually generate images on a video monitor. With this fast image generation, there is no aiscernable delay between specifying parameters zor an image (using knobs, switches, or cockpit controls) and the *This research was partially supported by NSF under Grants MCS79-00168 and MC579-02593, and was zacilitated by the use of
250|Efficient Processing of Spatial Joins Using R-Trees|Abstract: In this paper, we show that spatial joins are very suitable to be processed on a parallel hardware platform. The parallel system is equipped with a so-called shared virtual memory which is well-suited for the design and implementation of parallel spatial join algorithms. We start with an algorithm that consists of three phases: task creation, task assignment and parallel task execu-tion. In order to reduce CPU- and I/O-cost, the three phases are processed in a fashion that pre-serves spatial locality. Dynamic load balancing is achieved by splitting tasks into smaller ones and reassigning some of the smaller tasks to idle processors. In an experimental performance compar-ison, we identify the advantages and disadvantages of several variants of our algorithm. The most efficient one shows an almost optimal speed-up under the assumption that the number of disks is sufficiently large. Topics: spatial database systems, parallel database systems 1
252|Generalized Search Trees for Database Systems|This paper introduces the Generalized Search Tree (GiST), an index structure supporting an extensible set of queries and data types. The GiST allows new data types to be indexed in a manner supporting queries natural to the types; this is in contrast to previous work on tree extensibility which only supported the traditional set of equality and range predicates. In a single data structure, the GiST provides all the basic search tree logic required by a database system, thereby unifying disparate structures such as B+-trees and R-trees in a single piece of code, and opening the application of search trees to general extensibility. To illustrate the exibility of the GiST, we provide simple method implementations that allow it to behave like a B+-tree, an R-tree, and an RD-tree, a new index for data with set-valued attributes. We also present a preliminary performance analysis of RD-trees, which leads to discussion on the nature of tree indices and how they behave for various datasets.  
253|Hilbert R-tree: An Improved R-tree Using Fractals|We propose a new R-tree structure that outperforms all the older ones. The heart of the idea is to facilitate the deferred splitting approach in R-trees. This is done by proposing an ordering on the R-tree nodes. This ordering has to be &#039;good&#039;, in the sense that it should group &#039;similar &#039; data rectangles together, to minimize the area and perimeter of the resulting minimum bounding rectangles (MBRs). Following [19] we have chosen the so-called &#039;2D-c &#039; method, which sorts rectangles according to the Hilbert value of the center of the rectangles. Given the ordering, every node has a well-de ned set of sibling nodes; thus, we can use deferred splitting. By adjusting the split policy, the Hilbert R-tree can achieve as high utilization as desired. To the contrary, the R-tree has no control over the space utilization, typically achieving up to 70%. We designed the manipulation algorithms in detail, and we did a full implementation of the Hilbert R-tree. Our experiments show that the &#039;2-to-3 &#039; split policy provides a compromise between the insertion complexity and the search cost, giving up to 28 % savings over the R  tree [3] on real data. 1
254|Spatial SQL: A Query and Presentation Language|attention has been focused on spatial databases which combine conventional and spatially related data such as Geographic Information Systems, CAD/CAM, or VLSI. A language has been developed to query such spatial databases. It recognizes the significantly different requirements of spatial data handling and overcomes the inherent problems of the application of conventional database query languages. The spatial query language has been designed as a minimal extension to the interrogative part of SQL and distinguishes from previously designed SQL extensions by (1) the preservation of SQL concepts, (2) the highlevel treatment of spatial objects, and (3) the incorporation of spatial operations and relationships. It consists of two components, a query language to describe what information to retrieve and a presentation language to specify how to display query results. Users can ask standard SQL queries to retrieve non-spatial data based on non-spatial constraints, use Spatial SQL commands to inquire about situations involving spatial data, and give instructions in the Graphical Presentation Language GPL to manipulate or examine the graphical presentation. 1 Index Terms—Geographic Information Systems, graphical presentation, query
255|Fractals for Secondary Key Retrieval |In this paper we propose the use of fractals and especially the Hilbert curve, in order to design good distance-preserving mappings. Such mappings improve the performance of secondary-key- and spatial- access methods, where multi-dimensional points have to be stored on an 1-dimensional medium (e.g., disk). Good clustering reduces the number of disk accesses on retrieval, improving the response time. Our experiments on range queries and nearest neighbor queries showed that the proposed Hilbert curve achieves better clustering than older methods (&amp;quot;bit-shuffling&amp;quot;, or Peano curve), for every situation we tried.
256|Beyond uniformity and independence: Analysis of r-trees using the concept of fractal dimension|We propose the concept of fractal dimension of a set of points, in order to quantify the deviation from the uniformity distribution. Using measurements on real data sets (road intersections of U.S. counties, star coordinates from NASA’s Infrared-Ultraviolet Explorer etc.) we provide evidence that real data indeed are skewed, and, moreover, we show that they behave as mathematical fractals, with a measurable, non-integer fract al dimension. Armed with this tool, we then show its practical use in predicting the performance of spatial access methods, and specifically of the R-trees. We provide the jirst analysis of R-trees for skewed distributions of points: We develop a formula that estimates the number of disk accesses for range queries, given only the fractal dimension of the point set, and its count. Experiments on real data sets show that the formula is very accurate: the relative error is usually below 5%, and it rarely exceeds 10%. We believe that the fractal dimension will help replace the uniformity and independence assumptions, allowing more accurate analysis for any spatial access method, as well as better estimates for query optimization on multi-attribute queries. 1
257|Multi-Step Processing of Spatial Joins   |Spatial joins are one of the most importaot operations for combining spatial objects of several relations. IO this paper, spatial join processing is studied in detail for extended spatial objects in two-dimensional data space. We present an approach for spatial join processing that is based on three steps. First, a spatial join is performed on the minimum bounding rectangles of the objects returning a set of candidates. Various approaches for accelerating this step of join processing have been examined at the last year’s conference [BKS 93a]. In this paper, we focus on the problem how to compute the answers from the set of candidates which is handled by the foliowing two steps. First of all, sophisticated approximations are used to identify answers as well as to filter out false hits from the set of candidates. For this purpose, we investigate various types of conservative and progressive approximations. In the last step, the exact geometry of the remaioing candidates has to be tested against the join predicate. The time required for computing spatial joio predicates can essentially be reduced when objects are adequately organized in main memory. IO our approach, objects are fiist decomposed into simple components which are exclusively organized by a main-memory resident spatial data structure. Overall, we present a complete approach of spatial join processing on complex spatial objects. The performance of the individual steps of our approach is evaluated with data sets from real cartographic applications. The results show that our approach reduces the total execution time of the spatial join by factors.  
258|Estimating the Selectivity of Spatial Queries Using the `Correlation&#039; Fractal Dimension|We examine the estimation of selectivities for range and spatial join queries in real spatial databases. As we have shown earlier [FK94a], real point sets: (a) violate consistently the &#034;uniformity&#034; and &#034;independence&#034; assumptions, (b) can often be described as &#034;fractals&#034;, with non-integer (fractal) dimension. In this paper we show that, among the infinite family of fractal dimensions, the so called &#034;Correlation Dimension&#034; D 2 is the one that we need to predict the selectivity of spatial join. The main contribution is that, for all the real and synthetic point-sets we tried, the average number of neighbors for a given point of the point-set follows a power law, with D 2 as the exponent. This immediately solves the selectivity estimation for spatial joins, as well as for &#034;biased&#034; range queries (i.e., queries whose centers prefer areas of high point density). We present the formulas to estimate the selectivity for the biased queries, including an integration constant (K `shape  0  ) for ea...
259|Indexing for data models with constraints and classes|We examine I O-efficient data structures that provide indexing support for new data models. The database languages of these models include concepts from constraint programming (e.g., relational tuples are generated to conjunctions of constraints) and from object-oriented programming (e.g., objects are organized in class hierarchies). Let n be the size of the database, c the number of classes, B the page size on secondary storage, and t the size of the output of a query: (1) Indexing by one attribute in many constraint data models is equivalent to external dynamic interval management, which is a special case of external dynamic two-dimensional range searching. We present a semi-dynamic data structure for this problem that has worst-case space O(n B) pages, query I O time O(logB n+t B) and O(logB n+(logB n) 2 B) amortized insert I O time. Note that, for the static version of this problem, this is the first worst-case optimal solution. (2) Indexing by one attribute and by class name in an object-oriented model, where objects are organized
260|The lsd tree: spatial access to multidimensional point and non-point objects|We propose the Local Split Decision tree (LSD tree, for short), a data structure supporting efficient spatial access to geometric objects. Its main advantages over other structures are that it performs well for all reasonable data distributions, cover quotients (which measure the overlapping of the data objects), and bucket capacities, and that it maintains multidimensional points as well as arbitrary geometric objects. These properties demonstrated by an extensive performance, evaluation make the LSD tree extremely suitable for the implementation of spatial access paths in geometric databases. The paging algorithm for the binary tree directory is interesting in its own right because a practical solution for the problem of how to page a (multidimensional) binary tree without access path degeneration is presented. 1.
261|Efficient Computation of Spatial Joins|Spatial joins are join operations that involve spatial data types and operators. Due to some basic properties of spatial data, many conventional join processing strategies suffer serious performance penalties or are not applicable at all in this case. In this paper we explore which of the join strategies known from conventional databases can be applied to spatial joins as well, and how some of these techniques can be modified to be more efficient in the context of spatial data. Furthermore, we describe a class of tree structures, called generalization trees, that can be applied efficiently to compute spatial joins in a hierarchical manner. Finally, we model the performance of the most promising strategies analytically and conduct a comparative study.  Parts of this work have been carried out while the author was visiting the International Computer Science Institute and the University of California at Berkeley.  1  1 Introduction  Spatial databases have become a very active research top...
262|Parallel R-trees|We consider the problem of exploiting parallelism to accelerate the performance of spatial access methods and specifically, R-trees [11]. Our goal is to design a server for spatial data, so that to maximize the throughput of range queries. This can be achieved by (a) maximizing parallelism for large range queries, and (b) by engaging as few disks as possible on point queries [22]. We propose a simple hardware architecture consisting of one processor with several disks attached to it. On this architecture, we propose to distribute the nodes of a traditional R-tree, with cross-disk pointers (`Multiplexed&#039; R-tree). The R-tree code is identical to the one for a single-disk R-tree, with the only addition that we have to decide which disk a newly created R-tree node should be stored in. We propose and examine several criteria to choose a disk for a new node. The most successful one, termed `proximity index&#039; or PI, estimates the similarity of the new node with the other R-tree nodes already o...
263|Comparison of approximations of complex objects used for approximation-based query processing in spatial database systems|The management of geometric objects is a prime example of an application where efficiency is the bottleneck; this bottleneck cannot be eliminated without using suitable access structures. The most popular approach for handling complex spatial objects in spatial access methods is to use their minimum bounding boxes as a geometric key. Obviously, the rough approximation by bounding boxes provides a fast but inaccurate filter for the set of answers to a query. In order to speed up the query processing by a better approximation quality, we investigate six different types of approximations. Depending on the complexity of the objects and the type of queries, the approximations 5-corner, ellipse and rotated bounding box clearly outperform the bounding box. An important ingredient of our approach is to organize these approximations in efficient spatial access
264|A Qualitative Comparison Study of Data Structures for Large Line Segment Databases|A qualitative comparative study is performed of the performance of three popular spatial indexing methods -- the r    -tree, r  +  -tree, and the pmr quadtree -- in the context of processing spatial queries in large line segment databases. The data is drawn from the tiger/Line files used by the Bureau of the Census to deal with the road networks in the US. The goal is not to find the best data structure as this is not generally possible. Instead, their comparability is demonstrated and an indication is given as to when and why their performance differs. Tests are conducted with a number of large datasets and performance is tabulated in terms of the complexity of the disk activity in building them, their storage requirements, and the complexity of the disk activity for a number of tasks that include point and window queries, as well as finding the nearest line segment to a given point and an enclosing polygon.  1 Introduction  Spatial data consists of points, lines, regions, rectangles,...
265|DOT: A spatial access method using fractals|Existing Database Management Systems (DBMSs) do not handle efficiently multi-dimensional data such as boxes, polygons, or even points in a multi-dimensional space. We examine access methods for these data with two design goals in mind: (a) efficiency in terms of search speed and space overhead and (b) ability to be integrated in a DBMS easily. We propose a method to map multidimensional objects into points in a 1-dimensional space; thus, traditional primary-key access methods can be applied, with very few extensions on the part of the DBMS. We propose such mappings based on fractals; we implemented the whole method on top of a B +-tree, along with several mappings. Simulation experiments on several distributions of the input data show
266|Benchmarking Spatial Join Operations with Spatial Output|The spatial join operation is benchmarked using variants of well-known spatial data structures such as the R-tree, R    -tree, R  +  -tree, and the PMR quadtree. The focus is on a spatial join with spatial output because the result of the spatial join frequently serves as input to subsequent spatial operations (i.e., a cascaded spatial join as would be common in a spatial spreadsheet). Thus, in addition to the time required to perform the spatial join itself (whose output is not always required to be spatial), the time to build the spatial data structure also plays an important role in the benchmark. The studied quantities are the time to build the data structure and the time to do the spatial join in an application domain consisting of planar line segment data. Experiments reveal that spatial data structures based on a disjoint decomposition of space and bounding boxes (i.e., the R  +  -tree and the PMR quadtree with bounding boxes) outperform the other structures that are based upon ...
267|Optimal Redundancy in Spatial Database Systems|In spatial database systems rectangles are commonly used to approximate real spatial data. A technique that approximates extended objects with a collection  of rectangles is the z-ordering method. Since each of these rectangles eventually corresponds to an entry in a spatial index, the object may be referenced several times. This redundancy effect is controllable. In this paper, we present an empirically derived formula to assess the expected redundancy for the z-ordering approximation technique given some simple parameters. After showing the applicability of this formula to a large class of different object geometries, we make use of this result to determine the optimal redundancy for real spatial data by means of theoretical considerations. In order to verify our theoretical results, we conducted several experiments using real spatial data and found a good correspondence. 1 Introduction  Spatial indexing or spatial access methods have found great attention in recent years, culminatin...
268|An analysis of geometric modeling in database systems|The data-modeling and computational requirements for integrated computer aided manufacturing (CAM) databases are analyzed, and the most common representation schemes for modeling solid geometric objects in a computer are
269|Realms: A Foundation for Spatial Data Types in Database Systems|Spatial data types or algebras for database systems should (i) be fully general (which means, closed under set operations, hence e.g. a region value can be a set of polygons with holes), (ii) have formally defined semantics, (iii) be defined in terms of finite representations available in computers, (iv) offer facilities to enforce geometric consistency of related spatial objects, and (v) be independent of a particular DBMS data model, but cooperate with any. We offer such a definition in two papers. The central idea, introduced in this (first) paper, is to use realms as geometric domains underlying spatial data types. A realm as a general database concept is a finite, dynamic, user-defined structure underlying one or more system data types. A geometric realm defined here is a planar graph over a finite resolution grid. Problems of numerical robustness and topological correctness are solved below and within the realm layer so that spatial algebras defined above a realm enjoy very nice algebraic properties. Realms also interact with a DBMS to enforce geometric consistency on object creation or update.
270|Benchmarking Spatial Joins A La Carte|Spatial joins are join operations that involve spatial data types and operators. Spatial access methods are often used to speed up the computation of spatial joins. This paper addresses the issue of benchmarking spatial join operations. For this purpose, we first present a WWW-based benchmark generator to produce sets of rectangles. Using a Web browser, experimenters can specify the number of rectangles in a sample, as well as the statistical distributions of their sizes, shapes, and locations. Second, using the generator and a well-defined set of statistical models we define several tests to compare the performance of three spatial join algorithms: nested loop, scan-and-index, and synchronized tree traversal. We also added a real-life data set from the Sequoia 2000 storage benchmark. Our results show that the relative performance of the different techniques mainly depends on two parameters: sample size, and selectivity of the join predicate. All of the statistical models and algorithms are available on the Web, which allows for easy verification and modification of our experiments.
271|Analysis of n-dimensional Quadtrees Using the Hausdorff Fractal Dimension|There is mounting evidence [Man77, Sch91] that real datasets are statistically self-similar, and thus, `fractal&#039;. This is an important insight since it permits a compact statistical description of spatial datasets; subsequently, as we show, it also forms the basis for the theoretical analysis of spatial access methods, without using the typical, but unrealistic, uniformity assumption. In this paper, we focus on the estimation of the number of quadtree blocks that a real, spatial dataset will require. Using the the well-known Hausdorff fractal dimension, we derive some closed formulas which allow us to predict the number of quadtree blocks, given some few parameters. Using our formulas, it is possible to predict the space overhead and the response time of linear quadtrees/z-ordering [OM88], which are widely used in practice. In order to verify our analytical model, we performed an extensive experimental investigation using several real datasets coming from different domains. In these ex...
272|Enclosing Many Boxes By an Optimal Pair of Boxes|We look at the problem: Given a set  M  of  n d-dimensional  intervals, find two  d-  dimensional intervals  S, T  , such that all intervals in  M  are enclosed by  S  or by  T  , the  distribution is balanced and the intervals  S  and  T  fulfill a geometric criterion, e.g. like minimum  area sum. Up to now no polynomial time algorithm was known for that problem. We  present an  O(dn  log  n  +  d 2 n 2d\Gamma1 ) algorithm for finding an optimal solution.  1 Introduction  Throughout the years, several fast heuristics have been proposed for a combinatorial optimization  problem that is important in the area of spatial data structures. Given a set of (axis-parallel)  rectangles in the plane, the problem is to find two rectangles, say S and T , such that each given  rectangle is enclosed by S or by T (or both), each of S and T enclose at least a certain number  of given rectangles, and S and T together minimize some measure, e.g. the sum of their areas.  This problem must be solved whene...
273|Separability of Polyhedra for Optimal Filtering of Spatial and Constraint Data|The filtering method considered in this paper is based on approximation of a  spatial object in d-dimensional space by the minimal convex polyhedron that encloses  the object and whose facets are normal to preselected axes. These axes are  not necessarily the standard coordinate axes and, furthermore, their number is not  determined by the dimension of the space. We optimize filtering by selecting optimal  such axes based on a pre-processing analysis of stored objects or a sample thereof.  The number of axes selected represents a trade-off between access time and storage  overhead, as more axes usually lead to better filtering but require more overhead to  store the associated access structures. We address the problem of minimizing the  number of axes required to achieve a predefined quality of filtering and the reverse  problem of optimizing the quality of filtering when the number of axes is fixed. In  both cases we also show how to find an optimal collection of axes. In order to sol...
274|Spatial Access Methods and Query Processing in the Object-Oriented GIS GODOT|In this paper, we describe the spatial access method z-ordering and its application in the context of the research project GODOT, which is based on the commercial object-oriented database system ObjectStore [LLOW91]. After identifying a range of spatial predicates, we show that the intersection join is of crucial importance for spatial joins. Next, we propose an efficient method for query processing, which takes advantage of z-ordering and uses the conventional indexing mechanisms offered in current database systems (e.g., relational and object-oriented).
275|Extending a spatial access structure to support additional standard attributes| In recent years, many access structures have been proposed supporting access to objects via their spatial location. However, additional non-geometric properties are always associated with geometric objects, and in practice it is often necessary to use select conditions based on spatial and standard attributes. An obvious idea to improve the performance of queries with mixed select conditions is to extend spatial access structures with additional dimensions for standard attributes. Whereas this idea seems to be simple and promising at rst glance, a closer look brings up serious problems, especially with select conditions containing arithmetic expressions or select conditions for non-point objects and with Boolean operators like or and not. In this paper we present a solution to overcome the problems sketched above which is based on three pillars: (1) We present powerful basic techniques to deal with arithmetic conditions containing mathematical operations (like `+&#039;, `;&#039;,  ` &#039;, and `=&#039;) and range queries for non-point objects. (2) We introduce a technique which allows to decompose select conditions containing Boolean operators and to reduce the processing of such a select condition to the processing of its elementary parts. (3) We showhow other operations like joins and distance-scans can be integrated into this query processing architecture.  
276|Geometric Information Makes Spatial Query Processing More Efficient|In order to index complex and heterogeneous cartographic data by means of spatial access methods, one typically uses single, simple geometries to approximate the given geometries. Traditionally, no information regarding the quality of the approximation is stored. In this paper, we show that the availability of such information allows us to decide at an early stage that numerous objects fulfill the spatial predicate. This leads to significant performance improvements for the spatial selection as well as for the spatial intersection join. We present two techniques enabling such an early decision for the z-ordering method, and we demonstrate their efficiency by means of experiments.  1 Introduction  Fast access to spatial data stored in databases is essential for answering spatial queries efficiently. As a result, numerous spatial access methods have been proposed and studied in the past. The complexity and heterogeneity of spatial data makes it impossible to store cartographic data direc...
277|On the complexity of BV-tree updates|In [Fre95b] we discussed the use of multi-dimensional index methods in constraint databases. In [Fre95a] we showed how to overcome a fundamental problem which has afflicted all multi-dimensional indexing techniques based on the recursive partitioning of a dataspace: how to build the partition hierarchy so that the location of each object in the
278|Oversize Shelves: A Storage Management Technique for Large Spatial Data Objects|In this paper we present a new technique to improve the performance of spatial access methods by minimizing redundancy: the oversize shelf . Oversize shelves are additional disk pages that are attached to the interior nodes of a tree--based spatial access method (such as the R  +  --tree or the cell tree). These pages are used to accommodate very large data objects in order to avoid their excessive fragmentation. Whenever inserting a new object into the tree, one now has to decide whether to store it on an oversize shelf or insert it into the corresponding subtrees. For this purpose, we developed an analytic model for the behavior of dynamic spatial access methods under insertions and deletions. The model yields a threshold value for the size of an object, such that it is more favorable to put it on the oversize shelf if and only if its size is greater than the threshold value. Otherwise the insertion into the corresponding subtrees is preferable. Practical experiments indicate that th...
279|Adapting the Transformation Technique to Maintain Multi-Dimensional Non-Point Objects in k-d-Tree Based Access Structures|In [10, 18] the transformation technique has been proposed to store k-dimensional intervals -- which serve as bounding boxes for arbitrary geometric objects in many applications -- as 2k-dimensional points in a point access structure. Unfortunately the transformation technique has two pitfalls: (1) The transformation leads to a skew distribution of the 2k-dimensional image points. (2) Processing a range query searching all objects intersecting a given query region, there is a mismatch between thek-dimensional query region and the 2k-dimensional access structure. In this paper we propose two techniques to overcome these problems which can be directly applied tok-d-tree based point access structures: (1) We present a sophisticated split strategy to determine the split dimension and the split position in case of a bucket split which exploits the knowledge about the distribution of the image points of the transformation technique to gain an extremely exible and robust access structure. (2) We propose a re-transformation of the 2k-dimensional data regions in the access structure into the originalk-dimensional data space in order to compare these regions with thek-dimensional query region. Furthermore we state experimental results, which demonstrate, that the presented techniques allow to maintaink-dimensional non-point objects with nearly the same performance as k-dimensional point objects. 
280|Improving retrieval performance by relevance feedback|Relevance feedback is an automatic process, introduced over 20 years ago, designed to produce improved query formulations following an initial retrieval operation. The principal relevance feedback methods described over the years are examined briefly, and evaluation data are included to demonstrate the effectiveness of the various methods. Prescriptions are given for conducting text re-trieval operations iteratively using relevance feedback. Introduction to Relevance Feedback It is well known that the original query formulation process is not transparent to most information system users. In particular, without detailed knowledge of the collection make-up, and of the retrieval environment, most users find
281|Term-weighting approaches in automatic text retrieval|The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective term-weighting systems. This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared.  
282|Interactive Search Strategies and Dynamic File Organization in Information Retrieval |A great deal of effort has been devoted in recent years to the evaluation of automatic or semi-automatic information retrieval systems. Recent evaluation results indicate that the search effectiveness presently achieved, or likely to be achievable in the foreseeable future, is much smaller than expected by a majority of the potential user population. Furthermore, theoretical advances in language analysis and data organization promise only relatively modest future improvements. The most significant advances in retrieval effectiveness are likely to be obtained by adaptive interaction techniques that extract information from the user during the search process to improve the organization of the data space, thereby providing more effective search and retrieval operations. The various user feedback techniques described either modify the user queries in such a way as to bring these queries close:- to existing groups of relevant
283|Parallel Text Search Methods|A comparison of recently proposed parallel text search methods to alternative available search strategies that use serial processing machines suggests parallel methods do not provide large-scale gains in either retrieval effectiveness or efficiency.
284|Models and issues in data stream systems|In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues. 
285|Randomized Algorithms|Randomized algorithms, once viewed as a tool in computational number theory, have by now found widespread application. Growth has been fueled by the two major benefits of randomization: simplicity and speed. For many applications a randomized algorithm is the fastest algorithm available, or the simplest, or both. A randomized algorithm is an algorithm that uses random numbers to influence the choices it makes in the course of its computation. Thus its behavior (typically quantified as running time or quality of output) varies from
286|The space complexity of approximating the frequency moments|The frequency moments of a sequence containing mi elements of type i, for 1 = i = n, are the numbers Fk = ?n i=1 mki. We consider the space complexity of randomized algorithms that approximate the numbers Fk, when the elements of the sequence are given one by one and cannot be stored. Surprisingly, it turns out that the numbers F0, F1 and F2 can be approximated in logarithmic space, whereas the approximation of Fk for k = 6 requires n?(1) space. Applications to data bases are mentioned as well. 
287|Multiparty Communication Complexity|A given Boolean function has its input distributed among many parties. The aim is to determine which parties to tMk to and what information to exchange with each of them in order to evaluate the function while minimizing the total communication. This paper shows that it is possible to obtain the Boolean answer deterministically with only a polynomial increase in communication with respect to the information lower bound given by the nondeterministic communication complexity of the function.
288|Fast subsequence matching in time-series databases|We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an ecient and eective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.  
289|Mining high-speed data streams|Categories and Subject ?????????? ? ?¨?????????????????????????¦???¦??????????¡¤?? ¡  ? ¡????????????????¦¡¤????§?£????
290|Efficient Filtering of XML Documents for Selective Dissemination of Information|Information Dissemination applications are gaining increasing  popularity due to dramatic improvements in  communications bandwidth and ubiquity. The sheer  volume of data available necessitates the use of selective  approaches to dissemination in order to avoid overwhelming  users with unnecessaryinformation. Existing  mechanisms for selective dissemination typically rely  on simple keyword matching or &#034;bag of words&#034; information  retrieval techniques. The advent of XML as a  standard for information exchangeand the development  of query languages for XML data enables the development  of more sophisticated filtering mechanisms that  take structure information into account. We have developed  several index organizations and search algorithms  for performing efficient filtering of XML documents for  large-scale information dissemination systems. In this  paper we describe these techniques and examine their  performance across a range of document, workload, and  scale scenarios.  1 
291|External Memory Algorithms and Data Structures|Data sets in large applications are often too massive to fit completely inside the computer&#039;s internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this paper, we survey the state of the art in the design and analysis of external memory algorithms and data structures (which are sometimes referred to as &#034;EM&#034; or &#034;I/O&#034; or &#034;out-of-core&#034; algorithms and data structures). EM algorithms and data structures are often designed and analyzed using the parallel disk model (PDM). The three machine-independent measures of performance in PDM are the number of I/O operations, the CPU time, and the amount of disk space. PDM allows for multiple disks (or disk arrays) and parallel CPUs, and it can be generalized to handle tertiary storage and hierarchical memory. We discuss several important paradigms for how to solve batched and online problems efficiently in external memory. Programming tools and environments are available for simplifying the programming task. The TPIE system (Transparent Parallel I/O programming Environment) is both easy to use and efficient in terms of execution speed. We report on some experiments using TPIE in the domain of spatial databases. The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.  
292|Random sampling with a reservoir|We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O(n(1 + log(N/n))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.
293|Stable Distributions, Pseudorandom Generators, Embeddings and Data Stream Computation|In this paper we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular: ffl we show how to maintain (using only O(log n=ffl  2  ) words of storage) a sketch C(p) of a point p 2 l  n  1 under dynamic updates of its coordinates, such that given sketches C(p) and C(q) one can estimate jp \Gamma qj 1 up to a factor of (1 + ffl) with large probability. This solves the main open problem of [10].  ffl we obtain another sketch function C  0  which maps l  n  1 into a normed space l  m  1 (as opposed to C), such that m = m(n) is much smaller than n; to our knowledge this is the first dimensionality reduction lemma for l 1 norm  ffl we give an explicit embedding of l  n  2 into l  n  O(log n) 1  with distortion (1 + 1=n  \Theta(1)  ) and a nonconstructive embedding of l  n  2 into l  O(n)  1 with distortion  (1 + ffl) such that the embedding can be represented using only O(n log  2  n) bits (as opposed to at least...
294|Mining time-changing data streams|Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a station-ary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying pro-cesses generating them changed during this time, sometimes radically. Although a number of algorithms have been pro-posed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes ques-tionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.
295|Continuous Queries over Data Streams|In many recent applications, data may take the form of continuous data streams, rather than finite stored data sets. Several aspects of data management need to be re-considered in the presence of data streams, offering a new research direction for the database community. In this pa-per we focus primarily on the problem of query processing, specifically on how to define and evaluate continuous queries over data streams. We address semantic issues as well as efficiency concerns. Our main contributions are threefold. First, we specify a general and flexible architecture for query processing in the presence of data streams. Second, we use our basic architecture as a tool to clarify alternative semantics and processing techniques for continuous queries. The architecture also captures most previous work on continuous queries and data streams, as
297|Maintaining Stream Statistics over Sliding Windows (Extended Abstract)  (2002) |Mayur Datar  Aristides Gionis  y  Piotr Indyk  z  Rajeev Motwani  x  Abstract  We consider the problem of maintaining aggregates and statistics over data streams, with respect to the last N data elements seen so far. We refer to this model as the sliding window model. We consider the following basic problem: Given a stream of bits, maintain a count of the number of 1&#039;s in the last N elements seen from the stream. We show that using O(  1  ffl log  2  N) bits of memory, we can estimate the number of 1&#039;s to within a factor of 1 + ffl. We also give a matching lower bound of \Omega\Gamma  1  ffl log  2  N) memory bits for any deterministic or randomized algorithms. We extend our scheme to maintain the sum of the last N positive integers. We provide matching upper and lower bounds for this more general problem as well. We apply our techniques to obtain efficient algorithms for the Lp norms (for p 2 [1; 2]) of vectors under the sliding window model. Using the algorithm for the basic counting problem, one can adapt many other techniques to work for the sliding window model, with a multiplicative overhead of O(  1  ffl log N) in memory and a 1 + ffl factor loss in accuracy. These include maintaining approximate histograms, hash tables, and statistics or aggregates such as sum and averages.
298|Trajectory Sampling for Direct Traffic Observation|Traffic measurement is a critical component for the control and engineering of communication networks. We argue that traffic measurement should make it possible to obtain the spatial flow of traffic through the domain, i.e., the paths followed by packets between any ingress and egress point of the domain. Most resource allocation and capacity planning tasks can benefit from such information. Also, traffic measurements should be obtained without a routing model and without knowledge of network state. This allows the traffic measurement process to be resilient to network failures and state uncertainty.  We propose a method that allows the direct inference of traffic flows through a domain by observing the trajectories of a subset of all packets traversing the network. The key advantages of the method are that (i) it does not rely on routing state, (ii) its implementation cost is small, and (iii) the measurement reporting traffic is modest and can be controlled precisely. The key idea of the method is to sample packets based on a hash function computed over the packet content. Using the same hash function will yield the same sample set of packets in the entire domain, and enables us to reconstruct packet trajectories.  I. 
299|  Wavelet-Based Histograms for Selectivity Estimation |Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations. In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data values give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using
300|Surfing wavelets on streams: One-pass summaries for approximate aggregate queries|Abstract We present techniques for computing small spacerepresentations of massive data streams. These are inspired by traditional wavelet-based approx-imations that consist of specific linear projections of the underlying data. We present general&amp;quot;sketch &amp;quot; based methods for capturing various linear projections of the data and use them to pro-vide pointwise and rangesum estimation of data streams. These methods use small amounts ofspace and per-item time while streaming through the data, and provide accurate representation asour experiments with real data streams show.
301|Space-Efficient Online Computation of Quantile Summaries|An &amp;epsilon;-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of &amp;epsilon;N . We present a new online...
302| Approximate Computation of Multidimensional Aggregates of Sparse Data Using Wavelets |Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries. In this paper, we present anovel method that provides approximate answers to high-dimensional OLAP aggregation queries in massive sparse data sets in a time-efficient and space-efficient manner. We construct a compact data cube, which is an approximate and space-efficient representation of the underlying multidimensional array, based upon a multiresolution wavelet decomposition. In the on-line phase, each aggregation query can generally be answered using the compact data cube in one I/O or a small number of I/Os, depending upon the desired accuracy. We present two I/O-efficient algorithms to construct the compact data cube for the important case of sparse high-dimensional arrays, which often arise in practice. The traditional histogram methods are infeasible for the massive high-dimensional data sets in OLAP applications. Previously developed wavelet techniques are efficient only for dense data. Our on-line query processing algorithm is very fast and capable of refining answers as the user demands more accuracy. Experiments on real data show that our method provides significantly more accurate results for typical OLAP aggregation queries than other efficient approximation techniques such as random sampling.
303|Computing on Data Streams|In this paper we study the space requirement of algorithms that make  only one (or a small number of) pass(es) over the input data. We study such  algorithms under a model of data streams that we introduce here. We give  a number of upper and lower bounds for problems stemming from queryprocessing,  invoking in the process tools from the area of communication  complexity.
304|Processing Complex Aggregate Queries over Data Streams|Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (i.e., data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed.
305|Continual Queries for Internet Scale Event-Driven Information Delivery|In this paper we introduce the concept of continual queries, describe the design of a distributed  event-driven continual query system -- OpenCQ, and outline the initial implementation of OpenCQ  on top of the distributed interoperable information mediation system DIOM [21, 19]. Continual  queries are standing queries that monitor update of interest and return results whenever the update  reaches specified thresholds. In OpenCQ, users may specify to the system the information they would  like to monitor (such as the events or the update thresholds they are interested in). Whenever the  information of interest becomes available, the system immediately delivers it to the relevant users;  otherwise, the system continually monitors the arrival of the desired information and pushes it to the  relevant users as it meets the specified update thresholds. In contrast to conventional pull-based data  management systems such as DBMSs and Web search engines, OpenCQ exhibits two important featu...
306|Join synopses for approximate query answering|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses (join samples) as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known. We also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations. One of our key contributions is a detailed analysis of the error bounds obtained for approximate answers that demonstrates the trade-offs in various methods, as well as the advantages in certain scenarios of a new subsampling method we propose. Our extensive set of experiments on the TPC-D benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper. 1
307|An efficient cost-driven index selection tool for Microsoft SQL Server|In this paper we describe novel techniques that make it possible to build an industrial-strength tool for automating the choice of indexes in the physical design of a SQL database. The tool takes as input a workload of SQL queries, and suggests a set of suitable indexes. We ensure that the indexes chosen are effective in reducing the cost of the workload by keeping the index selection tool and the query optimizer &amp;quot;in step&amp;quot;. The number of index sets that must be evaluated to find the optimal configuration is very large. We reduce the complexity of this problem using three techniques. First, we remove a large number of spurious indexes from consideration by taking into account both query syntax and cost information. Second, we introduce optimizations that make it possible to cheaply evaluate the “goodness ” of an index set. Third, we describe an iterative approach to handle the complexity arising from multicolumn indexes. The tool has been implemented on Microsoft SQL Server 7.0. We performed extensive experiments over a range of workloads, including TPC-D. The results indicate that the tool is efficient and its choices are close to optimal. 1.
308|Reductions in Streaming Algorithms, with an Application to Counting Triangles in Graphs |We introduce reductions in the streaming model as a tool in the design of streaming algorithms. We develop
309|Computing iceberg queries efficiently|Many applications compute aggregate functions...
310|Data-Streams and Histograms|Histograms have been used widely to capture data distribution, to represent the data by a small number of step functions. Dynamic programming algorithms which provide optimal construction of these histograms exist, albeit running in quadratic time and linear space. In this paper we provide linear time construction of 1 + epsilon approximation of optimal histograms, running in polylogarithmic space. Our results extend to the context of data-streams, and in fact generalize to give 1 + epsilon approximation of several problems in data-streams which require partitioning the index set into intervals. The only assumptions required are that the cost of an interval is monotonic under inclusion (larger interval has larger cost) and that the cost can be computed or approximated in small space. This exhibits a nice class of problems for which we can have near optimal data-stream algorithms.
311|Rate-Based Query Optimization for Streaming Information Sources|Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences. We present a preliminary validation of our rate-based optimization framework on a prototype XML query engine, though it is generic enough to be used in other database contexts. The results show that rate-based optimization is feasible and can indeed yield correct decisions.
312|Making Views Self-Maintainable for Data Warehousing|A data warehouse stores materialized views over data from one or more sources in order to provide fast access to the integrated data, regardless of the availability of the data sources. Warehouse views need to be maintained inresponse to changes to the base data in the sources. Except for very simple views, maintaining a warehouse view requires access to data that is not available in the view itself. Hence, to maintain the view, one either has to query the data sources or store auxiliary data in the warehouse. We show that by using key and referential integrity constraints, we often can maintain a select-project-join view without going to the data sources or replicating the base relations in their entirety in the warehouse. We derive a set of auxiliary views such that the warehouse view and the auxiliary views together are self-maintainable|they can be maintained without going to the data sources or replicating all base data. In addition, our technique can be applied to simplify traditional materialized view maintenance by exploiting key and referential integrity constraints. 1
313|Approximate Medians and other Quantiles in One Pass and with Limited Memory|We present new algorithms for computing approximate quantiles of large datasets in a single pass. The approximation guarantees are explicit, and apply without regard to the value distribution or the arrival distributions of the dataset. The main memory requirements are smaller than those reported earlier by an order of magnitude.  We also discuss methods that couple the approximation algorithms with random sampling to further reduce memory requirements. With sampling, the approximation guarantees are explicit but probabilistic, i.e., they apply with respect to a (user controlled) confidence parameter.  We present the algorithms, their theoretical analysis and simulation results.  1 Introduction  This article studies the problem of computing order statistics  of large sequences of online or disk-resident data using as little main memory as possible. We focus on computing  quantiles, which are elements at specific positions in the sorted order of the input. The OE-quantile, for OE 2 [0; ...
314|Random Sampling for Histogram Construction: How much is enough?|Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining &#034;How much sampling is enough?&#034; We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a  conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose ...
315|Sampling From a Moving Window Over Streaming Data|We introduce the problem of sampling from a moving window of recent items from a data stream and develop the &#034;chain-sample&#034; and &#034;priority-sample&#034; algorithms for this problem.
316|Tracking join and self-join sizes in limited storage|This paper presents algorithms for tracking (approximate) join and self-join sizes in limited storage, in the presence of insertions and deletions to the data set(s). Such algorithms detect changes in join and self-join sizes without an expensive recomputation from the base data, and without the large space overhead required to maintain such sizes exactly. Query optimizers rely on fast, high-quality estimates of join sizes in order to select between various join plans, and estimates of self-join sizes are used to indicate the degree of skew in the data. For self-joins, we considertwo approaches proposed in [Alon, Matias, and Szegedy. The Space Complexity of Approximating the Frequency Moments. JCSS, vol. 58, 1999, p.137-147], which we denote tug-of-war and sample-count. Wepresent fast algorithms for implementing these approaches, and extensions to handle deletions as well as insertions. We also report on the rst experimental study of the two approaches, on a range of synthetic and real-world data sets. Our study shows that tug-of-war provides more accurate estimates for a given storage limit than sample-count, which in turn is far more accurate than a standard sampling-based approach. For example, tug-of-war needed only 4{256 memory words, depending on the data set, in order to estimate the self-join size
317|Data Cube Approximation and Histograms via Wavelets (Extended Abstract)  (1998) |) Jeffrey Scott Vitter   Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  jsv@cs.duke.edu Min Wang  y Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  minw@cs.duke.edu Bala Iyer  Database Technology Institute IBM Santa Teresa Laboratory P.O. Box 49023 San Jose, CA 95161 USA  balaiyer@vnet.ibm.com Abstract  There has recently been an explosion of interest in the analysis of data in data warehouses in the field of On-Line Analytical Processing (OLAP). Data warehouses can be extremely large, yet obtaining quick answers to queries is important. In many situations, obtaining the exact answer to an OLAP query is prohibitively expensive in terms of time and/or storage space. It can be advantageous to have fast, approximate answers to queries. In this paper, we present an I/O-efficient technique based upon a multiresolution wavelet decomposition that yields an approximate a...
318|Random sampling techniques for space efficient online computation of order statistics of large datasets|In a recent paper [MRL98], we had described a general framework for single pass approximate quantile nding algorithms. This framework included several known algorithms as special cases. We had identi ed a new algorithm, within the framework, which had a signi cantly smaller requirement for main memory than other known algorithms. In this paper, we address two issues left open in our earlier paper. First, all known and space e cient algorithms for approximate quantile nding require advance knowledge of the length of the input sequence. Many important database applications employing quantiles cannot provide this information. In this paper, we present anovel non-uniform random sampling scheme and an extension of our framework. Together, they form the basis of a new algorithm which computes approximate quantiles without knowing the input sequence length. Second, if the desired quantile is an extreme value (e.g., within the top 1 % of the elements), the space requirements of currently known algorithms are overly pessimistic. We provide a simple algorithm which estimates extreme values using less space than required by the earlier more general technique for computing all quantiles. Our principal observation here is that random sampling is quanti ably better when estimating extreme values than is the case with the median.  
319|Characterizing Memory Requirements for Queries over Continuous Data|This paper deals with continuous conjunctive queries with arithmetic comparisons and optional aggregation over multiple data streams. An algorithm is presented for determining whether or not any given query can be evaluated using a bounded amount of memory for all possible instances of the data streams. For queries that can be evaluated using bounded memory, an execution strategy based on constant-sized synopses of the data streams is proposed. For queries that cannot be evaluated using bounded memory, data stream scenarios are identified in which evaluating the queries requires memory linear in the size of the unbounded streams
320|Congressional samples for approximate answering of group-by queries|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex decision support queries using precomputed summary statistics, such as samples. Decision support queries routinely segment the data into groups and then aggregate the information in each group (group-by queries). Depending on the data, there can be a wide disparity between the number of data items in each group. As a result, approximate answers based on uniform random samples of the data can result in poor accuracy for groups with very few data items, since such groups will be represented in the sample by very few (often zero) tuples. In this paper, we propose a general class of techniques for obtaining fast, highly-accurate answers for group-by queries. These techniques rely on precomputed non-uniform (biased) samples of the data. In particular, we proposecongressional samples, ahybrid union of uniform and biased samples. Given a xed amount of space, congressional samples seek to maximize the accuracy for all possible group-by queries on a set of columns. We present a one pass algorithm for constructing a congressional sample and use this technique to also incrementally maintain the sample up-to-date without accessing the base relation. We also evaluate query rewriting strategies for providing approximate answers from congressional samples. Finally, we conduct an extensive set of experiments on the TPC-D database, which demonstrates the e cacy of the techniques proposed. 1
321|Histogram-Based Approximation of Set-Valued Query Answers|Answering queries approximately has recently  been proposed as a way to reduce query response  times in on-line decision support systems,  when the precise answer is not necessary  or early feedback is helpful. Most of the  work in this area uses sampling-based techniques  and handles aggregate queries, ignoring  queries that return relations as answers. In  this paper, we extend the scope of approximate  query answering to general queries. We propose  a novel and intuitive error measure for  quantifying the error in an approximate query  answer, which can be a multiset in general.
322|Data Integration using Self-Maintainable Views|.  In this paper we de#ne the concept of self-maintainable views  # these are views that can be maintained using only the contents of  the view and the database modi#cations, without accessing any of the  underlying databases. We derive tight conditions under which several  types of select-project-join are self-maintainable upon insertions, deletions  and updates. Self-Maintainability is a desirable property for e#-  ciently maintaining large views in applications where fast response and  high availability are important. One example of suchanenvironment  is data warehousing wherein views are used for integrating data from  multiple databases.  1 Introduction  Most large organizations have related data in distinct databases. Many of these databases may be legacy systems, or systems separated for organizational reasons like funding and ownership. Integrating data from such distinct databases is a pressing business need. A common approach for integration is to de#ne an integrated view and...
323|Monitoring XML Data on the Web|We consider the monitoring of a flow of incoming documents. More precisely, we present here the monitoring used in a very large warehouse built from XML documents found on the web. The flow of documents consists in XML pages (that are warehoused) and HTML pages (that are not). Our contributions are the following:  ffl a subscription language which specifies the monitoring of pages when fetched, the periodical evaluation of continuous queries and the production of XML reports. ffl the description of the architecture of the system we implemented that makes it possible to monitor a flow of millions of pages per day with millions of subscriptions on a single PC, and scales up by using more machines. ffl a new algorithm for processing alerts that can be used in a wider context. We support
325|Expiring data in a warehouse|Data warehouses collect data into materi-alized views for analysis. After some time, some of the data may no longer be needed or may not be of interest. In this pa-per, we handle this by expiring or remov-ing unneeded materialized view tuples. A framework supporting such expiration is presented. Within it, a user or adminis-trator can declaratively request expirations and can specify what type of modifications are expected from external sources. The lat-ter can significantly increase the amount of data that can be expired. We present effi-cient algorithms for determining what data can be expired (data not needed for main-tenance of other views), taking into account the types of updates that may occur. 1
327|Sequence Query Processing|Many applications require the ability to manipulate sequences of data. We motivate the importance of sequence query processing, and present a framework for the optimization of sequence queries based on several novel techniques. These include query transformations, optimizations that utilize meta--data, and caching of intermediate results. We present a bottom-up algorithm that generates an efficient query evaluation plan based on cost estimates. This work also identifies a number of directions in which future research can be directed.  1 Introduction  Many real life applications manipulate data that is inherently sequential. Such data is logically viewed and queried in terms of a sequence abstraction and is often physically stored as a sequence. Databases should (a) allow sequences to be queried in a declarative manner, utilizing the ordered semantics of the data, and (b) take advantage of the opportunities available for query optimization. Relational databases are inadequate in this re...
328|Sampling Algorithms: Lower Bounds and Applications (Extended Abstract)  (2001) |]  Ziv Bar-Yossef  y  Computer Science Division  U. C. Berkeley  Berkeley, CA 94720  zivi@cs.berkeley.edu  Ravi Kumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  ravi@almaden.ibm.com  D. Sivakumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  siva@almaden.ibm.com  ABSTRACT  We develop a framework to study probabilistic sampling algorithms that approximate general functions of the form f : A  n  ! B, where A and B are arbitrary sets. Our goal is to obtain lower bounds on the query complexity of functions, namely the number of input variables x i that any sampling algorithm needs to query to approximate f(x1 ; : : : ; xn ).  We define two quantitative properties of functions --- the block sensitivity and the minimum Hellinger distance --- that give us techniques to prove lower bounds on the query complexity. These techniques are quite general, easy to use, yet powerful enough to yield tight results. Our applications include the mean and higher statistical moments, the median and other selection functions, and the frequency moments, where we obtain lower bounds that are close to the corresponding upper bounds.  We also point out some connections between sampling and streaming algorithms and lossy compression schemes.  1. 
329|Approximating a Data Stream for Querying and Estimation: Algorithms and Performance Evaluation|Obtaining fast and good quality approximations to data distributions is a problem of central interest to database management. A variety of popular database applications including, approximate querying, similarity searching and data mining in most application domains, rely on such good quality approximations. Histogram based approximation is a very popular method in database theory and practice to succinctly represent a data distribution in a space efficient manner. In this paper, we place the problem of histogram construction into perspective and we generalize it by raising the requirement of a finite data set and/or known data set size. We consider the case of an infinite data set on which data arrive continuously forming an infinite data stream. In this context, we present the first single pass algorithms capable of constructing histograms of provable good quality. We present algorithms for the fixed window variant of the basic histogram construction problem, supporting incremental maintenance of the histograms. The proposed algorithms trade accuracy for speed and allow for a graceful tradeoff between the two, based on application requirements. In the case of approximate queries on infinite data streams, we present a detailed experimental evaluation comparing our algorithms with other applicable techniques using real data sets, demonstrating the superiority of our proposal. 1
330|A robust, optimization-based approach for approximate answering of aggregate queries|The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are “similar ” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work. 1
332|On Sampling and Relational Operators|A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. We highlight the primary difficulties, summarize the results of some recent work in this area, and indicate directions for future work.   
333|Primitives for the manipulation of general subdivisions and the computations of Voronoi diagrams|The following problem is discussed: Given n points in the plane (the sites) and an arbitrary query point 4, find the site that is closest to q. This problem can be solved by constructing the Voronoi diagram of the given sites and then locating the query point in one of its regions. Two algorithms are given, one that constructs the Voronoi diagram in O(n log n) time, and another that inserts a new site in O(n) time. Both are based on the use of the Voronoi dual, or Delaunay triangulation, and are simple enough to be of practical value. The simplicity of both algorithms can be attributed to the separation of the geometrical and topological aspects of the problem and to the use of two simple but powerful primitives, a geometric predicate and an operator for manipulating the topology of the diagram. The topology is represented by a new data structure for generalized diagrams, that is, embeddings of graphs in two-dimensional manifolds. This structure represents simultaneously an embedding, its dual, and its mirror image. Furthermore, just two operators are sufficient for building and modifying arbitrary diagrams.
335|Optimal search in planar subdivisions|Abstract. A planar subdivision is any partition of the plane into (possibly unbounded) polygonal regions. The subdivision search problem is the following: given a subdivision S with n line segments and a query point P, determine which region of S contains P. We present a practical algorithm for subdivision search that achieves the same (optimal) worst case complexity bounds as the significantly more complex algorithm of Lipton and Tarjan, namely O (log n) search time with O (n) storage. Our subdivision search structure can be constructed in linear time from the subdivision representation used in many applications. Key words, computational geometry, analysis of algorithms, point location, planar graphs, hierarchical search
336|Implementing data cubes efficiently|Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensional data cubes. Each cell of the data cube is a view consisting of an aggregation of interest, like total sales. The values of many of these cells are dependent on the values of other cells in the data cube..A common and powerful query optimization technique is to materialize some or all of these cells rather than compute them from raw data each time. Commercial systems differ mainly in their approach to materializing the data cube. In this paper, we investigate the issue of which cells (views) to materialize when it is too expensive to materialize all views. A lattice framework is used to express dependencies among views. We present greedy algorithms that work off this lattice and determine a good set of views to materialize. The greedy algorithm performs within a small constant factor of optimal under a variety of models. We then consider the most common case of the hypercube lattice and examine the choice of materialized views for hypercubes in detail, giving some good tradeoffs between the space used and the average time to answer a query. 1
337|Data cube: A relational aggregation operator generalizing group-by, cross-tab, and sub-totals|Abstract. Data analysis applications typically aggregate data across many dimensions looking for anomalies or unusual patterns. The SQL aggregate functions and the GROUP BY operator produce zero-dimensional or one-dimensional aggregates. Applications need the N-dimensional generalization of these operators. This paper defines that operator, called the data cube or simply cube. The cube operator generalizes the histogram, crosstabulation, roll-up, drill-down, and sub-total constructs found in most report writers. The novelty is that cubes are relations. Consequently, the cube operator can be imbedded in more complex non-procedural data analysis programs. The cube operator treats each of the N aggregation attributes as a dimension of N-space. The aggregate of a particular set of attribute values is a point in this space. The set of points forms an N-dimensional cube. Super-aggregates are computed by aggregating the N-cube to lower dimensional spaces. This paper (1) explains the cube and roll-up operators, (2) shows how they fit in SQL, (3) explains how users can define new aggregate functions for cubes, and (4) discusses efficient techniques to compute the cube. Many of these features are being added to the SQL Standard.
338|Multi-table joins through bitmapped join indices|This technical note shows how to combine some well-known techniques to create a method that will efficiently execute common multi-table joins. We concentrate on a commonly occurring type of join known as a star-join, although the method presented will generalize to any type of multi-table join. A star-join consists of a central detail table with large cardinality, such as an orders table (where an order row contains a single purchase) with foreign keys that join to descriptive tables, such as customers, products, and (sales) agents. The method presented in this note uses join indices with compressed bitmap representations, which allow predicates restricting columns of descriptive tables to determine an answer set (or foundsef) in the central detail table; the method uses different predicates on different descriptive tables in combination to restrict the detail table through compressed bitmap representations of join indices, and easily completes the join of the fully restricted detail table rows back to the descriptive tables. We outline realistic examples where the combination of these techniques yields substantial performance improvements over alternative, more traditional query evaluation plans. 1.
339|Aggregate-Query Processing in Data Warehousing Environments|In this paper we introduce generalized projections (GPs), an extension of duplicate eliminating projections, that capture aggregations, groupbys, duplicate-eliminating projections (distinct), and duplicate-preserving projections in a common unified framework. Using GPs we extend well known and simple algorithms for SQL queries that use distinct projections to derive algorithms for queries using aggregations like sum, max, min, count, and avg. We develop powerful query rewrite rules for aggregate queries that unify and extend rewrite rules previously known in the literature. We then illustrate the power of our approach by solving a very practical and important problem in data warehousing: how to answer an aggregate query on base tables using materialized aggregate views (summary tables).
340|Including Group-By in Query Optimization|In existing relational database systems, processing of group-by and computation of aggregate functions are always postponed until all joins are performed. In this paper, we present transformations that make it possible to push group-by operation past one or more joins and can potentially reduce the cost of processing a query significantly. Therefore, the placement of group-by should be decided based on cost estimation. We explain how the traditional System-R style optimizers can be modified by incorporating the greedy conservative heuristic that we developed. We prove that applications of greedy conservative heuristic produce plans that are better (or no worse) than the plans generated by a traditional optimizer. Our experimental study shows that the extent of improvement in the quality of plans is significant with only a modest increase in optimization cost. Our technique also applies to optimization of Select Distinct queries by pushing down duplicate elimination in a cost-based fas...
341|Eager Aggregation and Lazy Aggregation|Efficient processing of aggregation queries is essential for decision support applications. This paper describes a class of query transformations, called eager aggregation and lazy aggregation, that allows a query optimizer to move group-by operations up and down the query tree. Eager aggregation partially pushes a group-by past a join. After a group-by is partially pushed down, we still need to perform the original group-by in the upper query block. Eager aggregation reduces the number of input rows to the join and thus may result in a better overall plan. The reverse transformation, lazy aggregation, pulls a group-by above a join and combines two group-by operations into one. This transformation is typically of interest when an aggregation query references a grouped view (a view containing a group-by). Experimental results show that the technique is very beneficial for queries in the TPC-D benchmark. 1 Introduction Aggregation is widely used in decision support systems. All queries...
342|Data Integration: A Theoretical Perspective|Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. The problem of designing data integration systems is important in current real world applications, and is characterized by a number of issues that are interesting from a theoretical point of view. This document presents on overview of the material to be presented in a tutorial on data integration. The tutorial is focused on some of the theoretical issues that are relevant for data integration. Special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and reasoning on queries.
343|A Survey of Approaches to Automatic Schema Matching|Schema matching is a basic problem in many database application domains, such as data integration, E-business, data warehousing, and semantic query processing. In current implementations, schema matching is typically performed manually, which has significant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match operation for specific application domains. We present a taxonomy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distinguish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classification we review some previous match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component.
344|Answering Queries Using Views: A Survey|The problem of answering queries using views is to find efficient methods of answering a query using a set of previously defined materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems. In query optimization, finding a rewriting of a query using a set of materialized views can yield a more efficient query execution plan. To support the separation of the logical and physical views of data, a storage schema can be described using views over the logical schema. As a result, finding a query execution plan that accesses the storage amounts to solving the problem of answering queries using views. Finally, the problem arises in data integration systems, where data sources can be described as precomputed views over a mediated schema. This article surveys the state of the art on the problem of answering queries using views, and synthesizes the disparate works into a coherent framework. We describe the different applications of the problem, the algorithms proposed to solve it and the relevant theoretical results.
345|Information integration using logical views|A number of ideas concerning information-integration tools can be thought of as constructing answers to queries using views that represent the capabilities of information sources. We review the formal basis of these techniques, which are closely related to containment algorithms for conjunctive queries and/or Datalog programs. Then we compare the approaches taken by AT&amp;T Labs&#039; &#034;Information Manifold&#034; and the Stanford &#034;Tsimmis&#034; project in these terms.
346|The TSIMMIS Approach to Mediation: Data Models and Languages|TSIMMIS -- The Stanford-IBM Manager of Multiple Information Sources -- is a system for integrating information. It o ers a data model and a common query language that are designed to support the combining of information from many different sources. It also o ers tools for generating automatically the components that are needed to build systems for integrating information. In this paper we shall discuss the principal architectural features and their rationale. 
347|Query Answering in Inconsistent Databases|In this chapter, we summarize the research on querying inconsistent databases we have been conducting over the last five years. The formal framework we have used is based on two concepts: repair and consistent query answer. We describe different approaches to the issue of computing consistent query answers: query transformation, logic programming, inference in annotated logics, and specialized algorithms. We also characterize the computational complexity of this problem. Finally, we discuss related research in artificial intelligence, databases, and logic programming.
348|Index Structures for Path Expressions|In recent years there has been an increased interest in managing data which does not conform to traditional data models, like the relational or object oriented model. The reasons for this non-conformance are diverse. One one hand, data may not conform to such models at the physical level: it may be stored in data exchange formats, fetched from the Internet, or stored as structured les. One the other hand, it may not conform at the logical level: data may have missing attributes, some attributes may be of di erent types in di erent data items, there may be heterogeneous collections, or the data may be simply specified by a schema which is too complex or changes too often to be described easily as a traditional schema. The term semistructured data has been used to refer to such data. The data model proposed for this kind of data consists of an edge-labeled graph, in which nodes correspond to objects and edges to attributes or values. Figure 1 illustrates a semistructured database providing information about a city. Relational databases are traditionally queried with associative queries, retrieving tuples based on the value of some attributes. To answer such queries efciently, database management systems support indexes for translating attribute values into tuple ids (e.g. B-trees or hash tables). In object-oriented databases, path queries replace the simpler associative queries. Several data structures have been proposed for answering path queries e ciently: e.g., access support relations 14] and path indexes 4]. In the case of semistructured data, queries are even more complex, because they may contain generalized path expressions 1, 7, 8, 16]. The additional exibility is needed in order to traverse data whose structure is irregular, or partially unknown to the user.
349|Complexity of answering queries using materialized views|WC study the complexity of the problem of answering queries using materinlized views, This problem has attracted a lot of attention re-cently because of its relevance in data integration. Previous work considered only conjunctive view definitions. We examine the con-sequences of allowing more expressive view definition languages. Tl~olanguagcsweconsiderforviewdefinitionsanduserqueriesare: conjunctive qucrics with inequality, positive queries, datalog, and first-order logic. We show that the complexity of the problem de-pcnds on whether views are assumed to store all the tuples that sat-isfy the view definition, or only a subset of it. Finally, we apply the results to the view consistency and view self-maintainability prob-lems which nrise in data warehousing. 1
350|Semistructured data|In semistructured data, the information that is normally as-sociated with a schema is contained within the data, which is sometimes called “self-describing”. In some forms of semi-structured data there is no separate schema, in others it exists but only places loose constraints on the data. Semi-structured data has recently emerged as an important topic of study for a variety of reasons. First, there are data sources such as the Web, which we would like to treat as databases but which cannot be constrained by a schema. Second, it may be desirable to have an extremely flexible format for data exchange between disparate databases. Third, even when dealing with structured data, it may be helpful to view it. as semistructured for the purposes of browsing. This tu-torial will cover a number of issues surrounding such data: finding a concise formulation, building a sufficiently expres-sive language for querying and transformation, and opti-mizat,ion problems. 1 The motivation The topic of semistructured data (also called unstructured data) is relatively recent, and a tutorial on the topic may well be premature. It represents, if anything, the conver-gence of a number of lines of thinking about new ways to represent and query data that do not completely fit with conventional data models. The purpose of this tutorial is to to describe this motivation and to suggest areas in which further research may be fruitful. For a similar exposition, the reader is referred to Serge Abiteboul’s recent survey pa-per PI. The slides for this tutorial will be made available from a section of the Penn database home page
351| On the Decidability of Query Containment under Constraints |Query containment under constraints is the problem of checking whether for every database satisfying a given set of constraints, the result of one query is a subset of the result of another query. Recent research points out that this is a central problem in several database applications, and we address it within a setting where constraints are specified in the form of special inclusion dependencies over complex expressions, built by using intersection and difference of relations, special forms of quantification, regular expressions over binary relations, and cardinality constraints. These types of constraints capture a great variety of data models, including the relational, the entity-relational, and the object-oriented model. We study the problem of checking whether q is contained in q ' with respect to the constraints specified in a schema S, where q and q ' are nonrecursive Datalog programs whose atoms are complex expressions. We present the following results on query containment. For the case where q does not contain regular expressions, we provide a method for deciding query containment, and analyze its computational complexity. We do the same for the case where neither S nor q, q ' contain number restrictions. To the best of our knowledge, this yields the first decidability result on containment of conjunctive queries with regular expressions. Finally, we prove that the problem is undecidable for the case where we admit inequalities in q'.  
352|Integration of Heterogeneous Databases Without Common Domains Using Queries Based on Textual Similarity|Most databases contain &#034;name constants&#034; like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. However, in many cases, this assumption does not hold; determining if two name constants should be considered identical can require detailed knowledge of the world, the purpose of the user&#039;s query, or both. In this paper, we reject the assumption that global domains can be easily constructed, and assume instead that the names are given in natural language text. We then propose a logic called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. We describe an efficient implementation of WHIRL and evaluate it experimentally on data extracted from the World Wide Web. We show that WHIR...
353|Context Interchange: New Features and Formalisms for the Intelligent Integration of Information|The Context Interchange strategy presents a novel perspective for mediated data access in which semantic conflicts among heterogeneous systems are not identified a priori, but are detected and reconciled by a context mediator through comparison of contexts axioms corresponding to the systems engaged in data exchange. In this article, we show that queries formulated on shared views, export schema, and shared “ontologies ” can be mediated in the same way using the Context Interchange framework. The proposed framework provides a logic-based object-oriented formalism for representing and reasoning about data semantics in disparate systems, and has been validated in a prototype implementation providing mediated data access to both traditional and web-based information sources. Categories and Subject Descriptors: H.2.4 [Database Management]: Systems—Query processing; H.2.5 [Database Management]: Heterogeneous Databases—Data translation
354|Catching the Boat with Strudel: Experiences with a Web-Site Management System|The Strudel system applies concepts from database management systems to the process of building Web sites. Strudel&#039;s key idea is separating the management of the site&#039;s data, the creation and management of the site&#039;s structure, and the visual presentation of the site&#039;s pages. First, the site builder creates a uniform model of all data available at the site. Second, the builder uses this model to declaratively define the Web site&#039;s structure by applying a &#034;site-definition query&#034; to the underlying data. The result of evaluating this query is a &#034;site graph&#034;, which represents both the site&#039;s content and structure. Third, the builder specifies the visual presentation of pages in Strudel&#039;s HTML-template language. The data model underlying Strudel is a semi-structured model of labeled directed graphs. We describe Strudel&#039;s key characteristics, report on our experiences using Strudel, and present the technical problems that arose from our experience. We describe our experience constructing sev...
355|Answering Recursive Queries Using Views|We consider the problem of answering datalog queries using materialized views. The ability to answer queries using views is crucial in the context of information integration. Previous work on answering queries using views restricted queries to being conjunctive. We extend this work to general recursive queries: Given a datalog program P and a set of views, is it possible to find a datalog program that is equivalent to P and only uses views as EDB predicates? In this paper, we show that the problem of whether a datalog program can be rewritten into an equivalent program that only uses views is undecidable. On the other hand, we prove that a datalog program P can be effectively rewritten into a program that only uses views, that is contained in P,  and that contains all programs that only use views and are contained in P. As a consequence, if there exists a program equivalent to P that only uses views, then our construction is guaranteed to yield a program equivalent to P.  1 Introductio...
356|Managing Semantic Heterogeneity in Databases : A Theoretical Perspective, Tutorial at PODS|A full version of this tutorial appears at
357|Description Logics in Data Management|Description logics and reasoners, which are descendants of the kl-one language, have been studied in depth in Artificial Intelligence. After a brief introduction, we survey in this paper their application to the problems of information management, using the framework of an abstract information server equipped with several operations -- each involving one or more languages. Specifically, we indicate how one can achieve enhanced access to data and knowledge by using descriptions in languages for schema design and integration, queries, answers, updates, rules, and constraints.
358|The Information Manifold|We describe the Information Manifold (IM), a system for browsing and querying of multiple networked information sources. As a first contribution, the system demonstrates the viability of knowledge representation technology for retrieval and organization of information from disparate (structured and unstructured) information sources. Such an organization allows the user to pose high-level queries that use data from multiple information sources. As a second contribution, we describe novel query processing algorithms used to combine information from multiple sources. In particular, our algorithms are guaranteed to find exactly the set of information sources relevant to a query, and to  completely exploit knowledge about local closed world information (Etzioni et al. 1994). Introduction  We are currently witnessing an explosion in the amount of information that is available online. For example, the rapid rise in popularity of the World Wide Web (WWW) has increased the amount of information...
359|Description logic framework for information integration|Information Integration is one of the core problems in distributed databases, cooperative information systems, and data warehousing, which are key areas in the software development industry. Two critical factors for the design and maintenance of applications requiring Information Integration are conceptual modeling of the domain, and reasoning support over the conceptual representation. We demonstrate that Knowledge Representation and Reasoning techniques can play an important role for both of these factors, by proposing a Description Logic based framework for Information Integration. We show that the development of successful Information Integration solutions requires not only to resort to very expressive Description Logics, but also to significantly extend them. We present a novel approach to conceptual modeling for Information Integration, which allows for suitably modeling the global concepts of the application, the individual information sources, and the constraints among different sources. Moreover, we devise inference procedures for the fundamental reasoning services, namely relation and concept subsumption, and query containment. Finally, we present a methodological framework for Information Integration, which can be applied in several contexts, and highlights the role of reasoning services within the design process. 1
360|Equivalences among relational expressions with the union and difference operators|ABSTRACT Queries in relational databases can be formulated in terms of relational expressions using the relational operations elect, project, join, union, and difference The equivalence problem for these queries is studied with query optimization m mind It ts shown that testmg eqmvalence of relational expressions with the operators elect, project, join, and union is complete m the class FIt of the polynomial-time hierarchy A nonprocedural representation for queries formulated by these expressions i proposed This method of query representation can be viewed as a generahzatlon f tableaux or conjunctive queries (which are used to represent expressions with only select, project, and join) Furthermore, this method is extended to queries formulated by relatmnal expressions that also contain the difference operator, provided that the project operator isnot applied to subexpresstons with the difference operator A procedure for testing eqmvalence of these queries is given It ts shown that testmg containment of tableaux is a necessary step in testing equivalence of queries with union and difference Three important cases m which containment of tableaux can be tested m polynomial time are described, although the containment problem is shown to be NP-complete ven for tableaux that correspond to expressions with only one project and several join operators KEY WORDS AND PHRASES relatmnal database, relational algebra, query optimization, equivalence of queries, conjunctive query, tableau, NP-complete, polynomial-time hierarchy, H 2P-complete CR CATEGORIES 4 33, 5 25
361|Representing and Using Interschema Knowledge in Cooperative Information Systems|Managing interschema knowledge is an essential task when dealing with cooperative information systems. We propose a logical approach to the problem of both expressing interschema knowledge, and reasoning about it. In particular, we set up a structured representation language for expressing semantic interdependencies between classes belonging to different database schemas, and present a method for reasoning over such interdependencies. The language and the associated reasoning technique makes it possible to build a logic-based module that can draw useful inferences whenever the need arises of both comparing and combining the knowledge represented in the various schemas. Notable examples of such inferences include checking the coherence of interschema knowledge, and providing integrated access to a cooperative information system.
362|Navigational Plans For Data Integration|We consider the problem of building data integration systems when the data sources are webs of data, rather than sets of relations. Previous approaches to modeling data sources are inappropriate in this context because they do not capture the relationships between linked data and the need to navigate through paths in the data source in order to obtain the data. We describe a language for modeling data sources in this new context. We show that our language has the required expressive power, and that minor extensions to it would make query answering intractable. We provide a sound and complete algorithm for reformulating a user query into a query over the data sources, and we show how to create query execution plans that both query and navigate the data sources.  Introduction  The purpose of data integration is to provide a uniform  interface to a multitude of data sources. Data integration applications arise frequently as corporations attempt to provide their customers and employees wit...
363|Tableau Techniques For Querying Information Sources Through Global Schemas|. The foundational homomorphism techniques introduced by Chandra and Merlin for testing containment of conjunctive queries have recently attracted renewed interest due to their central role in information integration applications. We show that generalizations of the classical tableau representation of conjunctive queries are useful for computing query answers in information integration systems where information sources are modeled as views defined on a virtual global schema. We consider a general situation where sources may or may not be known to be correct and complete. We characterize the set of answers to a global query and give algorithms to compute a finite representation of this possibly infinite set, as well as its certain and possible approximations. We show how to rewrite a global query in terms of the sources in two special cases, and show that one of these is equivalent to the Information Manifold rewriting of Levy et al. 1 Introduction Information Integration systems [Ull...
364|What Can Databases Do for Peer-to-Peer?|The Internet community has recently been focused on peer-to-peer systems like Napster, Gnutella, and Freenet. The  grand vision --- a decentralized community of machines pooling their resources to benefit everyone --- is compelling for  many reasons: scalability, robustness, lack of need for administration, and even anonymity and resistance to censorship.
365|Data Integration under Integrity Constraints|Data integratio n systemspro vide accessto a seto fhetero - geneo us, auto no mo us data so urces thro ugh a so -called glo bal schema. There are basically two appro aches fo r designing a data integratio n system. In the glo bal-centric appro ach,o ne defines the elementso f the glo bal schema as viewso ver the so urces, whereas in the lo cal-centric appro ach, o e characterizes the so rces as viewso ver theglo al schema. It is well kno wn that pro cessing queries in the latter appro ach is similar to query answering with inc o plete infoC atio , and, therefo9 is a c o plex task. On theo ther hand, it is a co mmo no pinio n that query pro cessing is much easier in the fo rmer appro ach. In this paper we sho w the surprising result that, when theglo al schema is expressed in the relatio al mo del with integrity c o straints, eveno f simple types, the pr o lemo f inco6 plete info rmatio n implicitly arises, making querypro cessing di#cult in the glo al-centric approC h as well. We thenfo cuso n glo al schemas with key andfo eign key co straints, which represents a situat io which is veryco#=W in practice, and we illustrate techniques fo e#ectively answering queries po sed to the data integratio n system in this case. 1 
366|Towards Heterogeneous Multimedia Information Systems: The Garlic Approach|Abstract: We provide an overview of the Garlic project, a new project at the IBM Almaden Research Center. The goal of this project is to develop a system and associated tools for the management of large quantities of heterogeneous multimedia information. Garlic permits traditional and multimedia data to be
367|Rewriting of Regular Expressions and Regular Path Queries|Recent work on semi-structured data has revitalized the interest in path queries, i.e., queries that ask for all pairs of objects in the database that are connected by a path conforming to a certain specification, in particular to a regular expression. Also, in semi-structured data, as well as in data integration, data warehousing, and query optimization, the problem of view-based query rewriting is receiving much attention: Given a query and a collection of views, generate a new query which uses the views and provides the answer to the original one. In this paper we address the problem of view-based query rewriting in the context of semi-structured data. We present a method for computing the rewriting of a regular expression E in terms of other regular expressions. The method computes the exact rewriting (the one that defines the same regular language as E) if it exists, or the rewriting that defines the maximal language contained in the one defined by E, otherwise. We present a complexity analysis of both the problem and the method, showing that the latter is essentially optimal. Finally, we illustrate how to exploit the method for view-based rewriting of regular path queries in semi-structured data. The complexity results established for the rewriting of regular expressions apply also to the case of regular path queries. 
368|CARIN: A Representation Language Combining Horn Rules and Description Logics|.  We describe CARIN, a novel family of representation languages, which integrate the expressive power of Horn rules and of description logics. We address the key issue in designing such a language, namely, providing a sound and complete inference procedure. We identify existential entailment as a core problem in reasoning in  CARIN, and describe an existential entailment algorithm for CARIN  languages whose description logic component is ALCNR. This algorithm entails several important results for reasoning in CARIN, most notably: (1) a sound and complete inference procedure for non recursive  CARIN-ALCNR, and (2) an algorithm for determining rule subsumption over ALCNR. 1 Introduction  Horn rule languages have formed the basis for many Artificial Intelligence application languages because their expressive power is sufficient for many applications, and they have good computational properties. One of the significant limitations of Horn rules is that they are not expressive enough to mod...
369|Quality-driven Integration of Heterogeneous Information Systems|Integrated access to information that is spread over multiple, distributed, and heterogeneous sources is an important problem in many scientific and commercial domains. While much work has been done on query processing and choosing plans under cost criteria, very little is known about the important problem of incorporating the information quality aspect into query planning. In this paper we describe a framework for multidatabase query processing that fully includes the quality of information in many facets, such as completeness, timeliness, accuracy, etc. We seamlessly include information quality into a multidatabase query processor based on a view-rewriting mechanism. We model information quality at different levels to ultimately find a set of high-quality query answering plans.
370|Query Containment for Conjunctive Queries With Regular Expressions|The management of semistructured data has recently received significant attention because of the need of several applications to model and query large volumes of irregular data. This paper considers the problem of query containment for a query language over semistructured data, StruQL0 , that contains the essential feature common to all such languages, namely the ability to specify regular path expressions over the data. We show here that containment of StruQL0 queries is decidable. First, we give a semantic criterion for StruQL0 query containment: we show that it suffices to check containment on only finitely many canonical databases. Second, we give a syntactic criteria for query containment, based on a notion of query mappings, which extends containment mappings for conjunctive queries. Third, we consider a certain fragment of StruQL0 , obtained by imposing restrictions on the regular path expressions, and show that query containment for this fragment of  StruQL0 is NP complete.  1 ...
371|Recursive Plans for Information Gathering|Generating query-answering plans for information gathering agents requires to translate a user query, formulated in terms of a set of virtual relations, to a query that uses relations that are actually stored in information sources. Previous solutions to the translation problem produced sets of conjunctive plans, and were therefore limited in their ability to handle information sources with binding-pattern limitations, and to exploit functional dependencies in the domain model. As a result, these plans were incomplete w.r.t. sources encountered in practice (i.e., produced only a subset of the possible answers). We describe the novel class of recursive  information gathering plans, which enables us to settle two open problems. First, we describe an algorithm for finding a query plan that produces the maximal set of answers from the sources in the presence of functional dependencies. Second, we describe an analogous algorithm in the presence of binding-pattern restrictions in the sources...
372|EQUIVALENCES AMONG RELATIONAL EXPRESSIONS|Many database queries can be formulated in terms of expressions whose operands represent tables of information (relations) and whose operators are the relational operations select, project, and join. This paper studies the equivalence problem for these relational expressions, with expression optimization in mind. A matrix, called a tableau, is proposed as a natural representative for the value of an expression. It is shown how tableaux can be made to reflect functional dependencies among attributes. A polynomial time algorithm is presented for the equivalence of tableaux that correspond to an important subset of expressions, although the equivalence problem is shown to be NP-complete under slightly more general circumstances.
373|On the equivalence of recursive and nonrecursive Datalog programs|vardi Abstract: We study the problem of determining whether a given recursive Datalog program is equivalent to a given nonrecursive Datalog program. Since nonrecursive Dat-alog programs are equivalent to unions of conjunctive queries, we study also the problem of determining whether a given recursive Datalog program is contained in a union of con-junctive queries. For this problem, we prove doubly exponential upper and lower time bounds. For the equivalence problem, we prove triply exponential upper and lower time bounds. 1
374|Containment of conjunctive regular path queries with inverse |Reasoning on queries is a basic problem both in knowledge representation and databases. A fundamental form of reasoning on queries is checking containment, i.e., verifying whether one query yields necessarily a subset of the result of another query. Query containment is crucial in several contexts, such as query optimization, knowledge base verification, information integration, database integrity checking, and cooperative answering. In this paper we address the problem of query containment in the context of semistructured knowledge bases, where the basic querying mechanism, namely regular path queries,
375|Rewriting Aggregate Queries Using Views|We investigate the problem of rewriting queries with aggregate operators using views that mayormay not contain aggregate operators. A rewriting of a query is a second query that uses view predicates such that evaluating first the views and then the rewriting yields the same result as evaluating the original query. In this sense, the original query and the rewriting are equivalent modulo the view definitions. The queries and views we consider correspond to unnested SQL queries, possibly with union, that employ the operators min, max, count, and sum. Our approach is based on syntactic characterizations of the equivalence of aggregate queries. One contribution of this paper are characterizations of the equivalence of disjunctive aggregate queries, which generalize our previous results for the conjunctive case. For each operator &amp;alpha;, we introduce several types of queries using views as candidates for rewritings. We unfold such a candidate by replacing each occurrence of a view predicate with ...
376|An Extensible Framework for Data Cleaning|Data integration solutions dealing with large amounts of data have been strongly required in the last few years.  Besides the traditional data integration problems (e.g. schema integration, local to global schema mappings),  three additional data problems have to be dealt with: (1) the absence of universal keys across dierent databases  that is known as the object identity problem, (2) the existence of keyboard errors in the data, and (3) the presence  of inconsistencies in data coming from multiple sources. Dealing with these problems is globally called the data  cleaning process. In this work, we propose a framework which oers the fundamental services required by this  process: data transformation, duplicate elimination and multi-table matching. These services are implemented  using a set of purposely designed macro-operators. Moreover, we propose an SQL extension for specifying  each of the macro-operators. One important feature of the framework is the ability of explicitly includ...
377|Answering regular path queries using views|Query answering using views amounts to computing the answer to a query having information only on the extension of a set of views. This problem is relevant in several fields, such as information integration, data warehousing, query optimization, mobile computing, and maintaining physical data independence. We address query answering using views in a context where queries and views are regular path queries, i.e., regular expressions that denote the pairs of objects in the database connected by a matching path. Regular path queries are the basic query mechanism when the database is conceived as a graph, such as in semistructured data and data on the web. We study algorithms for answering regular path queries using views under different assumptions, namely, closed and open domain, and sound, complete, and exact information on view extensions. We characterize data, expression, and combined complexity of the problem, showing that the proposed algorithms are essentially optimal. Our results are the first to exhibit decidability in cases where the language for expressing the query and the views allows for recursion. 
378|Query Planning and Optimization in Information Integration|Information integration systems, also knows as mediators, information brokers, or information gathering agents, provide uniform user interfaces to varieties of different information sources. With corporate databases getting connected by intranets, and vast amounts of information becoming available over the Internet, the need for information integration systems is increasing steadily. Our work focusses on query planning in such systems...
379|Answering queries using views over description logics knowledge bases|Answering queries using views amounts to computing the answer to a query having information only on the extension of a set of precomputed queries (views). This problem is relevant in several fields, such as information integration, query optimization, and data warehousing, and has been studied recently in different settings. In this paper we address answering queries using views in a setting where intensional knowledge about the domain is represented using a very expressive Description Logic equipped with n-ary relations, and queries are nonrecursive datalog queries whose predicates are the concepts and relations that appear in the Description Logic knowledge base. We study the problem under different assumptions, namely, closed and open domain, and sound, complete, and exact information on view extensions. We show that under the closed domain assumption, in which the set of all objects in the knowledge base coincides with the set of objects stored in the views, answering queries using views is already intractable. We show also that under the open domain assumption the problem is decidable in double exponential time.
380|A Formal Perspective on the View Selection Problem|The view selection problem is to choose a set of views to materialize over a database schema, such that the cost of evaluating a set of workload queries is minimized and such that the views t into a prespeci ed storage constraint. The two main applications of the view selection problem are materializing views in a database to speed up query processing, and selecting views to materialize in a data warehouse to answer decision support queries. We describe several fundamental results concerning the view selection problem. We consider the problem for views and workloads that consist of equalityselection, project and join queries, and show that the complexity of the problem depends crucially on the quality of the estimates that a query optimizer has on the size of the views it is considering to materialize. When a query optimizer has good estimates of the sizes of the views, we show that an optimal choice of views may involve a number of views that is exponential in the size of the database schema. On the other hand, when an optimizer uses standard estimation heuristics, we show that the number of necessary views and the expression size of each view are polynomially bounded. 1
381|Query containment for data integration systems|The problem of query containment is fundamental to many aspects of database systems,including query optimization,determining independence of queries from updates,and rewriting queries using views. In the data-integration framework,however,the standard notion of query containment does not suffice. We define relative containment,which formalizes the notion of query containment relative to the sources available to the data-integration system. First,we provide optimal bounds for relative containment for several important classes of datalog queries,including the common case of conjunctive queries. Next,we provide bounds for the case when sources enforce access restrictions in the form of binding pattern constraints. Surprisingly,we show that relative containment for conjunctive queries is still decidable in this case,even though it is known that finding all answers to such queries may require a recursive datalog program over the sources. Finally,we provide tight bounds for variants of relative containment when the queries and source descriptions may contain comparison predicates.
382|Specifying and querying database repairs using logic programs with exceptions|Abstract Databases may be inconsistent with respect to a given set of integrity constraints. Nevertheless, most of the data may be consistent. In this paper we show how to specify consistent data and how to query a relational database in such a way that only consistent data is retrieved. The specification and queries are based on disjunctive extended logic programs with positive and negative exceptions that generalize those previously introduced by Kowalski and Sadri.
383|View-based query processing and constraint satisfaction|View-based query processing requires to answer a query posed to a database only on the basis of the information on a set of views, which are again queries over the same database. This problem is relevant in many aspects of database management, and has been addressed by means of two basic approaches, namely, query rewriting and query answering. In the former approach, one tries to compute a rewriting of the query in terms of the views, whereas in the latter, one aims at directly answering the query based on the view extensions. We study view-based query processing for the case of regular-path queries, which are the basic querying mechanisms for the emergent field of semistructured data. Based on recent results, we first show that a rewriting is in general a co-NP function wrt to the size of view extensions. Hence, the problem arises of characterizing which instances of the problem admit a rewriting that is PTIME. A second contribution of the work is to establish a tight connection between view-based query answering and constraint-satisfaction problems, which allows us to show that the above characterization is going to be difficult. As a third contribution of our work, we present two methods for computing PTIME rewritings of specific forms. The first method, which is based on the established connection with constraint-satisfaction problems, gives us rewritings expressed in Datalog with a fixed number of variables. The second method, based on automata-theoretic techniques, gives us rewritings that are formulated as unions of conjunctive regular-path queries with a fixed number of variables.  
384|Optimization Properties for Classes of Conjunctive Regular Path Queries |Abstract. We are interested in the theoretical foundations of the optimization of conjunctive regular path queries (CRPQs). The basic problem here is deciding query containment both in the absence and presence of constraints. Containment without constraints for CRPQs is EXPSPACE-complete, as opposed to only NP-complete for relational conjunctive queries. Our past experience with implementing similar algorithms suggests that staying in PSPACE might still be useful. Therefore we investigate the complexity of containment for a hierarchy of fragments of the CRPQ language. The classifying principle of the fragments is the expressivity of the regular path expressions allowed in the query atoms. For most of these fragments, we give matching lower and upper bounds for containment in the absence of constraints. We also introduce for every fragment a naturally corresponding class of constraints in whose presence we show both decidability and undecidability results for containment in various fragments. Finally, we apply our results to give a complete algorithm for rewriting with views in the presence of constraints for a fragment that contains Kleene-star and disjunction. 1
385|Deciding Containment for Queries with Complex Objects and Aggregations|We address the problem of query containment and query equivalence for complex objects. We show that for a certain conjunctive query language for complex objects, query containment and weak query equivalence are decidable. Our results have two consequences. First, when the answers of the two queries are guaranteed not to contain empty sets, then weak equivalence coincides with equivalence, and our result answers partially an open problem about the equivalence of nest; unnest queries for complex objects [GPG90]. Second, we derive an NP-complete algorithm for checking the equivalence of certain conjunctive queries with grouping and aggregates. Our results rely on a translation of the containment and equivalence conditions for complex objects into novel conditions on conjunctive queries, which we call simulation and strong simulation. These conditions are more complex than containment of conjunctive queries, because they involve arbitrary numbers of quantifier alternations. We prove that c...
386|Capability Based Mediation in TSIMMIS|Introduction  The TSIMMIS system [1] integrates data from multiple heterogeneous sources and provides users with seamless integrated views of the data. It translates a user query on the integrated views into a set of source queries and postprocessing steps that compute the answer to the user query from the results of the source queries. TSIMMIS uses a  mediation architecture [11] to accomplish this (Figure 1). User Source 1 Source 2 Source N  Mediator Figure 1: The TSIMMIS Architecture Many other data integration systems like Garlic [2, 9] and Information Manifold [4] employ a similar architecture. One of the distinguishing features of TSIMMIS is its use of a semi-structured data model (called the Object Exchange Model or OEM [7]) for dealing with the heterogeneity of the data sources. In particular, it employs source wrappers [3] that provide a uniform OEM interface to the mediator. In SIGMO
387|Query Answering in Information Systems with Integrity Constraints|The specifications of most of the nowadays ubiquitous informations systems include integrity constraints, i.e. conditions rejecting so-called &#034;invalid&#034; or &#034;inconsistent &#034; data. Information system consistency and query answering have been formalized referring to classical logic implicitly assuming that query answering only makes sense with consistent information systems. In practice, however, inconsistent as well as consistent information systems need to be queried. In this paper, it is first argued that classical logic is inappropriate for a formalization of information systems because of its global notion of inconsistency. It is claimed that information systems inconsistency should be understood as a  local notion. Then, it is shown that minimal logic, a constructivistic weakening of classical logic which precludes refutation proofs, provides for local inconsistencies that conveniently reflect a practitioner&#039;s intuition. Further, minimal logic is shown to be a convenient foundation fo...
388|Query Folding with Inclusion Dependencies|Query folding is a technique for determining how a query may be answered using a given set of resources, which may include materialized views, cached results of previous queries, or queries answerable by other databases. The power of query folding can be considerably enhanced by taking into account integrity constraints that are known to hold on base relations. This paper describes an extension of query folding that utilizes inclusion dependencies to find foldings of queries that would otherwise be overlooked. We describe a complete strategy for finding foldings in the presence of inclusion dependencies and present a basic algorithm that implements that strategy. We also describe extensions to this algorithm when both inclusion and functional dependencies are considered.
389|Information Integration: the MOMIS Project Demonstration|ranted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 26th VLDB Conference,  Cairo, Egypt, 2000.  2  MOMIS is a joint project among the Universit`a di Modena e Reggio Emilia, the Universit`a di Milano, and the Universit`a di Brescia within the national research project INTERDATA, theme n.3 &#034;Integration of Information over the Web&#034;, coordinated by V. De Antonellis, Universit`a di Brescia.  1. a common data model, ODM I 3  , which is defined according to the ODL I 3 language, to describe source schemas for integration purposes. ODM I 3 and ODL I 3&lt;F12.24
390|Query Planning with Limited Source Capabilities|In information-integration systems, sources may have diverse and limited query capabilities. In this paper we show that because sources have restrictions on retrieving their information, sources not mentioned in a query can contribute to the query result by providing useful bindings. In some cases we can access sources repeatedly to retrieve bindings to answer a query, and query planning thus becomes considerably more challenging. We find all the obtainable answers to a query by translating the query and source descriptions to a simple recursive Datalog program, and evaluating the program on the source relations. This program often accesses sources that are not in the query. Some of these accesses are essential, as they provide bindings that let us query sources, which we could not do otherwise. However, some of these accesses can be proven not to add anything to the query&#039;s answer. We show in which cases these off-query accesses are useless, and prove that in these cases we can comput...
391|Querying Aggregate Data|We introduce a first-order language with real polynomial arithmetic and aggregation operators (count, iterated sum and multiply), which is well suited for the definition of aggregate queries involving complex statistical functions. It offers a good trade-off between expressive power and complexity, with a tractable data complexity. Interestingly, some fundamental properties of first-order with real arithmetic are preserved in the presence of aggregates. In particular, there is an effective quantifier elimination for formulae with aggregation. We consider the problem of querying data that has already been aggregated in aggregate views, and focus on queries with an aggregation over a conjunctive query. Our main conceptual contribution is the introduction of a new equivalence relation among conjunctive queries, the isomorphism modulo a product. We prove that the equivalence of aggregate queries such as for instance averages reduces to it. Deciding if two queries are isomorphic modulo a p...
392|Accessing data integration systems through conceptual schemas|Abstract. Data integration systems provide access to a set of heterogeneous, autonomous data sources through a so-called global, or mediated view. There is a general consensus that the best way to describe the global view is through a conceptual data model, and that there are basically two approaches for designing a data integration system. In the global-as-view approach, one defines the concepts in the global schema as views over the sources, whereas in the local-as-view approach, one characterizes the sources as views over the global schema. It is well known that processing queries in the latter approach is similar to query answering with incomplete information, and, therefore, is a complex task. On the other hand, it is a common opinion that query processing is much easier in the former approach. In this paper we show the surprising result that, when the global schema is expressed in terms of a conceptual data model, even a very simple one, query processing becomes difficult in the global-as-view approach also. We demonstrate that the problem of incomplete information arises in this case too, and we illustrate some basic techniques for effectively answering queries posed to the global schema of the data integration system. 1
393|Answering Queries Using Materialized Views With Disjunctions|We consider the problem of answering datalog queries using  materialized views. More  specifi.
394|On Answering Queries in the Presence of Limited Access Patterns|. In information-integration systems, source relations often  have limitations on access patterns to their data; i.e., when one must  provide values for certain attributes of a relation in order to retrieve its  tuples. In this paper we consider the following fundamental problem: can  we compute the complete answer to a query by accessing the relations  with legal patterns? The complete answer to a query is the answer that  we could compute if we could retrieve all the tuples from the relations.  We give algorithms for solving the problem for various classes of queries,  including conjunctive queries, unions of conjunctive queries, and conjunctive  queries with arithmetic comparisons. We prove the problem is  undecidable for datalog queries. If the complete answer to a query cannot  be computed, we often need to compute its maximal answer. The  second problem we study is, given two conjunctive queries on relations  with limited access patterns, how to test whether the maximal answer to...
395|Answering Queries Using Limited External Query Processors|When answering queries using external information sources, their contents can be described by views. To answer a query, we must rewrite it using the set of views presented by the sources. When the external information sources also have the ability to answer some (perhaps limited) sets of queries that require performing operations on their data, the set of views presented by the source may be infinite (albeit encoded in some finite fashion). Previous work on answering queries using views has only considered the case where the set of views is finite. In order to exploit the ability of information sources to answer more complex queries, we consider the problem of answering conjunctive queries using infinite sets of conjunctive views. Our first result is that an infinite set of conjunctive views can be partitioned into a finite number of equivalence classes, such that picking one view from every nonempty class is sufficient to determine whether the query can be answered using the views. Se...
396|On the Content of Materialized Aggregate Views|We consider the problem of answering queries using only materialized views. We first show that if the views subsume the query from the point of view of the information content, then the query can be answered using only the views, but the resulting query might be extremely inefficient. We then focus on aggregate views and queries over a single relation, which are fundamental in many applications such as data warehousing. We show that in this case, it is possible to guarantee that as soon as the views subsume the query, it can be completely rewritten in terms of the views in a simple query language. Our main contribution is the conception of various rewriting algorithms which run in polynomial time, and the proof of their completeness which relies on combinatorial arguments. Finally, we discuss the choice of materializing or not ratio views such as average and percentage, important for the design of materialized views. We show that it has an impact on the information content, which can b...
397|Lossless Regular Views|If the only information we have on a certain database is through a set of views, the question arises of whether this is sucient to answer completely a given query. We say that the set of views is lossless with respect to the query, if, no matter what the database is, we can answer the query by solely relying on the content of the views. The question of losslessness has various applications, for example in query optimization, mobile computing, data warehousing, and data integration. We study this problem in a context where the database is semistructured, and both the query and the views are expressed as regular path queries. The form of recursion present in this class prevents us from applying known results to our case.
398|View-based query answering and query containment over semistructured data|Abstract. The basic querying mechanism over semistructured data, namely regular path queries, asks for all pairs of objects that are connected by a path conforming to a regular expression. We consider conjunctive two-way regular path queries (C2RPQc’s), which extend regular path queries with two features. First, they add the inverse operator, which allows for expressing navigations in the database that traverse the edges both backward and forward. Second, they allow for using conjunctions of atoms, where each atom specifies that a regular path query with inverse holds between two terms, where each term is either a variable or a constant. For such queries we address the problem of view-based query answering, which amounts to computing the result of a query only on the basis of a set of views. More specifically, we present the following results: (1) We exhibit a mutual reduction between query containment and the recognition problem for view-based query answering for C2RPQc’s, i.e., checking whether a given tuple is in the certain answer to a query. Based on such a result, we can show that the problem of view-based query answering for C2RPQc’s is EXPSPACE-complete. (2) By exploiting techniques based on alternating two-way automata we show that for the restricted class of tree two-way regular path queries (in which the links between variables form a tree), query containment and view-based query answering are, rather surprisingly, in PSPACE (and hence, PSPACE-complete). (3) We present a technique to obtain view-based query answering algorithms that compute the whole set of tuples in the certain answer, instead of requiring to check each tuple separately. The technique is parametric wrt the query language, and can be applied both to C2RPQc’s and to tree-queries. 1
399|Models for Information Integration: Turning Local-as-View Into Global-as-View|There are basically two approaches for designing a data integration system. In the global-as-view approach, one defines the concepts in the global schema as views over the sources, whereas in the local-as-view approach, one characterizes the sources as views over the global schema. The goal of this paper is to verify whether we can transform a data integration system built with the local-as-view approach into a system following the global-as-view approach. We study the problem in a setting where the global schema is expressed in the relational model with inclusion dependencies, and the queries used in the integration systems (both the queries on the global schema, and the views in the mapping) are expressed in the language of conjunctive queries. The result we present is that such a transformation exists: we can always transform a local-as-view system into a global-as-view system such that, for each query, the set of answers to the query wrt the former is the same as the set of answers wrt the latter.
400|Query Rewriting using Semistructured Views|We address the problem of query rewriting for MSL, a semistructured language developed at Stanford in the TSIMMIS project for information integration. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting  queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing containment mappings, the chase, and unification -- techniques which were developed for structured, relational data. At the same time we develop an algorithm for equivalence checking of MSL queries. We show that the rewriting algorithm is sound and complete, i.e., it always finds every conjunctive MSL rewriting query of q, and we discuss its complexity. We currently incorporate the algorithm in the TSIMMIS system. 1 Introduction  Recently, many semistructured data models, query and view definition languages have been proposed [GM  +  97, MAG  +  97, BDHS96, AV97a, MM97, KS95, PGMU96,...
401|Intensional Query Answering by Partial Evaluation|. Intensional query answering aims at providing a response to a query addressed to a knowledge base by making use of the intensional knowledge as opposed to extensional. Such a response is an abstract description of the conventional answer that can be of interest in many situations, for example it may increase the cooperativeness of the system, or it may replace the conventional answer in case access to the extensional part of the knowledge base is costly as for Mobile Systems. In this paper we present a general framework to generate intensional answers in knowledge bases adhering to the logic programming paradigm. Such a framework is based on a program transformation technique, namely Partial Evaluation, and allows for generating complete and procedurally complete (wrt SLDNF-resolution) sets of intensional answers, treating both recursion and negation conveniently. Keywords: Knowledge bases, intensional query answering, logic programs, partial evaluation 1. Introduction Intensional an...
402|FastMap: A Fast Algorithm for Indexing, Data-Mining and Visualization of Traditional and Multimedia Datasets|A very promising idea for fast searching in traditional and multimedia databases is to map objects into points in k-d  space, using k feature-extraction functions, provided by a domain expert [25]. Thus, we can subsequently use highly fine-tuned spatial access methods (SAMs), to answer several types of queries, including the `Query By Example&#039; type (which translates to a range query); the `all pairs&#039; query (which translates to a spatial join [8]); the nearest-neighbor or best-match query, etc. However, designing feature extraction functions can be hard. It is relatively easier for a domain expert to assess the similarity/distance of two objects. Given only the distance information though, it is not obvious how to map objects into points. This is exactly the topic of this paper. We describe a fast algorithm to map objects into points in some k-dimensional  space (k is user-defined), such that the dis-similarities are preserved. There are two benefits from this mapping: (a) efficient ret...
403|The R + -tree: A dynamic index for multidimensional objects|The problem of indexing multidimensional objects is considered. First, a classification of existing methods is given along with a discussion of the major issues involved in multidimensional data indexing. Second, a variation to Guttman’s R-trees (R +-trees) that avoids overlapping rectangles in intermediate nodes of the tree is introduced. Algorithms for searching, updating, initial packing and reorganization of the structure are discussed in detail. Finally, we provide analytical results indicating that R +-trees achieve up to 50 % savings in disk accesses compared to an R-tree when searching files of thousands of rectangles. 1
404|The hB-tree: A multiattribute indexing method with good guaranteed performance|A new multiattribute index structure called the hB-tree is introduced. It is derived from the K-D-B-tree of Robinson [15] but has additional desirable properties. The hB-tree internode search and growth processes are precisely analogous to the corresponding processes in B-trees [l]. The intranode processes are unique. A k-d tree is used as the structure within nodes for very efficient searching. Node splitting requires that this k-d tree be split. This produces nodes which no longer represent brick-like regions in k-space, but that can be characterized as holey bricks, bricks in which subregions have been extracted. We present results that guarantee hB-tree users decent storage utilization, reasonable size index terms, and good search and insert performance. These results guarantee that the hB-tree copes well with arbitrary distributions of keys.
405|Latent Semantic Indexing (LSI) and TREC-2  (1994) |this paper. The &#034;ltc&#034; weights were computed on this matrix. 3.2 SVD analysis
406|QBISM: A Prototype 3-D Medical Image Database System|this paper. However, these automatic or semi-automatic warping algorithms are extremely important for this application. It is precisely this technology that permits anatomic structure-based access to acquired medical images as well as comparisons among studies, even of different patients, as long as they have been warped to the same atlas. Furthermore, it enables the database to grow, and be queryable, without time-consuming manual segmentation of the data.
407|Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications|A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.
408|A Scalable Content-Addressable Network|Hash tables – which map “keys ” onto “values” – are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internet-like scales. The CAN is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation. 
409|Resilient Overlay Networks|A Resilient Overlay Network (RON) is an architecture that allows distributed Internet applications to detect and recover from path outages and periods of degraded performance within several seconds, improving over today’s wide-area routing protocols that take at least several minutes to recover. A RON is an application-layer overlay on top of the existing Internet routing substrate. The RON nodes monitor the functioning and quality of the Internet paths among themselves, and use this information to decide whether to route packets directly over the Internet or by way of other RON nodes, optimizing application-specific routing metrics. Results from two sets of measurements of a working RON deployed at sites scattered across the Internet demonstrate the benefits of our architecture. For instance, over a 64-hour sampling period in March 2001 across a twelve-node RON, there were 32 significant outages, each lasting over thirty minutes, over the 132 measured paths. RON’s routing mechanism was able to detect, recover, and route around all of them, in less than twenty seconds on average, showing that its methods for fault detection and recovery work well at discovering alternate paths in the Internet. Furthermore, RON was able to improve the loss rate, latency, or throughput perceived by data transfers; for example, about 5 % of the transfers doubled their TCP throughput and 5 % of our transfers saw their loss probability reduced by 0.05. We found that forwarding packets via at most one intermediate RON node is sufficient to overcome faults and improve performance in most cases. These improvements, particularly in the area of fault detection and recovery, demonstrate the benefits of moving some of the control over routing into the hands of end-systems. 
410|Oceanstore: An architecture for global-scale persistent storage|OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development. 1
411|Freenet: A Distributed Anonymous Information Storage and Retrieval System|We describe Freenet, an adaptive peer-to-peer network application  that permits the publication, replication, and retrieval of data  while protecting the anonymity of both authors and readers. Freenet operates  as a network of identical nodes that collectively pool their storage  space to store data files and cooperate to route requests to the most  likely physical location of data. No broadcast search or centralized location  index is employed. Files are referred to in a location-independent  manner, and are dynamically replicated in locations near requestors and  deleted from locations where there is no interest. It is infeasible to discover  the true origin or destination of a file passing through the network,  and difficult for a node operator to determine or be held responsible for  the actual physical contents of her own node.
412|Wide-area cooperative storage with CFS|The Cooperative File System (CFS) is a new peer-to-peer readonly storage system that provides provable guarantees for the efficiency, robustness, and load-balance of file storage and retrieval. CFS does this with a completely decentralized architecture that can scale to large systems. CFS servers provide a distributed hash table (DHash) for block storage. CFS clients interpret DHash blocks as a file system. DHash distributes and caches blocks at a fine granularity to achieve load balance, uses replication for robustness, and decreases latency with server selection. DHash finds blocks using the Chord location protocol, which operates in time logarithmic in the number of servers. CFS is implemented using the SFS file system toolkit and runs on Linux, OpenBSD, and FreeBSD. Experience on a globally deployed prototype shows that CFS delivers data to clients as fast as FTP. Controlled tests show that CFS is scalable: with 4,096 servers, looking up a block of data involves contacting only seven servers. The tests also demonstrate nearly perfect robustness and unimpaired performance even when as many as half the servers fail.
413|A Scalable Location Service for Geographic Ad Hoc Routing |GLS is a new distributed location service which tracks mobile node locations. GLS combined with geographic forwarding allows the construction of ad hoc mobile networks that scale to a larger number of nodes than possible with previous work. GLS is decentralized and runs on the mobile nodes themselves, requiring no fixed infrastructure. Each mobile node periodically updates a small set of other nodes (its location servers) with its current location. A node sends its position updates to its location servers without knowing their actual identities, assisted by a predefined ordering of node identifiers and a predefined geographic hierarchy. Queries for a mobile node’s location also use the predefined identifier ordering and spatial hierarchy to find a location server for that node. Experiments using the ns simulator for up to 600 mobile nodes show that the storage and bandwidth requirements of GLS grow slowly with the size of the network. Furthermore, GLS tolerates node failures well: each failure has only a limited effect and query performance degrades gracefully as nodes fail and restart. The query performance of GLS is also relatively insensitive to node speeds. Simple geographic forwarding combined with GLS compares favorably with Dynamic Source Routing (DSR): in larger networks (over 200 nodes) our approach delivers more packets, but consumes fewer network resources.
414|Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the World Wide Web|We describe a family of caching protocols for distrib-uted networks that can be used to decrease or eliminate the occurrence of hot spots in the network. Our protocols are particularly designed for use with very large networks such as the Internet, where delays caused by hot spots can be severe, and where it is not feasible for every server to have complete information about the current state of the entire network. The protocols are easy to implement using existing network protocols such as TCP/IP, and require very little overhead. The protocols work with local control, make efficient use of existing resources, and scale gracefully as the network grows. Our caching protocols are based on a special kind of hashing that we call consistent hashing. Roughly speaking, a consistent hash function is one which changes minimally as the range of the function changes. Through the development of good consistent hash functions, we are able to develop caching protocols which do not require users to have a current or even consistent view of the network. We believe that consistent hash functions may eventually prove to be useful in other applications such as distributed name servers and/or quorum systems.  
415|The design and implementation of an intentional naming system|This paper presents the design and implementation of the Intentional Naming System (INS), a resource discovery and service location system for dynamic and mobile networks of devices and computers. Such environments require a naming system that is (i) expressive, to describe and make requests based on specific properties of services, (ii) responsive, to track changes due to mobility and performance, (iii) robust, to handle failures, and (iv) easily configurable. INS uses a simple language based on attributes and values for its names. Applications use the language to describe what they are looking for (i.e., their intent), not where to find things (i.e., not hostnames). INS implements a late binding mechanism that integrates name resolution and message routing, enabling clients to continue communicating with end-nodes even if the name-to-address mappings change while a session is in progress. INS resolvers self-configure to form an application-level overlay network, which they use to discover new services, perform late binding, and maintain weak consistency of names using soft-state name exchanges and updates. We analyze the performance of the INS algorithms and protocols, present measurements of a Java-based implementation, and describe three applications we have implemented that demonstrate the feasibility and utility of INS.
416|The synchronization of periodic routing messages|Abstract — The paper considers a network with many apparently-independent periodic processes and discusses one method by which these processes can inadvertent Iy become synchronized. In particular, we study the synchronization of periodic routing messages, and offer guidelines on how to avoid inadvertent synchronization. Using simulations and analysis, we study the process of synchronization and show that the transition from unsynchronized to synchronized traffic is not one of gradual degradation but is instead a very abrupt ‘phase transition’: in general, the addition of a single router will convert a completely unsynchronized traffic stream into a completely synchronized one. We show that synchronization can be avoided by the addition of randomization to the tra~c sources and quantify how much randomization is necessary. In addition, we argue that the inadvertent synchronization of periodic processes is likely to become an increasing problem in computer networks.
417|Development of the Domain Name System|(Originally published in the Proceedings of SIGCOMM ‘88,
418|Separating key management from file system security|No secure network file system has ever grown to span the In-ternet. Existing systems all lack adequate key management for security at a global scale. Given the diversity of the In-ternet, any particular mechanism a file system employs to manage keys will fail to support many types of use. We propose separating key management from file system security, letting the world share a single global file system no matter how individuals manage keys. We present SFS, a se-cure file system that avoids internal key management. While other file systems need key management to map file names to encryption keys, SFS file names effectively contain public keys, making them self-certifying pathnames. Key manage-ment in SFS occurs outside of the file system, in whatever procedure users choose to generate file names. Self-certifying pathnames free SFS clients from any notion of administrative realm, making inter-realm file sharing triv-ial. They let users authenticate servers through a number of different techniques. The file namespace doubles as a key certification namespace, so that people can realize many key management schemes using only standard file utilities. Fi-nally, with self-certifying pathnames, people can bootstrap one key management mechanism using another. These prop-erties make SFS more versatile than any file system with built-in key management.  
419|Building peer-to-peer systems with chord, a distributed lookup service|We argue that the core problem facing peer-to-peer systems is locating documents in a decentralized network and propose Chord, a distributed lookup primitive. Chord provides an efficient method of locating documents while placing few constraints on the applications that use it. As proof that Chord’s functionality is useful in the development of peer-to-peer applications, we outline the implementation of a peer-to-peer file sharing system based on Chord. 1
420|A Prototype Implementation of Archival Intermemory|An Archival Intermemory solves the problem of highly survivable digital data storage in the spirit of the Internet. In this paper we describe a prototype implementation of Intermemory, including an overall system architecture and implementations of key system components. The result is a working Intermemory that tolerates up to 17 simultaneous node failures, and includes a Web gateway for browser-based access to data. Our work demonstrates the basic feasibility of Intermemory and represents significant progress towards a deployable system.
422|Algorithmic Design of the Globe Wide-Area Location Service|this paper, we use the term mobile object to collectively refer to any component - implemented in hardware, software, or a combination thereof- that is capable of changing locations. We assume that a mobile object can be distributed or replicated across multiple locations, meaning that there may be several locations where the object resides at the same time. This can be the case, for example, with a whiteboard application shared between a number of mobile users. The existence of (worldwide) mobile objects introduces a location problem: The need for a scalable facility that maintains a binding (i.e., a mapping) between an object&#039;s permanent name and its current address(es). Such facilities are normally offered by wide-area naming systems such as the Internet&#039;s Domain Name System (DNS) [9], DEC&#039;s Global Name Service (GNS) [10], and the X.500 Directory Ser- vice [11]
423|Chord: A Scalable Peer-to-Peer Lookup Protocol for Internet Applications|A fundamental problem that confronts peer-to-peer applications is the efficient location of the node that stores a desired data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis and simulations show that Chord is scalable: communication cost and the state maintained by each node scale logarithmically with the number of Chord nodes.
424|Pastry: Scalable, distributed object location and routing for large-scale peer-to-peer systems|This paper presents the design and evaluation of Pastry, a scalable, distributed object location and routing scheme for wide-area peer-to-peer applications. Pastry provides application-level routing and object location in a potentially very large overlay network of nodes connected via the Internet. It can be used to support a wide range of peer-to-peer applications like global data storage, global data sharing, and naming. An insert operation in Pastry stores an object at a user-defined number of diverse nodes within the Pastry network. A lookup operation reliably retrieves a copy of the requested object if one exists. Moreover, a lookup is usually routed to the node nearest the client issuing the lookup (by some measure of proximity), among the nodes storing the requested object. Pastry is completely decentralized, scalable, and self-configuring; it automatically adapts to the arrival, departure and failure of nodes. Experimental results obtained with a prototype implementation on a simulated network of 100,000 nodes confirm Pastry&#039;s scalability, its ability to self-configure and adapt to node failures, and its good network locality properties.
425|Tapestry: An infrastructure for fault-tolerant wide-area location and routing|In today’s chaotic network, data and services are mobile and replicated widely for availability, durability, and locality. Components within this infrastructure interact in rich and complex ways, greatly stressing traditional approaches to name service and routing. This paper explores an alternative to traditional approaches called Tapestry. Tapestry is an overlay location and routing infrastructure that provides location-independent routing of messages directly to the closest copy of an object or service using only point-to-point links and without centralized resources. The routing and directory information within this infrastructure is purely soft state and easily repaired. Tapestry is self-administering, faulttolerant, and resilient under load. This paper presents the architecture and algorithms of Tapestry and explores their advantages through a number of experiments. 1
426|Analysis of the Evolution of Peer-to-Peer Systems|In this paper, we give a theoretical analysis of peer-to-peer (P2P) networks operating in the face of concurrent joins and unexpected departures. We focus on Chord, a recently developed P2P system that implements a distributed hash table abstraction, and study the process by which Chord maintains its distributed state as nodes join and leave the system. We argue that traditional performance measures based on run-time are uninformative for a continually running P2P network, and that the rate at which nodes in the network need to participate to maintain system state is a more useful metric. We give a general lower bound on this rate for a network to remain connected, and prove that an appropriately modified version of Chord&#039;s maintenance rate is within a logarithmic factor of the optimum rate. 1. 
427|Towards Global Network Positioning|We propose a new approach to predict Internet network distance called Global Network Positioning (GNP). This approach models the Internet as a geometric space and distributedly computes geometric coordinates to characterize the positions of hosts in the Internet. By conducting Internet experiments, we show that the geometric distances implied by the GNP host coordinates can accurately predict the Internet network distances.  I. 
428|Observations on the Dynamic Evolution of Peer-to-Peer Networks|A fundamental theoretical challenge in peer-to-peer systems is proving statements about the evolution of the system while nodes are continuously joining and leaving. Because the system will operate for an infinite time, performance measures based on runtime are uninformative; instead, we must study the rate at which nodes consume resources to maintain the system state.
429|ConChord: Cooperative SDSI Certificate Storage and Name Resolution|We present ConChord, a large-scale certificate distribution system built on a peer-to-peer distributed hash table. ConChord provides load-balanced storage while eliminating many of the administrative difficulties of traditional, hierarchical server architectures. ConChord is specifically designed to support SDSI, a fully-decentralized public key infrastructure that allows principals to define local names and link their namespaces to delegate trust. We discuss the particular challenges ConChord must address to support SDSI efficiently, and we present novel algorithms and distributed data structures to address them. Experiments show that our techniques are effective and practical for large SDSI name hierarchies.
430|Access path selection in a relational database management system|ABSTRACT: In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research&#039;Laboratory. 1.
431|A Performance Evaluation of Four Parallel Join Algorithms in a Shared-Nothing Multiprocessor Environment|The join operator has been a cornerstone of relational database systems since their inception. As such, much time and effort has gone into making joins efficient. With the obvious trend towards multiprocessors, attention has focused on efficiently parallelizing the join operation. In this paper we analyze and compare four parallel join algorithms. Grace and Hybrid hash represent the class of hash-based join methods, Simple hash represents a looping algorithm with hashing, and our last algorithm is the more traditional sort-merge. The Gamma database machine serves as the host for the performance comparison. Gamma’s shared-nothing architecture with commercially available components is becoming increasingly common, both in research and in industry. 1.
432|Data Placement in Bubba|Thus paper examines the problem of data placement in Bubba, a highly-parallel system for data-intensive applications bemg developed at MCC “Highly-parallel” lmplres that load balancmng IS a cntlcal performance issue “Data-mtenave” means data IS so large that operatrons should be executed where the data resides As a result, data placement becomes a cntlcal performance issue In general, determmmg the optimal placement of data across processmg nodes for performance IS a difficult problem We describe our heuristic approach to solvmg the data placement problem w Bubba We then present expenmental results using a specific workload to provide msrght into the problem Several researchers have argued the benefits of deelustering (1 e, spreading each base relation over many nodes) We show that as declustermg IS increased. load balancing contmues to improve However, for transactions mvolvmg complex Joins, further declusterrng reduces throughput because of communications, startup and termmatron overhead We argue that data placement, especially declustermg, m a highly-parallel system must be considered early in the design, so that mechanrsms can be included for supportmg variable declustermg, for mmtmlzmg the most significant overheads associated with large-scale declustenng, and for gathering the required statistics.
433|The Design Of Xprs|This paper presents an overview of the techniques we are using to build a DBMS at Berkeley that will simultaneously provide high performance and high availability in transaction processing environments, in applications with complex ad-hoc queries and in applications with large objects such as images or CAD layouts. We plan to achieve these goals using a general purpose DBMS and operating system and a shared memory multiprocessor. The hardware and software tactics which we are using to accomplish these goals are described in this paper and include a novel &#034;fast path&#034; feature, a special purpose concurrency control scheme, a two-dimensional file system, exploitation of parallelism and a novel method to efficiently mirror disks. 
434|A benchmark of NonStop SQL release 2 demonstrating near-linear speedup and scaleup on large databases|its second release, NonStop SQL transparently and automatically implements parallelism within an SQL statement. This parallelism allows query execution speed to increase almost linearly as processors and discs are added to the system-- speedup. In addition, this parallelism can help jobs restricted to a fIxed &amp;quot;batch window&amp;quot;. When the job doubles in size, its elapsed processing time will not change ifproportionately more equipment is available to process the job-- scaleup. This paper describes the parallelism features of NonStop SQL and an audited benchmark that demonstrates these speedup and scaleup claims.
435|A Framework for Dynamic Graph Drawing|Drawing graphs is an important problem that combines flavors of computational geometry and graph theory. Applications can be found in a variety of areas including circuit layout, network management, software engineering, and graphics. The main contributions of this paper can be summarized as follows:  ffl We devise a model for dynamic graph algorithms, based on performing queries and updates on an implicit representation of the drawing, and we show its applications.  ffl We present several efficient dynamic drawing algorithms for trees, series-parallel digraphs, planar st-digraphs, and planar graphs. These algorithms adopt a variety of representations (e.g., straight-line, polyline, visibility), and update the drawing in a smooth way.  
436|Self-adjusting binary search trees|  The splay tree, a self-adjusting form of binary search tree, is developed and analyzed. The binary search tree is a data structure for representing tables and lists so that accessing, inserting, and deleting items is easy. On an n-node splay tree, all the standard search tree operations have an amortized time bound of O(log n) per operation, where by “amortized time ” is meant the time per operation averaged over a worst-case sequence of operations. Thus splay trees are as efficient as balanced trees when total running time is the measure of interest. In addition, for sufficiently long access sequences, splay trees are as efficient, to within a constant factor, as static optimum search trees. The efftciency of splay trees comes not from an explicit structural constraint, as with balanced trees, but from applying a simple restructuring heuristic, called splaying, whenever the tree is accessed. Extensions of splaying give simplified forms of two other data structures: lexicographic or multidimensional search trees and link/ cut trees.
437|A Data Structure for Dynamic Trees|A data structure is proposed to maintain a collection of vertex-disjoint trees under a sequence of two kinds of operations: a link operation that combines two trees into one by adding an edge, and a cut operation that divides one tree into two by deleting an edge. Each operation requires O(log n) time. Using this data structure, new fast algorithms are obtained for the following problems: (1) Computing nearest common ancestors. (2) Solving various network flow problems including finding maximum flows, blocking flows, and acyclic flows. (3) Computing certain kinds of constrained minimum spanning trees. (4) Implementing the network simplex algorithm for minimum-cost flows. The most significant application is (2); an O(mn log n)-time algorithm is obtained to find a maximum flow in a network of n vertices and m edges, beating by a factor of log n the fastest algorithm previously known for sparse graphs. 
438|Tidier drawing of trees|Abstract-Various algorithms have been proposed for producing tidy drawings of trees-drawings that are aesthetically pleasing and use minimum drawing space. We show that these algorithms contain some difficulties that lead to aesthetically unpleasing, wider than necessary drawings. We then present a new algorithm with comparable time and storage requirements that produces tidier drawings. Generalizations to forests and m-ary trees are discussed, as are some problems in discretization when alphanumeric output devices are used. Index Terns-Data structures, trees, tree structures.
439|Upward Planarity Testing|Acyclic digraphs, such as the covering digraphs of ordered sets, are usually drawn upward, i.e., with the edges monotonically increasing in the vertical direction. A digraph is upward planar if it admits an upward planar drawing. In this survey paper, we overview the literature on the problem of upward planarity testing. We present several characterizations of upward planarity and describe upward planarity testing algorithms for special classes of digraphs, such as embedded digraphs and single-source digraphs. We also sketch the proof of NP-completeness of upward planarity testing.
441|Dynamic Trees and Dynamic Point Location|This paper describes new methods for maintaining a point-location data structure for a dynamically-changing monotone subdivision S. The main approach is based on the maintenance of two interlaced spanning trees, one for S and one for the graphtheoretic planar dual of S. Queries are answered by using a centroid decomposition of the dual tree to drive searches in the primal tree. These trees are maintained via the link-cut trees structure of Sleator and Tarjan, leading to a scheme that achieves vertex insertion/deletion in O(log n) time, insertion/deletion of k-edge monotone chains in  O(log n + k) time, and answers queries in O(log  2  n) time, with O(n) space, where n  is the current size of subdivision S. The techniques described also allow for the dual operations expand and contract to be implemented in O(log n) time, leading to an improved method for spatial point-location in a 3-dimensional convex subdivision. In addition, the interlaced-tree approach is applied to on-line point-lo...
442|Planar embedding of planar graphs|Planar embedding with minimal area of graphs on an integer grid is an interesting problem in VLSI theory. Valiant [12] gave an algorithm to construct a planar embedding for trees in linear area, he also proved that there are planar graphs that require quadratic area. We fill in a spectrum between Valiant&#039;s results by showing that an Nnode planar graph has a planar-embedding with area O(NF), where F is a bound on the path length from any node to the exterior face. In particular, an outerplanar graph can be embedded without crossings in linear area. This bound is tight, up to constant factors. For any N and F, there exist graphs requiring Q (NF) area for planar embedding. Furthermore, those graphs need that much area even if o(N) crossing are allowed. Also, finding a minimal embedding area is shown to be NP-complete for forests and, hence, for more general types of graphs.
443|FULLY DYNAMIC POINT LOCATION IN A MONOTONE SUBDIVISION| In this paper a dynamic technique for locating a point in a monotone planar subdivision, whose current number of vertices is n, is presented. The (complete set of) update operations are insertion of a point on an edge and of a chain of edges between two vertices, and their reverse operations. The data structure uses space O(n). The query time is O(log n), the time for insertion/deletion of a point is O(log n), and the time for insertion/deletion of a chain with k edges is O(log n + k), all worst-case. The technique is conceptually a special case of the chain method of Lee and Preparata and uses the same query algorithm. The emergence of full dynamic capabilities is afforded by a subtle choice of the chain set (separators), which induces a total order on the set of regions of the planar subdivision.
444|Parallel transitive closure and point location in planar structures|Parallel algorithms for several graph and geometric problems are presented, including transitive closure and topological sorting in planar st-graphs, preprocessing planar subdivisions for point location queries, and construction of visibility representations and drawings of planar graphs. Most of these algorithms achieve optimal O(log n) running time using n = log n processors in the EREW PRAM model, n being the number of vertices. 
445|Upward Planar Drawing of Single Source Acyclic Digraphs|A upward plane drawing of a directed acyclic graph is a straight line drawing in the Euclidean plane such that all directed arcs point upwards. Thomassen [30] has given a non-algorithmic, graph-theoretic characterization of those directed graphs with a single source that admit an upward drawing. We present an efficient algorithm to test whether a given single-source acyclic digraph has a plane upward drawing and, if so, to find a representation of one such drawing. The algorithm decomposes the graph into biconnected and triconnected components, and defines conditions for merging the components into an upward drawing of the original graph. For the triconnected components we provide a linear algorithm to test whether a given plane representation admits an upward drawing with the same faces and outer face, which also gives a simpler (and algorithmic) proof of Thomassen&#039;s result. The entire testing algorithm (for general single source directed acyclic graphs) operates in O(n²) time and...
