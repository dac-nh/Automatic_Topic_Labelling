ID|Title|Summary
1|Contention-Based Performance Evaluation of Multidimensional Range Search in Peer-to-peer Networks ABSTRACT |Performance evaluation of peer-to-peer search techniques has been based on simple performance metrics, such as message hop counts and total network traffic, mostly disregarding their inherent concurrent nature, where contention may arise. This paper is concerned with the effect of contention in complex P2P network search, focusing on techniques for multidimensional range search. We evaluate peerto-peer networks derived from recently proposed works, introducing two novel metrics related to concurrency and contention, namely responsiveness and throughput. Our results highlight the impact of contention on these networks, and demonstrate that some studied networks do not scale in the presence of contention. Also, our results indicate that certain network properties believed to be desirable (e.g. uniform data distribution or peer accesses) may not be as critical as previously believed.
2|Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications|A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.
3|A Scalable Content-Addressable Network|Hash tables – which map “keys ” onto “values” – are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internet-like scales. The CAN is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation. 
4|Pastry: Scalable, distributed object location and routing for large-scale peer-to-peer systems|This paper presents the design and evaluation of Pastry, a scalable, distributed object location and routing scheme for wide-area peer-to-peer applications. Pastry provides application-level routing and object location in a potentially very large overlay network of nodes connected via the Internet. It can be used to support a wide range of peer-to-peer applications like global data storage, global data sharing, and naming. An insert operation in Pastry stores an object at a user-defined number of diverse nodes within the Pastry network. A lookup operation reliably retrieves a copy of the requested object if one exists. Moreover, a lookup is usually routed to the node nearest the client issuing the lookup (by some measure of proximity), among the nodes storing the requested object. Pastry is completely decentralized, scalable, and self-configuring; it automatically adapts to the arrival, departure and failure of nodes. Experimental results obtained with a prototype implementation on a simulated network of 100,000 nodes confirm Pastry&#039;s scalability, its ability to self-configure and adapt to node failures, and its good network locality properties.
5|Kademlia: A Peer-to-peer Information System Based on the XOR Metric|We describe a peer-to-peer system which has provable consistency and performance in a fault-prone environment. Our system routes queries and locates nodes using a novel XOR-based metric topology that simplifies the algorithm and facilitates our proof. The topology has the property that every message exchanged conveys or reinforces useful contact information. The system exploits this information to send parallel, asynchronous query messages that tolerate node failures without imposing timeout delays on users.
6|Querying the Internet with PIER|The database research community prides itself on scalable technologies. Yet database systems traditionally do not excel on one important scalability dimension: the degree of distribution. This limitation has hampered the impact of database technologies on massively distributed systems like the Internet. In this paper, we present the initial design of PIER, a massively distributed query engine based on overlay networks, which is intended to bring database query processing facilities to new, widely distributed environments. We motivate the need for massively distributed queries, and argue for a relaxation of certain traditional database research goals in the pursuit of scalability and widespread adoption. We present simulation results showing PIER gracefully running relational queries across thousands of machines, and show results from the same software base in actual deployment on a large experimental cluster.
7|Mercury: Supporting scalable multi-attribute range queries|This paper presents the design of Mercury, a scalable protocol for supporting multi-attribute rangebased searches. Mercury differs from previous range-based query systems in that it supports multiple attributes as well as performs explicit load balancing. Efficient routing and load balancing are implemented using novel light-weight sampling mechanisms for uniformly sampling random nodes in a highly dynamic overlay network. Our evaluation shows that Mercury is able to achieve its goals of logarithmic-hop routing and near-uniform load balancing. We also show that a publish-subscribe system based on the Mercury protocol can be used to construct a distributed object repository providing efficient and scalable object lookups and updates. By providing applications a range-based query language to express their subscriptions to object updates, Mercury considerably simplifies distributed state management. Our experience with the design and implementation of a simple distributed multiplayer game built on top of this object management framework shows that indicates that this indeed is a useful building block for distributed applications. Keywords: Range queries, Peer-to-peer systems, Distributed applications, Multiplayer games 1
8|Skip Graphs|Skip graphs are a novel distributed data structure, based on skip lists, that provide the full functionality of a balanced tree in a distributed system where resources are stored in separate nodes that may fail at any time. They are designed for use in searching peer-to-peer systems, and by providing the ability to perform queries based on key ordering, they improve on existing search tools that provide only hash table functionality. Unlike skip lists or other tree data structures, skip graphs are highly resilient, tolerating a large fraction of failed nodes without losing connectivity. In addition, constructing, inserting new nodes into, searching a skip graph, and detecting and repairing errors in the data structure introduced by node failures can be done using simple and straightforward algorithms. 1
9|Analysis of the Evolution of Peer-to-Peer Systems|In this paper, we give a theoretical analysis of peer-to-peer (P2P) networks operating in the face of concurrent joins and unexpected departures. We focus on Chord, a recently developed P2P system that implements a distributed hash table abstraction, and study the process by which Chord maintains its distributed state as nodes join and leave the system. We argue that traditional performance measures based on run-time are uninformative for a continually running P2P network, and that the rate at which nodes in the network need to participate to maintain system state is a more useful metric. We give a general lower bound on this rate for a network to remain connected, and prove that an appropriately modified version of Chord&#039;s maintenance rate is within a logarithmic factor of the optimum rate. 1. 
10|Hilbert R-tree: An Improved R-tree Using Fractals|We propose a new R-tree structure that outperforms all the older ones. The heart of the idea is to facilitate the deferred splitting approach in R-trees. This is done by proposing an ordering on the R-tree nodes. This ordering has to be &#039;good&#039;, in the sense that it should group &#039;similar &#039; data rectangles together, to minimize the area and perimeter of the resulting minimum bounding rectangles (MBRs). Following [19] we have chosen the so-called &#039;2D-c &#039; method, which sorts rectangles according to the Hilbert value of the center of the rectangles. Given the ordering, every node has a well-de ned set of sibling nodes; thus, we can use deferred splitting. By adjusting the split policy, the Hilbert R-tree can achieve as high utilization as desired. To the contrary, the R-tree has no control over the space utilization, typically achieving up to 70%. We designed the manipulation algorithms in detail, and we did a full implementation of the Hilbert R-tree. Our experiments show that the &#039;2-to-3 &#039; split policy provides a compromise between the insertion complexity and the search cost, giving up to 28 % savings over the R  tree [3] on real data. 1
11|Simple Efficient Load Balancing algorithms for Peer-to-Peer Systems|Load balancing is a critical issue for the efficient operation of peer-to-peer networks. We give two new load-balancing protocols whose provable performance guarantees are within a constant factor of optimal. Our protocols refine the consistent hashing data structure that underlies the Chord (and Koorde) P2P network. Both preserve Chord’s logarithmic query time and near-optimal data migration cost. Consistent hashing is an instance of the distributed hash table (DHT) paradigm for assigning items to nodes in a peer-to-peer system: items and nodes are mapped to a common address space, and nodes have to store all items residing closeby in the address space. Our first protocol balances the distribution of the key address space to nodes, which yields a load-balanced system when the DHT maps items “randomly” into the address space. To our knowledge, this yields the first P2P scheme simultaneously achieving O(log n) degree, O(log n) look-up cost, and constant-factor load balance (previous schemes settled for any two of the three). Our second protocol aims to directly balance the distribution of items among the nodes. This is useful when the distribution of items in the address space cannot be randomized. We give a simple protocol that balances load by moving nodes to arbitrary locations “where they are needed.” As an application, we use the last protocol to give an optimal implementation of a distributed data structure for range searches on ordered data.
12|P-Grid: A Self-organizing Structured P2P System|this paper was supported in part by  the National Competence Center in Research on Mobile  Information and Communication Systems (NCCR-MICS), a  center supported by the Swiss National Science Foundation  under grant number 5005-67322 and by SNSF grant 2100064994,  &#034;Peer-to-Peer Information Systems.&#034;  messages. From the responses it (randomly) selects certain peers to which direct network links are established
13|BATON: A Balanced Tree Structure for Peer-to-Peer Networks|We propose a balanced tree structure overlay on a peer-to-peer network capable of supporting both exact queries and range queries efficiently. In spite of the tree structure causing distinctions to be made between nodes at different levels in the tree, we show that the load at each node is approximately equal. In spite of the tree structure providing precisely one path between any pair of nodes, we show that sideways routing tables maintained at each node provide sufficient fault tolerance to permit efficient repair. Specifically, in a network with N nodes, we guarantee that both exact queries and range queries can be answered in O(logN) steps and also that update operations (to both data and network) have an amortized cost of O(logN). An experimental assessment validates the practicality of our proposal. 1
14|Modeling Peer-Peer File Sharing Systems|Peer-peer networking has recently emerged as a new paradigm for building distributed networked applications. In this paper we develop simple mathematical models to explore and illustrate fundamental performance issues of peer-peer file sharing systems. The modeling framework introduced and the corresponding solution method are flexible enough to accommodate different characteristics of such systems. Through the specification of model parameters, we apply our framework to three different peer-peer architectures: centralized indexing, distributed indexing with flooded queries, and distributed indexing with hashing directed queries. Using our model, we investigate the effects of system scaling, freeloaders, file popularity and availability on system performance. In particular, we observe that a system with distributed indexing and flooded queries cannot exploit the full capacity of peer-peer systems. We further show that peer-peer file sharing systems can tolerate a significant number of freeloaders without suffering much performance degradation. In many cases, freeloaders can benefit from the available spare capacity of peer-peer systems and increase overall system throughput. Our work shows that simple models coupled with efficient solution methods can be used to understand and answer questions related to the performance of peer-peer file sharing systems.
15|Approximate Range Selection Queries in Peer-to-Peer|We present an architecture for a data sharing  peer-to-peer system where the data is shared  in the form of database relations. In general,  peer-to-peer systems try to locate exactmatch  data objects to simple user queries.
16|One torus to rule them all: Multi-dimensional queries in p2p systems|Peer-to-peer systems enable access to data spread over an extremely large number of machines. Most P2P systems support only simple lookup queries. However, many new applications, such as P2P photo sharing and massively multiplayer games, would benefit greatly from support for multidimensional range queries. We show how such queries may be supported in a P2P system by adapting traditional spatialdatabase technologies with novel P2P routing networks and load-balancing algorithms. We show how to adapt two popular spatial-database solutions – kd-trees and space-filling curves – and experimentally compare their effectiveness. 1.
17|Querying peer-to-peer networks using p-trees|Peer-to-peer (P2P) systems provide a robust, scalable and decentralized way to share and publish data. However, most existing P2P systems only provide a very rudimentary query facility; they only support equality or keyword search queries over files. We believe that future P2P applications, such as resource discovery on a grid, will require more complex query functionality. As a first step towards this goal, we propose a new distributed, fault-tolerant P2P index structure for resource discovery applications called the P-tree. Ptrees efficiently evaluate range queries in addition to equality queries. We describe algorithms to maintain a P-tree under insertions and deletions of data items/peers, and evaluate its performance using both a simulation and a real distributed implementation. Our results show the efficacy of our approach. 1.
18|A casestudy in building layered DHT applications|Recent research has shown that one can use Distributed Hash Tables (DHTs) to build scalable, robust and efficient applications. One question that is often left unanswered is that of simplicity of implementation and deployment. In this paper, we explore a case study of building an application for which ease of deployment dominated the need for high performance. The application we focus on is Place Lab, an end-user positioning system. We evaluate whether it is feasible to use DHTs as an application-independent building block to implement a key component of Place Lab: its “mapping infrastructure.” We present Prefix Hash Trees, a data structure used by Place Lab for geographic range queries that is built entire on top of a standard DHT. By strictly layering Place Lab’s data structures on top of a generic DHT service, we were able to decouple the deployment and management of Place Lab from that of the underlying DHT. We identify the characteristics of Place Lab that made it amenable for deploying in this layered manner, and comment on its effect on performance.
19|Distributed segment tree: Support of range query and cover query over dht|Range query, which is defined as to find all the keys in a certain range over the underlying P2P network, has received a lot of research attentions recently. However, cover query, which is to find all the ranges currently in the system that cover a given key, is rarely touched. In this paper, we first identify that cover query is a highly desired functionality by some popular P2P applications, and then propose distributed segment tree (DST), a layered DHT structure that incorporates the concept of segment tree. Due to the intrinsic capability of segment tree in maintaining the sturcture of ranges, DST is shown to be very efficient for supporting both range query and cover query in a uniform way. It also possesses excellent parallelizability in query operations and can achieve O(1) complexity for moderate query ranges. To balance the load among DHT nodes, we design a downward load stripping mechanism that controls tradeoffs between load and performance. We implemented DST on publicly available OpenDHT service and performed extensive real experiments. All the results and comparisons demonstrate the effectiveness of DST for several important metrics. 1.
20|Vbi-tree: A peer-to-peer framework for supporting multi-dimensional indexing schemes|Multi-dimensional data indexing has received much attention in a centralized database. However, not so much work has been done on this topic in the context of Peerto-Peer systems. In this paper, we propose a new Peer-to-Peer framework based on a balanced tree structure overlay, which can support extensible centralized mapping methods and query processing based on a variety of multidimensional tree structures, including R-Tree, X-Tree, SS-Tree, and M-Tree. Specifically, in a network with N nodes, our framework guarantees that point queries and range queries can be answered within O(logN) hops. We also provide an effective load balancing strategy to allow nodes to balance their work load efficiently. An experimental assessment validates the practicality of our proposal. 1.
21|Range Queries in Trie-Structured Overlays|Among the open problems in P2P systems, support for non-trivial search predicates, standardized query languages, distributed query processing, query load balancing, and quality of query results have been identified as some of the most relevant issues. This paper describes how range queries as an important non-trivial search predicate can be supported in a structured overlay network that provides O(log n) search complexity on top of a trie abstraction. We provide analytical results that show that the proposed approach is efficient, supports arbitrary granularity of ranges, and demonstrate that its algorithmic complexity in terms of messages is independent of the size of the queried ranges and only depends on the size of the result set. In contrast to other systems which provide evaluation results only through simulations, we validate the theoretical analysis of the algorithms with large-scale experiments on the PlanetLab infrastructure using a fully-fledged implementation of our approach.  
22|On Scaling Latent Semantic Indexing for Large Peer-To-Peer Systems|The exponential growth of data demands scalable infrastructures capable of indexing and searching rich content such as text, music, and images. A promising direction is to combine information retrieval with peer-to-peer technology for scalability, fault-tolerance, and low administration cost. One pioneering work along this direction is pSearch [32, 33]. pSearch places documents onto a peerto -peer overlay network according to semantic vectors produced using Latent Semantic Indexing (LSI). The search cost for a query is reduced since documents related to the query are likely to be co-located on a small number of nodes. Unfortunately, because of its reliance on LSI, pSearch also inherits the limitations of LSI. (1) When the corpus is large and heterogeneous, LSI&#039;s retrieval quality is inferior to methods such as Okapi. (2) The Singular Value Decomposition (SVD) used in LSI is unscalable in terms of both memory consumption and computation time.
23|Analysis and Comparison of P2P Search Methods |The popularity and bandwidth consumption attributed to current Peer-to-Peer file-sharing applications makes  the operation of these distributed systems very important for the Internet community. Efficient object discovery is  the first step towards the realization of distributed resource-sharing. In this work, we present a detailed overview  of recent and existing search methods for unstructured Peer-to-Peer networks. We analyze the performance of the  algorithms relative to various metrics, giving emphasis on the success rate, bandwidth-efficiency and adaptation to  dynamic network conditions. Simulation results are used to empirically evaluate the behavior of nine representative  schemes under a variety of different environments.
24|Answering similarity queries in peer-to-peer networks. http://www.comp.nus.edu.sg/-kalnis/ASQ.pdf |www.comp.nus.edu.sg/~{kalnis, ngws, ooibc, tankl}
25|Skip-webs: Efficient distributed data structures for multi-dimensional data sets|large(at)daimi.au.dk eppstein(at)ics.uci.edu goodrich(at)acm.org We present a framework for designing efficient distributed data structures for multi-dimensional data. Our structures, which we call skip-webs, extend and improve previous randomized distributed data structures, including skipnets and skip graphs. Our framework applies to a general class of data querying scenarios, which include linear (one-dimensional) data, such as sorted sets, as well as multi-dimensional data, such as d-dimensional octrees and digital tries of character strings defined over a fixed alphabet. We show how to perform a query over such a set of n items spread among n hosts using O(log n/log log n) messages for one-dimensional data, or O(log n) messages for fixed-dimensional data, while using only O(log n) space per host. We also show how to make such structures dynamic so as to allow for insertions and deletions in O(log n) messages for quadtrees, octrees, and digital tries, and O(log n/log log n) messages for onedimensional data. Finally, we show how to apply a blocking strategy to skip-webs to further improve message complexity for one-dimensional data when hosts can store more data.
26|Real datasets for file-sharing peer-to-peer systems|Abstract. The fundamental drawback of unstructured peer-to-peer (P2P) networks is the flooding-based query processing protocol that seriously limits their scalability. As a result, a significant amount of research work has focused on designing efficient search protocols that reduce the overall communication cost. What is lacking, however, is the availability of real data, regarding the exact content of users ’ libraries and the queries that these users ask. Using trace-driven simulations will clearly generate more meaningful results and further illustrate the efficiency of a generic query processing protocol under a real-life scenario. Motivated by this fact, we developed a Gnutella-style probe and collected detailed data over a period of two months. They involve around 4,500 users and contain the exact files shared by each user, together with any available metadata (e.g., artist for songs) and information about the nodes (e.g., connection speed). We also collected the queries initiated by these users. After filtering, the data were organized in XML format and are available to researchers. Here, we analyze this dataset and present its statistical characteristics. Additionally, as a case study, we employ it to evaluate two recently proposed P2P searching techniques. 1
27|Multidimensional Access Methods|Search operations in databases require special support at the physical level. This is true for conventional databases as well as spatial databases, where typical search operations include the point query (find all objects that contain a given search point) and the region query (find all objects that overlap a given search region).
28|The ubiquitous B-tree|B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and nnplemented using B-trees This paper reviews B-trees and shows why they have been so successful It discusses the major variations of the B-tree, especially the B+-tree,
29|On visible surface generation by a priori tree structures|This paper describes a new algorithm for solving the hidden surface (or line) problem, to more rapidly generate realistic images of 3-D scenes composed of polygons, and presents the development of theoretical foundations in the area as well as additional related algorithms. As in many applications the environment to be displayed consists of polygons many of whose relative geometric relations are static, we attempt to capitalize on this by pre processing tile environment,s database so as to decrease the run-time computations required to generate a scene. This preprocessing is based on generating a &amp;quot;ninary space partitioning &amp;quot; tree whose inorder traversal of visibility priority at run-time will produce a lineaL &amp;quot; order, dependent upon the viewing position, on (parts of) the polygons, which can then be used to easily solve the hidden surfac6 problem. In the application where the entire environment is static with only the viewing-position changing, as is common in simulation, the results presented will be safficient to solve completely tlae llidden surface proulem.;_N~a~OUCZZON One of the long-term goals of computer graphics has been, and continues to be, the rapid, possibly real-time generation of £ealistic images of simulated 3-D environments. &amp;quot;Real-time,&amp;quot; in current practice, has come to mean creating an image in 1/30 of a second--fast enough to continually generate images on a video monitor. With this fast image generation, there is no aiscernable delay between specifying parameters zor an image (using knobs, switches, or cockpit controls) and the *This research was partially supported by NSF under Grants MCS79-00168 and MC579-02593, and was zacilitated by the use of
30|Efficient Processing of Spatial Joins Using R-Trees|Abstract: In this paper, we show that spatial joins are very suitable to be processed on a parallel hardware platform. The parallel system is equipped with a so-called shared virtual memory which is well-suited for the design and implementation of parallel spatial join algorithms. We start with an algorithm that consists of three phases: task creation, task assignment and parallel task execu-tion. In order to reduce CPU- and I/O-cost, the three phases are processed in a fashion that pre-serves spatial locality. Dynamic load balancing is achieved by splitting tasks into smaller ones and reassigning some of the smaller tasks to idle processors. In an experimental performance compar-ison, we identify the advantages and disadvantages of several variants of our algorithm. The most efficient one shows an almost optimal speed-up under the assumption that the number of disks is sufficiently large. Topics: spatial database systems, parallel database systems 1
32|Generalized Search Trees for Database Systems|This paper introduces the Generalized Search Tree (GiST), an index structure supporting an extensible set of queries and data types. The GiST allows new data types to be indexed in a manner supporting queries natural to the types; this is in contrast to previous work on tree extensibility which only supported the traditional set of equality and range predicates. In a single data structure, the GiST provides all the basic search tree logic required by a database system, thereby unifying disparate structures such as B+-trees and R-trees in a single piece of code, and opening the application of search trees to general extensibility. To illustrate the exibility of the GiST, we provide simple method implementations that allow it to behave like a B+-tree, an R-tree, and an RD-tree, a new index for data with set-valued attributes. We also present a preliminary performance analysis of RD-trees, which leads to discussion on the nature of tree indices and how they behave for various datasets.  
33|Spatial SQL: A Query and Presentation Language|attention has been focused on spatial databases which combine conventional and spatially related data such as Geographic Information Systems, CAD/CAM, or VLSI. A language has been developed to query such spatial databases. It recognizes the significantly different requirements of spatial data handling and overcomes the inherent problems of the application of conventional database query languages. The spatial query language has been designed as a minimal extension to the interrogative part of SQL and distinguishes from previously designed SQL extensions by (1) the preservation of SQL concepts, (2) the highlevel treatment of spatial objects, and (3) the incorporation of spatial operations and relationships. It consists of two components, a query language to describe what information to retrieve and a presentation language to specify how to display query results. Users can ask standard SQL queries to retrieve non-spatial data based on non-spatial constraints, use Spatial SQL commands to inquire about situations involving spatial data, and give instructions in the Graphical Presentation Language GPL to manipulate or examine the graphical presentation. 1 Index Terms—Geographic Information Systems, graphical presentation, query
34|Beyond uniformity and independence: Analysis of r-trees using the concept of fractal dimension|We propose the concept of fractal dimension of a set of points, in order to quantify the deviation from the uniformity distribution. Using measurements on real data sets (road intersections of U.S. counties, star coordinates from NASA’s Infrared-Ultraviolet Explorer etc.) we provide evidence that real data indeed are skewed, and, moreover, we show that they behave as mathematical fractals, with a measurable, non-integer fract al dimension. Armed with this tool, we then show its practical use in predicting the performance of spatial access methods, and specifically of the R-trees. We provide the jirst analysis of R-trees for skewed distributions of points: We develop a formula that estimates the number of disk accesses for range queries, given only the fractal dimension of the point set, and its count. Experiments on real data sets show that the formula is very accurate: the relative error is usually below 5%, and it rarely exceeds 10%. We believe that the fractal dimension will help replace the uniformity and independence assumptions, allowing more accurate analysis for any spatial access method, as well as better estimates for query optimization on multi-attribute queries. 1
35|Fractals for Secondary Key Retrieval |In this paper we propose the use of fractals and especially the Hilbert curve, in order to design good distance-preserving mappings. Such mappings improve the performance of secondary-key- and spatial- access methods, where multi-dimensional points have to be stored on an 1-dimensional medium (e.g., disk). Good clustering reduces the number of disk accesses on retrieval, improving the response time. Our experiments on range queries and nearest neighbor queries showed that the proposed Hilbert curve achieves better clustering than older methods (&amp;quot;bit-shuffling&amp;quot;, or Peano curve), for every situation we tried.
36|Multi-Step Processing of Spatial Joins   |Spatial joins are one of the most importaot operations for combining spatial objects of several relations. IO this paper, spatial join processing is studied in detail for extended spatial objects in two-dimensional data space. We present an approach for spatial join processing that is based on three steps. First, a spatial join is performed on the minimum bounding rectangles of the objects returning a set of candidates. Various approaches for accelerating this step of join processing have been examined at the last year’s conference [BKS 93a]. In this paper, we focus on the problem how to compute the answers from the set of candidates which is handled by the foliowing two steps. First of all, sophisticated approximations are used to identify answers as well as to filter out false hits from the set of candidates. For this purpose, we investigate various types of conservative and progressive approximations. In the last step, the exact geometry of the remaioing candidates has to be tested against the join predicate. The time required for computing spatial joio predicates can essentially be reduced when objects are adequately organized in main memory. IO our approach, objects are fiist decomposed into simple components which are exclusively organized by a main-memory resident spatial data structure. Overall, we present a complete approach of spatial join processing on complex spatial objects. The performance of the individual steps of our approach is evaluated with data sets from real cartographic applications. The results show that our approach reduces the total execution time of the spatial join by factors.  
37|Estimating the Selectivity of Spatial Queries Using the `Correlation&#039; Fractal Dimension|We examine the estimation of selectivities for range and spatial join queries in real spatial databases. As we have shown earlier [FK94a], real point sets: (a) violate consistently the &#034;uniformity&#034; and &#034;independence&#034; assumptions, (b) can often be described as &#034;fractals&#034;, with non-integer (fractal) dimension. In this paper we show that, among the infinite family of fractal dimensions, the so called &#034;Correlation Dimension&#034; D 2 is the one that we need to predict the selectivity of spatial join. The main contribution is that, for all the real and synthetic point-sets we tried, the average number of neighbors for a given point of the point-set follows a power law, with D 2 as the exponent. This immediately solves the selectivity estimation for spatial joins, as well as for &#034;biased&#034; range queries (i.e., queries whose centers prefer areas of high point density). We present the formulas to estimate the selectivity for the biased queries, including an integration constant (K `shape  0  ) for ea...
38|Indexing for data models with constraints and classes|We examine I O-efficient data structures that provide indexing support for new data models. The database languages of these models include concepts from constraint programming (e.g., relational tuples are generated to conjunctions of constraints) and from object-oriented programming (e.g., objects are organized in class hierarchies). Let n be the size of the database, c the number of classes, B the page size on secondary storage, and t the size of the output of a query: (1) Indexing by one attribute in many constraint data models is equivalent to external dynamic interval management, which is a special case of external dynamic two-dimensional range searching. We present a semi-dynamic data structure for this problem that has worst-case space O(n B) pages, query I O time O(logB n+t B) and O(logB n+(logB n) 2 B) amortized insert I O time. Note that, for the static version of this problem, this is the first worst-case optimal solution. (2) Indexing by one attribute and by class name in an object-oriented model, where objects are organized
39|The lsd tree: spatial access to multidimensional point and non-point objects|We propose the Local Split Decision tree (LSD tree, for short), a data structure supporting efficient spatial access to geometric objects. Its main advantages over other structures are that it performs well for all reasonable data distributions, cover quotients (which measure the overlapping of the data objects), and bucket capacities, and that it maintains multidimensional points as well as arbitrary geometric objects. These properties demonstrated by an extensive performance, evaluation make the LSD tree extremely suitable for the implementation of spatial access paths in geometric databases. The paging algorithm for the binary tree directory is interesting in its own right because a practical solution for the problem of how to page a (multidimensional) binary tree without access path degeneration is presented. 1.
40|Efficient Computation of Spatial Joins|Spatial joins are join operations that involve spatial data types and operators. Due to some basic properties of spatial data, many conventional join processing strategies suffer serious performance penalties or are not applicable at all in this case. In this paper we explore which of the join strategies known from conventional databases can be applied to spatial joins as well, and how some of these techniques can be modified to be more efficient in the context of spatial data. Furthermore, we describe a class of tree structures, called generalization trees, that can be applied efficiently to compute spatial joins in a hierarchical manner. Finally, we model the performance of the most promising strategies analytically and conduct a comparative study.  Parts of this work have been carried out while the author was visiting the International Computer Science Institute and the University of California at Berkeley.  1  1 Introduction  Spatial databases have become a very active research top...
41|Parallel R-trees|We consider the problem of exploiting parallelism to accelerate the performance of spatial access methods and specifically, R-trees [11]. Our goal is to design a server for spatial data, so that to maximize the throughput of range queries. This can be achieved by (a) maximizing parallelism for large range queries, and (b) by engaging as few disks as possible on point queries [22]. We propose a simple hardware architecture consisting of one processor with several disks attached to it. On this architecture, we propose to distribute the nodes of a traditional R-tree, with cross-disk pointers (`Multiplexed&#039; R-tree). The R-tree code is identical to the one for a single-disk R-tree, with the only addition that we have to decide which disk a newly created R-tree node should be stored in. We propose and examine several criteria to choose a disk for a new node. The most successful one, termed `proximity index&#039; or PI, estimates the similarity of the new node with the other R-tree nodes already o...
42|Comparison of approximations of complex objects used for approximation-based query processing in spatial database systems|The management of geometric objects is a prime example of an application where efficiency is the bottleneck; this bottleneck cannot be eliminated without using suitable access structures. The most popular approach for handling complex spatial objects in spatial access methods is to use their minimum bounding boxes as a geometric key. Obviously, the rough approximation by bounding boxes provides a fast but inaccurate filter for the set of answers to a query. In order to speed up the query processing by a better approximation quality, we investigate six different types of approximations. Depending on the complexity of the objects and the type of queries, the approximations 5-corner, ellipse and rotated bounding box clearly outperform the bounding box. An important ingredient of our approach is to organize these approximations in efficient spatial access
43|A Qualitative Comparison Study of Data Structures for Large Line Segment Databases|A qualitative comparative study is performed of the performance of three popular spatial indexing methods -- the r    -tree, r  +  -tree, and the pmr quadtree -- in the context of processing spatial queries in large line segment databases. The data is drawn from the tiger/Line files used by the Bureau of the Census to deal with the road networks in the US. The goal is not to find the best data structure as this is not generally possible. Instead, their comparability is demonstrated and an indication is given as to when and why their performance differs. Tests are conducted with a number of large datasets and performance is tabulated in terms of the complexity of the disk activity in building them, their storage requirements, and the complexity of the disk activity for a number of tasks that include point and window queries, as well as finding the nearest line segment to a given point and an enclosing polygon.  1 Introduction  Spatial data consists of points, lines, regions, rectangles,...
44|DOT: A spatial access method using fractals|Existing Database Management Systems (DBMSs) do not handle efficiently multi-dimensional data such as boxes, polygons, or even points in a multi-dimensional space. We examine access methods for these data with two design goals in mind: (a) efficiency in terms of search speed and space overhead and (b) ability to be integrated in a DBMS easily. We propose a method to map multidimensional objects into points in a 1-dimensional space; thus, traditional primary-key access methods can be applied, with very few extensions on the part of the DBMS. We propose such mappings based on fractals; we implemented the whole method on top of a B +-tree, along with several mappings. Simulation experiments on several distributions of the input data show
45|Optimal Redundancy in Spatial Database Systems|In spatial database systems rectangles are commonly used to approximate real spatial data. A technique that approximates extended objects with a collection  of rectangles is the z-ordering method. Since each of these rectangles eventually corresponds to an entry in a spatial index, the object may be referenced several times. This redundancy effect is controllable. In this paper, we present an empirically derived formula to assess the expected redundancy for the z-ordering approximation technique given some simple parameters. After showing the applicability of this formula to a large class of different object geometries, we make use of this result to determine the optimal redundancy for real spatial data by means of theoretical considerations. In order to verify our theoretical results, we conducted several experiments using real spatial data and found a good correspondence. 1 Introduction  Spatial indexing or spatial access methods have found great attention in recent years, culminatin...
46|An analysis of geometric modeling in database systems|The data-modeling and computational requirements for integrated computer aided manufacturing (CAM) databases are analyzed, and the most common representation schemes for modeling solid geometric objects in a computer are
47|Realms: A Foundation for Spatial Data Types in Database Systems|Spatial data types or algebras for database systems should (i) be fully general (which means, closed under set operations, hence e.g. a region value can be a set of polygons with holes), (ii) have formally defined semantics, (iii) be defined in terms of finite representations available in computers, (iv) offer facilities to enforce geometric consistency of related spatial objects, and (v) be independent of a particular DBMS data model, but cooperate with any. We offer such a definition in two papers. The central idea, introduced in this (first) paper, is to use realms as geometric domains underlying spatial data types. A realm as a general database concept is a finite, dynamic, user-defined structure underlying one or more system data types. A geometric realm defined here is a planar graph over a finite resolution grid. Problems of numerical robustness and topological correctness are solved below and within the realm layer so that spatial algebras defined above a realm enjoy very nice algebraic properties. Realms also interact with a DBMS to enforce geometric consistency on object creation or update.
48|Benchmarking Spatial Joins A La Carte|Spatial joins are join operations that involve spatial data types and operators. Spatial access methods are often used to speed up the computation of spatial joins. This paper addresses the issue of benchmarking spatial join operations. For this purpose, we first present a WWW-based benchmark generator to produce sets of rectangles. Using a Web browser, experimenters can specify the number of rectangles in a sample, as well as the statistical distributions of their sizes, shapes, and locations. Second, using the generator and a well-defined set of statistical models we define several tests to compare the performance of three spatial join algorithms: nested loop, scan-and-index, and synchronized tree traversal. We also added a real-life data set from the Sequoia 2000 storage benchmark. Our results show that the relative performance of the different techniques mainly depends on two parameters: sample size, and selectivity of the join predicate. All of the statistical models and algorithms are available on the Web, which allows for easy verification and modification of our experiments.
49|Analysis of n-dimensional Quadtrees Using the Hausdorff Fractal Dimension|There is mounting evidence [Man77, Sch91] that real datasets are statistically self-similar, and thus, `fractal&#039;. This is an important insight since it permits a compact statistical description of spatial datasets; subsequently, as we show, it also forms the basis for the theoretical analysis of spatial access methods, without using the typical, but unrealistic, uniformity assumption. In this paper, we focus on the estimation of the number of quadtree blocks that a real, spatial dataset will require. Using the the well-known Hausdorff fractal dimension, we derive some closed formulas which allow us to predict the number of quadtree blocks, given some few parameters. Using our formulas, it is possible to predict the space overhead and the response time of linear quadtrees/z-ordering [OM88], which are widely used in practice. In order to verify our analytical model, we performed an extensive experimental investigation using several real datasets coming from different domains. In these ex...
50|Enclosing Many Boxes By an Optimal Pair of Boxes|We look at the problem: Given a set  M  of  n d-dimensional  intervals, find two  d-  dimensional intervals  S, T  , such that all intervals in  M  are enclosed by  S  or by  T  , the  distribution is balanced and the intervals  S  and  T  fulfill a geometric criterion, e.g. like minimum  area sum. Up to now no polynomial time algorithm was known for that problem. We  present an  O(dn  log  n  +  d 2 n 2d\Gamma1 ) algorithm for finding an optimal solution.  1 Introduction  Throughout the years, several fast heuristics have been proposed for a combinatorial optimization  problem that is important in the area of spatial data structures. Given a set of (axis-parallel)  rectangles in the plane, the problem is to find two rectangles, say S and T , such that each given  rectangle is enclosed by S or by T (or both), each of S and T enclose at least a certain number  of given rectangles, and S and T together minimize some measure, e.g. the sum of their areas.  This problem must be solved whene...
51|Separability of Polyhedra for Optimal Filtering of Spatial and Constraint Data|The filtering method considered in this paper is based on approximation of a  spatial object in d-dimensional space by the minimal convex polyhedron that encloses  the object and whose facets are normal to preselected axes. These axes are  not necessarily the standard coordinate axes and, furthermore, their number is not  determined by the dimension of the space. We optimize filtering by selecting optimal  such axes based on a pre-processing analysis of stored objects or a sample thereof.  The number of axes selected represents a trade-off between access time and storage  overhead, as more axes usually lead to better filtering but require more overhead to  store the associated access structures. We address the problem of minimizing the  number of axes required to achieve a predefined quality of filtering and the reverse  problem of optimizing the quality of filtering when the number of axes is fixed. In  both cases we also show how to find an optimal collection of axes. In order to sol...
52|Spatial Access Methods and Query Processing in the Object-Oriented GIS GODOT|In this paper, we describe the spatial access method z-ordering and its application in the context of the research project GODOT, which is based on the commercial object-oriented database system ObjectStore [LLOW91]. After identifying a range of spatial predicates, we show that the intersection join is of crucial importance for spatial joins. Next, we propose an efficient method for query processing, which takes advantage of z-ordering and uses the conventional indexing mechanisms offered in current database systems (e.g., relational and object-oriented).
53|Extending a spatial access structure to support additional standard attributes| In recent years, many access structures have been proposed supporting access to objects via their spatial location. However, additional non-geometric properties are always associated with geometric objects, and in practice it is often necessary to use select conditions based on spatial and standard attributes. An obvious idea to improve the performance of queries with mixed select conditions is to extend spatial access structures with additional dimensions for standard attributes. Whereas this idea seems to be simple and promising at rst glance, a closer look brings up serious problems, especially with select conditions containing arithmetic expressions or select conditions for non-point objects and with Boolean operators like or and not. In this paper we present a solution to overcome the problems sketched above which is based on three pillars: (1) We present powerful basic techniques to deal with arithmetic conditions containing mathematical operations (like `+&#039;, `;&#039;,  ` &#039;, and `=&#039;) and range queries for non-point objects. (2) We introduce a technique which allows to decompose select conditions containing Boolean operators and to reduce the processing of such a select condition to the processing of its elementary parts. (3) We showhow other operations like joins and distance-scans can be integrated into this query processing architecture.  
54|Geometric Information Makes Spatial Query Processing More Efficient|In order to index complex and heterogeneous cartographic data by means of spatial access methods, one typically uses single, simple geometries to approximate the given geometries. Traditionally, no information regarding the quality of the approximation is stored. In this paper, we show that the availability of such information allows us to decide at an early stage that numerous objects fulfill the spatial predicate. This leads to significant performance improvements for the spatial selection as well as for the spatial intersection join. We present two techniques enabling such an early decision for the z-ordering method, and we demonstrate their efficiency by means of experiments.  1 Introduction  Fast access to spatial data stored in databases is essential for answering spatial queries efficiently. As a result, numerous spatial access methods have been proposed and studied in the past. The complexity and heterogeneity of spatial data makes it impossible to store cartographic data direc...
55|On the complexity of BV-tree updates|In [Fre95b] we discussed the use of multi-dimensional index methods in constraint databases. In [Fre95a] we showed how to overcome a fundamental problem which has afflicted all multi-dimensional indexing techniques based on the recursive partitioning of a dataspace: how to build the partition hierarchy so that the location of each object in the
56|Oversize Shelves: A Storage Management Technique for Large Spatial Data Objects|In this paper we present a new technique to improve the performance of spatial access methods by minimizing redundancy: the oversize shelf . Oversize shelves are additional disk pages that are attached to the interior nodes of a tree--based spatial access method (such as the R  +  --tree or the cell tree). These pages are used to accommodate very large data objects in order to avoid their excessive fragmentation. Whenever inserting a new object into the tree, one now has to decide whether to store it on an oversize shelf or insert it into the corresponding subtrees. For this purpose, we developed an analytic model for the behavior of dynamic spatial access methods under insertions and deletions. The model yields a threshold value for the size of an object, such that it is more favorable to put it on the oversize shelf if and only if its size is greater than the threshold value. Otherwise the insertion into the corresponding subtrees is preferable. Practical experiments indicate that th...
57|Adapting the Transformation Technique to Maintain Multi-Dimensional Non-Point Objects in k-d-Tree Based Access Structures|In [10, 18] the transformation technique has been proposed to store k-dimensional intervals -- which serve as bounding boxes for arbitrary geometric objects in many applications -- as 2k-dimensional points in a point access structure. Unfortunately the transformation technique has two pitfalls: (1) The transformation leads to a skew distribution of the 2k-dimensional image points. (2) Processing a range query searching all objects intersecting a given query region, there is a mismatch between thek-dimensional query region and the 2k-dimensional access structure. In this paper we propose two techniques to overcome these problems which can be directly applied tok-d-tree based point access structures: (1) We present a sophisticated split strategy to determine the split dimension and the split position in case of a bucket split which exploits the knowledge about the distribution of the image points of the transformation technique to gain an extremely exible and robust access structure. (2) We propose a re-transformation of the 2k-dimensional data regions in the access structure into the originalk-dimensional data space in order to compare these regions with thek-dimensional query region. Furthermore we state experimental results, which demonstrate, that the presented techniques allow to maintaink-dimensional non-point objects with nearly the same performance as k-dimensional point objects. 
58|The particel swarm: Explosion, stability, and convergence in a multi-dimensional complex space  |The particle swarm is an algorithm for finding optimal regions of complex search spaces through interaction of individuals in a population of particles. Though the algorithm, which is based on a metaphor of social interaction, has been shown to perform well, researchers have not adequately explained how it works. Further, traditional versions of the algorithm have had some dynamical properties that were not considered to be desirable, notably the particles’ velocities needed to be limited in order to control their trajectories. The present paper analyzes the particle’s trajectory as it moves in discrete time (the algebraic view), then progresses to the view of it in continuous time (the analytical view). A 5-dimensional depiction is developed, which completely describes the system. These analyses lead to a generalized model of the algorithm, containing a set of coefficients to control the system’s convergence tendencies. Some results of the particle swarm optimizer, implementing modifications derived from the analysis, suggest methods for altering the original algorithm in ways that eliminate problems and increase the optimization power of the particle swarm
59|Particle swarm optimization| A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described.
60|Comparing inertia weights and constriction factors in particle swarm optimization|The performance of particle swarm optimization using an inertia weight is compared with performance using a constriction factor. Five benchmark functions are used for the comparison. It is concluded that the best approach is to use the constriction factor while limiting the maximum velocity Vmax to the dynamic range of the variable Xmax on each dimension. This approach provides performance on the benchmark functions superior to any other published results known by the authors.
61|Efficient similarity search in sequence databases|We propose an indexing method for time sequences for processing similarity queries. We use the Discrete Fourier Transform (DFT) to map time sequences to the frequency domain, the crucial observation being that, for most sequences of practical interest, only the first few frequencies are strong. Another important observation is Parseval&#039;s theorem, which specifies that the Fourier transform preserves the Euclidean distance in the time or frequency domain. Having thus mapped sequences to a lower-dimensionality space by using only the first few Fourier coe cients, we use R-trees to index the sequences and e ciently answer similarity queries. We provide experimental results which show that our method is superior to search based on sequential scanning. Our experiments show that a few coefficients (1-3) are adequate to provide good performance. The performance gain of our method increases with the number and length of sequences. 
62|A Survey of Image Registration Techniques|Registration is a fundamental task in image processing used to  match two or more pictures taken, for example, at different times,  from different sensors or from different viewpoints. Over the years, a  broad range of techniques have been developed for the various types of  data and problems. These techniques have been independently studied  for several different applications resulting in a large body of research.  This paper organizes this material by establishing the relationship  between the distortions in the image and the type of registration techniques  which are most suitable. Two major types of distortions are  distinguished. The first type are those which are the source of misregistration,  i.e., they are the cause of the misalignment between the two  images. Distortions which are the source of misregistration determine  the transformation class which will optimally align the two images.  The transformation class in turn influences the general technique that  should be taken....
63|Voronoi diagrams -- a survey of a fundamental geometric data structure|This paper presents a survey of the Voronoi diagram, one of the most fundamental data structures in computational geometry. It demonstrates the importance and usefulness of the Voronoi diagram in a wide variety of fields inside and outside computer science and surveys the history of its development. The paper puts particular emphasis on the unified exposition of its mathematical and algorithmic properties. Finally, the paper provides the first comprehensive bibliography on Voronoi diagrams and related structures.
64|Database Mining: A Performance Perspective|We present our perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology. We describe three classes of database mining problems involving classification, associations, and sequences, and argue that these problems can be uniformly viewed as requiring discovery of rules embedded in massive data. We describe a model and some basic operations for the process of rule discovery. We show how the database mining problems we consider map to this model and how they can be solved by using the basic operations we propose. We give an example of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm not only is efficient in discovering classification rules but also has accuracy comparable to ID3, one of the current best classifiers.  Index Terms. database mining, knowledge discovery, classification, associations, sequences, decision trees   Current address: Computer Science De...
65|Temporal databases|A temporal database (see Temporal Database) contains time-varying data. Time is an important aspect of all real-world phenomena. Events occur at specific points in time; objects and the relationships among objects exist over time. The ability to model this temporal dimension of the real world is essential to many computer applications, such as accounting, banking, econometrics, geographical information systems, inventory control, law, medical records, multi-media, process control, reservation systems, and scientific data analysis. Conventional databases represent the state of an enterprise at a single moment of time. Although the contents of the database continue to change as new information is added, these changes are viewed as modifications to the state, with the old, out-of-date data being deleted from the database. The current contents of the database may be viewed as a snapshot of the enterprise. When a conventional database is used, the attributes involving time are manipulated solely by the application programs, with little help
66|The hB-tree: A multiattribute indexing method with good guaranteed performance|A new multiattribute index structure called the hB-tree is introduced. It is derived from the K-D-B-tree of Robinson [15] but has additional desirable properties. The hB-tree internode search and growth processes are precisely analogous to the corresponding processes in B-trees [l]. The intranode processes are unique. A k-d tree is used as the structure within nodes for very efficient searching. Node splitting requires that this k-d tree be split. This produces nodes which no longer represent brick-like regions in k-space, but that can be characterized as holey bricks, bricks in which subregions have been extracted. We present results that guarantee hB-tree users decent storage utilization, reasonable size index terms, and good search and insert performance. These results guarantee that the hB-tree copes well with arbitrary distributions of keys.
67|Vague: a user interface to relational databases that permits vague queries|A specific query establishes a rigid qualification and is concerned only with data that match it precisely. A vague query establishes a target qualification and is concerned also with data that are close to this target. Most conventional database systems cannot handle vague queries directly, forcing their users to retry specific queries repeatedly with minor modifications until they match data that are satisfactory. This article describes a system called VAGUE that can handle vague queries directly. The principal concept behind VAGUE is its extension to the relational data model with data metrics, which are definitions of distances between values of the same domain. A problem with implementing data distances is that different users may have different interpretations for the notion of distance. VAGUE incorporates several features that enable it to adapt itself to the individual views and priorities of its users.
68|New Techniques for Best-Match Retrieval|A scheme to answer best-match queries from a file containing a collection of objects is described. A best-match query is to find the objects in the file that are closest (according to some (dis)similarity measure) to a given target. Previous work [5, 331 suggests that one can reduce the number of comparisons required to achieve the desired results using the triangle inequality, starting with a data structure for the file that reflects some precomputed intrafile distances. We generalize the technique to allow the optimum use of any given set of precomputed intrafile distances. Some empirical results are presented which illustrate the effectiveness of our scheme, and its performance relative to previous algorithms.
69|Tabu Search -- Part I|This paper presents the fundamental principles underlying tabu search as a strategy for combinatorial optimization problems. Tabu search has achieved impressive practical successes in applications ranging from scheduling and computer channel balancing to cluster analysis and space planning, and more recently has demonstrated its value in treating classical problems such as the traveling salesman and graph coloring problems. Nevertheless, the approach is still in its infancy, and a good deal remains to be discovered about its most effective forms of implementation and about the range of problems for which it is best suited. This paper undertakes to present the major ideas and findings to date, and to indicate challenges for future research. Part I of this study indicates the basic principles, ranging from the short-term memory process at the core of the search to the intermediate and long term memory processes for intensifying and diversifying the search. Included are illustrative data structures for implementing the tabu conditions (and associated aspiration criteria) that underlie these processes. Part I concludes with a discussion of probabilistic tabu search and a summary of computational experience for a variety of applications. Part I1 of this study (to appear in a subsequent issue) examines more advanced considerations, applying the basic ideas to special settings and outlining a dynamic move structure to insure finiteness. Part I1 also describes tabu search methods for solving mixed integer programming problems and gives a brief summary of additional practical experience, including the use of tabu search to guide other types of processes, such as those
70|M-tree: An Efficient Access Method for Similarity Search in Metric Spaces|A new access meth d, called M-tree, is proposed to organize and search large data sets from a generic &#034;metric space&#034;, i.e. whE4 object proximity is only defined by a distance function satisfyingth positivity, symmetry, and triangle inequality postulates. We detail algorith[ for insertion of objects and split management, whF h keep th M-tree always balanced - severalheralvFV split alternatives are considered and experimentally evaluated. Algorithd for similarity (range and k-nearest neigh bors) queries are also described. Results from extensive experimentationwith a prototype system are reported, considering as th performance criteria th number of page I/O&#039;s and th number of distance computations. Th results demonstratethm th Mtree indeed extendsth domain of applicability beyond th traditional vector spaces, performs reasonably well inhE[94Kv#E44V[vh data spaces, and scales well in case of growing files. 1 
71|Nearest Neighbor Queries|A frequently encountered type of query in Geographic Information Systems is to find the k nearest neighbor objects to a given point in space. Processing such queries requires substantially different search algorithms than those for location or range queries. In this paper we present an efficient branch-and-bound R-tree traversal algorithm to find the nearest neighbor object to a point, and then generalize it to finding the k nearest neighbors. We also discuss metrics for an optimistic and a pessimistic search ordering strategy as well as for pruning. Finally, we present the results of several experiments obtained using the implementation of our algorithm and examine the behavior of the metrics and the scalability of the algorithm. 
72|Fast subsequence matching in time-series databases|We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an ecient and eective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.  
73|Efficient and Effective Querying by Image Content|In the QBIC (Query By Image Content) project we are studying methods to query large  on-line image databases using the images&#039; content as the basis of the queries. Examples of  the content we use include color, texture, and shape of image objects and regions. Potential  applications include medical (&#034;Give me other images that contain a tumor with a texture like this  one&#034;), photo-journalism (&#034;Give me images that have blue at the top and red at the bottom&#034;),  and many others in art, fashion, cataloging, retailing, and industry.  We describe a set of novel features and similarity measures allowing query by color, texture,  and shape of image object. We demonstrate the effectiveness of the QBIC system with normalized  precision and recall experiments on test databases containing over 1000 images and 1000  objects populated from commercially available photo clip art images, and of images of airplane  silhouettes. We also consider the efficient indexing of these features, specifically addre...
74|FastMap: A Fast Algorithm for Indexing, Data-Mining and Visualization of Traditional and Multimedia Datasets|A very promising idea for fast searching in traditional and multimedia databases is to map objects into points in k-d  space, using k feature-extraction functions, provided by a domain expert [25]. Thus, we can subsequently use highly fine-tuned spatial access methods (SAMs), to answer several types of queries, including the `Query By Example&#039; type (which translates to a range query); the `all pairs&#039; query (which translates to a spatial join [8]); the nearest-neighbor or best-match query, etc. However, designing feature extraction functions can be hard. It is relatively easier for a domain expert to assess the similarity/distance of two objects. Given only the distance information though, it is not obvious how to map objects into points. This is exactly the topic of this paper. We describe a fast algorithm to map objects into points in some k-dimensional  space (k is user-defined), such that the dis-similarities are preserved. There are two benefits from this mapping: (a) efficient ret...
75|The R + -tree: A dynamic index for multidimensional objects|The problem of indexing multidimensional objects is considered. First, a classification of existing methods is given along with a discussion of the major issues involved in multidimensional data indexing. Second, a variation to Guttman’s R-trees (R +-trees) that avoids overlapping rectangles in intermediate nodes of the tree is introduced. Algorithms for searching, updating, initial packing and reorganization of the structure are discussed in detail. Finally, we provide analytical results indicating that R +-trees achieve up to 50 % savings in disk accesses compared to an R-tree when searching files of thousands of rectangles. 1
76|Content-based classification, search, and retrieval of audio|say that it belongs to the class of speech sounds or the class of applause sounds, where the system has previously been trained on other sounds in this class. I Acoustical/perceptual features: describing the sounds in terms of commonly understood physical characteristics such as brightness, pitch, and loudness. I Subjective features: describing the sounds using personal descriptive language. This requires training the system (in our case, by example) to understand the meaning of these descriptive terms. For example, a user might be looking for a “shimmering ” sound.
77|Near neighbor search in large metric spaces|Given user data, one often wants to find approximate matches in a large database. A good example of such a task is finding images similar to a given image in a large collection of images. We focus on the important and technically difficult case where each data element is high dimensional, or more generally, is represented by a point in a large metric spaceand distance calculations are computationally expensive. In this paper we introduce a data structure to solve this problem called a GNAT- Geometric Near-neighbor Access Tree. It is based on the philosophy that the data structure should act as a hierarchical geometrical model of the data as opposed to a simple decomposition of the data that does not use its intrinsic geometry. In experiments, we find that GNAT’s outperform previous data structures in a number of applications.
78|Distance-based indexing for high-dimensional metric spaces|In many database applications, one of the common queries is to find approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance based index structures are proposed for applications where the data domain is high dimensional, or the distance function used to compute distances between data objects is non-Euclidean. In this paper, we introduce a distance based index structure called multi-vantage point (mvp) tree for similarity queries on high-dimensional metric spaces. The mvptree uses more than one vantage point to partition the space into spherical cuts at each level. It also utilizes the pre-computed (at construction time) distances between the data points and the vantage points. We have done experiments to compare mvp-trees with vp-trees which have a similar partitioning strategy, but use only one vantage point at each level, and do not make use of the pre-computed distances. Empirical studies show that mvptree outperforms the vp-tree 20 % to 80 % for varying query ranges and different distance distributions. 1.
79|Costly search and mutual fund flows|This paper studies the flows of funds into and out of equity mutual funds. Consumers base their fund purchase decisions on prior performance information, but do so asymmetrically, investing disproportionately more in funds that performed very well the prior period. Search costs seem to be an important determinant of fund flows. High performance appears to be most salient for funds that exert higher marketing effort, as measured by higher fees. Flows are directly related to the size of the fund’s complex as well as the current media attention received by the fund, which lower consumers ’ search costs. ALTHOUGH MUCH ACADEMIC RESEARCH on mutual funds addresses issues of performance measurement and attribution, we can learn more from this industry than whether fund managers can consistently earn risk-adjusted excess returns. Researchers studying funds have shed light on how incentives affect fund managers ’ behavior, 1 how board structure affects oversight activities, 2 and how scale and scope economies affect mutual fund costs and fees. 3 More generally, the fund industry is a laboratory in which to study the actions of individual investors who buy fund shares. In this paper, we study the flows of funds into and out of individual U.S. equity mutual funds to better understand the behavior of households that buy funds and the fund complexes and marketers that sell them.
82|The performance of mutual funds in the period 1945-1964|In this paper I derive a risk-adjusted measure of portfolio performance (now known as &#034;Jensen&#039;s Alpha&#034;) that estimates how much a manager&#039;s forecasting ability contributes to the fund&#039;s returns. The measure is based on the theory of the pricing of capital assets by Sharpe (1964), Lintner (1965a) and Treynor (Undated). I apply the measure to estimate the predictive ability of 115 mutual fund managers in the period 1945-1964—that is their ability to earn returns which are higher than those we would expect given the level of risk of each of the portfolios. The foundations of the model and the properties of the performance measure suggested here are discussed in Section II. The evidence on mutual fund performance indicates not only that these 115 mutual funds were on average not able to predict security prices well enough to outperform a buy-the-marketand-hold policy, but also that there is very little evidence that any individual fund was able to do significantly better than that which we expected from mere random chance. It is also important to note that these conclusions hold even when we measure the fund returns gross of management expenses (that is assume their bookkeeping, research, and other expenses except brokerage commissions were obtained free). Thus on average the funds apparently were not quite successful enough in their trading activities to recoup even their brokerage expenses.  
83|Cognitive Dissonance and Mutual Fund Investors|We present evidence from questionnaire studies of mutual fund  investors about recollections of past fund performance. We find  that investor memories exhibit a positive bias, consistent with  current psychological models. We find that the degree of bias is  conditional upon previous investor choice, a phenomenon related  to the well known theory of cognitive dissonance.
84|Pushing the Envelope: Planning, Propositional Logic, and Stochastic Search|Planning is a notoriously hard combinatorial search problem. In many interesting domains, current planning algorithms fail to scale up gracefully. By combining a general, stochastic search algorithm and appropriate problem encodings based on propositional logic, we are able to solve hard planning problems many times faster than the best current planning systems. Although stochastic methods have been shown to be very e ective on a wide range of scheduling problems, this is the rst demonstration of its power on truly challenging classical planning instances. This work also provides a new perspective on representational issues in planning.
85|Fast Planning Through Planning Graph Analysis|We introduce a new approach to planning in STRIPS-like domains based on constructing and analyzing a compact structure we call a Planning Graph. We describe a new planner, Graphplan, that uses this paradigm. Graphplan always returns a shortest possible partial-order plan, or states that no valid plan exists. We provide empirical evidence in favor of this approach, showing that Graphplan outperforms the total-order planner, Prodigy, and the partial-order planner, UCPOP, on a variety of interesting natural and artificial planning problems. We also give empirical evidence that the plans produced by Graphplan are quite sensible. Since searches made by this approach are fundamentally different from the searches of other common planning methods, they provide a new perspective on the planning problem.
86|Using Temporal Logic to Control Search in a Forward Chaining Planner|. Over the years increasingly sophisticated planning algorithms have been developed. These have made for more efficient planners, but unfortunately these planners still suffer from combinatorial explosion. Indeed, recent theoretical results demonstrate that such an explosion is inevitable. It has long been acknowledged that domain independent planners need domain dependent information to help them plan effectively. In this work we describe how natural domain information, of a &#034;strategic&#034; nature, can be expressed in a temporal logic, and then utilized to effectively control a forward-chaining planner. There are numerous advantages to our approach, including a declarative semantics for the search control knowledge; a high degree of modularity (the more search control knowledge utilized the more efficient search becomes); and an independence of this knowledge from the details of the planning algorithm. We have implemented our ideas in the TLPLAN system, and have been able to demonstrate i...
87|Partial-Order Planning: Evaluating Possible Efficiency Gains|Although most people believe that planners that delay step-ordering decisions as long as possible are more efficient than those that manipulate totally ordered sequences of actions, this intuition has received little formal justification or empirical validation. In this paper we do both, characterizing the types of domains that offer performance differentiation and the features that distinguish the relative overhead of three planning algorithms. As expected, the partial-order (nonlinear) planner often has an advantage when confronted with problems in which the specific order of the plan steps is critical. We argue that the observed performance differences are best understood with an extension of Korf&#039;s taxonomy of subgoal collections. Each planner quickly solved problems whose subgoals were independent or trivially serializable, but problems with laboriously serializable or nonserializable subgoals were intractable for all planners. Since different plan representations induce distinct ...
88|Planning as Temporal Reasoning|This paper describes a reasoning system based on a temporal logic that can solve planning problems along the lines of traditional planning systems. Because it is cast as inference in a general representation, however, the ranges of problems that can be described is considerably greater than in traditional planning systems. In addition, other modes of plan reasoning, such as plan recognition or plan monitoring, can be formalized within the same framework. 1
89|Fast Parallel Algorithms for Short-Range Molecular Dynamics|Three parallel algorithms for classical molecular dynamics are presented. The first assigns each  processor a fixed subset of atoms; the second assigns each a fixed subset of inter-atomic forces to compute;  the third assigns each a fixed spatial region. The algorithms are suitable for molecular dynamics models  which can be difficult to parallelize efficiently -- those with short-range forces where the neighbors of  each atom change rapidly. They can be implemented on any distributed--memory parallel machine which  allows for message--passing of data between independently executing processors. The algorithms are  tested on a standard Lennard-Jones benchmark problem for system sizes ranging from 500 to 100,000,000  atoms on several parallel supercomputers -- the nCUBE 2, Intel iPSC/860 and Paragon, and Cray T3D.  Comparing the results to the fastest reported vectorized Cray Y-MP and C90 algorithm shows that  the current generation of parallel machines is competitive with conventi...
90|A Fast Algorithm for Particle Simulations|this paper to the case where  the potential (or force) at a point is a sum of pairwise An algorithm is presented for the rapid evaluation of the potential and force fields in systems involving large numbers of particles interactions. More specifically, we consider potentials of  whose interactions are Coulombic or gravitational in nature. For a the form  system of N particles, an amount of work of the order O(N  2  ) has traditionally been required to evaluate all pairwise interactions, un- F5F far 1 (F near 1F external ), less some approximation or truncation method is used. The algorithm of the present paper requires an amount of work proportional to N to evaluate all interactions to within roundoff error, making it where F near (when present) is a rapidly decaying potential  con
91|The Torus-Wrap Mapping For Dense Matrix Calculations On Massively Parallel Computers| Dense linear systems of equations are quite common in science and engineering, arising in boundary element methods, least squares problems and other settings. Massively parallel computers will be necessary to solve the large systems required by scientists and engineers, and scalable parallel algorithms for the linear algebra applications must be devised for these machines. A critical step in these algorithms is the mapping of matrix elements to processors. In this paper, we study the use of the torus--wrap mapping in general dense matrix algorithms, from both theoretical and practical viewpoints. We prove that, under reasonable assumptions, this assignment scheme leads to dense matrix algorithms that achieve (to within a constant factor) the lower bound on interprocessor communication. We also show that the torus--wrap mapping allows algorithms to exhibit less idle time, better load balancing and less memory overhead than the more common row and column mappings. Finally, we discuss ...
92|A New Parallel Method for Molecular Dynamics Simulation of Macromolecular Systems|Short--range molecular dynamics simulations of molecular systems are commonly parallelized by  replicated--data methods, where each processor stores a copy of all atom positions. This enables computation  of bonded 2--, 3--, and 4--body forces within the molecular topology to be partitioned among  processors straightforwardly. A drawback to such methods is that the inter--processor communication  scales as N , the number of atoms, independent of P , the number of processors. Thus, their parallel efficiency  falls off rapidly when large numbers of processors are used. In this article a new parallel method  for simulating macromolecular or small--molecule systems is presented, called force--decomposition. Its  memory and communication costs scale as N=  p  P , allowing larger problems to be run faster on greater  numbers of processors. Like replicated--data techniques, and in contrast to spatial--decomposition approaches,  the new method can be simply load--balanced and performs well eve...
93|Parallel Many-Body Simulations Without All-to-All Communication|Simulations of interacting particles are common in science and engineering, appearing in such diverse disciplines as astrophysics, fluid dynamics, molecular physics, and materials science. These simulations are often computationally intensive and so natural candidates for massively  parallel computing. Many-body simulations that directly compute interactions between pairs  of particles, be they short-range or long-range interactions, have been parallelized in several  standard ways. The simplest approaches require all-to-all communication, an expensive communication  step. The fastest methods assign a group of nearby particles to a processor, which  can lead to load imbalance and be difficult to implement efficiently. We present a new approach,  suitable for direct simulations, that avoids all-to-all communication without requiring  any geometric clustering. For some computations we find the new method to be the fastest  parallel algorithm available; we demonstrate its utility...
94|A High Performance Communications and Memory Caching Scheme for Molecular Dynamics on the CM-5|We present several techniques that we have used to optimize the performance of a message-passing C code for molecular dynamics on the CM-5. We describe our use of the CM-5 vector units and a parallel memory caching scheme that we have developed to speed up the code by more than 50%. A modification that decreases our communication time by 35% is also presented along with a discussion of how we have been able to take advantage of the CM-5 hardware without significantly compromising code portability. We have been able to speed up our original code by a factor of ten and we feel that our modifications may be useful in optimizing the performance of other message-passing C applications on the CM-5. 1 Introduction  For several decades, the method of molecular dynamics (MD)[1] has been a useful technique for studying the dynamical properties of solids and liquids. In a molecular dynamics simulation, the motion of a large collection of N atoms is modeled directly by solving Newton&#039;s equations o...
95|A Parallel Scalable Approach to Short-Range Molecular Dynamics on the CM-5|We present an scalable algorithm for short-range Molecular Dynamics which minimizes interprocessor communications at the expense of a modest computational redundancy. The method combines Verlet neighbor lists with coarse-grained cells. Each processing node is associated with a cubic volume of space and the particles it owns are those initially contained in the volume. Data structures for &#034;own&#034; and &#034;visitor &#034; particle coordinates are maintained in each node. Visitors are particles owned by one of the 26 neighboring cells but lying within an interaction range of a face. The Verlet neighbor list includes pointers to own--own and own--visitor interactions. To communicate, each of the 26 neighbor cells sends a corresponding block of particle coordinates using message-passing calls. The algorithm has the numerical properties of the standard serial Verlet method and is efficient for hundreds to thousands of particles per node allowing the simulation of large systems with millions of particles...
96|A Fast Quantum Mechanical Algorithm for Database Search|Imagine a phone directory containing N names arranged in completely random order. In order to find someone&#039;s phone number with a probability of , any classical algorithm (whether deterministic or probabilistic)
will need to look at a minimum of names. Quantum mechanical systems can be in a superposition of states and simultaneously examine multiple names. By properly adjusting the phases of various operations, successful computations reinforce each other while others interfere randomly. As a result, the desired phone number can be obtained in only steps. The algorithm is within a small constant factor of the fastest possible quantum mechanical algorithm.
97|Algorithms for Quantum Computation: Discrete Logarithms and Factoring|A computer is generally considered to be a universal computational device; i.e., it is believed able to simulate any physical computational device with a cost in com-putation time of at most a polynomial factol: It is not clear whether this is still true when quantum mechanics is taken into consideration. Several researchers, starting with David Deutsch, have developed models for quantum mechanical computers and have investigated their compu-tational properties. This paper gives Las Vegas algorithms for finding discrete logarithms and factoring integers on a quantum computer that take a number of steps which is polynomial in the input size, e.g., the number of digits of the integer to be factored. These two problems are generally considered hard on a classical computer and have been used as the basis of several proposed cryptosystems. (We thus give the first examples of quantum cryptanulysis.)
98|Quantum theory, the Church-Turing principle and the universal quantum computer|computer
99|Quantum complexity theory|Abstract. In this paper we study quantum computation from a complexity theoretic viewpoint. Our first result is the existence of an efficient universal quantum Turing machine in Deutsch’s model of a quantum Turing machine (QTM) [Proc. Roy. Soc. London Ser. A, 400 (1985), pp. 97–117]. This construction is substantially more complicated than the corresponding construction for classical Turing machines (TMs); in fact, even simple primitives such as looping, branching, and composition are not straightforward in the context of quantum Turing machines. We establish how these familiar primitives can be implemented and introduce some new, purely quantum mechanical primitives, such as changing the computational basis and carrying out an arbitrary unitary transformation of polynomially bounded dimension. We also consider the precision to which the transition amplitudes of a quantum Turing machine need to be specified. We prove that O(log T) bits of precision suffice to support a T step computation. This justifies the claim that the quantum Turing machine model should be regarded as a discrete model of computation and not an analog one. We give the first formal evidence that quantum Turing machines violate the modern (complexity theoretic) formulation of the Church–Turing thesis. We show the existence of a problem, relative to an oracle, that can be solved in polynomial time on a quantum Turing machine, but requires superpolynomial time on a bounded-error probabilistic Turing machine, and thus not in the class BPP. The class BQP of languages that are efficiently decidable (with small error-probability) on a quantum Turing machine satisfies BPP ? BQP ? P ?P. Therefore, there is no possibility of giving a mathematical proof that quantum Turing machines are more powerful than classical probabilistic Turing machines (in the unrelativized setting) unless there is a major breakthrough in complexity theory.
100|Rapid solution of problems by quantum computation|A class of problems is described which can be solved more efficiently by quantum computation than by any classical or stochastic method. The quantum computation solves the problem with certainty in exponentially less time than any classical deterministic computation.
102|Strengths and Weaknesses of quantum computing|  Recently a great deal of attention has been focused on quantum computation following a
103|Quantum Circuit Complexity|We study a complexity model of quantum circuits analogous to the standard (acyclic) Boolean circuit model. It is shown that any function computable in polynomial time by a quantum Turing machine has a polynomial-size quantum circuit. This result also enables us to construct a universal quantum computer which can simulate, with a polynomial factor slowdown, a broader class of quantum machines than that considered by Bernstein and Vazirani [BV93], thus answering an open question raised in [BV93]. We also develop a theory of quantum communication complexity, and use it as a tool to prove that the majority function does not have a linear-size quantum formula. Keywords. Boolean circuit complexity, communication complexity, quantum communication complexity, quantum computation  AMS subject classifications. 68Q05, 68Q15 1  This research was supported in part by the National Science Foundation under grant CCR-9301430.  1 Introduction One of the most intriguing questions in computation theroy ...
104|Matching is as Easy as Matrix Inversion|A new algorithm for finding a maximum matching in a general graph is presented; its special feature being that the only computationally non-trivial step required in its execution is the inversion of a single integer matrix. Since this step can be parallelized, we get a simple parallel (RNC2) algorithm. At the heart of our algorithm lies a probabilistic lemma, the isolating lemma. We show applications of this lemma to parallel computation and randomized reductions. 
105|Oracle quantum computing|\Because nature isn&#039;t classical, dammit...&#034;
106|A fast quantum mechanical algorithm for estimating the median. Quantum Physics e-Print archive |Consider the problem of estimating the median of N items to a precision e, i.e. the estimate µ should be such that, with a large probability, the number of items with values smaller than µ is less than and those with values greater than µ is also less than. Any classical algorithm to do this will need at least samples. Quantum mechanical systems can simultaneously carry out multiple computations due to their wave like properties. This paper gives an step algorithm for the above problem. 1
107|The FF planning system: Fast plan generation through heuristic search|We describe and evaluate the algorithmic techniques that are used in the FF planning system. Like the HSP system, FF relies on forward state space search, using a heuristic that estimates goal distances by ignoring delete lists. Unlike HSP&#039;s heuristic, our method does not assume facts to be independent. We introduce a novel search strategy that combines Hill-climbing with systematic search, and we show how other powerful heuristic information can be extracted and used to prune the search space. FF was the most successful automatic planner at the recent AIPS-2000 planning competition. We review the results of the competition, give data for other benchmark domains, and investigate the reasons for the runtime performance of FF compared to HSP.  
108|Systematic Nonlinear Planning|This paper presents a simple, sound, complete, and systematic algorithm for domain independent STRIPS planning. Simplicity is achieved by starting with a ground procedure and then applying a general, and independently verifiable, lifting transformation. Previous planners have been designed directly as lifted procedures. Our ground procedure is a ground version of Tate&#039;s NONLIN procedure. In Tate&#039;s procedure one is not required to determine whether a prerequisite of a step in an unfinished plan is guaranteed to hold in all linearizations. This allows Tate&#039;s procedure to avoid the use of Chapman&#039;s modal truth criterion. Systematicity is the property that the same plan, or partial plan, is never examined more than once. Systematicity is achieved through a simple modification of Tate&#039;s procedure.
109|The Computational Complexity of Propositional STRIPS Planning|I present several computational complexity results for propositional STRIPS planning, i.e., STRIPS planning restricted to ground formulas. Different planning problems can be defined by restricting the type of formulas, placing limits on the number of pre- and postconditions, by restricting negation in pre- and postconditions, and by requiring optimal plans. For these types of restrictions, I show when planning is tractable (polynomial) and intractable (NPhard) . In general, it is PSPACE-complete to determine if a given planning instance has any solutions. Extremely severe restrictions on both the operators and the formulas are required to guarantee polynomial time or even NP-completeness. For example, when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. When definite Horn ground formulas are permitted, determining plan existence is PSPACE-complete even if operators are limited t...
110|Unifying SAT-based and Graph-based Planning|The Blackbox planning system unifies the plan-ning as satisfiability framework (Kautz and Sel-man 1992, 1996) with the plan graph approach to STRIPS planning (Blum and Furst 1995). We show that STRIPS problems can be directly translated into SAT and efficiently solved using new random-ized systematic solvers. For certain computation-ally challenging benchmark problems this unified approach outperforms both SATPLAN and Graph-plan alone. We also demonstrate that polynomial-time SAT simplification algorithms applied to the encoded problem instances are a powerful com-plement to the “mutex ” propagation algorithm that works directly on the plan graph. 1
111|Hard and Easy Distributions of SAT Problems|We report results from large-scale experiments in satisfiability testing. As has been observed by others, testing the satisfiability of random formulas often appears surprisingly easy. Here we show that by using the right distribution of instances, and appropriate parameter values, it is possible to generate random formulas that are hard, that is, for which satisfiability testing is quite difficult. Our results provide a benchmark for the evaluation of satisfiability-testing procedures.  Introduction  Many computational tasks of interest to AI, to the extent that they can be precisely characterized at all, can be shown to be NP-hard in their most general form. However, there is fundamental disagreement, at least within the AI community, about the implications of this. It is claimed on the one hand that since the performance of algorithms designed to solve NP-hard tasks degrades rapidly with small increases in input size, something will need to be given up to obtain acceptable behavior....
112|Planning as Heuristic Search: New Results|In the recent AIPS98 Planning Competition, the hsp planner,  based on a forward state search and a domain-independent heuristic,  showed that heuristic search planners can be competitive with state of  the art Graphplan and Satisfiability planners. hsp solved more problems  than the other planners but it often took more time or produced longer  plans. The main bottleneck in hsp is the computation of the heuristic  for every new state. This computation may take up to 85% of the processing  time. In this paper, we present a solution to this problem that  uses a simple change in the direction of the search. The new planner,  that we call hspr, is based on the same ideas and heuristic as hsp, but  searches backward from the goal rather than forward from the initial  state. This allows hspr to compute the heuristic estimates only once. As  a result, hspr can produce better plans, often in less time. For example,  hspr solves each of the 30 logistics problems from Kautz and Selman in  less than 3 seconds. This is two orders of magnitude faster than blackbox. At the same time
113|Extending planning graphs to an ADL subset| We describe an extension of graphplan to a subset of ADL that allows conditional and universally quantified effects in operators in such away that almost all interesting properties of the original graphplan algorithm are preserved.
114|A Robust and Fast Action Selection Mechanism for Planning|The ability to plan and react in dynamic environments is central to intelligent behavior yet few algorithms have managed to combine fast planning with a robust execution. In this paper we develop one such algorithm by looking at planning as real time search. For that we develop a variation of Korf&#039;s Learning Real Time A algorithm together with a suitable heuristic function. The resulting algorithm interleaves lookahead with execution and never builds a plan. It is an action selection mechanism that decides at each time point what to do next. Yet it solves hard planning problems faster than any domain independent planning algorithm known to us, including the powerful SAT planner recently introduced by Kautz and Selman. It also works in the presence of perturbations and noise, and can be given a fixed time window to operate. We illustrate each of these features by running the algorithm on a number of benchmark problems. 1 Introduction The ability to plan and react ...
115|The automatic inference of state invariants in TIM|As planning is applied to larger and richer domains the e ort involved in constructing domain descriptions increases and becomes a signi cant burden on the human application designer. If general planners are to be applied successfully to large and complex domains it is necessary to provide the domain designer with some assistance in building correctly encoded domains. One way of doing this is to provide domain-independent techniques for extracting, from a domain description, knowledge that is implicit in that description and that can assist domain designers in debugging domain descriptions. This knowledge can also be exploited to improve the performance of planners: several researchers have explored the potential of state invariants in speeding up the performance of domain-independent planners. In this paper we describe a process by which state invariants can be extracted from the automatically inferred type structure of a domain. These techniques are being developed for exploitation by stan, a Graphplan based planner that employs state analysis techniques to enhance its performance. 1.
116|Combining the expressivity of UCPOP with the efficiency of Graphplan|  There has been a great deal of recent work on new approaches to efficiently generating plans in systems such as Graphplan and SATplan. However, these systems only provide an impoverished representation language compared to other planners, such as UCPOP, ADL, or Prodigy. This makes it difficult to represent planning problems using these new planners. This paper addresses this problem by providing a completely automated set of transformations for converting a UCPOP domain representation into a Graphplan representation. The set of transformations extends the Graphplan representation language to include disjunctions, negations, universal quantification, conditional effects, and axioms. We tested the resulting planner on the 18 test domains and 41 problems that come with the UCPOP 4.0 distribution. Graphplan with the new preprocessor is able to solve every problem in the test set and on the hard problems (i.e., those that require more than one second of CPU time) it can solve them significantly faster than UCPOP. While UCPOP was unable to solve 7 of the test problems within a search limit of 100,000 nodes (which requires 414 to 980 CPU seconds), Graphplan with the preprocessor solved them all in under 15 CPU seconds (including the preprocessing time). 
117|A Heuristic Estimator for Means-Ends Analysis in Planning|Means-ends analysis is a seemingly well understood search technique, which can be described, using planning terminology, as: keep adding actions that are feasible and achieve pieces of the goal. Unfortunately, it is often the case that no action is both feasible and relevant in this sense. The traditional answer is to make subgoals out of the preconditions of relevant but infeasible actions. These subgoals become part of the search state. An alternative, surprisingly good, idea is to recompute the entire subgoal hierarchy after every action. This hierarchy is represented by a greedy regression-match graph. The actions near the leaves of this graph are feasible and relevant to a sub. . . subgoals of the original goal. Furthermore, each subgoal is assigned an estimate of the number of actions required to achieve it. This number can be shown in practice to be a useful heuristic estimator for domains that are otherwise intractable.  Keywords: planning, search, means-ends analysis   Reinven...
118|On the compilability and expressive power of propositional planning formalisms|The recent approaches of extending the GRAPHPLAN algorithm to handle more expressive planning formalisms raise the question of what the formal meaning of “expressive power ” is. We formalize the intuition that expressive power is a measure of how concisely planning domains and plans can be expressed in a particular formalism by introducing the notion of “compilation schemes ” between planning formalisms. Using this notion, we analyze the expressiveness of a large family of propositional planning formalisms, ranging from basic STRIPS to a formalism with conditional effects, partial state specifications, and propositional formulae in the preconditions. One of the results is that conditional effects cannot be compiled away if plan size should grow only linearly but can be compiled away if we allow for polynomial growth of the resulting plans. This result confirms that the recently proposed extensions to the GRAPHPLAN algorithm concerning conditional effects are optimal with respect to the “compilability ” framework. Another result is that general propositional formulae cannot be compiled into conditional effects if the plan size should be preserved linearly. This implies that allowing general propositional formulae in preconditions and effect conditions adds another level of difficulty in generating a plan.
119|Ignoring Irrelevant Facts and Operators in Plan Generation|It is traditional wisdom that one should start from the goals when  generating a plan in order to focus the plan generation process on potentially  relevant actions. The graphplan system, however, which is the  most efficient planning system nowadays, builds a &#034;planning graph&#034; in a  forward-chaining manner. Although this strategy seems to work well, it  may possibly lead to problems if the planning task description contains irrelevant  information. Although some irrelevant information can be filtered  out by graphplan, most cases of irrelevance are not noticed.  In this paper, we analyze the effects arising from &#034;irrelevant&#034; information  to planning task descriptions for different types of planners. Based  on that, we propose a family of heuristics that select relevant information  by minimizing the number of initial facts that are used when approximating  a plan by backchaining from the goals ignoring any conflicts. These  heuristics, although not solution-preserving, turn out to be v...
120|Using Regression-Match Graphs to Control Search in Planning|Classical planning is the problem of finding a sequence of actions to achieve a goal given an exact characterization of a domain. An algorithm to solve this problem is presented, which searches a space of plan prefixes, trying to extend one of them to a complete sequence of actions. It is guided by a heuristic estimator based on regression-match graphs, which attempt to characterize the entire subgoal structure of the remaining part of the problem. These graphs simplify the structure by neglecting goal interactions and by assuming that variables in goal conjunctions should be bound in such a way as to make as many conjuncts as possible true without further work. In some domains, these approximations work very well, and experiments show that many classical-planning problems can solved with very little search. 1 Definition of the Problem  The classical planning problem is to generate a sequence of actions that make a given proposition true, in a domain in which there is perfect informati...
121|Efficient Implementation of the Plan Graph in STAN|Stan is a Graphplan-based planner, so-called because it uses a variety of STate ANalysis techniques to enhance its performance. Stan competed in the AIPS-98 planning competition where it compared well with the other competitors in terms of speed, finding solutions fastest to many of the problems posed. Although the domain analysis techniques  Stan exploits are an important factor in its overall performance, we believe that the speed at which Stan solved the competition problems is largely due to the implementation of its plan graph. The implementation is based on two insights: that many of the graph construction operations can be implemented as bit-level logical operations on bit vectors, and that the graph should not be explicitly constructed beyond the fix point. This paper describes the implementation of Stan&#039;s plan graph and provides experimental results which demonstrate the circumstances under which advantages can be obtained from using this implementation. 1. Introduction  Stan ...
122|When Gravity Fails: Local Search Topology|Local search algorithms for combinatorial search problems frequently encounter a sequence of states in which it is impossible to improve the value of the objective function; moves through these regions, called plateau moves, dominate the time spent in local search. We analyze and characterize plateaus for three different classes of randomly generated Boolean Satisfiability problems. We identify several interesting features of plateaus that impact the performance of local search algorithms. We show that local minima tend to be small but occasionally may be very large. We also show that local minima can be escaped without unsatisfying a large number of clauses, but that systematically searching for an escape route may be computationally expensive if the local minimum is large. We show that plateaus with exits, called benches, tend to be much larger than minima, and that some benches have very few exit states which local search can use to escape. We show that the solutions (i.e., global m...
123|On Reasonable and Forced Goal Orderings and their Use in an Agenda-Driven Planning Algorithm|The paper addresses the problem of computing goal orderings, which is one of the longstanding issues in AI planning. It makes two new contributions. First, it formally defines and discusses two different goal orderings, which are called the reasonable and the forced ordering. Both orderings are defined for simple STRIPS operators as well as for more complex ADL operators supporting negation and conditional effects. The complexity of these orderings is investigated and their practical relevance is discussed. Secondly, two different methods to compute reasonable goal orderings are developed. One of them is based on planning graphs, while the other investigates the set of actions directly. Finally, it is shown how the ordering relations, which have been derived for a given set of goals G, can be used to compute a so-called goal agenda that divides G into an ordered set of subgoals. Any planner can then, in principle, use the goal agenda to plan for increasing sets of subgoals. This ...
124|A Heuristic for Domain Independent Planning and its Use in an Enforced Hill-climbing Algorithm |  We present a new heuristic method to evaluate planning states, which is based on solving a relaxation of the planning problem. The solutions to the relaxed problem give a good estimate for the length of a real solution, and they can also be used to guide action selection during planning. Using these informations, we employ a search strategy that combines Hill-climbing with systematic search. The algorithm is complete on what we call deadlock-free domains. Though it does not guarantee the solution plans to be optimal, it does find close to optimal plans in most cases. Often, it solves the problems almost without any search at all. In particular, it outperforms all state-of-the-art planners on a large range of domains. 
125|Conditional Effects in Graphplan|Graphplan has attracted considerable interest because of its extremely high performance, but the algorithm&#039;s inability to handle action representations more expressive than STRIPS is a major limitation. In particular, extending Graphplan to handle conditional effects is a surprisingly subtle enterprise. In this paper, we describe the space of possible alternatives, and then concentrate on one particular approach we call factored expansion. Factored expansion splits an action with conditional effects into several new actions called components, one for each conditional effect. Because these action components are not independent, factored expansion complicates both the mutual exclusion and backward chaining phases of Graphplan. As compensation, factored expansion often produces dramatically smaller domain models than does the more obvious full-expansion into exclusive STRIPS actions. We present experimental...
126|Hybrid STAN: Identifying and Managing Combinatorial Optimisation Sub-problems in Planning|It is well-known that planning is hard but it  is less well-known how to approach the hard  parts of a problem instance eectively. Using  static domain analysis techniques we can identify  and abstract certain combinatorial subproblems  from a planning instance, and deploy  specialised technology to solve these subproblems  in a way that is integrated with the  broader planning activities. We have developed  a hybrid planning system (STAN4) which  brings together alternative planning strategies  and specialised algorithms and selects between  them according to the structure of the planning  domain. STAN4 participated successfully  in the AIPS-2000 planning competition. We  describe how sub-problem abstraction is done,  with particular reference to route-planning abstraction,  and present some of the competition  data to demonstrate the potential power of the  hybrid approach.  1 Introduction  The knowledge-sparse, or domain-independent, planning community is often criticised for its o...
127|GRT: A Domain Independent Heuristic for STRIPS Worlds based on Greedy Regression Tables|This paper presents Greedy Regression Tables (GRT), a new domain  independent heuristic for STRIPS worlds. The heuristic can be used to guide  the search process of any state-space planner, estimating the distance between  each intermediate state and the goals. At the beginning of the problem solving  process a table is created, the records of which contain the ground facts of the  domain, among with estimates for their distances from the goals. Additionally,  the records contain information about interactions that occur while trying to  achieve different ground facts simultaneously. During the search process, the  heuristic, using this table, extracts quite accurate estimates for the distances  between intermediate states and the goals. A simple best-first search planner  that uses this heuristic has been implemented in C++ and has been tested on  several &#034;classical&#034; problem instances taken from the bibliography and on some  new taken from the AIPS-98 planning competition. Our planner has proved to  be faster in all of the cases, finding also in most (but not all) of the cases shorter  solutions.
128|Goal ordering in partially ordered plans|t-o. _E
129|Elevator Control as a Planning Problem|The synthesis of elevator control commands is a difficult problem when new service requirements such as VIP service, access restrictions, nonstop travel etc. have to be individually tailored to each passenger. AI planning technology offers a very elegant and flexible solution because the possible actions of a control system can be made explicit and their preconditions and e ects can be speci ed using expressive representation formalisms. Based on the specification, a planner can flexibly synthesize the required control and changes in the specification do not require any reimplementation of the control software. In this paper, we describe the application and investigate how currently available domain-independent planning formalisms can cope with it.
130|Ordering problem subgoals|Most past research work on problem subgoal ordering are of a heuristic nature and very little attempt has been made to reveal the inherent relationship between subgoal ordering constraints and problem operator schemata. As a result, subgoal ordering strategies which have been developed tend to be either overly committed, imposing ordering on subgoals subjectively or randomly, or overly restricted, ordering subgoals only after a violation of ordering constraints becomes explicit during the development of a problem solution or plan. This paper proposes a new approach characterized by a formal representation of subgoal ordering constraints which makes explicit the relationship between the constraints and the problem operator schemata. Following this approach, it becomes straightforward to categorize various types of subgoal ordering constraints, to manipulate or extend the relational representation of the constraints, to systematically detect important subgoal ordering constraints from problem specifications, and to apply the detected constraints to multiple problem instances. 1
131|Plateaus and Plateau Search in Boolean Satisfiability Problems: When to Give Up Searching and Start Again|: We empirically investigate the properties of the search space and the behavior of hill-climbing search for solving hard, random Boolean satisfiability problems. In these experiments it was frequently observed that rather than attempting to escape from plateaus by extensive search, it was better to completely restart from a new random initial state. The optimum point to terminate search and restart was determined empirically over a range of problem sizes and complexities. The growth rate of the optimum cutoff is faster than linear with the number of features, although the exact growth rate was not determined. Based on these empirical results, a simple run-time heuristic is proposed to determine when to give up searching a plateau and restart. This heuristic closely approximates the empirically determined optimum values over a range of problem sizes and complexities, and consequently allows the search algorithm to automatically adjust its strategy for each particular problem without pr...
132|Solving Complex Planning Tasks Through Extraction of Subproblems|The paper introduces an approach to derive a total ordering  between increasing sets of subgoals by defining  a relation over atomic goals. The ordering is represented  in a so-called goal agenda that is used by the  planner to incrementally plan for the increasing sets  of subgoals. This can lead to an exponential complexity  reduction because the solution to a complex planning  problem is found by solving easier subproblems.  Since only a polynomial overhead is caused by the goal  agenda computation, a potential exists to dramatically  speed up planning algorithms as we demonstrate in the  empirical evaluation.  Introduction  How to effectively plan for interdependent subgoals has been in the focus of AI planning research for a very long time (Chapman 1987). But until today planners have made only some progress to solve larger sets of subgoals and scalability of classical planning systems is still a problem.  Previous approaches fell into two categories: On one hand, one can focus on...
133|Solving the Entailment Problem in the Fluent Calculus using Binary Decision Diagrams|The paper is an exercise in formal program development. We rigorously show how planning  problems encoded as entailment problems in the fluent calculus can be mapped onto  satisfiability problems for propositional formulas, which in turn can be mapped to the problem  to find models using binary decision diagrams. The mapping is shown to be sound and  complete. Preliminary experimental results of an implementation are discussed.
134|Extracting Route-Planning: First Steps in Automatic Problem Decomposition|The divide between knowledge-intensive, domaindependent  planning and knowledge-sparse, searchintensive  domain-independent planning has a long history  in the planning community. It has been argued  that the only route to ecient planning with real application  domains is to exploit human expert knowledge,  encoded in the domain structures used by a planner.  On the other hand, it has also been observed  that the exploitationof knowledge in this way restricts  planners to domains in which the knowledge is valid  and available, and signicantly reduces the extent to  which the planners can be exibly redeployed. In this  paper, we present the rst steps in a direction which  might bridge this gap: the automatic identication  and exploitation of fundamental behaviours within a  knowledge-sparse encoding of a planning domain, with  the objective of allowing specialised problem-solving  technology access to the components of a problem for  which they are best suited, while not increasing ...
135|Heuristic search planning with BDDs|Abstract. In this paper we study traditional and enhanced BDDbased exploration procedures capable of handling large planning problems. On the one hand, reachability analysis and model checking have eventually approached AI-Planning. Unfortunately, they typically rely on uninformed blind search. On the other hand, heuristic search and especially lower bound techniques have matured in effectively directing the exploration even for large problem spaces. Therefore, with heuristic symbolic search we address the unexplored middle ground between single state and symbolic planning engines to establish algorithms that can gain from both sides. To this end we implement and evaluate heuristics found in state-of-the-art heuristic single-state search planners. 1
136|On the Instantiation of ADL Operators Involving Arbitrary First-Order Formulas | The generation of the set of all ground actions for a given set of ADL operators, which are allowed to have conditional effects and preconditions that can be represented using arbitrary first-order formulas is a complex process which heavily influences the performance of any planner or pre-planning analysis method. The paper describes a sophisticated instantiation procedure that determines so-called inertia in a given problem representation and uses them to perform simplifications of formulas during the instantiation process. As a result, many inapplicable actions are detected and ruled out from the domain representation yielding a much smaller search space for the planner. 
137|Suffix arrays: A new method for on-line string searches|A new and conceptually simple data structure, called a suffix array, for on-line string searches is intro-duced in this paper. Constructing and querying suffix arrays is reduced to a sort and search paradigm that employs novel algorithms. The main advantage of suffix arrays over suffix trees is that, in practice, they use three to five times less space. From a complexity standpoint, suffix arrays permit on-line string searches of the type, &#034;Is W a substring of A?&#034; to be answered in time O(P + log N), where P is the length of W and N is the length of A, which is competitive with (and in some cases slightly better than) suffix trees. The only drawback is that in those instances where the underlying alphabet is finite and small, suffix trees can be constructed in O(N) time in the worst case, versus O(N log N) time for suffix arrays. However, we give an augmented algorithm that, regardless of the alphabet size, constructs suffix arrays in O(N) expected time, albeit with lesser space efficiency. We believe that suffix arrays will prove to be better in practice than suffix trees for many applications.  
138|Linear pattern matching algorithms|In 1970, Knuth, Pratt, and Morris [1] showed how to do basic pattern matching in linear time. Related problems, such as those discussed in [4], have previously been solved by efficient but sub-optimal algorithms. In this paper, we introduce an interesting data structure called a bi-tree. A linear time algorithm  for obtaining a compacted version of a bi-tree associated with a given string is presented. With this construction as the basic tool, we indicate how to solve several pattern matching problems, including some from [4], in linear time. 
139|All-Against-All Sequence Matching |In this paper we present an algorithm which attempts to align pairs of subsequences from a database of DNA sequences. The algorithm simulates the classical dynamic programming alignment algorithm over a digital index of the database. The running time of the algorithm is subquadratic on average with respect to the database size. A similar algorithm solves the approximate string matching problem in sublinear average time. 1 Introduction  An all-against-all matching is defined as an attempt to find an alignment for each possible subsequence against each other possible subsequence in a DNA or peptide database. These alignments are done with the standard methods of dynamic programming (Needleman &amp; Wunsch algorithm [NW70]) and are based on a matrix describing the likelihood of homology between pairs of entries in the sequence (e.g. a Dayhoff matrix [DSO78]). The output of this matching will consist of all pairs of subsequences which achieve a given level of similarity. Note that we are not a...
141|The principles of psychology|This Thesis is brought to you for free and open access. It has been accepted for inclusion in University Honors Theses by an authorized administrator of
142|A Model of Saliency-based Visual Attention for Rapid Scene Analysis|A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail. Index terms: Visual attention, scene analysis, feature extraction, target detection, visual search. \Pi I. Introduction Primates have a remarkable ability to interpret complex scenes in real time, despite the limited speed of the neuronal hardware available for such tasks. Intermediate and higher visual processes appear to select a subset of the available sensory information before further processing [1], most likely to reduce the complexity of scene analysis [2]. This selection appears to be implemented in the ...
143|The Laplacian Pyramid as a Compact Image Code| We describe a technique for image encoding in which local operators of many scales but identical shape serve as the basis functions. The representation differs from established techniques in that the code elements are localized in spatial frequency as well as in space. Pixel-to-pixel correlations are first removed by subtracting a lowpass filtered copy of the image from the image itself. The result is a net data compression since the difference, or error, image has low variance and entropy, and the low-pass filtered image may represented at reduced sample density. Further data compression is achieved by quantizing the difference image. These steps are then repeated to compress the low-pass image. Iteration of the process at appropriately expanded scales generates a pyramid data structure. The encoding process is equivalent to sampling the image with Laplacian operators of many scales. Thus, the code tends to enhance salient image features. A further advantage of the present code is that it is well suited for many image analysis tasks as well as for image compression. Fast algorithms are described for coding and decoding. A
144|Shiftable Multi-scale Transforms|Orthogonal wavelet transforms have recently become a popular representation for multiscale signal and image analysis. One of the major drawbacks of these representations is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal, and in two dimensions, rotations of the input signal. We formalize these problems by defining a type of translation invariance that we call &#034;shiftability&#034;. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be considered in the context of other domains, particularly orientation and scale. We explore &#034;jointly shiftable&#034; transforms that are simultaneously shiftable in more than one domain. Two examples of jointly shiftable transforms are designed and implemented: a one-dimensional tran...
145|Preattentive texture discrimination with early vision mechanisms|mechanisms
146|Sustained and transient components of focal visual attention|Abstract-Human observers fixated the center of a search array and were required to discriminate the color of an odd target if it was present. The array consisted of horizontal or vertical black or white bars. In the simple case, only orientation was necessary to define the odd target, whereas in the conjunctive case, both orientation and color were necessary. A cue located at the critical target position was either visible all the time (sustained cuing) or it appeared at a short variable delay before the array presentation (transient cuing). Sustained visual cuing enhanced perception greatly in the conjunctive, but not in the simple condition. Perception of the odd target in the conjunctive display was improved even further by transient cuing, and peak discrimination performance occurred if the cue preceded the target array by 70-150 msec. Longer delays led to a marked downturn in performance. Control experiments indicated that this transient attentional component was independent of the observers ’ prior knowledge of target position and was not subject to voluntary control. We provide evidence to suggest hat the transient component does not originate at the earliest stages of visual processing, since it could not be extended in duration by flickering the cue, nor did it require a local sensory transient o trigger its onset. Neither the variation in retinal eccentricity nor changing the paradigm to a vernier acuity task altered the basic pattern of results. Our findings indicate the existence of a sustained and a transient component of attention, and we hypothesize that of the two, the transient component is operative at an earlier stage of visual cortical processing. Focal attention Visual search Pattern recognition Vernier acuity
147|A Comparison of Feature Combination Strategies for Saliency-Based Visual Attention Systems|Bottom-up or saliency-based visual attention allows primates to detect non-specific conspicuous targets in cluttered scenes. A classical metaphor, derived from electrophysiological and psychophysical studies, describes attention as a rapidly shiftable &#034;spotlight&#034;. The model described here reproduces the attentional scanpaths of this spotlight: Simple multi-scale &#034;feature maps&#034; detect local spatial discontinuities in intensity, color, orientation or optical flow, and are combined into a unique &#034;master&#034; or &#034;saliency&#034; map. The saliency map is sequentially scanned, in order of decreasing saliency, by the focus of attention. We study the problem of combining feature maps, from different visual modalities and with unrelated dynamic ranges (such as color and motion), into a unique saliency map. Four combination strategies are compared using three databases of natural color images: (1) Simple normalized summation, (2) linear combination with learned weights, (3) global non-linear normalization...
148|An Active Vision Architecture based on Iconic Representations|Active vision systems have the capability of continuously interacting with the environment. The rapidly changing environment of such systems means that it is attractive to replace static representations with visual routines that compute information on demand. Such routines place a premium on image data structures that are easily computed and used. The purpose of this paper is to propose a general active vision architecture based on efficiently computable iconic representations. This architecture employs two primary visual routines, one for identifying the visual image near the fovea (object identification), and another for locating a stored prototype on the retina (object location). This design allows complex visual behaviors to be obtained by composing these two routines with different parameters. The iconic representations are comprised of high-dimensional feature vectors obtained from the responses of an ensemble of Gaussian derivative spatial filters at a number of orientations and...
149|Clustered intrinsic connections in cat visual cortex|The intrinsic connections of the cortex have long been known to run vertically, across the cortical layers. In the present study we have found that individual neurons in the cat primary visual cortex can communicate over suprisingly long distances horizontally (up to 4 mm), in directions parallel to the cortical surface. For all of the cells having widespread projections, the collaterals within their axonal fields were distributed in repeating clusters, with an average periodicity of 1 mm. This pattern of extensive clustered projections has been revealed by combining the techniques of intracellular recording and injection of horseradish peroxidase with three-dimensional computer graphic reconstructions. The clustering pattern was most apparent when the cells were rotated to present a view parallel to the cortical surface. The pattern was observed in more than half of the pyramidal and spiny stellate cells in the cortex and was seen in all cortical layers. In our sample, cells made distant connections within their own layer and/or within another layer. The axon of one cell had clusters covering the same area in two layers, and the clusters in the deeper layer were located under those in the upper layer, suggesting a relationship between the clustering phenomenon and columnar cortical architecture. Some pyramidal cells did not project into the white matter,
150|Overcomplete steerable pyramid filters and rotation invariance|A given (overcomplete) discrete oriented pyramid may be converted into a steerable pyramid by interpolation. We present a technique for deriving the optimal interpolation functions (otherwise called steering coefficients). The proposed scheme is demonstrated on a computationally efficient oriented pyramid, which is a variation on the Burt and Adelson pyramid. We apply the generated steerable pyramid to orientation-invarianttexture analysis to demonstrate its excellent rotational isotropy. High classification rates and precise rotation identification are demonstrated. 1
151|Functional anatomy of macaque striate cortex. V. Spatial frequency|Macaque monkeys were shown retinotopically-specific vi-sual stimuli during %-2-deoxy-&amp;glucose (DG) infusion in a study of the retinotopic organization of primary visual cortex (Vl). In the central half of VI, the cortical magnification was found to be greater along the vertical than along the hori-zontal meridian, and overall magnification factors appeared to be scaled proportionate to brain size across different species. The cortical magnification factor (CMF) was found to reach a maximum of about 15 mm/deg at the represen-tation of the fovea, at a point of acute curvature in the Vl-V2 border. We find neither a duplication nor an overrepre-sentation of the vertical meridian. The magnification factor did not appear to be doubled in a direction perpendicular to the ocular dominance strips; it may not be increased at all. The DG borders in parvorecipient layer 4Cb were found to
152|Eye position effects on visual, memory, and saccade-related activity in areas LIP and 7a of macaque|We studied the effect of eye position on the light-sensitive, memory, and saccade-related activities of neurons of the lateral intraparietal area and area 7a in the posterior parietal cortex of rhesus monkeys. A majority of the cells showed significant effects of eye position, for each of the 3 types of response. The direction tuning of the light-sensitive, memory and saccade responses did not change with eye position but the magnitude of the response did. Since previous work showed a similar effect for the light-sensitive response of area 7a neurons (Andersen and Mountcastle, 1983; Ander-sen et al., 1985b), the present results indicate that this mod-ulating effect of eye position may be a general one, as it is found in 3 types of responses in 2 cortical areas. Gain fields were mapped by measuring the effect of eye position on the magnitude of the response at 9 different eye positions for
153|Incorporating Prior Information in Machine Learning by Creating Virtual Examples|One of the key problems in supervised learning is the insufficient size of the training set. The natural way for an intelligent learner to counter this problem and successfully generalize is to exploit prior information that may be available about the domain or that can be learned from prototypical examples. We discuss the notion of using prior knowledge by creating virtual examples and thereby expanding the effective training set size. We show that in some contexts, this idea is mathematically equivalent to incorporating the prior knowledge as a regularizer, suggesting that the strategy is well-motivated. The process of creating virtual examples in real world pattern recognition tasks is highly non-trivial. We provide demonstrative examples from object recognition and speech recognition to illustrate the idea.  1 Learning from Examples  Recently, machine learning techniques have become increasingly popular as an alternative to knowledge-based approaches to artificial intelligence pro...
154|Control of selective visual attention: Modelling the “where” pathway|Intermediate and higher vision processes require selection of a sub-set of the available sensory information before further processing. Usually, this selection is implemented in the form of a spatially circumscribed region of the visual field, the so-called &#034;focus of at-tention &#034; which scans the visual scene dependent on the input and on the attentional state of the subject. We here present a model for the control of the focus of attention in primates, based on a saliency map. This mechanism is not only expected to model the functional-ity of biological vision but also to be essential for the understanding of complex scenes in machine vision. 1 Introduction: &#034;What &#034; and &#034;Where &#034; In Vision It is a generally accepted fact that the computations of early vision are massively parallel operations, i.e., applied in parallel to all parts of the visual field. This high degree of parallelism cannot be sustained in in~ermediate and higher vision because
155|Multimodal integration for the representation of space in the posterior parietal cortex|The posterior parietal cortex has long been considered an a`ssociation&#039;area that combines information from di¡erent sensory modalities to form a cognitive representation of space. However, until recently little has been known about the neural mechanisms responsible for this important cognitive process. Recent experi-ments from the author&#039;s laboratory indicate that visual, somatosensory, auditory and vestibular signals are combined in areas LIP and 7a of the posterior parietal cortex. The integration of these signals can repre-sent the locations of stimuli with respect to the observer and within the environment. Area MSTd combines visual motion signals, similar to those generated during an observer&#039;s movement through the environment, with eye-movement and vestibular signals. This integration appears to play a role in specifying the path on which the observer is moving. All three cortical areas combine di¡erent modalities into common spatial frames by using a gain-¢eld mechanism. The spatial representations in areas LIP and 7a appear to be important for specifying the locations of targets for actions such as eye movements or reaching; the spatial representation within area MSTd appears to be important for navigation and the perceptual stabi-lity of motion signals. 1.
156|Withdrawing attention at little or no cost: detection and discrimination tasks. Percept Psychophys|Weused a concurrent-task paradigm to investigate the attentional cost of simple visual tasks. As in earlier studies, we found that detecting a unique orientation in an array of oriented elements (&#034;pop-out&#034;) carries little or no attentional cost. Surprisingly, this is true at all levels of performance and holds even when pop-out is barely discriminable. Wediscuss this finding in the context of our previous re-port that the attentional cost of stimulus detection is strongly influenced by the presence and nature of other stimuli in the display (Braun, 1994b). For discrimination tasks, we obtained a similarly mixed outcome: Discrimination of letter shape carried a high attentional cost whereas discrimination of color and orientation did not. Taken together, these findings lead us to modify our earlier position on the at-tentional costs of detection and discrimination tasks (Sagi &amp; Julesz, 1985). We now believe that ob-servers enjoy a significant degree of &#034;ambient &#034; visual awareness outside the focus of attention, per-mitting them to both detect and discriminate certain visual information. We hypothesize that the information in question is selected by a competition for saliency at the level of early vision. It has long been recognized that visual perception is in-fluenced by the observer&#039;s attentional state (Helmholtz, 1850/1962; James, 1890/1981). Psychophysical studies show that attention enhances visual sensitivity for stim-uli that are relevant to the observer and his/her behavior
157|Gapped Blast and PsiBlast: a new generation of protein database search programs|The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSIBLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.
158|Attention, similarity, and the identification-Categorization Relationship|A unified quantitative approach to modeling subjects &#039; identification and categorization of multidimensional perceptual stimuli is proposed and tested. Two subjects identified and categorized the same set of perceptually confusable stimuli varying on separable dimensions. The identification data were modeled using Sbepard&#039;s (1957) multidimensional scaling-choice framework. This framework was then extended to model the subjects &#039; categorization performance. The categorization model, which generalizes the context theory of classification developed by Medin and Schaffer (1978), assumes that subjects store category exemplars in memory. Classification decisions are based on the similarity of stimuli to the stored exemplars. It is assumed that the same multidimensional perceptual representation underlies performance in both the identification and Categorization paradigms. However, because of the influence of selective attention, similarity relationships change systematically across the two paradigms. Some support was gained for the hypothesis that subjects distribute attention among component dimensions so as to optimize categorization performance. Evidence was also obtained that subjects may have augmented their category representations with inferred exemplars. Implications of the results for theories of multidimensional scaling and categorization are discussed. 
159|Attention and learning processes in the identification and categorization of integral stimuli|The relationship between subjects &#039; identification and categorization learning of integral-dimension stimuli was studied within the framework of an exemplar-based generalization model. The model was used to predict subjects &#039; learning in six different categorization conditions on the basis of data obtained in a single identification learning condition. A crucial assumption in the model is that because of selective attention to component dimensions, similarity relations may change in systematic ways across different experimental contexts. The theoretical analysis provided evidence that, at least under unspeeded conditions, selective attention may play a critical role in determining the identification-categorization relationship for integral stimuli. Evidence was also provided that similarity among exemplars decreased as a function of identification learning. Various alternative classification models, including prototype, multiple-prototype, average distance, and &#034;value-on-dimensions&#034; models, were unable to account for the results. This article seeks to characterize performance relations between the two fundamental classification paradigms of identification and categorization. Whereas in an identification paradigm people identify stimuli as unique items (a one-to-one
160|Strategies and classification learning|How do strategies affect the learning of categories that lack necessary and suf-ficient attributes? The usual answer is that different strategies correspond to different models. In this article we provide evidence for an alternative view— Strategy variations induced by instructions affect only the amount of information represented about attributes, not the process operating on these representations. The experiment required subjects to classify schematic faces into two categories. Three groups of subjects worked with different sets of instructions: roughly, form
161|An evaluation of the identification|ollections of tiny, inexpensive wire-less sensor nodes capable of contin-uous, detailed, and unobtrusive measurement have attracted much attention in the past few years.1 Prototypes exist for applications such as early detection of factory equipment failure, opti-mization of building energy use, habitat mon-itoring, microclimate monitoring, and moni-toring structural integrity against earthquakes. Unfortunately, the very prop-erties that make sensor nodes attractive for these applica-tions—low cost, small size, wireless functioning, and timely,
162|An experimental and theoretical investigation of the constant-ratio rule and other models of visual letter confusion|The constant-ratio rule (CRR) and four interpretations of R. D. Lute’s (In R. D. Lute,
163|Classification in well-defined and ill-defined categories: Evidence for common processing strategies|had criterial features and that category membership could be determined by logical rules for the combination of features. More recent theories have assumed that categories have an ill-defined structure and have proposed probabilistic or global similarity models for the verification of category membership. In the experiments reported here, several models of categorization were compared, using one set of categories having criterial features and another set having an ill-defined structure. Schematic faces were used as exemplars in both cases. Because many models depend on distance in a multidimensional space for their predictions, in Experiment 1 a multidimensional scaling study was performed using the faces of both sets as stimuli. In Experiment 2, subjects learned the category membership of faces for the cate-gories having criterial features. After learning, reaction times for category verification and typicality judgments were obtained. Subjects also judged the similarity of pairs of faces. Since these categories had characteristic as well as defining features, it was possible to test the predictions of the feature comparison model (Smith et al.), which
164|A psychophysical approach to dimensional separability|Combinations of some physically independent dimensions appear to fuse into a single perceptual attribute, whereas combinations of other dimensions leave the dimensions perceptually distinct. This apparent difference in the perceived dis-tinctiveness of visual dimensions has previously been explained by the postulation of two types of internal representations-integral and separable. It is argued that apparent integrality, as well as its intermediate forms, can result from a single type of representation (the separable type), due to various degrees of correspondence between physical and separable psychological dimensions. Three experiments tested predictions of this new conceptualization of dimensional separability. Ex-periment 1 demonstrated that a physical dimension corresponding to a separable psychological dimension did not produce interference, whereas a physical di-mension not corresponding to a separable psychological dimension did produce interference. Experiment 2 showed that the pattern of results obtained in Exper-iment 1 could not be accounted for by similarity relations between stimuli. Ex-periment 3 showed that degrees of correspondence could account for different
165|Searching Distributed Collections With Inference Networks|The use of information retrieval systems in networked environments raises a new set of issues that have received little attention. These issues include ranking document collections for relevance to a query, selecting the best set of collections from a ranked list, and merging the document rankings that are returned from a set of collections. This paper describes methods of addressing each issue in the inference network model, discusses their implementation in the INQUERY system, and presents experimental results demonstrating their effectiveness.  
166|Okapi at TREC-3|this document length correction factor is #global&#034;: it is added at the end, after the weights for the individual terms have been summed, and is independentofwhich terms match.
167|The INQUERY Retrieval System|As larger and more heterogeneous text databases become available, information retrieval research will depend on the development of powerful, efficient and flexible retrieval engines. In this paper, we describe a retrieval system (INQUERY) that is based on a probabilistic retrieval model and provides support for sophisticated indexing and complex query formulation. INQUERY has been used successfully with databases containing nearly 400,000 documents. 1 Introduction  The increasing interest in sophisticated information retrieval (IR) techniques has led to a number of large text databases becoming available for research. The size of these databases, both in terms of the number of documents in them, and the length of the documents that are typically full text, has presented significant challenges to IR researchers who are used to experimenting with two or three thousand document abstracts. In order to carry out research with different types of text representations, retrieval models, learni...
168|Evaluation of an Inference Network-Based Retrieval Model|The use of inference networks to support document retrieval is introduced. A network-based retrieval model is described and compared to conventional probabilistic and Boolean models. The performance of a retrieval system based on the inference network model is evaluated and compared to performance with conventional retrieval models,
169|Latent Semantic Indexing (LSI) and TREC-2  (1994) |this paper. The &#034;ltc&#034; weights were computed on this matrix. 3.2 SVD analysis
170|Information Retrieval Systems for Large Document Collections|Practical information retrieval systems must manage large volumes of data, often divided into several collections that may be held on separate machines. Techniques for locating matches to queries must therefore consider identification of probable collections as well as identification of documents that are probable answers. Furthermore, the large amounts of data involved motivates the use of compression, but in a dynamic environment compression is problematic, because as new text is added the compression model slowly becomes inappropriate. In this paper we describe solutions to both of these problems. We show that use of centralised blocked indexes can reduce overall query processing costs in a multi-collection environment, and that careful application of text compression techniques allow collections to grow by several orders of magnitude without recompression becoming necessary. 1 Introduction  Practical information systems are required to store many gigabytes of data while supporting ...
171|Distributed Indexing: A Scalable Mechanism for Distributed Information Retrieval|Despite blossoming computer network bandwidths and the emergence of hypertext and CD-ROM databases, little progress has been made towards uniting the world&#039;s library-style bibliographic databases. While a few advanced distributed retrieval systems can broadcast a query to hundreds of participating databases, experience shows that local users almost always clog library retrieval systems. Hence broadcast remote queries will clog nearly every system. The premise of this work is that broadcast-based systems do not scale to world-wide systems. This project describes an indexing scheme that will permit thorough yet efficient searches of millions of retrieval systems. Our architecture will work with an arbitrary number of indexing companies and information providers, and, in the market place, could provide economic incentive for cooperation between database and indexing services. We call our scheme distributed indexing, and believe it will help researchers disseminate and locate both publishe...
172|TREC-3 Ad-Hoc, Routing Retrieval and Thresholding Experiments using PIRCS|The PIRCS retrieval system has been upgraded in TREC-3 to handle the full English collections of 2 GB in an efficient manner. For ad-hoc retrieval, we use recurrent spreading of activation in our network to implement query learning and expansion based on the best-ranked subdocuments of an initial retrieval. We also augment our standard retrieval algorithm with a soft-Boolean component. For routing, we use learning from signal-rich short documents or subdocument segments. For the optional thresholding experiment, we tried two approaches to transforming retrieval status values (RSV&#039;s) so that they could be used to partition documents into retrieved and nonretrieved sets. The first method normalizes RSV&#039;s using a query self-retrieval score. The second, which requires training data, uses logistic regression to convert RSV&#039;s into estimates of probability of relevance. Overall, our results are highly competitive with those of other participants. 1. INTRODUCTION  PIRCS is an experimental info...
173|Using Discriminant Eigenfeatures for Image Retrieval|This paper describes the automatic selection of features from an image training set using the theories of multi-dimensional linear discriminant analysis and the associated optimal linear projection. We demonstrate the effectiveness of these Most Discriminating Features for view-based class retrieval from a large database of widely varying real-world objects presented as &#034;well-framed&#034; views, and compare it with that of the principal component analysis.   
174|View-Based and Modular Eigenspaces for Face Recognition|In this work we describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of o(10^3) faces. The problem of  recognition under general viewing orientation is also explained. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose, mouth, in a eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demostrated. 
175|Probabilistic Visual Learning for Object Detection|We present an unsupervised technique for visual learning which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for a unimodal distribution) and a multivariate Mixture-of-Gaussians model (for multimodal distributions). These probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition. This learning technique is tested in experiments with modeling and subsequent detection of human faces and non-rigid objects such as hands.
176|Face Recognition From One Example View|To create a pose-invariant face recognizer, one strategy is the view-based approach, which uses a set of example views at different poses. But what if we only have one example view available, such as a scanned passport photo -- can we still recognize faces under different poses? Given one example view at a known pose, it is still possible to use the view-based approach by exploiting prior knowledge of faces to generate  virtual views, or views of the face as seen from different poses. To represent prior knowledge, we use 2D example views of prototype faces under different rotations. We will develop example-based techniques for applying the rotation seen in the prototypes to essentially &#034;rotate&#034; the single real view which is available. Next, the combined set of one real and multiple virtual views is used as example views in a view-based, pose-invariant face recognizer. Our experiments suggest that for expressing prior knowledge of faces, 2D example-based approaches should be considered ...
177|Nonlinear manifold learning for visual speech recognition|A technique for representing and learning smooth nonlinear manifolds is presented and applied to several lip reading tasks. Given a set of points drawn from a smooth manifold in an abstract feature space, the technique is capable of determining the structure of the surface and offinding the closest manifold point to a given query point. We use this technique to learn the &#034;space of lips &#034; in a visual speech recognition task. The learned manifold is used for tracking and extracting the lips, for interpolating between frames in an image sequence and for providing features for recognition. We describe a system based on Hidden Markov Models and this learned lip manifold that significantly improves the performance of acoustic speech recognizers in degraded environments. We also present preliminary results on a purely visual lip reader. 1
178|Genetic Algorithms For Object Recognition In A Complex Scene|A real-world computer vision module must deal with a wide variety of environmental parameters. Object recognition, one of the major tasks of this vision module, typically requires a preprocessing step to locate objects in the scenes that ought to be recognized. Genetic algorithms are a search technique for dealing with a very large search space, such as the one encountered in image segmentation or object recognition. This work describes a technique for using genetic algorithms to combine the image segmentation and object recognition steps for a complex scene. The results show that this approach is a viable method for successfully combining the image segmentation and object recognition steps for a computer vision module. 1. INTRODUCTION  A central task of the computer vision module is to recognize objects from images of the machine&#039;s environment. Navigation systems require the localization and recognition of landmarks or threats; robotic systems must find objects to manipulate; image re...
179|A System for Combining Traditional Alphanumeric Queries with Content-Based Queries by Example in Image Databases|Large image databases are commonly employed in applications like criminal records, customs, plant root database, and voters&#039; registration database. Efficient and convenient mechanisms for database organization and retrieval are essential. A quick and easy-to-use interface is needed which should also mesh naturally with the overall image management system. In this paper we describe the design and implementation of an integrated image database system. This system offers support for both alphanumeric query, based on alphanumeric data attached to the image file, and content-based query utilizing image examples. Content-based retrieval, specifically Query by Image Example, is made possible by the SHOSLIF approach. Alphanumeric query is implemented by a collection of parsing and query modules. All these are accessible from within a user-friendly GUI.  Key words: Image database, content-based retrieval, query by example, alphanumeric query, similarity-based query.  1 Introduction  The abilit...
180|Efficient Image Retrieval using a Network with Complex Neurons|We describe a self-organizing framework for the generation of a network useful in content-based retrieval of image databases. The system uses the theories of optimal projection for optimal feature selection and a hierarchical network structure of the image database for rapid retrieval rates. We demonstrate the query technique on a large database of widely varying real-world objects in natural settings, and show the applicability of the approach even for large variability within a particular object class.  1 Introduction  The ability of computers to rapidly and successfully retrieve information from image databases based on the objects contained in the images has a direct impact on the progress of the revolution in communication precipitated with the increasing availability of digital video [4]. The complexity in the very nature of twodimensional image data gives rise to a host of problems that alphanumeric information systems were never designed to handle [1]. A central task of these m...
181|The eyes have it: A task by data type taxonomy for information visualizations| A useful starting point for designing advanced graphical user interjaces is the Visual lnformation-Seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. This paper offers a task by data type taxonomy with seven data types (one-, two-, three-dimensional datu, temporal and multi-dimensional data, and tree and network data) and seven tasks (overview, Zoom, filter, details-on-demand, relate, history, and extracts). 
182|Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Displays|This paper offers new principles for visual information seeking (VIS). A key concept is to support browsing, which is distinguished from familiar query composition and information retrieval because of its emphasis on rapid filtering to reduce result sets, progressive refinement of search parameters, continuous reformulation of goals, and visual scanning to identify results. VIS principles developed include: dynamic query filters (query parameters are rapidly adjusted with sliders, buttons, maps, etc.), starfield displays (two-dimensional scatterplots to structure result sets and zooming to reduce clutter), and tight coupling (interrelating query components to preserve display invariants and support progressive refinement combined with an emphasis on using search output to foster search input). A FilmFinder prototype using a movie database demonstrates these principles in a VIS environment.
183|Tree visualization with Tree-maps: A 2-d space-filling approach|this paper deals with a two-dimensional (2-d) space-filling approach in which each node is a rectangle whose area is proportional to some attribute such as node size. Research on relationships between 2-d images and their representation in tree structures has focussed on node and link representations of 2-d images. This work includes quad-trees (Samet, 1989) and their variants which are important in image processing. The goal of quad trees is to provide a tree representation for storage compression and efficient operations on bit-mapped images. XY-trees (Nagy &amp; Seth, 1984) are a traditional tree representation of two-dimensional layouts found in newspaper, magazine, or book pages. Related concepts include k-d trees (Bentley and Freidman, 1979), which are often explained with the help of a
184|Treemaps: a space-filling approach to the visualization of hierarchical information structures|This paper describes a novel methodfor the visualization of hierarchically structured information. The Tree-Map visualization technique makes 100 % use of the available display space, mapping the full hierarchy onto a rectangular region in a space-filling manner. This efficient use of space allows very large hierarchies to be displayed in their entirety and facilitates the presentation of semantic information. 1
185|The Table Lens: Merging Graphical and Symbolic Representations in an Interactive Focus+Context Visualization for Tabular Information|We present a new visualization, called the Table Lens, for visualizing and making sense of large tables. The visualization uses a focus context (fisheye) technique that works effectively on tabular information because it allows display of crucial label information and multiple distal focal areas. In addition, a graphical mapping scheme for depicting table contents has been developed for the most widespread kind of tables, the cases-by-variables table. The Table Lens fuses symbolic and graphical representations into a single coherent view that can be fluidly adjusted by the user. This fusion and interactivity enables an extremely rich and natural style of direct manipulation exploratory data analysis.
186|Dynamic Queries for Information Exploration: An Implementation and Evaluation|We designed, implemented and evaluated a new concept for direct manipulation of databases, called dynamic queries, that allows users to formulate queries with graphical widgets, such as sliders. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. Eighteen undergraduate chemistry students performed statistically significantly faster usingadynamicqueries interface compared to two interfaces both providing form fillin as input method, one with graphical visualization output and one with all-textual output. The interfaces were used to expore the periodic table of elements and search on their properties. 1. INTRODUCTION Mostdatabasesystems require the user to create andformulate a complex query, whichpresumes that the user is familiar with the logical structure of the database [4]. The queries on a database are usually expressed in high level query languages (such as SQL,QUEL). This works well for many applications, but it ...
187|Visualizing the non-visual: spatial analysis and interaction with information from test documents|This paper describes an approach to IV that involves spatializing text content for enhanced visual browsing and analysis. The application arena is large text document corpora such as digital libraries, regulations andprocedures, archived reports, etc. The basic idea is that text content from these sources may be transformed to a spatial representation that preserves informational characteristics from the documents. The spatial representation may then be visually browsed and analyzed in ways that avoid language processing and that reduce the analysts’ mental workload. The result is an interaction with text that more nearly resembles perception and action with the natural world than with the abstractions of written language. 1
188|Dynamic Queries for Visual Information Seeking|Dynamic queries are a novel approach to information seeking that may enable users to cope with information overload. They allow users to see an overview of the database, rapidly (100 msec updates) explore and conveniently filter out unwanted information. Users fly through information spaces by incrementally adjusting a query (with sliders, buttons, and other filters) while continuously viewing the changing results. Dynamic queries on the chemical table of elements, computer directories, and a real estate database were built and tested in three separate exploratory experiments. These results show statistically significant performance improvements and user enthusiasm more commonly seen with video games. Widespread application seems possible but research issues remain in database and display algorithms, and user interface design. Challenges include methods for rapidly displaying and changing many points, colors, and areas; multidimensional pointing; incorporation of sound and visual displ...
189|LifeLines: Visualizing Personal Histories|LifeLines provide a general visualization environment for personal histories that can be applied to medical and court records, professional histories and other types of biographical data. A one screen overview shows multiple facets of the records. Aspects, for example medical conditions or legal cases, are displayed as individual time lines, while icons indicate discrete events, such as physician consultations or legal reviews. Line color and thickness illustrate relationships or significance, rescaling tools and filters allow users to focus on part of the information. LifeLines reduce the chances of missing information, facilitate spotting anomalies and trends, streamline access to details, while remaining tailorable and easily transferable between applications. The paper describes the use of LifeLines for youth records of the Maryland Department of Juvenile Justice and also for medical records. User&#039;s feedback was collected using a Visual Basic prototype for the youth record. Techniq...
190|Graphical Fisheye Views|A fisheye camera lens is a very wide angle lens that magnifies nearby objects while shrinking distant objects. It is a valuable tool for seeing both &#034;local detail&#034; and &#034;global context&#034; simultaneously. This paper describes a system for viewing and browsing graphs using a software analog of a fisheye lens. We first show how to implement such a view using solely geometric transformations. We then describe a more general transformation that allows global information about the graph to affect the view. Our general transformation is a fundamental extension to previous research in fisheye views.  
191|The dynamic HomeFinder: evaluating dynamic queries in a real-estate information exploration system|We designed, implemented, and evaluated a new concept for visualizing and searching databases utilizing direct manipulation called dynarruc queries. Dynamic queries allow users to formulate queries by adjusting graphical widgets, such as sliders, and see the results immediately. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. User testing was done with eighteen undergraduate students who performed significantly faster using a dynamic queries interface compared to both a natural language system and paper printouts. The interfaces were used to explore a real-estate database and find homes meeting specific search criteria. 1
192|InfoCrystal: a visual tool for information retrieval|This paper introduces a novel representation, called the I n f o C r y s t a l T M,  that can be used as a v isual i za t ion tool as well as a visual query lan-g u a g e to help users search for information. The lnfoCrysta1 visualizes all the possible relation-ships among N concepts. Users can assign relevance weights to the concepts and use thresholding to select relationships of interest. The lnfocrystal allows users to specify Boolean as well as v e c t o r-s p a c e queries graphically. Arbitrarily complex queries can be created by using the 1nfoCrystals as building blocks and organizing them in a hierarchical structure. The 1nfoCrystal enables users to explore and filter information in a flexible, dynamic and interactive way.
193|IVEE: An Information Visualization &amp; Exploration Environment|The Information Visualization and Exploration Environment (IVEE) is a system for automatic creation of dynamic queries applications. IVEE imports database relations and automatically creates environments holding visualizations and query devices. IVEE offers multiple visualizations such as maps and starfields, and multiple query devices, such as sliders, alphasliders, and toggles. Arbitrary graphical objects can be attached to database objects in visualizations. Multiple visualizations may be active simultaneously. Users can interactively lay out and change between types of query devices. Users may retrieve details-on-demand by clicking on visualization objects. An HTML file may be provided along with the database, specifying how details-ondemand information should be presented, allowing for presentation of multimedia information in database objects. Finally, multiple IVEE clients running on separate workstations on a network can communicate by letting one users actions affect the visua...
194|The Information Mural: A Technique for Displaying and Navigating Large Information Spaces|Abstract—Information visualizations must allow users to browse information spaces and focus quickly on items of interest. Being able to see some representation of the entire information space provides an initial gestalt overview and gives context to support browsing and search tasks. However, the limited number of pixels on the screen constrain the information bandwidth and make it difficult to completely display large information spaces. The Information Mural is a two-dimensional, reduced representation of an entire information space that fits entirely within a display window or screen. The Mural creates a miniature version of the information space using visual attributes, such as gray-scale shading, intensity, color, and pixel size, along with antialiased compression techniques. Information Murals can be used as stand-alone visualizations or in global navigational views. We have built several prototypes to demonstrate the use of Information Murals in visualization applications; subject matter for these views includes computer software, scientific data, text documents, and geographic information. Index Terms—Information visualization, software visualization, data visualization, focus+context, navigation, browsers. 1 INFORMATION MURALS
195|Visdb: Database exploration using multidimensional visualization|In this paper we describe the VisDB system, which allows an exploration of large databases using visualization techniques. The goal of the system is to support the query specification process by using each pixel of the display to represent one data item of the database. By arranging and coloring the pixels according to the relevance of the data items with respect to the query, the user gets a visual impression of the resulting data set. Using sliders for each condition of the query, the user may change the query dynamically and receives immediate feedback from the visual representation of the resulting data set. Different visualization techniques are available for different stages of exploration. The first technique uses multiple windows for the different query parts, providing visual feedback for each part of the query and helping the user to understand the overall result. The second technique is an extension of the first one, providing additional information by assigning two dimensions to the axes. The third technique uses a grouping of dimensions and is designed to support a focused search on smaller data sets.
196|The Alphaslider: A Compact and Rapid Selector|Research has suggested that rapid, serial, visual presentation of text (RSVP) may be an effective way to scan and search through lists of text strings in search of words, names, etc. The Alphaslider widget employs RSVP as a method for rapidly scanning and searching lists or menus in a graphical user interface environment. The Alphaslider only uses an area less than 7 cm x 2.5 cm. The tiny size of the Alphaslider allows it to be placed on a credit card, on a control panel for a VCR, or as a widget in a direct manipulation based database interface. An experiment was conducted with four Alphaslider designs which showed that novice Alphaslider users could locate one item in a list of 10,000 film titles in 24 seconds on average, an expert user in about 13 seconds.  KEYWORDS: Alphaslider, widget, selection technology, menus, dynamic queries  INTRODUCTION  Selecting items from lists is a common task in today&#039;s society. New and exciting applications for selection technology are credit card siz...
197|The Continuous Zoom: A Constrained Fisheye Technique for Viewing and Navigating Large Information Spaces|Navigating and viewing large information spaces, such as hierarchically-organized networks from complex realtime systems, suffer the problems of viewing a large space on a small screen. Distorted-view approaches, such as fisheye techniques, have great potential to reduce these problems by representing detail within its larger context but introduce new issues of focus, transition between views and user disorientation from excessive distortion. We present a fisheyebased method which supports multiple focus points, enhances continuity through smooth transitions between views, and maintains location constraints to reduce the user’s sense of spatial disorientation. These are important requirements for the representation and navigation of networked systems in supervisory control applications. The method consists of two steps: a global allocation of space to rectangular sections of the display, based on scale factors, followed by degree-of-interest adjustments. Previous versions of the algorithm relied solely on relative scale factors to assign size; we present a new version which allocates space more efficiently using a dynamically calculated degree of interest. In addition to the automatic system sizing, manual user control over the amount of space assigned each area is supported. The amount of detail shown in various parts of the network is controlled by pruning the hierarchy and presenting those sections in summary form. KEYWORDS: graphical user interface, supervisory control systems, information space, hierarchical network, information visualization, fisheye view, navigation.
198|Enhanced Dynamic Queries via Movable Filters|Traditional database query systems allow users to construct complicated database queries from specialized database language primitives. While powerful and expressive, such systems are not easy to use, especially for browsing or exploring the data. Information visualization systems address this problem by providing graphical presentations of the data and direct manipulation tools for exploring the data. Recent work in this area has reported the value of dynamic queries coupled with two-dimensional data representations for progressive refinement of user queries. However, the queries generated by these systems are limited to conjunctions of global ranges of parameter values. In this paper, we extend dynamic queries by encoding each operand of the query as a Magic Lens filter. Compound queries can be constructed by overlapping the lenses. Each lens includes a slider and a set of buttons to control the value of the filter function and to define the compostion operation generated by overlapp...
199|Navigating Large Networks with Hierarchies|This paper is aimed at the exploratory visualization of networks where there is a strength or weight associated with each link, and makes use of any hierarchy present on the nodes to aid the investigation of large networks. It describes a method of placing nodes on the plane that gives meaning to their relative positions. The paper discusses how linking and interaction principles aid the user in the exploration. Two examples are given; one of electronic mail communication over eight months within a department, another concerned with changes to a large section of a computer program. I. THE PROBLEM It has almost become a clichŽ to start a paper with the observation that the amount of data in the world is growing rapidly, and that current efforts to extract useful information from data lag far behind the ability to create data. However the clichŽ is true, and no less so in the field of network analysis and visualization than in any other. In many areas, scientists are realizing that the tools they have been using are limited in utility when applied to large, information-rich networks. Not only are networks of interest large in terms of size (as measured by number of nodes or links between nodes), but also in terms of the data collected for each node or link. The ability to examine statistics on the nodes and relate them to the network is of crucial importance. Examples of areas in which the analysis of large networks is important include: i. Trade flows. The concern in this area is monitoring imports and exports of various products at several levels; international, interstate and local. Besides examining many types of trade goods, there is also strong interest in spotting temporal patterns. ii. Communication networks. This is an important and wide category, covering not only telecommunication networks, but also electronic mail (email), financial transaction, ATM/bank data transferal and other data distribution networks.
200|Using Aggregation and Dynamic Queries for Exploring Large Data Sets|When working with large data sets, users perform three primary types of activities: data manipulation, data analysis, and data visualization. The data manipulation process involves the selection and transformation of data prior to viewing. This paper addresses user goals for this process and the interactive interface mechanisms that support them. We consider three classes of data manipulation goals: controlling the scope (selecting the desired portion of the data), selecting the focus of attention (concentrating on the attributes of data that are relevant to current analysis), and choosing the level of detail (creating and decomposing aggregates of data). We use this classification to evaluate the functionality of existing data exploration interface techniques. Based on these results, we have expanded an interface mechanism called the Aggregate Manipulator (AM) and combined it with Dynamic Query (DQ) to provide complete coverage of the data manipulation goals. We use real estate sales data to demonstrate how the AM and DQ synergistically function in our interface.
201|Interacting with Huge Hierarchies: Beyond Cone Trees|This paper describes an implementation of a tool for visualizing and interacting with huge information hierarchies. Existing systems for visualizing huge hierarchies using cone trees &#034;break down&#034; once the hierarchy to be displayed exceeds roughly 1000 nodes, due to increasing visual clutter. This paper describes a system called fsviz which visualizes arbitrarily large hierarchies while retaining user control. This is accomplished by augmenting cone trees with several graphical and interaction techniques: usage-based filtering, animated zooming, handcoupled rotation, fish-eye zooming, coalescing of nodes, texturing, effective use of colour for depth cueing, and the applications of dynamic queries. The fsviz system also improves upon earlier cone tree visualization systems through a more elaborate node layout algorithm. This algorithm enhances the usefulness of cone tree visualization for large hierarchies by all but eliminating clutter.  Keywords: Information Visualization, Information ...
202|Using treemaps to visualize the Analytic Hierarchy Process|this article. References
203|Exploratory Access to Geographic Data Based on the Map-Overlay Metaphor|Many geographic information systems (GISs) attempt to imitate the manual process of laying transparent map layers over one another on a light table and analyzing the resulting configurations. While this map-overlay metaphor, familiar to many geo-scientists, has been used as a design principle for the underlying architecture of GISs, it has not yet been visually manifested at the user interface. To overcome this shortage, a new direct manipulation user interface for overlay-based GISs has been designed and prototyped. It is characterized by the separation of map layers into data cubes and map templates such that different thematic data can be combined and the same kind of data can be displayed in different formats. This paper introduces the conceptual objects that the user manipulates at the screen surface and discusses ways to visualize effectively the objects and operations upon them.
204|Visualizing Network Data|Networks are critical to modern society, and a thorough understanding of how they behave is crucial to their efficient operation. Fortunately, data on networks is plentiful; by visualizing this data, it is possible to greatly improve our understanding. Our focus is on visualizing the data associated with a network and not on simply visualizing the structure of the network itself. We begin with three static network displays; two of these use geographical relationships, while the third is a matrix arrangement that gives equal emphasis to all network links. Static displays can be swamped with large amounts of data; hence we introduce directmanipulation techniques that permit the graphs to continue to reveal relationships in the context of much more data. In effect, the static displays are parameterized so that interesting views may easily be discovered interactively. The software to carry out this network visualization is called SeeNet.  1. INTRODUCTION  We are currently in the midst of a...
205|Networks versus Markets in International Trade|I propose a network/search view of international trade in differentiated products. I present evidence that supports the view that proximity and common language/colonial ties are more important for differentiated products than for products traded on organized exchanges in matching international buyers and sellers, and that search barriers to trade are higher for differentiated than for homogeneous products. I also discuss alternative
206|CONTINENTAL TRADING BLOCS: ARE THEY NATURAL, OR SUPER-NATURAL?|Using the gravity model, we find evidence of three continental trading blocs: the Americas, Europe and Pacific Asia. Intra-regional trade exceeds what can be explained by the proximity of a pair of countries, their sizes and GNP/capitas, and whether they share a common border or language. We then turn from the econometrics to the economic welfare implications. Krugman has supplied an argument against a three-bloc world, assuming no transport costs, and another argument in favor, assuming prohibitively high transportation costs between continents. We complete the model for the realistic case where intercontinental transport costs are neither prohibitive nor zero. If transport costs are low, continental Free Trade Areas can reduce welfare. We call such blocs super-natural. Partial liberalization is better than full liberalization within regional Preferential Trading Arrangements, despite the GATT&#039;s Article 24. The super-natural zone occurs when the regionalization of trade policy exceeds what is justified by natural factors.
207|Convergence Properties of the Nelder-Mead Simplex Method in Low Dimensions|Abstract. The Nelder–Mead simplex algorithm, first published in 1965, is an enormously popular direct search method for multidimensional unconstrained minimization. Despite its widespread use, essentially no theoretical results have been proved explicitly for the Nelder–Mead algorithm. This paper presents convergence properties of the Nelder–Mead algorithm applied to strictly convex functions in dimensions 1 and 2. We prove convergence to a minimizer for dimension 1, and various limited convergence results for dimension 2. A counterexample of McKinnon gives a family of strictly convex functions in two dimensions and a set of initial conditions for which the Nelder–Mead algorithm converges to a nonminimizer. It is not yet known whether the Nelder–Mead method can be proved to converge to a minimizer for a more specialized class of convex functions in two dimensions. Key words. direct search methods, Nelder–Mead simplex methods, nonderivative optimization AMS subject classifications. 49D30, 65K05
208|On the Convergence of Pattern Search Algorithms|. We introduce an abstract definition of pattern search methods for solving nonlinear unconstrained optimization problems. Our definition unifies an important collection of optimization methods that neither computenor explicitly approximate derivatives. We exploit our characterization of pattern search methods to establish a global convergence theory that does not enforce a notion of sufficient decrease. Our analysis is possible because the iterates of a pattern search method lie on a scaled, translated integer lattice. This allows us to relax the classical requirements on the acceptance of the step, at the expense of stronger conditions on the form of the step, and still guarantee global convergence.  Key words. unconstrained optimization, convergence analysis, direct search methods, globalization strategies, alternating variable search, axial relaxation, local variation, coordinate search, evolutionary operation, pattern search, multidirectional search, downhill simplex search  AMS(M...
209|Direct Search Methods On Parallel Machines|. This paper describes an approach to constructing derivative-free algorithms for unconstrained optimization that are easy to implement on parallel machines. A special feature of this approach is the ease with which algorithms can be generated to take advantage of any number of processors and to adapt to any cost ratio of communication to function evaluation. Numerical tests show speed-ups on two fronts. The cost of synchronization being minimal, the speed-up is almost linear with the addition of more processors, i.e., given a problem and a search strategy, the decrease in execution time is proportional to the number of processors added. Even more encouraging, however, is that different search strategies, devised to take advantage of additional (or more powerful) processors, may actually lead to dramatic improvements in the performance of the basic algorithm. Thus search strategies intended for many processors actually may generate algorithms that are better even when implemented seque...
210|Detection And Remediation Of Stagnation In The Nelder-Mead Algorithm Using A Sufficient Decrease Condition| The Nelder-Mead algorithm can stagnate and converge to a non-optimal point, even for very simple problems. In this note we propose a test for sufficient decrease which, if passed for the entire iteration, will guarantee convergence of the Nelder-Mead iteration to a stationary point if the objective function is smooth. Failure of this condition is an indicator of potential stagnation. As a remedy we propose a new step, which we call an oriented restart, which reinitializes the simplex to a smaller one with orthogonal edges which contains an approximate steepest descent step from the current best point. We also give results that apply when objective function is a low-amplitude perturbation of a smooth function. We illustrate our results with some numerical examples.
211|Fortified-Descent Simplicial Search Method: A General Approach|We propose a new simplex-based direct search method for unconstrained minimization of a realvalued function f of n variables. As in other methods of this kind, the intent is to iteratively improve an n-dimensional simplex through certain reflection/expansion/contraction steps. The method has three novel features. First, a user-chosen integer  m k specifies the number of &#034;good&#034; vertices to be retained in constructing the initial trial simplices--reflected, then either expanded or contracted--at iteration k. Second, a trial simplex is accepted only when it satisfies the criteria of fortified descent,  which are stronger than the criterion of strict descent used in most direct search methods. Third, the number of additional function evaluations needed to check a trial reflected/expanded simplex for fortified descent can be controlled. If one of the initial trial simplices satisfies the fortified descent criteria, it is accepted as the new simplex; otherwise, the simplex is shrunk a fracti...
212|Convergence of the restricted Nelder-Mead algorithm in two dimensions|The Nelder–Mead algorithm, a longstanding direct search method for unconstrained optimization published in 1965, is designed to minimize a scalar-valued function f of n real variables using only function values, without any derivative information. Each Nelder–Mead iteration is associated with a nondegenerate simplex defined by n + 1 vertices and their function values; a typical iteration produces a new simplex by replacing the worst vertex by a new point. Despite the method’s widespread use, theoretical results have been limited: for strictly convex objective functions of one variable with bounded level sets, the algorithm always converges to the minimizer; for such functions of two variables, the diameter of the simplex converges to zero, but examples constructed by McKinnon show that the algorithm may converge to a nonminimizing point. This paper considers the restricted Nelder–Mead algorithm, a variant that does not allow expansion steps. In two dimensions we show that, for any nondegenerate starting simplex and any twice-continuously differentiable function with positive definite Hessian and bounded level sets, the algorithm always converges to the minimizer. The proof is based on treating the method as a discrete dynamical system, and relies on several techniques that are non-standard in convergence proofs for unconstrained optimization.  
213|An algorithm for finding best matches in logarithmic expected time|An algorithm and data structure are presented for searching a file containing N records, each described by k real valued keys, for the m closest matches or nearest neighbors to a given query record. The computation required to organize the file is proportional to kNlogN. The expected number of records examined in each search is independent of the file size. The expected computation to perform each search is proportional-to 1ogN. Empirical evidence suggests that except for very small files, this algorithm is considerably faster than other methods.
214|Direct spatial search on pictorial databases using packed r-trees|Pictorial databases require efficient and duect spatml search based on the analog form of spatial obJects and relatlonshlps instead of search based on some cumbersome alphanumeric encodings of the pmtures R-trees (two-dimensional B-trees) are excellent devices for indexing spatial ObJects and relationships found on pictures Their most important feature 1s that they provide high level ObJect onented search rather than search based on the low level elements of spatial ObJects This paper presents an efficient initial packing technique for creatmg R-trees to index spatial ObJects Since pictorial databases are not update mtensive but rather static, the beneflts of this technique are very significant 1.
215|Refinements to Nearest-Neighbor Searching in k-Dimensional Trees| This note presents a simplification and generalization of an algorithm for searching k-dimensional trees for nearest neighbors reported by Friedman et al. I-3]. If the distance between records is measured using Lz, the Euclidean orm, the data structure used by the algorithm to determine the bounds of the search space can be simplified to a single number. Moreover, because distance measurements in L2 are rotationally invariant, the algorithm can be generalized to allow a partition plane to have an arbitrary orientation, rather than insisting that it be perpendicular to a coordinate axis, as in the original algorithm. When a k-dimensional tree is built, this plane can be found from the principal eigenvector f the covariance matrix of the records to be partitioned. These techniques and others yield variants of k-dimensional trees customized for specific applications. It is wrong to assume that k-dimensional trees guarantee that a nearest-neighbor query completes in logarithmic expected time. For small k, logarithmic behavior isobserved on all but tiny trees. However, for larger k, logarithmic behavior is achievable only with extremely large numbers of records. For k = 16, a search of a k-dimensional tree of 76,000 records examines almost every record.
216|Web Server Workload Characterization: The Search for Invariants (Extended Version)  (1996) |The phenomenal growth in popularity of the World Wide Web (WWW, or the Web) has made WWW traffic the largest contributor to packet and byte traffic on the NSFNET backbone. This growth has triggered recent research aimed at reducing the volume of network traffic produced by Web clients and servers, by using caching, and reducing the latency for WWW users, by using improved protocols for Web interaction. Fundamental to the goal of improving WWW performance is an understanding of WWW workloads. This paper presents a workload characterization study for Internet Web servers. Six different data sets are used in this study: three from academic environments, two from scientific research organizations, and one from a commercial Internet provider. These data sets represent three different orders of magnitude in server activity, and two different orders of magnitude in time duration, ranging from one week of activity to one year of activity. Throughout the study, emphasis is placed on finding wor...
217|On the Self-similar Nature of Ethernet Traffic (Extended Version)  (1994) | We demonstrate that Ethernet LAN traffic is statistically self-similar, that none of the commonly used traffic models is able to capture this fractal-like behavior, that such behavior has serious implications for the design, control, and analysis of high-speed, cell-based networks, and that aggregating streams of such traffic typically intensifies the self-similarity (“burstiness”) instead of smoothing it. Our conclusions are supported by a rigorous statistical analysis of hundreds of millions of high quality Ethernet traffic measurements collected between 1989 and 1992, coupled with a discussion of the underlying mathematical and statistical properties of self-similarity and their relationship with actual network behavior. We also present traffic models based on self-similar stochastic processes that provide simple, accurate, and realistic descriptions of traffic scenarios expected during B-ISDN deployment. 
218|Wide-Area Traffic: The Failure of Poisson Modeling|Network arrivals are often modeled as Poisson processes for analytic simplicity, even though a number of traffic studies have shown that packet interarrivals are not exponentially distributed. We evaluate 24 wide-area traces, investigating a number of wide-area TCP arrival processes (session and connection arrivals, FTP data connection arrivals within FTP sessions, and TELNET packet arrivals) to determine the error introduced by modeling them using Poisson processes. We find that user-initiated TCP session arrivals, such as remotelogin and file-transfer, are well-modeled as Poisson processes with fixed hourly rates, but that other connection arrivals deviate considerably from Poisson; that modeling TELNET packet interarrivals as exponential grievously underestimates the burstiness of TELNET traffic, but using the empirical Tcplib [Danzig et al, 1992] interarrivals preserves burstiness over many time scales; and that FTP data connection arrivals within FTP sessions come bunched into “connection bursts,” the largest of which are so large that they completely dominate FTP data traffic. Finally, we offer some results regarding how our findings relate to the possible self-similarity of widearea traffic.  
219|Characteristics of WWW Client-based Traces|The explosion of WWW traffic necessitates an accurate picture of WWW use, and in particular requires a good understanding of client requests for WWW documents. To address this need, we have collectedtraces of actual executions of NCSA Mosaic, reflecting over half a million user requests for WWW documents. In this paper we describe the methods we used to collect our traces, and the formats of the collected data. Next, we present a descriptive statistical summary of the traces we collected, which identifies a number of trends and reference patterns in WWW use. In particular, we show that many characteristics of WWW use can be modelled using power-law distributions, including the distribution of document sizes, the popularity of documents as a function of size, the distribution of user requests for documents, and the number of references to documents as a function of their overall rank in popularity (Zipf&#039;s law). Finally, we show how the power-law distributions derived from our traces can beused to guide system designers interested in caching WWW documents.
220|Empirically-Derived Analytic Models of Wide-Area TCP Connections: Extended Report|We analyze 2.5 million TCP connections that occurred during 14 wide-area traffic traces. The traces were gathered at five &#034;stub&#034; networks and two internetwork gateways, providing a diverse look at wide-area traffic. We derive analytic models describing the random variables associated with telnet, nntp, smtp, and ftp connections, and present a methodology for comparing the effectiveness of the analytic models with empirical models such as tcplib [DJ91]. Overall we find that the analytic models provide good descriptions, generally modeling the various distributions as well as empirical models and in some cases better.
221|A Caching Relay for the World Wide Web|We describe the design and performance of a caching relay for the World Wide Web. We model the distribution of requests for pages from the web and see how this distribution affects the performance of a cache. We use the data gathered from the relay to make some general characterizations about the web. (A version of this paper is available at http://www.research.digital.com/- SRC/personal/Steve Glassman/-  CachingTheWeb.html or .../CachingTheWeb.ps)  1 Overview  In January 1994, we set up a caching World Wide Web [10] relay for Digital Equipment Corporation &#039;s facilities in Palo Alto, California. We use a relay to reach the Web because Digital has a security firewall that restricts direct interaction between Digital internal computers and machines outside of Digital. We added caching to the relay because we wanted to improve the relay&#039;s performance and reduce its external network traffic. Clients use the relay for accessing the Web outside of Digital; requests for internal Digital pages...
222|Characteristics of Wide-Area TCP/IP Conversations|In this paper, we characterize wide-area network applications that use the TCP transport protocol. We also describe a new way to model the wide-area traffic generated by a stub network. We believe the traffic model presented here will be useful in studying congestion control, routing algorithms, and other resource management schemes for existing and future networks.  Our model is based on trace analysis of TCP/IP widearea internetwork traffic. We collected the TCP/IP packet headers of USC, UCB, and Bellcore networks at the point they connect with their respective regional access networks. We then wrote a handful of programs to analyze the traces. Our model characterizes individual TCP conversations by the distributions of: number of bytes transferred, duration, number of packets transferred, packet size, and packet interarrival time.  Our trace analysis shows that both interactive and bulk transfer traffic from all sites reflect a large number of short conversations. Similarly, it shows that a very large percentage of traffic is bidirectional, even for bulk transfer. We observed that interactive applications send significantly different amounts of data in each direction of a conversation, and that interarrival times for interactive applications closely follow a constant plus exponential model. Half of the conversations are directed to a handful of networks, but the other half are directed to hundreds of networks. Many of these observations contradict commonly held beliefs regarding wide-area traffic.  
223|Explaining World Wide Web Traffic Self-Similarity|Recently the notion of self-similarity has been shown to apply to wide-area and local-area network traffic. In this paper we examine the mechanisms that give rise to self-similar network traffic. We present an explanation for traffic self-similarity by using a particular subset of wide area traffic: traffic due to the World Wide Web (WWW). Using an extensive set of traces of actual user executions of NCSA Mosaic, reflecting over half a million requests for WWW documents, we show evidence that WWW traffic is selfsimilar. Then we show that the self-similarity in such traffic can be explained based on the underlying distributions of WWW document sizes, the effects of caching and user preference in file transfer, the effect of user &#034;think time&#034;, and the superimposition of many such transfers in a local area network. To do this we rely on empirically measured distributions both from our traces and from data independently collected at over thirty WWW sites. 1 Introduction  Understanding the ...
224|Growth Trends in Wide-Area TCP Connections|We analyze the growth of a medium-sized research laboratory &#039;s wide-area TCP connections over a period of more than two years. Our data consisted of six month-long traces of all TCP connections made between the site and the rest of the world. We find that smtp, ftp, and X11 traffic all exhibited exponential growth in the number of connections and bytes transferred, at rates significantly greater than that at which the site&#039;s overall computing resources grew; that individual users increasingly affected the site&#039;s traffic profile by making wide-area connections from background scripts; that the proportion of local computers participating in wide-area traffic outpaces the site&#039;s overall growth; that use of the network by individual computers appears to be constant for some protocols  (telnet) and growing exponentially for others (ftp, smtp);  and that wide-area traffic geography is diverse and dynamic. 1 Introduction  To properly design future networks, we need a thorough understanding of...
225|Application-level document caching in the Internet|With the increasing demand for document transfer services such as the World Wide Web comes a need for better resource management to reduce the latency of documents in these systems. To address this need, we analyze the potential for documentcaching at the application level in document transfer services. Wehave collected traces of actual executions of Mosaic, reflecting over half a million user requests for WWW documents. Using those traces, we study the tradeoffs between caching at three levels in the system, and the potential for use of application-level information in the caching system. Our traces show that while a high hit rate in terms of URLs is achievable, a muchlower hit rate is possible in terms of bytes, because most profitably-cached documents are small. We consider the performance of caching when applied at the level of individual user sessions, at the level of individual hosts, and at the level of a collection of hosts on a single LAN. We show that the performance gain achievable bycaching at the session level (which is straightforward to implement) is nearly all of that achievable at the LAN level (where caching is more difficult to implement). However, when resource requirements are considered, LAN level caching becomes much more desirable, since it can achieveagiven level of caching performance using a much smaller amountofcache space. Finally,we consider the use of organizational boundary information as an example of the potential for use of application-level information in caching. Our results suggest that distinguishing between documents produced locally and those produced remotely can provide useful leverage in designing caching policies, because of differences in the potential for sharing these two documenttypes among multiple users.
226|The effect of client caching on file server workloads|A distributed file systetn provides file service from one or rnore &amp;red file servers to a community of clierit workstations o’ver Q network. Wlaile the client-server paradigm has many advantages, it also presents new challenges to system designers concerning perfor-rnance and reliability. As both client workstations and file servers become increasingly well-resourced, a nu~n-ber of system design decisions need to be re-examined. This research concerus the caching of disk blocks in a distributed client-server enviromnent. Some recent re-search has suggested that various strategies for cache rnauagernent may not be equally suited to the circurn-stances at both the client and the server. Since any caching strategy is based on assumptions concerning the characteristics of the denland, the performance of the strategy is only as good as the accuracy of this assurnp-tion. The perfornlance of a caching strategy at a file server is strongly influenced by the presence of client cnches since these caches alter the characteristics of the stream of requests that reaches the server. This paper presents the results of an investigation of the effect of client caching on the nature of the server workload as a step towards understanding the performnnce of caching strategies at the server. The results demonstrate that client caches alter worklond characteristics in a way that will have CI profound impact on server cnche per-forrnance, and suggest worthwhile directions for future development of server caching strategies. 1
227|Using a Wide-Area File System Within the World-Wide Web|This paper proposes the use of a wide-area file system for storing and retrieving documents.  We demonstrate that most of the functionality of the World-Wide Web (WWW)  information service can be provided by storing documents in AFS. The approach addresses  several performance problems experienced by WWW servers and clients, such  as increased server and network load, network latency and inadequate security. In addition,  the mechanism demonstrates the value of a global, general purpose file sharing  system and the advantage of layering existing technologies.  1 Introduction  The dramatic increase in networked information access through the World-Wide Web [1] demonstrates the value of wide-area information sharing. However, the Web faces many challenges as it continues to scale to accommodate the ever increasing number of users. Among those are increased server and network load, network latency and inadequate security.  Several decades of distributed systems research address the proble...
228|Controlled and automatic human information processing: II. Perceptual learning, automatic attending and a general theory|The two-process theory of detection, search, and attention presented by Schneider and Shiffrin is tested and extended in a series of experiments. The studies demonstrate the qualitative difference between two modes of information processing: automatic detection and controlled search. They trace the course of the learning of automatic detection, of categories, and of automaticattention responses. They show the dependence of automatic detection on attending responses and demonstrate how such responses interrupt controlled processing and interfere with the focusing of attention. The learning of categories is shown to improve controlled search performance. A general framework for human information processing is proposed; the framework emphasizes the roles of automatic and controlled processing. The theory is compared to and contrasted with extant models of search and attention.
229|Controlled and automatic human information processing|A two-process theory of human information processing is proposed and applied to detection, search, and attention phenomena. Automatic processing is activa-tion of a learned sequence of elements in long-term memory that is initiated by appropriate inputs and then proceeds automatically—without subject control, without stressing the capacity limitations of the system, and without necessarily demanding attention. Controlled processing is a temporary activation of a se-quence of elements that can be set up quickly and easily but requires attention, is capacity-limited (usually serial in nature), and is controlled by the subject. A series of studies using both reaction time and accuracy measures is presented, which traces these concepts in the form of automatic detection and controlled, search through the areas of detection, search, and attention. Results in these areas are shown to arise from common mechanisms. Automatic detection is shown to develop following consistent mapping of stimuli to responses over trials. Controlled search is utilized in varied-mapping paradigms, and in our studies, it takes the form of serial, terminating search. The approach resolves a number of apparent conflicts in the literature.
230|Test stimulus representation and experimental context effects in memory scanning |The 5s performed a memory-scanning task in which they indicated whether or not a given test stimulus (letter or picture) matched one of a previously memorized set of letters. The test stimuli presented during a given session were either exclusively letters (a letter session), exclusively pictures (a picture session), or a random sequence of both (a mixed session). Reactiontime functions relating response latency to the size of the memorized set of letters were plotted, and the data are discussed in the context of the scanning models previously proposed by S. Sternberg. The reaction time functions of letter sessions and picture sessions were found to be consistent with the exhaustive model for memory scanning. However, the functions for mixed sessions deviated markedly from the predictions of such a model. The context in which a scanning task is imbedded appears to have a substantial effect on reaction time functions. Evidence that scans of information stored in short-term memory are serial and exhaustive
231|Stimulus probability and stimulus set size in memory scanning|In many recent studies of speeded scanning of immediate memory, variations in the size of the positive set (s) were confounded with variations in the probability (P) of the individual items within the positive set: As s increased, P decreased. The present experiment sought to determine whether the effect on RT attributed to s could be accounted for by variations in P. This was accomplished by factorially varying both s and P. Probability effects were confined to items in the positive set and were insufficient to account for the effect of s. The results are discussed in terms of a model in which s and P affect different information-processing stages. The s affects the number of compari-sons between the encoded item and the items stored in the memory of the positive set, as proposed by Sternberg. The P affects response selection— information as to the particular digit that was presented is available to the mechanisms for response selection along with the knowledge that there was or was not a match. The response selection mechanisms are assumed to be biased in tune with the P values of the items within the positive set. The number of things that one has to think about and the expectancy as to the likelihood of occurrence of these things— stimulus number and stimulus probability —have long been regarded as fundamental variables in the study of cognition. The common rinding that longer RTs would be produced by an increase in the number of possible stimuli or a decrease in stimulus probability was a result that was compati-ble with most theories of stimulus recogni-tion. Discriminating among the various theoretical accounts for these effects has been a more elusive task. One class of models holds that variations in stimulus probability and stimulus num-ber affect only a single commodity such as information (in bits) or repetitions. Ex-amples of such models are those that posit
233|Assessing coping strategies: A theoretically based approach|We developed a multidimensional coping inventory to assess the different ways in which people respond to stress. Five scales (of four items each) measure conceptually distinct aspects of problem-focused coping (active coping, planning, suppression of competing activities, restraint coping, seek-ing of instrumental social support); five scales measure aspects of what might be viewed as emotion-focused coping (seeking of emotional social support, positive reinterpretation, acceptance, denial, turning to religion); and three scales measure coping responses that arguably are less useful (focus on and venting of emotions, behavioral disengagement, mental disengagement). Study 1 reports the development of scale items. Study 2 reports correlations between the various coping scales and sev-eral theoretically relevant personality measures in an effort to provide preliminary information about the inventory&#039;s convergent and discriminant validity. Study 3 uses the inventory to assess coping responses among a group of undergraduates who were attempting to cope with a specific stressful episode. This study also allowed an initial examination of associations between dispositional and situational coping tendencies. Interest in the processes by which people cope with stress has grown dramatically over the past decade (cf. Moos, 1986). The
234|Approach, avoidance, and coping with stress|ABSTRACT: The study of stress and coping points to two concepts central to an understanding of the response to trauma: approach and avoidance. This pair of concepts refers to two basic modes of coping with stress. Approach and avoidance are simply metaphors for cognitive and emotional activity that is oriented either toward or away from threat. An approach-avoidance model of coping is presented in the context of contemporary theoretical ap-proaches to coping. The research literature on coping ef-fectiveness, including evidence from our laboratory, is dis-cussed, and speculations are made about he implications for future research. The study of stress and coping has become quite popular in recent years, particularly in regard to traumatic life events. Although the area is broad and the coping process
235|The JPEG still picture compression standard|This paper is a revised version of an article by the same title and author which appeared in the April 1991 issue of Communications of the ACM. For the past few years, a joint ISO/CCITT committee known as JPEG (Joint Photographic Experts Group) has been working to establish the first international compression standard for continuous-tone still images, both grayscale and color. JPEG’s proposed standard aims to be generic, to support a wide variety of applications for continuous-tone images. To meet the differing needs of many applications, the JPEG standard includes two basic compression methods, each with various modes of operation. A DCT-based method is specified for “lossy’ ’ compression, and a predictive method for “lossless’ ’ compression. JPEG features a simple lossy technique known as the Baseline method, a subset of the other DCT-based modes of operation. The Baseline method has been by far the most widely implemented JPEG method to date, and is sufficient in its own right for a large number of applications. This article provides an overview of the JPEG standard, and focuses in detail on the Baseline method. 1
236|The grid file: an adaptable, symmetric multikey file structure|Traditional file structures that provide multikey access to records, for example, inverted files, are extensions of file structures originally designed for single-key access. They manifest various deficiencies in particular for multikey access to highly dynamic files. We study the dynamic aspects of tile structures that treat all keys symmetrically, that is, file structures which avoid the distinction between primary and secondary keys. We start from a bitmap approach and treat the problem of file design as one of data compression of a large sparse matrix. This leads to the notions of a grid partition of the search space and of a grid directory, which are the keys to a dynamic file structure called the grid file. This tile system adapts gracefully to its contents under insertions and deletions, and thus achieves an upper hound of two disk accesses for single record retrieval; it also handles range queries and partially specified queries efficiently. We discuss in detail the design decisions that led to the grid file, present simulation results of its behavior, and compare it to other multikey access file structures.
237|QBISM: A Prototype 3-D Medical Image Database System|this paper. However, these automatic or semi-automatic warping algorithms are extremely important for this application. It is precisely this technology that permits anatomic structure-based access to acquired medical images as well as comparisons among studies, even of different patients, as long as they have been warped to the same atlas. Furthermore, it enables the database to grow, and be queryable, without time-consuming manual segmentation of the data.
238|Chaff: Engineering an Efficient SAT Solver|Boolean Satisfiability is probably the most studied of combinatorial optimization/search problems. Significant effort has been devoted to trying to provide practical solutions to this problem for problem instances encountered in a range of applications in Electronic Design Automation (EDA), as well as in Artificial Intelligence (AI). This study has culminated in the development of several SAT packages, both proprietary and in the public domain (e.g. GRASP, SATO) which find significant use in both research and industry. Most existing complete solvers are variants of the Davis-Putnam (DP) search algorithm. In this paper we describe the development of a new complete solver, Chaff, which achieves significant performance gains through careful engineering of all aspects of the search – especially a particularly efficient implementation of Boolean constraint propagation (BCP) and a novel low overhead decision strategy. Chaff has been able to obtain one to two orders of magnitude performance improvement on difficult SAT benchmarks in comparison with other solvers (DP or otherwise), including GRASP and SATO.
239|GRASP: A Search Algorithm for Propositional Satisfiability|AbstractÐThis paper introduces GRASP (Generic seaRch Algorithm for the Satisfiability Problem), a new search algorithm for Propositional Satisfiability (SAT). GRASP incorporates several search-pruning techniques that proved to be quite powerful on a wide variety of SAT problems. Some of these techniques are specific to SAT, whereas others are similar in spirit to approaches in other fields of Artificial Intelligence. GRASP is premised on the inevitability of conflicts during the search and its most distinguishing feature is the augmentation of basic backtracking search with a powerful conflict analysis procedure. Analyzing conflicts to determine their causes enables GRASP to backtrack nonchronologically to earlier levels in the search tree, potentially pruning large portions of the search space. In addition, by ªrecordingº the causes of conflicts, GRASP can recognize and preempt the occurrence of similar conflicts later on in the search. Finally, straightforward bookkeeping of the causality chains leading up to conflicts allows GRASP to identify assignments that are necessary for a solution to be found. Experimental results obtained from a large number of benchmarks indicate that application of the proposed conflict analysis techniques to SAT algorithms can be extremely effective for a large number of representative classes of SAT instances. Index TermsÐSatisfiability, search algorithms, conflict diagnosis, conflict-directed nonchronological backtracking, conflict-based equivalence, failure-driven assertions, unique implication points. 1
240|Using CSP look-back techniques to solve real-world SAT instances|We report on the performance of an enhanced version of the “Davis-Putnam ” (DP) proof procedure for propositional satisfiability (SAT) on large instances derived from realworld problems in planning, scheduling, and circuit diagnosis and synthesis. Our results show that incorporating CSP lookback techniques-- especially the relatively new technique of relevance-bounded learning-- renders easy many problems which otherwise are beyond DP’s reach. Frequently they make DP, a systematic algorithm, perform as well or better than stochastic SAT algorithms such as GSAT or WSAT. We recommend that such techniques be included as options in implementations of DP, just as they are in systematic algorithms for the more general constraint satisfaction problem.
241|Evidence for Invariants in Local Search|It is well known that the performance of a stochastic local search procedure depends upon the setting of its noise parameter, and that the optimal setting varies with the problem distribution. It is therefore desirable to develop general priniciples for tuning the procedures. We present two statistical measures of the local search process that allow one to quickly find the optimal noise settings. These properties are independent of the fine details of the local search strategies, and appear to be relatively independent of the structure of the problem domains. We applied these principles to the problem of evaluating new search heuristics, and discovered two promising new strategies.
242|Improvements To Propositional Satisfiability Search Algorithms|... quickly across a wide range of hard SAT problems than any other SAT tester in the literature on comparable platforms. On a Sun SPARCStation 10 running SunOS 4.1.3 U1, POSIT can solve hard random 400-variable 3-SAT problems in about 2 hours on the average. In general, it can solve hard n-variable random 3-SAT problems with search trees of size O(2  n=18:7  ).  In addition to justifying these claims, this dissertation describes the most significant achievements of other researchers in this area, and discusses all of the widely known general techniques for speeding up SAT search algorithms. It should be useful to anyone interested in NP-complete problems or combinatorial optimization in general, and it should be particularly useful to researchers in either Artificial Intelligence or Operations Research. 
243|Using Randomization and Learning to Solve Hard Real-World Instances of Satisfiability|This paper addresses the interaction between randomization, with restart strategies, and learning, an often crucial technique for proving unsatisfiability. We use instances of SAT from the hardware verification domain to provide evidence that randomization can indeed be essential in solving real-world satis able instances of SAT. More interestingly, our results indicate that randomized restarts and learning may cooperate in proving both satisfiability and unsatisfiability. Finally, we utilize and expand the idea of algorithm portfolio design to propose an alternative approach for solving hard unsatisfiable instances of SAT.
244|The impact of branching heuristics in propositional satisfiability algorithms|Abstract. This paper studies the practical impact of the branching heuristics used in Propositional Satisfiability (SAT) algorithms, when applied to solving real-world instances of SAT. In addition, different SAT algorithms are experimentally evaluated. The main conclusion of this study is that even though branching heuristics are crucial for solving SAT, other aspects of the organization of SAT algorithms are also essential. Moreover, we provide empirical evidence that for practical instances of SAT, the search pruning techniques included in the most competitive SAT algorithms may be of more fundamental significance than branching heuristics.
245|Improved algorithms for optimal winner determination in combinatorial auctions and generalizations|Combinatorial auctions can be used to reach efficient resource and task allocations in multiagent systems where the items are complementary. Determining the winners is NP-complete and inapproximable, but it was recently shown that optimal search algorithms do very well on average. This paper presents a more sophisticated search algorithm for optimal (and anytime) winner determination, including structural improvements that reduce search tree size, faster data structures, and optimizations at search nodes based on driving toward, identifying and solving tractable special cases. We also uncover a more general tractable special case, and design algorithms for solving it as well as for solving known tractable special cases substantially faster. We generalize combinatorial auctions to multiple units of each item, to reserve prices on singletons as well as combinations, and to combinatorial exchanges -- all allowing for substitutability. Finally, we present algorithms for determining the winners in these generalizations.
246|Computationally Manageable Combinatorial Auctions|There is interest in designing simultaneous auctions for situations in which the value of assets to a bidder depends upon which other assets he or she wins. In such cases, bidders may well wish to submit bids for combinations of assets. When this is allowed, the problem of determining the revenue maximizing set of nonconflicting bids can be a difficult one. We analyze this problem, identifying several different structures of combinatorial bids for which computational tractability is constructively demonstrated and some structures for which computational tractability  1 Introduction  Some auctions sell many assets simultaneously. Often these assets, like U.S. treasury bills, are interchangeable. However, sometimes the assets and the bids for them are distinct. This happens frequently, as in the U.S. Department of the Interior&#039;s simultaneous sales of off-shore oil leases, in some private farm land auctions, and in the Federal Communications Commission&#039;s recent multi-billion dollar sales...
247|Taming the computational complexity of combinatorial auctions: Optimal and approximate approaches|In combinatorial auctions, multiple goods are sold simultaneously and bidders may bid for arbitrary combinations of goods. Determining the outcome of such an auction is an optimization problem that is NP-complete in the general case. We propose two methods of overcoming this apparent intractability. The first method, which is guaranteed to be optimal, reduces running time by structuring the search space so that a modified depth-first search usually avoids even considering allocations that contain conflicting bids. Caching and pruning are also used to speed searching. Our second method is a heuristic, market-based approach. It sets up a virtual multi-round auction in which a virtual agent represents each original bid bundle and places bids, according to a fixed strategy, for each good in that bundle. We show through experiments on synthetic data that (a) our first method finds optimal allocations quickly and offers good anytime performance, and (b) in many cases our second method, despite lacking guarantees regarding optimality or running time, quickly reaches solutions that are nearly optimal. 1 Combinatorial Auctions Auction theory has received increasing attention from computer scientists in recent years. 1 One reason is the explosion of internet-based auctions. The use of auctions in business-to-business trades is also increasing rapidly [Cortese and Stepanek, 1998]. Within AI there is growing interest in using auction mechanisms to solve distributed resource allocation problems. For example, auctions and other market mechanisms are used in network bandwidth allocation, distributed configuration design, factory scheduling, and operating system memory allocation [Clearwater, 1996]. Market-oriented programming has
248|Limitations of the Vickrey Auction in Computational Multiagent Systems|Auctions provide an efficient distributed  mechanism for solving problems such as task  and resource allocation in multiagent systems.
249|eMediator: A Next Generation Electronic Commerce Server|This paper presents eMediator, an electronic commerce server prototype that demonstrates ways in which algorithmic support and game-theoretic incentive engineering can jointly improve the efficiency of ecommerce. eAuctionHouse, the configurable auction server, includes a variety of generalized combinatorial auctions and exchanges, pricing schemes, bidding languages, mobile agents, and user support for choosing an auction type. We introduce two new logical bidding languages for combinatorial markets: the XOR bidding language and the OR-of-XORs bidding language. Unlike the traditional OR bidding language, these are fully expressive. They therefore enable the use of the Clarke-Groves pricing mechanism for motivating the bidders to bid truthfully. eAuctionHouse also supports supply/demand curve bidding. eCommitter, the leveled commitment contract optimizer, determines the optimal contract price and decommitting penalties for a variety of leveled commitment contracting mechanisms, taking into account that rational agents will decommit strategically in Nash equilibrium. It also determines the optimal decommitting strategies for any given leveled commitment contract. eExchangeHouse, the safe exchange planner, enables unenforced anonymous exchanges by dividing the exchange into chunks and sequencing those chunks to be delivered safely in alternation between the buyer and the seller.
250|Some Tractable Combinatorial Auctions|Auctions are the most widely used strategic gametheoretic mechanism in the Internet. Auctions have been mostly studied from a game-theoretic and economic perspective, although recent work in AI and OR has been concerned with computational aspects of auctions as well. When faced from a computational perspective, combinatorial auctions are perhaps the most challenging type of auctions. Combinatorial auctions are auctions where agents may submit bids for bundles of goods. Given that finding an optimal allocation of the goods in a combinatorial auction is intractable, researchers have been concerned with exposing tractable instances of combinatorial auctions.
251|Image retrieval: Current techniques, promising directions and open issues|This paper provides a comprehensive survey of the technical achievements in the research area of image retrieval, especially content-based image retrieval, an area that has been so active and prosperous in the past few years. The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multidimensional indexing, and system design, three of the fundamental bases of content-based image retrieval. Furthermore, based on the state-of-the-art technology available now and the demand from real-world applications, open research issues are identified and future promising research directions are suggested. C ? 1999 Academic Press 1.
252|Photobook: Content-Based Manipulation of Image Databases|We describe the Photobook system, which is a set of interactive tools for browsing and searching images and image sequences. These query tools differ from those used in standard image databases in that they make direct use of the image content rather than relying on text annotations. Direct search on image content is made possible by use of semantics-preserving image compression, which reduces images to a small set of perceptually-significant coefficients. We describe three types of Photobook descriptions in detail: one that allows search based on appearance, one that uses 2-D shape, and a third that allows search based on textural properties. These image content descriptions can be combined with each other and with textbased descriptions to provide a sophisticated browsing and search capability. In this paper we demonstrate Photobook on databases containing images of people, video keyframes, hand tools, fish, texture swatches, and 3-D medical data.  
253|Image Indexing Using Color Correlograms|We define a new image feature called the color correlogram  and use it for image indexing and comparison. This feature distills the spatial correlation of colors, and is both effective and inexpensive for content-based image retrieval. The correlogramrobustly tolerates large changesin appearance and shape caused by changes in viewing positions, camera zooms, etc. Experimental evidence suggests that this new feature outperforms not only the traditional color histogram method but also the recently proposed histogram refinement methods for image indexing/retrieval.  
254|NeTra: A toolbox for navigating large image databases|. We present here an implementation of NeTra, a prototype image retrieval system that uses color, texture, shape and spatial location information in segmented image regions to search and retrieve similar regions from the database. A distinguishing aspect of this system is its incorporation of a robust automated image segmentation algorithm that allows object- or region-based search. Image segmentation significantly improves the quality of image retrieval when images contain multiple complex objects. Images are segmented into homogeneous regions at the time of ingest into the database, and image attributes that represent each of these regions are computed. In addition to image segmentation, other important components of the system include an efficient color representation, and indexing of color, texture, and shape features for fast search and retrieval. This representation allows the user to compose interesting queries such as &#034;retrieve all images that contain regions that have the colo...
255|Texture analysis and classification with tree-structured wavelet transform|Abstract-One difficulty of texture analysis in the past was the lack of adequate tools to characterize different scales of textures effectively. Recent developments in multiresolution analysis such as the Gabor and wavelet transforms help to overcome this difficulty. In this research, we propose a multiresolution approach based on a modified wavelet transform called the tree-structured wavelet transform or wavelet packets for texture analysis and classification. The development of this new transform is motivated by the observation that a large class of natural textures can be modeled as quasi-periodic signals whose dominant frequencies are located in the middle frequency channels. With the transform, we are able to zoom into any desired frequency channels for further decomposition. In contrast, the conventional pyramid-structured wavelet transform performs further decomposition only in low frequency channels. We develop a progressive texture classification algorithm which is not only computationally attrac-tive but also has excellent performance. The performance of our new method is compared with that of several other methods using the DCT, DST, DHT, pyramid-structured wavelet transforms, Gabor filters, and Laws filters.
256|Visual Information Retrieval|ND BUSINESSMAN CALVIN MOORES COINED the term information retrieval [10] to describe the process through which a prospective user of information can convert a request for information into a useful collection of references. &#034;Information retrieval,&#034; he wrote, &#034;embraces the intellectual aspects of the description of information and its specification for search, and also whatever systems, techniques, or machines that are employed Amarnath Gupta and Ramesh Jain 72 May 1997/Vol. 40, No. 5 COMMUNICATIONS OF THE ACM lar expressions to describe a clip. There is also a deeper reason: The information sought is inherently in the form of imagery that a textual language, however powerful, is unable to express adequately, making query processing inefficient. HE ROLE OF THE EMERGING FIELD OF visual information retrieval (VIR) systems is to go beyond text-based descri
257|Comparing Images Using Color Coherence Vectors|Color histograms are used to compare images in many applications. Their advantages are efficiency, and insensitivity to small changes in camera viewpoint. However, color histograms lack spatial information, so images with very di#erent appearances can have similar histograms. For example, a picture of fall foliage might contain a large number of scattered red pixels
258|Texture classification by wavelet packet signatures|This paper introduces a new approach tocharacterize textures at multiple scales. The performance of wavelet packet spaces are measured in terms of sensitivity and selectivity for the classi cation of twenty- ve natural textures. Both energy and entropy metrics were computed for each wavelet packet and incorporated into distinct scale space representations, where each wavelet packet (channel) re ected a speci c scale and orientation sensitivity. Wavelet packet representations for twenty- ve natural textures were classi ed without error by a simple two-layer network classi er. An analyzing function of large regularity (D 20) was shown to be slightly more e cient inrepresentation and discrimination than a similar function with fewer vanishing moments (D6). In addition, energy representations computed from the standard wavelet decomposition alone (17 features) provided classi cation without error for the twenty- ve textures included in our study. The reliability exhibited by texture signatures based on wavelet packets analysis suggest that the multiresolution properties of such transforms are bene cial for accomplishing segmentation, classication and subtle discrimination of texture. Index Terms{Feature extraction, texture analysis, texture classi cation, wavelet transform, wavelet packet, neural networks.
259|Incremental Clustering and Dynamic Information Retrieval|Motivated by applications such as document and image classification in information retrieval, we consider the problem of clustering dynamic point sets in a metric space. We propose a model called incremental clustering which is based on a careful analysis of the requirements of the information retrieval application, and which should also be useful in other applications. The goal is to efficiently maintain clusters of small diameter as new points are inserted. We analyze several natural greedy algorithms and demonstrate that they perform poorly. We propose new deterministic and randomized incremental clustering algorithms which have a provably good performance. We complement our positive results with lower bounds on the performance of incremental algorithms. Finally, we consider the dual clustering problem where the clusters are of fixed diameter, and the goal is to minimize the number of clusters.  
260|  Interactive learning using a &#034;society of models&#034; |Digital library access is driven by features, but features are often context-dependent and noisy, and their relevance for a query is not always obvious. This paper describes an approach for utilizing many data-dependent, user-dependent, and task-dependent features in a semi-automated tool. Instead of requiring universal similarity measures or manual selection of relevant features, the approach provides a learning algorithm for selecting and combining groupings of the data, where groupings can be induced by highlyspecialized and context-dependent features. The selection process is guided by arichexample-based interaction with the user. The inherent combinatorics
261|An Eigenspace Update Algorithm for Image Analysis|this paper  However, the vision research community has largely overlooked makes the following contributions:  parallel developments in signal processing and numerical linear algebra concerning efficient eigenspace updating algorithms. . We provide a comparison of some of the popular tech-  These new developments are significant for two reasons: Adopt-  niques existing in the vision literature for SVD/KLT com-  ing them will make some of the current vision algorithms more  putations and point out the problems associated with  robust and efficient. More important is the fact that incremental those techniques
262|WebSeer: An Image Search Engine for the World Wide Web|Because of the size of the World Wide Web and its inherent lack of structure, finding what one is looking for can be a challenge. PC-Meter&#039;s March, 1996, survey found that three of the five most visited Web sites were search engines. However, while Web pages typically contain both text and images, all the currently available search engines only index text. This paper describes WebSeer, a system for locating images on the Web. WebSeer uses image content in addition to associated text to index images, presenting the user with a selection that potentially fits her needs. 1 This work was supported in part by ONR contract N00014-93-1-0332 and NSF Grant No. IRI-9210763A01. 2 Introduction The explosive growth of the World Wide Web has proven to be a double-edged sword. While an immense amount of material is now easily accessible on the Web, locating specific information remains a difficult task. An inexperienced user may find it next to impossible to find the information she wants; even a...
263|Bayesian Relevance Feedback for Image Retrieval|This paper  1  describes PicHunter, an image retrieval system that implements a novel approach to relevance feedback, such that the entire history of user selections contributes to the system&#039;s estimate of the user&#039;s goal image. To accomplish this, PicHunter uses Bayesian learning based on a probabilistic model of a user&#039;s behavior. The predictions of this model are combined with the selections made during a search to estimate the probability associated with each image. These probabilities are then used to select images for display. The details of our model of a user&#039;s behavior were tuned using an offline learning algorithm. For clarity, our studies were done with the simplest possible user interface but the algorithm can easily be incorporated into systems which support complex queries, including most previously proposed systems. However, even with this constraint and simple image features, PicHunter is able to locate randomly selected targets in a database of 4522 images after displa...
265|SHAPE MEASURES FOR CONTENT BASED IMAGE RETRIEVAL: A COMPARISON|A great deal of work has been done on the evaluation of information retrieval systems for alphanumeric data. The same thing can not be said about the newly emerging multimedia and image database systems. One of the central concerns in these systems is the automatic characterization of image content and retrieval of images based on similarity of image content. In this paper, we discuss effectiveness of several shape measures for content based similarity retrieval of images. The different shape measures we have implemented include outline based features (chain code based string features, Fourier descriptors, UNL Fourier features), region based features (invariant moments, Zemike moments, pseudo-Zemike moments), and combined features (invariant moments &amp; Fourier descriptors, invariant moments &amp; UNL Fourier features). Given an image, all these shape feature measures (vectors) are computed automatically, and the feature vector can either be used for the retrieval purpose or can be stored in the database for future queries. We have tested all of the above shape features for image retrieval on a database of 500 trademark images. The average retrieval efficiency values computed over a set of fifteen representative queries for all the methods is presented. The output of a sample shape similarity query using all the features is also shown.  
266|An Optimized Interaction Strategy for Bayesian Relevance Feedback|A new algorithm and systematic evaluation is presented for searching a database via relevance feedback. It represents a new image display strategy for the PicHunter system [2, 1]. The algorithm takes feedback in the form of relative judgments (&#034;item A is more relevant than item B&#034;) as opposed to the stronger assumption of categorical relevance judgments (&#034;item A is relevant but item B is not&#034;). It also exploits a learned probabilistic model of human behavior to make better use of the feedback it obtains. The algorithm can be viewed as an extension of indexing schemes like the k-d tree to a stochastic setting, hence the name &#034;stochastic-comparison search.&#034; In simulations, the amount of feedback required for the new algorithm scales like log 2  |D|,  where  |D|  is the size of the database, while a simple query-by-exampleapproach scales like  |D|  a  , where a &lt; 1 depends on the structure of the database. This theoretical advantage is reflected by experiments with real users on a database of 1500 stock photographs.  1 
267|Edge Flow: A Framework of Boundary Detection and Image Segmentation|A novel boundary detection scheme based on &#034;edge flow&#034; is proposed in this paper. This scheme utilizes a predictive coding model to identify the direction of change in color and texture at each image location at a given scale, and constructs an edge flow vector. By iteratively propagating the edge flow, the boundaries can be detected at image locations which encounter two opposite directions of flow in the stable state. A user defined image scale is the only significant control parameter that is needed by the algorithm. The scheme facilitates integration of color and texture into a single framework for boundary detection.  1 Introduction  In most computer vision applications, the edge/boundary detection and image segmentation constitute a crucial initial step before performing high-level tasks such as object recognition and scene interpretation. However, despite considerable research and progress made in this area, the robustness and generality of the algorithms on large image datasets...
268|Supporting content-based queries over images in MARS|While advances in technology allow us to generate, transmit, and store large amounts of digital images, video, and audio, research in indexing and retrieval of multimedia information is still at its infancy. To address
269|A Society of Models for Video and Image Libraries|The average person with a computer will soon have access to the world&#039;s collections of digital video and images. However, unlike text which can be alphabetized or numbers which can be ordered, image and video has no general language to aid in its organization. Although tools which can &#034;see&#034; and &#034;understand&#034; the content of imagery are still in their infancy, they are now at the point where they can provide substantial assistance to users in navigating through visual media. This paper describes new tools based on &#034;vision texture&#034; for modeling image and video. The focus of this research is the use of a society of low-level models for performing relatively high-level tasks, such as retrieval and annotation of image and video libraries. This paper surveys our recent and present research in this fast-growing area.  1 Introduction: Vision Texture  Suppose you have a set of vacation photos of Paris and the surrounding countryside, and you accidentally drop them on the floor. They get out of or...
270|Target Testing and the PicHunter Bayesian Multimedia Retrieval System|This paper addresses how the effectiveness of a contentbased, multimedia information retrieval system can be measured, and how such a system should best use response feedback in performing searches. We propose a simple, quantifiable measure of an image retrieval system&#039;s effectiveness, &#034;target testing&#034;, in which effectiveness is measured as the average number of images that a user must examine in searching for a given random target. We describe an initial version of PicHunter, a retrieval system designed to test a novel approach to relevance-feedback. This approach is based on a Bayesian framework that incorporates an explicit model of the user&#039;s selection process. PicHunter is intentionally designed to have a minimal, &#034;queryless&#034; user interface, so that its performance reflects only the performance of the relevance feedback algorithm. The algorithm, however, can easily be incorporated into more traditional, query-based systems. Employing no explicit query, and only a small amount of i...
271|Relevance Feedback With Too Much Data|Modern text collections often contain large documents which span several subject areas.  Such documents are problematic for relevance feedback since inappropriate terms can easily  be chosen. This study explores the highly effective approach of feeding back passages of large  documents. A less-expensive method which discards long documents is also reviewed and found  to be effective if there are enough relevant documents. A hybrid approach which feeds back  short documents and passages of long documents may be the best compromise.  1  1 Introduction  As the amount of on-line text has increased, so has the size of individual documents in those collections. Information retrieval methods that could easily be applied to the full text of abstracts or short documents are sometimes less effective or prohibitively expensive for large documents. This problem has led to a resurgence of interest in techniques for handling large texts, including passage retrieval, theme identification, document su...
272|Distinguishing Photographs and Graphics on the World Wide Web|When we search for images in multimedia documents, we often have in mind specific image types that we are interested in; examples are photographs, graphics, maps, cartoons, portraits of people, and so on. This paper describes an automated system that classifies Web images as photographs or graphics, based on their content. The system first submits the images into some tests, which look at the image content, and then feeds the results of those tests into a classifier. The classifier is built using learning techniques, which take advantage of the vast amount of training data that is available on the Web. Text associated with an image can be used to further improve the accuracy of the classification. The system is used as a part of WebSeer, an image search engine for the Web. 1 Introduction Collections of multimedia documents can contain a vast amount of textual and visual information. However, the bigger the size of such collections grows, the harder it gets to locate specific informat...
273|Next-Generation Content Representation, Creation and Searching for New Media Applications in Education|Content creation, editing, and searching are extremely time consuming tasks that often require substantial  training and experience, especially when high-quality audio and video are involved. &#034;New media&#034; represents a new paradigm for multimedia information representation and processing, in which the emphasis is placed on the actual content. It thus brings the tasks of content creation and searching much closer to actual users and enables them to be active producers of audiovisual information rather than passive recipients. We discuss the state-of-the-art and present next-generation techniques for content representation, searching, creation, and editing. We discuss our experiences in developing a Web-based distributed compressed video editing and searching system (WebClip), a media representation language (Flavor) and an object-based video authoring system (Zest) based on it, and large image/video search engines for the World-Wide Web (WebSEEk and VideoQ). We also present a case study of new media applications based on specific planned multimedia education experiments with the above systems in several K-12 schools in Manhattan.
274|Toward a Visual Thesaurus|A thesaurus is a book containing synonyms in a given language; it provides similarity links when trying to retrieve articles or stories about a particular topic. A &#034;visual thesaurus&#034; works with pictures, not words. It aids in recognizing visually similar events, &#034;visual synonyms,&#034; including both spatial and motion similarity. This paper describes a method for building such a tool, and recent research results in the MIT Media Lab which contribute toward this goal. The heart of the method is a learning system which gathers information by interacting with a user of a database. The learning system is also capable of incorporating audio and other perceptual information, ultimately constructing a representation of common sense knowledge.  1 Introduction  Collections of digital imagery are growing at a rapid pace. The contexts are broad, including areas such as entertainment (e.g. searching for a funny movie scene), education (e.g. hunting down illustrations for a book report), science (e.g. ...
275|Image Indexing Using a Texture Dictionary|We propose a new method for indexing large image databases. The method incorporates neural network learning algorithms and pattern recognition techniques to construct an image pattern dictionary. Image retrieval is then formulated as a process of dictionary search to compute the best matching codeword, which in turn indexes into the database items. Experimental results are presented. Keywords: content-based image retrieval, texture, indexing, neural networks, pattern recognition, vector quantization. 1 INTRODUCTION Searching for similar image patterns in a large database can be conceptually visualized as a two step process. In the first step, a set of image processing operations are performed on the query pattern to compute a feature representation. The next step is to search through the database to retrieve patterns which are similar in the feature space. The features could be color, shape, texture, or any other image attributes of interest. Assuming that one can design appropriate ...
276|Digital Libraries: Meeting Place For High-Level And Low-Level Vision|The average person with a networked computer can now understand why computers should have vision -- to search the world&#039;s collections of digital video and images and &#034;retrieve a picture of .&#034; Computer vision for intelligent browsing, querying, and retrieval of imagery is needed now, and yet traditional approaches to computer vision remain far from a general solution to the scene understanding problem. In this paper I discuss the need for a solution based on combining high-level and low-level vision, that works in concert with input from a human user. The solution is based on: 1) Learning from the user what is important visually, and 2) Learning associations between text descriptions and visual data. I describe some recent results in these areas, and overview key challenges for future research in computer vision for digital libraries.  1. INTRODUCTION  Collections of digital imagery are growing at a rapid pace. The contexts are broad, including areas such as entertainment (e.g. searchin...
277|Computationally Fast Bayesian Recognition of Complex Objects Based on Mutual Algebraic Invariants|An effective approach has appeared in the literature for recognizing 2D curve or 3D surface objects of modest complexity based on representing an object by a single implicit polynomial of 3  rd  or 4  th  degree, computing a vector of Euclidean or affine invariants which are functions of the polynomial coefficients, and doing Bayesian object recognition of the invariants [5], thus producing low computational cost robust recognition. This paper extends the approach, as well as an initial work on mutual invariants recognizers [4], to the recognition of objects too complicated to be represented by a single polynomial(Figure 1). Hence, an object to be recognized is partitioned into patches, each patch is represented by a single implicit polynomial, mutual invariants are computed for pairs of polynomials for pairs of patches, and object recognition is Bayesian recognition of vectors of self and mutual invariants. We will discuss why complete object geometry can be captured by the geometry o...
278|Image Segmentation by Directed Region Subdivision|In this paper, an image segmentation method based on directed image region partitioning is proposed. The method consists of two separate stages: a splitting phase followed by a merging phase. The splitting phase starts with an initial coarse triangulation and employs the incremental Delaunay triangulation as a directed image region splitting technique. The triangulation process is accomplished by adding points as vertices one by one into the triangulation. A top-down point selection strategy is proposed for selecting these points in the image domain of grey-value and color images. The merging phase coalesces the oversegmentation, generated by the splitting phase, into homogeneous image regions. Because images might be negatively affected by changes in intensity due to shading or surface orientation change, we propose homogeneity criteria which are robust to intensity changes caused by these phenomena for both grey-value and color images. Performance of the image segmentation method has...
279|Computing Invariants using Elimination Methods|Geometric invariants appear to play an important role in object recognition as an aid to building model libraries of objects. Useful invariants are often found by extensive experience and they are based on geometric invariant properties studied by algebraists over many years. Given a geometric configuration, there is however a need to systematically generate and search for its invariants. In this paper we give a complete solution, in principle, to computing a single invariant for a geometric configuration, if it exists. The algorithm works in three steps: (i) the problem formulation step in which algebraic relations are established between object parameters and image parameters (or equivalently, parameters of two different images) using an imaging transformation, (ii) elimination of transformation parameters resulting in an invariant relation between object and image parameters, and (iii) finally, extraction of a single invariant from the algebraic relation. The main contribution of th...
280|Computer Learning of Subjectivity|tic control knobs.&#034; For example, the Wold model for retrieving perceptually similar visual patterns has knobs corresponding to periodicity, directionality, and randomness [2]. However, rarely can models with semantic control knobs be found. Even when they exist, it is an effort to know how to optimally set them. Moreover, usually a person does not make a single query, but a succession of queries, with slight variations each time. Therefore, she not only needs to know how to set the knobs when initiating a query session, but also how to adjust them with each new query. What I have described is the current trend in content-based retrieval and annotation systems, and it needs to change. The system must recognize that the user&#039;s goals evolve while they browse; subjectivity, mood-dependence, and fickleness are to be expected. Furthermore, a system that tracks the evolving goals of a subjective human will also be helpful for the difficult but common query sessions best described as &#034;I&#039;ll kno
281|Compressed-domain content-based image and video retrieval|With more and more visual material produced and stored in visual information systems (VIS) (i.e., image databases or video servers), the need for efficient, effective methods for indexing, searching, and retrieving images and videos from large collections has become critical. Users of large VIS will desire a more powerful method for searching images than just traditional text-based query (e.g., keywords). Manual creation of keywords
282|Visual detection in relation to display size and redundancy of critical elements |Visual detection was studied in relation to displays of discrete elements, randomly selected consonant letters, distributed in random subsets of cells of a matrix, the subject being required on each trial to indicate only which member of a predesignated pair of critical elements was present in a given display. Experimental variables were number of elements per display and number of redundant critical elements per display. Estimates of the number of elements effectively processed by a subject during a 50 ms. exposure increased with display size, but not in the manner that would be expected if the subject sampled a fixed proportion of the elements present in a display of given area. Test-retest data indicated substantial correlations over long intervals of time in the particular elements sampled by a subject
283|A Guided Tour to Approximate String Matching|We survey the current techniques to cope with the problem of string matching allowing  errors. This is becoming a more and more relevant issue for many fast growing areas such  as information retrieval and computational biology. We focus on online searching and mostly  on edit distance, explaining the problem and its relevance, its statistical behavior, its history  and current developments, and the central ideas of the algorithms and their complexities. We  present a number of experiments to compare the performance of the different algorithms and  show which are the best choices according to each case. We conclude with some future work  directions and open problems.   
284|Modern Information Retrieval|Information retrieval (IR) has changed considerably in the last years with the expansion of the Web (World Wide Web) and the advent of modern and inexpensive graphical user interfaces and mass storage devices. As a result, traditional IR textbooks have become quite out-of-date which has led to the introduction of new IR books recently. Nevertheless, we believe that there is still great need of a book that approaches the field in a rigorous and complete way from a computer-science perspective (in opposition to a user-centered perspective). This book is an effort to partially fulfill this gap and should be useful for a first course on information retrieval as well as for a graduate course on the topic. The book
285|A New Approach to Text Searching |We introduce a family of simple and fast algorithms for solving the classical string matching problem, string matching with classes of symbols, don&#039;t care symbols and complement symbols, and multiple patterns. In addition we solve the same problems allowing up to k mismatches. Among the features of these algorithms are that they don&#039;t need to buffer the input, they are real time algorithms (for constant size patterns), and they are suitable to be implemented in hardware. 1 Introduction  String searching is a very important component of many problems, including text editing, bibliographic retrieval, and symbol manipulation. Recent surveys of string searching can be found in [17, 4]. The string matching problem consists of finding all occurrences of a pattern of length  m in a text of length n. We generalize the problem allowing &#034;don&#039;t care&#034; symbols, the complement of a symbol, and any finite class of symbols. We solve this problem for one or more patterns, with or without mismatches. Fo...
286|An O(ND) Difference Algorithm and Its Variations  (1986) |The problems of finding a longest common subsequence of two sequences A and B and a shortest edit script for transforming A into B have long been known to be dual problems. In this paper, they are shown to be equivalent to finding a shortest/longest path in an edit graph. Using this perspective, a simple O(ND) time and space algorithm is developed where N is the sum of the lengths of A and B and D is the size of the minimum edit script for A and B. The algorithm performs well when differences are small (sequences are similar) and is consequently fast in typical applications. The algorithm is shown to have O(N +D    expected-time performance under a basic stochastic model. A refinement of the algorithm requires only O(N) space, and the use of suffix trees leads to an O(NlgN +D    ) time variation.
287|A fast bit-vector algorithm for approximate string matching based on dynamic programming|Abstract. The approximate string matching problem is to find all locations at which a query of length m matches a substring of a text of length n with k-or-fewer differences. Simple and practical bit-vector algorithms have been designed for this problem, most notably the one used in agrep. These algorithms compute a bit representation of the current state-set of the k-difference automaton for the query, and asymptotically run in either O(nmk/w) orO(nm log ?/w) time where w is the word size of the machine (e.g., 32 or 64 in practice), and ? is the size of the pattern alphabet. Here we present an algorithm of comparable simplicity that requires only O(nm/w) time by virtue of computing a bit representation of the relocatable dynamic programming matrix for the problem. Thus, the algorithm’s performance is independent of k, and it is found to be more efficient than the previous results for many choices of k and small m. Moreover, because the algorithm is not dependent on k, it can be used to rapidly compute blocks of the dynamic programming matrix as in the 4-Russians algorithm of Wu et al. [1996]. This gives rise to an O(kn/w) expected-time algorithm for the case where m may be arbitrarily large. In practice this new algorithm, that computes a region of the dynamic programming (d.p.) matrix w entries at a time using the basic algorithm as a subroutine, is significantly faster than our previous 4-Russians algorithm, that computes the same region 4 or 5 entries at a time using table lookup. This performance improvement yields a code that is either superior or competitive with all existing algorithms except for some filtration algorithms that are superior when k/m is sufficiently small.
288|Approximate string matching|Approximate matching of strings is reviewed with the aim of surveying techniques suitable for finding an item in a database when there may be a spelling mistake or other error in the keyword. The methods found are classified as either equivalence or similarity problems. Equivalence problems are seen to be readily solved using canonical forms. For sinuiarity problems difference measures are surveyed, with a full description of the well-establmhed dynamic programming method relating this to the approach using probabilities and likelihoods. Searches for approximate matches in large sets using a difference function are seen to be an open problem still, though several promising ideas have been suggested. Approximate matching (error correction) during parsing is briefly reviewed.
289|Practical fast searching in strings|The problem of searching through text to find a specified substring is considered in a practical setting. It is discovered that a method developed by Boyer and Moore can outperform even special-purpose search instructions that may be built into the, computer hardware. For very short substrings however, these special purpose instructions are fastest-provided that they are used in an optimal way. KEY WORDS String searching Pattern matching Text editing Bibliographic search
290|Speeding Up Two String-Matching Algorithms| We show how to speed up two string-matching algorithms: the Boyer-Moore algorithm (BM algorithm), and its version called here the reverse factor algorithm (RF algorithm). The RF algorithm is based on factor graphs for the reverse of the pattern.The main feature of both algorithms is that they scan the text right-to-left from the supposed right position of the pattern. The BM algorithm goes as far as the scanned segment (factor) is a suffix of the pattern. The RF algorithm scans while the segment is a factor of the pattern. Both algorithms make a shift of the pattern, forget the history, and start again. The RF algorithm usually makes bigger shifts than BM, but is quadratic in the worst case. We show that it is enough to remember the last matched segment (represented by two pointers to the text) to speed up the RF algorithm considerably (to make a linear number of inspections of text symbols, with small coefficient), and to speed up the BM algorithm (to make at most 2.n comparisons). Only a constant additional memory is needed for the search phase. We give alternative versions of an accelerated RF algorithm: the first one is based on combinatorial properties of primitive words, and the other two use the power of suffix trees extensively. The paper demonstrates the techniques to transform algorithms, and also shows interesting new applications of data structures representing all subwords of the pattern in compact form.
291|Transducers and repetitions|Abstract. The factor transducer of a word associates to each of its factors (or subwc~rds) their first occurrence. Optimal bounds on the size of minimal factor transducers together with an algorithm for building them are given. Analogue results and a simple algorithm are given for the case of subsequential suffix transducers. Algorithms are applied to repetition searching in words. Rl~sum~. Le transducteur des facteurs d&#039;un mot associe a chacun de ses facteurs leur premiere occurrence. On donne des bornes optimales sur la taille du transducteur minimal d&#039;un mot ainsi qu&#039;un algorithme pour sa construction. On donne des r6sultats analogues et un algorithme simple dans le cas du transducteur sous-s~luentiel des suffixes d&#039;un mot. On donne une application la d6tection de r6p6titions dans les mots. Contents
292|Faster Approximate String Matching|We present a new algorithm for on-line approximate string matching. The algorithm is based on the simulation of a non-deterministic finite automaton built from the pattern and using the text as input. This simulation uses bit operations on a RAM machine with word length w = \Omega\Gamma137 n) bits, where n is the text size. This is essentially similar to the model used in Wu and Manber&#039;s work, although we improve the search time by packing the automaton states differently. The running time achieved is O(n) for small patterns (i.e. whenever mk = O(log n)),  where m is the pattern length and k ! m the number of allowed errors. This is in contrast with the result of Wu and Manber, which is O(kn) for m = O(log n). Longer patterns can be processed by partitioning the automaton into many machine words, at O(mk=w n) search cost. We allow generalizations in the pattern, such as classes of characters, gaps and others, at essentially the same search cost. We then explore other novel techniques t...
294|Text Retrieval: Theory and Practice|We present the state of the art of the main component of text retrieval systems: the searching engine. We outline the main lines of research and issues involved. We survey recently published results for text searching and we explore the gap between theoretical vs. practical algorithms. The main observation is that simpler ideas are better in practice.  1597 Shaks. Lover&#039;s Compl. 2 From off a hill whose concaue wombe reworded A plaintfull story from a sistring vale.  OED2, reword, sistering  1 1 Introduction  Full text retrieval systems are becoming a popular way of providing support for on-line text. Their main advantage is that they avoid the complicated and expensive process of semantic indexing. From the end-user point of view, full text searching of on-line documents is appealing because a valid query is just any word or sentence of the document. However, when the desired answer cannot be obtained with a simple query, the user must perform his/her own semantic processing to guess w...
295|Block Edit Models for Approximate String Matching|In this paper we examine string block edit distance, in which two strings A and B  are compared by extracting collections of substrings and placing them into correspondence. This model accounts for certain phenomena encountered in important real-world applications, including pen computing and molecular biology. The basic problem admits a family of variations depending on whether the strings must be matched in their entireties, and whether overlap is permitted. We show that several variants are NPcomplete, and give polynomial-time algorithms for solving the remainder. Keywords: block edit distance, approximate string matching, sequence comparison, approximate ink matching, dynamic programming. 1 Introduction  The edit distance model for string comparison [Lev66, NW70, WF74] has found widespread application in fields ranging from molecular biology to bird song classification [SK83]. A great deal of research has been devoted to this area, and numerous algorithms have been proposed for com...
296|Incremental String Comparison|The problem of comparing two sequences A and B to determine their LCS or the edit distance between them has been much studied. In this paper we consider the following incremental version of these problems: given an appropriate encoding of a comparison between A and B, can one incrementally compute the answer for A and bB, and the answer for A and Bb with equal efficiency, where b is an additional symbol? Our main result is a theorem exposing a surprising relationship between the dynamic programming solutions for two such &#034;adjacent&#034; problems. Given a threshold k  on the number of differences to be permitted in an alignment, the theorem leads directly to an O(k)  algorithm for incrementally computing a new solution from an old one, as contrasts the O(k²) time required to compute a solution from scratch. We further show with a series of applications that this algorithm is indeed more powerful than its non-incremental counterpart by solving the applications with greater asymptotic ef...
297|A Comparison of Approximate String Matching Algorithms|Experimental comparison of the running time of approximate string matching algorithms for the?differences problem is presented. Given a pattern string, a text string, and integer?, the task is to find all approximate occurrences of the pattern in the text with at most?differences (insertions, deletions, changes). We consider seven algorithms based on different approaches including dynamic programming, Boyer-Moore string matching, suffix automata, and the distribution of characters. It turns out that none of the algorithms is the best for all values of the problem parameters, and the speed differences between the methods can be considerable. 2??? KEY WORDS String matching Edit distance k differences problem
298|Block Addressing Indices for Approximate Text Retrieval|Although the issue of approximate text retrieval is gaining importance in the last years, it is currently addressed by only a few indexing schemes. To reduce space requirements, the indices may point to text blocks instead of exact word positions. This is called &#034;block addressing&#034;. The most notorious index of this kind is Glimpse. However, block addressing has not been well studied yet, especially regarding approximate searching. Our main contribution is an analytical study of the spacetime trade-offs related to the block size. We find that, under reasonable assumptions, it is possible to build an index which is simultaneously sublinear in space overhead and in query time. We validate the analysis with extensive experiments, obtaining typical performance figures. These results are valid not only for approximate searching queries but also for classical ones. Finally, we propose a new strategy for approximate searching on block addressing indices, which we experimentally find 4-5 times f...
299|A Suboptimal Lossy Data Compression Based On Approximate Pattern Matching|A practical suboptimal (variable source coding) algorithm for lossy data compression is presented. This scheme is based on approximate string matching, and it naturally extends the lossless Lempel-Ziv data compression scheme. Among others we consider the typical length of approximately repeated pattern within the first n positions of a stationary mixing sequence where D% of mismatches is allowed. We prove that there exists a constant r 0 (D) such that the length of such an approximately repeated pattern converges in probability to 1=r 0 (D) log n (pr.) but it almost surely oscillates between 1=r \Gamma1 (D) log n and 2=r 1 (D) log n,  where r \Gamma1 (D) ? r 0 (D) ? r 1 (D)=2 are some constants. These constants are natural generalizations of R&#039;enyi entropies to the lossy environment. More importantly, we show that the compression ratio of a lossy data compression scheme based on such an approximate pattern matching is asymptotically equal to r 0 (D). We also establish the asymptotic be...
300|NR-grep: A Fast and Flexible Pattern Matching Tool|We present nrgrep (&#034;nondeterministic reverse grep&#034;), a new pattern matching tool designed  for efficient search of complex patterns. Unlike previous tools of the grep family, such as agrep  and Gnu grep, nrgrep is based on a single and uniform concept: the bit-parallel simulation  of a nondeterministic suffix automaton. As a result, nrgrep can find from simple patterns to  regular expressions, exactly or allowing errors in the matches, with an efficiency that degrades  smoothly as the complexity of the searched pattern increases. Another concept fully integrated  into nrgrep and that contributes to this smoothness is the selection of adequate subpatterns for  fast scanning, which is also absent in many current tools. We show that the efficiency of nrgrep  is similar to that of the fastest existing string matching tools for the simplest patterns, and by  far unpaired for more complex patterns.
301|Approximate String Matching: A Simpler Faster Algorithm|Abstract. We give two algorithms for finding all approximate matches of a pattern in a text, where the edit distance between the pattern and the matching text substring is at most k. The first algorithm, which is quite simple, runs in time O ( nk3 + n + m) on all patterns except k-break periodic m strings (defined later). The second algorithm runs in time O ( nk4 + n + m) onk-break periodic m patterns. The two classes of patterns are easily distinguished in O(m) time.
302|Large Text Searching Allowing Errors|. We present a full inverted index for exact and approximate string matching in large texts. The index is composed of a table containing the vocabulary of words of the text and a list of positions in the text corresponding to each word. The size of the table of words is usually much less than 1% of the text size and hence can be kept in main memory, where most query processing takes place. The text, on the other hand, is not accessed at all. The algorithm permits a large number of variations of the exact and approximate string search problem, such as phrases, string matching with sets of characters (range and arbitrary set of characters, complements, wild cards), approximate search with nonuniform costs and arbitrary regular expressions. The whole index can be built in linear time, in a single sequential pass over the text, takes near 1=3 the space of the text, and retrieval times are near O(  p  n)  for typical cases. Experimental results show that the algorithm works well in practice...
303|Approximate multiple string search|Abstract. This paper presents a fast algorithm for searching a large text for multiple strings allowing one error. On a fast workstation, the algo-rithm can process a megabyte of text searching for 1000 patterns (with one error) in less than a second. Although we combine several interest-ing techniques, overall the algorithm is not deep theoretically. The emphasis of this paper is on the experimental side of algorithm design. We show the importance of careful design, experimentation, and utiliza-tion of current architectures. In particular, we discuss the issues of locality and cache performance, fast hash functions, and incremental hashing techniques. We introduce the notion of two-level hashing, which utilizes cache behavior to speed up hashing, especially in cases where unsuccessful searches are not uncommon. Two-level hashing may be useful for many other applications. The end result is also interesting by itself. We show that multiple search with one error is fast enough for most text applications. 1.
304|A Practical q-Gram Index for Text Retrieval Allowing Errors|We propose an indexing technique for approximate text searching, which is practical and powerful, and especially optimized for natural language text. Unlike other indices of this kind, it is able to retrieve any string that approximately matches the search pattern, not only words. Every text substring of a fixed length q is stored in the index, together with pointers to all the text positions where it appears. The search pattern is partitioned into pieces which are searched in the index, and all their occurrences in the text are verified for a complete match. To reduce space requirements, pointers to blocks instead of exact positions can be used, which increases querying costs. We design an algorithm to optimize the pattern partition into pieces so that the total number of verifications is minimized. This is especially well suited for natural language texts, and allows to know in advance the expected cost of the search and the expected relevance of the query to the user. We show experi...
305|Episode matching |Abstract. Given two words, text T of length n and episode P of length m, the episode matching problem is to find all minimal length substrings of text T that contain episode P as a subsequence. The respective optimization problem is to find the smallest number w, s.t. text T has a subword of length w which contains episode P. In this paper, we introduce a few efficient off-line as well as on-line algorithms for the entire problem, where by on-line algorithms we mean algorithms which search from left to right consecutive text symbols only once. We present two alphabet independent algorithms which work in time O(nm). The off-line algorithm operates in O(1) additional space while the on-line algorithm pays for its property with O(m) additional space. Two other on-line algorithms have subquadratic time complexity. One of them works in time O(nm/log m) and O(m) additional space. The other one gives a time/space trade-off, i.e., it works in time O(n + s +nm log log s ~ log(s/m)) when additional space is limited to O(s). Finally, we present two approximation algorithms for the optimization problem. The off-line algorithm is alphabet independent, it has superlinear time complexity O(n/e + nloglog(n/m)) and it uses only constant space. The on-line algorithm works in time O(n/e + n) and uses space O(m). Both approximation algorithms achieve 1 + e approximation ratio, for any e&gt; 0. 1
306|Pattern Matching with Swaps|Let a text string T of n symbols and a pattern string P of m symbols from alphabet \Sigma be given. A swapped version T  0  of T is a length n string derived from T by a series of local swaps,  (i.e. t  0  ` / t `+1 and t  0  `+1 / t ` ) where each element can participate in no more than one swap.  The Pattern Matching with Swaps problem is that of finding all locations i for which there exists a swapped version T  0  of T where there is an exact matching of P in location i of T  0  . It has been an open problem whether swapped matching can be done in less than O(mn) time. In this paper we show the first algorithm that solves the pattern matching with swaps problem in time o(mn). We present an algorithm whose time complexity is O(nm  1=3  log m log  2  min(m; j\Sigmaj))  for a general alphabet \Sigma.  Key Words: Design and analysis of algorithms, combinatorial algorithms on words, pattern matching, pattern matching with swaps, non-standard pattern matching.   Department of Mathematics...
307|Applications of Approximate Word Matching in Information Retrieval|As more online databases are integrated into digital libraries, the issue of quality control of the data becomes increasingly important, especially as it relates to the effective retrieval of information. The need to discover and reconcile variant forms of strings in bibliographic entries, i.e., authority work, will become more critical in the future. Spelling variants, misspellings, and transllteration differences will all increase the difficulty of retrieving information. Approximate string matching has traditionally been used to help with this problem. In this paper we introduce the notion of approximate word matching and show how it can be used to improve detection and categorization of variant forms.
308|On the Searchability of Electronic Ink|Pen-based computers and personal digital assistant&#039;s (PDA&#039;s) are new technologies that are growing in importance. In previous papers, we have espoused a philosophy we call &#034;Computing in the Ink Domain&#034; that treats ink as a first-class datatype. One of the most important questions that arises under this model concerns the searching of large quantities of previously stored pen-stroke data. In this paper, we examine the ink search problem. We present an algorithm based on a known dynamic programming technique, and examine its performance under a variety of circumstances.  Keywords: pen computing, approximate string matching, edit distance. 1 Introduction  Despite several early, high-profile &#034;flops,&#034; pen-based computers and personal digital assistants (PDA&#039;s) are important technologies that are now starting to find acceptance. This synthesis of new hardware and software raises many systems-level issues, including the possibility of new paradigms for human-computer interaction. In previous ...
309|Multiple Approximate String Matching|We present two new algorithms for on-line multiple approximate string matching. These are extensions of previous algorithms that search for a single pattern. The single-pattern version of the first one is based on the simulation with bits of a non-deterministic finite automaton built from the pattern and using the text as input. To search for multiple patterns, we superimpose their automata, using the result as a filter. The second algorithm partitions the pattern in sub-patterns that are searched with no errors, with a fast exact multipattern search algorithm. To handle multiple patterns, we search the sub-patterns of all of them together. The average running time achieved is in both cases O(n) for moderate error level, pattern length and number of patterns. They adapt (with higher costs) to the other cases. However, the algorithms differ in speed and thresholds of usefulness. We analyze theoretically when each algorithm should be used, and show experimentally that they are faster ...
310|Multiple Approximate String Matching by Counting|. We present a very simple and efficient algorithm for online multiple approximate string matching. It uses a previously known counting-based filter [9] that searches for a single pattern by quickly discarding uninteresting parts of the text. Our multi-pattern algorithm is based on the simulation of many parallel filters using bits of the computer word. Our average complexity to search r patterns of length m is  O(rn log m= log n), being n is the text size. We can search patterns of different length, each one with a different number of errors. We show experimentally that our algorithm is competitive with the fastest known algorithms, being the fastest for a wide range of intermediate error ratios. We give the first average-case analysis of the filtering efficiency of the counting method, applicable also to [9]. 1 Introduction  A number of important problems related to string processing lead to algorithms for approximate string matching: text searching, pattern recognition, computationa...
311|Fast String Matching with Mismatches|We describe and analyze three simple and fast algorithms on the average for solving the problem of string matching with a bounded number of mismatches. These are the naive algorithm, an algorithm based on the Boyer-Moore approach, and ad-hoc deterministic finite automata searching. We include simulation results that compare these algorithms to previous works. 1 Introduction  The problem of string matching with k mismatches consists of finding all occurrences of a pattern of length m in a text of length n such that in at most k positions the text and the pattern have different symbols. In the following, we assume that 0 ! k ! m and m  n. The case of k = 0 is the well known exact string matching problem, and if k = m the solution is trivial. Landau and Vishkin [LV86] gave the first efficient algorithm to solve this particular problem. Their algorithm uses O(kn + km log m)) time and O(k(n + m)) space. While it is fast, the space required is unacceptable for most practical purposes. Galil ...
312|Improving an Algorithm for Approximate Pattern Matching|We study a recent algorithm for fast on-line approximate string matching. This is the  problem of searching a pattern in a text allowing errors in the pattern or in the text. The  algorithm is based on a very fast kernel which is able to search short patterns using a nondeterministic  finite automaton, which is simulated using bit-parallelism. A number of techniques  to extend this kernel for longer patterns are presented in that work. However, the techniques  can be integrated in many ways and the optimal interplay among them is by no means obvious.  The solution to this problem starts at a very low level, by obtaining basic probabilistic  information about the problem which was not previously known, and ends integrating analytical  results with empirical data to obtain the optimal heuristic. The conclusions obtained via analysis  are experimentally confirmed. We also improve many of the techniques and obtain a combined  heuristic which is faster than the original work.  This work sho...
313|New and Faster Filters for Multiple Approximate String Matching|We present three new algorithms for on-line multiple string matching allowing errors. These  are extensions of previous algorithms that search for a single pattern. The average running  time achieved is in all cases linear in the text size for moderate error level, pattern length and number of patterns. They adapt (with higher costs) to the other cases. However, the algorithms  differ in speed and thresholds of usefulness. We analyze theoretically when each algorithm should be used, and show experimentally their performance. The only previous solution for this  problem allows only one error. Our algorithms are the first to allow more errors, and are faster  than previous work for a moderate number of patterns (e.g. less than 50-100 on English text, depending on the pattern length). 
314|Approximate string searching under weighted edit distance|Abstract. Let p ? S * be a string of length m and t ? S * be a string of length n. The approximate string searching problem is to find all approximate matches of p in t having weighted edit distance at most k from p. We present a new method that preprocesses the pattern into a DFA which scans t online in linear time, thereby recognizing all positions in t where an approximate match ends. We show how to reduce the exponential preprocessing effort and propose two practical algorithms. The first algorithm constructs the states of the DFA up to a certain depth r = 1. It runs in O(|S | r+1 · m + q · m + n) time and O(|S | r+1 + |S | r ·m) space where q = n decreases as r increases. The second algorithm constructs the transitions of the DFA when they are demanded. It runs in O(qs·|S|+qt·m+n) time and O(qs·(|S|+m)) space where qs = qt = n depend on the problem instance. Practical measurements show that our algorithms work well in practice and beat previous methods for problems of interest in molecular biology. 1
315|A Unified View to String Matching Algorithms|  We present a unified view to sequential algorithms for many  pattern matching problems, using a finite automaton built from the pattern  which uses the text as input. We show the limitations of deterministic  finite automata (DFA) and the advantages of using a bitwise  simulation of non-deterministic finite automata (NFA). This approach  gives very fast practical algorithms which have good complexity for small  patterns on a RAM machine with word length O(log n), where n is the  size of the text. For generalized string matching the time complexity is  O(mn= log n) which for small patterns is linear. For approximate string  matching we show that the two main known approaches to the problem  are variations of the NFA simulation. For this case we present a different  simulation technique which gives a running time of O(n) independently  of the maximum number of errors allowed, k, for small patterns. This  algorithm improves the best bit-wise or comparison based algorithms of  running ti...
316|A Partial Deterministic Automaton for Approximate String Matching|. One of the simplest approaches to approximate string matching is to consider the associated non-deterministic finite automaton and make it deterministic. Besides automaton generation, the search time is  O(n) in the worst case, where n is the text size. This solution is mentioned in the classical literature but has not been further pursued, due to the large number of automaton states that may be generated. We study the idea of generating the deterministic automaton on the fly. That is, we only generate the states that are actually reached when the text is traversed. We show that this limits drastically the number of states actually generated. Moreover, the algorithm is competitive, being the fastest one for intermediate error ratios and pattern lengths. 1 Introduction  Approximate string matching is one of the main problems in classical string algorithms, with applications to text searching, computational biology, pattern recognition, etc. The problem is defined as follows: given a t...
317|Improved Approximate Pattern Matching on Hypertext|. The problem of approximate pattern matching on hypertext is defined and solved by Amir et al. in O(m(n log m + e)) time, where  m is the length of the pattern, n is the total text size and e is the total number of edges. Their space complexity is O(mn). We present a new algorithm which is O(mk(n + e)) time and needs only O(n) extra space, where k ! m is the number of allowed errors in the pattern. If the graph is acyclic, our time complexity drops to O(m(n + e)), improving Amir&#039;s results. 1 Introduction  Approximate string matching problems appear in a number of important areas related to string processing: text searching, pattern recognition, computational biology, audio processing, etc. The edit distance between two strings a and b, ed(a; b), is defined as the minimum number of edit operations that must be carried out to make them equal. The allowed operations are insertion, deletion and substitution of characters in  a or b. The problem of approximate string matching is defined as...
318|Estimating the Probability of Approximate Matches|this paper addresses how to define S k (P ) and how to solve the algorithmic sub-problems involved in an efficient realization with respect to this definition. Section 2 introduces as our choice for S k (P ) the set of what we call the condensed, canonical edit scripts. Our choice attempts to keep small, both (i) the number of edit scripts for which X(s) = 0, and (ii) the size of g(v). Doing so improves the convergence of the estimator as it places S k (P ) and CN k (P ) in closer correspondence. The remaining sections present dynamic programming algorithms for the following subtasks:
319|Efficient Algorithms for Approximate String Matching with Swaps|this paper we include the swap operation that interchanges two adjacent characters  into the set of allowable edit operations, and we present an O(t min(m, n))-time  algorithm for the extended edit distance problem, where t is the edit distance  between the given strings, and an O(kn)-time algorithm for the extended k-differ-  ences problem. That is, we add swaps into the set of edit operations without  increasing the time complexities of previous algorithms that consider only changes,  insertions, and deletions for the edit distance and k-differences problems. # 1999  Academic Press  1. INTRODUCTION  Given two strings A[1}}}m] and B[1}}}n] over an alphabet 7, the edit distance between A and&lt;F12
320|Fast Multi-Dimensional Approximate Pattern Matching|. We address the problem of approximate string matching in  d dimensions, that is, to find a pattern of size m  d  in a text of size n  d  with at most k ! m  d  errors (substitutions, insertions and deletions along any dimension). We use a novel and very flexible error model, for which there exists only an algorithm to evaluate the similarity between two elements in two dimensions at O(m  4  ) time. We extend the algorithm to d dimensions, at O(d!m  2d  ) time and O(d!m  2d\Gamma1  ) space. We also give the first search algorithm for such model, which is O(d!m  d  n  d  ) time and O(d!m  d  n  d\Gamma1  ) space. We show how to reduce the space cost to O(d!3  d  m  2d\Gamma1  ) with little time penalty. Finally, we present the first sublinear-time (on average) searching algorithm (i.e. not all text cells are inspected), which is O(kn  d  =m  d\Gamma1  ) for k ! (m=(d(log oe m \Gamma log oe d)))  d\Gamma1  , where oe is the alphabet size. After that error level the filter still remains ...
321|Adaptive Constraint Satisfaction|Many different approaches have been applied to constraint satisfaction. These range from complete backtracking algorithms to sophisticated distributed configurations. However, most research effort in the field of constraint satisfaction algorithms has concentrated on the use of a single algorithm for solving all problems. At the same time, a consensus appears to have developed to the effect that it is unlikely that any single algorithm is always the best choice for all classes of problem. In this paper we argue that an adaptive approach should play an important part in constraint satisfaction. This approach relaxes the commitment to using a single algorithm once search commences. As a result, we claim that it is possible to undertake a more focused approach to problem solving, allowing for the correction of bad algorithm choices and for capitalising on opportunities for gain by dynamically changing to more suitable candidates.
322|Minimizing conflicts: a heuristic repair method for constraint satisfaction and scheduling problems| This paper describes a simple heuristic approach to solving large-scale constraint satisfaction and scheduling problems. In this approach one starts with an inconsistent assignment for a set of variables and searches through the space of possible repairs. The search can be guided by a value-ordering heuristic, the min-conflicts heuristic, that attempts to minimize the number of constraint violations after each step. The heuristic can be used with a variety of different search strategies. We demonstrate empirically that on the n-queens problem, a technique based on this approach performs orders of magnitude better than traditional backtracking techniques. We also describe a scheduling application where the approach has been used successfully. A theoretical analysis is presented both to explain why this method works well on certain types of problems and to predict when it is likely to be One of the most promising general approaches for solving combinatorial search problems is to generate an
323|Algorithms for Constraint-Satisfaction Problems: A Survey|A large number of problems in AI and other areas of computer science can be viewed as special cases of the constraint-satisfaction problem. Some examples are machine vision, belief maintenance, scheduling, temporal reasoning, graph problems, floor plan design, the planning of genetic experiments, and the satisfiability problem. A number of different approaches have been developed for solving these problems. Some of them use constraint propagation to simplify the original problem. Others use backtracking to directly search for possible solutions. Some are a combination of these two techniques. This article overviews many of these approaches in a tutorial fashion.  
324|Noise strategies for improving local search|It has recently been shown that local search issurprisingly good at nding satisfying assignments for certain computationally hard classes of CNF formulas. The performance of basic local search methods can be further enhanced by introducing mechanisms for escaping from local minima in the search space. We will compare three such mechanisms: simulated annealing, random noise, and a strategy called \mixed random walk&#034;. We show that mixed random walk is the superior strategy. Wealso present results demonstrating the e ectiveness of local search withwalk for solving circuit synthesis and circuit diagnosis problems. Finally, wedemonstrate that mixed random walk improves upon the best known methods for solving MAX-SAT problems.
325|Hybrid Algorithms for the Constraint Satisfaction Problem|problem (csp), namely, naive backtracking (BT), backjumping (BJ), conflict-directed backjumping
326|Domain-Independent Extensions to GSAT: Solving Large Structured Satisfiability Problems|GSAT is a randomized local search procedure  for solving propositional satisfiability  problems (Selman et al. 1992). GSAT can  solve hard, randomly generated problems that  are an order of magnitude larger than those  that can be handled by more traditional approaches  such as the Davis-Putnam procedure.  GSAT also efficiently solves encodings  of graph coloring problems, N-queens, and  Boolean induction. However, GSAT does not  perform as well on handcrafted encodings of  blocks-world planning problems and formulas  with a high degree of asymmetry. We  present three strategies that dramatically improve  GSAT&#039;s performance on such formulas.  These strategies, in effect, manage to uncover  hidden structure in the formula under considerations,  thereby significantly extending the  applicability of the GSAT algorithm.  
327|Practical Applications of Constraint Programming|Constraint programming is newly flowering in industry. Several companies have recently started up to exploit the technology, and the number of industrial applications is now growing very quickly. This survey will seek, by examples,
328|Genet: A connectionist architecture for solving constraint satisfaction problems by iterative improvement|New approaches to solving constraint satisfaction problems using iterative improvement techniques have been found to be successful on certain, very large problems such as the million queens. However, on highly constrained problems it is possible for these methods to get caught in local minima. In this paper we present genet, a connectionist architecture for solving binary and general constraint satisfaction problems by iterative improvement. genet incorporates a learning strategy to escape from local minima. Although genet has been designed to be implemented on vlsi hardware, we present empirical evidence to show that even when simulated on a single processor genet can outperform existing iterative improvement techniques on hard instances of certain constraint satisfaction problems.
329|The Birth of Prolog|The programming language, Prolog, was born of a project aimed not at producing a programming language but at processing natural languages; in this case, French. The project gave rise to a preliminary version of Prolog at the end of 1971 and a more definitive version at the end of 1972. This article gives the history of this project and describes in detail the preliminary and then the final versions of Prolog. The authors also felt it appropriate to describe the Q-systems since it was a language which played a prominent part in Prolog’s genesis.
330|Guided local search and its application to the traveling salesman problem|The Traveling Salesman Problem (TSP) is one of the most famous problems in combinatorial optimization. In this paper, we are going to examine how the techniques of Guided Local Search (GLS) and Fast Local Search (FLS) can be applied to the problem. Guided Local Search sits on top of local search heuristics and has as a main aim to guide these procedures in exploring efficiently and effectively the vast search spaces of combinatorial optimization problems. Guided Local Search can be combined with the neighborhood reduction scheme of Fast Local Search which significantly speeds up the operations of the algorithm. The combination of GLS and FLS with TSP local search heuristics of different efficiency and effectiveness is studied in an effort to determine the dependence of GLS on the underlying local search heuristic used. Comparisons are made with some of the best TSP heuristic algorithms and general optimization techniques which demonstrate the advantages of GLS over alternative heuristic approaches suggested for the problem.
331|An Empirical Analysis of Search in GSAT|We describe an extensive study of search in GSAT, an approximation procedure for  propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied  clauses in a truth assignment. Our experiments provide a more complete picture of GSAT&#039;s  search than previous accounts. We describe in detail the two phases of search: rapid hillclimbing  followed by a long plateau search. We demonstrate that when applied to randomly  generated 3-SAT problems, there is a very simple scaling with problem size for both the  mean number of satisfied clauses and the mean branching rate. Our results allow us to  make detailed numerical conjectures about the length of the hill-climbing phase, the average  gradient of this phase, and to conjecture that both the average score and average branching  rate decay exponentially during plateau search. We end by showing how these results can  be used to direct future theoretical analysis. This work provides a case study of how  computer experiments can be used to improve understanding of the theoretical properties  of algorithms.  1. 
333|Adaptive Constraint Satisfaction: The Quickest First Principle|The choice of a particular algorithm for solving a given class of constraint satisfaction problems is often confused by exceptional behaviour of algorithms. One method of reducing the impact of this exceptional behaviour is to adopt an adaptive philosophy to constraint satisfaction problem solving. In this report we describe one such adaptive algorithm, based on the principle of chaining. It is designed to avoid the phenomenon of exceptionally hard problem instances. Our algorithm shows how the speed of more naïve algorithms can be utilised safe in the knowledge that the exceptional behaviour can be bounded. Our work clearly demonstrates the potential benefits of the adaptive approach and opens a new front of research for the constraint satisfaction community.
334|Partial constraint satisfaction problems and guided local search|A largely unexplored aspect of Constraint Satisfaction Problem (CSP) is that of over-constrained instances for which no solution exists that satisfies all the constraints. In these problems, mentioned in the literature as Partial Constraint Satisfaction Problems (PCSPs), we are often looking for solutions which violate the minimum number of constraints. In more realistic settings, constraints violations incur different costs and solutions are sought that minimize the total cost from constraint violations and possibly other criteria. Problems in this category present enormous difficulty to complete search algorithms. In practical terms, complete search has more or less to resemble the traditional Branch and Bound taking no advantage of the efficient pruning techniques recently developed for CSPs. In this report, we examine how the stochastic search method of Guided Local Search (GLS) can be applied to these problems. The effectiveness of the method is demonstrated on instances of the Radio Link Frequency Assignment Problem (RLFAP), which is a real-world Partial CSP.
335|Solving Constraint Satisfaction Problems with Heuristic-based Evolutionary Algorithms|Evolutionary algorithms (EAs) for solving constraint satisfaction problems  (CSPs) can be roughly divided into two classes: EAs using adaptive  fitness functions and EAs using heuristics. In [5] the most effective EAs of the  first class have been compared experimentally using a large set of benchmark  instances consisting of randomly generated binary CSPs. In this paper we  complete this comparison by studying the most effective EAs that use heuristics.
336|Constraint Logic Programming for Scheduling and Planning|This paper provides an introduction to Finite-domain Constraint Logic Programming (CLP) and its application to problems in scheduling and planning. We cover the fundamentals of CLP and indicate recent developments and trends in the field. Some current limitations are identified, and areas of research that may contribute to addressing these limitations are suggested.
337|An attempt to map the performance of a range of algorithm and heuristic combinations|Constraint satisfaction is the core of many AI and real life problems and much research has been done in this field in recent years. Work has been done in the past on comparing the performance of different algorithms and heuristics. Much of such work has focused on finding &#034;the best&#034; algorithm and heuristic combination for all problems. The objective of this paper is to prove that there is no universally best algorithm and heuristic for all problems-- different problems can be solved most efficiently by different algorithm and heuristic combinations. The implication of this is important because it means that instead of trying to find &#034;the best &#034; algorithms and heuristics, future research should try to identify the application domain of each algorithm and heuristic (i.e. when they are most effective). Furthermore our results point to future research which focuses on how to retrieve the most efficient algorithm for a given problem. The results in this paper provide a first step towards achieving such goals. 
338|On the Selection of Constraint Satisfaction Problem Formulations|This paper outlines a possible method for discriminating between formulations of the same problem. We attempt to relate different ZDC formulations in terms of their relative difficulty. This difficulty is quantified in terms of a new measure known as the T-factor. The result of our work is to demonstrate that in some cases, when very different formulations of the same problem exist, it is possible to identify the formulation that is most likely to be easiest to solve. In the next section we define the T-factors of formulations. In section 3 we present alternative formulations of the well known N-Queens and Zebra problems, together with evaluation of their T-factors. Finally in section 4 we discuss our findings and propose directions for future work.
339|Tackling car sequencing problems using a generic genetic algorithm|The car sequencing problem (CarSP) was seen as a challenge to artificial intelligence. The CarSP is a version of the job-shop scheduling problem which is known to be NP-complete. The task in the CarSP is to schedule a given number of cars (of different types) in a sequence to allow the teams in each work station on the assembly line to fit the required options (e.g. radio, sunroof) on the cars within the capacity of that work station. In unsolvable problems, one would like to minimize the penalties associated to the violation of the capacity constraints. Previous attempts to tackle the problem have either been unsuccessful or restricted to solvable CarSPs only. In this paper, we report on promising results in applying a generic genetic algorithm, which we call GAcSP, to tackle both solvable and unsolvable CarSPs.
340|Ng-Backmarking - an Algorithm for Constraint Satisfaction|Ng-backmarking with Min-conflict repair, a hybrid algorithm for solving constraint satisfaction problems, is presented in the context of the four main approaches to constraint satisfaction and optimisation: tree-search, domainfiltering, solution repair, and learning while searching. Repair-based techniques are often designed to use local gradients to direct the search for a solution to a constraint satisfaction problem. It has been shown experimentally that such techniques are often well suited to solving large scale problems. One drawback is that they do not guarantee a (optimal) solution if one exists. The motivation behind ng-backmarking is to allow the search to follow local gradients in the search space whilst ensuring a (optimal) solution if one exists. The search space of this combined approach is controlled by the ng-backmarking process, a method of learning constraints during search (at each failure point  1  ) that may be used to avoid the repeated traversing of failed paths ...
341|The architecture of complexity|A number of proposals have been advanced in recent years for the development of “general systems theory ” that, abstracting from properties peculiar to physical, biological, or social systems, would be applicable to all of them. 1 We might well feel that, while the goal is laudable, systems of such diverse kinds could hardly be expected to have any nontrivial properties in common. Metaphor and analogy can be helpful, or they can be misleading. All depends on whether the similarities the metaphor captures are significant or superficial. It may not be entirely vain, however, to search for common properties among diverse kinds of complex systems. The ideas that go by the name of cybernetics constitute, if not a theory, at least a point of view that has been proving fruitful over a wide range of applications. 2 It has been useful to look at the behavior of adaptive systems in terms of the concepts of feedback and homeostasis, and to analyze adaptiveness in terms of the theory of selective information. 3 The ideas of feedback and information provide a frame of reference for viewing a wide range of situations, just as do the ideas of evolution, of relativism, of axiomatic method, and of
342|An Efficient Boosting Algorithm for Combining Preferences|The problem of combining preferences arises in several applications, such as combining the results of different search engines. This work describes an efficient algorithm for combining multiple preferences. We first give a formal framework for the problem. We then describe and analyze a new boosting algorithm for combining preferences called RankBoost. We also describe an efficient implementation of the algorithm for certain natural cases. We discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different WWW search strategies, each of which is a query expansion for a given domain. For this task, we compare the performance of RankBoost to the individual search strategies. The second experiment is a collaborative-filtering task for making movie recommendations. Here, we present results comparing RankBoost to nearest-neighbor and regression algorithms.
343|The Nature of Statistical Learning Theory| Statistical learning theory was introduced in the late 1960’s. Until the 1990’s it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990’s new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more
344|Support-Vector Networks|The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the supportvector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.
345|Bagging Predictors|Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy. 1. Introduction A learning set of L consists of data f(y n ; x n ), n = 1; : : : ; Ng where the y&#039;s are either class labels or a numerical response. We have a procedure for using this learning set to form a predictor &#039;(x; L) --- if the input is x we ...
347|Experiments with a New Boosting Algorithm|In an earlier paper, we introduced a new “boosting” algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that consistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a “pseudo-loss ” which is a method for forcing a learning algorithm of multi-label conceptsto concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman’s “bagging ” method when used to aggregate various classifiers (including decision trees and single attribute-value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem. 
348|A Theory of the Learnable|  Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems.
349|A training algorithm for optimal margin classifiers|A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classifiaction functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.  
350|Additive Logistic Regression: a Statistical View of Boosting|Boosting (Freund &amp; Schapire 1996, Schapire &amp; Singer 1998) is one of the most important recent developments in classification methodology. The performance of many classification algorithms can often be dramatically improved by sequentially applying them to reweighted versions of the input data, and taking a weighted majority vote of the sequence of classifiers thereby produced. We show that this seemingly mysterious phenomenon can be understood in terms of well known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multi-class generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multi-class generalizations of boosting in most...
351|GroupLens: An Open Architecture for Collaborative Filtering of Netnews|Collaborative filters help people make choices based on the opinions of other people. GroupLens is a system for collaborative filtering of netnews, to help people find articles they will like in the huge stream of available articles. News reader clients display predicted scores and make it easy for users to rate articles after they read them. Rating servers, called Better Bit Bureaus, gather and disseminate the ratings. The rating servers predict scores based on the heuristic that people who agreed in the past will probably agree again. Users can protect their privacy by entering ratings under pseudonyms, without reducing the effectiveness of the score prediction. The entire architecture is open: alternative software for news clients and Better Bit Bureaus can be developed independently and can interoperate with the components we have developed.
352|Empirical Analysis of Predictive Algorithm for Collaborative Filtering|1
353|Social Information Filtering: Algorithms for Automating &#034;Word of Mouth&#034;|This paper describes a technique for making personalized recommendations from any type of database to a user based on similarities between the interest profile of that user and those of other users. In particular, we discuss the implementation of a networked system called Ringo, which makes personalized recommendations for music albums and artists. Ringo&#039;s database of users and artists grows dynamically as more people use the system and enter more information. Four different algorithms for making recommendations by using social information filtering were tested and compared. We present quantitative and qualitative results obtained from the use of Ringo by more than 2000 people.
354|Boosting the margin: A new explanation for the effectiveness of voting methods|One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik’s support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.  
355|The Weighted Majority Algorithm|We study the construction of prediction algorithms in a situation in which a learner faces a sequence of trials, with a prediction to be made in each, and the goal of the learner is to make few mistakes. We are interested in the case that the learner has reason to believe that one of some pool of known algorithms will perform well, but the learner does not know which one. A simple and effective method, based on weighted voting, is introduced for constructing a compound algorithm in such a circumstance. We call this method the Weighted Majority Algorithm. We show that this algorithm is robust in the presence of errors in the data. We discuss various versions of the Weighted Majority Algorithm and prove mistake bounds for them that are closely related to the mistake bounds of the best algorithms of the pool. For example, given a sequence of trials, if there is an algorithm in the pool A that makes at most m mistakes then the Weighted Majority Algorithm will make at most c(log jAj + m) mi...
356|The Strength of Weak Learnability|Abstract. This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a Source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent. A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error e.
357|An experimental comparison of three methods for constructing ensembles of decision trees|Abstract. Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a “base ” learning algorithm. Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the effectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classification noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classification noise, bagging is much better than boosting, and sometimes better than randomization.
358|Boosting a Weak Learning Algorithm By Majority|We present an algorithm for improving the accuracy of algorithms for learning binary concepts. The improvement is achieved by combining a large number of hypotheses, each of which is generated by training the given learning algorithm on a different set of examples. Our algorithm is based on ideas presented by Schapire in his paper &#034;The strength of weak learnability&#034;, and represents an improvement over his results. The analysis of our algorithm provides general upper bounds on the resources required for learning in Valiant&#039;s polynomial PAC learning framework, which are the best general upper bounds known today. We show that the number of hypotheses that are combined by our algorithm is the smallest number possible. Other outcomes of our analysis are results regarding the representational power of threshold circuits, the relation between learnability and compression, and a method for parallelizing PAC learning algorithms. We provide extensions of our algorithms to cases in which the conc...
359|Learning to Order Things|There are many applications in which it is desirable to order rather than classify  instances. Here we consider the problem of learning how to order, given feedback  in the form of preference judgments, i.e., statements to the effect that one instance  should be ranked ahead of another. We outline a two-stage approach in which one  first learns by conventional means a preference function, of the form PREF(u; v),  which indicates whether it is advisable to rank u before v. New instances are  then ordered so as to maximize agreements with the learned preference function.  We show that the problem of finding the ordering that agrees best with  a preference function is NP-complete, even under very restrictive assumptions.  Nevertheless, we describe a simple greedy algorithm that is guaranteed to find a  good approximation. We then discuss an on-line learning algorithm, based on the  &#034;Hedge&#034; algorithm, for finding a good linear combination of ranking &#034;experts.&#034;  We use the ordering algorith...
360|Cryptographic Limitations on Learning Boolean Formulae and Finite Automata|In this paper we prove the intractability of learning several classes of Boolean functions in the distribution-free model (also called the Probably Approximately Correct or PAC model) of learning from examples. These results are representation independent, in that they hold regardless of the syntactic form in which the learner chooses to represent its hypotheses. Our methods reduce the problems of cracking a number of well-known public-key cryptosystems to the learning problems. We prove that a polynomial-time learning algorithm for Boolean formulae, deterministic finite automata or constant-depth threshold circuits would have dramatic consequences for cryptography and number theory: in particular, such an algorithm could be used to break the RSA cryptosystem, factor Blum integers (composite numbers equivalent to 3 modulo 4), and detect quadratic residues. The results hold even if the learning algorithm is only required to obtain a slight advantage in prediction over random guessing. The techniques used demonstrate an interesting duality between learning and cryptography. We also apply our results to obtain strong intractability results for approximating a generalization of graph coloring.
361|Discriminative Reranking for Natural Language Parsing|This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model’s score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.  
362|Bagging, Boosting, and C4.5|Breiman&#039;s bagging and Freund and Schapire&#039;s  boosting are recent methods for improving the predictive power of classifier learning systems. Both form a set of classifiers that are combined by voting, bagging by generating replicated bootstrap samples of the data, and boosting by adjusting the weights of training instances. This paper reports results of applying both techniques to a system that learns decision trees and testing on a representative collection of datasets. While both approaches substantially improve predictive accuracy, boosting shows the greater benefit. On the other hand, boosting also produces severe degradation on some datasets. A small change to the way that boosting combines the votes of learned classifiers reduces this downside and also leads to slightly better results on most of the datasets considered. Introduction  Designers of empirical machine learning systems are concerned with such issues as the computational cost of the learning method and the accuracy and ...
363|Pranking with Ranking|We discuss the problem of ranking instances. In our framework each instance is associated with a rank or a rating, which is an integer from 1 to k. Our goal is to find a rank-prediction rule that assigns each instance a rank which is as close as possible to the instance&#039;s true rank. We describe a simple and efficient online algorithm, analyze its performance in the mistake bound model, and prove its correctness. We describe two sets of experiments, with synthetic data and with the EachMovie dataset for collaborative filtering. In the experiments we performed, our algorithm outperforms online algorithms for regression and classification applied to ranking.
364|The Sample Complexity of Pattern Classification With Neural Networks: The Size of the Weights is More Important Than the Size of the Network|Sample complexity results from computational learning theory, when applied to neural network learning for pattern classification problems, suggest that for good generalization performance the number of training examples should grow at least linearly with the number of adjustable parameters in the network. Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. For example, consider a two-layer feedforward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A and the input dimension is n. We show that the misclassification probability is no more than a certain error estimate (that is related to squared error on the training set) plus A³ p  (log n)=m (ignori...
365|Bias plus variance decomposition for zero-one loss functions|We present a bias-variance decomposition of expected misclassi cation rate, the most commonly used loss function in supervised classi cation learning. The bias-variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms, yet no decomposition was o ered for the more commonly used zero-one (misclassi cation) loss functions until the recent work of Kong &amp; Dietterich (1995) and Breiman (1996). Their decomposition su ers from some major shortcomings though (e.g., potentially negative variance), which our decomposition avoids. We show that, in practice, the naive frequency-based estimation of the decomposition terms is by itself biased and show how to correct for this bias. We illustrate the decomposition on various algorithms and datasets from the UCI repository. 1
366|Error-Correcting Output Coding Corrects Bias and Variance|Previous research has shown that a technique called error-correcting output coding (ECOC) can dramatically improve the classification accuracy of supervised learning algorithms that learn to classify data points into one of k AE 2 classes. This paper presents an investigation of why the ECOC technique works, particularly when employed with decision-tree learning algorithms. It shows that the ECOC method--- like any form of voting or committee---can reduce the variance of the learning algorithm. Furthermore---unlike methods that simply combine multiple runs of the same learning algorithm---ECOC can correct for errors caused by the bias of the learning algorithm. Experiments show that this bias correction ability relies on the non-local behavior of C4.5. 1 Introduction  Error-correcting output coding (ECOC) is a method for applying binary (two-class) learning algorithms to solve k-class supervised learning problems. It works by converting the k-class supervised learning problem into a la...
367|Adaptive game playing using multiplicative weights|We present a simple algorithm for playing a repeated game. We show that a player using this algorithm suffers average loss that is guaranteed to come close to the minimum loss achievable by any fixed strategy. Our bounds are nonasymptotic and hold for any opponent. The algorithm, which uses the multiplicative-weight methods of Littlestone and Warmuth, is analyzed using the Kullback–Liebler divergence. This analysis yields a new, simple proof of the min–max theorem, as well as a provable method of approximately solving a game. A variant of our game-playing algorithm is proved to be optimal in a very strong sense.  
368|Automatic Combination of Multiple Ranked Retrieval Systems|Retrieval performance can often be improved significantly by using a number of different retrieval algorithms and combining the results, in contrast to using just a single retrieval algorithm. This is because different retrieval algorithms, or retrieval experts, often emphasize different document and query features when determining relevance and therefore retrieve different sets of documents. However, it is unclear how the different experts are to be combined, in general, to yield a superior overall estimate. We propose a method by which the relevance estimates made by different experts can be automatically combined to result in superior retrieval performance. We apply the method to two expert combination tasks. The applications demonstrate that the method can identify high performance combinations of experts and also is a novel means for determining the combined effectiveness of experts.  1 Introduction  In text retrieval, two heads are definitely better than one. Retrieval performanc...
369|Game Theory, On-line Prediction and Boosting|We study the close connections between game theory, on-line prediction and boosting. After a brief review of game theory, we describe an algorithm for learning to play repeated games based on the on-line prediction methods of Littlestone and Warmuth. The analysis of this algorithm yields a simple proof of von Neumann’s famous minmax theorem, as well as a provable method of approximately solving a game. We then show that the on-line prediction model is obtained by applying this game-playing algorithm to an appropriate choice of game and that boosting is obtained by applying the same algorithm to the “dual” of this game. 
370|A Game of Prediction with Expert Advice|We consider the following problem. At each point of discrete time the learner must make a prediction; he is given the predictions made by a pool of experts. Each prediction and the outcome, which is disclosed after the learner has made his prediction, determine the incurred loss. It is known that, under weak regularity, the learner can ensure that his cumulative loss never exceeds cL+ a ln n, where c and a are some constants, n is the size of the pool, and L is the cumulative loss incurred by the best expert in the pool. We find the set of those pairs (c; a) for which this is true.
371|Pruning Adaptive Boosting|The boosting algorithm AdaBoost, developed by Freund and Schapire, has exhibited outstanding performance on several benchmark problems when using C4.5 as the &#034;weak&#034; algorithm to be &#034;boosted.&#034; Like other ensemble learning approaches, AdaBoost constructs a composite hypothesis by voting many individual hypotheses. In practice, the large amount of memory required to store these hypotheses can make ensemble methods hard to deploy in applications. This paper shows that by selecting a subset of the hypotheses, it is possible to obtain nearly the same levels of performance as the entire set. The results also provide some insight into the behavior of AdaBoost.
372|Using Output Codes to Boost Multiclass Learning Problems|This paper describes a new technique for solving multiclass learning problems by combining Freund and Schapire&#039;s boosting algorithm with the main ideas of Dietterich and Bakiri&#039;s method of error-correcting output codes (ECOC). Boosting is a general method of improving the accuracy of a given base or &#034;weak&#034; learning algorithm. ECOC is a robust method of solving multiclass learning problems by reducing to a sequence of two-class problems. We show that our new hybrid method has advantages of both: Like ECOC, our method only requires that the base learning algorithm work on binary-labeled data. Like boosting, we prove that the method comes with strong theoretical guarantees on the training and generalization error of the final combined hypothesis assuming only that the base learning algorithm perform slightly better than random guessing. Although previous methods were known for boosting multiclass problems, the new method may be significantly faster and require less programming effort in creating the base
learning algorithm. We also compare the new algorithm
experimentally to other voting methods.
373|An Adaptive Version of the Boost By Majority Algorithm|We propose a new boosting algorithm. This boosting algorithm is an adaptive version of the boost by  majority algorithm and combines bounded goals of the boost by majority algorithm with the adaptivity  of AdaBoost.
374|An empirical evaluation of bagging and boosting|An ensemble consists of a set of independently trained classi ers (such as neural networks or decision trees) whose predictions are combined when classifying novel instances. Previous research has shown that an ensemble as a whole is often more accurate than any of the single classiers in the ensemble. Bagging (Breiman 1996a) and Boosting (Freund &amp; Schapire 1996) are two relatively new but popular methods for producing ensembles. In this paper we evaluate these methods using both neural networks and decision trees as our classi cation algorithms. Our results clearly showtwo important facts. The rst is that even though Bagging almost always produces a better classi er than any of its individual component classi ers and is relatively impervious to over tting, it does not generalize any better than a baseline neural-network ensemble method. The second is that Boosting is apowerful technique that can usually produce better ensembles than Bagging ? however, it is more susceptible to noise and can quickly over t a data set.
375|A New Family of Online Algorithms for Category Ranking|We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stems from recent advances in online learning algorithms. The algorithms we present are simple to implement and are time and memory ecient. We evaluate the algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora the algorithms we present outperform adaptations to topic-ranking of Rocchio&#039;s algorithm and the Perceptron algorithm. We also outline the formal analysis of the algorithm in the mistake bound model. To our knowledge, this work is the  rst to report performance results with the entire new Reuters corpus.
376|Arcing the edge|Recent work has shown that adaptively reweighting the training set, growing a classifier using the new weights, and combining the classifiers constructed to date can significantly decrease generalization error. Procedures of this type were called arcing by Breiman[1996]. The first successful arcing procedure was introduced by Freund and Schapire[1995,1996] and called Adaboost. In an effort to explain why Adaboost works, Schapire et.al. [1997] derived a bound on the generalization error of a convex combination of classifiers in terms of the margin. We introduce a function called the edge, which differs from the margin only if there are more than two classes. A framework for understanding arcing algorithms is defined. In this framework, we see that the arcing algorithms currently in the literature are optimization algorithms which minimize some function of the edge. A relation is derived between the optimal reduction in the maximum value of the edge and the PAC concept of weak learner. Two algorithms are described which achieve the optimal reduction. Tests on both synthetic and real data cast doubt on the Schapire et.al. explanation.
377|T.: Using the future to sort out the present: Rankprop and multitask learning for medical risk evaluation|A patient visits the doctor; the doctor reviews the patient&#039;s history, asks questions, makes basic measurements (blood pressure,...), and prescribes tests or treatment. The prescribed course of action is based on an assessment of patient risk|patients at higher risk are given more and faster attention. It is also sequential|it is too expensive to immediately order all tests which might later be of value. This paper presents two methods that together improve the accuracy of backprop nets on a pneumonia risk assessment problem by 10-50%. Rankprop improves on backpropagation with sum of squares error in ranking patients by risk. Multitask learning takes advantage of future lab tests available in the training set, but not available in practice when predictions must be made. Both methods are broadly applicable. 1
378|Cranking: Combining Rankings Using Conditional Probability Models on Permutations|A new approach to ensemble learning is introduced  that takes ranking rather than classification  as fundamental, leading to models on the symmetric  group and its cosets. The approach uses a  generalization of the Mallows model on permutations  to combine multiple input rankings. Applications  include the task of combining the output  of multiple search engines and multiclass or multilabel  classification, where a set of input classifiers  is viewed as generating a ranking of class labels.
379|Direct Optimization of Margins Improves Generalization in Combined Classifiers|Sonar Cumulative training margin distributions  for AdaBoost versus  our &#034;Direct Optimization Of  Margins&#034; (DOOM) algorithm.
380|Data Filtering and Distribution Modeling Algorithms for Machine Learning|vi Acknowledgments vii 1. Introduction 1  1.1 Boosting by majority : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.2 Query By Committee : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 7 1.3 Learning distributions of binary vectors : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 8  2. Boosting a weak learning algorithm by majority 10  2.1 Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 10 2.2 The majority-vote game : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 14 2.2.1 Optimality of the weighting scheme : : : : : : : : : : : : : : : : : : : : : : : : : : : 19 2.2.2 The representational power of majority gates : : : : : : : : : : : : : : : : : : : : : : 20 2.3 Boosting a weak learner using a majority vote : : : : : : : : : : : : : : : : : : : : : : : : : : 22 2.3.1 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : :...
381|Using the future to \sort out&amp;quot; the present: Rankprop and multitask learning for medical risk evaluation|A patient visits the doctor; the doctor reviews the patient&#039;s history, asks questions, makes basic measurements (blood pressure,...), and prescribes tests or treatment. The prescribed course of action is based on an assessment of patient risk|patients at higher risk are given more and faster attention. It is also sequential|it is too expensive to immediately order all tests which might later be of value. This paper presents two methods that together improve the accuracy of backprop nets on a pneumonia risk assessment problem by 10-50%. Rankprop improves on backpropagation with sum of squares error in ranking patients by risk. Multitask learning takes advantage of future lab tests available in the training set, but not available in practice when predictions must be made. Both methods are broadly applicable. 1
382|A Roadmap of Agent Research and Development|  This paper provides an overview of research and development activities in the field of autonomous agents and multi-agent systems. It aims to identify key concepts and applications, and to indicate how they relate to one-another. Some historical context to the field of agent-based computing is given, and contemporary research directions are presented. Finally, a range of open issues and future challenges are highlighted.
383|A translation approach to portable ontology specifications|To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse — definitions of classes, relations, functions, and other objects — is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms. 
384|The tragedy of the commons|At the end of a thoughtful article on the future of nuclear war, Wiesner and York (1) concluded that: “Both sides in the arms race are... confronted by the dilemma of steadily increasing military power and steadily de-creasing national security. It is our considered professional judgment that this dilemma has no technical solution. If the great powers continue to look for solutions in the area of science and technology only, the result will be to worsen the situation.” I would like to focus your attention not on the subject of the article (national secu-rity in a nuclear world) but on the kind of conclusion they reached, namely that there
385|Intelligence Without Representation|Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporateeverything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environ...
387|Intelligence without reason|Computers and Thought are the two categories that together define Artificial Intelligence as a discipline. It is generally accepted that work in Artificial Intelligence over the last thirty years has had a strong influence on aspects of computer architectures. In this paper we also make the converse claim; that the state of computer architecture has been a strong influence on our models of thought. The Von Neumann model of computation has lead Artificial Intelligence in particular directions. Intelligence in biological systems is completely different. Recent work in behavior-based Artificial Intelligence has produced new models of intelligence that are much closer in spirit to biological systems. The non-Von Neumann computational models they use share many characteristics with biological computation.
390|The Role of Emotion in Believable Agents|Articial intelligence researchers attempting to create engaging  apparently living creatures may nd important insight in the work of artists who have explored the idea of believable character  In particular  appropriately timed and clearly expressed emotion is a central requirement for believable characters  We discuss these ideas and suggest how they may apply to believable interactive characters  which we call believable agents This work was supported in part by Fujitsu Laboratories and Mitsubishi Electric Research Laborato ries  The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ocial policies  either expressed or implied  of any other parties Keywords  articial intelligence  emotion  believable agents art  animation  believable characters  BELIEVABILITY   Believability There is a notion in the Arts of believable character  It does not mean an honest or reliable character  but one that provides the illusion of life  and thus permits the audience s suspension of disbelief The idea of believability has long been studied and explored in literature  theater lm  radio drama  and other media  Traditional character animators are among those artists who have sought to create believable characters  and the Disney animators of the   	 s made great strides toward this goal  The rst page of the enormous classic reference work on Disney animation Thomas and Johnston     begins with these words Disney animation makes audiences really believe in   characters  whose adventures and misfortunes make people laugh  and even cry  There is a special ingredient in our type of animation that produces drawings that appear to think and make decisions and act of their own volition  it is what creates the illusion of life Many articial intelligence researchers have long wished to build robots  and their cousins called agents  that seem to think  feel  and live  These are creatures with whom you	d want to share some of your life  as with a companion  or a social pet For instance  in his 
391|Collaborative plans for complex group action|The original formulation of SharedPlans by B. Grosz and C. Sidner ( 1990) was developed to provide a model of collaborative planning in which it was not necessary for one agent to have intentions-to toward an act of a different agent. Unlike other contemporaneous approaches (J.R. Searle, 1990), this formulation provided for two agents to coordinate their activities without introducing any notion of irreducible joint intentions. However, it only treated activities that directly decomposed into single-agent actions, did not address the need for agents to commit to their joint activity, and did not adequately deal with agents having only partial knowledge of the way in which to perform an action. This paper provides a revised and expanded version of SharedPlans that addresses these shortcomings. It also reformulates Pollack’s ( 1990) definition of individual plans to handle cases in which a single agent has only partial knowledge; this reformulation meshes with the definition of SharedPlans. The new definitions also allow for contracting out certain actions. The formalization that results has the features required by Bratrnan’s ( 1992) account of shared cooperative activity and is more general than alternative accounts (H. Levesque et al., 1990; E. Sonenberg et al., 1992).  
392|Plans And Resource-Bounded Practical Reasoning|An architecture for a rational agent must allow for means-end reasoning, for the weighing of competing alternatives, and for interactions between these two forms of reasoning. Such an architecture must also address the problem of resource boundedness. We sketch a solution of the first problem that points the way to a solution of the second. In particular, we present a high-level specification of the practical-reasoning component of an architecture for a resource-bounded rational agent. In this architecture, a major role of the agent&#039;s plans is to constrain the amount of further practical reasoning she must perform.
393|Kasbah: An Agent Marketplace for Buying and Selling Goods|While there are many Web services which help users find things to buy,we know of none which actually try to automate the process of buying and selling. Kasbah is a virtual marketplace on the Web where users create autonomous agents to buy and sell goods on their behalf. Users specify parameters to guide and constrain an agent&#039;s overall behavior. A simple prototype has been built to test the viability of this concept.
394|Experiences with an Architecture for Intelligent, Reactive Agents  |This paper describes an implementation of the 3T robot architecture  which has been under development for the last eightyears. The architecture  uses three levels of abstraction and description languages whichare  compatible between levels. The makeup of the architecture helps to coordinate  planful activities with real-time behaviors for dealing with dynamic  environments. In recent years, other architectures have been created with  similar attributes but two features distinguish the 3T architecture: 1) a  variety of useful software tools have been created to help implement this  architecture on multiple real robots;, and 2) this architecture, or parts of it, have been implemented on a varietyofvery different robot systems  using different processors, operating systems, effectors and sensor suites.
395|A Scalable Comparison-Shopping Agent for the World-Wide Web|The Web is less agent-friendly than we might hope. Most information on the Web is presented in loosely structured natural language text with no agent-readable semantics. HTML annotations structure the display of Web pages, but provide virtually no insight into their content. Thus, the designers of intelligent Web agents need to address the following questions: (1) To what extent can an agent understand information published at Web sites? (2) Is the agent&#039;s understanding sufficient to provide genuinely useful assistance to users? (3) Is site-specific hand-coding necessary, or can the agent automatically extract information from unfamiliar Web sites? (4) What aspects of the Web facilitate this competence? In this paper we investigate these issues with a case study using the ShopBot. ShopBot is a fullyimplemented, domain-independent comparison-shopping agent. Given the home pages of several on-line stores, ShopBot autonomously learns how to shop at those vendors. After its learning is com...
396|Controlling Cooperative Problem Solving in Industrial Multi-Agent Systems using Joint Intentions|One reason why Distributed AI (DAI) technology has been deployed in relatively few real-size applications is that it lacks a clear and implementable model of cooperative problem solving which specifies how agents should operate and interact in complex, dynamic and unpredictable environments. As a consequence of the experience gained whilst building a number of DAI systems for industrial applications, a new principled model of cooperation has been developed. This model, called Joint Responsibility, has the notion of joint intentions at its core. It specifies pre-conditions which must be attained before collaboration can commence and prescribes how individuals should behave both when joint activity is progressing satisfactorily and also when it runs into difficulty. The theoretical model has been used to guide the implementation of a general-purpose cooperation framework and the qualitative and quantitative benefits of this implementation have been assessed through a series of comparativ...
397|Commitments and conventions: The foundation of coordination in multi-agent systems|Distributed Artificial Intelligence systems, in which multiple agents interact to improve their individual performance and to enhance the system’s overall utility, are becoming an increasingly pervasive means of conceptualising a diverse range of applications. As the discipline matures, researchers are beginning to strive for the underlying theories and principles which guide the central processes of coordination and cooperation. Here agent communities are modelled using a distributed goal search formalism and it is argued that commitments (pledges to undertake a specified course of action) and conventions (means of monitoring commitments in changing circumstances) are the foundation of coordination in multi-agent systems. An analysis of existing coordination models which use concepts akin to commitments and conventions is undertaken before a new unifying framework is presented. Finally a number of prominent coordination techniques which do not explicitly involve commitments or conventions are reformulated in these terms to demonstrate their compliance with the central hypothesis of this paper. 1
398|Middle-Agents for the Internet|Like middle-men in physical commerce, middleagents  support the flow of information in electronic  commerce, assisting in locating and connecting the  ultimate information provider with the ultimate information  requester. Many different types of middleagents  will be useful in realistic, large, distributed,  open multi-agent problem solving systems. These  include matchmakers or yellow page agents that process  advertisements, blackboard agents that collect requests,  and brokers that process both. The behaviors  of each type of middle-agent have certain performance  characteristics---privacy, robustness, and  adaptiveness qualities---that are related to characteristics  of the external environment and of the agents  themselves. For example, while brokered systems are  more vulnerable to certain failures, they are also able  to cope more quickly with a rapidly fluctuating agent  workforce and meet certain privacy considerations.  This paper identifies a spectrum of middle-agents,  cha...
399|WebMate: A Personal Agent for Browsing and Searching|The World-Wide Web is developing very fast. Currently, finding useful information on the Web is a time consuming process. In this paper, we present WebMate, an agent that helps users to effectively browse and search the Web. WebMate extends the state of the art in Web-based information retrieval in many ways. First, it uses multiple TF-IDF vectors to keep track of user interests in different domains. These domains are automatically learned by WebMate. Second, WebMate uses the Trigger Pair Model to automatically extract keywords for refining document search. Third, during search, the user can provide multiple pages as similarity/relevance guidance for the search. The system extracts and combines relevant keywords from these relevant pages and uses them for keyword refinement. Using these techniques, WebMate provides effective browsing and searching help and also compiles and sends to users personal newspaper by automatically spiding news sources. We have experimentally evaluated the per...
400|Designing a Family of Coordination Algorithms|Many researchers have shown that there is no single best organization or coordination mechanism  for all environments. This paper discusses the design and implementation of an extendable  family of coordination mechanisms, called Generalized Partial Global Planning (GPGP). The set  of coordination mechanisms described here assists in scheduling activities for teams of cooperative  computational agents. The GPGP approach has several unique features. First, it is not tied to  a single domain. Each mechanism is defined as a response to certain features in the current task  environment. We show that different combinations of mechanisms are appropriate for different  task environments. Secondly, the approach works in conjunction with an agent&#039;s existing local  planner/scheduler. Finally, the initial set of five mechanisms presented here generalizes and extends  the Partial Global Planning (PGP) algorithm. In comparison to PGP, GPGP allows more  agent heterogeneity, it exchanges less global ...
401|editors. Blackboard Systems|Retroviral vectors containing putative internal ribosome entry sites: development of a polycistronic gene transfer
402|Trends in Cooperative Distributed Problem Solving|Introduction Cooperative Distributed Problem-Solving (CDPS) studies how a loosely-coupled network of problem solvers can work together to solve problems that are beyond their individual capabilities. Each problem-solving node in the network is capable of sophisticated problem solving and can work independently, but the problems faced by the nodes cannot be completed without cooperation. Cooperation is necessary because no single node has sufficient expertise, resources, and information to solve a problem, and different nodes might have expertise for solving different parts of the problem. For example, if the problem is to design a house, one node might have expertise on the strength of structural materials, another on the space requirements for different types of rooms, another on plumbing, another on electrical wiring, and so on. Different nodes might have different resources: some might be very fast at computation, others might have connections that speed communication, whil
403|Multiagent negotiation under time constraints|Research in distributed artificial intelligence (DAI) is concerned with how automated agents can be designed to interact effectively. Negotiation is proposed as a means for agents to communicate and compromise to reach mutually beneficial agreements. The paper examines the problems of resource allocation and task distribution among autonomous agents which can benefit from sharing a common resource or distributing a set of common tasks. We propose a strategic model of negotiation that takes the passage of time during the negotiation process itself into account. A distributed negotiation mechanism is introduced that is simple, efficient, stable, and flexible in various situations. The model considers situations characterized by complete as well as incomplete information, and ones in which some agents lose over time while others gain over time. Using this negotiation mechanism autonomous agents have simple and stable negotiation strategies that result in efficient agreements without delays even when there are dynamic changes in the environment.  
404|The behavior of computational ecologies|We describe a form of distributed computation in which agents have incomplete knowledge and imperfect information on the state of the system, and an instantiation of such systems based on market mechanisms. When agents can choose among several resources, the dynamics of the system can be oscillatory and even chaotic. A mechanism is described for achieving global stability through local controls. 1
405|A Retrospective View of FA/C Distributed Problem Solving|The Functionally-Accurate, Cooperative (FA/C) paradigm provides a model for task decomposition and agent interaction in a distributed problem-solving system. In this model, agents need not have all the necessary information locally to solve their subproblems, and agents interact through the asynchronous, co-routine exchange of partial results. This model leads to the possibility that agents may behave in an uncoordinated manner. This paper traces the development of a series of increasingly sophisticated cooperative control mechanisms for coordinating agents. They include integrating data- and goal-directed control, using static meta-level information specified by an organizational structure, and using dynamic meta-level information developed in partial global planning. The framework of distributed search motivates these developments. Major themes of this work are the importance of sophisticated local control, the interplay between local control and cooperative control, and the use of s...
406|The use of meta-level control for coordination in a distributed problem solving network|This paper was presented at IJCAI-83. Distributed problem-solving networks provide an interesting application area for meta-level control through the use of organizational structuring. We describe a decentralized approach to network coordination that relies on each node making sophisticated local decisions that balance its own perceptions of appropriate problem-solving activity with activities deemed important by other nodes. Each node is guided by a high-level strategic plan for cooperation among the nodes in the network. The high-level strategic plan, which is a form of meta-level control, is represented as a network organizational structure that specifies in a general way the information and control relationships among the nodes. An implementation of these ideas is briefly described along with the results of preliminary experiments with various network problem-solving strategies specified via organizational structuring. In addition to its application to Distributed Artificial Intelligence, this research has implications for organizing and controlling complex knowledge-based systems that involve semi-autonomous problem solving agents. 1
407|Agent-Based Business Process Management|This paper describes work undertaken in the ADEPT (Advanced Decision Environment for Process Tasks) project towards developing an agent-based infrastructure for managing business processes. We describe how the key technology of negotiating, service providing, autonomous agents was realised and demonstrate how this was applied to the BT (British Telecom) business process of providing a customer quote for network services.
408|Moving Up the Information Food Chain: Deploying Softbots on the World Wide Web|I view the World Wide Web as an information food chain (figure 1). The maze of pages and hyperlinks that comprise the Web are at the very bottom of the chain. The WebCrawlers and Alta Vistas of the world are information herbivores; they graze on Web pages and regurgitate them as searchable indices. Today, most Web users feed near the bottom of the information food chain, but the time is ripe to move up. Since 1991, we have been building information carnivores, which intelligently hunt and feast on herbivores
409|Designing behaviors for information agents|To facilitate the rapid development and open system interoperability of autonomous agents we need to carefully specify and effectively implement various classes of agent behaviors. Our current focus is on the behaviors and underlying architecture of WWW-based autonomous software agents that collect and supply information to humans and other computational agents. This paper discusses a set of architectural building blocks that support the specification of behaviors for these information agents in a way that allows periodic actions, interleaving of planning and execution, and the concurrent activation of multiple behaviors with asynchronous components. We present an initial set of information agent behaviors, including responding to repetitive queries, monitoring information sources, advertising capabilities, and self cloning. We have implemented and tested these behaviors on the WWW in the context of WAR-REN, an open multi-agent organization for financial portfolio management.
411|TouringMachines: An Architecture for Dynamic, Rational, Mobile Agents|ion-Partitioned Evaluator (APE) architecture which has been tested in a simulated, single-agent, indoor navigation domain [SH90].  The APE architecture is composed of a number of concurrent, hierarchically abstract action control layers, each representing and reasoning about some particular aspect of the agent&#039;s task domain. Implemented as a parallel blackboard-based planner, the five layers --- sensor/motor, spatial, temporal, causal, and conventional (general knowledge) --- effectively partition the agent&#039;s data processing duties along a number of dimensions including temporal granularity, information/resource use, and functional abstraction. Perceptual information flows strictly from the agent sensors (connected to the sensor /motor level) toward the higher levels, while command or goal-achievement information flows strictly downward towards the agent&#039;s effectors (also connected to the sensor/motor level).  Besides mechanisms for communicating with other layers, each layer in the AP...
412|Cooperative Transportation Scheduling: an Application Domain for DAI|A multiagent approach to designing the transportation domain is presented. The Mars system is described which models cooperative order scheduling within a society of shipping companies. We argue why Distributed Artificial Intelligence (DAI) offers suitable tools to deal with the hard problems in this domain. We present three important instances for DAI techniques that proved useful in the transportation application: cooperation among the agents, task decomposition and task allocation, and decentralised planning. An extension of the contract net protocol for task decomposition and task allocation is presented; we show that it can be used to obtain good initial solutions for complex resource allocation problems. By introducing global information based upon auction protocols, this initial solution can be improved significantly. We demonstrate that the auction mechanism used for schedule optimisation can also be used for implementing dynamic replanning. Experimental results are provided ev...
413|Foundations of a Logical Approach to Agent Programming|This paper describes a novel approach to high-level agent programming based on a highly  developed logical theory of action. The user provides a specification of the agents&#039; basic actions  (preconditions and effects) as well as of relevant aspects of the environment, in an extended  version of the situation calculus. He can then specify behaviors for the agents in terms of these  actions in a programming language where one can refer to conditions in effect in the environment.  When an implementation of the basic actions is provided, the programs can be executed in a  real environment; otherwise, a simulated execution is still possible. The interpreter automatically  maintains the world model required to execute programs based on the specification. The theoretical  framework includes a solution to the frame problem, allows agents to have incomplete knowledge  of their environment, and handles perceptual actions. The theory can also be used to prove  programs correct. A simple meeting sc...
414|Increasing believability in animated pedagogical agents|Animated pedagogical agents o er great promise for knowledge-based learning environments. In addition to coupling feedback capabilities with a strong visual presence, these agents play a critical role in motivating students. The extent to which they exhibit life-like behaviors strongly increases their motivational impact, but these behaviors must always complement and never interfere with students &#039; problem solving. To address this problem, we havedeveloped a framework for dynamically sequencing animated pedagogical agents &#039; believability-enhancing behaviors. By monitoring a student&#039;s problemsolving history and the agent&#039;s past activities, a competition-based behavior sequencing engine produces realtime life-like character animations that are pedagogically appropriate. Behaviors in the agent&#039;s repertoire compete with one another. At each moment, the strongest eligible behavior is heuristically selected as the winner and is exhibited. We haveimplemented this framework in Herman the Bug, an animated pedagogical agent that inhabits a knowledge-based learning environment for the domain of botanical anatomy and physiology.
415|Towards a Social Level Characterisation of Socially Responsible Agents|This paper presents a high-level framework for analysing and designing intelligent agents. The framework&#039;s key abstraction mechanism is a new computer level called the  Social Level. The Social Level sits immediately above the Knowledge Level, as defined by Allen Newell, and is concerned with the inherently social aspects of multiple agent systems. To illustrate the working of this framework, an important new class of agent is identified and then specified. Socially responsible agents retain their local autonomy but still draw from, and provide resources to, the larger community. Through empirical evaluation, it is shown that such agents produce both good system-wide performance and good individual performance. 1. INTRODUCTION The number of multi-agent systems being designed and built is rapidly increasing as software agents gain acceptance as a powerful and useful technology for solving complex problems (Chaib-draa, 1995; Jennings, 1994; PAAM, 1996). As applications become more comple...
416|Representing and Executing Agent-Based Systems|In this paper we describe an approach to the representation and implementation  of agent-based systems where the behaviour of an individual agent is  represented by a set of logical rules in a particular form. This not only provides a  logical specification of the agent, but also allows us to directly execute the rules in  order to implement the agent&#039;s behaviour. Agents communicate with each other  through a simple, and logically well-founded, broadcast communication mechanism.
417|Entertaining Agents: a sociological case study|Traditional AI has not concerned itself extensively with sociology nor with what emotional reactions might be produced in its users. On the other hand, entertainment is very concerned indeed with these issues. AI and ALife programs which are to be used in entertainment must therefore be viewed both as AI/ALife endeavors and as psychological and sociological endeavors. This paper presents a brief description of Julia [Mauldin 94], an implemented software agent, and then examines the sociology of those who encounter her, using both transcripts of interactions with Julia, and direct interviews with users. Julia is designed to pass as human in restricted environments while being both entertaining and informative, and often elicits surprisingly intense emotional reactions in those who encounter her. An introduction to MUDs and Julia Julia [Mauldin 94] is a MUD [Curtis 92] [Bruckman 93] [Evard 93] robot. A MUD is a text-only, multiperson, virtual reality. [Mauldin 94], while describing Julia’s internal structure, gives very little ‘feel ’ for what it like to interact with her outside of the strictures of a formal Turing test; hence, transcripts of many interactions with her appear below as examples. (Since Julia adamantly insists that she is female, I refer to the program here as ‘she’.)
418|Using ARCHON to develop real-world DAI applications for electricity transportation management and particle accelerator control|ARCHON ^TM (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe&#039;s largest ever project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Two of these applications, electricity transportation management and particle accelerator control, have been run successfully on-line in the organisation for which they were developed (respectively, Iberdrola an electricity utility in the north of Spain and CERN the European Centre for high energy physics research near Geneva). This paper recounts the problems, insights and experiences gained whilst deploying ARCHON technology in these real-world industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applications and highlights the key design forces which shape work in this important domain. Secondly, the...
419|An Agent-based Approach to Health Care Management|The provision of medical care typically involves a number of individuals, located in a number of different institutions, whose decisions and actions need to be coordinated if the care is to be effective and efficient. To facilitate this decision making and to ensure the coordination process runs smoothly, the use of software support is becoming increasingly widespread. To this end, this paper describes an agent-based system which was developed to help manage the care process in real world settings. The agents themselves are implemented using a layered architecture, called AADCare, which combines a number of AI and agent techniques: a symbolic decision procedure for decision making with incomplete and conflicting information, a concept of accountability for task allocation, the notions of commitments and conventions for managing coherent cooperation, and a set of communication primitives for interagent interaction. The utility of this approach is demonstrated through the development of ...
420|The Negotiating Agents Approach to Runtime Feature Interaction Resolution|. This article describes how to use the Negotiating Agents approach on a telecommunications platform. Negotiation is used in this approach to resolve conflicts between features of one user and of different users. The theory behind the approach is discussed briefly. Methods for implementing the approach are given along with the methods for defining IN features in terms of the Negotiating Agents approach in order to resolve conflicts between these features. 1 Introduction  Rapid change in the telecommunications industry increases the complexity not only of building but also of using telecommunications services. Much of the complexity arises from the feature interaction problem. When features interact, a user must understand the behavior of features in combination -- even how features of other users may affect the negbehavior of her features. Similarly, a service provider must determine how combinations of features will behave, including combinations of its own features with other provide...
421|Specification and implementation of a belief desire joint-intention architecture for collaborative problem solving|Systems composed of multiple interacting problem solvers are becoming increasingly pervasive and have been championed in some quarters as the basis of the next generation of intelligent information systems. If this technology is to fulfill its true potential then it is important that the systems which are developed have a sound theoretical grounding. One aspect of this foundation, namely the model of collaborative problem solving, is examined in this paper. A synergistic review of existing models of cooperation is presented, their weaknesses are highlighted and a new model (called joint responsibility) is introduced. Joint responsibility is then used to specify a novel high-level agent architecture for cooperative problem solving in which the mentalistic notions of belief, desire, intention and joint intention play a central role in guiding an individual’s and the group’s problem solving behaviour. An implementation of this high-level architecture is then discussed and its utility is illustrated for the real-world domain of electricity transportation management.
422|Industrial Applications of Distributed AI|This article argues that a DAI approach can be used to cope with the complexity of industrial applications. DAI techniques are beginning to have a broad impact; the current introduction of these techniques by an ESPRIT project, a Palo Alto consortium, ARPA, Carnegie Mellon University, MCC, and others are good examples. In the near future, other industrial products will emerge from the application of DAI techniques to other domains, including distributed databases, computer-supported cooperative work, and air traffic control. An important advantage of a DAI approach is the ability to integrate existing standalone knowledge-based systems. This factor is important because software for industrial applications is often developed in an ad hoc fashion. Thus, organizations possess a large number of standalone systems developed at different times by different people using different techniques. These systems all operate in the same physical environment, all have expertise that is related but distinct, and all could benefit from cooperation with other such standalone systems
423|Using mobile agents to support interorganizational workflow-management|This paper argues that the mobile agent approach is well suited for sporadic communication in open distributed systems- especially for rather ‘loose’ cooperations across local and organizational borders: In an increasing number of cases, management of distributed business procedures reaches beyond such borders. This means for most existing workflow management systems that cooperating partners are required to give up their local autonomy. However, for cases in which business partners intend to cooperate but still need to preserve their local autonomy, process participation on the basis of mobile agents represents an attractive and appropriate mechanism. This article shows how such kind of process integration can be achieved. It further demonstrates how the COSM (Common Open Service Market) system software can be extended in order to use petri net based process definitions which realize mobile agents in an integrated distributed system platform.
424|Creatures: Entertainment Software Agents with Artificial Life|We present a technical description of Creatures, a commercial home-entertainment software package. Creatures provides a simulatedenvironment in which exist a number of synthetic agents that a user can interact with in real-time. The agents (known as &#034;creatures&#034;) are intended as sophisticated &#034;virtual pets&#034;. The internal architecture of the creatures is strongly inspired by animal biology. Each creature has a neural network responsible for sensory-motorcoordinationand behavior selection, and an &#034;artificial biochemistry&#034; that models a simple energy metabolism along with a &#034;hormonal&#034; system that interacts with the neural network to model diffuse modulation of neuronal activity and staged ontogenetic development. A biologically inspired learning mechanism allows the neural network to adapt during the lifetime of a creature. Learning includes the ability to acquire a simple verb--object language.
425|Some Issues in the Design of Market-Oriented Agents|. In a computational market, distributed market agents interact with other agents primarily through the exchange of goods and services. Thanks to a welldeveloped underlying economic framework, we can draw on a rich source of analytic tools and theoretical techniques for designing individual agents and predicting aggregate behavior. For many narrowly scoped static problems, design of a computational market is relatively straightforward. We consider some issues that arise in attempting to design computational economies for broadly scoped, dynamic environments. These issues include how to specify the goods and services beingexchanged,how these market-oriented agents should set their exchangepolicies, and how computational market mechanisms appropriate for idealized environments can be adapted to work in a larger class of non-ideal environments. 1 Introduction  Approaches to resource allocation in distributed systems can be bounded by two extremes. At one end (the &#034;software engineering&#034; ap...
426|Application of multi-agent systems in traffic and transportation|Agent-oriented techniques offer a new approach aimed at supporting the whole software development process. All the phases in the software development process are treated with a single uniform concept, namely that of agents, and a system modelled by a collection of agents is called a multi-agent system. AOT as a new advance in information technology can help to respond to the growing interest in making traffic and transportation more efficient, resource-saving and ecological. This article gives an overview of a diverse range of applications where multi-agent systems promise to create a great impact in this domain. To demonstrate the ideas behind AOT and their applicability in this domain, two applications currently under development at Daimler-Benz Research will be described in some detail. 1
427|Managing heterogeneous transaction workflows with cooperating agents|This paper describes how a set of autonomous computational agents can cooperate in providing coherent management of transaction workflows in environments where there are many diverse information resources. The agents use models of themselves and of the resources that are local to them. Resource models may be the schemas of databases, frame systems of knowledge bases, domain models of business environments, or process models of business operations. Models enable the agents and information resources to use the appropriate semantics when they interoperate. This is accomplished by specifying the semantics in terms of a common ontology. We discuss the contents of the models, where they come from, and how the agents acquire them. We then describe a set of agents for telecommunication service provisioning and show how the agents use such models to cooperate. The agents implement virtual state machines, and interact by exchanging state information. Their interactions produce an implementation of relaxed transaction processing. 1
428|SIMPLIcity: Semantics-Sensitive Integrated Matching for Picture LIbraries|The need for efficient content-based image retrieval has increased tremendously in many application areas such as biomedicine, military, commerce, education, and Web image classification and searching. We present here SIMPLIcity (Semanticssensitive Integrated Matching for Picture LIbraries), an image retrieval system, which uses semantics classification methods, a wavelet-based approach for feature extraction, and integrated region matching based upon image segmentation. As in other regionbased retrieval systems, an image is represented by a set of regions, roughly corresponding to objects, which are characterized by color, texture, shape, and location. The system classifies images into semantic categories, such as textured-nontextured, graphphotograph. Potentially, the categorization enhances retrieval by permitting semantically-adaptive searching methods and narrowing down the searching range in a database. A measure for the overall similarity between images is developed using a region-matching scheme that integrates properties of all the regions in the images. Compared with retrieval based on individual regions, the overall similarity approach 1) reduces the adverse effect of inaccurate segmentation, 2) helps to clarify the semantics of a particular region, and 3) enables a simple querying interface for region-based image retrieval systems. The application of SIMPLIcity to several databases, including a database of about 200,000 general-purpose images, has demonstrated that our system performs significantly better and faster than existing ones. The system is fairly robust to image alterations.
429|Blobworld: A System for Region-Based Image Indexing and Retrieval|. Blobworld is a system for image retrieval based on finding coherent image regions which roughly correspond to objects. Each image is automatically segmented into regions (&#034;blobs&#034;) with associated color and texture descriptors. Querying is based on the attributes of one or two regions of interest, rather than a description of the entire image. In order to make large-scale retrieval feasible, we index the blob descriptions using a tree. Because indexing in the high-dimensional feature space is computationally prohibitive, we use a lower-rank approximation to the high-dimensional distance. Experiments show encouraging results for both querying and indexing. 1 Introduction  From a user&#039;s point of view, the performance of an information retrieval system can be measured by the quality and speed with which it answers the user&#039;s information need. Several factors contribute to overall performance:  -- the time required to run each individual query,  -- the quality (precision/recall) of each i...
430|A probabilistic approach to object recognition using local photometry and global geometry|Abstract. Many object classes, including human faces, can be modeled as a set of characteristic parts arranged in a variable spatial con guration. We introduce a simpli ed model of a deformable object class and derive the optimal detector for this model. However, the optimal detector is not realizable except under special circumstances (independent part positions). A cousin of the optimal detector is developed which uses \soft &#034; part detectors with a probabilistic description of the spatial arrangement of the parts. Spatial arrangements are modeled probabilistically using shape statistics to achieve invariance to translation, rotation, and scaling. Improved recognition performance over methods based on \hard &#034; part detectors is demonstrated for the problem of face detection in cluttered scenes. 1
431|WALRUS: A Similarity Retrieval Algorithm for Image Databases|Traditional approaches for content-based image querying typically compute a single signature for each image based on color histograms, texture, wavelet transforms etc., and return as the query result, images whose signatures are closest to the signature of the query image. Therefore, most traditional methods break down when images contain similar objects that are scaled differently or at different locations, or only certain regions of the image match.  In this paper, we propose WALRUS (WAveLet-based Retrieval of User-specified Scenes), a novel similarity retrieval algorithm that is robust to scaling and translation of objects within an image. WALRUS employs a novel similarity model in which each image is first decomposed into its regions, and the similarity measure between a pair of images is then defined to be the fraction of the area of the two images covered by matching regions from the images. In order to extract regions for an image, WALRUS considers sliding windows of varying siz...
432|Content-based image indexing and searching using Daubechies&#039; wavelets|This paper describes WBIIS (Wavelet-Based Image Indexing and Searching), a new image indexing and retrieval algorithm with partial sketch image searching capability for large image databases. The algorithm characterizes the color variations over the spatial extent of the image in a manner that provides semantically meaningful image comparisons. The indexing algorithm applies a Daubechies&#039; wavelet transform for each of the three opponent color components. The wavelet coefficients in the lowest few frequency bands, and their variances, are stored as feature vectors. To speed up retrieval, a two-step procedure is used that first does a crude selection based on the variances, and then renes the search by performing a feature vector match between the selected images and the query. For better accuracy in searching, two-level multiresolution matching may also be used. Masks are used for partial-sketch queries. This technique performs much better in capturing coherence of image, object granular...
433|The earth mover’s distance, multi-dimensional scaling, and color-based image retrieval|In this paper we present a novel approach tothe problem of navigating through a database of color images. We consider the images as points in a metric space in which we wish to move around so as to locate image neighborhoods of interest, based on color information. The data base images are mapped to distributions in color space, these distributions are appropriately compressed, and then the distances between all pairs I;J of images are computed based on the work needed to rearrange the mass in the compressed distribution representing I to that of J. We also propose the use of multi-dimensional scaling (MDS) techniques to embed a group of images as points in a two- or three-dimensional Euclidean space so that their distances are preserved as much as possible. Such geometric embeddings allow the user to perceive the dominant axes of variation in the displayed image group. In particular, displays of 2-d MDS embeddings can be used to organize and re ne the results of a nearest-neighbor query in a perceptually intuitive way. By iterating this process, the user is able to quickly navigate to the portion of the image space of interest. 1
434|Image Classification and Querying using Composite Region Templates|The tremendous growth in digital imagery is driving the need for more sophisticated  methods for automatic image analysis, cataloging, and searching.  We present a method for classifying and querying images based on the spatial  orderings of regions or objects using composite region templates (CRTs). The  CRTs capture the spatial information statistically and provide a robust way to  measure similarity in the presence of region insertions, deletions, substitutions, replications and relocations. The CRTs can be used for classifying and annotating  images by assigning symbols to the regions or objects and by extracting  symbol strings from spatial scans of the images. The symbol strings can be  decoded using a library of annotated CRTs to automatically label and classify  the images. The CRTs can also be used for searching bysketch or example by  measuring image similarity based on relative counts of the CRTs.  
435|Finding Similar Patterns in Large Image Databases|We address a new and rapidly growing application, automated searching through large sets of images to find a pattern &#034;similar to this one.&#034; Classical matched filtering fails at this problem since patterns, particularly textures, can differ in every pixel and still be perceptually similar. Most potential recognition methods have not been tested on large sets of imagery. This paper evaluates a key recognition method on a library of almost 1000 images, based on the entire Brodatz texture album. The features used for searching rely on a significant improvement to the traditional Karhunen-Lo&#039;eve (KL) transform which makes it shift-invariant. Results are shown for a variety of false alarm rates and for different subsets of KL features.  1 Introduction  As vastly increasing amounts of image and video are stored in computers it becomes harder for humans to locate a particular scene or video clip. It is currently impossible, in the general case, to semantically describe an image to the computer...
436|Unsupervised Multiresolution Segmentation for Images with Low Depth of Field|This paper describes a novel multiresolution image  segmentation algorithm for low DOF images. The algorithm is designed to  separate a sharply focused object-of-interest from other foreground or background objects. The algorithm is fully automatic in that all parameters are image  independent. A multiscale approach based on high frequency wavelet coefficients and their statistics is used to perform context-dependent classification of individual blocks of the image. Unlike other edge-based approaches, our algorithm does not rely on the process of connecting object boundaries. The algorithm has achieved high accuracy when tested on more than 100 low DOF images, many with  inhomogeneous foreground or background distractions. Compared with the state of the art algorithms, this new algorithm provides better accuracy at higher speed. Index TermsContent-based image retrieval, image region segmentation, low  depth-of-field, wavelet, multiresolution image analysis
437|Semantic Clustering and Querying on Heterogeneous Features for Visual data|  The effectiveness of the content-based image retrieval can be enhanced using the heterogeneous features embedded in the images. However, since the features in texture, color, and shape are generated using different computation methods and thus may require different similarity measurements, the integration of the...
438|System for Screening Objectionable Images|As computers and Internet become more and more available to families, access of objectionable graphics by children is increasingly a problem that many parents are concerned about. This paper describes WIPE TM (Wavelet Image Pornography Elimination), a system capable of classifying an image as objectionable or benign. The algorithm uses a combination of an icon filter, a graph-photo detector, a color histogram filter, a texture filter, and a wavelet-based shape matching algorithm to provide robust screening of on-line objectionable images. Semantically-meaningful feature vector matching is carried out so that comparisons between a given on-line image and images in a pre-marked training data set can be performed efficiently and effectively. The system is practical for real-world applications, processing queries at the speed of less than 2 seconds each, including the time to compute the feature vector for the query, on a Pentium Pro PC. Besides its exceptional speed, it has demonstrated 9...
439|Visual Similarity, Judgmental Certainty and Stereo Correspondence|Normal human vision is nearly infallible in modeling the visually sensed physical environment in which it evolved. In contrast, most currently available computer vision systems fall far short of human performance in this task, and further, they are generally not capable of being able to assert the correctness of their judgments. In computerized stereo matching systems, correctness of the similarity/identity-matching is almost never guaranteed. In this paper, we explore the question of the extent to which judgments of similarity/identity can be made essentially error-free in support of obtaining a relatively dense depth model of a natural outdoor scene. We argue for the necessity of simultaneously producing a crude scene-specific semantic &#034;overlay&#034;. For our experiments, we designed awavelet-based stereo matching algorithm and use &#034;classification-trees&#034; to create a primitive semantic overlay of the scene. A series of mutually independent filters has been designed and implemented based on the study of different error sources. Photometric appearance, camera imaging geometry and scene constraints are utilized in these filters. When tested on different sets of stereo images, our system has demonstrated above 97% correctness on asserted matches. Finally,we provide a principled basis for relatively dense depth recovery.
440|A learning algorithm for Boltzmann machines|The computotionol power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections con allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in o very short time. One kind of computation for which massively porollel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the con-straints in the domain being searched. We describe a generol parallel search method, based on statistical mechanics, and we show how it leads to a gen-eral learning rule for modifying the connection strengths so as to incorporate knowledge obout o task domain in on efficient way. We describe some simple examples in which the learning algorithm creates internal representations thot ore demonstrobly the most efficient way of using the preexisting connectivity structure. 1.
442|Understanding Line Drawings of Scenes with Shadows|this paper, how can we recognize the identity of Figs. 2.1 and 2.2? Do we use&#039; learning and knowledge to interpret what we see, or do we somehow automatically see the world as stable and independent bf lighting? What portions of scenes can we understand from local features alone, and what configurations require the use of 1obal hypotheses?  19 In this essay I describe a working collection of computer programs which reconstruct three-dimensional descriptions from line drawings which are obtained from scenes composed of plane-faced objects under various lighting conditions. The system identifies shadow lines and regions, groups regions which belong to the same object, and notices such relations as contact or lack of contact between the objects, support and in-front-of/behind relations between the objects as well as information about the spatial orientation of various regions, all using the description it has generated
443|Optimal perceptual inference|When a vision system creates an interpretation of some input datn, it assigns truth values or probabilities to intcrnal hypothcses about the world. We present a non-dctcrministic method for assigning truth values that avoids many of the problcms encountered by existing relaxation methods. Instead of rcprcscnting probabilitics with real-numbers, we usc a more dircct encoding in which thc probability associated with a hypotlmis is rcprcscntcd by the probability hat it is in one of two states, true or false. Wc give a particular non-deterministic operator, based on statistical mechanics, for updating the truth values of hypothcses. The operator ensures that the probability of discovering a particular combination of hypothcscs is a simplc function of how good that combination is. Wc show that thcrc is a simple relationship bctween this operator and Bayesian inference, and we describe a learning rule which allows a parallel system to converge on a set ofweights that optimizes its perccptt~al inferences. lnt roduction One way of interpreting images is to formulate hypotheses about parts or aspects of the imagc and then decide which of these hypotheses are likely to be correct. Thc probability that each hypothesis is correct is determined partly by its fit to the imagc and partly by its fit to other hypothcses (hat are taken to be correct, so the truth&#039;value of an individual hypothesis cannot be decided in isolation. One method of searching for the most plausible combination of hypotheses is to use a rclaxation process in which a probability is associated with each hypothesis, and the probabilities arc then iteratively modified on the basis of the fit to the imagc and the known relationships bctwcen hypotheses. An attractive property of rclaxation methods is that they can be implemented in parallel hardwarc where one computational unit is used for each possible hypothcsis, and the interactions betwcen hypotheses are implemented by dircct hardwarc connections betwcen the units. Many variations of the basic relaxation idea have becn However, all the current methods suffer from one or more of the following problems:
444|Schema selection and stochastic inference in modular environments|Given a set of stimuli presenting views of some environment, how can one characterize the natural modules or “objects ” that compose the environment? Should a given set of items be encoded as a collection of instances or as a set of rules? Restricted formulations of these questions are addressed by analysis within a new mathematical framework that describes stochastic parallel computation. An algorithm is given for simulating this computation once schemas encoding the modules of the environment have been seIected. The concept of computational temperature is introduced. As this temperature is Iowered, the system appears to display a dramatic tendency to interpret input, even if the evidence for any particular interpretation is very weak. IIltrodoction Our sensory systems are capabIe of representing a vast number of possible stimuli. Our environment presents us with only a smaI1 fraction of the possibilities; this se&amp;ted subset is characterized by many regularities. Our minds encode these regularities, and this gives us some ability to infer the probable current condition of unknown portions of the environment given some Iimited information about the current state. What kind of regularities exist in the environment, and how should they be encoded? This paper presents preliminary results of research founded on the hypothesis that in real environments there exist reguIarities that can be idealized as mathematical structures that are simpIe enough to be anaIyxabIe. Only the simpIest kind of reguhuity is considered here: I will assume that the environment contains modules (objects) that recur exactly, with various states of the environment being comprised of various combinations of these modules. Even this simplest kind of environmental regularity offers interesting Iearning problems and results. It also serves to introduce a general framework capable of treating more subtle types of regularities. And the probIem considered is an important one, for the delineation of moduIes at one level of conceptual representation is a major step in the construction of higher Ievel representations.
445|Dynamic topic models|Scientists need new tools to explore and browse large collections of scholarly literature. Thanks to organizations such as JSTOR, which scan and index the original bound archives of many journals, modern scientists can search digital libraries spanning hundreds of years. A scientist, suddenly
446|Latent dirichlet allocation|We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model. 1.
447|A bayesian hierarchical model for learning natural scene categories|We propose a novel approach to learn and recognize natural scene categories. Unlike previous work [9, 17], it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a “theme”. In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes. 1.
448|The Author-Topic Model for Authors and Documents |We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, &amp; Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics
that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact
inference is intractable for these datasets and
we use Gibbs sampling to estimate the topic
and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model)
and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications
to computing similarity between authors and
entropy of author output.
449|Sparse Gaussian processes using pseudo-inputs|We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M « N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M 2 N) training cost and O(M 2) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime. 1
450|Discovering object categories in image collections|Given a set of images containing multiple object categories, we seek to discover those categories and their image locations without supervision. We achieve this using generative models from the statistical text literature: probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA). In text analysis these are used to discover topics in a corpus using the bag-of-words document representation. Here we discover topics as object categories, so that an image containing instances of several categories is modelled as a mixture of topics. The models are applied to images by using a visual analogue of a word, formed by vector quantizing SIFT like region descriptors. We investigate a set of increasingly demanding scenarios, starting with image sets containing only two object categories through to sets containing multiple categories (including airplanes, cars, faces, motorbikes, spotted cats) and background clutter. The object categories sample both intra-class and scale variation, and both the categories and their approximate spatial layout are found without supervision. We also demonstrate classification of unseen images and images containing multiple objects. Performance of the proposed unsupervised method is compared to the semi-supervised approach of [7].  
451|Integrating topics and syntax|Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words. We present a generative model that uses both kinds of dependencies, and can be used to simultaneously find syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency. This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively. 1
452|A Generalized Mean Field Algorithm for Variational Inference in Exponential Families|We present a class of generalized mean field (GMF) algorithms for approximate inference in exponential family graphical models which is analogous to the generalized belief propagation (GBP) or cluster variational methods. While those methods are based on...
453|Collaborative Filtering: A Machine Learning Perspective|Collaborative filtering was initially proposed as a framework for filtering information based on the preferences of users, and has since been refined in many different ways. This thesis is a comprehensive study of rating-based, pure, non-sequential collaborative filtering. We analyze existing methods for the task of rating prediction from a machine learning perspective. We show that many existing methods proposed for this task are simple applications or modi  cations of one or more standard machine learning methods for classifi cation, regression, clustering, dimensionality reduction, and density estimation. We introduce new prediction methods in all of these classes. We introduce a new experimental procedure for testing stronger forms of generalization than has been used previously. We implement a total of nine prediction methods, and conduct large scale prediction accuracy experiments. We show interesting new results on the relative performance of these methods.
454|Applying Discrete PCA in Data Analysis|Methods for analysis of principal components in discrete data have existed for some time under various names such as grade of membership modelling, probabilistic latent semantic analysis, and genotype inference with admixture. In this paper we explore a number of extensions to the common theory, and present some application of these methods to some common statistical tasks. We show that these methods can be interpreted as a discrete version of ICA. We develop a hierarchical version yielding components at different levels of detail, and additional techniques for Gibbs sampling. We compare the algorithms on a text prediction task using support vector machines, and to information retrieval.  
455|The authorrecipienttopic model for topic and role discovery in social networks: Experiments with Enron and academic email|Previous work in social network analysis (SNA) has modeled the existence of links from one entity to another, but not the language content or topics on those links. We present the Author-Recipient-Topic (ART) model for social network analysis, which learns topic distributions based on the the directionsensitive messages sent between entities. The model builds on Latent Dirichlet Allocation and the Author-Topic (AT) model, adding the key attribute that distribution over topics is conditioned distinctly on both the sender and recipient—steering the discovery of topics according to the relationships between people. We give results on both the Enron email corpus and a researcher’s email archive, providing evidence not only that clearly relevant topics are discovered, but that the ART model better predicts people’s roles. 1
457|Simplicial mixtures of Markov chains: Distributed modelling of dynamic user profiles|To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the definition of individual specific and global models. This paper proposes a lineartime distributed model for finite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be ‘explained ’ by a relatively small number of structurally simple common behavioral patterns which may interleave randomly in a user-specific proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efficient representation scheme, reflected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations. 1
458|Model-Based Clustering, Discriminant Analysis, and Density Estimation|Cluster analysis is the automated search for groups of related observations in a data set. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as \How many clusters are there?&#034;, &#034;Which clustering method should be used?&#034; and \How should outliers be handled?&#034;. We outline a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, mineeld detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology, a...
459|Bayes Factors|In a 1935 paper, and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P -values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this paper we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology and psychology.
461|Bayesian Density Estimation and Inference Using Mixtures|We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation, and are exemplified by special cases where data are modelled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models. Keywords: Kernel estimation; Mixtures of Dirichlet processes; Multimodality; Normal mixtures; Posterior sampling; Smoothing parameter estimation * Michael D. Escobar is Assistant Professor, Department of Statistics and Department of Preventive Medicine and Biostatistics, University ...
462|Bayesian Model Selection in Social Research (with Discussion by Andrew Gelman &amp; Donald B. Rubin, and Robert M. Hauser, and a Rejoinder)  (1995) |It is argued that P-values and the tests based upon them give unsatisfactory results, especially in large samples. It is shown that, in regression, when there are many candidate independent variables, standard variable selection procedures can give very misleading results. Also, by selecting a single model, they ignore model uncertainty and so underestimate the uncertainty about quantities of interest. The Bayesian approach to hypothesis testing, model selection and accounting for model uncertainty is presented. Implementing this is straightforward using the simple and accurate BIC approximation, and can be done using the output from standard software. Specific results are presented for most of the types of model commonly used in sociology. It is shown that this approach overcomes the difficulties with P values and standard model selection procedures based on them. It also allows easy comparison of non-nested models, and permits the quantification of the evidence for a null hypothesis...
464|Regularized discriminant analysis|Linear and quadratic discriminant analysis are considered in the small sample high-dimensional setting. Alternatives to the usual maximum likelihood (plug-in) estimates for the covariance matrices are proposed. These alternatives are characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample based estimate of future misclassification risk. Computationally fast implementations are presented, and the efficacy of the approach is examined through simulation studies and application to data. These studies indicate that in many circumstances dramatic gains in classification accuracy can be achieved. Submitted to Journal of the American Statistical Association
466|Discriminant Analysis by Gaussian Mixtures|Fisher-Rao linear discriminant analysis (LDA) is a valuable tool for multigroup classification. LDA is equivalent to maximum likelihood classification assuming Gaussian distributions for each class. In this paper, we fit Gaussian mixtures to each class to facilitate effective classification in non-normal settings, especially when the classes are clustered. Low dimensional views are an important by-product of LDA---our new techniques inherit this feature. We are able to control the within-class spread of the subclass centers relative to the between-class spread. Our technique for fitting these models permits a natural blend with nonparametric versions of LDA.  Keywords: Classification, Pattern Recognition, Clustering, Nonparametric, Penalized. 1 Introduction  In the generic classification or discrimination problem, the outcome of interest  G falls into J unordered classes, which for convenience we denote by the set J =  f1; 2; 3; \Delta \Delta \Delta Jg. We wish to build a rule for pred...
467|Practical Bayesian Density Estimation Using Mixtures Of Normals|this paper, we propose some solutions to these problems. Our goal is to come up with a simple, practical method for estimating the density. This is an interesting problem in its own right, as well as a first step towards solving other inference problems, such as providing more flexible distributions in hierarchical models. To see why the posterior is improper under the usual reference prior, we write the model in the following way. Let Z = (Z 1 ; : : : ; Z n ) and X = (X 1 ; : : : ; X n ). The Z
469|Detecting Features in  Spatial Point Processes with . . .|We consider the problem of detecting features in spatial point processes in the presence of substantial clutter. One example is the detection of mine elds using reconnaissance aircraft images that erroneously identify many objects that are not mines. Another is the detection of seismic faults on the basis of earthquake catalogs: earthquakes tend to be clustered close to the faults, but there are many that are farther away. Our solution uses model-based clustering based on a mixture model for the process, in which features are assumed to generate points according to highly linear multivariate normal densities, and the clutter arises according to a spatial Poisson process. Very nonlinear features are represented by several highly linear multivariate normal densities, giving a piecewise linear representation. The model is estimated in two stages. In the rst stage, hierarchical model-based clustering is used to provide a rst estimate of the features. In the second stage, this clustering is re ned using the EM algorithm. The number of features is found using an approximation to the posterior probability of each number of features. For the minefield
470|MCLUST: Software for Model-based Cluster Analysis|MCLUST is a software package for cluster analysis written in Fortran and interfaced to the S-PLUS commercial software package1. It implements parameterized Gaussian hierarchical clustering algorithms [16, 1, 7] and the EM algorithm for parameterized Gaussian mixture models [5, 13, 3, 14] with the possible addition of a Poisson noise term. MCLUST also includes functions that combine hierarchical clustering, EM and the Bayesian Information Criterion (BIC) in a comprehensive clustering strategy [4, 8]. Methods of this type have shown promise in a number of practical applications, including character recognition [16], tissue segmentation [1], mine eld and seismic fault detection [4], identi cation of textile aws from images [2], and classi cation of astronomical data [3, 15]. Aweb page with related links can be found at
471|Algorithms for model-based Gaussian hierarchical clustering|1 Funded by the O ce of Naval Research under contracts N00014-96-1-0192 and N00014-96-1-
472|Regularized Gaussian Discriminant Analysis Through Eigenvalue Decomposition|Friedman (1989) has proposed a regularization technique (RDA) of discriminant analysis in the Gaussian framework. RDA makes use of two regularization parameters to design an intermediate classi cation rule between linear and quadratic discriminant analysis. In this paper, we propose an alternative approach to design classi cation rules which have also a median position between linear and quadratic discriminant analysis. Our approach is based on the reparametrization of the covariance matrix k of a group Gk in terms of its eigenvalue decomposition, k = kDkAkD 0 k where k speci es the volume of Gk, Ak its shape, and Dk its orientation. Variations on constraints concerning k?Ak and Dk lead to 14 discrimination models of interest. For each model, we derived the maximum likelihood parameter estimates and our approach consists in selecting the model among the 14 possible models by minimizing the sample-based estimate of future misclassi cation risk by cross-validation. Numerical experiments show favorable behavior of this approach as compared to RDA.
473|Scaling EM (Expectation-Maximization) Clustering to Large Databases  (1999) |Practical statistical clustering algorithms typically center upon an iterative refinement optimization procedure to compute a locally optimal clustering solution that maximizes the fit to data. These algorithms typically require many database scans to converge, and within each scan they require the access to every record in the data table. For large databases, the scans become prohibitively expensive. We present a scalable implementation of the Expectation-Maximization (EM) algorithm. The database community has focused on distance-based clustering schemes and methods have been developed to cluster either numerical or categorical data. Unlike distancebased algorithms (such as K-Means), EM constructs proper statistical models of the underlying data source and naturally generalizes to cluster databases containing both discrete-valued and continuous-valued data. The scalable method is based on a decomposition of the basic statistics the algorithm needs: identifying regions of the data that...
474|Averaging, Maximum Penalized Likelihood and Bayesian Estimation for Improving Gaussian Mixture Probability Density Estimates|We apply the idea of averaging ensembles of estimators to probability density estimation.
475|Non Parametric Maximum Likelihood Estimation of Features in . . .|This paper addresses the problem of estimating the support domain of a bounded point process in presence of background noise. This situation occurs for example in the detection of a mine field from aerial observations. A Maximum Likelihood Estimator for a mixture of uniform point processes is derived using a natural partition of the space defined by the data themselves: the Voronoï tessellation. The methodology is tested on simulations and compared to a model-based clustering technique.
476|Inference in model-based cluster analysis|A new approach to cluster analysis has been introduced based on parsimonious geometric modelling of the within-group covariance matrices in a mixture of multivariate normal distributions, using hierarchical agglomeration and iterative relocation. It works well and is widely used via the MCLUST software available in S-PLUS and StatLib. However, it has several limitations: there is no assessment of the uncertainty about the classification, the partition can be suboptimal, parameter estimates are biased, the shape matrix has to be specified by the user, prior group probabilities are assumed to be equal, the method for choosing the number of groups is based on a crude approximation, and no formal way of choosing between the various possible models is included. Here, we propose a new approach which overcomes all these difficulties. It consists of exact Bayesian inference via Gibbs sampling, and the calculation of Bayes factors (for choosing the model and the number of groups) from the output using the Laplace-Metropolis estimator. It works well in several real and simulated examples.  
477|Principal curve clustering with noise|was supported by ONR grants N00014-96-1-0192 and N00014-96-1-0330. The authors are Clustering on principal curves combines parametric modeling of noise with nonparametric modeling of feature shape. This is useful for detecting curvilinear features in spatial point patterns, with or without background noise. Applications of this include the detection of curvilinear mine elds from reconnaissance images, some of the points in which represent false detections, and the detection of seismic faults from earthquake catalogs. Our algorithm for principal curve clustering is in two steps: the rst is hierarchical and agglomerative (HPCC), and the second consists of iterative relocation based on the Classi cation EM algorithm (CEM-PCC). HPCC is used to combine potential feature clusters, while CEM-PCC re nes the results and deals with background noise. It is importanttohave a good starting point for the algorithm: this can be found manually or automatically using, for example, nearest neighbor clutter removal or model-based clustering. We choose the number of features and the amount of smoothing simultaneously using approximate Bayes
478|Fitting mixtures of Kent distributions to aid in joint set identification|When examining a rock mass, joint sets and their orientations can play a significant role with regard to how the rock mass will behave. To identify joint sets present in the rock mass, the orientation of individual fracture planes can be measured on exposed rock faces and the resulting data can be examined for heterogeneity. In this article, the expectation–maximization algorithm is used to fit mixtures of Kent component distributions to the fracture data to aid in the identification of joint sets. An additional uniform component is also included in the model to accommodate the noise present in the data.
479|From Massive Data Sets to Science Catalogs: Applications and Challenges|With hardware advances in scientific instruments and data gathering techniques comes the  inevitable flood of data that can render traditional approaches to science data analysis severely  inadequate. The traditional approach of manual and exhaustive analysis of a data set is no longer  feasible for many tasks ranging from remote sensing, astronomy, and atmospherics to medicine,  molecular biology, and biochemistry. In this paper we present our views as practitioners engaged  in building computational systems to help scientists deal with large data sets. We focus on  what we view as challenges and shortcomings of the current state-of-the-art in data analysis in  view of the massive data sets that are still awaiting analysis. The presentation is grounded in  applications in astronomy, planetary sciences, solar physics, and atmospherics that are currently  driving much of our work at JPL.  keywords: science data analysis, limitations of current methods, challenges for massive data sets, ...
480|Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions|This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multcriteria ratings, and a provision of more flexible and less intrusive types of recommendations. 
483|Grouplens: Applying collaborative filtering to usenet news|... a collaborative filtering system for Usenet news—a high-volume, high-turnover discussion list service on the Internet. Usenet newsgroups—the individual discussion lists—may carry hundreds of messages each day. While in theory the newsgroup organization allows readers to select the content that most interests them, in practice most newsgroups carry a wide enough spread of messages to make most individuals consider Usenet news to be a high noise information resource. Furthermore, each user values a different set of messages. Both taste and prior knowledge are major factors in evaluating news articles. For example, readers of the rec.humor newsgroup, a group designed for jokes and other humorous postings, value articles based on whether they perceive them to be funny. Readers of technical groups, such as comp.lang.c? ? value articles based
484|Probabilistic Latent Semantic Analysis|Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two--mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.
485|Active Learning with Statistical Models|For manytypes of learners one can compute the statistically &#034;optimal&#034; way to select data. We review how  these techniques have been used with feedforward neural networks [MacKay, 1992# Cohn, 1994]. We then  showhow the same principles may be used to select data for two alternative, statistically-based learning  architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural  networks are expensive and approximate, the techniques for mixtures of Gaussians and locally weighted  regression are both efficient and accurate.
486|NewsWeeder: Learning to Filter Netnews|A significant problem in many information filtering systems is the dependence on the user for the creation and maintenance of a user profile, which describes the user&#039;s interests. NewsWeeder is a netnews-filtering system that addresses this problem by letting the user rate his or her interest level for each article being read (1-5), and then learning a user profile based on these ratings. This paper describes how NewsWeeder accomplishes this task, and examines the alternative learning methods used. The results show that a learning algorithm based on the Minimum Description Length (MDL) principle was able to raise the percentage of interesting articles to be shown to users from 14% to 52% on average. Further, this performance significantly outperformed (by 21%) one of the most successful techniques in Information Retrieval (IR), termfrequency /inverse-document-frequency (tf-idf) weighting. 1
487|Improving generalization with active learning|Abstract. Active learning differs from &amp;quot;learning from examples &amp;quot; in that the learning algorithm assumes at least some control over what part of the input domain it receives information about. In some situations, active learning is provably more powerful than learning from examples alone, giving better generalization for a fixed number of training examples. In this article, we consider the problem of learning a binary concept in the absence of noise. We describe a formalism for active concept learning called selective sampling and show how it may be approximately implemented by a neural network. In selective sampling, a learner receives distribution information from the environment and queries an oracle on parts of the domain it considers &amp;quot;useful. &amp;quot; We test our implementation, called an SGnetwork, on three domains and observe significant improvement in generalization.
488|Selective sampling using the Query by Committee algorithm|We analyze the &#034;query by committee&#034; algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons.
489|Learning and Revising User Profiles: The Identification of Interesting Web Sites|. We discuss algorithms for learning and revising user profiles that can determine which World Wide Web sites on a given topic would be interesting to a user. We describe the use of a naive Bayesian classifier for this task, and demonstrate that it can incrementally learn profiles from user feedback on the interestingness of Web sites. Furthermore, the Bayesian classifier may easily be extended to revise user provided profiles. In an experimental evaluation we compare the Bayesian classifier to computationally more intensive alternatives, and show that it performs at least as well as these approaches throughout a range of different domains. In addition, we empirically analyze the effects of providing the classifier with background knowledge in form of user defined profiles and examine the use of lexical knowledge for feature selection. We find that both approaches can substantially increase the prediction accuracy.  Keywords: Information filtering, intelligent agents, multistrategy lea...
490|Eigentaste: A Constant Time Collaborative Filtering Algorithm|Eigentaste is a collaborative filtering algorithm that uses universal queries to elicit real-valued user ratings on a common set of items and applies principal component analysis (PCA) to the resulting dense subset of the ratings matrix. PCA facilitates dimensionality reduction for offline clustering of users and rapid computation of recommendations. For a database of n users, standard nearest-neighbor techniques require O(n) processing time to compute recommendations, whereas Eigentaste requires O(1) (constant) time. We compare Eigentaste to alternative algorithms using data from Jester, an online joke recommending system. Jester has collected approximately 2,500,000 ratings from 57,000 users. We use the Normalized Mean Absolute Error (NMAE) measure to compare performance of different algorithms. In the Appendix we use Uniform and Normal distribution models to derive analytic estimates of NMAE when predictions are random. On the Jester dataset, Eigentaste computes recommendations two ...
491|Learning Collaborative Information Filters|Predicting items a user would like on the basis of other users’ ratings for these items has become a well-established strategy adopted by many recommendation services on the Internet. Although this can be seen as a classification problem, algo-rithms proposed thus far do not draw on results from the ma-chine learning literature. We propose a representation for collaborative filtering tasks that allows the application of virtually any machine learning algorithm. We identify the shortcomings of current collaborative filtering techniques and propose the use of learning algorithms paired with feature extraction techniques that specifically address the limitations of previous approaches. Our best-performing algorithm is based on the singular value decomposition of an initial matrix of user ratings, exploiting latent structure that essentially eliminates the need for users to rate common items in order to become predictors for one another&#039;s preferences. We evaluate the proposed algorithm on a large database of user ratings for motion pictures and find that our approach significantly out-performs current collaborative filtering algorithms.
492|Recommendation as Classification: Using Social and Content-Based Information in Recommendation|Recommendation systems make suggestions about artifacts to a user. For instance, they may predict whether a user would be interested in seeing a particular movie. Social recomendation methods collect ratings of artifacts from many individuals and use nearest-neighbor techniques to make recommendations to a user concerning new artifacts. However, these methods do not use the significant amount of other information that is often available about the nature of each artifact --- such as cast lists or movie reviews, for example. This paper presents an inductive learning approach to recommendation that is able to use both ratings information and other forms of information about each artifact in predicting user preferences. We show that our method outperforms an existing social-filtering method in the domain of movie recommendations on a dataset of more than 45,000 movie ratings collected from a community of over 250 users.  Introduction  Recommendations are a part of everyday life. We usually...
493|Latent Semantic Models for Collaborative filtering |Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, that is, a database of available user preferences. In this article, we describe a new family of model-based algorithms designed for this task. These algorithms rely on a statistical modelling technique that introduces latent class variables in a mixture model setting to discover user communities and prototypical interest profiles. We investigate several variations to deal with discrete and continuous response variables as well as with different objective functions. The main advantages of this technique over standard memory-based methods are higher accuracy, constant time prediction, and an explicit and compact model representation. The latter can also be used to mine for user communitites. The experimental evaluation shows that substantial improvements in accucracy over existing methods and published results can be obtained.
494|Content-Boosted Collaborative Filtering for Improved Recommendations|Most recommender systems use Collaborative Filtering or Content-based methods to predict new items of interest for a user. While both methods have their own advantages, individually they fail to provide good recommendations in many situations. Incorporating components from both methods, a hybrid recommender system can overcome these shortcomings.
495|Content-Based Book Recommending Using Learning for Text Categorization|Recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user&#039;s likes and dislikes. Most existing recommender systems use collaborative filtering methods that base recommendations on other users&#039; preferences. By contrast, content-based methods use information about an item itself to make suggestions. This approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations. We describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization. Initial experimental results demonstrate that this approach can produce accurate recommendations.
496|Multicriteria Optimization|n Using some real-world examples I illustrate the important role of multiobjective optimization in decision making and its interface with preference handling. I explain what optimization in the presence of multiple objectives means and discuss some of the most common methods of solving multiobjective optimization problems using transformations to single-objective optimization problems. Finally, I address linear and combinatorial optimization problems with multiple objectives and summarize techniques for solving them. Throughout the article I
497|Heterogeneous uncertainty sampling for supervised learning|Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger. 1
498|Item-Based Top-N Recommendation Algorithms|... In this paper we present one such class of model-based recommendation algorithms that first determines the similarities between the various items and then uses them to identify the set of items to be recommended. The key steps in this class of algorithms are (i) the method used to compute the similarity between the items, and (ii) the method used to combine these similarities in order to compute the similarity between a basket of items and a candidate recommender item. Our experimental evaluation on eight real datasets shows that these item-based algorithms are up to two orders of magnitude faster than the traditional user-neighborhood based recommender systems and provide recommendations with comparable or better quality
499|Combining collaborative filtering with personal agents for better recommendations|Information filtering agents and collaborative filtering both attempt to alleviate information overload by identifying which items a user will find worthwhile. Information filtering (IF) focuses on the analysis of item content and the development of a personal user interest profile. Collaborative filtering (CF) focuses on identification of other users with similar tastes and the use of their opinions to recommend items. Each technique has advantages and limitations that suggest that the two could be beneficially combined. This paper shows that a CF framework can be used to combine personal IF agents and the opinions of a community of users to produce better recommendations than either agents or users can produce alone. It also shows that using CF to create a personal combination of a set of agents produces better results than either individual agents or other combination mechanisms. One key implication of these results is that users can avoid having to select among agents; they can use them all and let the CF framework select the best ones for them.
500|Incorporating Contextual Information in Recommender Systems Using a Multidimensional Approach|The paper presents a multidimensional (MD) approach to recommender systems that can provide recommendations based on additional contextual information besides the typical information on users and items used in most of the current recommender systems. This approach supports multiple dimensions, extensive profiling, and hierarchical aggregation of recommendations. The paper also presents a multidimensional rating estimation method capable of selecting two-dimensional segments of ratings pertinent to the recommendation context and applying standard collaborative filtering or other traditional two-dimensional rating estimation techniques to these segments. A comparison of the multidimensional and two-dimensional rating estimation approaches is made, and the tradeoffs between the two are studied. Moreover, the paper introduces a combined rating estimation method that identifies the situations where the MD approach outperforms the standard two-dimensional approach and uses the MD approach in those situations and the standard two-dimensional approach elsewhere. Finally, the paper presents a pilot empirical study of the combined approach, using a multidimensional movie recommender system that was developed for implementing this approach and testing its performance. 1 1.
501|Collaborative Filtering by Personality Diagnosis: A Hybrid Memory- and Model-Based Approach|The growth of Internet commerce has stimulated  the use of collaborative filtering (CF) algorithms  as recommender systems. Such systems leverage  knowledge about the known preferences of  multiple users to recommend items of interest to  other users. CF methods have been harnessed  to make recommendations about such items as  web pages, movies, books, and toys. Researchers  have proposed and evaluated many approaches  for generating recommendations. We describe  and evaluate a new method called personality  diagnosis (PD). Given a user&#039;s preferences for  some items, we compute the probability that he  or she is of the same &#034;personality type&#034; as other  users, and, in turn, the probability that he or she  will like new items. PD retains some of the advantages  of traditional similarity-weighting techniques  in that all data is brought to bear on each  prediction and new data can be added easily and  incrementally. Additionally, PD has a meaningful  probabilistic interpretation, which ma...
502|Clustering Methods for Collaborative Filtering|Grouping people into clusters based on the items they have purchased allows accurate recommendations of new items for purchase: if you and I have liked many of the same movies, then I will probably enjoy other movies that you like. Recommending items based on similarity of interest (a.k.a. collaborative filtering) is attractive for many domains: books, CDs, movies, etc., but does not always work well. Because data are always sparse -- any given person has seen only a small fraction of all movies -- much more accurate predictions can be made by grouping people into clusters with similar movies and grouping movies into clusters which tend to be liked by the same people. Finding optimal clusters is tricky because the movie groups should be used to help determine the people groups and visa versa. We present a formal statistical model of collaborative filtering, and compare different algorithms for estimating the model parameters including variations of K-means clustering and Gibbs Sampling. This...
503|E-Commerce Recommendation Applications|Recommender systems are being used by an ever-increasing number of E-commerce sites to help consumers find products to purchase. What started as a novelty has turned into a serious business tool. Recommender systems use product knowledge -- either hand-coded knowledge provided by experts or &#034;mined&#034; knowledge learned from the behavior of consumers -- to guide consumers through the often-overwhelming task of locating products they will like. In this article we present an explanation of how recommender systems are related to some traditional database analysis techniques. We examine how recommender systems help E-commerce sites increase sales and analyze the recommender systems at six market-leading sites. Based on these examples, we create a taxonomy of recommender systems, including the inputs required from the consumers, the additional knowledge required from the database, the ways the recommendations are presented to consumers, the technologies used to create the recommendations, and t...
504|The digitization of word of mouth: Promise and challenges of online feedback mechanisms|Online feedback mechanisms harness the bidirectional communication capabilities of the Internet to engineer large-scale, word-of-mouth networks. Best known so far as a technology for building trust and fostering cooperation in online marketplaces, such as eBay, these mechanisms are poised to have a much wider impact on organizations. Their growing popularity has potentially important implications for a wide range of management activities such as brand building, customer acquisition and retention, product development, and quality assurance. This paper surveys our progress in understanding the new possibilities and challenges that these mechanisms represent. It discusses some important dimensions in which Internet-based feedback mechanisms differ from traditional word-of-mouth networks and surveys the most important issues related to their design, evaluation, and use. It provides an overview of relevant work in game theory and economics on the topic of reputation. It discusses how this body of work is being extended and combined with insights from computer science, management science, sociology, and psychology to take into consideration the special properties of online environments. Finally, it identifies opportunities that this new area presents for operations research/management science (OR/MS) research.
505|Horting Hatches an Egg: A New Graph-Theoretic Approach to Collaborative Filtering|This paper introduces a new and novel approach to ratingbased collaborative filtering. The new technique is most appropriate for e-commerce merchants offering one or more groups of relatively homogeneous items such as compact disks, videos, books, software and the like. In contrast with other known collaborative filtering techniques, the new algorithm is graph-theoretic, based on the twin new concepts of horting and predictability. As is demonstrated in this paper, the technique is fast, scalable, accurate, and requires only a modest learning curve. It makes use of a hierarchical classification scheme in order to introduce context into the rating process, and uses so-called creative links in order to find surprising and atypical items to recommend, perhaps even items which cross the group boundaries. The new technique is one of the key engines of the Intelligent Recommendation Algorithm (IRA) project, now being developed at IBM Research. In addition to several other recommendation engines, IRA contains a situation analyzer to determine the most appropriate mix of engines for a particular e-commerce merchant, as well as an engine for optimizing the placement of advertisements.
506|Discovery and Evaluation of Aggregate Usage Profiles for Web Personalization|Web usage mining, possibly used in conjunction with standard approaches to personalization such as collaborative filtering, can help address some of the shortcomings of these techniques, including reliance on subjective user ratings, lack of scalability, and poor performance in the face of high-dimensional and sparse data. However, the discovery of patterns from usage data by itself is not sufficient for performing the personalization tasks. The critical step is the effective derivation of good quality and useful (i.e., actionable) &#034;aggregate usage profiles&#034; from these patterns. In this paper we present and experimentally evaluate two techniques, based on clustering of user transactions and clustering of pageviews, in order to discover overlapping aggregate profiles that can be effectively used by recommender systems for real-time Web personalization. We evaluate these techniques both in terms of the quality of the individual profiles generated, as well as in the context of providing recommendations as an integrated part of a personalization engine. In particular, our results indicate that using the generated aggregate profiles, we can achieve effective personalization at early stages of users&#039; visits to a site, based only on anonymous clickstream data and without the benefit of explicit input by these users or deeper knowledge about them.
507|Applying Associative Retrieval Techniques to Alleviate the Sparsity Problem in Collaborative Filtering|this article, we propose to deal with this sparsity problem by applying an associative retrieval framework and related spreading activation algorithms to explore transitive associations among consumers through their past transactions and feedback. Such transitive associations are a valuable source of information to help infer consumer interests and can be explored to deal with the sparsity problem. To evaluate the effectiveness of our approach, we have conducted an experimental study using a data set from an online bookstore. We experimented with three spreading activation algorithms including a constrained Leaky Capacitor algorithm, a branch-and-bound serial symbolic search algorithm, and a Hopfield net parallel relaxation search algorithm. These algorithms were compared with several collaborative filtering approaches that do not consider the transitive associations: a simple graph search approach, two variations of the user-based approach, and an item-based approach. Our experimental results indicate that spreading activation-based approaches significantly outperformed the other collaborative filtering methods as measured by recommendation precision, recall, the F-measure, and the rank score. We also observed the over-activation effect of the spreading activation approach, that is, incorporating transitive associations with past transactional data that is not sparse may &#034;dilute&#034; the data used to infer user preferences and lead to degradation in recommendation performance
508|MovieLens Unplugged: Experiences with an Occasionally Connected Recommender System|Recommender systems have changed the way people shop online. Recommender systems on wireless mobile devices may have the same impact on the way people shop in stores. We present our experience with implementing a recommender system on a PDA that is occasionally connected to the network. This interface helps users of the MovieLens movie recommendation service select movies to rent, buy, or see while away from their computer. The results of a nine month field study show that although there are several challenges to overcome, mobile recommender systems have the potential to provide value to their users today.
509|Implicit Feedback for Recommender System|Can implicit feedback substitute for explicit ratings in recommender systems? If so, we could avoid the difficulties associated with gathering explicit ratings from users. How, then, can we capture useful information unobtrusively, and how might we use that information to make recommendations? In this paper we identify three types of implicit feedback and suggest two strategies for using implicit feedback to make recommendations.
510|Collaborative Filtering via Gaussian Probabilistic Latent Semantic Analysis|Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, i.e. a database of available user preferences. In this paper, we describe a new model-based algorithm designed for this task, which is based on a generalization of probabilistic latent semantic analysis to continuous-valued response variables. More specifically, we assume that the observed user ratings can be modeled as a mixture of user communities or interest groups, where users may participate probabilistically in one or more groups. Each community is characterized by a Gaussian distribution on the normalized ratings for each item. The normalization of ratings is performed in a user-specific manner to account for variations in absolute shift and variance of ratings. Experiments on the EachMovie data set show that the proposed approach compares favorably with other collaborative filtering techniques.
511|Flexible Mixture Model for Collaborative Filtering|This paper presents a flexible mixture model  (FMM) for collaborative filtering. FMM extends  existing partitioning/clustering algorithms for  collaborative filtering by clustering both users  and items together simultaneously without  assuming that each user and item should only  belong to a single cluster. Furthermore, with the  introduction of `preference&#039; nodes, the proposed  framework is able to explicitly model how users  rate items, which can vary dramatically, even  among the users with similar tastes on items.
513|Combining Content and Collaboration in Text Filtering|We describe a technique for combining collaborative input and document content for text filtering. This technique uses latent semantic indexing to create a collaborative view of a collection of user profiles. The profiles themselves are term vectors constructed from documents deemed relevant to the user&#039;s information need. Using standard text collections, this approach performs quite favorably compared to other content-based approaches.  1 Introduction  Filtering is a process of comparing an incoming document stream to a profile of a user&#039;s interests and recommending the documents according to that profile [ Belkin and Croft, 1992 ] . A simple approach for filtering textual content might be to look at each document&#039;s similarity to an average of known relevant documents. Collaborative filtering takes into account the similarities and differences among the profiles of several users in determining how to recommend a document. Typically, collaborative filtering is done by correlating users...
514|Recommendation Systems: A Probabilistic Analysis|A recommendation system tracks past actions of a group of users to make recommendations to individual members of the group. The growth of computer-mediated marketing and commerce has led to increased interest in such systems. We introduce a simple analytical framework for recommendation systems, including a basis for defining the utility of such a system. We perform probabilistic analyses of algorithmic methods within this framework. These analyses yield insights into how much utility can be derived from the memory of past actions and on how this memory can be exploited. 1. Introduction  Collaborative filtering (sometimes known as a recommendation  system) is a process by which information on the preferences and actions of a group of users is tracked by a system which then, based on the patterns it observes, tries to make useful recommendations to individual users [10, 12, 18, 19, 20, 22, 23]. For instance, a book recommendation system might recommend Jules Verne to someone interested ...
515|Bayesian Mixed-Effects Models for Recommender Systems|We propose a Bayesian methodology for recommender systems that incorporates user ratings, user features, and item features in a single unified framework. In principle our approach should address the cold-start issue and can address both scalability issues as well as sparse ratings. However, our early experiments have shown mixed results.  1 Introduction  Recommender systems have emerged as an important application area and have been the focus of considerable recent academic and commercial interest. The 1997 special issue of the Communications of the ACM [14] contains some key papers. Other important contributions include [2], [4], [8], [13], [16], [9], [1], [12], and [15]. In addition, many online retailers are using this technology to recommend new items to their customers, based on what they have bought in the past. Currently, most recommender systems are either  content-based or collaborative, depending on the type of information that the system uses to recommend items to a user. Co...
516|The TREC-6 Filtering Track: Description and Analysis|This article details the experiments conducted in the TREC-6 filtering track. The filtering track is an extension of the routing track which adds time sequencing of the document stream and set-based evaluation strategies which simulate immediate distribution of the retrieved documents. It also introduces an adaptive filtering subtrack which is designed to simulate on-line or sequential filtering of documents. In addition to motivating the task and describing the practical details of participating in the track, this document includes a detailed graphical presentation of the experimental results and attempts to analyze and explain the observed patterns. The final section suggests some ways to extend the current research in future experiments. 1 Introduction  There is increasing evidence that text filtering will become a critical tool in searching and managing the flow of data in the information age. New companies are appearing daily which offer push services or intelligent agents centere...
517|Expert-driven validation of rule-based user models in personalization 9 |Abstract. In many e-commerce applications, ranging from dynamic Web content presentation, to personalized ad targeting, to individual recommendations to the customers, it is important to build personalized profiles of individual users from their transactional histories. These profiles constitute models of individual user behavior and can be specified with sets of rules learned from user transactional histories using various data mining techniques. Since many discovered rules can be spurious, irrelevant, or trivial, one of the main problems is how to perform post-analysis of the discovered rules, i.e., how to validate user profiles by separating “good ” rules from the “bad.” This validation process should be done with an explicit participation of the human expert. However, complications may arise because there can be very large numbers of rules discovered in the applications that deal with many users, and the expert cannot perform the validation on a rule-by-rule basis in a reasonable period of time. This paper presents a framework for building behavioral profiles of individual users. It also introduces a new approach to expert-driven validation of a very large number of rules pertaining to these users. In particular, it presents several types of validation operators, including rule grouping, filtering, browsing, and redundant rule elimination operators, that allow a human expert validate many individual rules at a time. By iteratively applying such operators, the human expert can validate a significant part of all the initially discovered rules in an acceptable time period. These validation operators were implemented as a part of a one-to-one profiling system. The paper also presents
518|Privacy risks in recommender systems|this article, we can state some general guidelines. Like most problems in computer security, the ideal deterrents are better awareness of the issues and more openness in how systems operate in the marketplace. In particular, individual sites should clearly state the policies and methodologies they employ with recommender systems, including the role played by straddlers in their data sets and system designs. This is especially true for sites with multiple homogeneous networks (as in Figures 6c and 6d).   By conveying benefits and risks to users intuitively, recommender systems could achieve greater acceptance. We envisage three general ways to highlight the implications of our analyses. # Present plots of benefit and risk versus usermodifiable parameters such as ratings, w, and l (if the algorithm allows their direct specification) to allow users to make informed choices about their levels of involvement
519|Combining Usage, Content, and Structure Data to Improve Web Site Recommendation|Web recommender systems anticipate the needs of web users  and provide them with recommendations to personalize their navigation.
520|A Bayesian Model for Collaborative Filtering|Consider the general setup where a set of items have been partially rated by a set of judges, in the sense that not every item has been rated by every judge. For this setup, we propose a Bayesian approach for the problem of predicting the missing ratings from the observed ratings. This approach incorporates similarity by assuming the set of judges can be partitioned into groups which share the same ratings probability distribution. This leads to a predictive distribution of missing ratings based on the posterior distribution of the groupings and associated ratings probabilities. Markov chain Monte Carlo methods and a hybrid search algorithm are then used to obtain predictions of the missing ratings. 1
521|A maximum entropy approach to collaborative filtering in dynamic, sparse, high-dimensional domains|We develop a maximum entropy (maxent) approach to generating recommendations in the context of a user’s current navigation stream, suitable for environments where data is sparse, highdimensional, and dynamic—conditions typical of many recommendation applications. We address sparsity and dimensionality reduction by first clustering items based on user access patterns so as to attempt to minimize the apriori probability that recommendations will cross cluster boundaries and then recommending only within clusters. We address the inherent dynamic nature of the problem by explicitly modeling the data as a time series; we show how this representational expressivity fits naturally into a maxent framework. We conduct experiments on data from ResearchIndex, a popular online repository of over 470,000 computer science documents. We show that our maxent formulation outperforms several competing algorithms in offline tests simulating the recommendation of documents to ResearchIndex users. 1
522|Using Probabilistic Relational Models for Collaborative Filtering|Recent projects in collaborative filtering and information filtering address the task of inferring user preference relationships for products or information. The data on which these inferences are based typically consists of pairs of people and items. The items may be information sources (such as web pages or newspaper articles) or products (such as books, software, movies or CDs). We are interested in making recommendations or predictions. Traditional approaches to the problem derive from classical algorithms in statistical pattern recognition and machine learning. The majority of these approaches assume a &#034;flat&#034; data representation for each object, and focus on a single dyadic relationship between the objects. In this paper, we examine a richer model that allows us to reason about many different relations at the same time. We build on the recent work on probabilistic relational models (PRMs), and describe how PRMs can be applied to the task of collaborative filtering. PRMs allow us t...
523|Characterization and construction of radial basis functions|We review characterizations of (conditional) positive deniteness and show how they apply to the theory of radial basis functions. We then give complete proofs for the (conditional) positive deniteness of all practically relevant basis functions. Furthermore, we show how some of these characterizations may lead to construction tools for positive denite functions. Finally, we give new construction techniques based on discrete methods which lead to non-radial, even nontranslation invariant, local basis functions.  
524|Collaborative Learning for Recommender Systems|Recommender systems use ratings from users on  items such as movies and music for the purpose  of predicting the user preferences on items that  have not been rated. Predictions are normally  done by using the ratings of other users of the  system, by learning the user preference as a function  of the features of the items or by a combination  of both these methods.
525|Book Recommending using Text Categorization with Extracted Information|Content-based recommender systems suggest documents,  items, and services to users based on learning  a profile of the user from rated examples containing  information about the given items. Text categorization  methods are very useful for this task but generally  rely on unstructured text. We have developed a bookrecommending  system that utilizes semi-structured information  about items gathered from the web using  simple information extraction techniques. Initial experimental  results demonstrate that this approach can  produce fairly accurate recommendations.
526|Memory-Based Weighted-Majority Prediction For Recommender Systems|Recommender Systems are learning systems that make use of data representing multi-user preferences over items (e.g. Vote [user, item] matrix), to try to predict the preference towards new items or products regarding a particular user. User preferences are in fact the learning target functions. The main objective of the system is to filter items according to the predicted preferences and present to the user the options that are most attractive to him; i.e. he would probably like the most. We study Recommender Systems viewed as a pool of independent prediction algorithms, one per every user, in situations in which each learner faces a sequence of trials, with a prediction to make in each step. The goal is to make as few mistakes as possible. We are interested in the case that each learner has reasons to believe that there exists some other target functions in the pool that consistently behaves similar, neutral or opposite to the target function it is trying to learn. The learner doesn&#039;t ...
527|Customer Lifetime Value Modeling and Its Use for Customer Retention Planning|We present and discuss the important business problem of estimating the effect of retention efforts on the Lifetime Value of a customer in the Telecommunications industry. We discuss the components of this problem, in particular customer value and length of service (or tenure) modeling, and present a novel segment-based approach, motivated by the segment-level view marketing analysts usually employ. We then describe how we build on this approach to estimate the effects of retention on Lifetime Value. Our solution has been successfully implemented in Amdocs&#039; Business Insight (BI) platform, and we illustrate its usefulness in real-world scenarios.
528|Preference-based Graphic Models for Collaborative Filtering|Collaborative filtering is a very useful general technique for exploiting the preference patterns of a group of users to predict the utility of items to a particular user. Previous research has studied several probabilistic graphic models for collaborative filtering with promising results. However, while these models have succeeded in capturing the similarity among users and items, none of them has considered the fact that users with similar interests in items can have very different rating patterns; some users tend to assign a higher rating to all items than other users. In this paper, we propose and study two new graphic models that address the distinction between user preferences and ratings. In one model, called the decoupled model, we introduce two different variables to decouple a user’s preferences from his/her ratings. In the other, called the preference model, we model the orderings of items preferred by a user, rather than the user’s numerical ratings of items. Empirical study over two datasets of movie ratings shows that, due to its appropriate modeling of the distinction between user preferences and ratings, the proposed decoupled model significantly outperforms all the five existing approaches that we compared with. The preference model, however, performs much worse than the decoupled model, suggesting that while explicit modeling of the underlying user preferences is very important for collaborative filtering, we can not afford ignoring the rating information completely. 1.
529|Adaptive lightweight text filtering|Abstract. We present a lightweight text filtering algorithm intended for use with personal Web information agents. Fast response and low resource usage were the key design criteria, in order to allow the algorithm to run on the client side. The algorithm learns adaptive queries and dissemination thresholds for each topic of interest in its user profile. We describe a factorial experiment used to test the robustness of the algorithm under different learning parameters and more importantly, under limited training feedback. The experiment borrows from standard practice in TREC by using TREC-5 data to simulate a user reading and categorizing documents. Results indicate that the algorithm is capable of achieving good filtering performance, even with little user feedback. 1
530|A theory of memory retrieval|A theory of memory retrieval is developed and is shown to apply over a range of experimental paradigms. Access to memory traces is viewed in terms of a resonance metaphor. The probe item evokes the search set on the basis of probe-memory item relatedness, just as a ringing tuning fork evokes sympathetic vibrations in other tuning forks. Evidence is accumulated in parallel from each probe-memory item comparison, and each comparison is modeled by a continuous random walk process. In item recognition, the decision process is self-terminating on matching comparisons and exhaustive on nonmatching comparisons. The mathematical model produces predictions about accuracy, mean reaction time, error latency, and reaction time distributions that are in good accord with experimental data. The theory is applied to four item recognition paradigms (Sternberg, prememorized list, study-test, and continuous) and to speed-accuracy paradigms; results are found to provide a basis for comparison of these paradigms. It is noted that neural network models can be interfaced to the retrieval theory with little difficulty and that semantic memory models may benefit from such a retrieval scheme.  
531|Distinctive features, categorical perception, and probability learning: some applications of a neural model|A previously proposed model for memory based on neurophysiological considerations is reviewed. We assume that (a) nervous system activity is usefully represented as the set of simultaneous individual neuron activities in a group of neurons; (b) different memory traces make use of the same synapses; and (c) synapses associate two patterns of neural activity by incrementing synaptic connectivity proportionally to the product of pre- and postsynaptic activity, forming a matrix of synaptic connectivities. We extend this model by (a) introducing positive feedback of a set of neurons onto itself and (b) allowing the individual neurons to saturate. A hybrid model, partly analog and partly binary, arises. The system has certain characteristics reminiscent of analysis by distinctive features. Next, we apply the model to &amp;quot;categorical perception. &amp;quot; Finally, we discuss probability learning. The model can predict overshooting, recency data, and probabilities occurring in systems with more than two events with reasonably good accuracy. In the beginner&#039;s mind there are many possibilities, but in the expert&#039;s there are few. —Shunryu Suzuki 1970 I.
532|Retrieval processes in recognition memory|A method of analyzing reaction time data in recognition memory is presented, which uses an explicit model of latency distributions. This distributional method allows us to distinguish between processes in a way that the traditional measure, mean latency, can not. The behavior of latency distributions is described, and four experiments are reported that show how recognition accuracy and latency vary with independent variables such as study and test position, rate of presentation, and list length. These data are used to develop and test the empirical model. The resulting analyses, together with functional relationships derived from the experimental data, are used to test several theories of recognition memory. The theories examined all show problems in light of these stringent tests, and general properties required by a model to account for the data are suggested. As well as arguing for distributional analyses of reaction time data, this paper presents a wide range of phenomena that any theory of recognition memory must explain. Over the last few years, researchers have been developing theories of recognition memory based not only on accuracy measures but also on latency measures. In this article, we consider latency measures in recognition memory. Results from four experiments are presented, and an empirical model for latency distributions is developed. Latency distributions are shown to provide much more information than can be obtained from mean latency, the most common dependent variable in reaction time measurements. From this, a strong case is made for the study of distributional properties by showing how some current theories are inadequate or wrong when examined in the light of distributional analyses. These recent theories are further evaluated using functional relationships extracted from results of the four experiments presented.
533|A threshold theory for simple detection experiments |The two-state &#034;high &#034; threshold model is generalized by assuming that (with low probability) the threshold may be exceeded when there is no stimulus. Existing Yes-No data (that rejected the high threshold theory) are compatible with the resulting isosensitivity (ROC) curves, namely, 2 line segments that intersect at the true threshold prob-abilities. The corresponding 2-alternative forced-choice curve is a 45° line through this intersection. A simple learning process is suggested to predict S&#039;s location along these curves, asymptotic means are derived, and comparisons are made with data. These asymptotic biases are coupled with the von Bdk&amp;y-Stevens neural quantum model to show how the theoretical linear psychometric functions are distorted into nonsymmetric, nonlinear response curves. A classic postulate of psychophysics is that some stimuli or differences between stimuli never manage to affect the central decision making centers; others, of course, do. In a phrase, peripheral thresholds were assumed to exist. At least three types have been distinguished: absolute, difference, and detection. It is not, however, clear that there is any real difference among them. Absolute thresholds seem to be the same as detection ones except that the only noise is internal, and many difference threshold experiments differ from de-tection experiments only in the nature of the background stimulus, e.g., a pure tone or noise. Recently the literal interpretation of the threshold postulate has been
ID|Title|Summary
1|Contention-Based Performance Evaluation of Multidimensional Range Search in Peer-to-peer Networks ABSTRACT |Performance evaluation of peer-to-peer search techniques has been based on simple performance metrics, such as message hop counts and total network traffic, mostly disregarding their inherent concurrent nature, where contention may arise. This paper is concerned with the effect of contention in complex P2P network search, focusing on techniques for multidimensional range search. We evaluate peerto-peer networks derived from recently proposed works, introducing two novel metrics related to concurrency and contention, namely responsiveness and throughput. Our results highlight the impact of contention on these networks, and demonstrate that some studied networks do not scale in the presence of contention. Also, our results indicate that certain network properties believed to be desirable (e.g. uniform data distribution or peer accesses) may not be as critical as previously believed.
2|Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications|A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.
3|A Scalable Content-Addressable Network|Hash tables – which map “keys ” onto “values” – are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internet-like scales. The CAN is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation. 
4|Pastry: Scalable, distributed object location and routing for large-scale peer-to-peer systems|This paper presents the design and evaluation of Pastry, a scalable, distributed object location and routing scheme for wide-area peer-to-peer applications. Pastry provides application-level routing and object location in a potentially very large overlay network of nodes connected via the Internet. It can be used to support a wide range of peer-to-peer applications like global data storage, global data sharing, and naming. An insert operation in Pastry stores an object at a user-defined number of diverse nodes within the Pastry network. A lookup operation reliably retrieves a copy of the requested object if one exists. Moreover, a lookup is usually routed to the node nearest the client issuing the lookup (by some measure of proximity), among the nodes storing the requested object. Pastry is completely decentralized, scalable, and self-configuring; it automatically adapts to the arrival, departure and failure of nodes. Experimental results obtained with a prototype implementation on a simulated network of 100,000 nodes confirm Pastry&#039;s scalability, its ability to self-configure and adapt to node failures, and its good network locality properties.
5|Kademlia: A Peer-to-peer Information System Based on the XOR Metric|We describe a peer-to-peer system which has provable consistency and performance in a fault-prone environment. Our system routes queries and locates nodes using a novel XOR-based metric topology that simplifies the algorithm and facilitates our proof. The topology has the property that every message exchanged conveys or reinforces useful contact information. The system exploits this information to send parallel, asynchronous query messages that tolerate node failures without imposing timeout delays on users.
6|Querying the Internet with PIER|The database research community prides itself on scalable technologies. Yet database systems traditionally do not excel on one important scalability dimension: the degree of distribution. This limitation has hampered the impact of database technologies on massively distributed systems like the Internet. In this paper, we present the initial design of PIER, a massively distributed query engine based on overlay networks, which is intended to bring database query processing facilities to new, widely distributed environments. We motivate the need for massively distributed queries, and argue for a relaxation of certain traditional database research goals in the pursuit of scalability and widespread adoption. We present simulation results showing PIER gracefully running relational queries across thousands of machines, and show results from the same software base in actual deployment on a large experimental cluster.
7|Mercury: Supporting scalable multi-attribute range queries|This paper presents the design of Mercury, a scalable protocol for supporting multi-attribute rangebased searches. Mercury differs from previous range-based query systems in that it supports multiple attributes as well as performs explicit load balancing. Efficient routing and load balancing are implemented using novel light-weight sampling mechanisms for uniformly sampling random nodes in a highly dynamic overlay network. Our evaluation shows that Mercury is able to achieve its goals of logarithmic-hop routing and near-uniform load balancing. We also show that a publish-subscribe system based on the Mercury protocol can be used to construct a distributed object repository providing efficient and scalable object lookups and updates. By providing applications a range-based query language to express their subscriptions to object updates, Mercury considerably simplifies distributed state management. Our experience with the design and implementation of a simple distributed multiplayer game built on top of this object management framework shows that indicates that this indeed is a useful building block for distributed applications. Keywords: Range queries, Peer-to-peer systems, Distributed applications, Multiplayer games 1
8|Skip Graphs|Skip graphs are a novel distributed data structure, based on skip lists, that provide the full functionality of a balanced tree in a distributed system where resources are stored in separate nodes that may fail at any time. They are designed for use in searching peer-to-peer systems, and by providing the ability to perform queries based on key ordering, they improve on existing search tools that provide only hash table functionality. Unlike skip lists or other tree data structures, skip graphs are highly resilient, tolerating a large fraction of failed nodes without losing connectivity. In addition, constructing, inserting new nodes into, searching a skip graph, and detecting and repairing errors in the data structure introduced by node failures can be done using simple and straightforward algorithms. 1
9|Analysis of the Evolution of Peer-to-Peer Systems|In this paper, we give a theoretical analysis of peer-to-peer (P2P) networks operating in the face of concurrent joins and unexpected departures. We focus on Chord, a recently developed P2P system that implements a distributed hash table abstraction, and study the process by which Chord maintains its distributed state as nodes join and leave the system. We argue that traditional performance measures based on run-time are uninformative for a continually running P2P network, and that the rate at which nodes in the network need to participate to maintain system state is a more useful metric. We give a general lower bound on this rate for a network to remain connected, and prove that an appropriately modified version of Chord&#039;s maintenance rate is within a logarithmic factor of the optimum rate. 1. 
10|Hilbert R-tree: An Improved R-tree Using Fractals|We propose a new R-tree structure that outperforms all the older ones. The heart of the idea is to facilitate the deferred splitting approach in R-trees. This is done by proposing an ordering on the R-tree nodes. This ordering has to be &#039;good&#039;, in the sense that it should group &#039;similar &#039; data rectangles together, to minimize the area and perimeter of the resulting minimum bounding rectangles (MBRs). Following [19] we have chosen the so-called &#039;2D-c &#039; method, which sorts rectangles according to the Hilbert value of the center of the rectangles. Given the ordering, every node has a well-de ned set of sibling nodes; thus, we can use deferred splitting. By adjusting the split policy, the Hilbert R-tree can achieve as high utilization as desired. To the contrary, the R-tree has no control over the space utilization, typically achieving up to 70%. We designed the manipulation algorithms in detail, and we did a full implementation of the Hilbert R-tree. Our experiments show that the &#039;2-to-3 &#039; split policy provides a compromise between the insertion complexity and the search cost, giving up to 28 % savings over the R  tree [3] on real data. 1
11|Simple Efficient Load Balancing algorithms for Peer-to-Peer Systems|Load balancing is a critical issue for the efficient operation of peer-to-peer networks. We give two new load-balancing protocols whose provable performance guarantees are within a constant factor of optimal. Our protocols refine the consistent hashing data structure that underlies the Chord (and Koorde) P2P network. Both preserve Chord’s logarithmic query time and near-optimal data migration cost. Consistent hashing is an instance of the distributed hash table (DHT) paradigm for assigning items to nodes in a peer-to-peer system: items and nodes are mapped to a common address space, and nodes have to store all items residing closeby in the address space. Our first protocol balances the distribution of the key address space to nodes, which yields a load-balanced system when the DHT maps items “randomly” into the address space. To our knowledge, this yields the first P2P scheme simultaneously achieving O(log n) degree, O(log n) look-up cost, and constant-factor load balance (previous schemes settled for any two of the three). Our second protocol aims to directly balance the distribution of items among the nodes. This is useful when the distribution of items in the address space cannot be randomized. We give a simple protocol that balances load by moving nodes to arbitrary locations “where they are needed.” As an application, we use the last protocol to give an optimal implementation of a distributed data structure for range searches on ordered data.
12|P-Grid: A Self-organizing Structured P2P System|this paper was supported in part by  the National Competence Center in Research on Mobile  Information and Communication Systems (NCCR-MICS), a  center supported by the Swiss National Science Foundation  under grant number 5005-67322 and by SNSF grant 2100064994,  &#034;Peer-to-Peer Information Systems.&#034;  messages. From the responses it (randomly) selects certain peers to which direct network links are established
13|BATON: A Balanced Tree Structure for Peer-to-Peer Networks|We propose a balanced tree structure overlay on a peer-to-peer network capable of supporting both exact queries and range queries efficiently. In spite of the tree structure causing distinctions to be made between nodes at different levels in the tree, we show that the load at each node is approximately equal. In spite of the tree structure providing precisely one path between any pair of nodes, we show that sideways routing tables maintained at each node provide sufficient fault tolerance to permit efficient repair. Specifically, in a network with N nodes, we guarantee that both exact queries and range queries can be answered in O(logN) steps and also that update operations (to both data and network) have an amortized cost of O(logN). An experimental assessment validates the practicality of our proposal. 1
14|Modeling Peer-Peer File Sharing Systems|Peer-peer networking has recently emerged as a new paradigm for building distributed networked applications. In this paper we develop simple mathematical models to explore and illustrate fundamental performance issues of peer-peer file sharing systems. The modeling framework introduced and the corresponding solution method are flexible enough to accommodate different characteristics of such systems. Through the specification of model parameters, we apply our framework to three different peer-peer architectures: centralized indexing, distributed indexing with flooded queries, and distributed indexing with hashing directed queries. Using our model, we investigate the effects of system scaling, freeloaders, file popularity and availability on system performance. In particular, we observe that a system with distributed indexing and flooded queries cannot exploit the full capacity of peer-peer systems. We further show that peer-peer file sharing systems can tolerate a significant number of freeloaders without suffering much performance degradation. In many cases, freeloaders can benefit from the available spare capacity of peer-peer systems and increase overall system throughput. Our work shows that simple models coupled with efficient solution methods can be used to understand and answer questions related to the performance of peer-peer file sharing systems.
15|Approximate Range Selection Queries in Peer-to-Peer|We present an architecture for a data sharing  peer-to-peer system where the data is shared  in the form of database relations. In general,  peer-to-peer systems try to locate exactmatch  data objects to simple user queries.
16|One torus to rule them all: Multi-dimensional queries in p2p systems|Peer-to-peer systems enable access to data spread over an extremely large number of machines. Most P2P systems support only simple lookup queries. However, many new applications, such as P2P photo sharing and massively multiplayer games, would benefit greatly from support for multidimensional range queries. We show how such queries may be supported in a P2P system by adapting traditional spatialdatabase technologies with novel P2P routing networks and load-balancing algorithms. We show how to adapt two popular spatial-database solutions – kd-trees and space-filling curves – and experimentally compare their effectiveness. 1.
17|Querying peer-to-peer networks using p-trees|Peer-to-peer (P2P) systems provide a robust, scalable and decentralized way to share and publish data. However, most existing P2P systems only provide a very rudimentary query facility; they only support equality or keyword search queries over files. We believe that future P2P applications, such as resource discovery on a grid, will require more complex query functionality. As a first step towards this goal, we propose a new distributed, fault-tolerant P2P index structure for resource discovery applications called the P-tree. Ptrees efficiently evaluate range queries in addition to equality queries. We describe algorithms to maintain a P-tree under insertions and deletions of data items/peers, and evaluate its performance using both a simulation and a real distributed implementation. Our results show the efficacy of our approach. 1.
18|A casestudy in building layered DHT applications|Recent research has shown that one can use Distributed Hash Tables (DHTs) to build scalable, robust and efficient applications. One question that is often left unanswered is that of simplicity of implementation and deployment. In this paper, we explore a case study of building an application for which ease of deployment dominated the need for high performance. The application we focus on is Place Lab, an end-user positioning system. We evaluate whether it is feasible to use DHTs as an application-independent building block to implement a key component of Place Lab: its “mapping infrastructure.” We present Prefix Hash Trees, a data structure used by Place Lab for geographic range queries that is built entire on top of a standard DHT. By strictly layering Place Lab’s data structures on top of a generic DHT service, we were able to decouple the deployment and management of Place Lab from that of the underlying DHT. We identify the characteristics of Place Lab that made it amenable for deploying in this layered manner, and comment on its effect on performance.
19|Distributed segment tree: Support of range query and cover query over dht|Range query, which is defined as to find all the keys in a certain range over the underlying P2P network, has received a lot of research attentions recently. However, cover query, which is to find all the ranges currently in the system that cover a given key, is rarely touched. In this paper, we first identify that cover query is a highly desired functionality by some popular P2P applications, and then propose distributed segment tree (DST), a layered DHT structure that incorporates the concept of segment tree. Due to the intrinsic capability of segment tree in maintaining the sturcture of ranges, DST is shown to be very efficient for supporting both range query and cover query in a uniform way. It also possesses excellent parallelizability in query operations and can achieve O(1) complexity for moderate query ranges. To balance the load among DHT nodes, we design a downward load stripping mechanism that controls tradeoffs between load and performance. We implemented DST on publicly available OpenDHT service and performed extensive real experiments. All the results and comparisons demonstrate the effectiveness of DST for several important metrics. 1.
20|Vbi-tree: A peer-to-peer framework for supporting multi-dimensional indexing schemes|Multi-dimensional data indexing has received much attention in a centralized database. However, not so much work has been done on this topic in the context of Peerto-Peer systems. In this paper, we propose a new Peer-to-Peer framework based on a balanced tree structure overlay, which can support extensible centralized mapping methods and query processing based on a variety of multidimensional tree structures, including R-Tree, X-Tree, SS-Tree, and M-Tree. Specifically, in a network with N nodes, our framework guarantees that point queries and range queries can be answered within O(logN) hops. We also provide an effective load balancing strategy to allow nodes to balance their work load efficiently. An experimental assessment validates the practicality of our proposal. 1.
21|Range Queries in Trie-Structured Overlays|Among the open problems in P2P systems, support for non-trivial search predicates, standardized query languages, distributed query processing, query load balancing, and quality of query results have been identified as some of the most relevant issues. This paper describes how range queries as an important non-trivial search predicate can be supported in a structured overlay network that provides O(log n) search complexity on top of a trie abstraction. We provide analytical results that show that the proposed approach is efficient, supports arbitrary granularity of ranges, and demonstrate that its algorithmic complexity in terms of messages is independent of the size of the queried ranges and only depends on the size of the result set. In contrast to other systems which provide evaluation results only through simulations, we validate the theoretical analysis of the algorithms with large-scale experiments on the PlanetLab infrastructure using a fully-fledged implementation of our approach.  
22|On Scaling Latent Semantic Indexing for Large Peer-To-Peer Systems|The exponential growth of data demands scalable infrastructures capable of indexing and searching rich content such as text, music, and images. A promising direction is to combine information retrieval with peer-to-peer technology for scalability, fault-tolerance, and low administration cost. One pioneering work along this direction is pSearch [32, 33]. pSearch places documents onto a peerto -peer overlay network according to semantic vectors produced using Latent Semantic Indexing (LSI). The search cost for a query is reduced since documents related to the query are likely to be co-located on a small number of nodes. Unfortunately, because of its reliance on LSI, pSearch also inherits the limitations of LSI. (1) When the corpus is large and heterogeneous, LSI&#039;s retrieval quality is inferior to methods such as Okapi. (2) The Singular Value Decomposition (SVD) used in LSI is unscalable in terms of both memory consumption and computation time.
23|Analysis and Comparison of P2P Search Methods |The popularity and bandwidth consumption attributed to current Peer-to-Peer file-sharing applications makes  the operation of these distributed systems very important for the Internet community. Efficient object discovery is  the first step towards the realization of distributed resource-sharing. In this work, we present a detailed overview  of recent and existing search methods for unstructured Peer-to-Peer networks. We analyze the performance of the  algorithms relative to various metrics, giving emphasis on the success rate, bandwidth-efficiency and adaptation to  dynamic network conditions. Simulation results are used to empirically evaluate the behavior of nine representative  schemes under a variety of different environments.
24|Answering similarity queries in peer-to-peer networks. http://www.comp.nus.edu.sg/-kalnis/ASQ.pdf |www.comp.nus.edu.sg/~{kalnis, ngws, ooibc, tankl}
25|Skip-webs: Efficient distributed data structures for multi-dimensional data sets|large(at)daimi.au.dk eppstein(at)ics.uci.edu goodrich(at)acm.org We present a framework for designing efficient distributed data structures for multi-dimensional data. Our structures, which we call skip-webs, extend and improve previous randomized distributed data structures, including skipnets and skip graphs. Our framework applies to a general class of data querying scenarios, which include linear (one-dimensional) data, such as sorted sets, as well as multi-dimensional data, such as d-dimensional octrees and digital tries of character strings defined over a fixed alphabet. We show how to perform a query over such a set of n items spread among n hosts using O(log n/log log n) messages for one-dimensional data, or O(log n) messages for fixed-dimensional data, while using only O(log n) space per host. We also show how to make such structures dynamic so as to allow for insertions and deletions in O(log n) messages for quadtrees, octrees, and digital tries, and O(log n/log log n) messages for onedimensional data. Finally, we show how to apply a blocking strategy to skip-webs to further improve message complexity for one-dimensional data when hosts can store more data.
26|Real datasets for file-sharing peer-to-peer systems|Abstract. The fundamental drawback of unstructured peer-to-peer (P2P) networks is the flooding-based query processing protocol that seriously limits their scalability. As a result, a significant amount of research work has focused on designing efficient search protocols that reduce the overall communication cost. What is lacking, however, is the availability of real data, regarding the exact content of users ’ libraries and the queries that these users ask. Using trace-driven simulations will clearly generate more meaningful results and further illustrate the efficiency of a generic query processing protocol under a real-life scenario. Motivated by this fact, we developed a Gnutella-style probe and collected detailed data over a period of two months. They involve around 4,500 users and contain the exact files shared by each user, together with any available metadata (e.g., artist for songs) and information about the nodes (e.g., connection speed). We also collected the queries initiated by these users. After filtering, the data were organized in XML format and are available to researchers. Here, we analyze this dataset and present its statistical characteristics. Additionally, as a case study, we employ it to evaluate two recently proposed P2P searching techniques. 1
27|Multidimensional Access Methods|Search operations in databases require special support at the physical level. This is true for conventional databases as well as spatial databases, where typical search operations include the point query (find all objects that contain a given search point) and the region query (find all objects that overlap a given search region).
28|The ubiquitous B-tree|B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and nnplemented using B-trees This paper reviews B-trees and shows why they have been so successful It discusses the major variations of the B-tree, especially the B+-tree,
29|On visible surface generation by a priori tree structures|This paper describes a new algorithm for solving the hidden surface (or line) problem, to more rapidly generate realistic images of 3-D scenes composed of polygons, and presents the development of theoretical foundations in the area as well as additional related algorithms. As in many applications the environment to be displayed consists of polygons many of whose relative geometric relations are static, we attempt to capitalize on this by pre processing tile environment,s database so as to decrease the run-time computations required to generate a scene. This preprocessing is based on generating a &amp;quot;ninary space partitioning &amp;quot; tree whose inorder traversal of visibility priority at run-time will produce a lineaL &amp;quot; order, dependent upon the viewing position, on (parts of) the polygons, which can then be used to easily solve the hidden surfac6 problem. In the application where the entire environment is static with only the viewing-position changing, as is common in simulation, the results presented will be safficient to solve completely tlae llidden surface proulem.;_N~a~OUCZZON One of the long-term goals of computer graphics has been, and continues to be, the rapid, possibly real-time generation of £ealistic images of simulated 3-D environments. &amp;quot;Real-time,&amp;quot; in current practice, has come to mean creating an image in 1/30 of a second--fast enough to continually generate images on a video monitor. With this fast image generation, there is no aiscernable delay between specifying parameters zor an image (using knobs, switches, or cockpit controls) and the *This research was partially supported by NSF under Grants MCS79-00168 and MC579-02593, and was zacilitated by the use of
30|Efficient Processing of Spatial Joins Using R-Trees|Abstract: In this paper, we show that spatial joins are very suitable to be processed on a parallel hardware platform. The parallel system is equipped with a so-called shared virtual memory which is well-suited for the design and implementation of parallel spatial join algorithms. We start with an algorithm that consists of three phases: task creation, task assignment and parallel task execu-tion. In order to reduce CPU- and I/O-cost, the three phases are processed in a fashion that pre-serves spatial locality. Dynamic load balancing is achieved by splitting tasks into smaller ones and reassigning some of the smaller tasks to idle processors. In an experimental performance compar-ison, we identify the advantages and disadvantages of several variants of our algorithm. The most efficient one shows an almost optimal speed-up under the assumption that the number of disks is sufficiently large. Topics: spatial database systems, parallel database systems 1
32|Generalized Search Trees for Database Systems|This paper introduces the Generalized Search Tree (GiST), an index structure supporting an extensible set of queries and data types. The GiST allows new data types to be indexed in a manner supporting queries natural to the types; this is in contrast to previous work on tree extensibility which only supported the traditional set of equality and range predicates. In a single data structure, the GiST provides all the basic search tree logic required by a database system, thereby unifying disparate structures such as B+-trees and R-trees in a single piece of code, and opening the application of search trees to general extensibility. To illustrate the exibility of the GiST, we provide simple method implementations that allow it to behave like a B+-tree, an R-tree, and an RD-tree, a new index for data with set-valued attributes. We also present a preliminary performance analysis of RD-trees, which leads to discussion on the nature of tree indices and how they behave for various datasets.  
33|Spatial SQL: A Query and Presentation Language|attention has been focused on spatial databases which combine conventional and spatially related data such as Geographic Information Systems, CAD/CAM, or VLSI. A language has been developed to query such spatial databases. It recognizes the significantly different requirements of spatial data handling and overcomes the inherent problems of the application of conventional database query languages. The spatial query language has been designed as a minimal extension to the interrogative part of SQL and distinguishes from previously designed SQL extensions by (1) the preservation of SQL concepts, (2) the highlevel treatment of spatial objects, and (3) the incorporation of spatial operations and relationships. It consists of two components, a query language to describe what information to retrieve and a presentation language to specify how to display query results. Users can ask standard SQL queries to retrieve non-spatial data based on non-spatial constraints, use Spatial SQL commands to inquire about situations involving spatial data, and give instructions in the Graphical Presentation Language GPL to manipulate or examine the graphical presentation. 1 Index Terms—Geographic Information Systems, graphical presentation, query
34|Beyond uniformity and independence: Analysis of r-trees using the concept of fractal dimension|We propose the concept of fractal dimension of a set of points, in order to quantify the deviation from the uniformity distribution. Using measurements on real data sets (road intersections of U.S. counties, star coordinates from NASA’s Infrared-Ultraviolet Explorer etc.) we provide evidence that real data indeed are skewed, and, moreover, we show that they behave as mathematical fractals, with a measurable, non-integer fract al dimension. Armed with this tool, we then show its practical use in predicting the performance of spatial access methods, and specifically of the R-trees. We provide the jirst analysis of R-trees for skewed distributions of points: We develop a formula that estimates the number of disk accesses for range queries, given only the fractal dimension of the point set, and its count. Experiments on real data sets show that the formula is very accurate: the relative error is usually below 5%, and it rarely exceeds 10%. We believe that the fractal dimension will help replace the uniformity and independence assumptions, allowing more accurate analysis for any spatial access method, as well as better estimates for query optimization on multi-attribute queries. 1
35|Fractals for Secondary Key Retrieval |In this paper we propose the use of fractals and especially the Hilbert curve, in order to design good distance-preserving mappings. Such mappings improve the performance of secondary-key- and spatial- access methods, where multi-dimensional points have to be stored on an 1-dimensional medium (e.g., disk). Good clustering reduces the number of disk accesses on retrieval, improving the response time. Our experiments on range queries and nearest neighbor queries showed that the proposed Hilbert curve achieves better clustering than older methods (&amp;quot;bit-shuffling&amp;quot;, or Peano curve), for every situation we tried.
36|Multi-Step Processing of Spatial Joins   |Spatial joins are one of the most importaot operations for combining spatial objects of several relations. IO this paper, spatial join processing is studied in detail for extended spatial objects in two-dimensional data space. We present an approach for spatial join processing that is based on three steps. First, a spatial join is performed on the minimum bounding rectangles of the objects returning a set of candidates. Various approaches for accelerating this step of join processing have been examined at the last year’s conference [BKS 93a]. In this paper, we focus on the problem how to compute the answers from the set of candidates which is handled by the foliowing two steps. First of all, sophisticated approximations are used to identify answers as well as to filter out false hits from the set of candidates. For this purpose, we investigate various types of conservative and progressive approximations. In the last step, the exact geometry of the remaioing candidates has to be tested against the join predicate. The time required for computing spatial joio predicates can essentially be reduced when objects are adequately organized in main memory. IO our approach, objects are fiist decomposed into simple components which are exclusively organized by a main-memory resident spatial data structure. Overall, we present a complete approach of spatial join processing on complex spatial objects. The performance of the individual steps of our approach is evaluated with data sets from real cartographic applications. The results show that our approach reduces the total execution time of the spatial join by factors.  
37|Estimating the Selectivity of Spatial Queries Using the `Correlation&#039; Fractal Dimension|We examine the estimation of selectivities for range and spatial join queries in real spatial databases. As we have shown earlier [FK94a], real point sets: (a) violate consistently the &#034;uniformity&#034; and &#034;independence&#034; assumptions, (b) can often be described as &#034;fractals&#034;, with non-integer (fractal) dimension. In this paper we show that, among the infinite family of fractal dimensions, the so called &#034;Correlation Dimension&#034; D 2 is the one that we need to predict the selectivity of spatial join. The main contribution is that, for all the real and synthetic point-sets we tried, the average number of neighbors for a given point of the point-set follows a power law, with D 2 as the exponent. This immediately solves the selectivity estimation for spatial joins, as well as for &#034;biased&#034; range queries (i.e., queries whose centers prefer areas of high point density). We present the formulas to estimate the selectivity for the biased queries, including an integration constant (K `shape  0  ) for ea...
38|Indexing for data models with constraints and classes|We examine I O-efficient data structures that provide indexing support for new data models. The database languages of these models include concepts from constraint programming (e.g., relational tuples are generated to conjunctions of constraints) and from object-oriented programming (e.g., objects are organized in class hierarchies). Let n be the size of the database, c the number of classes, B the page size on secondary storage, and t the size of the output of a query: (1) Indexing by one attribute in many constraint data models is equivalent to external dynamic interval management, which is a special case of external dynamic two-dimensional range searching. We present a semi-dynamic data structure for this problem that has worst-case space O(n B) pages, query I O time O(logB n+t B) and O(logB n+(logB n) 2 B) amortized insert I O time. Note that, for the static version of this problem, this is the first worst-case optimal solution. (2) Indexing by one attribute and by class name in an object-oriented model, where objects are organized
39|The lsd tree: spatial access to multidimensional point and non-point objects|We propose the Local Split Decision tree (LSD tree, for short), a data structure supporting efficient spatial access to geometric objects. Its main advantages over other structures are that it performs well for all reasonable data distributions, cover quotients (which measure the overlapping of the data objects), and bucket capacities, and that it maintains multidimensional points as well as arbitrary geometric objects. These properties demonstrated by an extensive performance, evaluation make the LSD tree extremely suitable for the implementation of spatial access paths in geometric databases. The paging algorithm for the binary tree directory is interesting in its own right because a practical solution for the problem of how to page a (multidimensional) binary tree without access path degeneration is presented. 1.
40|Efficient Computation of Spatial Joins|Spatial joins are join operations that involve spatial data types and operators. Due to some basic properties of spatial data, many conventional join processing strategies suffer serious performance penalties or are not applicable at all in this case. In this paper we explore which of the join strategies known from conventional databases can be applied to spatial joins as well, and how some of these techniques can be modified to be more efficient in the context of spatial data. Furthermore, we describe a class of tree structures, called generalization trees, that can be applied efficiently to compute spatial joins in a hierarchical manner. Finally, we model the performance of the most promising strategies analytically and conduct a comparative study.  Parts of this work have been carried out while the author was visiting the International Computer Science Institute and the University of California at Berkeley.  1  1 Introduction  Spatial databases have become a very active research top...
41|Parallel R-trees|We consider the problem of exploiting parallelism to accelerate the performance of spatial access methods and specifically, R-trees [11]. Our goal is to design a server for spatial data, so that to maximize the throughput of range queries. This can be achieved by (a) maximizing parallelism for large range queries, and (b) by engaging as few disks as possible on point queries [22]. We propose a simple hardware architecture consisting of one processor with several disks attached to it. On this architecture, we propose to distribute the nodes of a traditional R-tree, with cross-disk pointers (`Multiplexed&#039; R-tree). The R-tree code is identical to the one for a single-disk R-tree, with the only addition that we have to decide which disk a newly created R-tree node should be stored in. We propose and examine several criteria to choose a disk for a new node. The most successful one, termed `proximity index&#039; or PI, estimates the similarity of the new node with the other R-tree nodes already o...
42|Comparison of approximations of complex objects used for approximation-based query processing in spatial database systems|The management of geometric objects is a prime example of an application where efficiency is the bottleneck; this bottleneck cannot be eliminated without using suitable access structures. The most popular approach for handling complex spatial objects in spatial access methods is to use their minimum bounding boxes as a geometric key. Obviously, the rough approximation by bounding boxes provides a fast but inaccurate filter for the set of answers to a query. In order to speed up the query processing by a better approximation quality, we investigate six different types of approximations. Depending on the complexity of the objects and the type of queries, the approximations 5-corner, ellipse and rotated bounding box clearly outperform the bounding box. An important ingredient of our approach is to organize these approximations in efficient spatial access
43|A Qualitative Comparison Study of Data Structures for Large Line Segment Databases|A qualitative comparative study is performed of the performance of three popular spatial indexing methods -- the r    -tree, r  +  -tree, and the pmr quadtree -- in the context of processing spatial queries in large line segment databases. The data is drawn from the tiger/Line files used by the Bureau of the Census to deal with the road networks in the US. The goal is not to find the best data structure as this is not generally possible. Instead, their comparability is demonstrated and an indication is given as to when and why their performance differs. Tests are conducted with a number of large datasets and performance is tabulated in terms of the complexity of the disk activity in building them, their storage requirements, and the complexity of the disk activity for a number of tasks that include point and window queries, as well as finding the nearest line segment to a given point and an enclosing polygon.  1 Introduction  Spatial data consists of points, lines, regions, rectangles,...
44|DOT: A spatial access method using fractals|Existing Database Management Systems (DBMSs) do not handle efficiently multi-dimensional data such as boxes, polygons, or even points in a multi-dimensional space. We examine access methods for these data with two design goals in mind: (a) efficiency in terms of search speed and space overhead and (b) ability to be integrated in a DBMS easily. We propose a method to map multidimensional objects into points in a 1-dimensional space; thus, traditional primary-key access methods can be applied, with very few extensions on the part of the DBMS. We propose such mappings based on fractals; we implemented the whole method on top of a B +-tree, along with several mappings. Simulation experiments on several distributions of the input data show
45|Benchmarking Spatial Join Operations with Spatial Output|The spatial join operation is benchmarked using variants of well-known spatial data structures such as the R-tree, R    -tree, R  +  -tree, and the PMR quadtree. The focus is on a spatial join with spatial output because the result of the spatial join frequently serves as input to subsequent spatial operations (i.e., a cascaded spatial join as would be common in a spatial spreadsheet). Thus, in addition to the time required to perform the spatial join itself (whose output is not always required to be spatial), the time to build the spatial data structure also plays an important role in the benchmark. The studied quantities are the time to build the data structure and the time to do the spatial join in an application domain consisting of planar line segment data. Experiments reveal that spatial data structures based on a disjoint decomposition of space and bounding boxes (i.e., the R  +  -tree and the PMR quadtree with bounding boxes) outperform the other structures that are based upon ...
46|Optimal Redundancy in Spatial Database Systems|In spatial database systems rectangles are commonly used to approximate real spatial data. A technique that approximates extended objects with a collection  of rectangles is the z-ordering method. Since each of these rectangles eventually corresponds to an entry in a spatial index, the object may be referenced several times. This redundancy effect is controllable. In this paper, we present an empirically derived formula to assess the expected redundancy for the z-ordering approximation technique given some simple parameters. After showing the applicability of this formula to a large class of different object geometries, we make use of this result to determine the optimal redundancy for real spatial data by means of theoretical considerations. In order to verify our theoretical results, we conducted several experiments using real spatial data and found a good correspondence. 1 Introduction  Spatial indexing or spatial access methods have found great attention in recent years, culminatin...
47|An analysis of geometric modeling in database systems|The data-modeling and computational requirements for integrated computer aided manufacturing (CAM) databases are analyzed, and the most common representation schemes for modeling solid geometric objects in a computer are
48|Realms: A Foundation for Spatial Data Types in Database Systems|Spatial data types or algebras for database systems should (i) be fully general (which means, closed under set operations, hence e.g. a region value can be a set of polygons with holes), (ii) have formally defined semantics, (iii) be defined in terms of finite representations available in computers, (iv) offer facilities to enforce geometric consistency of related spatial objects, and (v) be independent of a particular DBMS data model, but cooperate with any. We offer such a definition in two papers. The central idea, introduced in this (first) paper, is to use realms as geometric domains underlying spatial data types. A realm as a general database concept is a finite, dynamic, user-defined structure underlying one or more system data types. A geometric realm defined here is a planar graph over a finite resolution grid. Problems of numerical robustness and topological correctness are solved below and within the realm layer so that spatial algebras defined above a realm enjoy very nice algebraic properties. Realms also interact with a DBMS to enforce geometric consistency on object creation or update.
49|Benchmarking Spatial Joins A La Carte|Spatial joins are join operations that involve spatial data types and operators. Spatial access methods are often used to speed up the computation of spatial joins. This paper addresses the issue of benchmarking spatial join operations. For this purpose, we first present a WWW-based benchmark generator to produce sets of rectangles. Using a Web browser, experimenters can specify the number of rectangles in a sample, as well as the statistical distributions of their sizes, shapes, and locations. Second, using the generator and a well-defined set of statistical models we define several tests to compare the performance of three spatial join algorithms: nested loop, scan-and-index, and synchronized tree traversal. We also added a real-life data set from the Sequoia 2000 storage benchmark. Our results show that the relative performance of the different techniques mainly depends on two parameters: sample size, and selectivity of the join predicate. All of the statistical models and algorithms are available on the Web, which allows for easy verification and modification of our experiments.
50|Analysis of n-dimensional Quadtrees Using the Hausdorff Fractal Dimension|There is mounting evidence [Man77, Sch91] that real datasets are statistically self-similar, and thus, `fractal&#039;. This is an important insight since it permits a compact statistical description of spatial datasets; subsequently, as we show, it also forms the basis for the theoretical analysis of spatial access methods, without using the typical, but unrealistic, uniformity assumption. In this paper, we focus on the estimation of the number of quadtree blocks that a real, spatial dataset will require. Using the the well-known Hausdorff fractal dimension, we derive some closed formulas which allow us to predict the number of quadtree blocks, given some few parameters. Using our formulas, it is possible to predict the space overhead and the response time of linear quadtrees/z-ordering [OM88], which are widely used in practice. In order to verify our analytical model, we performed an extensive experimental investigation using several real datasets coming from different domains. In these ex...
51|Enclosing Many Boxes By an Optimal Pair of Boxes|We look at the problem: Given a set  M  of  n d-dimensional  intervals, find two  d-  dimensional intervals  S, T  , such that all intervals in  M  are enclosed by  S  or by  T  , the  distribution is balanced and the intervals  S  and  T  fulfill a geometric criterion, e.g. like minimum  area sum. Up to now no polynomial time algorithm was known for that problem. We  present an  O(dn  log  n  +  d 2 n 2d\Gamma1 ) algorithm for finding an optimal solution.  1 Introduction  Throughout the years, several fast heuristics have been proposed for a combinatorial optimization  problem that is important in the area of spatial data structures. Given a set of (axis-parallel)  rectangles in the plane, the problem is to find two rectangles, say S and T , such that each given  rectangle is enclosed by S or by T (or both), each of S and T enclose at least a certain number  of given rectangles, and S and T together minimize some measure, e.g. the sum of their areas.  This problem must be solved whene...
52|Separability of Polyhedra for Optimal Filtering of Spatial and Constraint Data|The filtering method considered in this paper is based on approximation of a  spatial object in d-dimensional space by the minimal convex polyhedron that encloses  the object and whose facets are normal to preselected axes. These axes are  not necessarily the standard coordinate axes and, furthermore, their number is not  determined by the dimension of the space. We optimize filtering by selecting optimal  such axes based on a pre-processing analysis of stored objects or a sample thereof.  The number of axes selected represents a trade-off between access time and storage  overhead, as more axes usually lead to better filtering but require more overhead to  store the associated access structures. We address the problem of minimizing the  number of axes required to achieve a predefined quality of filtering and the reverse  problem of optimizing the quality of filtering when the number of axes is fixed. In  both cases we also show how to find an optimal collection of axes. In order to sol...
53|Spatial Access Methods and Query Processing in the Object-Oriented GIS GODOT|In this paper, we describe the spatial access method z-ordering and its application in the context of the research project GODOT, which is based on the commercial object-oriented database system ObjectStore [LLOW91]. After identifying a range of spatial predicates, we show that the intersection join is of crucial importance for spatial joins. Next, we propose an efficient method for query processing, which takes advantage of z-ordering and uses the conventional indexing mechanisms offered in current database systems (e.g., relational and object-oriented).
54|Extending a spatial access structure to support additional standard attributes| In recent years, many access structures have been proposed supporting access to objects via their spatial location. However, additional non-geometric properties are always associated with geometric objects, and in practice it is often necessary to use select conditions based on spatial and standard attributes. An obvious idea to improve the performance of queries with mixed select conditions is to extend spatial access structures with additional dimensions for standard attributes. Whereas this idea seems to be simple and promising at rst glance, a closer look brings up serious problems, especially with select conditions containing arithmetic expressions or select conditions for non-point objects and with Boolean operators like or and not. In this paper we present a solution to overcome the problems sketched above which is based on three pillars: (1) We present powerful basic techniques to deal with arithmetic conditions containing mathematical operations (like `+&#039;, `;&#039;,  ` &#039;, and `=&#039;) and range queries for non-point objects. (2) We introduce a technique which allows to decompose select conditions containing Boolean operators and to reduce the processing of such a select condition to the processing of its elementary parts. (3) We showhow other operations like joins and distance-scans can be integrated into this query processing architecture.  
55|Geometric Information Makes Spatial Query Processing More Efficient|In order to index complex and heterogeneous cartographic data by means of spatial access methods, one typically uses single, simple geometries to approximate the given geometries. Traditionally, no information regarding the quality of the approximation is stored. In this paper, we show that the availability of such information allows us to decide at an early stage that numerous objects fulfill the spatial predicate. This leads to significant performance improvements for the spatial selection as well as for the spatial intersection join. We present two techniques enabling such an early decision for the z-ordering method, and we demonstrate their efficiency by means of experiments.  1 Introduction  Fast access to spatial data stored in databases is essential for answering spatial queries efficiently. As a result, numerous spatial access methods have been proposed and studied in the past. The complexity and heterogeneity of spatial data makes it impossible to store cartographic data direc...
56|On the complexity of BV-tree updates|In [Fre95b] we discussed the use of multi-dimensional index methods in constraint databases. In [Fre95a] we showed how to overcome a fundamental problem which has afflicted all multi-dimensional indexing techniques based on the recursive partitioning of a dataspace: how to build the partition hierarchy so that the location of each object in the
57|Oversize Shelves: A Storage Management Technique for Large Spatial Data Objects|In this paper we present a new technique to improve the performance of spatial access methods by minimizing redundancy: the oversize shelf . Oversize shelves are additional disk pages that are attached to the interior nodes of a tree--based spatial access method (such as the R  +  --tree or the cell tree). These pages are used to accommodate very large data objects in order to avoid their excessive fragmentation. Whenever inserting a new object into the tree, one now has to decide whether to store it on an oversize shelf or insert it into the corresponding subtrees. For this purpose, we developed an analytic model for the behavior of dynamic spatial access methods under insertions and deletions. The model yields a threshold value for the size of an object, such that it is more favorable to put it on the oversize shelf if and only if its size is greater than the threshold value. Otherwise the insertion into the corresponding subtrees is preferable. Practical experiments indicate that th...
58|Adapting the Transformation Technique to Maintain Multi-Dimensional Non-Point Objects in k-d-Tree Based Access Structures|In [10, 18] the transformation technique has been proposed to store k-dimensional intervals -- which serve as bounding boxes for arbitrary geometric objects in many applications -- as 2k-dimensional points in a point access structure. Unfortunately the transformation technique has two pitfalls: (1) The transformation leads to a skew distribution of the 2k-dimensional image points. (2) Processing a range query searching all objects intersecting a given query region, there is a mismatch between thek-dimensional query region and the 2k-dimensional access structure. In this paper we propose two techniques to overcome these problems which can be directly applied tok-d-tree based point access structures: (1) We present a sophisticated split strategy to determine the split dimension and the split position in case of a bucket split which exploits the knowledge about the distribution of the image points of the transformation technique to gain an extremely exible and robust access structure. (2) We propose a re-transformation of the 2k-dimensional data regions in the access structure into the originalk-dimensional data space in order to compare these regions with thek-dimensional query region. Furthermore we state experimental results, which demonstrate, that the presented techniques allow to maintaink-dimensional non-point objects with nearly the same performance as k-dimensional point objects. 
59|The particel swarm: Explosion, stability, and convergence in a multi-dimensional complex space  |The particle swarm is an algorithm for finding optimal regions of complex search spaces through interaction of individuals in a population of particles. Though the algorithm, which is based on a metaphor of social interaction, has been shown to perform well, researchers have not adequately explained how it works. Further, traditional versions of the algorithm have had some dynamical properties that were not considered to be desirable, notably the particles’ velocities needed to be limited in order to control their trajectories. The present paper analyzes the particle’s trajectory as it moves in discrete time (the algebraic view), then progresses to the view of it in continuous time (the analytical view). A 5-dimensional depiction is developed, which completely describes the system. These analyses lead to a generalized model of the algorithm, containing a set of coefficients to control the system’s convergence tendencies. Some results of the particle swarm optimizer, implementing modifications derived from the analysis, suggest methods for altering the original algorithm in ways that eliminate problems and increase the optimization power of the particle swarm
60|Particle swarm optimization| A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described.
61|Comparing inertia weights and constriction factors in particle swarm optimization|The performance of particle swarm optimization using an inertia weight is compared with performance using a constriction factor. Five benchmark functions are used for the comparison. It is concluded that the best approach is to use the constriction factor while limiting the maximum velocity Vmax to the dynamic range of the variable Xmax on each dimension. This approach provides performance on the benchmark functions superior to any other published results known by the authors.
62|Efficient similarity search in sequence databases|We propose an indexing method for time sequences for processing similarity queries. We use the Discrete Fourier Transform (DFT) to map time sequences to the frequency domain, the crucial observation being that, for most sequences of practical interest, only the first few frequencies are strong. Another important observation is Parseval&#039;s theorem, which specifies that the Fourier transform preserves the Euclidean distance in the time or frequency domain. Having thus mapped sequences to a lower-dimensionality space by using only the first few Fourier coe cients, we use R-trees to index the sequences and e ciently answer similarity queries. We provide experimental results which show that our method is superior to search based on sequential scanning. Our experiments show that a few coefficients (1-3) are adequate to provide good performance. The performance gain of our method increases with the number and length of sequences. 
63|A Survey of Image Registration Techniques|Registration is a fundamental task in image processing used to  match two or more pictures taken, for example, at different times,  from different sensors or from different viewpoints. Over the years, a  broad range of techniques have been developed for the various types of  data and problems. These techniques have been independently studied  for several different applications resulting in a large body of research.  This paper organizes this material by establishing the relationship  between the distortions in the image and the type of registration techniques  which are most suitable. Two major types of distortions are  distinguished. The first type are those which are the source of misregistration,  i.e., they are the cause of the misalignment between the two  images. Distortions which are the source of misregistration determine  the transformation class which will optimally align the two images.  The transformation class in turn influences the general technique that  should be taken....
64|Voronoi diagrams -- a survey of a fundamental geometric data structure|This paper presents a survey of the Voronoi diagram, one of the most fundamental data structures in computational geometry. It demonstrates the importance and usefulness of the Voronoi diagram in a wide variety of fields inside and outside computer science and surveys the history of its development. The paper puts particular emphasis on the unified exposition of its mathematical and algorithmic properties. Finally, the paper provides the first comprehensive bibliography on Voronoi diagrams and related structures.
65|Database Mining: A Performance Perspective|We present our perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology. We describe three classes of database mining problems involving classification, associations, and sequences, and argue that these problems can be uniformly viewed as requiring discovery of rules embedded in massive data. We describe a model and some basic operations for the process of rule discovery. We show how the database mining problems we consider map to this model and how they can be solved by using the basic operations we propose. We give an example of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm not only is efficient in discovering classification rules but also has accuracy comparable to ID3, one of the current best classifiers.  Index Terms. database mining, knowledge discovery, classification, associations, sequences, decision trees   Current address: Computer Science De...
66|Temporal databases|A temporal database (see Temporal Database) contains time-varying data. Time is an important aspect of all real-world phenomena. Events occur at specific points in time; objects and the relationships among objects exist over time. The ability to model this temporal dimension of the real world is essential to many computer applications, such as accounting, banking, econometrics, geographical information systems, inventory control, law, medical records, multi-media, process control, reservation systems, and scientific data analysis. Conventional databases represent the state of an enterprise at a single moment of time. Although the contents of the database continue to change as new information is added, these changes are viewed as modifications to the state, with the old, out-of-date data being deleted from the database. The current contents of the database may be viewed as a snapshot of the enterprise. When a conventional database is used, the attributes involving time are manipulated solely by the application programs, with little help
67|The hB-tree: A multiattribute indexing method with good guaranteed performance|A new multiattribute index structure called the hB-tree is introduced. It is derived from the K-D-B-tree of Robinson [15] but has additional desirable properties. The hB-tree internode search and growth processes are precisely analogous to the corresponding processes in B-trees [l]. The intranode processes are unique. A k-d tree is used as the structure within nodes for very efficient searching. Node splitting requires that this k-d tree be split. This produces nodes which no longer represent brick-like regions in k-space, but that can be characterized as holey bricks, bricks in which subregions have been extracted. We present results that guarantee hB-tree users decent storage utilization, reasonable size index terms, and good search and insert performance. These results guarantee that the hB-tree copes well with arbitrary distributions of keys.
68|Vague: a user interface to relational databases that permits vague queries|A specific query establishes a rigid qualification and is concerned only with data that match it precisely. A vague query establishes a target qualification and is concerned also with data that are close to this target. Most conventional database systems cannot handle vague queries directly, forcing their users to retry specific queries repeatedly with minor modifications until they match data that are satisfactory. This article describes a system called VAGUE that can handle vague queries directly. The principal concept behind VAGUE is its extension to the relational data model with data metrics, which are definitions of distances between values of the same domain. A problem with implementing data distances is that different users may have different interpretations for the notion of distance. VAGUE incorporates several features that enable it to adapt itself to the individual views and priorities of its users.
69|New Techniques for Best-Match Retrieval|A scheme to answer best-match queries from a file containing a collection of objects is described. A best-match query is to find the objects in the file that are closest (according to some (dis)similarity measure) to a given target. Previous work [5, 331 suggests that one can reduce the number of comparisons required to achieve the desired results using the triangle inequality, starting with a data structure for the file that reflects some precomputed intrafile distances. We generalize the technique to allow the optimum use of any given set of precomputed intrafile distances. Some empirical results are presented which illustrate the effectiveness of our scheme, and its performance relative to previous algorithms.
70|Tabu Search -- Part I|This paper presents the fundamental principles underlying tabu search as a strategy for combinatorial optimization problems. Tabu search has achieved impressive practical successes in applications ranging from scheduling and computer channel balancing to cluster analysis and space planning, and more recently has demonstrated its value in treating classical problems such as the traveling salesman and graph coloring problems. Nevertheless, the approach is still in its infancy, and a good deal remains to be discovered about its most effective forms of implementation and about the range of problems for which it is best suited. This paper undertakes to present the major ideas and findings to date, and to indicate challenges for future research. Part I of this study indicates the basic principles, ranging from the short-term memory process at the core of the search to the intermediate and long term memory processes for intensifying and diversifying the search. Included are illustrative data structures for implementing the tabu conditions (and associated aspiration criteria) that underlie these processes. Part I concludes with a discussion of probabilistic tabu search and a summary of computational experience for a variety of applications. Part I1 of this study (to appear in a subsequent issue) examines more advanced considerations, applying the basic ideas to special settings and outlining a dynamic move structure to insure finiteness. Part I1 also describes tabu search methods for solving mixed integer programming problems and gives a brief summary of additional practical experience, including the use of tabu search to guide other types of processes, such as those
71|M-tree: An Efficient Access Method for Similarity Search in Metric Spaces|A new access meth d, called M-tree, is proposed to organize and search large data sets from a generic &#034;metric space&#034;, i.e. whE4 object proximity is only defined by a distance function satisfyingth positivity, symmetry, and triangle inequality postulates. We detail algorith[ for insertion of objects and split management, whF h keep th M-tree always balanced - severalheralvFV split alternatives are considered and experimentally evaluated. Algorithd for similarity (range and k-nearest neigh bors) queries are also described. Results from extensive experimentationwith a prototype system are reported, considering as th performance criteria th number of page I/O&#039;s and th number of distance computations. Th results demonstratethm th Mtree indeed extendsth domain of applicability beyond th traditional vector spaces, performs reasonably well inhE[94Kv#E44V[vh data spaces, and scales well in case of growing files. 1 
72|Nearest Neighbor Queries|A frequently encountered type of query in Geographic Information Systems is to find the k nearest neighbor objects to a given point in space. Processing such queries requires substantially different search algorithms than those for location or range queries. In this paper we present an efficient branch-and-bound R-tree traversal algorithm to find the nearest neighbor object to a point, and then generalize it to finding the k nearest neighbors. We also discuss metrics for an optimistic and a pessimistic search ordering strategy as well as for pruning. Finally, we present the results of several experiments obtained using the implementation of our algorithm and examine the behavior of the metrics and the scalability of the algorithm. 
73|Fast subsequence matching in time-series databases|We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an ecient and eective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.  
74|Efficient and Effective Querying by Image Content|In the QBIC (Query By Image Content) project we are studying methods to query large  on-line image databases using the images&#039; content as the basis of the queries. Examples of  the content we use include color, texture, and shape of image objects and regions. Potential  applications include medical (&#034;Give me other images that contain a tumor with a texture like this  one&#034;), photo-journalism (&#034;Give me images that have blue at the top and red at the bottom&#034;),  and many others in art, fashion, cataloging, retailing, and industry.  We describe a set of novel features and similarity measures allowing query by color, texture,  and shape of image object. We demonstrate the effectiveness of the QBIC system with normalized  precision and recall experiments on test databases containing over 1000 images and 1000  objects populated from commercially available photo clip art images, and of images of airplane  silhouettes. We also consider the efficient indexing of these features, specifically addre...
75|FastMap: A Fast Algorithm for Indexing, Data-Mining and Visualization of Traditional and Multimedia Datasets|A very promising idea for fast searching in traditional and multimedia databases is to map objects into points in k-d  space, using k feature-extraction functions, provided by a domain expert [25]. Thus, we can subsequently use highly fine-tuned spatial access methods (SAMs), to answer several types of queries, including the `Query By Example&#039; type (which translates to a range query); the `all pairs&#039; query (which translates to a spatial join [8]); the nearest-neighbor or best-match query, etc. However, designing feature extraction functions can be hard. It is relatively easier for a domain expert to assess the similarity/distance of two objects. Given only the distance information though, it is not obvious how to map objects into points. This is exactly the topic of this paper. We describe a fast algorithm to map objects into points in some k-dimensional  space (k is user-defined), such that the dis-similarities are preserved. There are two benefits from this mapping: (a) efficient ret...
76|The R + -tree: A dynamic index for multidimensional objects|The problem of indexing multidimensional objects is considered. First, a classification of existing methods is given along with a discussion of the major issues involved in multidimensional data indexing. Second, a variation to Guttman’s R-trees (R +-trees) that avoids overlapping rectangles in intermediate nodes of the tree is introduced. Algorithms for searching, updating, initial packing and reorganization of the structure are discussed in detail. Finally, we provide analytical results indicating that R +-trees achieve up to 50 % savings in disk accesses compared to an R-tree when searching files of thousands of rectangles. 1
77|Content-based classification, search, and retrieval of audio|say that it belongs to the class of speech sounds or the class of applause sounds, where the system has previously been trained on other sounds in this class. I Acoustical/perceptual features: describing the sounds in terms of commonly understood physical characteristics such as brightness, pitch, and loudness. I Subjective features: describing the sounds using personal descriptive language. This requires training the system (in our case, by example) to understand the meaning of these descriptive terms. For example, a user might be looking for a “shimmering ” sound.
78|Near neighbor search in large metric spaces|Given user data, one often wants to find approximate matches in a large database. A good example of such a task is finding images similar to a given image in a large collection of images. We focus on the important and technically difficult case where each data element is high dimensional, or more generally, is represented by a point in a large metric spaceand distance calculations are computationally expensive. In this paper we introduce a data structure to solve this problem called a GNAT- Geometric Near-neighbor Access Tree. It is based on the philosophy that the data structure should act as a hierarchical geometrical model of the data as opposed to a simple decomposition of the data that does not use its intrinsic geometry. In experiments, we find that GNAT’s outperform previous data structures in a number of applications.
79|Distance-based indexing for high-dimensional metric spaces|In many database applications, one of the common queries is to find approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance based index structures are proposed for applications where the data domain is high dimensional, or the distance function used to compute distances between data objects is non-Euclidean. In this paper, we introduce a distance based index structure called multi-vantage point (mvp) tree for similarity queries on high-dimensional metric spaces. The mvptree uses more than one vantage point to partition the space into spherical cuts at each level. It also utilizes the pre-computed (at construction time) distances between the data points and the vantage points. We have done experiments to compare mvp-trees with vp-trees which have a similar partitioning strategy, but use only one vantage point at each level, and do not make use of the pre-computed distances. Empirical studies show that mvptree outperforms the vp-tree 20 % to 80 % for varying query ranges and different distance distributions. 1.
80|Costly search and mutual fund flows|This paper studies the flows of funds into and out of equity mutual funds. Consumers base their fund purchase decisions on prior performance information, but do so asymmetrically, investing disproportionately more in funds that performed very well the prior period. Search costs seem to be an important determinant of fund flows. High performance appears to be most salient for funds that exert higher marketing effort, as measured by higher fees. Flows are directly related to the size of the fund’s complex as well as the current media attention received by the fund, which lower consumers ’ search costs. ALTHOUGH MUCH ACADEMIC RESEARCH on mutual funds addresses issues of performance measurement and attribution, we can learn more from this industry than whether fund managers can consistently earn risk-adjusted excess returns. Researchers studying funds have shed light on how incentives affect fund managers ’ behavior, 1 how board structure affects oversight activities, 2 and how scale and scope economies affect mutual fund costs and fees. 3 More generally, the fund industry is a laboratory in which to study the actions of individual investors who buy fund shares. In this paper, we study the flows of funds into and out of individual U.S. equity mutual funds to better understand the behavior of households that buy funds and the fund complexes and marketers that sell them.
83|The performance of mutual funds in the period 1945-1964|In this paper I derive a risk-adjusted measure of portfolio performance (now known as &#034;Jensen&#039;s Alpha&#034;) that estimates how much a manager&#039;s forecasting ability contributes to the fund&#039;s returns. The measure is based on the theory of the pricing of capital assets by Sharpe (1964), Lintner (1965a) and Treynor (Undated). I apply the measure to estimate the predictive ability of 115 mutual fund managers in the period 1945-1964—that is their ability to earn returns which are higher than those we would expect given the level of risk of each of the portfolios. The foundations of the model and the properties of the performance measure suggested here are discussed in Section II. The evidence on mutual fund performance indicates not only that these 115 mutual funds were on average not able to predict security prices well enough to outperform a buy-the-marketand-hold policy, but also that there is very little evidence that any individual fund was able to do significantly better than that which we expected from mere random chance. It is also important to note that these conclusions hold even when we measure the fund returns gross of management expenses (that is assume their bookkeeping, research, and other expenses except brokerage commissions were obtained free). Thus on average the funds apparently were not quite successful enough in their trading activities to recoup even their brokerage expenses.  
84|Cognitive Dissonance and Mutual Fund Investors|We present evidence from questionnaire studies of mutual fund  investors about recollections of past fund performance. We find  that investor memories exhibit a positive bias, consistent with  current psychological models. We find that the degree of bias is  conditional upon previous investor choice, a phenomenon related  to the well known theory of cognitive dissonance.
85|Pushing the Envelope: Planning, Propositional Logic, and Stochastic Search|Planning is a notoriously hard combinatorial search problem. In many interesting domains, current planning algorithms fail to scale up gracefully. By combining a general, stochastic search algorithm and appropriate problem encodings based on propositional logic, we are able to solve hard planning problems many times faster than the best current planning systems. Although stochastic methods have been shown to be very e ective on a wide range of scheduling problems, this is the rst demonstration of its power on truly challenging classical planning instances. This work also provides a new perspective on representational issues in planning.
86|Fast Planning Through Planning Graph Analysis|We introduce a new approach to planning in STRIPS-like domains based on constructing and analyzing a compact structure we call a Planning Graph. We describe a new planner, Graphplan, that uses this paradigm. Graphplan always returns a shortest possible partial-order plan, or states that no valid plan exists. We provide empirical evidence in favor of this approach, showing that Graphplan outperforms the total-order planner, Prodigy, and the partial-order planner, UCPOP, on a variety of interesting natural and artificial planning problems. We also give empirical evidence that the plans produced by Graphplan are quite sensible. Since searches made by this approach are fundamentally different from the searches of other common planning methods, they provide a new perspective on the planning problem.
87|Using Temporal Logic to Control Search in a Forward Chaining Planner|. Over the years increasingly sophisticated planning algorithms have been developed. These have made for more efficient planners, but unfortunately these planners still suffer from combinatorial explosion. Indeed, recent theoretical results demonstrate that such an explosion is inevitable. It has long been acknowledged that domain independent planners need domain dependent information to help them plan effectively. In this work we describe how natural domain information, of a &#034;strategic&#034; nature, can be expressed in a temporal logic, and then utilized to effectively control a forward-chaining planner. There are numerous advantages to our approach, including a declarative semantics for the search control knowledge; a high degree of modularity (the more search control knowledge utilized the more efficient search becomes); and an independence of this knowledge from the details of the planning algorithm. We have implemented our ideas in the TLPLAN system, and have been able to demonstrate i...
88|Partial-Order Planning: Evaluating Possible Efficiency Gains|Although most people believe that planners that delay step-ordering decisions as long as possible are more efficient than those that manipulate totally ordered sequences of actions, this intuition has received little formal justification or empirical validation. In this paper we do both, characterizing the types of domains that offer performance differentiation and the features that distinguish the relative overhead of three planning algorithms. As expected, the partial-order (nonlinear) planner often has an advantage when confronted with problems in which the specific order of the plan steps is critical. We argue that the observed performance differences are best understood with an extension of Korf&#039;s taxonomy of subgoal collections. Each planner quickly solved problems whose subgoals were independent or trivially serializable, but problems with laboriously serializable or nonserializable subgoals were intractable for all planners. Since different plan representations induce distinct ...
89|Planning as Temporal Reasoning|This paper describes a reasoning system based on a temporal logic that can solve planning problems along the lines of traditional planning systems. Because it is cast as inference in a general representation, however, the ranges of problems that can be described is considerably greater than in traditional planning systems. In addition, other modes of plan reasoning, such as plan recognition or plan monitoring, can be formalized within the same framework. 1
90|Fast Parallel Algorithms for Short-Range Molecular Dynamics|Three parallel algorithms for classical molecular dynamics are presented. The first assigns each  processor a fixed subset of atoms; the second assigns each a fixed subset of inter-atomic forces to compute;  the third assigns each a fixed spatial region. The algorithms are suitable for molecular dynamics models  which can be difficult to parallelize efficiently -- those with short-range forces where the neighbors of  each atom change rapidly. They can be implemented on any distributed--memory parallel machine which  allows for message--passing of data between independently executing processors. The algorithms are  tested on a standard Lennard-Jones benchmark problem for system sizes ranging from 500 to 100,000,000  atoms on several parallel supercomputers -- the nCUBE 2, Intel iPSC/860 and Paragon, and Cray T3D.  Comparing the results to the fastest reported vectorized Cray Y-MP and C90 algorithm shows that  the current generation of parallel machines is competitive with conventi...
91|A Fast Algorithm for Particle Simulations|this paper to the case where  the potential (or force) at a point is a sum of pairwise An algorithm is presented for the rapid evaluation of the potential and force fields in systems involving large numbers of particles interactions. More specifically, we consider potentials of  whose interactions are Coulombic or gravitational in nature. For a the form  system of N particles, an amount of work of the order O(N  2  ) has traditionally been required to evaluate all pairwise interactions, un- F5F far 1 (F near 1F external ), less some approximation or truncation method is used. The algorithm of the present paper requires an amount of work proportional to N to evaluate all interactions to within roundoff error, making it where F near (when present) is a rapidly decaying potential  con
92|The Torus-Wrap Mapping For Dense Matrix Calculations On Massively Parallel Computers| Dense linear systems of equations are quite common in science and engineering, arising in boundary element methods, least squares problems and other settings. Massively parallel computers will be necessary to solve the large systems required by scientists and engineers, and scalable parallel algorithms for the linear algebra applications must be devised for these machines. A critical step in these algorithms is the mapping of matrix elements to processors. In this paper, we study the use of the torus--wrap mapping in general dense matrix algorithms, from both theoretical and practical viewpoints. We prove that, under reasonable assumptions, this assignment scheme leads to dense matrix algorithms that achieve (to within a constant factor) the lower bound on interprocessor communication. We also show that the torus--wrap mapping allows algorithms to exhibit less idle time, better load balancing and less memory overhead than the more common row and column mappings. Finally, we discuss ...
93|A New Parallel Method for Molecular Dynamics Simulation of Macromolecular Systems|Short--range molecular dynamics simulations of molecular systems are commonly parallelized by  replicated--data methods, where each processor stores a copy of all atom positions. This enables computation  of bonded 2--, 3--, and 4--body forces within the molecular topology to be partitioned among  processors straightforwardly. A drawback to such methods is that the inter--processor communication  scales as N , the number of atoms, independent of P , the number of processors. Thus, their parallel efficiency  falls off rapidly when large numbers of processors are used. In this article a new parallel method  for simulating macromolecular or small--molecule systems is presented, called force--decomposition. Its  memory and communication costs scale as N=  p  P , allowing larger problems to be run faster on greater  numbers of processors. Like replicated--data techniques, and in contrast to spatial--decomposition approaches,  the new method can be simply load--balanced and performs well eve...
94|Parallel Many-Body Simulations Without All-to-All Communication|Simulations of interacting particles are common in science and engineering, appearing in such diverse disciplines as astrophysics, fluid dynamics, molecular physics, and materials science. These simulations are often computationally intensive and so natural candidates for massively  parallel computing. Many-body simulations that directly compute interactions between pairs  of particles, be they short-range or long-range interactions, have been parallelized in several  standard ways. The simplest approaches require all-to-all communication, an expensive communication  step. The fastest methods assign a group of nearby particles to a processor, which  can lead to load imbalance and be difficult to implement efficiently. We present a new approach,  suitable for direct simulations, that avoids all-to-all communication without requiring  any geometric clustering. For some computations we find the new method to be the fastest  parallel algorithm available; we demonstrate its utility...
95|A High Performance Communications and Memory Caching Scheme for Molecular Dynamics on the CM-5|We present several techniques that we have used to optimize the performance of a message-passing C code for molecular dynamics on the CM-5. We describe our use of the CM-5 vector units and a parallel memory caching scheme that we have developed to speed up the code by more than 50%. A modification that decreases our communication time by 35% is also presented along with a discussion of how we have been able to take advantage of the CM-5 hardware without significantly compromising code portability. We have been able to speed up our original code by a factor of ten and we feel that our modifications may be useful in optimizing the performance of other message-passing C applications on the CM-5. 1 Introduction  For several decades, the method of molecular dynamics (MD)[1] has been a useful technique for studying the dynamical properties of solids and liquids. In a molecular dynamics simulation, the motion of a large collection of N atoms is modeled directly by solving Newton&#039;s equations o...
96|A Parallel Scalable Approach to Short-Range Molecular Dynamics on the CM-5|We present an scalable algorithm for short-range Molecular Dynamics which minimizes interprocessor communications at the expense of a modest computational redundancy. The method combines Verlet neighbor lists with coarse-grained cells. Each processing node is associated with a cubic volume of space and the particles it owns are those initially contained in the volume. Data structures for &#034;own&#034; and &#034;visitor &#034; particle coordinates are maintained in each node. Visitors are particles owned by one of the 26 neighboring cells but lying within an interaction range of a face. The Verlet neighbor list includes pointers to own--own and own--visitor interactions. To communicate, each of the 26 neighbor cells sends a corresponding block of particle coordinates using message-passing calls. The algorithm has the numerical properties of the standard serial Verlet method and is efficient for hundreds to thousands of particles per node allowing the simulation of large systems with millions of particles...
97|A Fast Quantum Mechanical Algorithm for Database Search|Imagine a phone directory containing N names arranged in completely random order. In order to find someone&#039;s phone number with a probability of , any classical algorithm (whether deterministic or probabilistic)
will need to look at a minimum of names. Quantum mechanical systems can be in a superposition of states and simultaneously examine multiple names. By properly adjusting the phases of various operations, successful computations reinforce each other while others interfere randomly. As a result, the desired phone number can be obtained in only steps. The algorithm is within a small constant factor of the fastest possible quantum mechanical algorithm.
98|Algorithms for Quantum Computation: Discrete Logarithms and Factoring|A computer is generally considered to be a universal computational device; i.e., it is believed able to simulate any physical computational device with a cost in com-putation time of at most a polynomial factol: It is not clear whether this is still true when quantum mechanics is taken into consideration. Several researchers, starting with David Deutsch, have developed models for quantum mechanical computers and have investigated their compu-tational properties. This paper gives Las Vegas algorithms for finding discrete logarithms and factoring integers on a quantum computer that take a number of steps which is polynomial in the input size, e.g., the number of digits of the integer to be factored. These two problems are generally considered hard on a classical computer and have been used as the basis of several proposed cryptosystems. (We thus give the first examples of quantum cryptanulysis.)
99|Quantum theory, the Church-Turing principle and the universal quantum computer|computer
100|Quantum complexity theory|Abstract. In this paper we study quantum computation from a complexity theoretic viewpoint. Our first result is the existence of an efficient universal quantum Turing machine in Deutsch’s model of a quantum Turing machine (QTM) [Proc. Roy. Soc. London Ser. A, 400 (1985), pp. 97–117]. This construction is substantially more complicated than the corresponding construction for classical Turing machines (TMs); in fact, even simple primitives such as looping, branching, and composition are not straightforward in the context of quantum Turing machines. We establish how these familiar primitives can be implemented and introduce some new, purely quantum mechanical primitives, such as changing the computational basis and carrying out an arbitrary unitary transformation of polynomially bounded dimension. We also consider the precision to which the transition amplitudes of a quantum Turing machine need to be specified. We prove that O(log T) bits of precision suffice to support a T step computation. This justifies the claim that the quantum Turing machine model should be regarded as a discrete model of computation and not an analog one. We give the first formal evidence that quantum Turing machines violate the modern (complexity theoretic) formulation of the Church–Turing thesis. We show the existence of a problem, relative to an oracle, that can be solved in polynomial time on a quantum Turing machine, but requires superpolynomial time on a bounded-error probabilistic Turing machine, and thus not in the class BPP. The class BQP of languages that are efficiently decidable (with small error-probability) on a quantum Turing machine satisfies BPP ? BQP ? P ?P. Therefore, there is no possibility of giving a mathematical proof that quantum Turing machines are more powerful than classical probabilistic Turing machines (in the unrelativized setting) unless there is a major breakthrough in complexity theory.
101|Rapid solution of problems by quantum computation|A class of problems is described which can be solved more efficiently by quantum computation than by any classical or stochastic method. The quantum computation solves the problem with certainty in exponentially less time than any classical deterministic computation.
103|Strengths and Weaknesses of quantum computing|  Recently a great deal of attention has been focused on quantum computation following a
104|Quantum Circuit Complexity|We study a complexity model of quantum circuits analogous to the standard (acyclic) Boolean circuit model. It is shown that any function computable in polynomial time by a quantum Turing machine has a polynomial-size quantum circuit. This result also enables us to construct a universal quantum computer which can simulate, with a polynomial factor slowdown, a broader class of quantum machines than that considered by Bernstein and Vazirani [BV93], thus answering an open question raised in [BV93]. We also develop a theory of quantum communication complexity, and use it as a tool to prove that the majority function does not have a linear-size quantum formula. Keywords. Boolean circuit complexity, communication complexity, quantum communication complexity, quantum computation  AMS subject classifications. 68Q05, 68Q15 1  This research was supported in part by the National Science Foundation under grant CCR-9301430.  1 Introduction One of the most intriguing questions in computation theroy ...
105|Matching is as Easy as Matrix Inversion|A new algorithm for finding a maximum matching in a general graph is presented; its special feature being that the only computationally non-trivial step required in its execution is the inversion of a single integer matrix. Since this step can be parallelized, we get a simple parallel (RNC2) algorithm. At the heart of our algorithm lies a probabilistic lemma, the isolating lemma. We show applications of this lemma to parallel computation and randomized reductions. 
106|Oracle quantum computing|\Because nature isn&#039;t classical, dammit...&#034;
107|A fast quantum mechanical algorithm for estimating the median. Quantum Physics e-Print archive |Consider the problem of estimating the median of N items to a precision e, i.e. the estimate µ should be such that, with a large probability, the number of items with values smaller than µ is less than and those with values greater than µ is also less than. Any classical algorithm to do this will need at least samples. Quantum mechanical systems can simultaneously carry out multiple computations due to their wave like properties. This paper gives an step algorithm for the above problem. 1
108|The FF planning system: Fast plan generation through heuristic search|We describe and evaluate the algorithmic techniques that are used in the FF planning system. Like the HSP system, FF relies on forward state space search, using a heuristic that estimates goal distances by ignoring delete lists. Unlike HSP&#039;s heuristic, our method does not assume facts to be independent. We introduce a novel search strategy that combines Hill-climbing with systematic search, and we show how other powerful heuristic information can be extracted and used to prune the search space. FF was the most successful automatic planner at the recent AIPS-2000 planning competition. We review the results of the competition, give data for other benchmark domains, and investigate the reasons for the runtime performance of FF compared to HSP.  
109|Systematic Nonlinear Planning|This paper presents a simple, sound, complete, and systematic algorithm for domain independent STRIPS planning. Simplicity is achieved by starting with a ground procedure and then applying a general, and independently verifiable, lifting transformation. Previous planners have been designed directly as lifted procedures. Our ground procedure is a ground version of Tate&#039;s NONLIN procedure. In Tate&#039;s procedure one is not required to determine whether a prerequisite of a step in an unfinished plan is guaranteed to hold in all linearizations. This allows Tate&#039;s procedure to avoid the use of Chapman&#039;s modal truth criterion. Systematicity is the property that the same plan, or partial plan, is never examined more than once. Systematicity is achieved through a simple modification of Tate&#039;s procedure.
110|The Computational Complexity of Propositional STRIPS Planning|I present several computational complexity results for propositional STRIPS planning, i.e., STRIPS planning restricted to ground formulas. Different planning problems can be defined by restricting the type of formulas, placing limits on the number of pre- and postconditions, by restricting negation in pre- and postconditions, and by requiring optimal plans. For these types of restrictions, I show when planning is tractable (polynomial) and intractable (NPhard) . In general, it is PSPACE-complete to determine if a given planning instance has any solutions. Extremely severe restrictions on both the operators and the formulas are required to guarantee polynomial time or even NP-completeness. For example, when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. When definite Horn ground formulas are permitted, determining plan existence is PSPACE-complete even if operators are limited t...
111|Unifying SAT-based and Graph-based Planning|The Blackbox planning system unifies the plan-ning as satisfiability framework (Kautz and Sel-man 1992, 1996) with the plan graph approach to STRIPS planning (Blum and Furst 1995). We show that STRIPS problems can be directly translated into SAT and efficiently solved using new random-ized systematic solvers. For certain computation-ally challenging benchmark problems this unified approach outperforms both SATPLAN and Graph-plan alone. We also demonstrate that polynomial-time SAT simplification algorithms applied to the encoded problem instances are a powerful com-plement to the “mutex ” propagation algorithm that works directly on the plan graph. 1
112|Hard and Easy Distributions of SAT Problems|We report results from large-scale experiments in satisfiability testing. As has been observed by others, testing the satisfiability of random formulas often appears surprisingly easy. Here we show that by using the right distribution of instances, and appropriate parameter values, it is possible to generate random formulas that are hard, that is, for which satisfiability testing is quite difficult. Our results provide a benchmark for the evaluation of satisfiability-testing procedures.  Introduction  Many computational tasks of interest to AI, to the extent that they can be precisely characterized at all, can be shown to be NP-hard in their most general form. However, there is fundamental disagreement, at least within the AI community, about the implications of this. It is claimed on the one hand that since the performance of algorithms designed to solve NP-hard tasks degrades rapidly with small increases in input size, something will need to be given up to obtain acceptable behavior....
113|Planning as Heuristic Search: New Results|In the recent AIPS98 Planning Competition, the hsp planner,  based on a forward state search and a domain-independent heuristic,  showed that heuristic search planners can be competitive with state of  the art Graphplan and Satisfiability planners. hsp solved more problems  than the other planners but it often took more time or produced longer  plans. The main bottleneck in hsp is the computation of the heuristic  for every new state. This computation may take up to 85% of the processing  time. In this paper, we present a solution to this problem that  uses a simple change in the direction of the search. The new planner,  that we call hspr, is based on the same ideas and heuristic as hsp, but  searches backward from the goal rather than forward from the initial  state. This allows hspr to compute the heuristic estimates only once. As  a result, hspr can produce better plans, often in less time. For example,  hspr solves each of the 30 logistics problems from Kautz and Selman in  less than 3 seconds. This is two orders of magnitude faster than blackbox. At the same time
114|Extending planning graphs to an ADL subset| We describe an extension of graphplan to a subset of ADL that allows conditional and universally quantified effects in operators in such away that almost all interesting properties of the original graphplan algorithm are preserved.
115|A Robust and Fast Action Selection Mechanism for Planning|The ability to plan and react in dynamic environments is central to intelligent behavior yet few algorithms have managed to combine fast planning with a robust execution. In this paper we develop one such algorithm by looking at planning as real time search. For that we develop a variation of Korf&#039;s Learning Real Time A algorithm together with a suitable heuristic function. The resulting algorithm interleaves lookahead with execution and never builds a plan. It is an action selection mechanism that decides at each time point what to do next. Yet it solves hard planning problems faster than any domain independent planning algorithm known to us, including the powerful SAT planner recently introduced by Kautz and Selman. It also works in the presence of perturbations and noise, and can be given a fixed time window to operate. We illustrate each of these features by running the algorithm on a number of benchmark problems. 1 Introduction The ability to plan and react ...
116|The automatic inference of state invariants in TIM|As planning is applied to larger and richer domains the e ort involved in constructing domain descriptions increases and becomes a signi cant burden on the human application designer. If general planners are to be applied successfully to large and complex domains it is necessary to provide the domain designer with some assistance in building correctly encoded domains. One way of doing this is to provide domain-independent techniques for extracting, from a domain description, knowledge that is implicit in that description and that can assist domain designers in debugging domain descriptions. This knowledge can also be exploited to improve the performance of planners: several researchers have explored the potential of state invariants in speeding up the performance of domain-independent planners. In this paper we describe a process by which state invariants can be extracted from the automatically inferred type structure of a domain. These techniques are being developed for exploitation by stan, a Graphplan based planner that employs state analysis techniques to enhance its performance. 1.
117|Combining the expressivity of UCPOP with the efficiency of Graphplan|  There has been a great deal of recent work on new approaches to efficiently generating plans in systems such as Graphplan and SATplan. However, these systems only provide an impoverished representation language compared to other planners, such as UCPOP, ADL, or Prodigy. This makes it difficult to represent planning problems using these new planners. This paper addresses this problem by providing a completely automated set of transformations for converting a UCPOP domain representation into a Graphplan representation. The set of transformations extends the Graphplan representation language to include disjunctions, negations, universal quantification, conditional effects, and axioms. We tested the resulting planner on the 18 test domains and 41 problems that come with the UCPOP 4.0 distribution. Graphplan with the new preprocessor is able to solve every problem in the test set and on the hard problems (i.e., those that require more than one second of CPU time) it can solve them significantly faster than UCPOP. While UCPOP was unable to solve 7 of the test problems within a search limit of 100,000 nodes (which requires 414 to 980 CPU seconds), Graphplan with the preprocessor solved them all in under 15 CPU seconds (including the preprocessing time). 
118|A Heuristic Estimator for Means-Ends Analysis in Planning|Means-ends analysis is a seemingly well understood search technique, which can be described, using planning terminology, as: keep adding actions that are feasible and achieve pieces of the goal. Unfortunately, it is often the case that no action is both feasible and relevant in this sense. The traditional answer is to make subgoals out of the preconditions of relevant but infeasible actions. These subgoals become part of the search state. An alternative, surprisingly good, idea is to recompute the entire subgoal hierarchy after every action. This hierarchy is represented by a greedy regression-match graph. The actions near the leaves of this graph are feasible and relevant to a sub. . . subgoals of the original goal. Furthermore, each subgoal is assigned an estimate of the number of actions required to achieve it. This number can be shown in practice to be a useful heuristic estimator for domains that are otherwise intractable.  Keywords: planning, search, means-ends analysis   Reinven...
119|On the compilability and expressive power of propositional planning formalisms|The recent approaches of extending the GRAPHPLAN algorithm to handle more expressive planning formalisms raise the question of what the formal meaning of “expressive power ” is. We formalize the intuition that expressive power is a measure of how concisely planning domains and plans can be expressed in a particular formalism by introducing the notion of “compilation schemes ” between planning formalisms. Using this notion, we analyze the expressiveness of a large family of propositional planning formalisms, ranging from basic STRIPS to a formalism with conditional effects, partial state specifications, and propositional formulae in the preconditions. One of the results is that conditional effects cannot be compiled away if plan size should grow only linearly but can be compiled away if we allow for polynomial growth of the resulting plans. This result confirms that the recently proposed extensions to the GRAPHPLAN algorithm concerning conditional effects are optimal with respect to the “compilability ” framework. Another result is that general propositional formulae cannot be compiled into conditional effects if the plan size should be preserved linearly. This implies that allowing general propositional formulae in preconditions and effect conditions adds another level of difficulty in generating a plan.
120|Ignoring Irrelevant Facts and Operators in Plan Generation|It is traditional wisdom that one should start from the goals when  generating a plan in order to focus the plan generation process on potentially  relevant actions. The graphplan system, however, which is the  most efficient planning system nowadays, builds a &#034;planning graph&#034; in a  forward-chaining manner. Although this strategy seems to work well, it  may possibly lead to problems if the planning task description contains irrelevant  information. Although some irrelevant information can be filtered  out by graphplan, most cases of irrelevance are not noticed.  In this paper, we analyze the effects arising from &#034;irrelevant&#034; information  to planning task descriptions for different types of planners. Based  on that, we propose a family of heuristics that select relevant information  by minimizing the number of initial facts that are used when approximating  a plan by backchaining from the goals ignoring any conflicts. These  heuristics, although not solution-preserving, turn out to be v...
121|Using Regression-Match Graphs to Control Search in Planning|Classical planning is the problem of finding a sequence of actions to achieve a goal given an exact characterization of a domain. An algorithm to solve this problem is presented, which searches a space of plan prefixes, trying to extend one of them to a complete sequence of actions. It is guided by a heuristic estimator based on regression-match graphs, which attempt to characterize the entire subgoal structure of the remaining part of the problem. These graphs simplify the structure by neglecting goal interactions and by assuming that variables in goal conjunctions should be bound in such a way as to make as many conjuncts as possible true without further work. In some domains, these approximations work very well, and experiments show that many classical-planning problems can solved with very little search. 1 Definition of the Problem  The classical planning problem is to generate a sequence of actions that make a given proposition true, in a domain in which there is perfect informati...
122|Efficient Implementation of the Plan Graph in STAN|Stan is a Graphplan-based planner, so-called because it uses a variety of STate ANalysis techniques to enhance its performance. Stan competed in the AIPS-98 planning competition where it compared well with the other competitors in terms of speed, finding solutions fastest to many of the problems posed. Although the domain analysis techniques  Stan exploits are an important factor in its overall performance, we believe that the speed at which Stan solved the competition problems is largely due to the implementation of its plan graph. The implementation is based on two insights: that many of the graph construction operations can be implemented as bit-level logical operations on bit vectors, and that the graph should not be explicitly constructed beyond the fix point. This paper describes the implementation of Stan&#039;s plan graph and provides experimental results which demonstrate the circumstances under which advantages can be obtained from using this implementation. 1. Introduction  Stan ...
123|When Gravity Fails: Local Search Topology|Local search algorithms for combinatorial search problems frequently encounter a sequence of states in which it is impossible to improve the value of the objective function; moves through these regions, called plateau moves, dominate the time spent in local search. We analyze and characterize plateaus for three different classes of randomly generated Boolean Satisfiability problems. We identify several interesting features of plateaus that impact the performance of local search algorithms. We show that local minima tend to be small but occasionally may be very large. We also show that local minima can be escaped without unsatisfying a large number of clauses, but that systematically searching for an escape route may be computationally expensive if the local minimum is large. We show that plateaus with exits, called benches, tend to be much larger than minima, and that some benches have very few exit states which local search can use to escape. We show that the solutions (i.e., global m...
124|On Reasonable and Forced Goal Orderings and their Use in an Agenda-Driven Planning Algorithm|The paper addresses the problem of computing goal orderings, which is one of the longstanding issues in AI planning. It makes two new contributions. First, it formally defines and discusses two different goal orderings, which are called the reasonable and the forced ordering. Both orderings are defined for simple STRIPS operators as well as for more complex ADL operators supporting negation and conditional effects. The complexity of these orderings is investigated and their practical relevance is discussed. Secondly, two different methods to compute reasonable goal orderings are developed. One of them is based on planning graphs, while the other investigates the set of actions directly. Finally, it is shown how the ordering relations, which have been derived for a given set of goals G, can be used to compute a so-called goal agenda that divides G into an ordered set of subgoals. Any planner can then, in principle, use the goal agenda to plan for increasing sets of subgoals. This ...
125|A Heuristic for Domain Independent Planning and its Use in an Enforced Hill-climbing Algorithm |  We present a new heuristic method to evaluate planning states, which is based on solving a relaxation of the planning problem. The solutions to the relaxed problem give a good estimate for the length of a real solution, and they can also be used to guide action selection during planning. Using these informations, we employ a search strategy that combines Hill-climbing with systematic search. The algorithm is complete on what we call deadlock-free domains. Though it does not guarantee the solution plans to be optimal, it does find close to optimal plans in most cases. Often, it solves the problems almost without any search at all. In particular, it outperforms all state-of-the-art planners on a large range of domains. 
126|Conditional Effects in Graphplan|Graphplan has attracted considerable interest because of its extremely high performance, but the algorithm&#039;s inability to handle action representations more expressive than STRIPS is a major limitation. In particular, extending Graphplan to handle conditional effects is a surprisingly subtle enterprise. In this paper, we describe the space of possible alternatives, and then concentrate on one particular approach we call factored expansion. Factored expansion splits an action with conditional effects into several new actions called components, one for each conditional effect. Because these action components are not independent, factored expansion complicates both the mutual exclusion and backward chaining phases of Graphplan. As compensation, factored expansion often produces dramatically smaller domain models than does the more obvious full-expansion into exclusive STRIPS actions. We present experimental...
127|Hybrid STAN: Identifying and Managing Combinatorial Optimisation Sub-problems in Planning|It is well-known that planning is hard but it  is less well-known how to approach the hard  parts of a problem instance eectively. Using  static domain analysis techniques we can identify  and abstract certain combinatorial subproblems  from a planning instance, and deploy  specialised technology to solve these subproblems  in a way that is integrated with the  broader planning activities. We have developed  a hybrid planning system (STAN4) which  brings together alternative planning strategies  and specialised algorithms and selects between  them according to the structure of the planning  domain. STAN4 participated successfully  in the AIPS-2000 planning competition. We  describe how sub-problem abstraction is done,  with particular reference to route-planning abstraction,  and present some of the competition  data to demonstrate the potential power of the  hybrid approach.  1 Introduction  The knowledge-sparse, or domain-independent, planning community is often criticised for its o...
128|GRT: A Domain Independent Heuristic for STRIPS Worlds based on Greedy Regression Tables|This paper presents Greedy Regression Tables (GRT), a new domain  independent heuristic for STRIPS worlds. The heuristic can be used to guide  the search process of any state-space planner, estimating the distance between  each intermediate state and the goals. At the beginning of the problem solving  process a table is created, the records of which contain the ground facts of the  domain, among with estimates for their distances from the goals. Additionally,  the records contain information about interactions that occur while trying to  achieve different ground facts simultaneously. During the search process, the  heuristic, using this table, extracts quite accurate estimates for the distances  between intermediate states and the goals. A simple best-first search planner  that uses this heuristic has been implemented in C++ and has been tested on  several &#034;classical&#034; problem instances taken from the bibliography and on some  new taken from the AIPS-98 planning competition. Our planner has proved to  be faster in all of the cases, finding also in most (but not all) of the cases shorter  solutions.
129|Goal ordering in partially ordered plans|t-o. _E
130|Elevator Control as a Planning Problem|The synthesis of elevator control commands is a difficult problem when new service requirements such as VIP service, access restrictions, nonstop travel etc. have to be individually tailored to each passenger. AI planning technology offers a very elegant and flexible solution because the possible actions of a control system can be made explicit and their preconditions and e ects can be speci ed using expressive representation formalisms. Based on the specification, a planner can flexibly synthesize the required control and changes in the specification do not require any reimplementation of the control software. In this paper, we describe the application and investigate how currently available domain-independent planning formalisms can cope with it.
131|Ordering problem subgoals|Most past research work on problem subgoal ordering are of a heuristic nature and very little attempt has been made to reveal the inherent relationship between subgoal ordering constraints and problem operator schemata. As a result, subgoal ordering strategies which have been developed tend to be either overly committed, imposing ordering on subgoals subjectively or randomly, or overly restricted, ordering subgoals only after a violation of ordering constraints becomes explicit during the development of a problem solution or plan. This paper proposes a new approach characterized by a formal representation of subgoal ordering constraints which makes explicit the relationship between the constraints and the problem operator schemata. Following this approach, it becomes straightforward to categorize various types of subgoal ordering constraints, to manipulate or extend the relational representation of the constraints, to systematically detect important subgoal ordering constraints from problem specifications, and to apply the detected constraints to multiple problem instances. 1
132|Plateaus and Plateau Search in Boolean Satisfiability Problems: When to Give Up Searching and Start Again|: We empirically investigate the properties of the search space and the behavior of hill-climbing search for solving hard, random Boolean satisfiability problems. In these experiments it was frequently observed that rather than attempting to escape from plateaus by extensive search, it was better to completely restart from a new random initial state. The optimum point to terminate search and restart was determined empirically over a range of problem sizes and complexities. The growth rate of the optimum cutoff is faster than linear with the number of features, although the exact growth rate was not determined. Based on these empirical results, a simple run-time heuristic is proposed to determine when to give up searching a plateau and restart. This heuristic closely approximates the empirically determined optimum values over a range of problem sizes and complexities, and consequently allows the search algorithm to automatically adjust its strategy for each particular problem without pr...
133|Solving Complex Planning Tasks Through Extraction of Subproblems|The paper introduces an approach to derive a total ordering  between increasing sets of subgoals by defining  a relation over atomic goals. The ordering is represented  in a so-called goal agenda that is used by the  planner to incrementally plan for the increasing sets  of subgoals. This can lead to an exponential complexity  reduction because the solution to a complex planning  problem is found by solving easier subproblems.  Since only a polynomial overhead is caused by the goal  agenda computation, a potential exists to dramatically  speed up planning algorithms as we demonstrate in the  empirical evaluation.  Introduction  How to effectively plan for interdependent subgoals has been in the focus of AI planning research for a very long time (Chapman 1987). But until today planners have made only some progress to solve larger sets of subgoals and scalability of classical planning systems is still a problem.  Previous approaches fell into two categories: On one hand, one can focus on...
134|Solving the Entailment Problem in the Fluent Calculus using Binary Decision Diagrams|The paper is an exercise in formal program development. We rigorously show how planning  problems encoded as entailment problems in the fluent calculus can be mapped onto  satisfiability problems for propositional formulas, which in turn can be mapped to the problem  to find models using binary decision diagrams. The mapping is shown to be sound and  complete. Preliminary experimental results of an implementation are discussed.
135|Extracting Route-Planning: First Steps in Automatic Problem Decomposition|The divide between knowledge-intensive, domaindependent  planning and knowledge-sparse, searchintensive  domain-independent planning has a long history  in the planning community. It has been argued  that the only route to ecient planning with real application  domains is to exploit human expert knowledge,  encoded in the domain structures used by a planner.  On the other hand, it has also been observed  that the exploitationof knowledge in this way restricts  planners to domains in which the knowledge is valid  and available, and signicantly reduces the extent to  which the planners can be exibly redeployed. In this  paper, we present the rst steps in a direction which  might bridge this gap: the automatic identication  and exploitation of fundamental behaviours within a  knowledge-sparse encoding of a planning domain, with  the objective of allowing specialised problem-solving  technology access to the components of a problem for  which they are best suited, while not increasing ...
136|Heuristic search planning with BDDs|Abstract. In this paper we study traditional and enhanced BDDbased exploration procedures capable of handling large planning problems. On the one hand, reachability analysis and model checking have eventually approached AI-Planning. Unfortunately, they typically rely on uninformed blind search. On the other hand, heuristic search and especially lower bound techniques have matured in effectively directing the exploration even for large problem spaces. Therefore, with heuristic symbolic search we address the unexplored middle ground between single state and symbolic planning engines to establish algorithms that can gain from both sides. To this end we implement and evaluate heuristics found in state-of-the-art heuristic single-state search planners. 1
137|On the Instantiation of ADL Operators Involving Arbitrary First-Order Formulas | The generation of the set of all ground actions for a given set of ADL operators, which are allowed to have conditional effects and preconditions that can be represented using arbitrary first-order formulas is a complex process which heavily influences the performance of any planner or pre-planning analysis method. The paper describes a sophisticated instantiation procedure that determines so-called inertia in a given problem representation and uses them to perform simplifications of formulas during the instantiation process. As a result, many inapplicable actions are detected and ruled out from the domain representation yielding a much smaller search space for the planner. 
138|Suffix arrays: A new method for on-line string searches|A new and conceptually simple data structure, called a suffix array, for on-line string searches is intro-duced in this paper. Constructing and querying suffix arrays is reduced to a sort and search paradigm that employs novel algorithms. The main advantage of suffix arrays over suffix trees is that, in practice, they use three to five times less space. From a complexity standpoint, suffix arrays permit on-line string searches of the type, &#034;Is W a substring of A?&#034; to be answered in time O(P + log N), where P is the length of W and N is the length of A, which is competitive with (and in some cases slightly better than) suffix trees. The only drawback is that in those instances where the underlying alphabet is finite and small, suffix trees can be constructed in O(N) time in the worst case, versus O(N log N) time for suffix arrays. However, we give an augmented algorithm that, regardless of the alphabet size, constructs suffix arrays in O(N) expected time, albeit with lesser space efficiency. We believe that suffix arrays will prove to be better in practice than suffix trees for many applications.  
139|Linear pattern matching algorithms|In 1970, Knuth, Pratt, and Morris [1] showed how to do basic pattern matching in linear time. Related problems, such as those discussed in [4], have previously been solved by efficient but sub-optimal algorithms. In this paper, we introduce an interesting data structure called a bi-tree. A linear time algorithm  for obtaining a compacted version of a bi-tree associated with a given string is presented. With this construction as the basic tool, we indicate how to solve several pattern matching problems, including some from [4], in linear time. 
140|All-Against-All Sequence Matching |In this paper we present an algorithm which attempts to align pairs of subsequences from a database of DNA sequences. The algorithm simulates the classical dynamic programming alignment algorithm over a digital index of the database. The running time of the algorithm is subquadratic on average with respect to the database size. A similar algorithm solves the approximate string matching problem in sublinear average time. 1 Introduction  An all-against-all matching is defined as an attempt to find an alignment for each possible subsequence against each other possible subsequence in a DNA or peptide database. These alignments are done with the standard methods of dynamic programming (Needleman &amp; Wunsch algorithm [NW70]) and are based on a matrix describing the likelihood of homology between pairs of entries in the sequence (e.g. a Dayhoff matrix [DSO78]). The output of this matching will consist of all pairs of subsequences which achieve a given level of similarity. Note that we are not a...
142|The principles of psychology|This Thesis is brought to you for free and open access. It has been accepted for inclusion in University Honors Theses by an authorized administrator of
143|A Model of Saliency-based Visual Attention for Rapid Scene Analysis|A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail. Index terms: Visual attention, scene analysis, feature extraction, target detection, visual search. \Pi I. Introduction Primates have a remarkable ability to interpret complex scenes in real time, despite the limited speed of the neuronal hardware available for such tasks. Intermediate and higher visual processes appear to select a subset of the available sensory information before further processing [1], most likely to reduce the complexity of scene analysis [2]. This selection appears to be implemented in the ...
144|The Laplacian Pyramid as a Compact Image Code| We describe a technique for image encoding in which local operators of many scales but identical shape serve as the basis functions. The representation differs from established techniques in that the code elements are localized in spatial frequency as well as in space. Pixel-to-pixel correlations are first removed by subtracting a lowpass filtered copy of the image from the image itself. The result is a net data compression since the difference, or error, image has low variance and entropy, and the low-pass filtered image may represented at reduced sample density. Further data compression is achieved by quantizing the difference image. These steps are then repeated to compress the low-pass image. Iteration of the process at appropriately expanded scales generates a pyramid data structure. The encoding process is equivalent to sampling the image with Laplacian operators of many scales. Thus, the code tends to enhance salient image features. A further advantage of the present code is that it is well suited for many image analysis tasks as well as for image compression. Fast algorithms are described for coding and decoding. A
145|Shiftable Multi-scale Transforms|Orthogonal wavelet transforms have recently become a popular representation for multiscale signal and image analysis. One of the major drawbacks of these representations is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal, and in two dimensions, rotations of the input signal. We formalize these problems by defining a type of translation invariance that we call &#034;shiftability&#034;. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be considered in the context of other domains, particularly orientation and scale. We explore &#034;jointly shiftable&#034; transforms that are simultaneously shiftable in more than one domain. Two examples of jointly shiftable transforms are designed and implemented: a one-dimensional tran...
146|Preattentive texture discrimination with early vision mechanisms|mechanisms
147|Sustained and transient components of focal visual attention|Abstract-Human observers fixated the center of a search array and were required to discriminate the color of an odd target if it was present. The array consisted of horizontal or vertical black or white bars. In the simple case, only orientation was necessary to define the odd target, whereas in the conjunctive case, both orientation and color were necessary. A cue located at the critical target position was either visible all the time (sustained cuing) or it appeared at a short variable delay before the array presentation (transient cuing). Sustained visual cuing enhanced perception greatly in the conjunctive, but not in the simple condition. Perception of the odd target in the conjunctive display was improved even further by transient cuing, and peak discrimination performance occurred if the cue preceded the target array by 70-150 msec. Longer delays led to a marked downturn in performance. Control experiments indicated that this transient attentional component was independent of the observers ’ prior knowledge of target position and was not subject to voluntary control. We provide evidence to suggest hat the transient component does not originate at the earliest stages of visual processing, since it could not be extended in duration by flickering the cue, nor did it require a local sensory transient o trigger its onset. Neither the variation in retinal eccentricity nor changing the paradigm to a vernier acuity task altered the basic pattern of results. Our findings indicate the existence of a sustained and a transient component of attention, and we hypothesize that of the two, the transient component is operative at an earlier stage of visual cortical processing. Focal attention Visual search Pattern recognition Vernier acuity
148|A Comparison of Feature Combination Strategies for Saliency-Based Visual Attention Systems|Bottom-up or saliency-based visual attention allows primates to detect non-specific conspicuous targets in cluttered scenes. A classical metaphor, derived from electrophysiological and psychophysical studies, describes attention as a rapidly shiftable &#034;spotlight&#034;. The model described here reproduces the attentional scanpaths of this spotlight: Simple multi-scale &#034;feature maps&#034; detect local spatial discontinuities in intensity, color, orientation or optical flow, and are combined into a unique &#034;master&#034; or &#034;saliency&#034; map. The saliency map is sequentially scanned, in order of decreasing saliency, by the focus of attention. We study the problem of combining feature maps, from different visual modalities and with unrelated dynamic ranges (such as color and motion), into a unique saliency map. Four combination strategies are compared using three databases of natural color images: (1) Simple normalized summation, (2) linear combination with learned weights, (3) global non-linear normalization...
149|An Active Vision Architecture based on Iconic Representations|Active vision systems have the capability of continuously interacting with the environment. The rapidly changing environment of such systems means that it is attractive to replace static representations with visual routines that compute information on demand. Such routines place a premium on image data structures that are easily computed and used. The purpose of this paper is to propose a general active vision architecture based on efficiently computable iconic representations. This architecture employs two primary visual routines, one for identifying the visual image near the fovea (object identification), and another for locating a stored prototype on the retina (object location). This design allows complex visual behaviors to be obtained by composing these two routines with different parameters. The iconic representations are comprised of high-dimensional feature vectors obtained from the responses of an ensemble of Gaussian derivative spatial filters at a number of orientations and...
150|Clustered intrinsic connections in cat visual cortex|The intrinsic connections of the cortex have long been known to run vertically, across the cortical layers. In the present study we have found that individual neurons in the cat primary visual cortex can communicate over suprisingly long distances horizontally (up to 4 mm), in directions parallel to the cortical surface. For all of the cells having widespread projections, the collaterals within their axonal fields were distributed in repeating clusters, with an average periodicity of 1 mm. This pattern of extensive clustered projections has been revealed by combining the techniques of intracellular recording and injection of horseradish peroxidase with three-dimensional computer graphic reconstructions. The clustering pattern was most apparent when the cells were rotated to present a view parallel to the cortical surface. The pattern was observed in more than half of the pyramidal and spiny stellate cells in the cortex and was seen in all cortical layers. In our sample, cells made distant connections within their own layer and/or within another layer. The axon of one cell had clusters covering the same area in two layers, and the clusters in the deeper layer were located under those in the upper layer, suggesting a relationship between the clustering phenomenon and columnar cortical architecture. Some pyramidal cells did not project into the white matter,
151|Overcomplete steerable pyramid filters and rotation invariance|A given (overcomplete) discrete oriented pyramid may be converted into a steerable pyramid by interpolation. We present a technique for deriving the optimal interpolation functions (otherwise called steering coefficients). The proposed scheme is demonstrated on a computationally efficient oriented pyramid, which is a variation on the Burt and Adelson pyramid. We apply the generated steerable pyramid to orientation-invarianttexture analysis to demonstrate its excellent rotational isotropy. High classification rates and precise rotation identification are demonstrated. 1
152|Functional anatomy of macaque striate cortex. V. Spatial frequency|Macaque monkeys were shown retinotopically-specific vi-sual stimuli during %-2-deoxy-&amp;glucose (DG) infusion in a study of the retinotopic organization of primary visual cortex (Vl). In the central half of VI, the cortical magnification was found to be greater along the vertical than along the hori-zontal meridian, and overall magnification factors appeared to be scaled proportionate to brain size across different species. The cortical magnification factor (CMF) was found to reach a maximum of about 15 mm/deg at the represen-tation of the fovea, at a point of acute curvature in the Vl-V2 border. We find neither a duplication nor an overrepre-sentation of the vertical meridian. The magnification factor did not appear to be doubled in a direction perpendicular to the ocular dominance strips; it may not be increased at all. The DG borders in parvorecipient layer 4Cb were found to
153|Eye position effects on visual, memory, and saccade-related activity in areas LIP and 7a of macaque|We studied the effect of eye position on the light-sensitive, memory, and saccade-related activities of neurons of the lateral intraparietal area and area 7a in the posterior parietal cortex of rhesus monkeys. A majority of the cells showed significant effects of eye position, for each of the 3 types of response. The direction tuning of the light-sensitive, memory and saccade responses did not change with eye position but the magnitude of the response did. Since previous work showed a similar effect for the light-sensitive response of area 7a neurons (Andersen and Mountcastle, 1983; Ander-sen et al., 1985b), the present results indicate that this mod-ulating effect of eye position may be a general one, as it is found in 3 types of responses in 2 cortical areas. Gain fields were mapped by measuring the effect of eye position on the magnitude of the response at 9 different eye positions for
154|Incorporating Prior Information in Machine Learning by Creating Virtual Examples|One of the key problems in supervised learning is the insufficient size of the training set. The natural way for an intelligent learner to counter this problem and successfully generalize is to exploit prior information that may be available about the domain or that can be learned from prototypical examples. We discuss the notion of using prior knowledge by creating virtual examples and thereby expanding the effective training set size. We show that in some contexts, this idea is mathematically equivalent to incorporating the prior knowledge as a regularizer, suggesting that the strategy is well-motivated. The process of creating virtual examples in real world pattern recognition tasks is highly non-trivial. We provide demonstrative examples from object recognition and speech recognition to illustrate the idea.  1 Learning from Examples  Recently, machine learning techniques have become increasingly popular as an alternative to knowledge-based approaches to artificial intelligence pro...
155|Control of selective visual attention: Modelling the “where” pathway|Intermediate and higher vision processes require selection of a sub-set of the available sensory information before further processing. Usually, this selection is implemented in the form of a spatially circumscribed region of the visual field, the so-called &#034;focus of at-tention &#034; which scans the visual scene dependent on the input and on the attentional state of the subject. We here present a model for the control of the focus of attention in primates, based on a saliency map. This mechanism is not only expected to model the functional-ity of biological vision but also to be essential for the understanding of complex scenes in machine vision. 1 Introduction: &#034;What &#034; and &#034;Where &#034; In Vision It is a generally accepted fact that the computations of early vision are massively parallel operations, i.e., applied in parallel to all parts of the visual field. This high degree of parallelism cannot be sustained in in~ermediate and higher vision because
156|Multimodal integration for the representation of space in the posterior parietal cortex|The posterior parietal cortex has long been considered an a`ssociation&#039;area that combines information from di¡erent sensory modalities to form a cognitive representation of space. However, until recently little has been known about the neural mechanisms responsible for this important cognitive process. Recent experi-ments from the author&#039;s laboratory indicate that visual, somatosensory, auditory and vestibular signals are combined in areas LIP and 7a of the posterior parietal cortex. The integration of these signals can repre-sent the locations of stimuli with respect to the observer and within the environment. Area MSTd combines visual motion signals, similar to those generated during an observer&#039;s movement through the environment, with eye-movement and vestibular signals. This integration appears to play a role in specifying the path on which the observer is moving. All three cortical areas combine di¡erent modalities into common spatial frames by using a gain-¢eld mechanism. The spatial representations in areas LIP and 7a appear to be important for specifying the locations of targets for actions such as eye movements or reaching; the spatial representation within area MSTd appears to be important for navigation and the perceptual stabi-lity of motion signals. 1.
157|Withdrawing attention at little or no cost: detection and discrimination tasks. Percept Psychophys|Weused a concurrent-task paradigm to investigate the attentional cost of simple visual tasks. As in earlier studies, we found that detecting a unique orientation in an array of oriented elements (&#034;pop-out&#034;) carries little or no attentional cost. Surprisingly, this is true at all levels of performance and holds even when pop-out is barely discriminable. Wediscuss this finding in the context of our previous re-port that the attentional cost of stimulus detection is strongly influenced by the presence and nature of other stimuli in the display (Braun, 1994b). For discrimination tasks, we obtained a similarly mixed outcome: Discrimination of letter shape carried a high attentional cost whereas discrimination of color and orientation did not. Taken together, these findings lead us to modify our earlier position on the at-tentional costs of detection and discrimination tasks (Sagi &amp; Julesz, 1985). We now believe that ob-servers enjoy a significant degree of &#034;ambient &#034; visual awareness outside the focus of attention, per-mitting them to both detect and discriminate certain visual information. We hypothesize that the information in question is selected by a competition for saliency at the level of early vision. It has long been recognized that visual perception is in-fluenced by the observer&#039;s attentional state (Helmholtz, 1850/1962; James, 1890/1981). Psychophysical studies show that attention enhances visual sensitivity for stim-uli that are relevant to the observer and his/her behavior
158|Gapped Blast and PsiBlast: a new generation of protein database search programs|The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSIBLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.
159|Attention, similarity, and the identification-Categorization Relationship|A unified quantitative approach to modeling subjects &#039; identification and categorization of multidimensional perceptual stimuli is proposed and tested. Two subjects identified and categorized the same set of perceptually confusable stimuli varying on separable dimensions. The identification data were modeled using Sbepard&#039;s (1957) multidimensional scaling-choice framework. This framework was then extended to model the subjects &#039; categorization performance. The categorization model, which generalizes the context theory of classification developed by Medin and Schaffer (1978), assumes that subjects store category exemplars in memory. Classification decisions are based on the similarity of stimuli to the stored exemplars. It is assumed that the same multidimensional perceptual representation underlies performance in both the identification and Categorization paradigms. However, because of the influence of selective attention, similarity relationships change systematically across the two paradigms. Some support was gained for the hypothesis that subjects distribute attention among component dimensions so as to optimize categorization performance. Evidence was also obtained that subjects may have augmented their category representations with inferred exemplars. Implications of the results for theories of multidimensional scaling and categorization are discussed. 
160|Attention and learning processes in the identification and categorization of integral stimuli|The relationship between subjects &#039; identification and categorization learning of integral-dimension stimuli was studied within the framework of an exemplar-based generalization model. The model was used to predict subjects &#039; learning in six different categorization conditions on the basis of data obtained in a single identification learning condition. A crucial assumption in the model is that because of selective attention to component dimensions, similarity relations may change in systematic ways across different experimental contexts. The theoretical analysis provided evidence that, at least under unspeeded conditions, selective attention may play a critical role in determining the identification-categorization relationship for integral stimuli. Evidence was also provided that similarity among exemplars decreased as a function of identification learning. Various alternative classification models, including prototype, multiple-prototype, average distance, and &#034;value-on-dimensions&#034; models, were unable to account for the results. This article seeks to characterize performance relations between the two fundamental classification paradigms of identification and categorization. Whereas in an identification paradigm people identify stimuli as unique items (a one-to-one
161|Strategies and classification learning|How do strategies affect the learning of categories that lack necessary and suf-ficient attributes? The usual answer is that different strategies correspond to different models. In this article we provide evidence for an alternative view— Strategy variations induced by instructions affect only the amount of information represented about attributes, not the process operating on these representations. The experiment required subjects to classify schematic faces into two categories. Three groups of subjects worked with different sets of instructions: roughly, form
162|An evaluation of the identification|ollections of tiny, inexpensive wire-less sensor nodes capable of contin-uous, detailed, and unobtrusive measurement have attracted much attention in the past few years.1 Prototypes exist for applications such as early detection of factory equipment failure, opti-mization of building energy use, habitat mon-itoring, microclimate monitoring, and moni-toring structural integrity against earthquakes. Unfortunately, the very prop-erties that make sensor nodes attractive for these applica-tions—low cost, small size, wireless functioning, and timely,
163|An experimental and theoretical investigation of the constant-ratio rule and other models of visual letter confusion|The constant-ratio rule (CRR) and four interpretations of R. D. Lute’s (In R. D. Lute,
164|Classification in well-defined and ill-defined categories: Evidence for common processing strategies|had criterial features and that category membership could be determined by logical rules for the combination of features. More recent theories have assumed that categories have an ill-defined structure and have proposed probabilistic or global similarity models for the verification of category membership. In the experiments reported here, several models of categorization were compared, using one set of categories having criterial features and another set having an ill-defined structure. Schematic faces were used as exemplars in both cases. Because many models depend on distance in a multidimensional space for their predictions, in Experiment 1 a multidimensional scaling study was performed using the faces of both sets as stimuli. In Experiment 2, subjects learned the category membership of faces for the cate-gories having criterial features. After learning, reaction times for category verification and typicality judgments were obtained. Subjects also judged the similarity of pairs of faces. Since these categories had characteristic as well as defining features, it was possible to test the predictions of the feature comparison model (Smith et al.), which
165|A psychophysical approach to dimensional separability|Combinations of some physically independent dimensions appear to fuse into a single perceptual attribute, whereas combinations of other dimensions leave the dimensions perceptually distinct. This apparent difference in the perceived dis-tinctiveness of visual dimensions has previously been explained by the postulation of two types of internal representations-integral and separable. It is argued that apparent integrality, as well as its intermediate forms, can result from a single type of representation (the separable type), due to various degrees of correspondence between physical and separable psychological dimensions. Three experiments tested predictions of this new conceptualization of dimensional separability. Ex-periment 1 demonstrated that a physical dimension corresponding to a separable psychological dimension did not produce interference, whereas a physical di-mension not corresponding to a separable psychological dimension did produce interference. Experiment 2 showed that the pattern of results obtained in Exper-iment 1 could not be accounted for by similarity relations between stimuli. Ex-periment 3 showed that degrees of correspondence could account for different
166|Searching Distributed Collections With Inference Networks|The use of information retrieval systems in networked environments raises a new set of issues that have received little attention. These issues include ranking document collections for relevance to a query, selecting the best set of collections from a ranked list, and merging the document rankings that are returned from a set of collections. This paper describes methods of addressing each issue in the inference network model, discusses their implementation in the INQUERY system, and presents experimental results demonstrating their effectiveness.  
167|Okapi at TREC-3|this document length correction factor is #global&#034;: it is added at the end, after the weights for the individual terms have been summed, and is independentofwhich terms match.
168|The INQUERY Retrieval System|As larger and more heterogeneous text databases become available, information retrieval research will depend on the development of powerful, efficient and flexible retrieval engines. In this paper, we describe a retrieval system (INQUERY) that is based on a probabilistic retrieval model and provides support for sophisticated indexing and complex query formulation. INQUERY has been used successfully with databases containing nearly 400,000 documents. 1 Introduction  The increasing interest in sophisticated information retrieval (IR) techniques has led to a number of large text databases becoming available for research. The size of these databases, both in terms of the number of documents in them, and the length of the documents that are typically full text, has presented significant challenges to IR researchers who are used to experimenting with two or three thousand document abstracts. In order to carry out research with different types of text representations, retrieval models, learni...
169|Evaluation of an Inference Network-Based Retrieval Model|The use of inference networks to support document retrieval is introduced. A network-based retrieval model is described and compared to conventional probabilistic and Boolean models. The performance of a retrieval system based on the inference network model is evaluated and compared to performance with conventional retrieval models,
170|Latent Semantic Indexing (LSI) and TREC-2  (1994) |this paper. The &#034;ltc&#034; weights were computed on this matrix. 3.2 SVD analysis
171|Information Retrieval Systems for Large Document Collections|Practical information retrieval systems must manage large volumes of data, often divided into several collections that may be held on separate machines. Techniques for locating matches to queries must therefore consider identification of probable collections as well as identification of documents that are probable answers. Furthermore, the large amounts of data involved motivates the use of compression, but in a dynamic environment compression is problematic, because as new text is added the compression model slowly becomes inappropriate. In this paper we describe solutions to both of these problems. We show that use of centralised blocked indexes can reduce overall query processing costs in a multi-collection environment, and that careful application of text compression techniques allow collections to grow by several orders of magnitude without recompression becoming necessary. 1 Introduction  Practical information systems are required to store many gigabytes of data while supporting ...
172|Distributed Indexing: A Scalable Mechanism for Distributed Information Retrieval|Despite blossoming computer network bandwidths and the emergence of hypertext and CD-ROM databases, little progress has been made towards uniting the world&#039;s library-style bibliographic databases. While a few advanced distributed retrieval systems can broadcast a query to hundreds of participating databases, experience shows that local users almost always clog library retrieval systems. Hence broadcast remote queries will clog nearly every system. The premise of this work is that broadcast-based systems do not scale to world-wide systems. This project describes an indexing scheme that will permit thorough yet efficient searches of millions of retrieval systems. Our architecture will work with an arbitrary number of indexing companies and information providers, and, in the market place, could provide economic incentive for cooperation between database and indexing services. We call our scheme distributed indexing, and believe it will help researchers disseminate and locate both publishe...
173|TREC-3 Ad-Hoc, Routing Retrieval and Thresholding Experiments using PIRCS|The PIRCS retrieval system has been upgraded in TREC-3 to handle the full English collections of 2 GB in an efficient manner. For ad-hoc retrieval, we use recurrent spreading of activation in our network to implement query learning and expansion based on the best-ranked subdocuments of an initial retrieval. We also augment our standard retrieval algorithm with a soft-Boolean component. For routing, we use learning from signal-rich short documents or subdocument segments. For the optional thresholding experiment, we tried two approaches to transforming retrieval status values (RSV&#039;s) so that they could be used to partition documents into retrieved and nonretrieved sets. The first method normalizes RSV&#039;s using a query self-retrieval score. The second, which requires training data, uses logistic regression to convert RSV&#039;s into estimates of probability of relevance. Overall, our results are highly competitive with those of other participants. 1. INTRODUCTION  PIRCS is an experimental info...
174|Using Discriminant Eigenfeatures for Image Retrieval|This paper describes the automatic selection of features from an image training set using the theories of multi-dimensional linear discriminant analysis and the associated optimal linear projection. We demonstrate the effectiveness of these Most Discriminating Features for view-based class retrieval from a large database of widely varying real-world objects presented as &#034;well-framed&#034; views, and compare it with that of the principal component analysis.   
175|View-Based and Modular Eigenspaces for Face Recognition|In this work we describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of o(10^3) faces. The problem of  recognition under general viewing orientation is also explained. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose, mouth, in a eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demostrated. 
176|Probabilistic Visual Learning for Object Detection|We present an unsupervised technique for visual learning which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for a unimodal distribution) and a multivariate Mixture-of-Gaussians model (for multimodal distributions). These probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition. This learning technique is tested in experiments with modeling and subsequent detection of human faces and non-rigid objects such as hands.
177|Face Recognition From One Example View|To create a pose-invariant face recognizer, one strategy is the view-based approach, which uses a set of example views at different poses. But what if we only have one example view available, such as a scanned passport photo -- can we still recognize faces under different poses? Given one example view at a known pose, it is still possible to use the view-based approach by exploiting prior knowledge of faces to generate  virtual views, or views of the face as seen from different poses. To represent prior knowledge, we use 2D example views of prototype faces under different rotations. We will develop example-based techniques for applying the rotation seen in the prototypes to essentially &#034;rotate&#034; the single real view which is available. Next, the combined set of one real and multiple virtual views is used as example views in a view-based, pose-invariant face recognizer. Our experiments suggest that for expressing prior knowledge of faces, 2D example-based approaches should be considered ...
178|Nonlinear manifold learning for visual speech recognition|A technique for representing and learning smooth nonlinear manifolds is presented and applied to several lip reading tasks. Given a set of points drawn from a smooth manifold in an abstract feature space, the technique is capable of determining the structure of the surface and offinding the closest manifold point to a given query point. We use this technique to learn the &#034;space of lips &#034; in a visual speech recognition task. The learned manifold is used for tracking and extracting the lips, for interpolating between frames in an image sequence and for providing features for recognition. We describe a system based on Hidden Markov Models and this learned lip manifold that significantly improves the performance of acoustic speech recognizers in degraded environments. We also present preliminary results on a purely visual lip reader. 1
179|Genetic Algorithms For Object Recognition In A Complex Scene|A real-world computer vision module must deal with a wide variety of environmental parameters. Object recognition, one of the major tasks of this vision module, typically requires a preprocessing step to locate objects in the scenes that ought to be recognized. Genetic algorithms are a search technique for dealing with a very large search space, such as the one encountered in image segmentation or object recognition. This work describes a technique for using genetic algorithms to combine the image segmentation and object recognition steps for a complex scene. The results show that this approach is a viable method for successfully combining the image segmentation and object recognition steps for a computer vision module. 1. INTRODUCTION  A central task of the computer vision module is to recognize objects from images of the machine&#039;s environment. Navigation systems require the localization and recognition of landmarks or threats; robotic systems must find objects to manipulate; image re...
180|A System for Combining Traditional Alphanumeric Queries with Content-Based Queries by Example in Image Databases|Large image databases are commonly employed in applications like criminal records, customs, plant root database, and voters&#039; registration database. Efficient and convenient mechanisms for database organization and retrieval are essential. A quick and easy-to-use interface is needed which should also mesh naturally with the overall image management system. In this paper we describe the design and implementation of an integrated image database system. This system offers support for both alphanumeric query, based on alphanumeric data attached to the image file, and content-based query utilizing image examples. Content-based retrieval, specifically Query by Image Example, is made possible by the SHOSLIF approach. Alphanumeric query is implemented by a collection of parsing and query modules. All these are accessible from within a user-friendly GUI.  Key words: Image database, content-based retrieval, query by example, alphanumeric query, similarity-based query.  1 Introduction  The abilit...
181|Efficient Image Retrieval using a Network with Complex Neurons|We describe a self-organizing framework for the generation of a network useful in content-based retrieval of image databases. The system uses the theories of optimal projection for optimal feature selection and a hierarchical network structure of the image database for rapid retrieval rates. We demonstrate the query technique on a large database of widely varying real-world objects in natural settings, and show the applicability of the approach even for large variability within a particular object class.  1 Introduction  The ability of computers to rapidly and successfully retrieve information from image databases based on the objects contained in the images has a direct impact on the progress of the revolution in communication precipitated with the increasing availability of digital video [4]. The complexity in the very nature of twodimensional image data gives rise to a host of problems that alphanumeric information systems were never designed to handle [1]. A central task of these m...
182|The eyes have it: A task by data type taxonomy for information visualizations| A useful starting point for designing advanced graphical user interjaces is the Visual lnformation-Seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. This paper offers a task by data type taxonomy with seven data types (one-, two-, three-dimensional datu, temporal and multi-dimensional data, and tree and network data) and seven tasks (overview, Zoom, filter, details-on-demand, relate, history, and extracts). 
183|Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Displays|This paper offers new principles for visual information seeking (VIS). A key concept is to support browsing, which is distinguished from familiar query composition and information retrieval because of its emphasis on rapid filtering to reduce result sets, progressive refinement of search parameters, continuous reformulation of goals, and visual scanning to identify results. VIS principles developed include: dynamic query filters (query parameters are rapidly adjusted with sliders, buttons, maps, etc.), starfield displays (two-dimensional scatterplots to structure result sets and zooming to reduce clutter), and tight coupling (interrelating query components to preserve display invariants and support progressive refinement combined with an emphasis on using search output to foster search input). A FilmFinder prototype using a movie database demonstrates these principles in a VIS environment.
184|Tree visualization with Tree-maps: A 2-d space-filling approach|this paper deals with a two-dimensional (2-d) space-filling approach in which each node is a rectangle whose area is proportional to some attribute such as node size. Research on relationships between 2-d images and their representation in tree structures has focussed on node and link representations of 2-d images. This work includes quad-trees (Samet, 1989) and their variants which are important in image processing. The goal of quad trees is to provide a tree representation for storage compression and efficient operations on bit-mapped images. XY-trees (Nagy &amp; Seth, 1984) are a traditional tree representation of two-dimensional layouts found in newspaper, magazine, or book pages. Related concepts include k-d trees (Bentley and Freidman, 1979), which are often explained with the help of a
185|Treemaps: a space-filling approach to the visualization of hierarchical information structures|This paper describes a novel methodfor the visualization of hierarchically structured information. The Tree-Map visualization technique makes 100 % use of the available display space, mapping the full hierarchy onto a rectangular region in a space-filling manner. This efficient use of space allows very large hierarchies to be displayed in their entirety and facilitates the presentation of semantic information. 1
186|The Table Lens: Merging Graphical and Symbolic Representations in an Interactive Focus+Context Visualization for Tabular Information|We present a new visualization, called the Table Lens, for visualizing and making sense of large tables. The visualization uses a focus context (fisheye) technique that works effectively on tabular information because it allows display of crucial label information and multiple distal focal areas. In addition, a graphical mapping scheme for depicting table contents has been developed for the most widespread kind of tables, the cases-by-variables table. The Table Lens fuses symbolic and graphical representations into a single coherent view that can be fluidly adjusted by the user. This fusion and interactivity enables an extremely rich and natural style of direct manipulation exploratory data analysis.
187|Dynamic Queries for Information Exploration: An Implementation and Evaluation|We designed, implemented and evaluated a new concept for direct manipulation of databases, called dynamic queries, that allows users to formulate queries with graphical widgets, such as sliders. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. Eighteen undergraduate chemistry students performed statistically significantly faster usingadynamicqueries interface compared to two interfaces both providing form fillin as input method, one with graphical visualization output and one with all-textual output. The interfaces were used to expore the periodic table of elements and search on their properties. 1. INTRODUCTION Mostdatabasesystems require the user to create andformulate a complex query, whichpresumes that the user is familiar with the logical structure of the database [4]. The queries on a database are usually expressed in high level query languages (such as SQL,QUEL). This works well for many applications, but it ...
188|Visualizing the non-visual: spatial analysis and interaction with information from test documents|This paper describes an approach to IV that involves spatializing text content for enhanced visual browsing and analysis. The application arena is large text document corpora such as digital libraries, regulations andprocedures, archived reports, etc. The basic idea is that text content from these sources may be transformed to a spatial representation that preserves informational characteristics from the documents. The spatial representation may then be visually browsed and analyzed in ways that avoid language processing and that reduce the analysts’ mental workload. The result is an interaction with text that more nearly resembles perception and action with the natural world than with the abstractions of written language. 1
189|Dynamic Queries for Visual Information Seeking|Dynamic queries are a novel approach to information seeking that may enable users to cope with information overload. They allow users to see an overview of the database, rapidly (100 msec updates) explore and conveniently filter out unwanted information. Users fly through information spaces by incrementally adjusting a query (with sliders, buttons, and other filters) while continuously viewing the changing results. Dynamic queries on the chemical table of elements, computer directories, and a real estate database were built and tested in three separate exploratory experiments. These results show statistically significant performance improvements and user enthusiasm more commonly seen with video games. Widespread application seems possible but research issues remain in database and display algorithms, and user interface design. Challenges include methods for rapidly displaying and changing many points, colors, and areas; multidimensional pointing; incorporation of sound and visual displ...
190|LifeLines: Visualizing Personal Histories|LifeLines provide a general visualization environment for personal histories that can be applied to medical and court records, professional histories and other types of biographical data. A one screen overview shows multiple facets of the records. Aspects, for example medical conditions or legal cases, are displayed as individual time lines, while icons indicate discrete events, such as physician consultations or legal reviews. Line color and thickness illustrate relationships or significance, rescaling tools and filters allow users to focus on part of the information. LifeLines reduce the chances of missing information, facilitate spotting anomalies and trends, streamline access to details, while remaining tailorable and easily transferable between applications. The paper describes the use of LifeLines for youth records of the Maryland Department of Juvenile Justice and also for medical records. User&#039;s feedback was collected using a Visual Basic prototype for the youth record. Techniq...
191|Graphical Fisheye Views|A fisheye camera lens is a very wide angle lens that magnifies nearby objects while shrinking distant objects. It is a valuable tool for seeing both &#034;local detail&#034; and &#034;global context&#034; simultaneously. This paper describes a system for viewing and browsing graphs using a software analog of a fisheye lens. We first show how to implement such a view using solely geometric transformations. We then describe a more general transformation that allows global information about the graph to affect the view. Our general transformation is a fundamental extension to previous research in fisheye views.  
192|The dynamic HomeFinder: evaluating dynamic queries in a real-estate information exploration system|We designed, implemented, and evaluated a new concept for visualizing and searching databases utilizing direct manipulation called dynarruc queries. Dynamic queries allow users to formulate queries by adjusting graphical widgets, such as sliders, and see the results immediately. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. User testing was done with eighteen undergraduate students who performed significantly faster using a dynamic queries interface compared to both a natural language system and paper printouts. The interfaces were used to explore a real-estate database and find homes meeting specific search criteria. 1
193|InfoCrystal: a visual tool for information retrieval|This paper introduces a novel representation, called the I n f o C r y s t a l T M,  that can be used as a v isual i za t ion tool as well as a visual query lan-g u a g e to help users search for information. The lnfoCrysta1 visualizes all the possible relation-ships among N concepts. Users can assign relevance weights to the concepts and use thresholding to select relationships of interest. The lnfocrystal allows users to specify Boolean as well as v e c t o r-s p a c e queries graphically. Arbitrarily complex queries can be created by using the 1nfoCrystals as building blocks and organizing them in a hierarchical structure. The 1nfoCrystal enables users to explore and filter information in a flexible, dynamic and interactive way.
194|IVEE: An Information Visualization &amp; Exploration Environment|The Information Visualization and Exploration Environment (IVEE) is a system for automatic creation of dynamic queries applications. IVEE imports database relations and automatically creates environments holding visualizations and query devices. IVEE offers multiple visualizations such as maps and starfields, and multiple query devices, such as sliders, alphasliders, and toggles. Arbitrary graphical objects can be attached to database objects in visualizations. Multiple visualizations may be active simultaneously. Users can interactively lay out and change between types of query devices. Users may retrieve details-on-demand by clicking on visualization objects. An HTML file may be provided along with the database, specifying how details-ondemand information should be presented, allowing for presentation of multimedia information in database objects. Finally, multiple IVEE clients running on separate workstations on a network can communicate by letting one users actions affect the visua...
195|The Information Mural: A Technique for Displaying and Navigating Large Information Spaces|Abstract—Information visualizations must allow users to browse information spaces and focus quickly on items of interest. Being able to see some representation of the entire information space provides an initial gestalt overview and gives context to support browsing and search tasks. However, the limited number of pixels on the screen constrain the information bandwidth and make it difficult to completely display large information spaces. The Information Mural is a two-dimensional, reduced representation of an entire information space that fits entirely within a display window or screen. The Mural creates a miniature version of the information space using visual attributes, such as gray-scale shading, intensity, color, and pixel size, along with antialiased compression techniques. Information Murals can be used as stand-alone visualizations or in global navigational views. We have built several prototypes to demonstrate the use of Information Murals in visualization applications; subject matter for these views includes computer software, scientific data, text documents, and geographic information. Index Terms—Information visualization, software visualization, data visualization, focus+context, navigation, browsers. 1 INFORMATION MURALS
196|Visdb: Database exploration using multidimensional visualization|In this paper we describe the VisDB system, which allows an exploration of large databases using visualization techniques. The goal of the system is to support the query specification process by using each pixel of the display to represent one data item of the database. By arranging and coloring the pixels according to the relevance of the data items with respect to the query, the user gets a visual impression of the resulting data set. Using sliders for each condition of the query, the user may change the query dynamically and receives immediate feedback from the visual representation of the resulting data set. Different visualization techniques are available for different stages of exploration. The first technique uses multiple windows for the different query parts, providing visual feedback for each part of the query and helping the user to understand the overall result. The second technique is an extension of the first one, providing additional information by assigning two dimensions to the axes. The third technique uses a grouping of dimensions and is designed to support a focused search on smaller data sets.
197|The Alphaslider: A Compact and Rapid Selector|Research has suggested that rapid, serial, visual presentation of text (RSVP) may be an effective way to scan and search through lists of text strings in search of words, names, etc. The Alphaslider widget employs RSVP as a method for rapidly scanning and searching lists or menus in a graphical user interface environment. The Alphaslider only uses an area less than 7 cm x 2.5 cm. The tiny size of the Alphaslider allows it to be placed on a credit card, on a control panel for a VCR, or as a widget in a direct manipulation based database interface. An experiment was conducted with four Alphaslider designs which showed that novice Alphaslider users could locate one item in a list of 10,000 film titles in 24 seconds on average, an expert user in about 13 seconds.  KEYWORDS: Alphaslider, widget, selection technology, menus, dynamic queries  INTRODUCTION  Selecting items from lists is a common task in today&#039;s society. New and exciting applications for selection technology are credit card siz...
198|The Continuous Zoom: A Constrained Fisheye Technique for Viewing and Navigating Large Information Spaces|Navigating and viewing large information spaces, such as hierarchically-organized networks from complex realtime systems, suffer the problems of viewing a large space on a small screen. Distorted-view approaches, such as fisheye techniques, have great potential to reduce these problems by representing detail within its larger context but introduce new issues of focus, transition between views and user disorientation from excessive distortion. We present a fisheyebased method which supports multiple focus points, enhances continuity through smooth transitions between views, and maintains location constraints to reduce the user’s sense of spatial disorientation. These are important requirements for the representation and navigation of networked systems in supervisory control applications. The method consists of two steps: a global allocation of space to rectangular sections of the display, based on scale factors, followed by degree-of-interest adjustments. Previous versions of the algorithm relied solely on relative scale factors to assign size; we present a new version which allocates space more efficiently using a dynamically calculated degree of interest. In addition to the automatic system sizing, manual user control over the amount of space assigned each area is supported. The amount of detail shown in various parts of the network is controlled by pruning the hierarchy and presenting those sections in summary form. KEYWORDS: graphical user interface, supervisory control systems, information space, hierarchical network, information visualization, fisheye view, navigation.
199|Enhanced Dynamic Queries via Movable Filters|Traditional database query systems allow users to construct complicated database queries from specialized database language primitives. While powerful and expressive, such systems are not easy to use, especially for browsing or exploring the data. Information visualization systems address this problem by providing graphical presentations of the data and direct manipulation tools for exploring the data. Recent work in this area has reported the value of dynamic queries coupled with two-dimensional data representations for progressive refinement of user queries. However, the queries generated by these systems are limited to conjunctions of global ranges of parameter values. In this paper, we extend dynamic queries by encoding each operand of the query as a Magic Lens filter. Compound queries can be constructed by overlapping the lenses. Each lens includes a slider and a set of buttons to control the value of the filter function and to define the compostion operation generated by overlapp...
200|Navigating Large Networks with Hierarchies|This paper is aimed at the exploratory visualization of networks where there is a strength or weight associated with each link, and makes use of any hierarchy present on the nodes to aid the investigation of large networks. It describes a method of placing nodes on the plane that gives meaning to their relative positions. The paper discusses how linking and interaction principles aid the user in the exploration. Two examples are given; one of electronic mail communication over eight months within a department, another concerned with changes to a large section of a computer program. I. THE PROBLEM It has almost become a clichŽ to start a paper with the observation that the amount of data in the world is growing rapidly, and that current efforts to extract useful information from data lag far behind the ability to create data. However the clichŽ is true, and no less so in the field of network analysis and visualization than in any other. In many areas, scientists are realizing that the tools they have been using are limited in utility when applied to large, information-rich networks. Not only are networks of interest large in terms of size (as measured by number of nodes or links between nodes), but also in terms of the data collected for each node or link. The ability to examine statistics on the nodes and relate them to the network is of crucial importance. Examples of areas in which the analysis of large networks is important include: i. Trade flows. The concern in this area is monitoring imports and exports of various products at several levels; international, interstate and local. Besides examining many types of trade goods, there is also strong interest in spotting temporal patterns. ii. Communication networks. This is an important and wide category, covering not only telecommunication networks, but also electronic mail (email), financial transaction, ATM/bank data transferal and other data distribution networks.
201|Using Aggregation and Dynamic Queries for Exploring Large Data Sets|When working with large data sets, users perform three primary types of activities: data manipulation, data analysis, and data visualization. The data manipulation process involves the selection and transformation of data prior to viewing. This paper addresses user goals for this process and the interactive interface mechanisms that support them. We consider three classes of data manipulation goals: controlling the scope (selecting the desired portion of the data), selecting the focus of attention (concentrating on the attributes of data that are relevant to current analysis), and choosing the level of detail (creating and decomposing aggregates of data). We use this classification to evaluate the functionality of existing data exploration interface techniques. Based on these results, we have expanded an interface mechanism called the Aggregate Manipulator (AM) and combined it with Dynamic Query (DQ) to provide complete coverage of the data manipulation goals. We use real estate sales data to demonstrate how the AM and DQ synergistically function in our interface.
202|Interacting with Huge Hierarchies: Beyond Cone Trees|This paper describes an implementation of a tool for visualizing and interacting with huge information hierarchies. Existing systems for visualizing huge hierarchies using cone trees &#034;break down&#034; once the hierarchy to be displayed exceeds roughly 1000 nodes, due to increasing visual clutter. This paper describes a system called fsviz which visualizes arbitrarily large hierarchies while retaining user control. This is accomplished by augmenting cone trees with several graphical and interaction techniques: usage-based filtering, animated zooming, handcoupled rotation, fish-eye zooming, coalescing of nodes, texturing, effective use of colour for depth cueing, and the applications of dynamic queries. The fsviz system also improves upon earlier cone tree visualization systems through a more elaborate node layout algorithm. This algorithm enhances the usefulness of cone tree visualization for large hierarchies by all but eliminating clutter.  Keywords: Information Visualization, Information ...
203|Using treemaps to visualize the Analytic Hierarchy Process|this article. References
204|Exploratory Access to Geographic Data Based on the Map-Overlay Metaphor|Many geographic information systems (GISs) attempt to imitate the manual process of laying transparent map layers over one another on a light table and analyzing the resulting configurations. While this map-overlay metaphor, familiar to many geo-scientists, has been used as a design principle for the underlying architecture of GISs, it has not yet been visually manifested at the user interface. To overcome this shortage, a new direct manipulation user interface for overlay-based GISs has been designed and prototyped. It is characterized by the separation of map layers into data cubes and map templates such that different thematic data can be combined and the same kind of data can be displayed in different formats. This paper introduces the conceptual objects that the user manipulates at the screen surface and discusses ways to visualize effectively the objects and operations upon them.
205|Visualizing Network Data|Networks are critical to modern society, and a thorough understanding of how they behave is crucial to their efficient operation. Fortunately, data on networks is plentiful; by visualizing this data, it is possible to greatly improve our understanding. Our focus is on visualizing the data associated with a network and not on simply visualizing the structure of the network itself. We begin with three static network displays; two of these use geographical relationships, while the third is a matrix arrangement that gives equal emphasis to all network links. Static displays can be swamped with large amounts of data; hence we introduce directmanipulation techniques that permit the graphs to continue to reveal relationships in the context of much more data. In effect, the static displays are parameterized so that interesting views may easily be discovered interactively. The software to carry out this network visualization is called SeeNet.  1. INTRODUCTION  We are currently in the midst of a...
206|Networks versus Markets in International Trade|I propose a network/search view of international trade in differentiated products. I present evidence that supports the view that proximity and common language/colonial ties are more important for differentiated products than for products traded on organized exchanges in matching international buyers and sellers, and that search barriers to trade are higher for differentiated than for homogeneous products. I also discuss alternative
207|CONTINENTAL TRADING BLOCS: ARE THEY NATURAL, OR SUPER-NATURAL?|Using the gravity model, we find evidence of three continental trading blocs: the Americas, Europe and Pacific Asia. Intra-regional trade exceeds what can be explained by the proximity of a pair of countries, their sizes and GNP/capitas, and whether they share a common border or language. We then turn from the econometrics to the economic welfare implications. Krugman has supplied an argument against a three-bloc world, assuming no transport costs, and another argument in favor, assuming prohibitively high transportation costs between continents. We complete the model for the realistic case where intercontinental transport costs are neither prohibitive nor zero. If transport costs are low, continental Free Trade Areas can reduce welfare. We call such blocs super-natural. Partial liberalization is better than full liberalization within regional Preferential Trading Arrangements, despite the GATT&#039;s Article 24. The super-natural zone occurs when the regionalization of trade policy exceeds what is justified by natural factors.
208|Convergence Properties of the Nelder-Mead Simplex Method in Low Dimensions|Abstract. The Nelder–Mead simplex algorithm, first published in 1965, is an enormously popular direct search method for multidimensional unconstrained minimization. Despite its widespread use, essentially no theoretical results have been proved explicitly for the Nelder–Mead algorithm. This paper presents convergence properties of the Nelder–Mead algorithm applied to strictly convex functions in dimensions 1 and 2. We prove convergence to a minimizer for dimension 1, and various limited convergence results for dimension 2. A counterexample of McKinnon gives a family of strictly convex functions in two dimensions and a set of initial conditions for which the Nelder–Mead algorithm converges to a nonminimizer. It is not yet known whether the Nelder–Mead method can be proved to converge to a minimizer for a more specialized class of convex functions in two dimensions. Key words. direct search methods, Nelder–Mead simplex methods, nonderivative optimization AMS subject classifications. 49D30, 65K05
209|On the Convergence of Pattern Search Algorithms|. We introduce an abstract definition of pattern search methods for solving nonlinear unconstrained optimization problems. Our definition unifies an important collection of optimization methods that neither computenor explicitly approximate derivatives. We exploit our characterization of pattern search methods to establish a global convergence theory that does not enforce a notion of sufficient decrease. Our analysis is possible because the iterates of a pattern search method lie on a scaled, translated integer lattice. This allows us to relax the classical requirements on the acceptance of the step, at the expense of stronger conditions on the form of the step, and still guarantee global convergence.  Key words. unconstrained optimization, convergence analysis, direct search methods, globalization strategies, alternating variable search, axial relaxation, local variation, coordinate search, evolutionary operation, pattern search, multidirectional search, downhill simplex search  AMS(M...
210|Direct Search Methods On Parallel Machines|. This paper describes an approach to constructing derivative-free algorithms for unconstrained optimization that are easy to implement on parallel machines. A special feature of this approach is the ease with which algorithms can be generated to take advantage of any number of processors and to adapt to any cost ratio of communication to function evaluation. Numerical tests show speed-ups on two fronts. The cost of synchronization being minimal, the speed-up is almost linear with the addition of more processors, i.e., given a problem and a search strategy, the decrease in execution time is proportional to the number of processors added. Even more encouraging, however, is that different search strategies, devised to take advantage of additional (or more powerful) processors, may actually lead to dramatic improvements in the performance of the basic algorithm. Thus search strategies intended for many processors actually may generate algorithms that are better even when implemented seque...
211|Detection And Remediation Of Stagnation In The Nelder-Mead Algorithm Using A Sufficient Decrease Condition| The Nelder-Mead algorithm can stagnate and converge to a non-optimal point, even for very simple problems. In this note we propose a test for sufficient decrease which, if passed for the entire iteration, will guarantee convergence of the Nelder-Mead iteration to a stationary point if the objective function is smooth. Failure of this condition is an indicator of potential stagnation. As a remedy we propose a new step, which we call an oriented restart, which reinitializes the simplex to a smaller one with orthogonal edges which contains an approximate steepest descent step from the current best point. We also give results that apply when objective function is a low-amplitude perturbation of a smooth function. We illustrate our results with some numerical examples.
212|Fortified-Descent Simplicial Search Method: A General Approach|We propose a new simplex-based direct search method for unconstrained minimization of a realvalued function f of n variables. As in other methods of this kind, the intent is to iteratively improve an n-dimensional simplex through certain reflection/expansion/contraction steps. The method has three novel features. First, a user-chosen integer  m k specifies the number of &#034;good&#034; vertices to be retained in constructing the initial trial simplices--reflected, then either expanded or contracted--at iteration k. Second, a trial simplex is accepted only when it satisfies the criteria of fortified descent,  which are stronger than the criterion of strict descent used in most direct search methods. Third, the number of additional function evaluations needed to check a trial reflected/expanded simplex for fortified descent can be controlled. If one of the initial trial simplices satisfies the fortified descent criteria, it is accepted as the new simplex; otherwise, the simplex is shrunk a fracti...
213|Convergence of the restricted Nelder-Mead algorithm in two dimensions|The Nelder–Mead algorithm, a longstanding direct search method for unconstrained optimization published in 1965, is designed to minimize a scalar-valued function f of n real variables using only function values, without any derivative information. Each Nelder–Mead iteration is associated with a nondegenerate simplex defined by n + 1 vertices and their function values; a typical iteration produces a new simplex by replacing the worst vertex by a new point. Despite the method’s widespread use, theoretical results have been limited: for strictly convex objective functions of one variable with bounded level sets, the algorithm always converges to the minimizer; for such functions of two variables, the diameter of the simplex converges to zero, but examples constructed by McKinnon show that the algorithm may converge to a nonminimizing point. This paper considers the restricted Nelder–Mead algorithm, a variant that does not allow expansion steps. In two dimensions we show that, for any nondegenerate starting simplex and any twice-continuously differentiable function with positive definite Hessian and bounded level sets, the algorithm always converges to the minimizer. The proof is based on treating the method as a discrete dynamical system, and relies on several techniques that are non-standard in convergence proofs for unconstrained optimization.  
214|An algorithm for finding best matches in logarithmic expected time|An algorithm and data structure are presented for searching a file containing N records, each described by k real valued keys, for the m closest matches or nearest neighbors to a given query record. The computation required to organize the file is proportional to kNlogN. The expected number of records examined in each search is independent of the file size. The expected computation to perform each search is proportional-to 1ogN. Empirical evidence suggests that except for very small files, this algorithm is considerably faster than other methods.
215|Direct spatial search on pictorial databases using packed r-trees|Pictorial databases require efficient and duect spatml search based on the analog form of spatial obJects and relatlonshlps instead of search based on some cumbersome alphanumeric encodings of the pmtures R-trees (two-dimensional B-trees) are excellent devices for indexing spatial ObJects and relationships found on pictures Their most important feature 1s that they provide high level ObJect onented search rather than search based on the low level elements of spatial ObJects This paper presents an efficient initial packing technique for creatmg R-trees to index spatial ObJects Since pictorial databases are not update mtensive but rather static, the beneflts of this technique are very significant 1.
216|Refinements to Nearest-Neighbor Searching in k-Dimensional Trees| This note presents a simplification and generalization of an algorithm for searching k-dimensional trees for nearest neighbors reported by Friedman et al. I-3]. If the distance between records is measured using Lz, the Euclidean orm, the data structure used by the algorithm to determine the bounds of the search space can be simplified to a single number. Moreover, because distance measurements in L2 are rotationally invariant, the algorithm can be generalized to allow a partition plane to have an arbitrary orientation, rather than insisting that it be perpendicular to a coordinate axis, as in the original algorithm. When a k-dimensional tree is built, this plane can be found from the principal eigenvector f the covariance matrix of the records to be partitioned. These techniques and others yield variants of k-dimensional trees customized for specific applications. It is wrong to assume that k-dimensional trees guarantee that a nearest-neighbor query completes in logarithmic expected time. For small k, logarithmic behavior isobserved on all but tiny trees. However, for larger k, logarithmic behavior is achievable only with extremely large numbers of records. For k = 16, a search of a k-dimensional tree of 76,000 records examines almost every record.
217|Web Server Workload Characterization: The Search for Invariants (Extended Version)  (1996) |The phenomenal growth in popularity of the World Wide Web (WWW, or the Web) has made WWW traffic the largest contributor to packet and byte traffic on the NSFNET backbone. This growth has triggered recent research aimed at reducing the volume of network traffic produced by Web clients and servers, by using caching, and reducing the latency for WWW users, by using improved protocols for Web interaction. Fundamental to the goal of improving WWW performance is an understanding of WWW workloads. This paper presents a workload characterization study for Internet Web servers. Six different data sets are used in this study: three from academic environments, two from scientific research organizations, and one from a commercial Internet provider. These data sets represent three different orders of magnitude in server activity, and two different orders of magnitude in time duration, ranging from one week of activity to one year of activity. Throughout the study, emphasis is placed on finding wor...
218|On the Self-similar Nature of Ethernet Traffic (Extended Version)  (1994) | We demonstrate that Ethernet LAN traffic is statistically self-similar, that none of the commonly used traffic models is able to capture this fractal-like behavior, that such behavior has serious implications for the design, control, and analysis of high-speed, cell-based networks, and that aggregating streams of such traffic typically intensifies the self-similarity (“burstiness”) instead of smoothing it. Our conclusions are supported by a rigorous statistical analysis of hundreds of millions of high quality Ethernet traffic measurements collected between 1989 and 1992, coupled with a discussion of the underlying mathematical and statistical properties of self-similarity and their relationship with actual network behavior. We also present traffic models based on self-similar stochastic processes that provide simple, accurate, and realistic descriptions of traffic scenarios expected during B-ISDN deployment. 
219|Wide-Area Traffic: The Failure of Poisson Modeling|Network arrivals are often modeled as Poisson processes for analytic simplicity, even though a number of traffic studies have shown that packet interarrivals are not exponentially distributed. We evaluate 24 wide-area traces, investigating a number of wide-area TCP arrival processes (session and connection arrivals, FTP data connection arrivals within FTP sessions, and TELNET packet arrivals) to determine the error introduced by modeling them using Poisson processes. We find that user-initiated TCP session arrivals, such as remotelogin and file-transfer, are well-modeled as Poisson processes with fixed hourly rates, but that other connection arrivals deviate considerably from Poisson; that modeling TELNET packet interarrivals as exponential grievously underestimates the burstiness of TELNET traffic, but using the empirical Tcplib [Danzig et al, 1992] interarrivals preserves burstiness over many time scales; and that FTP data connection arrivals within FTP sessions come bunched into “connection bursts,” the largest of which are so large that they completely dominate FTP data traffic. Finally, we offer some results regarding how our findings relate to the possible self-similarity of widearea traffic.  
220|Characteristics of WWW Client-based Traces|The explosion of WWW traffic necessitates an accurate picture of WWW use, and in particular requires a good understanding of client requests for WWW documents. To address this need, we have collectedtraces of actual executions of NCSA Mosaic, reflecting over half a million user requests for WWW documents. In this paper we describe the methods we used to collect our traces, and the formats of the collected data. Next, we present a descriptive statistical summary of the traces we collected, which identifies a number of trends and reference patterns in WWW use. In particular, we show that many characteristics of WWW use can be modelled using power-law distributions, including the distribution of document sizes, the popularity of documents as a function of size, the distribution of user requests for documents, and the number of references to documents as a function of their overall rank in popularity (Zipf&#039;s law). Finally, we show how the power-law distributions derived from our traces can beused to guide system designers interested in caching WWW documents.
221|Empirically-Derived Analytic Models of Wide-Area TCP Connections: Extended Report|We analyze 2.5 million TCP connections that occurred during 14 wide-area traffic traces. The traces were gathered at five &#034;stub&#034; networks and two internetwork gateways, providing a diverse look at wide-area traffic. We derive analytic models describing the random variables associated with telnet, nntp, smtp, and ftp connections, and present a methodology for comparing the effectiveness of the analytic models with empirical models such as tcplib [DJ91]. Overall we find that the analytic models provide good descriptions, generally modeling the various distributions as well as empirical models and in some cases better.
222|A Caching Relay for the World Wide Web|We describe the design and performance of a caching relay for the World Wide Web. We model the distribution of requests for pages from the web and see how this distribution affects the performance of a cache. We use the data gathered from the relay to make some general characterizations about the web. (A version of this paper is available at http://www.research.digital.com/- SRC/personal/Steve Glassman/-  CachingTheWeb.html or .../CachingTheWeb.ps)  1 Overview  In January 1994, we set up a caching World Wide Web [10] relay for Digital Equipment Corporation &#039;s facilities in Palo Alto, California. We use a relay to reach the Web because Digital has a security firewall that restricts direct interaction between Digital internal computers and machines outside of Digital. We added caching to the relay because we wanted to improve the relay&#039;s performance and reduce its external network traffic. Clients use the relay for accessing the Web outside of Digital; requests for internal Digital pages...
223|Characteristics of Wide-Area TCP/IP Conversations|In this paper, we characterize wide-area network applications that use the TCP transport protocol. We also describe a new way to model the wide-area traffic generated by a stub network. We believe the traffic model presented here will be useful in studying congestion control, routing algorithms, and other resource management schemes for existing and future networks.  Our model is based on trace analysis of TCP/IP widearea internetwork traffic. We collected the TCP/IP packet headers of USC, UCB, and Bellcore networks at the point they connect with their respective regional access networks. We then wrote a handful of programs to analyze the traces. Our model characterizes individual TCP conversations by the distributions of: number of bytes transferred, duration, number of packets transferred, packet size, and packet interarrival time.  Our trace analysis shows that both interactive and bulk transfer traffic from all sites reflect a large number of short conversations. Similarly, it shows that a very large percentage of traffic is bidirectional, even for bulk transfer. We observed that interactive applications send significantly different amounts of data in each direction of a conversation, and that interarrival times for interactive applications closely follow a constant plus exponential model. Half of the conversations are directed to a handful of networks, but the other half are directed to hundreds of networks. Many of these observations contradict commonly held beliefs regarding wide-area traffic.  
224|Explaining World Wide Web Traffic Self-Similarity|Recently the notion of self-similarity has been shown to apply to wide-area and local-area network traffic. In this paper we examine the mechanisms that give rise to self-similar network traffic. We present an explanation for traffic self-similarity by using a particular subset of wide area traffic: traffic due to the World Wide Web (WWW). Using an extensive set of traces of actual user executions of NCSA Mosaic, reflecting over half a million requests for WWW documents, we show evidence that WWW traffic is selfsimilar. Then we show that the self-similarity in such traffic can be explained based on the underlying distributions of WWW document sizes, the effects of caching and user preference in file transfer, the effect of user &#034;think time&#034;, and the superimposition of many such transfers in a local area network. To do this we rely on empirically measured distributions both from our traces and from data independently collected at over thirty WWW sites. 1 Introduction  Understanding the ...
225|Growth Trends in Wide-Area TCP Connections|We analyze the growth of a medium-sized research laboratory &#039;s wide-area TCP connections over a period of more than two years. Our data consisted of six month-long traces of all TCP connections made between the site and the rest of the world. We find that smtp, ftp, and X11 traffic all exhibited exponential growth in the number of connections and bytes transferred, at rates significantly greater than that at which the site&#039;s overall computing resources grew; that individual users increasingly affected the site&#039;s traffic profile by making wide-area connections from background scripts; that the proportion of local computers participating in wide-area traffic outpaces the site&#039;s overall growth; that use of the network by individual computers appears to be constant for some protocols  (telnet) and growing exponentially for others (ftp, smtp);  and that wide-area traffic geography is diverse and dynamic. 1 Introduction  To properly design future networks, we need a thorough understanding of...
226|Application-level document caching in the Internet|With the increasing demand for document transfer services such as the World Wide Web comes a need for better resource management to reduce the latency of documents in these systems. To address this need, we analyze the potential for documentcaching at the application level in document transfer services. Wehave collected traces of actual executions of Mosaic, reflecting over half a million user requests for WWW documents. Using those traces, we study the tradeoffs between caching at three levels in the system, and the potential for use of application-level information in the caching system. Our traces show that while a high hit rate in terms of URLs is achievable, a muchlower hit rate is possible in terms of bytes, because most profitably-cached documents are small. We consider the performance of caching when applied at the level of individual user sessions, at the level of individual hosts, and at the level of a collection of hosts on a single LAN. We show that the performance gain achievable bycaching at the session level (which is straightforward to implement) is nearly all of that achievable at the LAN level (where caching is more difficult to implement). However, when resource requirements are considered, LAN level caching becomes much more desirable, since it can achieveagiven level of caching performance using a much smaller amountofcache space. Finally,we consider the use of organizational boundary information as an example of the potential for use of application-level information in caching. Our results suggest that distinguishing between documents produced locally and those produced remotely can provide useful leverage in designing caching policies, because of differences in the potential for sharing these two documenttypes among multiple users.
227|The effect of client caching on file server workloads|A distributed file systetn provides file service from one or rnore &amp;red file servers to a community of clierit workstations o’ver Q network. Wlaile the client-server paradigm has many advantages, it also presents new challenges to system designers concerning perfor-rnance and reliability. As both client workstations and file servers become increasingly well-resourced, a nu~n-ber of system design decisions need to be re-examined. This research concerus the caching of disk blocks in a distributed client-server enviromnent. Some recent re-search has suggested that various strategies for cache rnauagernent may not be equally suited to the circurn-stances at both the client and the server. Since any caching strategy is based on assumptions concerning the characteristics of the denland, the performance of the strategy is only as good as the accuracy of this assurnp-tion. The perfornlance of a caching strategy at a file server is strongly influenced by the presence of client cnches since these caches alter the characteristics of the stream of requests that reaches the server. This paper presents the results of an investigation of the effect of client caching on the nature of the server workload as a step towards understanding the performnnce of caching strategies at the server. The results demonstrate that client caches alter worklond characteristics in a way that will have CI profound impact on server cnche per-forrnance, and suggest worthwhile directions for future development of server caching strategies. 1
228|Using a Wide-Area File System Within the World-Wide Web|This paper proposes the use of a wide-area file system for storing and retrieving documents.  We demonstrate that most of the functionality of the World-Wide Web (WWW)  information service can be provided by storing documents in AFS. The approach addresses  several performance problems experienced by WWW servers and clients, such  as increased server and network load, network latency and inadequate security. In addition,  the mechanism demonstrates the value of a global, general purpose file sharing  system and the advantage of layering existing technologies.  1 Introduction  The dramatic increase in networked information access through the World-Wide Web [1] demonstrates the value of wide-area information sharing. However, the Web faces many challenges as it continues to scale to accommodate the ever increasing number of users. Among those are increased server and network load, network latency and inadequate security.  Several decades of distributed systems research address the proble...
229|Controlled and automatic human information processing: II. Perceptual learning, automatic attending and a general theory|The two-process theory of detection, search, and attention presented by Schneider and Shiffrin is tested and extended in a series of experiments. The studies demonstrate the qualitative difference between two modes of information processing: automatic detection and controlled search. They trace the course of the learning of automatic detection, of categories, and of automaticattention responses. They show the dependence of automatic detection on attending responses and demonstrate how such responses interrupt controlled processing and interfere with the focusing of attention. The learning of categories is shown to improve controlled search performance. A general framework for human information processing is proposed; the framework emphasizes the roles of automatic and controlled processing. The theory is compared to and contrasted with extant models of search and attention.
230|Controlled and automatic human information processing|A two-process theory of human information processing is proposed and applied to detection, search, and attention phenomena. Automatic processing is activa-tion of a learned sequence of elements in long-term memory that is initiated by appropriate inputs and then proceeds automatically—without subject control, without stressing the capacity limitations of the system, and without necessarily demanding attention. Controlled processing is a temporary activation of a se-quence of elements that can be set up quickly and easily but requires attention, is capacity-limited (usually serial in nature), and is controlled by the subject. A series of studies using both reaction time and accuracy measures is presented, which traces these concepts in the form of automatic detection and controlled, search through the areas of detection, search, and attention. Results in these areas are shown to arise from common mechanisms. Automatic detection is shown to develop following consistent mapping of stimuli to responses over trials. Controlled search is utilized in varied-mapping paradigms, and in our studies, it takes the form of serial, terminating search. The approach resolves a number of apparent conflicts in the literature.
231|Test stimulus representation and experimental context effects in memory scanning |The 5s performed a memory-scanning task in which they indicated whether or not a given test stimulus (letter or picture) matched one of a previously memorized set of letters. The test stimuli presented during a given session were either exclusively letters (a letter session), exclusively pictures (a picture session), or a random sequence of both (a mixed session). Reactiontime functions relating response latency to the size of the memorized set of letters were plotted, and the data are discussed in the context of the scanning models previously proposed by S. Sternberg. The reaction time functions of letter sessions and picture sessions were found to be consistent with the exhaustive model for memory scanning. However, the functions for mixed sessions deviated markedly from the predictions of such a model. The context in which a scanning task is imbedded appears to have a substantial effect on reaction time functions. Evidence that scans of information stored in short-term memory are serial and exhaustive
232|Stimulus probability and stimulus set size in memory scanning|In many recent studies of speeded scanning of immediate memory, variations in the size of the positive set (s) were confounded with variations in the probability (P) of the individual items within the positive set: As s increased, P decreased. The present experiment sought to determine whether the effect on RT attributed to s could be accounted for by variations in P. This was accomplished by factorially varying both s and P. Probability effects were confined to items in the positive set and were insufficient to account for the effect of s. The results are discussed in terms of a model in which s and P affect different information-processing stages. The s affects the number of compari-sons between the encoded item and the items stored in the memory of the positive set, as proposed by Sternberg. The P affects response selection— information as to the particular digit that was presented is available to the mechanisms for response selection along with the knowledge that there was or was not a match. The response selection mechanisms are assumed to be biased in tune with the P values of the items within the positive set. The number of things that one has to think about and the expectancy as to the likelihood of occurrence of these things— stimulus number and stimulus probability —have long been regarded as fundamental variables in the study of cognition. The common rinding that longer RTs would be produced by an increase in the number of possible stimuli or a decrease in stimulus probability was a result that was compati-ble with most theories of stimulus recogni-tion. Discriminating among the various theoretical accounts for these effects has been a more elusive task. One class of models holds that variations in stimulus probability and stimulus num-ber affect only a single commodity such as information (in bits) or repetitions. Ex-amples of such models are those that posit
234|Assessing coping strategies: A theoretically based approach|We developed a multidimensional coping inventory to assess the different ways in which people respond to stress. Five scales (of four items each) measure conceptually distinct aspects of problem-focused coping (active coping, planning, suppression of competing activities, restraint coping, seek-ing of instrumental social support); five scales measure aspects of what might be viewed as emotion-focused coping (seeking of emotional social support, positive reinterpretation, acceptance, denial, turning to religion); and three scales measure coping responses that arguably are less useful (focus on and venting of emotions, behavioral disengagement, mental disengagement). Study 1 reports the development of scale items. Study 2 reports correlations between the various coping scales and sev-eral theoretically relevant personality measures in an effort to provide preliminary information about the inventory&#039;s convergent and discriminant validity. Study 3 uses the inventory to assess coping responses among a group of undergraduates who were attempting to cope with a specific stressful episode. This study also allowed an initial examination of associations between dispositional and situational coping tendencies. Interest in the processes by which people cope with stress has grown dramatically over the past decade (cf. Moos, 1986). The
235|Approach, avoidance, and coping with stress|ABSTRACT: The study of stress and coping points to two concepts central to an understanding of the response to trauma: approach and avoidance. This pair of concepts refers to two basic modes of coping with stress. Approach and avoidance are simply metaphors for cognitive and emotional activity that is oriented either toward or away from threat. An approach-avoidance model of coping is presented in the context of contemporary theoretical ap-proaches to coping. The research literature on coping ef-fectiveness, including evidence from our laboratory, is dis-cussed, and speculations are made about he implications for future research. The study of stress and coping has become quite popular in recent years, particularly in regard to traumatic life events. Although the area is broad and the coping process
236|The JPEG still picture compression standard|This paper is a revised version of an article by the same title and author which appeared in the April 1991 issue of Communications of the ACM. For the past few years, a joint ISO/CCITT committee known as JPEG (Joint Photographic Experts Group) has been working to establish the first international compression standard for continuous-tone still images, both grayscale and color. JPEG’s proposed standard aims to be generic, to support a wide variety of applications for continuous-tone images. To meet the differing needs of many applications, the JPEG standard includes two basic compression methods, each with various modes of operation. A DCT-based method is specified for “lossy’ ’ compression, and a predictive method for “lossless’ ’ compression. JPEG features a simple lossy technique known as the Baseline method, a subset of the other DCT-based modes of operation. The Baseline method has been by far the most widely implemented JPEG method to date, and is sufficient in its own right for a large number of applications. This article provides an overview of the JPEG standard, and focuses in detail on the Baseline method. 1
237|The grid file: an adaptable, symmetric multikey file structure|Traditional file structures that provide multikey access to records, for example, inverted files, are extensions of file structures originally designed for single-key access. They manifest various deficiencies in particular for multikey access to highly dynamic files. We study the dynamic aspects of tile structures that treat all keys symmetrically, that is, file structures which avoid the distinction between primary and secondary keys. We start from a bitmap approach and treat the problem of file design as one of data compression of a large sparse matrix. This leads to the notions of a grid partition of the search space and of a grid directory, which are the keys to a dynamic file structure called the grid file. This tile system adapts gracefully to its contents under insertions and deletions, and thus achieves an upper hound of two disk accesses for single record retrieval; it also handles range queries and partially specified queries efficiently. We discuss in detail the design decisions that led to the grid file, present simulation results of its behavior, and compare it to other multikey access file structures.
238|QBISM: A Prototype 3-D Medical Image Database System|this paper. However, these automatic or semi-automatic warping algorithms are extremely important for this application. It is precisely this technology that permits anatomic structure-based access to acquired medical images as well as comparisons among studies, even of different patients, as long as they have been warped to the same atlas. Furthermore, it enables the database to grow, and be queryable, without time-consuming manual segmentation of the data.
239|Chaff: Engineering an Efficient SAT Solver|Boolean Satisfiability is probably the most studied of combinatorial optimization/search problems. Significant effort has been devoted to trying to provide practical solutions to this problem for problem instances encountered in a range of applications in Electronic Design Automation (EDA), as well as in Artificial Intelligence (AI). This study has culminated in the development of several SAT packages, both proprietary and in the public domain (e.g. GRASP, SATO) which find significant use in both research and industry. Most existing complete solvers are variants of the Davis-Putnam (DP) search algorithm. In this paper we describe the development of a new complete solver, Chaff, which achieves significant performance gains through careful engineering of all aspects of the search – especially a particularly efficient implementation of Boolean constraint propagation (BCP) and a novel low overhead decision strategy. Chaff has been able to obtain one to two orders of magnitude performance improvement on difficult SAT benchmarks in comparison with other solvers (DP or otherwise), including GRASP and SATO.
240|GRASP: A Search Algorithm for Propositional Satisfiability|AbstractÐThis paper introduces GRASP (Generic seaRch Algorithm for the Satisfiability Problem), a new search algorithm for Propositional Satisfiability (SAT). GRASP incorporates several search-pruning techniques that proved to be quite powerful on a wide variety of SAT problems. Some of these techniques are specific to SAT, whereas others are similar in spirit to approaches in other fields of Artificial Intelligence. GRASP is premised on the inevitability of conflicts during the search and its most distinguishing feature is the augmentation of basic backtracking search with a powerful conflict analysis procedure. Analyzing conflicts to determine their causes enables GRASP to backtrack nonchronologically to earlier levels in the search tree, potentially pruning large portions of the search space. In addition, by ªrecordingº the causes of conflicts, GRASP can recognize and preempt the occurrence of similar conflicts later on in the search. Finally, straightforward bookkeeping of the causality chains leading up to conflicts allows GRASP to identify assignments that are necessary for a solution to be found. Experimental results obtained from a large number of benchmarks indicate that application of the proposed conflict analysis techniques to SAT algorithms can be extremely effective for a large number of representative classes of SAT instances. Index TermsÐSatisfiability, search algorithms, conflict diagnosis, conflict-directed nonchronological backtracking, conflict-based equivalence, failure-driven assertions, unique implication points. 1
241|Using CSP look-back techniques to solve real-world SAT instances|We report on the performance of an enhanced version of the “Davis-Putnam ” (DP) proof procedure for propositional satisfiability (SAT) on large instances derived from realworld problems in planning, scheduling, and circuit diagnosis and synthesis. Our results show that incorporating CSP lookback techniques-- especially the relatively new technique of relevance-bounded learning-- renders easy many problems which otherwise are beyond DP’s reach. Frequently they make DP, a systematic algorithm, perform as well or better than stochastic SAT algorithms such as GSAT or WSAT. We recommend that such techniques be included as options in implementations of DP, just as they are in systematic algorithms for the more general constraint satisfaction problem.
242|Evidence for Invariants in Local Search|It is well known that the performance of a stochastic local search procedure depends upon the setting of its noise parameter, and that the optimal setting varies with the problem distribution. It is therefore desirable to develop general priniciples for tuning the procedures. We present two statistical measures of the local search process that allow one to quickly find the optimal noise settings. These properties are independent of the fine details of the local search strategies, and appear to be relatively independent of the structure of the problem domains. We applied these principles to the problem of evaluating new search heuristics, and discovered two promising new strategies.
243|Improvements To Propositional Satisfiability Search Algorithms|... quickly across a wide range of hard SAT problems than any other SAT tester in the literature on comparable platforms. On a Sun SPARCStation 10 running SunOS 4.1.3 U1, POSIT can solve hard random 400-variable 3-SAT problems in about 2 hours on the average. In general, it can solve hard n-variable random 3-SAT problems with search trees of size O(2  n=18:7  ).  In addition to justifying these claims, this dissertation describes the most significant achievements of other researchers in this area, and discusses all of the widely known general techniques for speeding up SAT search algorithms. It should be useful to anyone interested in NP-complete problems or combinatorial optimization in general, and it should be particularly useful to researchers in either Artificial Intelligence or Operations Research. 
244|Using Randomization and Learning to Solve Hard Real-World Instances of Satisfiability|This paper addresses the interaction between randomization, with restart strategies, and learning, an often crucial technique for proving unsatisfiability. We use instances of SAT from the hardware verification domain to provide evidence that randomization can indeed be essential in solving real-world satis able instances of SAT. More interestingly, our results indicate that randomized restarts and learning may cooperate in proving both satisfiability and unsatisfiability. Finally, we utilize and expand the idea of algorithm portfolio design to propose an alternative approach for solving hard unsatisfiable instances of SAT.
245|The impact of branching heuristics in propositional satisfiability algorithms|Abstract. This paper studies the practical impact of the branching heuristics used in Propositional Satisfiability (SAT) algorithms, when applied to solving real-world instances of SAT. In addition, different SAT algorithms are experimentally evaluated. The main conclusion of this study is that even though branching heuristics are crucial for solving SAT, other aspects of the organization of SAT algorithms are also essential. Moreover, we provide empirical evidence that for practical instances of SAT, the search pruning techniques included in the most competitive SAT algorithms may be of more fundamental significance than branching heuristics.
246|Improved algorithms for optimal winner determination in combinatorial auctions and generalizations|Combinatorial auctions can be used to reach efficient resource and task allocations in multiagent systems where the items are complementary. Determining the winners is NP-complete and inapproximable, but it was recently shown that optimal search algorithms do very well on average. This paper presents a more sophisticated search algorithm for optimal (and anytime) winner determination, including structural improvements that reduce search tree size, faster data structures, and optimizations at search nodes based on driving toward, identifying and solving tractable special cases. We also uncover a more general tractable special case, and design algorithms for solving it as well as for solving known tractable special cases substantially faster. We generalize combinatorial auctions to multiple units of each item, to reserve prices on singletons as well as combinations, and to combinatorial exchanges -- all allowing for substitutability. Finally, we present algorithms for determining the winners in these generalizations.
247|Computationally Manageable Combinatorial Auctions|There is interest in designing simultaneous auctions for situations in which the value of assets to a bidder depends upon which other assets he or she wins. In such cases, bidders may well wish to submit bids for combinations of assets. When this is allowed, the problem of determining the revenue maximizing set of nonconflicting bids can be a difficult one. We analyze this problem, identifying several different structures of combinatorial bids for which computational tractability is constructively demonstrated and some structures for which computational tractability  1 Introduction  Some auctions sell many assets simultaneously. Often these assets, like U.S. treasury bills, are interchangeable. However, sometimes the assets and the bids for them are distinct. This happens frequently, as in the U.S. Department of the Interior&#039;s simultaneous sales of off-shore oil leases, in some private farm land auctions, and in the Federal Communications Commission&#039;s recent multi-billion dollar sales...
248|Taming the computational complexity of combinatorial auctions: Optimal and approximate approaches|In combinatorial auctions, multiple goods are sold simultaneously and bidders may bid for arbitrary combinations of goods. Determining the outcome of such an auction is an optimization problem that is NP-complete in the general case. We propose two methods of overcoming this apparent intractability. The first method, which is guaranteed to be optimal, reduces running time by structuring the search space so that a modified depth-first search usually avoids even considering allocations that contain conflicting bids. Caching and pruning are also used to speed searching. Our second method is a heuristic, market-based approach. It sets up a virtual multi-round auction in which a virtual agent represents each original bid bundle and places bids, according to a fixed strategy, for each good in that bundle. We show through experiments on synthetic data that (a) our first method finds optimal allocations quickly and offers good anytime performance, and (b) in many cases our second method, despite lacking guarantees regarding optimality or running time, quickly reaches solutions that are nearly optimal. 1 Combinatorial Auctions Auction theory has received increasing attention from computer scientists in recent years. 1 One reason is the explosion of internet-based auctions. The use of auctions in business-to-business trades is also increasing rapidly [Cortese and Stepanek, 1998]. Within AI there is growing interest in using auction mechanisms to solve distributed resource allocation problems. For example, auctions and other market mechanisms are used in network bandwidth allocation, distributed configuration design, factory scheduling, and operating system memory allocation [Clearwater, 1996]. Market-oriented programming has
249|Limitations of the Vickrey Auction in Computational Multiagent Systems|Auctions provide an efficient distributed  mechanism for solving problems such as task  and resource allocation in multiagent systems.
250|eMediator: A Next Generation Electronic Commerce Server|This paper presents eMediator, an electronic commerce server prototype that demonstrates ways in which algorithmic support and game-theoretic incentive engineering can jointly improve the efficiency of ecommerce. eAuctionHouse, the configurable auction server, includes a variety of generalized combinatorial auctions and exchanges, pricing schemes, bidding languages, mobile agents, and user support for choosing an auction type. We introduce two new logical bidding languages for combinatorial markets: the XOR bidding language and the OR-of-XORs bidding language. Unlike the traditional OR bidding language, these are fully expressive. They therefore enable the use of the Clarke-Groves pricing mechanism for motivating the bidders to bid truthfully. eAuctionHouse also supports supply/demand curve bidding. eCommitter, the leveled commitment contract optimizer, determines the optimal contract price and decommitting penalties for a variety of leveled commitment contracting mechanisms, taking into account that rational agents will decommit strategically in Nash equilibrium. It also determines the optimal decommitting strategies for any given leveled commitment contract. eExchangeHouse, the safe exchange planner, enables unenforced anonymous exchanges by dividing the exchange into chunks and sequencing those chunks to be delivered safely in alternation between the buyer and the seller.
251|Some Tractable Combinatorial Auctions|Auctions are the most widely used strategic gametheoretic mechanism in the Internet. Auctions have been mostly studied from a game-theoretic and economic perspective, although recent work in AI and OR has been concerned with computational aspects of auctions as well. When faced from a computational perspective, combinatorial auctions are perhaps the most challenging type of auctions. Combinatorial auctions are auctions where agents may submit bids for bundles of goods. Given that finding an optimal allocation of the goods in a combinatorial auction is intractable, researchers have been concerned with exposing tractable instances of combinatorial auctions.
252|Image retrieval: Current techniques, promising directions and open issues|This paper provides a comprehensive survey of the technical achievements in the research area of image retrieval, especially content-based image retrieval, an area that has been so active and prosperous in the past few years. The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multidimensional indexing, and system design, three of the fundamental bases of content-based image retrieval. Furthermore, based on the state-of-the-art technology available now and the demand from real-world applications, open research issues are identified and future promising research directions are suggested. C ? 1999 Academic Press 1.
253|Photobook: Content-Based Manipulation of Image Databases|We describe the Photobook system, which is a set of interactive tools for browsing and searching images and image sequences. These query tools differ from those used in standard image databases in that they make direct use of the image content rather than relying on text annotations. Direct search on image content is made possible by use of semantics-preserving image compression, which reduces images to a small set of perceptually-significant coefficients. We describe three types of Photobook descriptions in detail: one that allows search based on appearance, one that uses 2-D shape, and a third that allows search based on textural properties. These image content descriptions can be combined with each other and with textbased descriptions to provide a sophisticated browsing and search capability. In this paper we demonstrate Photobook on databases containing images of people, video keyframes, hand tools, fish, texture swatches, and 3-D medical data.  
254|Image Indexing Using Color Correlograms|We define a new image feature called the color correlogram  and use it for image indexing and comparison. This feature distills the spatial correlation of colors, and is both effective and inexpensive for content-based image retrieval. The correlogramrobustly tolerates large changesin appearance and shape caused by changes in viewing positions, camera zooms, etc. Experimental evidence suggests that this new feature outperforms not only the traditional color histogram method but also the recently proposed histogram refinement methods for image indexing/retrieval.  
255|NeTra: A toolbox for navigating large image databases|. We present here an implementation of NeTra, a prototype image retrieval system that uses color, texture, shape and spatial location information in segmented image regions to search and retrieve similar regions from the database. A distinguishing aspect of this system is its incorporation of a robust automated image segmentation algorithm that allows object- or region-based search. Image segmentation significantly improves the quality of image retrieval when images contain multiple complex objects. Images are segmented into homogeneous regions at the time of ingest into the database, and image attributes that represent each of these regions are computed. In addition to image segmentation, other important components of the system include an efficient color representation, and indexing of color, texture, and shape features for fast search and retrieval. This representation allows the user to compose interesting queries such as &#034;retrieve all images that contain regions that have the colo...
256|Texture analysis and classification with tree-structured wavelet transform|Abstract-One difficulty of texture analysis in the past was the lack of adequate tools to characterize different scales of textures effectively. Recent developments in multiresolution analysis such as the Gabor and wavelet transforms help to overcome this difficulty. In this research, we propose a multiresolution approach based on a modified wavelet transform called the tree-structured wavelet transform or wavelet packets for texture analysis and classification. The development of this new transform is motivated by the observation that a large class of natural textures can be modeled as quasi-periodic signals whose dominant frequencies are located in the middle frequency channels. With the transform, we are able to zoom into any desired frequency channels for further decomposition. In contrast, the conventional pyramid-structured wavelet transform performs further decomposition only in low frequency channels. We develop a progressive texture classification algorithm which is not only computationally attrac-tive but also has excellent performance. The performance of our new method is compared with that of several other methods using the DCT, DST, DHT, pyramid-structured wavelet transforms, Gabor filters, and Laws filters.
257|Visual Information Retrieval|ND BUSINESSMAN CALVIN MOORES COINED the term information retrieval [10] to describe the process through which a prospective user of information can convert a request for information into a useful collection of references. &#034;Information retrieval,&#034; he wrote, &#034;embraces the intellectual aspects of the description of information and its specification for search, and also whatever systems, techniques, or machines that are employed Amarnath Gupta and Ramesh Jain 72 May 1997/Vol. 40, No. 5 COMMUNICATIONS OF THE ACM lar expressions to describe a clip. There is also a deeper reason: The information sought is inherently in the form of imagery that a textual language, however powerful, is unable to express adequately, making query processing inefficient. HE ROLE OF THE EMERGING FIELD OF visual information retrieval (VIR) systems is to go beyond text-based descri
258|Comparing Images Using Color Coherence Vectors|Color histograms are used to compare images in many applications. Their advantages are efficiency, and insensitivity to small changes in camera viewpoint. However, color histograms lack spatial information, so images with very di#erent appearances can have similar histograms. For example, a picture of fall foliage might contain a large number of scattered red pixels
259|Texture classification by wavelet packet signatures|This paper introduces a new approach tocharacterize textures at multiple scales. The performance of wavelet packet spaces are measured in terms of sensitivity and selectivity for the classi cation of twenty- ve natural textures. Both energy and entropy metrics were computed for each wavelet packet and incorporated into distinct scale space representations, where each wavelet packet (channel) re ected a speci c scale and orientation sensitivity. Wavelet packet representations for twenty- ve natural textures were classi ed without error by a simple two-layer network classi er. An analyzing function of large regularity (D 20) was shown to be slightly more e cient inrepresentation and discrimination than a similar function with fewer vanishing moments (D6). In addition, energy representations computed from the standard wavelet decomposition alone (17 features) provided classi cation without error for the twenty- ve textures included in our study. The reliability exhibited by texture signatures based on wavelet packets analysis suggest that the multiresolution properties of such transforms are bene cial for accomplishing segmentation, classication and subtle discrimination of texture. Index Terms{Feature extraction, texture analysis, texture classi cation, wavelet transform, wavelet packet, neural networks.
260|Incremental Clustering and Dynamic Information Retrieval|Motivated by applications such as document and image classification in information retrieval, we consider the problem of clustering dynamic point sets in a metric space. We propose a model called incremental clustering which is based on a careful analysis of the requirements of the information retrieval application, and which should also be useful in other applications. The goal is to efficiently maintain clusters of small diameter as new points are inserted. We analyze several natural greedy algorithms and demonstrate that they perform poorly. We propose new deterministic and randomized incremental clustering algorithms which have a provably good performance. We complement our positive results with lower bounds on the performance of incremental algorithms. Finally, we consider the dual clustering problem where the clusters are of fixed diameter, and the goal is to minimize the number of clusters.  
261|  Interactive learning using a &#034;society of models&#034; |Digital library access is driven by features, but features are often context-dependent and noisy, and their relevance for a query is not always obvious. This paper describes an approach for utilizing many data-dependent, user-dependent, and task-dependent features in a semi-automated tool. Instead of requiring universal similarity measures or manual selection of relevant features, the approach provides a learning algorithm for selecting and combining groupings of the data, where groupings can be induced by highlyspecialized and context-dependent features. The selection process is guided by arichexample-based interaction with the user. The inherent combinatorics
262|An Eigenspace Update Algorithm for Image Analysis|this paper  However, the vision research community has largely overlooked makes the following contributions:  parallel developments in signal processing and numerical linear algebra concerning efficient eigenspace updating algorithms. . We provide a comparison of some of the popular tech-  These new developments are significant for two reasons: Adopt-  niques existing in the vision literature for SVD/KLT com-  ing them will make some of the current vision algorithms more  putations and point out the problems associated with  robust and efficient. More important is the fact that incremental those techniques
263|WebSeer: An Image Search Engine for the World Wide Web|Because of the size of the World Wide Web and its inherent lack of structure, finding what one is looking for can be a challenge. PC-Meter&#039;s March, 1996, survey found that three of the five most visited Web sites were search engines. However, while Web pages typically contain both text and images, all the currently available search engines only index text. This paper describes WebSeer, a system for locating images on the Web. WebSeer uses image content in addition to associated text to index images, presenting the user with a selection that potentially fits her needs. 1 This work was supported in part by ONR contract N00014-93-1-0332 and NSF Grant No. IRI-9210763A01. 2 Introduction The explosive growth of the World Wide Web has proven to be a double-edged sword. While an immense amount of material is now easily accessible on the Web, locating specific information remains a difficult task. An inexperienced user may find it next to impossible to find the information she wants; even a...
264|Bayesian Relevance Feedback for Image Retrieval|This paper  1  describes PicHunter, an image retrieval system that implements a novel approach to relevance feedback, such that the entire history of user selections contributes to the system&#039;s estimate of the user&#039;s goal image. To accomplish this, PicHunter uses Bayesian learning based on a probabilistic model of a user&#039;s behavior. The predictions of this model are combined with the selections made during a search to estimate the probability associated with each image. These probabilities are then used to select images for display. The details of our model of a user&#039;s behavior were tuned using an offline learning algorithm. For clarity, our studies were done with the simplest possible user interface but the algorithm can easily be incorporated into systems which support complex queries, including most previously proposed systems. However, even with this constraint and simple image features, PicHunter is able to locate randomly selected targets in a database of 4522 images after displa...
266|SHAPE MEASURES FOR CONTENT BASED IMAGE RETRIEVAL: A COMPARISON|A great deal of work has been done on the evaluation of information retrieval systems for alphanumeric data. The same thing can not be said about the newly emerging multimedia and image database systems. One of the central concerns in these systems is the automatic characterization of image content and retrieval of images based on similarity of image content. In this paper, we discuss effectiveness of several shape measures for content based similarity retrieval of images. The different shape measures we have implemented include outline based features (chain code based string features, Fourier descriptors, UNL Fourier features), region based features (invariant moments, Zemike moments, pseudo-Zemike moments), and combined features (invariant moments &amp; Fourier descriptors, invariant moments &amp; UNL Fourier features). Given an image, all these shape feature measures (vectors) are computed automatically, and the feature vector can either be used for the retrieval purpose or can be stored in the database for future queries. We have tested all of the above shape features for image retrieval on a database of 500 trademark images. The average retrieval efficiency values computed over a set of fifteen representative queries for all the methods is presented. The output of a sample shape similarity query using all the features is also shown.  
267|An Optimized Interaction Strategy for Bayesian Relevance Feedback|A new algorithm and systematic evaluation is presented for searching a database via relevance feedback. It represents a new image display strategy for the PicHunter system [2, 1]. The algorithm takes feedback in the form of relative judgments (&#034;item A is more relevant than item B&#034;) as opposed to the stronger assumption of categorical relevance judgments (&#034;item A is relevant but item B is not&#034;). It also exploits a learned probabilistic model of human behavior to make better use of the feedback it obtains. The algorithm can be viewed as an extension of indexing schemes like the k-d tree to a stochastic setting, hence the name &#034;stochastic-comparison search.&#034; In simulations, the amount of feedback required for the new algorithm scales like log 2  |D|,  where  |D|  is the size of the database, while a simple query-by-exampleapproach scales like  |D|  a  , where a &lt; 1 depends on the structure of the database. This theoretical advantage is reflected by experiments with real users on a database of 1500 stock photographs.  1 
268|Edge Flow: A Framework of Boundary Detection and Image Segmentation|A novel boundary detection scheme based on &#034;edge flow&#034; is proposed in this paper. This scheme utilizes a predictive coding model to identify the direction of change in color and texture at each image location at a given scale, and constructs an edge flow vector. By iteratively propagating the edge flow, the boundaries can be detected at image locations which encounter two opposite directions of flow in the stable state. A user defined image scale is the only significant control parameter that is needed by the algorithm. The scheme facilitates integration of color and texture into a single framework for boundary detection.  1 Introduction  In most computer vision applications, the edge/boundary detection and image segmentation constitute a crucial initial step before performing high-level tasks such as object recognition and scene interpretation. However, despite considerable research and progress made in this area, the robustness and generality of the algorithms on large image datasets...
269|Supporting content-based queries over images in MARS|While advances in technology allow us to generate, transmit, and store large amounts of digital images, video, and audio, research in indexing and retrieval of multimedia information is still at its infancy. To address
270|A Society of Models for Video and Image Libraries|The average person with a computer will soon have access to the world&#039;s collections of digital video and images. However, unlike text which can be alphabetized or numbers which can be ordered, image and video has no general language to aid in its organization. Although tools which can &#034;see&#034; and &#034;understand&#034; the content of imagery are still in their infancy, they are now at the point where they can provide substantial assistance to users in navigating through visual media. This paper describes new tools based on &#034;vision texture&#034; for modeling image and video. The focus of this research is the use of a society of low-level models for performing relatively high-level tasks, such as retrieval and annotation of image and video libraries. This paper surveys our recent and present research in this fast-growing area.  1 Introduction: Vision Texture  Suppose you have a set of vacation photos of Paris and the surrounding countryside, and you accidentally drop them on the floor. They get out of or...
271|Target Testing and the PicHunter Bayesian Multimedia Retrieval System|This paper addresses how the effectiveness of a contentbased, multimedia information retrieval system can be measured, and how such a system should best use response feedback in performing searches. We propose a simple, quantifiable measure of an image retrieval system&#039;s effectiveness, &#034;target testing&#034;, in which effectiveness is measured as the average number of images that a user must examine in searching for a given random target. We describe an initial version of PicHunter, a retrieval system designed to test a novel approach to relevance-feedback. This approach is based on a Bayesian framework that incorporates an explicit model of the user&#039;s selection process. PicHunter is intentionally designed to have a minimal, &#034;queryless&#034; user interface, so that its performance reflects only the performance of the relevance feedback algorithm. The algorithm, however, can easily be incorporated into more traditional, query-based systems. Employing no explicit query, and only a small amount of i...
272|Relevance Feedback With Too Much Data|Modern text collections often contain large documents which span several subject areas.  Such documents are problematic for relevance feedback since inappropriate terms can easily  be chosen. This study explores the highly effective approach of feeding back passages of large  documents. A less-expensive method which discards long documents is also reviewed and found  to be effective if there are enough relevant documents. A hybrid approach which feeds back  short documents and passages of long documents may be the best compromise.  1  1 Introduction  As the amount of on-line text has increased, so has the size of individual documents in those collections. Information retrieval methods that could easily be applied to the full text of abstracts or short documents are sometimes less effective or prohibitively expensive for large documents. This problem has led to a resurgence of interest in techniques for handling large texts, including passage retrieval, theme identification, document su...
273|Distinguishing Photographs and Graphics on the World Wide Web|When we search for images in multimedia documents, we often have in mind specific image types that we are interested in; examples are photographs, graphics, maps, cartoons, portraits of people, and so on. This paper describes an automated system that classifies Web images as photographs or graphics, based on their content. The system first submits the images into some tests, which look at the image content, and then feeds the results of those tests into a classifier. The classifier is built using learning techniques, which take advantage of the vast amount of training data that is available on the Web. Text associated with an image can be used to further improve the accuracy of the classification. The system is used as a part of WebSeer, an image search engine for the Web. 1 Introduction Collections of multimedia documents can contain a vast amount of textual and visual information. However, the bigger the size of such collections grows, the harder it gets to locate specific informat...
274|Next-Generation Content Representation, Creation and Searching for New Media Applications in Education|Content creation, editing, and searching are extremely time consuming tasks that often require substantial  training and experience, especially when high-quality audio and video are involved. &#034;New media&#034; represents a new paradigm for multimedia information representation and processing, in which the emphasis is placed on the actual content. It thus brings the tasks of content creation and searching much closer to actual users and enables them to be active producers of audiovisual information rather than passive recipients. We discuss the state-of-the-art and present next-generation techniques for content representation, searching, creation, and editing. We discuss our experiences in developing a Web-based distributed compressed video editing and searching system (WebClip), a media representation language (Flavor) and an object-based video authoring system (Zest) based on it, and large image/video search engines for the World-Wide Web (WebSEEk and VideoQ). We also present a case study of new media applications based on specific planned multimedia education experiments with the above systems in several K-12 schools in Manhattan.
275|Toward a Visual Thesaurus|A thesaurus is a book containing synonyms in a given language; it provides similarity links when trying to retrieve articles or stories about a particular topic. A &#034;visual thesaurus&#034; works with pictures, not words. It aids in recognizing visually similar events, &#034;visual synonyms,&#034; including both spatial and motion similarity. This paper describes a method for building such a tool, and recent research results in the MIT Media Lab which contribute toward this goal. The heart of the method is a learning system which gathers information by interacting with a user of a database. The learning system is also capable of incorporating audio and other perceptual information, ultimately constructing a representation of common sense knowledge.  1 Introduction  Collections of digital imagery are growing at a rapid pace. The contexts are broad, including areas such as entertainment (e.g. searching for a funny movie scene), education (e.g. hunting down illustrations for a book report), science (e.g. ...
276|Image Indexing Using a Texture Dictionary|We propose a new method for indexing large image databases. The method incorporates neural network learning algorithms and pattern recognition techniques to construct an image pattern dictionary. Image retrieval is then formulated as a process of dictionary search to compute the best matching codeword, which in turn indexes into the database items. Experimental results are presented. Keywords: content-based image retrieval, texture, indexing, neural networks, pattern recognition, vector quantization. 1 INTRODUCTION Searching for similar image patterns in a large database can be conceptually visualized as a two step process. In the first step, a set of image processing operations are performed on the query pattern to compute a feature representation. The next step is to search through the database to retrieve patterns which are similar in the feature space. The features could be color, shape, texture, or any other image attributes of interest. Assuming that one can design appropriate ...
277|Digital Libraries: Meeting Place For High-Level And Low-Level Vision|The average person with a networked computer can now understand why computers should have vision -- to search the world&#039;s collections of digital video and images and &#034;retrieve a picture of .&#034; Computer vision for intelligent browsing, querying, and retrieval of imagery is needed now, and yet traditional approaches to computer vision remain far from a general solution to the scene understanding problem. In this paper I discuss the need for a solution based on combining high-level and low-level vision, that works in concert with input from a human user. The solution is based on: 1) Learning from the user what is important visually, and 2) Learning associations between text descriptions and visual data. I describe some recent results in these areas, and overview key challenges for future research in computer vision for digital libraries.  1. INTRODUCTION  Collections of digital imagery are growing at a rapid pace. The contexts are broad, including areas such as entertainment (e.g. searchin...
278|Computationally Fast Bayesian Recognition of Complex Objects Based on Mutual Algebraic Invariants|An effective approach has appeared in the literature for recognizing 2D curve or 3D surface objects of modest complexity based on representing an object by a single implicit polynomial of 3  rd  or 4  th  degree, computing a vector of Euclidean or affine invariants which are functions of the polynomial coefficients, and doing Bayesian object recognition of the invariants [5], thus producing low computational cost robust recognition. This paper extends the approach, as well as an initial work on mutual invariants recognizers [4], to the recognition of objects too complicated to be represented by a single polynomial(Figure 1). Hence, an object to be recognized is partitioned into patches, each patch is represented by a single implicit polynomial, mutual invariants are computed for pairs of polynomials for pairs of patches, and object recognition is Bayesian recognition of vectors of self and mutual invariants. We will discuss why complete object geometry can be captured by the geometry o...
279|Image Segmentation by Directed Region Subdivision|In this paper, an image segmentation method based on directed image region partitioning is proposed. The method consists of two separate stages: a splitting phase followed by a merging phase. The splitting phase starts with an initial coarse triangulation and employs the incremental Delaunay triangulation as a directed image region splitting technique. The triangulation process is accomplished by adding points as vertices one by one into the triangulation. A top-down point selection strategy is proposed for selecting these points in the image domain of grey-value and color images. The merging phase coalesces the oversegmentation, generated by the splitting phase, into homogeneous image regions. Because images might be negatively affected by changes in intensity due to shading or surface orientation change, we propose homogeneity criteria which are robust to intensity changes caused by these phenomena for both grey-value and color images. Performance of the image segmentation method has...
280|Computing Invariants using Elimination Methods|Geometric invariants appear to play an important role in object recognition as an aid to building model libraries of objects. Useful invariants are often found by extensive experience and they are based on geometric invariant properties studied by algebraists over many years. Given a geometric configuration, there is however a need to systematically generate and search for its invariants. In this paper we give a complete solution, in principle, to computing a single invariant for a geometric configuration, if it exists. The algorithm works in three steps: (i) the problem formulation step in which algebraic relations are established between object parameters and image parameters (or equivalently, parameters of two different images) using an imaging transformation, (ii) elimination of transformation parameters resulting in an invariant relation between object and image parameters, and (iii) finally, extraction of a single invariant from the algebraic relation. The main contribution of th...
281|Computer Learning of Subjectivity|tic control knobs.&#034; For example, the Wold model for retrieving perceptually similar visual patterns has knobs corresponding to periodicity, directionality, and randomness [2]. However, rarely can models with semantic control knobs be found. Even when they exist, it is an effort to know how to optimally set them. Moreover, usually a person does not make a single query, but a succession of queries, with slight variations each time. Therefore, she not only needs to know how to set the knobs when initiating a query session, but also how to adjust them with each new query. What I have described is the current trend in content-based retrieval and annotation systems, and it needs to change. The system must recognize that the user&#039;s goals evolve while they browse; subjectivity, mood-dependence, and fickleness are to be expected. Furthermore, a system that tracks the evolving goals of a subjective human will also be helpful for the difficult but common query sessions best described as &#034;I&#039;ll kno
282|Compressed-domain content-based image and video retrieval|With more and more visual material produced and stored in visual information systems (VIS) (i.e., image databases or video servers), the need for efficient, effective methods for indexing, searching, and retrieving images and videos from large collections has become critical. Users of large VIS will desire a more powerful method for searching images than just traditional text-based query (e.g., keywords). Manual creation of keywords
283|Visual detection in relation to display size and redundancy of critical elements |Visual detection was studied in relation to displays of discrete elements, randomly selected consonant letters, distributed in random subsets of cells of a matrix, the subject being required on each trial to indicate only which member of a predesignated pair of critical elements was present in a given display. Experimental variables were number of elements per display and number of redundant critical elements per display. Estimates of the number of elements effectively processed by a subject during a 50 ms. exposure increased with display size, but not in the manner that would be expected if the subject sampled a fixed proportion of the elements present in a display of given area. Test-retest data indicated substantial correlations over long intervals of time in the particular elements sampled by a subject
284|A Guided Tour to Approximate String Matching|We survey the current techniques to cope with the problem of string matching allowing  errors. This is becoming a more and more relevant issue for many fast growing areas such  as information retrieval and computational biology. We focus on online searching and mostly  on edit distance, explaining the problem and its relevance, its statistical behavior, its history  and current developments, and the central ideas of the algorithms and their complexities. We  present a number of experiments to compare the performance of the different algorithms and  show which are the best choices according to each case. We conclude with some future work  directions and open problems.   
285|Modern Information Retrieval|Information retrieval (IR) has changed considerably in the last years with the expansion of the Web (World Wide Web) and the advent of modern and inexpensive graphical user interfaces and mass storage devices. As a result, traditional IR textbooks have become quite out-of-date which has led to the introduction of new IR books recently. Nevertheless, we believe that there is still great need of a book that approaches the field in a rigorous and complete way from a computer-science perspective (in opposition to a user-centered perspective). This book is an effort to partially fulfill this gap and should be useful for a first course on information retrieval as well as for a graduate course on the topic. The book
286|A New Approach to Text Searching |We introduce a family of simple and fast algorithms for solving the classical string matching problem, string matching with classes of symbols, don&#039;t care symbols and complement symbols, and multiple patterns. In addition we solve the same problems allowing up to k mismatches. Among the features of these algorithms are that they don&#039;t need to buffer the input, they are real time algorithms (for constant size patterns), and they are suitable to be implemented in hardware. 1 Introduction  String searching is a very important component of many problems, including text editing, bibliographic retrieval, and symbol manipulation. Recent surveys of string searching can be found in [17, 4]. The string matching problem consists of finding all occurrences of a pattern of length  m in a text of length n. We generalize the problem allowing &#034;don&#039;t care&#034; symbols, the complement of a symbol, and any finite class of symbols. We solve this problem for one or more patterns, with or without mismatches. Fo...
287|An O(ND) Difference Algorithm and Its Variations  (1986) |The problems of finding a longest common subsequence of two sequences A and B and a shortest edit script for transforming A into B have long been known to be dual problems. In this paper, they are shown to be equivalent to finding a shortest/longest path in an edit graph. Using this perspective, a simple O(ND) time and space algorithm is developed where N is the sum of the lengths of A and B and D is the size of the minimum edit script for A and B. The algorithm performs well when differences are small (sequences are similar) and is consequently fast in typical applications. The algorithm is shown to have O(N +D    expected-time performance under a basic stochastic model. A refinement of the algorithm requires only O(N) space, and the use of suffix trees leads to an O(NlgN +D    ) time variation.
288|A fast bit-vector algorithm for approximate string matching based on dynamic programming|Abstract. The approximate string matching problem is to find all locations at which a query of length m matches a substring of a text of length n with k-or-fewer differences. Simple and practical bit-vector algorithms have been designed for this problem, most notably the one used in agrep. These algorithms compute a bit representation of the current state-set of the k-difference automaton for the query, and asymptotically run in either O(nmk/w) orO(nm log ?/w) time where w is the word size of the machine (e.g., 32 or 64 in practice), and ? is the size of the pattern alphabet. Here we present an algorithm of comparable simplicity that requires only O(nm/w) time by virtue of computing a bit representation of the relocatable dynamic programming matrix for the problem. Thus, the algorithm’s performance is independent of k, and it is found to be more efficient than the previous results for many choices of k and small m. Moreover, because the algorithm is not dependent on k, it can be used to rapidly compute blocks of the dynamic programming matrix as in the 4-Russians algorithm of Wu et al. [1996]. This gives rise to an O(kn/w) expected-time algorithm for the case where m may be arbitrarily large. In practice this new algorithm, that computes a region of the dynamic programming (d.p.) matrix w entries at a time using the basic algorithm as a subroutine, is significantly faster than our previous 4-Russians algorithm, that computes the same region 4 or 5 entries at a time using table lookup. This performance improvement yields a code that is either superior or competitive with all existing algorithms except for some filtration algorithms that are superior when k/m is sufficiently small.
289|Approximate string matching|Approximate matching of strings is reviewed with the aim of surveying techniques suitable for finding an item in a database when there may be a spelling mistake or other error in the keyword. The methods found are classified as either equivalence or similarity problems. Equivalence problems are seen to be readily solved using canonical forms. For sinuiarity problems difference measures are surveyed, with a full description of the well-establmhed dynamic programming method relating this to the approach using probabilities and likelihoods. Searches for approximate matches in large sets using a difference function are seen to be an open problem still, though several promising ideas have been suggested. Approximate matching (error correction) during parsing is briefly reviewed.
290|Practical fast searching in strings|The problem of searching through text to find a specified substring is considered in a practical setting. It is discovered that a method developed by Boyer and Moore can outperform even special-purpose search instructions that may be built into the, computer hardware. For very short substrings however, these special purpose instructions are fastest-provided that they are used in an optimal way. KEY WORDS String searching Pattern matching Text editing Bibliographic search
291|Speeding Up Two String-Matching Algorithms| We show how to speed up two string-matching algorithms: the Boyer-Moore algorithm (BM algorithm), and its version called here the reverse factor algorithm (RF algorithm). The RF algorithm is based on factor graphs for the reverse of the pattern.The main feature of both algorithms is that they scan the text right-to-left from the supposed right position of the pattern. The BM algorithm goes as far as the scanned segment (factor) is a suffix of the pattern. The RF algorithm scans while the segment is a factor of the pattern. Both algorithms make a shift of the pattern, forget the history, and start again. The RF algorithm usually makes bigger shifts than BM, but is quadratic in the worst case. We show that it is enough to remember the last matched segment (represented by two pointers to the text) to speed up the RF algorithm considerably (to make a linear number of inspections of text symbols, with small coefficient), and to speed up the BM algorithm (to make at most 2.n comparisons). Only a constant additional memory is needed for the search phase. We give alternative versions of an accelerated RF algorithm: the first one is based on combinatorial properties of primitive words, and the other two use the power of suffix trees extensively. The paper demonstrates the techniques to transform algorithms, and also shows interesting new applications of data structures representing all subwords of the pattern in compact form.
292|Transducers and repetitions|Abstract. The factor transducer of a word associates to each of its factors (or subwc~rds) their first occurrence. Optimal bounds on the size of minimal factor transducers together with an algorithm for building them are given. Analogue results and a simple algorithm are given for the case of subsequential suffix transducers. Algorithms are applied to repetition searching in words. Rl~sum~. Le transducteur des facteurs d&#039;un mot associe a chacun de ses facteurs leur premiere occurrence. On donne des bornes optimales sur la taille du transducteur minimal d&#039;un mot ainsi qu&#039;un algorithme pour sa construction. On donne des r6sultats analogues et un algorithme simple dans le cas du transducteur sous-s~luentiel des suffixes d&#039;un mot. On donne une application la d6tection de r6p6titions dans les mots. Contents
293|Faster Approximate String Matching|We present a new algorithm for on-line approximate string matching. The algorithm is based on the simulation of a non-deterministic finite automaton built from the pattern and using the text as input. This simulation uses bit operations on a RAM machine with word length w = \Omega\Gamma137 n) bits, where n is the text size. This is essentially similar to the model used in Wu and Manber&#039;s work, although we improve the search time by packing the automaton states differently. The running time achieved is O(n) for small patterns (i.e. whenever mk = O(log n)),  where m is the pattern length and k ! m the number of allowed errors. This is in contrast with the result of Wu and Manber, which is O(kn) for m = O(log n). Longer patterns can be processed by partitioning the automaton into many machine words, at O(mk=w n) search cost. We allow generalizations in the pattern, such as classes of characters, gaps and others, at essentially the same search cost. We then explore other novel techniques t...
295|Text Retrieval: Theory and Practice|We present the state of the art of the main component of text retrieval systems: the searching engine. We outline the main lines of research and issues involved. We survey recently published results for text searching and we explore the gap between theoretical vs. practical algorithms. The main observation is that simpler ideas are better in practice.  1597 Shaks. Lover&#039;s Compl. 2 From off a hill whose concaue wombe reworded A plaintfull story from a sistring vale.  OED2, reword, sistering  1 1 Introduction  Full text retrieval systems are becoming a popular way of providing support for on-line text. Their main advantage is that they avoid the complicated and expensive process of semantic indexing. From the end-user point of view, full text searching of on-line documents is appealing because a valid query is just any word or sentence of the document. However, when the desired answer cannot be obtained with a simple query, the user must perform his/her own semantic processing to guess w...
296|Block Edit Models for Approximate String Matching|In this paper we examine string block edit distance, in which two strings A and B  are compared by extracting collections of substrings and placing them into correspondence. This model accounts for certain phenomena encountered in important real-world applications, including pen computing and molecular biology. The basic problem admits a family of variations depending on whether the strings must be matched in their entireties, and whether overlap is permitted. We show that several variants are NPcomplete, and give polynomial-time algorithms for solving the remainder. Keywords: block edit distance, approximate string matching, sequence comparison, approximate ink matching, dynamic programming. 1 Introduction  The edit distance model for string comparison [Lev66, NW70, WF74] has found widespread application in fields ranging from molecular biology to bird song classification [SK83]. A great deal of research has been devoted to this area, and numerous algorithms have been proposed for com...
297|Incremental String Comparison|The problem of comparing two sequences A and B to determine their LCS or the edit distance between them has been much studied. In this paper we consider the following incremental version of these problems: given an appropriate encoding of a comparison between A and B, can one incrementally compute the answer for A and bB, and the answer for A and Bb with equal efficiency, where b is an additional symbol? Our main result is a theorem exposing a surprising relationship between the dynamic programming solutions for two such &#034;adjacent&#034; problems. Given a threshold k  on the number of differences to be permitted in an alignment, the theorem leads directly to an O(k)  algorithm for incrementally computing a new solution from an old one, as contrasts the O(k²) time required to compute a solution from scratch. We further show with a series of applications that this algorithm is indeed more powerful than its non-incremental counterpart by solving the applications with greater asymptotic ef...
298|A Comparison of Approximate String Matching Algorithms|Experimental comparison of the running time of approximate string matching algorithms for the?differences problem is presented. Given a pattern string, a text string, and integer?, the task is to find all approximate occurrences of the pattern in the text with at most?differences (insertions, deletions, changes). We consider seven algorithms based on different approaches including dynamic programming, Boyer-Moore string matching, suffix automata, and the distribution of characters. It turns out that none of the algorithms is the best for all values of the problem parameters, and the speed differences between the methods can be considerable. 2??? KEY WORDS String matching Edit distance k differences problem
299|Block Addressing Indices for Approximate Text Retrieval|Although the issue of approximate text retrieval is gaining importance in the last years, it is currently addressed by only a few indexing schemes. To reduce space requirements, the indices may point to text blocks instead of exact word positions. This is called &#034;block addressing&#034;. The most notorious index of this kind is Glimpse. However, block addressing has not been well studied yet, especially regarding approximate searching. Our main contribution is an analytical study of the spacetime trade-offs related to the block size. We find that, under reasonable assumptions, it is possible to build an index which is simultaneously sublinear in space overhead and in query time. We validate the analysis with extensive experiments, obtaining typical performance figures. These results are valid not only for approximate searching queries but also for classical ones. Finally, we propose a new strategy for approximate searching on block addressing indices, which we experimentally find 4-5 times f...
300|A Suboptimal Lossy Data Compression Based On Approximate Pattern Matching|A practical suboptimal (variable source coding) algorithm for lossy data compression is presented. This scheme is based on approximate string matching, and it naturally extends the lossless Lempel-Ziv data compression scheme. Among others we consider the typical length of approximately repeated pattern within the first n positions of a stationary mixing sequence where D% of mismatches is allowed. We prove that there exists a constant r 0 (D) such that the length of such an approximately repeated pattern converges in probability to 1=r 0 (D) log n (pr.) but it almost surely oscillates between 1=r \Gamma1 (D) log n and 2=r 1 (D) log n,  where r \Gamma1 (D) ? r 0 (D) ? r 1 (D)=2 are some constants. These constants are natural generalizations of R&#039;enyi entropies to the lossy environment. More importantly, we show that the compression ratio of a lossy data compression scheme based on such an approximate pattern matching is asymptotically equal to r 0 (D). We also establish the asymptotic be...
301|NR-grep: A Fast and Flexible Pattern Matching Tool|We present nrgrep (&#034;nondeterministic reverse grep&#034;), a new pattern matching tool designed  for efficient search of complex patterns. Unlike previous tools of the grep family, such as agrep  and Gnu grep, nrgrep is based on a single and uniform concept: the bit-parallel simulation  of a nondeterministic suffix automaton. As a result, nrgrep can find from simple patterns to  regular expressions, exactly or allowing errors in the matches, with an efficiency that degrades  smoothly as the complexity of the searched pattern increases. Another concept fully integrated  into nrgrep and that contributes to this smoothness is the selection of adequate subpatterns for  fast scanning, which is also absent in many current tools. We show that the efficiency of nrgrep  is similar to that of the fastest existing string matching tools for the simplest patterns, and by  far unpaired for more complex patterns.
302|Approximate String Matching: A Simpler Faster Algorithm|Abstract. We give two algorithms for finding all approximate matches of a pattern in a text, where the edit distance between the pattern and the matching text substring is at most k. The first algorithm, which is quite simple, runs in time O ( nk3 + n + m) on all patterns except k-break periodic m strings (defined later). The second algorithm runs in time O ( nk4 + n + m) onk-break periodic m patterns. The two classes of patterns are easily distinguished in O(m) time.
303|Large Text Searching Allowing Errors|. We present a full inverted index for exact and approximate string matching in large texts. The index is composed of a table containing the vocabulary of words of the text and a list of positions in the text corresponding to each word. The size of the table of words is usually much less than 1% of the text size and hence can be kept in main memory, where most query processing takes place. The text, on the other hand, is not accessed at all. The algorithm permits a large number of variations of the exact and approximate string search problem, such as phrases, string matching with sets of characters (range and arbitrary set of characters, complements, wild cards), approximate search with nonuniform costs and arbitrary regular expressions. The whole index can be built in linear time, in a single sequential pass over the text, takes near 1=3 the space of the text, and retrieval times are near O(  p  n)  for typical cases. Experimental results show that the algorithm works well in practice...
304|Approximate multiple string search|Abstract. This paper presents a fast algorithm for searching a large text for multiple strings allowing one error. On a fast workstation, the algo-rithm can process a megabyte of text searching for 1000 patterns (with one error) in less than a second. Although we combine several interest-ing techniques, overall the algorithm is not deep theoretically. The emphasis of this paper is on the experimental side of algorithm design. We show the importance of careful design, experimentation, and utiliza-tion of current architectures. In particular, we discuss the issues of locality and cache performance, fast hash functions, and incremental hashing techniques. We introduce the notion of two-level hashing, which utilizes cache behavior to speed up hashing, especially in cases where unsuccessful searches are not uncommon. Two-level hashing may be useful for many other applications. The end result is also interesting by itself. We show that multiple search with one error is fast enough for most text applications. 1.
305|A Practical q-Gram Index for Text Retrieval Allowing Errors|We propose an indexing technique for approximate text searching, which is practical and powerful, and especially optimized for natural language text. Unlike other indices of this kind, it is able to retrieve any string that approximately matches the search pattern, not only words. Every text substring of a fixed length q is stored in the index, together with pointers to all the text positions where it appears. The search pattern is partitioned into pieces which are searched in the index, and all their occurrences in the text are verified for a complete match. To reduce space requirements, pointers to blocks instead of exact positions can be used, which increases querying costs. We design an algorithm to optimize the pattern partition into pieces so that the total number of verifications is minimized. This is especially well suited for natural language texts, and allows to know in advance the expected cost of the search and the expected relevance of the query to the user. We show experi...
306|Episode matching |Abstract. Given two words, text T of length n and episode P of length m, the episode matching problem is to find all minimal length substrings of text T that contain episode P as a subsequence. The respective optimization problem is to find the smallest number w, s.t. text T has a subword of length w which contains episode P. In this paper, we introduce a few efficient off-line as well as on-line algorithms for the entire problem, where by on-line algorithms we mean algorithms which search from left to right consecutive text symbols only once. We present two alphabet independent algorithms which work in time O(nm). The off-line algorithm operates in O(1) additional space while the on-line algorithm pays for its property with O(m) additional space. Two other on-line algorithms have subquadratic time complexity. One of them works in time O(nm/log m) and O(m) additional space. The other one gives a time/space trade-off, i.e., it works in time O(n + s +nm log log s ~ log(s/m)) when additional space is limited to O(s). Finally, we present two approximation algorithms for the optimization problem. The off-line algorithm is alphabet independent, it has superlinear time complexity O(n/e + nloglog(n/m)) and it uses only constant space. The on-line algorithm works in time O(n/e + n) and uses space O(m). Both approximation algorithms achieve 1 + e approximation ratio, for any e&gt; 0. 1
307|Pattern Matching with Swaps|Let a text string T of n symbols and a pattern string P of m symbols from alphabet \Sigma be given. A swapped version T  0  of T is a length n string derived from T by a series of local swaps,  (i.e. t  0  ` / t `+1 and t  0  `+1 / t ` ) where each element can participate in no more than one swap.  The Pattern Matching with Swaps problem is that of finding all locations i for which there exists a swapped version T  0  of T where there is an exact matching of P in location i of T  0  . It has been an open problem whether swapped matching can be done in less than O(mn) time. In this paper we show the first algorithm that solves the pattern matching with swaps problem in time o(mn). We present an algorithm whose time complexity is O(nm  1=3  log m log  2  min(m; j\Sigmaj))  for a general alphabet \Sigma.  Key Words: Design and analysis of algorithms, combinatorial algorithms on words, pattern matching, pattern matching with swaps, non-standard pattern matching.   Department of Mathematics...
308|Applications of Approximate Word Matching in Information Retrieval|As more online databases are integrated into digital libraries, the issue of quality control of the data becomes increasingly important, especially as it relates to the effective retrieval of information. The need to discover and reconcile variant forms of strings in bibliographic entries, i.e., authority work, will become more critical in the future. Spelling variants, misspellings, and transllteration differences will all increase the difficulty of retrieving information. Approximate string matching has traditionally been used to help with this problem. In this paper we introduce the notion of approximate word matching and show how it can be used to improve detection and categorization of variant forms.
309|On the Searchability of Electronic Ink|Pen-based computers and personal digital assistant&#039;s (PDA&#039;s) are new technologies that are growing in importance. In previous papers, we have espoused a philosophy we call &#034;Computing in the Ink Domain&#034; that treats ink as a first-class datatype. One of the most important questions that arises under this model concerns the searching of large quantities of previously stored pen-stroke data. In this paper, we examine the ink search problem. We present an algorithm based on a known dynamic programming technique, and examine its performance under a variety of circumstances.  Keywords: pen computing, approximate string matching, edit distance. 1 Introduction  Despite several early, high-profile &#034;flops,&#034; pen-based computers and personal digital assistants (PDA&#039;s) are important technologies that are now starting to find acceptance. This synthesis of new hardware and software raises many systems-level issues, including the possibility of new paradigms for human-computer interaction. In previous ...
310|Multiple Approximate String Matching|We present two new algorithms for on-line multiple approximate string matching. These are extensions of previous algorithms that search for a single pattern. The single-pattern version of the first one is based on the simulation with bits of a non-deterministic finite automaton built from the pattern and using the text as input. To search for multiple patterns, we superimpose their automata, using the result as a filter. The second algorithm partitions the pattern in sub-patterns that are searched with no errors, with a fast exact multipattern search algorithm. To handle multiple patterns, we search the sub-patterns of all of them together. The average running time achieved is in both cases O(n) for moderate error level, pattern length and number of patterns. They adapt (with higher costs) to the other cases. However, the algorithms differ in speed and thresholds of usefulness. We analyze theoretically when each algorithm should be used, and show experimentally that they are faster ...
311|Multiple Approximate String Matching by Counting|. We present a very simple and efficient algorithm for online multiple approximate string matching. It uses a previously known counting-based filter [9] that searches for a single pattern by quickly discarding uninteresting parts of the text. Our multi-pattern algorithm is based on the simulation of many parallel filters using bits of the computer word. Our average complexity to search r patterns of length m is  O(rn log m= log n), being n is the text size. We can search patterns of different length, each one with a different number of errors. We show experimentally that our algorithm is competitive with the fastest known algorithms, being the fastest for a wide range of intermediate error ratios. We give the first average-case analysis of the filtering efficiency of the counting method, applicable also to [9]. 1 Introduction  A number of important problems related to string processing lead to algorithms for approximate string matching: text searching, pattern recognition, computationa...
312|Fast String Matching with Mismatches|We describe and analyze three simple and fast algorithms on the average for solving the problem of string matching with a bounded number of mismatches. These are the naive algorithm, an algorithm based on the Boyer-Moore approach, and ad-hoc deterministic finite automata searching. We include simulation results that compare these algorithms to previous works. 1 Introduction  The problem of string matching with k mismatches consists of finding all occurrences of a pattern of length m in a text of length n such that in at most k positions the text and the pattern have different symbols. In the following, we assume that 0 ! k ! m and m  n. The case of k = 0 is the well known exact string matching problem, and if k = m the solution is trivial. Landau and Vishkin [LV86] gave the first efficient algorithm to solve this particular problem. Their algorithm uses O(kn + km log m)) time and O(k(n + m)) space. While it is fast, the space required is unacceptable for most practical purposes. Galil ...
313|Improving an Algorithm for Approximate Pattern Matching|We study a recent algorithm for fast on-line approximate string matching. This is the  problem of searching a pattern in a text allowing errors in the pattern or in the text. The  algorithm is based on a very fast kernel which is able to search short patterns using a nondeterministic  finite automaton, which is simulated using bit-parallelism. A number of techniques  to extend this kernel for longer patterns are presented in that work. However, the techniques  can be integrated in many ways and the optimal interplay among them is by no means obvious.  The solution to this problem starts at a very low level, by obtaining basic probabilistic  information about the problem which was not previously known, and ends integrating analytical  results with empirical data to obtain the optimal heuristic. The conclusions obtained via analysis  are experimentally confirmed. We also improve many of the techniques and obtain a combined  heuristic which is faster than the original work.  This work sho...
314|New and Faster Filters for Multiple Approximate String Matching|We present three new algorithms for on-line multiple string matching allowing errors. These  are extensions of previous algorithms that search for a single pattern. The average running  time achieved is in all cases linear in the text size for moderate error level, pattern length and number of patterns. They adapt (with higher costs) to the other cases. However, the algorithms  differ in speed and thresholds of usefulness. We analyze theoretically when each algorithm should be used, and show experimentally their performance. The only previous solution for this  problem allows only one error. Our algorithms are the first to allow more errors, and are faster  than previous work for a moderate number of patterns (e.g. less than 50-100 on English text, depending on the pattern length). 
315|Approximate string searching under weighted edit distance|Abstract. Let p ? S * be a string of length m and t ? S * be a string of length n. The approximate string searching problem is to find all approximate matches of p in t having weighted edit distance at most k from p. We present a new method that preprocesses the pattern into a DFA which scans t online in linear time, thereby recognizing all positions in t where an approximate match ends. We show how to reduce the exponential preprocessing effort and propose two practical algorithms. The first algorithm constructs the states of the DFA up to a certain depth r = 1. It runs in O(|S | r+1 · m + q · m + n) time and O(|S | r+1 + |S | r ·m) space where q = n decreases as r increases. The second algorithm constructs the transitions of the DFA when they are demanded. It runs in O(qs·|S|+qt·m+n) time and O(qs·(|S|+m)) space where qs = qt = n depend on the problem instance. Practical measurements show that our algorithms work well in practice and beat previous methods for problems of interest in molecular biology. 1
316|A Unified View to String Matching Algorithms|  We present a unified view to sequential algorithms for many  pattern matching problems, using a finite automaton built from the pattern  which uses the text as input. We show the limitations of deterministic  finite automata (DFA) and the advantages of using a bitwise  simulation of non-deterministic finite automata (NFA). This approach  gives very fast practical algorithms which have good complexity for small  patterns on a RAM machine with word length O(log n), where n is the  size of the text. For generalized string matching the time complexity is  O(mn= log n) which for small patterns is linear. For approximate string  matching we show that the two main known approaches to the problem  are variations of the NFA simulation. For this case we present a different  simulation technique which gives a running time of O(n) independently  of the maximum number of errors allowed, k, for small patterns. This  algorithm improves the best bit-wise or comparison based algorithms of  running ti...
317|A Partial Deterministic Automaton for Approximate String Matching|. One of the simplest approaches to approximate string matching is to consider the associated non-deterministic finite automaton and make it deterministic. Besides automaton generation, the search time is  O(n) in the worst case, where n is the text size. This solution is mentioned in the classical literature but has not been further pursued, due to the large number of automaton states that may be generated. We study the idea of generating the deterministic automaton on the fly. That is, we only generate the states that are actually reached when the text is traversed. We show that this limits drastically the number of states actually generated. Moreover, the algorithm is competitive, being the fastest one for intermediate error ratios and pattern lengths. 1 Introduction  Approximate string matching is one of the main problems in classical string algorithms, with applications to text searching, computational biology, pattern recognition, etc. The problem is defined as follows: given a t...
318|Improved Approximate Pattern Matching on Hypertext|. The problem of approximate pattern matching on hypertext is defined and solved by Amir et al. in O(m(n log m + e)) time, where  m is the length of the pattern, n is the total text size and e is the total number of edges. Their space complexity is O(mn). We present a new algorithm which is O(mk(n + e)) time and needs only O(n) extra space, where k ! m is the number of allowed errors in the pattern. If the graph is acyclic, our time complexity drops to O(m(n + e)), improving Amir&#039;s results. 1 Introduction  Approximate string matching problems appear in a number of important areas related to string processing: text searching, pattern recognition, computational biology, audio processing, etc. The edit distance between two strings a and b, ed(a; b), is defined as the minimum number of edit operations that must be carried out to make them equal. The allowed operations are insertion, deletion and substitution of characters in  a or b. The problem of approximate string matching is defined as...
319|Estimating the Probability of Approximate Matches|this paper addresses how to define S k (P ) and how to solve the algorithmic sub-problems involved in an efficient realization with respect to this definition. Section 2 introduces as our choice for S k (P ) the set of what we call the condensed, canonical edit scripts. Our choice attempts to keep small, both (i) the number of edit scripts for which X(s) = 0, and (ii) the size of g(v). Doing so improves the convergence of the estimator as it places S k (P ) and CN k (P ) in closer correspondence. The remaining sections present dynamic programming algorithms for the following subtasks:
320|Efficient Algorithms for Approximate String Matching with Swaps|this paper we include the swap operation that interchanges two adjacent characters  into the set of allowable edit operations, and we present an O(t min(m, n))-time  algorithm for the extended edit distance problem, where t is the edit distance  between the given strings, and an O(kn)-time algorithm for the extended k-differ-  ences problem. That is, we add swaps into the set of edit operations without  increasing the time complexities of previous algorithms that consider only changes,  insertions, and deletions for the edit distance and k-differences problems. # 1999  Academic Press  1. INTRODUCTION  Given two strings A[1}}}m] and B[1}}}n] over an alphabet 7, the edit distance between A and&lt;F12
321|Fast Multi-Dimensional Approximate Pattern Matching|. We address the problem of approximate string matching in  d dimensions, that is, to find a pattern of size m  d  in a text of size n  d  with at most k ! m  d  errors (substitutions, insertions and deletions along any dimension). We use a novel and very flexible error model, for which there exists only an algorithm to evaluate the similarity between two elements in two dimensions at O(m  4  ) time. We extend the algorithm to d dimensions, at O(d!m  2d  ) time and O(d!m  2d\Gamma1  ) space. We also give the first search algorithm for such model, which is O(d!m  d  n  d  ) time and O(d!m  d  n  d\Gamma1  ) space. We show how to reduce the space cost to O(d!3  d  m  2d\Gamma1  ) with little time penalty. Finally, we present the first sublinear-time (on average) searching algorithm (i.e. not all text cells are inspected), which is O(kn  d  =m  d\Gamma1  ) for k ! (m=(d(log oe m \Gamma log oe d)))  d\Gamma1  , where oe is the alphabet size. After that error level the filter still remains ...
322|Adaptive Constraint Satisfaction|Many different approaches have been applied to constraint satisfaction. These range from complete backtracking algorithms to sophisticated distributed configurations. However, most research effort in the field of constraint satisfaction algorithms has concentrated on the use of a single algorithm for solving all problems. At the same time, a consensus appears to have developed to the effect that it is unlikely that any single algorithm is always the best choice for all classes of problem. In this paper we argue that an adaptive approach should play an important part in constraint satisfaction. This approach relaxes the commitment to using a single algorithm once search commences. As a result, we claim that it is possible to undertake a more focused approach to problem solving, allowing for the correction of bad algorithm choices and for capitalising on opportunities for gain by dynamically changing to more suitable candidates.
323|Minimizing conflicts: a heuristic repair method for constraint satisfaction and scheduling problems| This paper describes a simple heuristic approach to solving large-scale constraint satisfaction and scheduling problems. In this approach one starts with an inconsistent assignment for a set of variables and searches through the space of possible repairs. The search can be guided by a value-ordering heuristic, the min-conflicts heuristic, that attempts to minimize the number of constraint violations after each step. The heuristic can be used with a variety of different search strategies. We demonstrate empirically that on the n-queens problem, a technique based on this approach performs orders of magnitude better than traditional backtracking techniques. We also describe a scheduling application where the approach has been used successfully. A theoretical analysis is presented both to explain why this method works well on certain types of problems and to predict when it is likely to be One of the most promising general approaches for solving combinatorial search problems is to generate an
324|Algorithms for Constraint-Satisfaction Problems: A Survey|A large number of problems in AI and other areas of computer science can be viewed as special cases of the constraint-satisfaction problem. Some examples are machine vision, belief maintenance, scheduling, temporal reasoning, graph problems, floor plan design, the planning of genetic experiments, and the satisfiability problem. A number of different approaches have been developed for solving these problems. Some of them use constraint propagation to simplify the original problem. Others use backtracking to directly search for possible solutions. Some are a combination of these two techniques. This article overviews many of these approaches in a tutorial fashion.  
325|Noise strategies for improving local search|It has recently been shown that local search issurprisingly good at nding satisfying assignments for certain computationally hard classes of CNF formulas. The performance of basic local search methods can be further enhanced by introducing mechanisms for escaping from local minima in the search space. We will compare three such mechanisms: simulated annealing, random noise, and a strategy called \mixed random walk&#034;. We show that mixed random walk is the superior strategy. Wealso present results demonstrating the e ectiveness of local search withwalk for solving circuit synthesis and circuit diagnosis problems. Finally, wedemonstrate that mixed random walk improves upon the best known methods for solving MAX-SAT problems.
326|Hybrid Algorithms for the Constraint Satisfaction Problem|problem (csp), namely, naive backtracking (BT), backjumping (BJ), conflict-directed backjumping
327|Domain-Independent Extensions to GSAT: Solving Large Structured Satisfiability Problems|GSAT is a randomized local search procedure  for solving propositional satisfiability  problems (Selman et al. 1992). GSAT can  solve hard, randomly generated problems that  are an order of magnitude larger than those  that can be handled by more traditional approaches  such as the Davis-Putnam procedure.  GSAT also efficiently solves encodings  of graph coloring problems, N-queens, and  Boolean induction. However, GSAT does not  perform as well on handcrafted encodings of  blocks-world planning problems and formulas  with a high degree of asymmetry. We  present three strategies that dramatically improve  GSAT&#039;s performance on such formulas.  These strategies, in effect, manage to uncover  hidden structure in the formula under considerations,  thereby significantly extending the  applicability of the GSAT algorithm.  
328|Practical Applications of Constraint Programming|Constraint programming is newly flowering in industry. Several companies have recently started up to exploit the technology, and the number of industrial applications is now growing very quickly. This survey will seek, by examples,
329|Genet: A connectionist architecture for solving constraint satisfaction problems by iterative improvement|New approaches to solving constraint satisfaction problems using iterative improvement techniques have been found to be successful on certain, very large problems such as the million queens. However, on highly constrained problems it is possible for these methods to get caught in local minima. In this paper we present genet, a connectionist architecture for solving binary and general constraint satisfaction problems by iterative improvement. genet incorporates a learning strategy to escape from local minima. Although genet has been designed to be implemented on vlsi hardware, we present empirical evidence to show that even when simulated on a single processor genet can outperform existing iterative improvement techniques on hard instances of certain constraint satisfaction problems.
330|The Birth of Prolog|The programming language, Prolog, was born of a project aimed not at producing a programming language but at processing natural languages; in this case, French. The project gave rise to a preliminary version of Prolog at the end of 1971 and a more definitive version at the end of 1972. This article gives the history of this project and describes in detail the preliminary and then the final versions of Prolog. The authors also felt it appropriate to describe the Q-systems since it was a language which played a prominent part in Prolog’s genesis.
331|Guided local search and its application to the traveling salesman problem|The Traveling Salesman Problem (TSP) is one of the most famous problems in combinatorial optimization. In this paper, we are going to examine how the techniques of Guided Local Search (GLS) and Fast Local Search (FLS) can be applied to the problem. Guided Local Search sits on top of local search heuristics and has as a main aim to guide these procedures in exploring efficiently and effectively the vast search spaces of combinatorial optimization problems. Guided Local Search can be combined with the neighborhood reduction scheme of Fast Local Search which significantly speeds up the operations of the algorithm. The combination of GLS and FLS with TSP local search heuristics of different efficiency and effectiveness is studied in an effort to determine the dependence of GLS on the underlying local search heuristic used. Comparisons are made with some of the best TSP heuristic algorithms and general optimization techniques which demonstrate the advantages of GLS over alternative heuristic approaches suggested for the problem.
332|An Empirical Analysis of Search in GSAT|We describe an extensive study of search in GSAT, an approximation procedure for  propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied  clauses in a truth assignment. Our experiments provide a more complete picture of GSAT&#039;s  search than previous accounts. We describe in detail the two phases of search: rapid hillclimbing  followed by a long plateau search. We demonstrate that when applied to randomly  generated 3-SAT problems, there is a very simple scaling with problem size for both the  mean number of satisfied clauses and the mean branching rate. Our results allow us to  make detailed numerical conjectures about the length of the hill-climbing phase, the average  gradient of this phase, and to conjecture that both the average score and average branching  rate decay exponentially during plateau search. We end by showing how these results can  be used to direct future theoretical analysis. This work provides a case study of how  computer experiments can be used to improve understanding of the theoretical properties  of algorithms.  1. 
334|Adaptive Constraint Satisfaction: The Quickest First Principle|The choice of a particular algorithm for solving a given class of constraint satisfaction problems is often confused by exceptional behaviour of algorithms. One method of reducing the impact of this exceptional behaviour is to adopt an adaptive philosophy to constraint satisfaction problem solving. In this report we describe one such adaptive algorithm, based on the principle of chaining. It is designed to avoid the phenomenon of exceptionally hard problem instances. Our algorithm shows how the speed of more naïve algorithms can be utilised safe in the knowledge that the exceptional behaviour can be bounded. Our work clearly demonstrates the potential benefits of the adaptive approach and opens a new front of research for the constraint satisfaction community.
335|Partial constraint satisfaction problems and guided local search|A largely unexplored aspect of Constraint Satisfaction Problem (CSP) is that of over-constrained instances for which no solution exists that satisfies all the constraints. In these problems, mentioned in the literature as Partial Constraint Satisfaction Problems (PCSPs), we are often looking for solutions which violate the minimum number of constraints. In more realistic settings, constraints violations incur different costs and solutions are sought that minimize the total cost from constraint violations and possibly other criteria. Problems in this category present enormous difficulty to complete search algorithms. In practical terms, complete search has more or less to resemble the traditional Branch and Bound taking no advantage of the efficient pruning techniques recently developed for CSPs. In this report, we examine how the stochastic search method of Guided Local Search (GLS) can be applied to these problems. The effectiveness of the method is demonstrated on instances of the Radio Link Frequency Assignment Problem (RLFAP), which is a real-world Partial CSP.
336|Solving Constraint Satisfaction Problems with Heuristic-based Evolutionary Algorithms|Evolutionary algorithms (EAs) for solving constraint satisfaction problems  (CSPs) can be roughly divided into two classes: EAs using adaptive  fitness functions and EAs using heuristics. In [5] the most effective EAs of the  first class have been compared experimentally using a large set of benchmark  instances consisting of randomly generated binary CSPs. In this paper we  complete this comparison by studying the most effective EAs that use heuristics.
337|Constraint Logic Programming for Scheduling and Planning|This paper provides an introduction to Finite-domain Constraint Logic Programming (CLP) and its application to problems in scheduling and planning. We cover the fundamentals of CLP and indicate recent developments and trends in the field. Some current limitations are identified, and areas of research that may contribute to addressing these limitations are suggested.
338|An attempt to map the performance of a range of algorithm and heuristic combinations|Constraint satisfaction is the core of many AI and real life problems and much research has been done in this field in recent years. Work has been done in the past on comparing the performance of different algorithms and heuristics. Much of such work has focused on finding &#034;the best&#034; algorithm and heuristic combination for all problems. The objective of this paper is to prove that there is no universally best algorithm and heuristic for all problems-- different problems can be solved most efficiently by different algorithm and heuristic combinations. The implication of this is important because it means that instead of trying to find &#034;the best &#034; algorithms and heuristics, future research should try to identify the application domain of each algorithm and heuristic (i.e. when they are most effective). Furthermore our results point to future research which focuses on how to retrieve the most efficient algorithm for a given problem. The results in this paper provide a first step towards achieving such goals. 
339|On the Selection of Constraint Satisfaction Problem Formulations|This paper outlines a possible method for discriminating between formulations of the same problem. We attempt to relate different ZDC formulations in terms of their relative difficulty. This difficulty is quantified in terms of a new measure known as the T-factor. The result of our work is to demonstrate that in some cases, when very different formulations of the same problem exist, it is possible to identify the formulation that is most likely to be easiest to solve. In the next section we define the T-factors of formulations. In section 3 we present alternative formulations of the well known N-Queens and Zebra problems, together with evaluation of their T-factors. Finally in section 4 we discuss our findings and propose directions for future work.
340|Tackling car sequencing problems using a generic genetic algorithm|The car sequencing problem (CarSP) was seen as a challenge to artificial intelligence. The CarSP is a version of the job-shop scheduling problem which is known to be NP-complete. The task in the CarSP is to schedule a given number of cars (of different types) in a sequence to allow the teams in each work station on the assembly line to fit the required options (e.g. radio, sunroof) on the cars within the capacity of that work station. In unsolvable problems, one would like to minimize the penalties associated to the violation of the capacity constraints. Previous attempts to tackle the problem have either been unsuccessful or restricted to solvable CarSPs only. In this paper, we report on promising results in applying a generic genetic algorithm, which we call GAcSP, to tackle both solvable and unsolvable CarSPs.
341|Ng-Backmarking - an Algorithm for Constraint Satisfaction|Ng-backmarking with Min-conflict repair, a hybrid algorithm for solving constraint satisfaction problems, is presented in the context of the four main approaches to constraint satisfaction and optimisation: tree-search, domainfiltering, solution repair, and learning while searching. Repair-based techniques are often designed to use local gradients to direct the search for a solution to a constraint satisfaction problem. It has been shown experimentally that such techniques are often well suited to solving large scale problems. One drawback is that they do not guarantee a (optimal) solution if one exists. The motivation behind ng-backmarking is to allow the search to follow local gradients in the search space whilst ensuring a (optimal) solution if one exists. The search space of this combined approach is controlled by the ng-backmarking process, a method of learning constraints during search (at each failure point  1  ) that may be used to avoid the repeated traversing of failed paths ...
342|The architecture of complexity|A number of proposals have been advanced in recent years for the development of “general systems theory ” that, abstracting from properties peculiar to physical, biological, or social systems, would be applicable to all of them. 1 We might well feel that, while the goal is laudable, systems of such diverse kinds could hardly be expected to have any nontrivial properties in common. Metaphor and analogy can be helpful, or they can be misleading. All depends on whether the similarities the metaphor captures are significant or superficial. It may not be entirely vain, however, to search for common properties among diverse kinds of complex systems. The ideas that go by the name of cybernetics constitute, if not a theory, at least a point of view that has been proving fruitful over a wide range of applications. 2 It has been useful to look at the behavior of adaptive systems in terms of the concepts of feedback and homeostasis, and to analyze adaptiveness in terms of the theory of selective information. 3 The ideas of feedback and information provide a frame of reference for viewing a wide range of situations, just as do the ideas of evolution, of relativism, of axiomatic method, and of
343|An Efficient Boosting Algorithm for Combining Preferences|The problem of combining preferences arises in several applications, such as combining the results of different search engines. This work describes an efficient algorithm for combining multiple preferences. We first give a formal framework for the problem. We then describe and analyze a new boosting algorithm for combining preferences called RankBoost. We also describe an efficient implementation of the algorithm for certain natural cases. We discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different WWW search strategies, each of which is a query expansion for a given domain. For this task, we compare the performance of RankBoost to the individual search strategies. The second experiment is a collaborative-filtering task for making movie recommendations. Here, we present results comparing RankBoost to nearest-neighbor and regression algorithms.
344|The Nature of Statistical Learning Theory| Statistical learning theory was introduced in the late 1960’s. Until the 1990’s it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990’s new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more
345|Support-Vector Networks|The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the supportvector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.
346|Bagging Predictors|Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy. 1. Introduction A learning set of L consists of data f(y n ; x n ), n = 1; : : : ; Ng where the y&#039;s are either class labels or a numerical response. We have a procedure for using this learning set to form a predictor &#039;(x; L) --- if the input is x we ...
348|Experiments with a New Boosting Algorithm|In an earlier paper, we introduced a new “boosting” algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that consistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a “pseudo-loss ” which is a method for forcing a learning algorithm of multi-label conceptsto concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman’s “bagging ” method when used to aggregate various classifiers (including decision trees and single attribute-value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem. 
349|A Theory of the Learnable|  Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems.
350|A training algorithm for optimal margin classifiers|A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classifiaction functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.  
351|Additive Logistic Regression: a Statistical View of Boosting|Boosting (Freund &amp; Schapire 1996, Schapire &amp; Singer 1998) is one of the most important recent developments in classification methodology. The performance of many classification algorithms can often be dramatically improved by sequentially applying them to reweighted versions of the input data, and taking a weighted majority vote of the sequence of classifiers thereby produced. We show that this seemingly mysterious phenomenon can be understood in terms of well known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multi-class generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multi-class generalizations of boosting in most...
352|GroupLens: An Open Architecture for Collaborative Filtering of Netnews|Collaborative filters help people make choices based on the opinions of other people. GroupLens is a system for collaborative filtering of netnews, to help people find articles they will like in the huge stream of available articles. News reader clients display predicted scores and make it easy for users to rate articles after they read them. Rating servers, called Better Bit Bureaus, gather and disseminate the ratings. The rating servers predict scores based on the heuristic that people who agreed in the past will probably agree again. Users can protect their privacy by entering ratings under pseudonyms, without reducing the effectiveness of the score prediction. The entire architecture is open: alternative software for news clients and Better Bit Bureaus can be developed independently and can interoperate with the components we have developed.
353|Empirical Analysis of Predictive Algorithm for Collaborative Filtering|1
354|Social Information Filtering: Algorithms for Automating &#034;Word of Mouth&#034;|This paper describes a technique for making personalized recommendations from any type of database to a user based on similarities between the interest profile of that user and those of other users. In particular, we discuss the implementation of a networked system called Ringo, which makes personalized recommendations for music albums and artists. Ringo&#039;s database of users and artists grows dynamically as more people use the system and enter more information. Four different algorithms for making recommendations by using social information filtering were tested and compared. We present quantitative and qualitative results obtained from the use of Ringo by more than 2000 people.
355|Boosting the margin: A new explanation for the effectiveness of voting methods|One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik’s support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.  
356|The Weighted Majority Algorithm|We study the construction of prediction algorithms in a situation in which a learner faces a sequence of trials, with a prediction to be made in each, and the goal of the learner is to make few mistakes. We are interested in the case that the learner has reason to believe that one of some pool of known algorithms will perform well, but the learner does not know which one. A simple and effective method, based on weighted voting, is introduced for constructing a compound algorithm in such a circumstance. We call this method the Weighted Majority Algorithm. We show that this algorithm is robust in the presence of errors in the data. We discuss various versions of the Weighted Majority Algorithm and prove mistake bounds for them that are closely related to the mistake bounds of the best algorithms of the pool. For example, given a sequence of trials, if there is an algorithm in the pool A that makes at most m mistakes then the Weighted Majority Algorithm will make at most c(log jAj + m) mi...
357|The Strength of Weak Learnability|Abstract. This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a Source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent. A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error e.
358|An experimental comparison of three methods for constructing ensembles of decision trees|Abstract. Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a “base ” learning algorithm. Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the effectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classification noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classification noise, bagging is much better than boosting, and sometimes better than randomization.
359|Boosting a Weak Learning Algorithm By Majority|We present an algorithm for improving the accuracy of algorithms for learning binary concepts. The improvement is achieved by combining a large number of hypotheses, each of which is generated by training the given learning algorithm on a different set of examples. Our algorithm is based on ideas presented by Schapire in his paper &#034;The strength of weak learnability&#034;, and represents an improvement over his results. The analysis of our algorithm provides general upper bounds on the resources required for learning in Valiant&#039;s polynomial PAC learning framework, which are the best general upper bounds known today. We show that the number of hypotheses that are combined by our algorithm is the smallest number possible. Other outcomes of our analysis are results regarding the representational power of threshold circuits, the relation between learnability and compression, and a method for parallelizing PAC learning algorithms. We provide extensions of our algorithms to cases in which the conc...
360|Learning to Order Things|There are many applications in which it is desirable to order rather than classify  instances. Here we consider the problem of learning how to order, given feedback  in the form of preference judgments, i.e., statements to the effect that one instance  should be ranked ahead of another. We outline a two-stage approach in which one  first learns by conventional means a preference function, of the form PREF(u; v),  which indicates whether it is advisable to rank u before v. New instances are  then ordered so as to maximize agreements with the learned preference function.  We show that the problem of finding the ordering that agrees best with  a preference function is NP-complete, even under very restrictive assumptions.  Nevertheless, we describe a simple greedy algorithm that is guaranteed to find a  good approximation. We then discuss an on-line learning algorithm, based on the  &#034;Hedge&#034; algorithm, for finding a good linear combination of ranking &#034;experts.&#034;  We use the ordering algorith...
361|Cryptographic Limitations on Learning Boolean Formulae and Finite Automata|In this paper we prove the intractability of learning several classes of Boolean functions in the distribution-free model (also called the Probably Approximately Correct or PAC model) of learning from examples. These results are representation independent, in that they hold regardless of the syntactic form in which the learner chooses to represent its hypotheses. Our methods reduce the problems of cracking a number of well-known public-key cryptosystems to the learning problems. We prove that a polynomial-time learning algorithm for Boolean formulae, deterministic finite automata or constant-depth threshold circuits would have dramatic consequences for cryptography and number theory: in particular, such an algorithm could be used to break the RSA cryptosystem, factor Blum integers (composite numbers equivalent to 3 modulo 4), and detect quadratic residues. The results hold even if the learning algorithm is only required to obtain a slight advantage in prediction over random guessing. The techniques used demonstrate an interesting duality between learning and cryptography. We also apply our results to obtain strong intractability results for approximating a generalization of graph coloring.
362|Discriminative Reranking for Natural Language Parsing|This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model’s score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.  
363|Bagging, Boosting, and C4.5|Breiman&#039;s bagging and Freund and Schapire&#039;s  boosting are recent methods for improving the predictive power of classifier learning systems. Both form a set of classifiers that are combined by voting, bagging by generating replicated bootstrap samples of the data, and boosting by adjusting the weights of training instances. This paper reports results of applying both techniques to a system that learns decision trees and testing on a representative collection of datasets. While both approaches substantially improve predictive accuracy, boosting shows the greater benefit. On the other hand, boosting also produces severe degradation on some datasets. A small change to the way that boosting combines the votes of learned classifiers reduces this downside and also leads to slightly better results on most of the datasets considered. Introduction  Designers of empirical machine learning systems are concerned with such issues as the computational cost of the learning method and the accuracy and ...
364|Pranking with Ranking|We discuss the problem of ranking instances. In our framework each instance is associated with a rank or a rating, which is an integer from 1 to k. Our goal is to find a rank-prediction rule that assigns each instance a rank which is as close as possible to the instance&#039;s true rank. We describe a simple and efficient online algorithm, analyze its performance in the mistake bound model, and prove its correctness. We describe two sets of experiments, with synthetic data and with the EachMovie dataset for collaborative filtering. In the experiments we performed, our algorithm outperforms online algorithms for regression and classification applied to ranking.
365|The Sample Complexity of Pattern Classification With Neural Networks: The Size of the Weights is More Important Than the Size of the Network|Sample complexity results from computational learning theory, when applied to neural network learning for pattern classification problems, suggest that for good generalization performance the number of training examples should grow at least linearly with the number of adjustable parameters in the network. Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. For example, consider a two-layer feedforward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A and the input dimension is n. We show that the misclassification probability is no more than a certain error estimate (that is related to squared error on the training set) plus A³ p  (log n)=m (ignori...
366|Bias plus variance decomposition for zero-one loss functions|We present a bias-variance decomposition of expected misclassi cation rate, the most commonly used loss function in supervised classi cation learning. The bias-variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms, yet no decomposition was o ered for the more commonly used zero-one (misclassi cation) loss functions until the recent work of Kong &amp; Dietterich (1995) and Breiman (1996). Their decomposition su ers from some major shortcomings though (e.g., potentially negative variance), which our decomposition avoids. We show that, in practice, the naive frequency-based estimation of the decomposition terms is by itself biased and show how to correct for this bias. We illustrate the decomposition on various algorithms and datasets from the UCI repository. 1
367|Error-Correcting Output Coding Corrects Bias and Variance|Previous research has shown that a technique called error-correcting output coding (ECOC) can dramatically improve the classification accuracy of supervised learning algorithms that learn to classify data points into one of k AE 2 classes. This paper presents an investigation of why the ECOC technique works, particularly when employed with decision-tree learning algorithms. It shows that the ECOC method--- like any form of voting or committee---can reduce the variance of the learning algorithm. Furthermore---unlike methods that simply combine multiple runs of the same learning algorithm---ECOC can correct for errors caused by the bias of the learning algorithm. Experiments show that this bias correction ability relies on the non-local behavior of C4.5. 1 Introduction  Error-correcting output coding (ECOC) is a method for applying binary (two-class) learning algorithms to solve k-class supervised learning problems. It works by converting the k-class supervised learning problem into a la...
368|Adaptive game playing using multiplicative weights|We present a simple algorithm for playing a repeated game. We show that a player using this algorithm suffers average loss that is guaranteed to come close to the minimum loss achievable by any fixed strategy. Our bounds are nonasymptotic and hold for any opponent. The algorithm, which uses the multiplicative-weight methods of Littlestone and Warmuth, is analyzed using the Kullback–Liebler divergence. This analysis yields a new, simple proof of the min–max theorem, as well as a provable method of approximately solving a game. A variant of our game-playing algorithm is proved to be optimal in a very strong sense.  
369|Automatic Combination of Multiple Ranked Retrieval Systems|Retrieval performance can often be improved significantly by using a number of different retrieval algorithms and combining the results, in contrast to using just a single retrieval algorithm. This is because different retrieval algorithms, or retrieval experts, often emphasize different document and query features when determining relevance and therefore retrieve different sets of documents. However, it is unclear how the different experts are to be combined, in general, to yield a superior overall estimate. We propose a method by which the relevance estimates made by different experts can be automatically combined to result in superior retrieval performance. We apply the method to two expert combination tasks. The applications demonstrate that the method can identify high performance combinations of experts and also is a novel means for determining the combined effectiveness of experts.  1 Introduction  In text retrieval, two heads are definitely better than one. Retrieval performanc...
370|Game Theory, On-line Prediction and Boosting|We study the close connections between game theory, on-line prediction and boosting. After a brief review of game theory, we describe an algorithm for learning to play repeated games based on the on-line prediction methods of Littlestone and Warmuth. The analysis of this algorithm yields a simple proof of von Neumann’s famous minmax theorem, as well as a provable method of approximately solving a game. We then show that the on-line prediction model is obtained by applying this game-playing algorithm to an appropriate choice of game and that boosting is obtained by applying the same algorithm to the “dual” of this game. 
371|A Game of Prediction with Expert Advice|We consider the following problem. At each point of discrete time the learner must make a prediction; he is given the predictions made by a pool of experts. Each prediction and the outcome, which is disclosed after the learner has made his prediction, determine the incurred loss. It is known that, under weak regularity, the learner can ensure that his cumulative loss never exceeds cL+ a ln n, where c and a are some constants, n is the size of the pool, and L is the cumulative loss incurred by the best expert in the pool. We find the set of those pairs (c; a) for which this is true.
372|Pruning Adaptive Boosting|The boosting algorithm AdaBoost, developed by Freund and Schapire, has exhibited outstanding performance on several benchmark problems when using C4.5 as the &#034;weak&#034; algorithm to be &#034;boosted.&#034; Like other ensemble learning approaches, AdaBoost constructs a composite hypothesis by voting many individual hypotheses. In practice, the large amount of memory required to store these hypotheses can make ensemble methods hard to deploy in applications. This paper shows that by selecting a subset of the hypotheses, it is possible to obtain nearly the same levels of performance as the entire set. The results also provide some insight into the behavior of AdaBoost.
373|Using Output Codes to Boost Multiclass Learning Problems|This paper describes a new technique for solving multiclass learning problems by combining Freund and Schapire&#039;s boosting algorithm with the main ideas of Dietterich and Bakiri&#039;s method of error-correcting output codes (ECOC). Boosting is a general method of improving the accuracy of a given base or &#034;weak&#034; learning algorithm. ECOC is a robust method of solving multiclass learning problems by reducing to a sequence of two-class problems. We show that our new hybrid method has advantages of both: Like ECOC, our method only requires that the base learning algorithm work on binary-labeled data. Like boosting, we prove that the method comes with strong theoretical guarantees on the training and generalization error of the final combined hypothesis assuming only that the base learning algorithm perform slightly better than random guessing. Although previous methods were known for boosting multiclass problems, the new method may be significantly faster and require less programming effort in creating the base
learning algorithm. We also compare the new algorithm
experimentally to other voting methods.
374|An Adaptive Version of the Boost By Majority Algorithm|We propose a new boosting algorithm. This boosting algorithm is an adaptive version of the boost by  majority algorithm and combines bounded goals of the boost by majority algorithm with the adaptivity  of AdaBoost.
375|An empirical evaluation of bagging and boosting|An ensemble consists of a set of independently trained classi ers (such as neural networks or decision trees) whose predictions are combined when classifying novel instances. Previous research has shown that an ensemble as a whole is often more accurate than any of the single classiers in the ensemble. Bagging (Breiman 1996a) and Boosting (Freund &amp; Schapire 1996) are two relatively new but popular methods for producing ensembles. In this paper we evaluate these methods using both neural networks and decision trees as our classi cation algorithms. Our results clearly showtwo important facts. The rst is that even though Bagging almost always produces a better classi er than any of its individual component classi ers and is relatively impervious to over tting, it does not generalize any better than a baseline neural-network ensemble method. The second is that Boosting is apowerful technique that can usually produce better ensembles than Bagging ? however, it is more susceptible to noise and can quickly over t a data set.
376|A New Family of Online Algorithms for Category Ranking|We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stems from recent advances in online learning algorithms. The algorithms we present are simple to implement and are time and memory ecient. We evaluate the algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora the algorithms we present outperform adaptations to topic-ranking of Rocchio&#039;s algorithm and the Perceptron algorithm. We also outline the formal analysis of the algorithm in the mistake bound model. To our knowledge, this work is the  rst to report performance results with the entire new Reuters corpus.
377|Arcing the edge|Recent work has shown that adaptively reweighting the training set, growing a classifier using the new weights, and combining the classifiers constructed to date can significantly decrease generalization error. Procedures of this type were called arcing by Breiman[1996]. The first successful arcing procedure was introduced by Freund and Schapire[1995,1996] and called Adaboost. In an effort to explain why Adaboost works, Schapire et.al. [1997] derived a bound on the generalization error of a convex combination of classifiers in terms of the margin. We introduce a function called the edge, which differs from the margin only if there are more than two classes. A framework for understanding arcing algorithms is defined. In this framework, we see that the arcing algorithms currently in the literature are optimization algorithms which minimize some function of the edge. A relation is derived between the optimal reduction in the maximum value of the edge and the PAC concept of weak learner. Two algorithms are described which achieve the optimal reduction. Tests on both synthetic and real data cast doubt on the Schapire et.al. explanation.
378|T.: Using the future to sort out the present: Rankprop and multitask learning for medical risk evaluation|A patient visits the doctor; the doctor reviews the patient&#039;s history, asks questions, makes basic measurements (blood pressure,...), and prescribes tests or treatment. The prescribed course of action is based on an assessment of patient risk|patients at higher risk are given more and faster attention. It is also sequential|it is too expensive to immediately order all tests which might later be of value. This paper presents two methods that together improve the accuracy of backprop nets on a pneumonia risk assessment problem by 10-50%. Rankprop improves on backpropagation with sum of squares error in ranking patients by risk. Multitask learning takes advantage of future lab tests available in the training set, but not available in practice when predictions must be made. Both methods are broadly applicable. 1
379|Cranking: Combining Rankings Using Conditional Probability Models on Permutations|A new approach to ensemble learning is introduced  that takes ranking rather than classification  as fundamental, leading to models on the symmetric  group and its cosets. The approach uses a  generalization of the Mallows model on permutations  to combine multiple input rankings. Applications  include the task of combining the output  of multiple search engines and multiclass or multilabel  classification, where a set of input classifiers  is viewed as generating a ranking of class labels.
380|Direct Optimization of Margins Improves Generalization in Combined Classifiers|Sonar Cumulative training margin distributions  for AdaBoost versus  our &#034;Direct Optimization Of  Margins&#034; (DOOM) algorithm.
381|Data Filtering and Distribution Modeling Algorithms for Machine Learning|vi Acknowledgments vii 1. Introduction 1  1.1 Boosting by majority : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.2 Query By Committee : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 7 1.3 Learning distributions of binary vectors : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 8  2. Boosting a weak learning algorithm by majority 10  2.1 Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 10 2.2 The majority-vote game : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 14 2.2.1 Optimality of the weighting scheme : : : : : : : : : : : : : : : : : : : : : : : : : : : 19 2.2.2 The representational power of majority gates : : : : : : : : : : : : : : : : : : : : : : 20 2.3 Boosting a weak learner using a majority vote : : : : : : : : : : : : : : : : : : : : : : : : : : 22 2.3.1 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : :...
382|Using the future to \sort out&amp;quot; the present: Rankprop and multitask learning for medical risk evaluation|A patient visits the doctor; the doctor reviews the patient&#039;s history, asks questions, makes basic measurements (blood pressure,...), and prescribes tests or treatment. The prescribed course of action is based on an assessment of patient risk|patients at higher risk are given more and faster attention. It is also sequential|it is too expensive to immediately order all tests which might later be of value. This paper presents two methods that together improve the accuracy of backprop nets on a pneumonia risk assessment problem by 10-50%. Rankprop improves on backpropagation with sum of squares error in ranking patients by risk. Multitask learning takes advantage of future lab tests available in the training set, but not available in practice when predictions must be made. Both methods are broadly applicable. 1
383|A Roadmap of Agent Research and Development|  This paper provides an overview of research and development activities in the field of autonomous agents and multi-agent systems. It aims to identify key concepts and applications, and to indicate how they relate to one-another. Some historical context to the field of agent-based computing is given, and contemporary research directions are presented. Finally, a range of open issues and future challenges are highlighted.
384|A translation approach to portable ontology specifications|To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse — definitions of classes, relations, functions, and other objects — is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms. 
385|The tragedy of the commons|At the end of a thoughtful article on the future of nuclear war, Wiesner and York (1) concluded that: “Both sides in the arms race are... confronted by the dilemma of steadily increasing military power and steadily de-creasing national security. It is our considered professional judgment that this dilemma has no technical solution. If the great powers continue to look for solutions in the area of science and technology only, the result will be to worsen the situation.” I would like to focus your attention not on the subject of the article (national secu-rity in a nuclear world) but on the kind of conclusion they reached, namely that there
386|Intelligence Without Representation|Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporateeverything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environ...
388|Intelligence without reason|Computers and Thought are the two categories that together define Artificial Intelligence as a discipline. It is generally accepted that work in Artificial Intelligence over the last thirty years has had a strong influence on aspects of computer architectures. In this paper we also make the converse claim; that the state of computer architecture has been a strong influence on our models of thought. The Von Neumann model of computation has lead Artificial Intelligence in particular directions. Intelligence in biological systems is completely different. Recent work in behavior-based Artificial Intelligence has produced new models of intelligence that are much closer in spirit to biological systems. The non-Von Neumann computational models they use share many characteristics with biological computation.
391|The Role of Emotion in Believable Agents|Articial intelligence researchers attempting to create engaging  apparently living creatures may nd important insight in the work of artists who have explored the idea of believable character  In particular  appropriately timed and clearly expressed emotion is a central requirement for believable characters  We discuss these ideas and suggest how they may apply to believable interactive characters  which we call believable agents This work was supported in part by Fujitsu Laboratories and Mitsubishi Electric Research Laborato ries  The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ocial policies  either expressed or implied  of any other parties Keywords  articial intelligence  emotion  believable agents art  animation  believable characters  BELIEVABILITY   Believability There is a notion in the Arts of believable character  It does not mean an honest or reliable character  but one that provides the illusion of life  and thus permits the audience s suspension of disbelief The idea of believability has long been studied and explored in literature  theater lm  radio drama  and other media  Traditional character animators are among those artists who have sought to create believable characters  and the Disney animators of the   	 s made great strides toward this goal  The rst page of the enormous classic reference work on Disney animation Thomas and Johnston     begins with these words Disney animation makes audiences really believe in   characters  whose adventures and misfortunes make people laugh  and even cry  There is a special ingredient in our type of animation that produces drawings that appear to think and make decisions and act of their own volition  it is what creates the illusion of life Many articial intelligence researchers have long wished to build robots  and their cousins called agents  that seem to think  feel  and live  These are creatures with whom you	d want to share some of your life  as with a companion  or a social pet For instance  in his 
392|Collaborative plans for complex group action|The original formulation of SharedPlans by B. Grosz and C. Sidner ( 1990) was developed to provide a model of collaborative planning in which it was not necessary for one agent to have intentions-to toward an act of a different agent. Unlike other contemporaneous approaches (J.R. Searle, 1990), this formulation provided for two agents to coordinate their activities without introducing any notion of irreducible joint intentions. However, it only treated activities that directly decomposed into single-agent actions, did not address the need for agents to commit to their joint activity, and did not adequately deal with agents having only partial knowledge of the way in which to perform an action. This paper provides a revised and expanded version of SharedPlans that addresses these shortcomings. It also reformulates Pollack’s ( 1990) definition of individual plans to handle cases in which a single agent has only partial knowledge; this reformulation meshes with the definition of SharedPlans. The new definitions also allow for contracting out certain actions. The formalization that results has the features required by Bratrnan’s ( 1992) account of shared cooperative activity and is more general than alternative accounts (H. Levesque et al., 1990; E. Sonenberg et al., 1992).  
393|Plans And Resource-Bounded Practical Reasoning|An architecture for a rational agent must allow for means-end reasoning, for the weighing of competing alternatives, and for interactions between these two forms of reasoning. Such an architecture must also address the problem of resource boundedness. We sketch a solution of the first problem that points the way to a solution of the second. In particular, we present a high-level specification of the practical-reasoning component of an architecture for a resource-bounded rational agent. In this architecture, a major role of the agent&#039;s plans is to constrain the amount of further practical reasoning she must perform.
394|Kasbah: An Agent Marketplace for Buying and Selling Goods|While there are many Web services which help users find things to buy,we know of none which actually try to automate the process of buying and selling. Kasbah is a virtual marketplace on the Web where users create autonomous agents to buy and sell goods on their behalf. Users specify parameters to guide and constrain an agent&#039;s overall behavior. A simple prototype has been built to test the viability of this concept.
395|Experiences with an Architecture for Intelligent, Reactive Agents  |This paper describes an implementation of the 3T robot architecture  which has been under development for the last eightyears. The architecture  uses three levels of abstraction and description languages whichare  compatible between levels. The makeup of the architecture helps to coordinate  planful activities with real-time behaviors for dealing with dynamic  environments. In recent years, other architectures have been created with  similar attributes but two features distinguish the 3T architecture: 1) a  variety of useful software tools have been created to help implement this  architecture on multiple real robots;, and 2) this architecture, or parts of it, have been implemented on a varietyofvery different robot systems  using different processors, operating systems, effectors and sensor suites.
396|A Scalable Comparison-Shopping Agent for the World-Wide Web|The Web is less agent-friendly than we might hope. Most information on the Web is presented in loosely structured natural language text with no agent-readable semantics. HTML annotations structure the display of Web pages, but provide virtually no insight into their content. Thus, the designers of intelligent Web agents need to address the following questions: (1) To what extent can an agent understand information published at Web sites? (2) Is the agent&#039;s understanding sufficient to provide genuinely useful assistance to users? (3) Is site-specific hand-coding necessary, or can the agent automatically extract information from unfamiliar Web sites? (4) What aspects of the Web facilitate this competence? In this paper we investigate these issues with a case study using the ShopBot. ShopBot is a fullyimplemented, domain-independent comparison-shopping agent. Given the home pages of several on-line stores, ShopBot autonomously learns how to shop at those vendors. After its learning is com...
397|Controlling Cooperative Problem Solving in Industrial Multi-Agent Systems using Joint Intentions|One reason why Distributed AI (DAI) technology has been deployed in relatively few real-size applications is that it lacks a clear and implementable model of cooperative problem solving which specifies how agents should operate and interact in complex, dynamic and unpredictable environments. As a consequence of the experience gained whilst building a number of DAI systems for industrial applications, a new principled model of cooperation has been developed. This model, called Joint Responsibility, has the notion of joint intentions at its core. It specifies pre-conditions which must be attained before collaboration can commence and prescribes how individuals should behave both when joint activity is progressing satisfactorily and also when it runs into difficulty. The theoretical model has been used to guide the implementation of a general-purpose cooperation framework and the qualitative and quantitative benefits of this implementation have been assessed through a series of comparativ...
398|Commitments and conventions: The foundation of coordination in multi-agent systems|Distributed Artificial Intelligence systems, in which multiple agents interact to improve their individual performance and to enhance the system’s overall utility, are becoming an increasingly pervasive means of conceptualising a diverse range of applications. As the discipline matures, researchers are beginning to strive for the underlying theories and principles which guide the central processes of coordination and cooperation. Here agent communities are modelled using a distributed goal search formalism and it is argued that commitments (pledges to undertake a specified course of action) and conventions (means of monitoring commitments in changing circumstances) are the foundation of coordination in multi-agent systems. An analysis of existing coordination models which use concepts akin to commitments and conventions is undertaken before a new unifying framework is presented. Finally a number of prominent coordination techniques which do not explicitly involve commitments or conventions are reformulated in these terms to demonstrate their compliance with the central hypothesis of this paper. 1
399|Middle-Agents for the Internet|Like middle-men in physical commerce, middleagents  support the flow of information in electronic  commerce, assisting in locating and connecting the  ultimate information provider with the ultimate information  requester. Many different types of middleagents  will be useful in realistic, large, distributed,  open multi-agent problem solving systems. These  include matchmakers or yellow page agents that process  advertisements, blackboard agents that collect requests,  and brokers that process both. The behaviors  of each type of middle-agent have certain performance  characteristics---privacy, robustness, and  adaptiveness qualities---that are related to characteristics  of the external environment and of the agents  themselves. For example, while brokered systems are  more vulnerable to certain failures, they are also able  to cope more quickly with a rapidly fluctuating agent  workforce and meet certain privacy considerations.  This paper identifies a spectrum of middle-agents,  cha...
400|WebMate: A Personal Agent for Browsing and Searching|The World-Wide Web is developing very fast. Currently, finding useful information on the Web is a time consuming process. In this paper, we present WebMate, an agent that helps users to effectively browse and search the Web. WebMate extends the state of the art in Web-based information retrieval in many ways. First, it uses multiple TF-IDF vectors to keep track of user interests in different domains. These domains are automatically learned by WebMate. Second, WebMate uses the Trigger Pair Model to automatically extract keywords for refining document search. Third, during search, the user can provide multiple pages as similarity/relevance guidance for the search. The system extracts and combines relevant keywords from these relevant pages and uses them for keyword refinement. Using these techniques, WebMate provides effective browsing and searching help and also compiles and sends to users personal newspaper by automatically spiding news sources. We have experimentally evaluated the per...
401|Designing a Family of Coordination Algorithms|Many researchers have shown that there is no single best organization or coordination mechanism  for all environments. This paper discusses the design and implementation of an extendable  family of coordination mechanisms, called Generalized Partial Global Planning (GPGP). The set  of coordination mechanisms described here assists in scheduling activities for teams of cooperative  computational agents. The GPGP approach has several unique features. First, it is not tied to  a single domain. Each mechanism is defined as a response to certain features in the current task  environment. We show that different combinations of mechanisms are appropriate for different  task environments. Secondly, the approach works in conjunction with an agent&#039;s existing local  planner/scheduler. Finally, the initial set of five mechanisms presented here generalizes and extends  the Partial Global Planning (PGP) algorithm. In comparison to PGP, GPGP allows more  agent heterogeneity, it exchanges less global ...
402|editors. Blackboard Systems|Retroviral vectors containing putative internal ribosome entry sites: development of a polycistronic gene transfer
403|Trends in Cooperative Distributed Problem Solving|Introduction Cooperative Distributed Problem-Solving (CDPS) studies how a loosely-coupled network of problem solvers can work together to solve problems that are beyond their individual capabilities. Each problem-solving node in the network is capable of sophisticated problem solving and can work independently, but the problems faced by the nodes cannot be completed without cooperation. Cooperation is necessary because no single node has sufficient expertise, resources, and information to solve a problem, and different nodes might have expertise for solving different parts of the problem. For example, if the problem is to design a house, one node might have expertise on the strength of structural materials, another on the space requirements for different types of rooms, another on plumbing, another on electrical wiring, and so on. Different nodes might have different resources: some might be very fast at computation, others might have connections that speed communication, whil
404|Multiagent negotiation under time constraints|Research in distributed artificial intelligence (DAI) is concerned with how automated agents can be designed to interact effectively. Negotiation is proposed as a means for agents to communicate and compromise to reach mutually beneficial agreements. The paper examines the problems of resource allocation and task distribution among autonomous agents which can benefit from sharing a common resource or distributing a set of common tasks. We propose a strategic model of negotiation that takes the passage of time during the negotiation process itself into account. A distributed negotiation mechanism is introduced that is simple, efficient, stable, and flexible in various situations. The model considers situations characterized by complete as well as incomplete information, and ones in which some agents lose over time while others gain over time. Using this negotiation mechanism autonomous agents have simple and stable negotiation strategies that result in efficient agreements without delays even when there are dynamic changes in the environment.  
405|The behavior of computational ecologies|We describe a form of distributed computation in which agents have incomplete knowledge and imperfect information on the state of the system, and an instantiation of such systems based on market mechanisms. When agents can choose among several resources, the dynamics of the system can be oscillatory and even chaotic. A mechanism is described for achieving global stability through local controls. 1
406|A Retrospective View of FA/C Distributed Problem Solving|The Functionally-Accurate, Cooperative (FA/C) paradigm provides a model for task decomposition and agent interaction in a distributed problem-solving system. In this model, agents need not have all the necessary information locally to solve their subproblems, and agents interact through the asynchronous, co-routine exchange of partial results. This model leads to the possibility that agents may behave in an uncoordinated manner. This paper traces the development of a series of increasingly sophisticated cooperative control mechanisms for coordinating agents. They include integrating data- and goal-directed control, using static meta-level information specified by an organizational structure, and using dynamic meta-level information developed in partial global planning. The framework of distributed search motivates these developments. Major themes of this work are the importance of sophisticated local control, the interplay between local control and cooperative control, and the use of s...
407|The use of meta-level control for coordination in a distributed problem solving network|This paper was presented at IJCAI-83. Distributed problem-solving networks provide an interesting application area for meta-level control through the use of organizational structuring. We describe a decentralized approach to network coordination that relies on each node making sophisticated local decisions that balance its own perceptions of appropriate problem-solving activity with activities deemed important by other nodes. Each node is guided by a high-level strategic plan for cooperation among the nodes in the network. The high-level strategic plan, which is a form of meta-level control, is represented as a network organizational structure that specifies in a general way the information and control relationships among the nodes. An implementation of these ideas is briefly described along with the results of preliminary experiments with various network problem-solving strategies specified via organizational structuring. In addition to its application to Distributed Artificial Intelligence, this research has implications for organizing and controlling complex knowledge-based systems that involve semi-autonomous problem solving agents. 1
408|Agent-Based Business Process Management|This paper describes work undertaken in the ADEPT (Advanced Decision Environment for Process Tasks) project towards developing an agent-based infrastructure for managing business processes. We describe how the key technology of negotiating, service providing, autonomous agents was realised and demonstrate how this was applied to the BT (British Telecom) business process of providing a customer quote for network services.
409|Moving Up the Information Food Chain: Deploying Softbots on the World Wide Web|I view the World Wide Web as an information food chain (figure 1). The maze of pages and hyperlinks that comprise the Web are at the very bottom of the chain. The WebCrawlers and Alta Vistas of the world are information herbivores; they graze on Web pages and regurgitate them as searchable indices. Today, most Web users feed near the bottom of the information food chain, but the time is ripe to move up. Since 1991, we have been building information carnivores, which intelligently hunt and feast on herbivores
410|Designing behaviors for information agents|To facilitate the rapid development and open system interoperability of autonomous agents we need to carefully specify and effectively implement various classes of agent behaviors. Our current focus is on the behaviors and underlying architecture of WWW-based autonomous software agents that collect and supply information to humans and other computational agents. This paper discusses a set of architectural building blocks that support the specification of behaviors for these information agents in a way that allows periodic actions, interleaving of planning and execution, and the concurrent activation of multiple behaviors with asynchronous components. We present an initial set of information agent behaviors, including responding to repetitive queries, monitoring information sources, advertising capabilities, and self cloning. We have implemented and tested these behaviors on the WWW in the context of WAR-REN, an open multi-agent organization for financial portfolio management.
412|TouringMachines: An Architecture for Dynamic, Rational, Mobile Agents|ion-Partitioned Evaluator (APE) architecture which has been tested in a simulated, single-agent, indoor navigation domain [SH90].  The APE architecture is composed of a number of concurrent, hierarchically abstract action control layers, each representing and reasoning about some particular aspect of the agent&#039;s task domain. Implemented as a parallel blackboard-based planner, the five layers --- sensor/motor, spatial, temporal, causal, and conventional (general knowledge) --- effectively partition the agent&#039;s data processing duties along a number of dimensions including temporal granularity, information/resource use, and functional abstraction. Perceptual information flows strictly from the agent sensors (connected to the sensor /motor level) toward the higher levels, while command or goal-achievement information flows strictly downward towards the agent&#039;s effectors (also connected to the sensor/motor level).  Besides mechanisms for communicating with other layers, each layer in the AP...
413|Cooperative Transportation Scheduling: an Application Domain for DAI|A multiagent approach to designing the transportation domain is presented. The Mars system is described which models cooperative order scheduling within a society of shipping companies. We argue why Distributed Artificial Intelligence (DAI) offers suitable tools to deal with the hard problems in this domain. We present three important instances for DAI techniques that proved useful in the transportation application: cooperation among the agents, task decomposition and task allocation, and decentralised planning. An extension of the contract net protocol for task decomposition and task allocation is presented; we show that it can be used to obtain good initial solutions for complex resource allocation problems. By introducing global information based upon auction protocols, this initial solution can be improved significantly. We demonstrate that the auction mechanism used for schedule optimisation can also be used for implementing dynamic replanning. Experimental results are provided ev...
414|Foundations of a Logical Approach to Agent Programming|This paper describes a novel approach to high-level agent programming based on a highly  developed logical theory of action. The user provides a specification of the agents&#039; basic actions  (preconditions and effects) as well as of relevant aspects of the environment, in an extended  version of the situation calculus. He can then specify behaviors for the agents in terms of these  actions in a programming language where one can refer to conditions in effect in the environment.  When an implementation of the basic actions is provided, the programs can be executed in a  real environment; otherwise, a simulated execution is still possible. The interpreter automatically  maintains the world model required to execute programs based on the specification. The theoretical  framework includes a solution to the frame problem, allows agents to have incomplete knowledge  of their environment, and handles perceptual actions. The theory can also be used to prove  programs correct. A simple meeting sc...
415|Increasing believability in animated pedagogical agents|Animated pedagogical agents o er great promise for knowledge-based learning environments. In addition to coupling feedback capabilities with a strong visual presence, these agents play a critical role in motivating students. The extent to which they exhibit life-like behaviors strongly increases their motivational impact, but these behaviors must always complement and never interfere with students &#039; problem solving. To address this problem, we havedeveloped a framework for dynamically sequencing animated pedagogical agents &#039; believability-enhancing behaviors. By monitoring a student&#039;s problemsolving history and the agent&#039;s past activities, a competition-based behavior sequencing engine produces realtime life-like character animations that are pedagogically appropriate. Behaviors in the agent&#039;s repertoire compete with one another. At each moment, the strongest eligible behavior is heuristically selected as the winner and is exhibited. We haveimplemented this framework in Herman the Bug, an animated pedagogical agent that inhabits a knowledge-based learning environment for the domain of botanical anatomy and physiology.
416|Towards a Social Level Characterisation of Socially Responsible Agents|This paper presents a high-level framework for analysing and designing intelligent agents. The framework&#039;s key abstraction mechanism is a new computer level called the  Social Level. The Social Level sits immediately above the Knowledge Level, as defined by Allen Newell, and is concerned with the inherently social aspects of multiple agent systems. To illustrate the working of this framework, an important new class of agent is identified and then specified. Socially responsible agents retain their local autonomy but still draw from, and provide resources to, the larger community. Through empirical evaluation, it is shown that such agents produce both good system-wide performance and good individual performance. 1. INTRODUCTION The number of multi-agent systems being designed and built is rapidly increasing as software agents gain acceptance as a powerful and useful technology for solving complex problems (Chaib-draa, 1995; Jennings, 1994; PAAM, 1996). As applications become more comple...
417|Representing and Executing Agent-Based Systems|In this paper we describe an approach to the representation and implementation  of agent-based systems where the behaviour of an individual agent is  represented by a set of logical rules in a particular form. This not only provides a  logical specification of the agent, but also allows us to directly execute the rules in  order to implement the agent&#039;s behaviour. Agents communicate with each other  through a simple, and logically well-founded, broadcast communication mechanism.
418|Entertaining Agents: a sociological case study|Traditional AI has not concerned itself extensively with sociology nor with what emotional reactions might be produced in its users. On the other hand, entertainment is very concerned indeed with these issues. AI and ALife programs which are to be used in entertainment must therefore be viewed both as AI/ALife endeavors and as psychological and sociological endeavors. This paper presents a brief description of Julia [Mauldin 94], an implemented software agent, and then examines the sociology of those who encounter her, using both transcripts of interactions with Julia, and direct interviews with users. Julia is designed to pass as human in restricted environments while being both entertaining and informative, and often elicits surprisingly intense emotional reactions in those who encounter her. An introduction to MUDs and Julia Julia [Mauldin 94] is a MUD [Curtis 92] [Bruckman 93] [Evard 93] robot. A MUD is a text-only, multiperson, virtual reality. [Mauldin 94], while describing Julia’s internal structure, gives very little ‘feel ’ for what it like to interact with her outside of the strictures of a formal Turing test; hence, transcripts of many interactions with her appear below as examples. (Since Julia adamantly insists that she is female, I refer to the program here as ‘she’.)
419|Using ARCHON to develop real-world DAI applications for electricity transportation management and particle accelerator control|ARCHON ^TM (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe&#039;s largest ever project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Two of these applications, electricity transportation management and particle accelerator control, have been run successfully on-line in the organisation for which they were developed (respectively, Iberdrola an electricity utility in the north of Spain and CERN the European Centre for high energy physics research near Geneva). This paper recounts the problems, insights and experiences gained whilst deploying ARCHON technology in these real-world industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applications and highlights the key design forces which shape work in this important domain. Secondly, the...
420|An Agent-based Approach to Health Care Management|The provision of medical care typically involves a number of individuals, located in a number of different institutions, whose decisions and actions need to be coordinated if the care is to be effective and efficient. To facilitate this decision making and to ensure the coordination process runs smoothly, the use of software support is becoming increasingly widespread. To this end, this paper describes an agent-based system which was developed to help manage the care process in real world settings. The agents themselves are implemented using a layered architecture, called AADCare, which combines a number of AI and agent techniques: a symbolic decision procedure for decision making with incomplete and conflicting information, a concept of accountability for task allocation, the notions of commitments and conventions for managing coherent cooperation, and a set of communication primitives for interagent interaction. The utility of this approach is demonstrated through the development of ...
421|The Negotiating Agents Approach to Runtime Feature Interaction Resolution|. This article describes how to use the Negotiating Agents approach on a telecommunications platform. Negotiation is used in this approach to resolve conflicts between features of one user and of different users. The theory behind the approach is discussed briefly. Methods for implementing the approach are given along with the methods for defining IN features in terms of the Negotiating Agents approach in order to resolve conflicts between these features. 1 Introduction  Rapid change in the telecommunications industry increases the complexity not only of building but also of using telecommunications services. Much of the complexity arises from the feature interaction problem. When features interact, a user must understand the behavior of features in combination -- even how features of other users may affect the negbehavior of her features. Similarly, a service provider must determine how combinations of features will behave, including combinations of its own features with other provide...
422|Specification and implementation of a belief desire joint-intention architecture for collaborative problem solving|Systems composed of multiple interacting problem solvers are becoming increasingly pervasive and have been championed in some quarters as the basis of the next generation of intelligent information systems. If this technology is to fulfill its true potential then it is important that the systems which are developed have a sound theoretical grounding. One aspect of this foundation, namely the model of collaborative problem solving, is examined in this paper. A synergistic review of existing models of cooperation is presented, their weaknesses are highlighted and a new model (called joint responsibility) is introduced. Joint responsibility is then used to specify a novel high-level agent architecture for cooperative problem solving in which the mentalistic notions of belief, desire, intention and joint intention play a central role in guiding an individual’s and the group’s problem solving behaviour. An implementation of this high-level architecture is then discussed and its utility is illustrated for the real-world domain of electricity transportation management.
423|Industrial Applications of Distributed AI|This article argues that a DAI approach can be used to cope with the complexity of industrial applications. DAI techniques are beginning to have a broad impact; the current introduction of these techniques by an ESPRIT project, a Palo Alto consortium, ARPA, Carnegie Mellon University, MCC, and others are good examples. In the near future, other industrial products will emerge from the application of DAI techniques to other domains, including distributed databases, computer-supported cooperative work, and air traffic control. An important advantage of a DAI approach is the ability to integrate existing standalone knowledge-based systems. This factor is important because software for industrial applications is often developed in an ad hoc fashion. Thus, organizations possess a large number of standalone systems developed at different times by different people using different techniques. These systems all operate in the same physical environment, all have expertise that is related but distinct, and all could benefit from cooperation with other such standalone systems
424|Using mobile agents to support interorganizational workflow-management|This paper argues that the mobile agent approach is well suited for sporadic communication in open distributed systems- especially for rather ‘loose’ cooperations across local and organizational borders: In an increasing number of cases, management of distributed business procedures reaches beyond such borders. This means for most existing workflow management systems that cooperating partners are required to give up their local autonomy. However, for cases in which business partners intend to cooperate but still need to preserve their local autonomy, process participation on the basis of mobile agents represents an attractive and appropriate mechanism. This article shows how such kind of process integration can be achieved. It further demonstrates how the COSM (Common Open Service Market) system software can be extended in order to use petri net based process definitions which realize mobile agents in an integrated distributed system platform.
425|Creatures: Entertainment Software Agents with Artificial Life|We present a technical description of Creatures, a commercial home-entertainment software package. Creatures provides a simulatedenvironment in which exist a number of synthetic agents that a user can interact with in real-time. The agents (known as &#034;creatures&#034;) are intended as sophisticated &#034;virtual pets&#034;. The internal architecture of the creatures is strongly inspired by animal biology. Each creature has a neural network responsible for sensory-motorcoordinationand behavior selection, and an &#034;artificial biochemistry&#034; that models a simple energy metabolism along with a &#034;hormonal&#034; system that interacts with the neural network to model diffuse modulation of neuronal activity and staged ontogenetic development. A biologically inspired learning mechanism allows the neural network to adapt during the lifetime of a creature. Learning includes the ability to acquire a simple verb--object language.
426|Some Issues in the Design of Market-Oriented Agents|. In a computational market, distributed market agents interact with other agents primarily through the exchange of goods and services. Thanks to a welldeveloped underlying economic framework, we can draw on a rich source of analytic tools and theoretical techniques for designing individual agents and predicting aggregate behavior. For many narrowly scoped static problems, design of a computational market is relatively straightforward. We consider some issues that arise in attempting to design computational economies for broadly scoped, dynamic environments. These issues include how to specify the goods and services beingexchanged,how these market-oriented agents should set their exchangepolicies, and how computational market mechanisms appropriate for idealized environments can be adapted to work in a larger class of non-ideal environments. 1 Introduction  Approaches to resource allocation in distributed systems can be bounded by two extremes. At one end (the &#034;software engineering&#034; ap...
427|Application of multi-agent systems in traffic and transportation|Agent-oriented techniques offer a new approach aimed at supporting the whole software development process. All the phases in the software development process are treated with a single uniform concept, namely that of agents, and a system modelled by a collection of agents is called a multi-agent system. AOT as a new advance in information technology can help to respond to the growing interest in making traffic and transportation more efficient, resource-saving and ecological. This article gives an overview of a diverse range of applications where multi-agent systems promise to create a great impact in this domain. To demonstrate the ideas behind AOT and their applicability in this domain, two applications currently under development at Daimler-Benz Research will be described in some detail. 1
428|Managing heterogeneous transaction workflows with cooperating agents|This paper describes how a set of autonomous computational agents can cooperate in providing coherent management of transaction workflows in environments where there are many diverse information resources. The agents use models of themselves and of the resources that are local to them. Resource models may be the schemas of databases, frame systems of knowledge bases, domain models of business environments, or process models of business operations. Models enable the agents and information resources to use the appropriate semantics when they interoperate. This is accomplished by specifying the semantics in terms of a common ontology. We discuss the contents of the models, where they come from, and how the agents acquire them. We then describe a set of agents for telecommunication service provisioning and show how the agents use such models to cooperate. The agents implement virtual state machines, and interact by exchanging state information. Their interactions produce an implementation of relaxed transaction processing. 1
429|SIMPLIcity: Semantics-Sensitive Integrated Matching for Picture LIbraries|The need for efficient content-based image retrieval has increased tremendously in many application areas such as biomedicine, military, commerce, education, and Web image classification and searching. We present here SIMPLIcity (Semanticssensitive Integrated Matching for Picture LIbraries), an image retrieval system, which uses semantics classification methods, a wavelet-based approach for feature extraction, and integrated region matching based upon image segmentation. As in other regionbased retrieval systems, an image is represented by a set of regions, roughly corresponding to objects, which are characterized by color, texture, shape, and location. The system classifies images into semantic categories, such as textured-nontextured, graphphotograph. Potentially, the categorization enhances retrieval by permitting semantically-adaptive searching methods and narrowing down the searching range in a database. A measure for the overall similarity between images is developed using a region-matching scheme that integrates properties of all the regions in the images. Compared with retrieval based on individual regions, the overall similarity approach 1) reduces the adverse effect of inaccurate segmentation, 2) helps to clarify the semantics of a particular region, and 3) enables a simple querying interface for region-based image retrieval systems. The application of SIMPLIcity to several databases, including a database of about 200,000 general-purpose images, has demonstrated that our system performs significantly better and faster than existing ones. The system is fairly robust to image alterations.
430|Blobworld: A System for Region-Based Image Indexing and Retrieval|. Blobworld is a system for image retrieval based on finding coherent image regions which roughly correspond to objects. Each image is automatically segmented into regions (&#034;blobs&#034;) with associated color and texture descriptors. Querying is based on the attributes of one or two regions of interest, rather than a description of the entire image. In order to make large-scale retrieval feasible, we index the blob descriptions using a tree. Because indexing in the high-dimensional feature space is computationally prohibitive, we use a lower-rank approximation to the high-dimensional distance. Experiments show encouraging results for both querying and indexing. 1 Introduction  From a user&#039;s point of view, the performance of an information retrieval system can be measured by the quality and speed with which it answers the user&#039;s information need. Several factors contribute to overall performance:  -- the time required to run each individual query,  -- the quality (precision/recall) of each i...
431|A probabilistic approach to object recognition using local photometry and global geometry|Abstract. Many object classes, including human faces, can be modeled as a set of characteristic parts arranged in a variable spatial con guration. We introduce a simpli ed model of a deformable object class and derive the optimal detector for this model. However, the optimal detector is not realizable except under special circumstances (independent part positions). A cousin of the optimal detector is developed which uses \soft &#034; part detectors with a probabilistic description of the spatial arrangement of the parts. Spatial arrangements are modeled probabilistically using shape statistics to achieve invariance to translation, rotation, and scaling. Improved recognition performance over methods based on \hard &#034; part detectors is demonstrated for the problem of face detection in cluttered scenes. 1
432|WALRUS: A Similarity Retrieval Algorithm for Image Databases|Traditional approaches for content-based image querying typically compute a single signature for each image based on color histograms, texture, wavelet transforms etc., and return as the query result, images whose signatures are closest to the signature of the query image. Therefore, most traditional methods break down when images contain similar objects that are scaled differently or at different locations, or only certain regions of the image match.  In this paper, we propose WALRUS (WAveLet-based Retrieval of User-specified Scenes), a novel similarity retrieval algorithm that is robust to scaling and translation of objects within an image. WALRUS employs a novel similarity model in which each image is first decomposed into its regions, and the similarity measure between a pair of images is then defined to be the fraction of the area of the two images covered by matching regions from the images. In order to extract regions for an image, WALRUS considers sliding windows of varying siz...
433|Content-based image indexing and searching using Daubechies&#039; wavelets|This paper describes WBIIS (Wavelet-Based Image Indexing and Searching), a new image indexing and retrieval algorithm with partial sketch image searching capability for large image databases. The algorithm characterizes the color variations over the spatial extent of the image in a manner that provides semantically meaningful image comparisons. The indexing algorithm applies a Daubechies&#039; wavelet transform for each of the three opponent color components. The wavelet coefficients in the lowest few frequency bands, and their variances, are stored as feature vectors. To speed up retrieval, a two-step procedure is used that first does a crude selection based on the variances, and then renes the search by performing a feature vector match between the selected images and the query. For better accuracy in searching, two-level multiresolution matching may also be used. Masks are used for partial-sketch queries. This technique performs much better in capturing coherence of image, object granular...
434|The earth mover’s distance, multi-dimensional scaling, and color-based image retrieval|In this paper we present a novel approach tothe problem of navigating through a database of color images. We consider the images as points in a metric space in which we wish to move around so as to locate image neighborhoods of interest, based on color information. The data base images are mapped to distributions in color space, these distributions are appropriately compressed, and then the distances between all pairs I;J of images are computed based on the work needed to rearrange the mass in the compressed distribution representing I to that of J. We also propose the use of multi-dimensional scaling (MDS) techniques to embed a group of images as points in a two- or three-dimensional Euclidean space so that their distances are preserved as much as possible. Such geometric embeddings allow the user to perceive the dominant axes of variation in the displayed image group. In particular, displays of 2-d MDS embeddings can be used to organize and re ne the results of a nearest-neighbor query in a perceptually intuitive way. By iterating this process, the user is able to quickly navigate to the portion of the image space of interest. 1
435|Image Classification and Querying using Composite Region Templates|The tremendous growth in digital imagery is driving the need for more sophisticated  methods for automatic image analysis, cataloging, and searching.  We present a method for classifying and querying images based on the spatial  orderings of regions or objects using composite region templates (CRTs). The  CRTs capture the spatial information statistically and provide a robust way to  measure similarity in the presence of region insertions, deletions, substitutions, replications and relocations. The CRTs can be used for classifying and annotating  images by assigning symbols to the regions or objects and by extracting  symbol strings from spatial scans of the images. The symbol strings can be  decoded using a library of annotated CRTs to automatically label and classify  the images. The CRTs can also be used for searching bysketch or example by  measuring image similarity based on relative counts of the CRTs.  
436|Finding Similar Patterns in Large Image Databases|We address a new and rapidly growing application, automated searching through large sets of images to find a pattern &#034;similar to this one.&#034; Classical matched filtering fails at this problem since patterns, particularly textures, can differ in every pixel and still be perceptually similar. Most potential recognition methods have not been tested on large sets of imagery. This paper evaluates a key recognition method on a library of almost 1000 images, based on the entire Brodatz texture album. The features used for searching rely on a significant improvement to the traditional Karhunen-Lo&#039;eve (KL) transform which makes it shift-invariant. Results are shown for a variety of false alarm rates and for different subsets of KL features.  1 Introduction  As vastly increasing amounts of image and video are stored in computers it becomes harder for humans to locate a particular scene or video clip. It is currently impossible, in the general case, to semantically describe an image to the computer...
437|Unsupervised Multiresolution Segmentation for Images with Low Depth of Field|This paper describes a novel multiresolution image  segmentation algorithm for low DOF images. The algorithm is designed to  separate a sharply focused object-of-interest from other foreground or background objects. The algorithm is fully automatic in that all parameters are image  independent. A multiscale approach based on high frequency wavelet coefficients and their statistics is used to perform context-dependent classification of individual blocks of the image. Unlike other edge-based approaches, our algorithm does not rely on the process of connecting object boundaries. The algorithm has achieved high accuracy when tested on more than 100 low DOF images, many with  inhomogeneous foreground or background distractions. Compared with the state of the art algorithms, this new algorithm provides better accuracy at higher speed. Index TermsContent-based image retrieval, image region segmentation, low  depth-of-field, wavelet, multiresolution image analysis
438|Semantic Clustering and Querying on Heterogeneous Features for Visual data|  The effectiveness of the content-based image retrieval can be enhanced using the heterogeneous features embedded in the images. However, since the features in texture, color, and shape are generated using different computation methods and thus may require different similarity measurements, the integration of the...
439|System for Screening Objectionable Images|As computers and Internet become more and more available to families, access of objectionable graphics by children is increasingly a problem that many parents are concerned about. This paper describes WIPE TM (Wavelet Image Pornography Elimination), a system capable of classifying an image as objectionable or benign. The algorithm uses a combination of an icon filter, a graph-photo detector, a color histogram filter, a texture filter, and a wavelet-based shape matching algorithm to provide robust screening of on-line objectionable images. Semantically-meaningful feature vector matching is carried out so that comparisons between a given on-line image and images in a pre-marked training data set can be performed efficiently and effectively. The system is practical for real-world applications, processing queries at the speed of less than 2 seconds each, including the time to compute the feature vector for the query, on a Pentium Pro PC. Besides its exceptional speed, it has demonstrated 9...
440|Visual Similarity, Judgmental Certainty and Stereo Correspondence|Normal human vision is nearly infallible in modeling the visually sensed physical environment in which it evolved. In contrast, most currently available computer vision systems fall far short of human performance in this task, and further, they are generally not capable of being able to assert the correctness of their judgments. In computerized stereo matching systems, correctness of the similarity/identity-matching is almost never guaranteed. In this paper, we explore the question of the extent to which judgments of similarity/identity can be made essentially error-free in support of obtaining a relatively dense depth model of a natural outdoor scene. We argue for the necessity of simultaneously producing a crude scene-specific semantic &#034;overlay&#034;. For our experiments, we designed awavelet-based stereo matching algorithm and use &#034;classification-trees&#034; to create a primitive semantic overlay of the scene. A series of mutually independent filters has been designed and implemented based on the study of different error sources. Photometric appearance, camera imaging geometry and scene constraints are utilized in these filters. When tested on different sets of stereo images, our system has demonstrated above 97% correctness on asserted matches. Finally,we provide a principled basis for relatively dense depth recovery.
441|A learning algorithm for Boltzmann machines|The computotionol power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections con allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in o very short time. One kind of computation for which massively porollel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the con-straints in the domain being searched. We describe a generol parallel search method, based on statistical mechanics, and we show how it leads to a gen-eral learning rule for modifying the connection strengths so as to incorporate knowledge obout o task domain in on efficient way. We describe some simple examples in which the learning algorithm creates internal representations thot ore demonstrobly the most efficient way of using the preexisting connectivity structure. 1.
443|Understanding Line Drawings of Scenes with Shadows|this paper, how can we recognize the identity of Figs. 2.1 and 2.2? Do we use&#039; learning and knowledge to interpret what we see, or do we somehow automatically see the world as stable and independent bf lighting? What portions of scenes can we understand from local features alone, and what configurations require the use of 1obal hypotheses?  19 In this essay I describe a working collection of computer programs which reconstruct three-dimensional descriptions from line drawings which are obtained from scenes composed of plane-faced objects under various lighting conditions. The system identifies shadow lines and regions, groups regions which belong to the same object, and notices such relations as contact or lack of contact between the objects, support and in-front-of/behind relations between the objects as well as information about the spatial orientation of various regions, all using the description it has generated
444|Optimal perceptual inference|When a vision system creates an interpretation of some input datn, it assigns truth values or probabilities to intcrnal hypothcses about the world. We present a non-dctcrministic method for assigning truth values that avoids many of the problcms encountered by existing relaxation methods. Instead of rcprcscnting probabilitics with real-numbers, we usc a more dircct encoding in which thc probability associated with a hypotlmis is rcprcscntcd by the probability hat it is in one of two states, true or false. Wc give a particular non-deterministic operator, based on statistical mechanics, for updating the truth values of hypothcses. The operator ensures that the probability of discovering a particular combination of hypothcscs is a simplc function of how good that combination is. Wc show that thcrc is a simple relationship bctween this operator and Bayesian inference, and we describe a learning rule which allows a parallel system to converge on a set ofweights that optimizes its perccptt~al inferences. lnt roduction One way of interpreting images is to formulate hypotheses about parts or aspects of the imagc and then decide which of these hypotheses are likely to be correct. Thc probability that each hypothesis is correct is determined partly by its fit to the imagc and partly by its fit to other hypothcses (hat are taken to be correct, so the truth&#039;value of an individual hypothesis cannot be decided in isolation. One method of searching for the most plausible combination of hypotheses is to use a rclaxation process in which a probability is associated with each hypothesis, and the probabilities arc then iteratively modified on the basis of the fit to the imagc and the known relationships bctwcen hypotheses. An attractive property of rclaxation methods is that they can be implemented in parallel hardwarc where one computational unit is used for each possible hypothcsis, and the interactions betwcen hypotheses are implemented by dircct hardwarc connections betwcen the units. Many variations of the basic relaxation idea have becn However, all the current methods suffer from one or more of the following problems:
445|Schema selection and stochastic inference in modular environments|Given a set of stimuli presenting views of some environment, how can one characterize the natural modules or “objects ” that compose the environment? Should a given set of items be encoded as a collection of instances or as a set of rules? Restricted formulations of these questions are addressed by analysis within a new mathematical framework that describes stochastic parallel computation. An algorithm is given for simulating this computation once schemas encoding the modules of the environment have been seIected. The concept of computational temperature is introduced. As this temperature is Iowered, the system appears to display a dramatic tendency to interpret input, even if the evidence for any particular interpretation is very weak. IIltrodoction Our sensory systems are capabIe of representing a vast number of possible stimuli. Our environment presents us with only a smaI1 fraction of the possibilities; this se&amp;ted subset is characterized by many regularities. Our minds encode these regularities, and this gives us some ability to infer the probable current condition of unknown portions of the environment given some Iimited information about the current state. What kind of regularities exist in the environment, and how should they be encoded? This paper presents preliminary results of research founded on the hypothesis that in real environments there exist reguIarities that can be idealized as mathematical structures that are simpIe enough to be anaIyxabIe. Only the simpIest kind of reguhuity is considered here: I will assume that the environment contains modules (objects) that recur exactly, with various states of the environment being comprised of various combinations of these modules. Even this simplest kind of environmental regularity offers interesting Iearning problems and results. It also serves to introduce a general framework capable of treating more subtle types of regularities. And the probIem considered is an important one, for the delineation of moduIes at one level of conceptual representation is a major step in the construction of higher Ievel representations.
446|Dynamic topic models|Scientists need new tools to explore and browse large collections of scholarly literature. Thanks to organizations such as JSTOR, which scan and index the original bound archives of many journals, modern scientists can search digital libraries spanning hundreds of years. A scientist, suddenly
447|Latent dirichlet allocation|We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model. 1.
448|A bayesian hierarchical model for learning natural scene categories|We propose a novel approach to learn and recognize natural scene categories. Unlike previous work [9, 17], it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a “theme”. In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes. 1.
449|The Author-Topic Model for Authors and Documents |We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, &amp; Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics
that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact
inference is intractable for these datasets and
we use Gibbs sampling to estimate the topic
and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model)
and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications
to computing similarity between authors and
entropy of author output.
450|Sparse Gaussian processes using pseudo-inputs|We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M « N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M 2 N) training cost and O(M 2) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime. 1
451|Discovering object categories in image collections|Given a set of images containing multiple object categories, we seek to discover those categories and their image locations without supervision. We achieve this using generative models from the statistical text literature: probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA). In text analysis these are used to discover topics in a corpus using the bag-of-words document representation. Here we discover topics as object categories, so that an image containing instances of several categories is modelled as a mixture of topics. The models are applied to images by using a visual analogue of a word, formed by vector quantizing SIFT like region descriptors. We investigate a set of increasingly demanding scenarios, starting with image sets containing only two object categories through to sets containing multiple categories (including airplanes, cars, faces, motorbikes, spotted cats) and background clutter. The object categories sample both intra-class and scale variation, and both the categories and their approximate spatial layout are found without supervision. We also demonstrate classification of unseen images and images containing multiple objects. Performance of the proposed unsupervised method is compared to the semi-supervised approach of [7].  
452|Integrating topics and syntax|Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words. We present a generative model that uses both kinds of dependencies, and can be used to simultaneously find syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency. This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively. 1
453|A Generalized Mean Field Algorithm for Variational Inference in Exponential Families|We present a class of generalized mean field (GMF) algorithms for approximate inference in exponential family graphical models which is analogous to the generalized belief propagation (GBP) or cluster variational methods. While those methods are based on...
454|Collaborative Filtering: A Machine Learning Perspective|Collaborative filtering was initially proposed as a framework for filtering information based on the preferences of users, and has since been refined in many different ways. This thesis is a comprehensive study of rating-based, pure, non-sequential collaborative filtering. We analyze existing methods for the task of rating prediction from a machine learning perspective. We show that many existing methods proposed for this task are simple applications or modi  cations of one or more standard machine learning methods for classifi cation, regression, clustering, dimensionality reduction, and density estimation. We introduce new prediction methods in all of these classes. We introduce a new experimental procedure for testing stronger forms of generalization than has been used previously. We implement a total of nine prediction methods, and conduct large scale prediction accuracy experiments. We show interesting new results on the relative performance of these methods.
455|Applying Discrete PCA in Data Analysis|Methods for analysis of principal components in discrete data have existed for some time under various names such as grade of membership modelling, probabilistic latent semantic analysis, and genotype inference with admixture. In this paper we explore a number of extensions to the common theory, and present some application of these methods to some common statistical tasks. We show that these methods can be interpreted as a discrete version of ICA. We develop a hierarchical version yielding components at different levels of detail, and additional techniques for Gibbs sampling. We compare the algorithms on a text prediction task using support vector machines, and to information retrieval.  
456|The authorrecipienttopic model for topic and role discovery in social networks: Experiments with Enron and academic email|Previous work in social network analysis (SNA) has modeled the existence of links from one entity to another, but not the language content or topics on those links. We present the Author-Recipient-Topic (ART) model for social network analysis, which learns topic distributions based on the the directionsensitive messages sent between entities. The model builds on Latent Dirichlet Allocation and the Author-Topic (AT) model, adding the key attribute that distribution over topics is conditioned distinctly on both the sender and recipient—steering the discovery of topics according to the relationships between people. We give results on both the Enron email corpus and a researcher’s email archive, providing evidence not only that clearly relevant topics are discovered, but that the ART model better predicts people’s roles. 1
458|Simplicial mixtures of Markov chains: Distributed modelling of dynamic user profiles|To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the definition of individual specific and global models. This paper proposes a lineartime distributed model for finite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be ‘explained ’ by a relatively small number of structurally simple common behavioral patterns which may interleave randomly in a user-specific proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efficient representation scheme, reflected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations. 1
459|Model-Based Clustering, Discriminant Analysis, and Density Estimation|Cluster analysis is the automated search for groups of related observations in a data set. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as \How many clusters are there?&#034;, &#034;Which clustering method should be used?&#034; and \How should outliers be handled?&#034;. We outline a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, mineeld detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology, a...
460|Bayes Factors|In a 1935 paper, and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P -values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this paper we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology and psychology.
462|Bayesian Density Estimation and Inference Using Mixtures|We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation, and are exemplified by special cases where data are modelled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models. Keywords: Kernel estimation; Mixtures of Dirichlet processes; Multimodality; Normal mixtures; Posterior sampling; Smoothing parameter estimation * Michael D. Escobar is Assistant Professor, Department of Statistics and Department of Preventive Medicine and Biostatistics, University ...
463|Bayesian Model Selection in Social Research (with Discussion by Andrew Gelman &amp; Donald B. Rubin, and Robert M. Hauser, and a Rejoinder)  (1995) |It is argued that P-values and the tests based upon them give unsatisfactory results, especially in large samples. It is shown that, in regression, when there are many candidate independent variables, standard variable selection procedures can give very misleading results. Also, by selecting a single model, they ignore model uncertainty and so underestimate the uncertainty about quantities of interest. The Bayesian approach to hypothesis testing, model selection and accounting for model uncertainty is presented. Implementing this is straightforward using the simple and accurate BIC approximation, and can be done using the output from standard software. Specific results are presented for most of the types of model commonly used in sociology. It is shown that this approach overcomes the difficulties with P values and standard model selection procedures based on them. It also allows easy comparison of non-nested models, and permits the quantification of the evidence for a null hypothesis...
465|Regularized discriminant analysis|Linear and quadratic discriminant analysis are considered in the small sample high-dimensional setting. Alternatives to the usual maximum likelihood (plug-in) estimates for the covariance matrices are proposed. These alternatives are characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample based estimate of future misclassification risk. Computationally fast implementations are presented, and the efficacy of the approach is examined through simulation studies and application to data. These studies indicate that in many circumstances dramatic gains in classification accuracy can be achieved. Submitted to Journal of the American Statistical Association
467|Discriminant Analysis by Gaussian Mixtures|Fisher-Rao linear discriminant analysis (LDA) is a valuable tool for multigroup classification. LDA is equivalent to maximum likelihood classification assuming Gaussian distributions for each class. In this paper, we fit Gaussian mixtures to each class to facilitate effective classification in non-normal settings, especially when the classes are clustered. Low dimensional views are an important by-product of LDA---our new techniques inherit this feature. We are able to control the within-class spread of the subclass centers relative to the between-class spread. Our technique for fitting these models permits a natural blend with nonparametric versions of LDA.  Keywords: Classification, Pattern Recognition, Clustering, Nonparametric, Penalized. 1 Introduction  In the generic classification or discrimination problem, the outcome of interest  G falls into J unordered classes, which for convenience we denote by the set J =  f1; 2; 3; \Delta \Delta \Delta Jg. We wish to build a rule for pred...
468|Practical Bayesian Density Estimation Using Mixtures Of Normals|this paper, we propose some solutions to these problems. Our goal is to come up with a simple, practical method for estimating the density. This is an interesting problem in its own right, as well as a first step towards solving other inference problems, such as providing more flexible distributions in hierarchical models. To see why the posterior is improper under the usual reference prior, we write the model in the following way. Let Z = (Z 1 ; : : : ; Z n ) and X = (X 1 ; : : : ; X n ). The Z
470|Detecting Features in  Spatial Point Processes with . . .|We consider the problem of detecting features in spatial point processes in the presence of substantial clutter. One example is the detection of mine elds using reconnaissance aircraft images that erroneously identify many objects that are not mines. Another is the detection of seismic faults on the basis of earthquake catalogs: earthquakes tend to be clustered close to the faults, but there are many that are farther away. Our solution uses model-based clustering based on a mixture model for the process, in which features are assumed to generate points according to highly linear multivariate normal densities, and the clutter arises according to a spatial Poisson process. Very nonlinear features are represented by several highly linear multivariate normal densities, giving a piecewise linear representation. The model is estimated in two stages. In the rst stage, hierarchical model-based clustering is used to provide a rst estimate of the features. In the second stage, this clustering is re ned using the EM algorithm. The number of features is found using an approximation to the posterior probability of each number of features. For the minefield
471|MCLUST: Software for Model-based Cluster Analysis|MCLUST is a software package for cluster analysis written in Fortran and interfaced to the S-PLUS commercial software package1. It implements parameterized Gaussian hierarchical clustering algorithms [16, 1, 7] and the EM algorithm for parameterized Gaussian mixture models [5, 13, 3, 14] with the possible addition of a Poisson noise term. MCLUST also includes functions that combine hierarchical clustering, EM and the Bayesian Information Criterion (BIC) in a comprehensive clustering strategy [4, 8]. Methods of this type have shown promise in a number of practical applications, including character recognition [16], tissue segmentation [1], mine eld and seismic fault detection [4], identi cation of textile aws from images [2], and classi cation of astronomical data [3, 15]. Aweb page with related links can be found at
472|Algorithms for model-based Gaussian hierarchical clustering|1 Funded by the O ce of Naval Research under contracts N00014-96-1-0192 and N00014-96-1-
473|Regularized Gaussian Discriminant Analysis Through Eigenvalue Decomposition|Friedman (1989) has proposed a regularization technique (RDA) of discriminant analysis in the Gaussian framework. RDA makes use of two regularization parameters to design an intermediate classi cation rule between linear and quadratic discriminant analysis. In this paper, we propose an alternative approach to design classi cation rules which have also a median position between linear and quadratic discriminant analysis. Our approach is based on the reparametrization of the covariance matrix k of a group Gk in terms of its eigenvalue decomposition, k = kDkAkD 0 k where k speci es the volume of Gk, Ak its shape, and Dk its orientation. Variations on constraints concerning k?Ak and Dk lead to 14 discrimination models of interest. For each model, we derived the maximum likelihood parameter estimates and our approach consists in selecting the model among the 14 possible models by minimizing the sample-based estimate of future misclassi cation risk by cross-validation. Numerical experiments show favorable behavior of this approach as compared to RDA.
474|Scaling EM (Expectation-Maximization) Clustering to Large Databases  (1999) |Practical statistical clustering algorithms typically center upon an iterative refinement optimization procedure to compute a locally optimal clustering solution that maximizes the fit to data. These algorithms typically require many database scans to converge, and within each scan they require the access to every record in the data table. For large databases, the scans become prohibitively expensive. We present a scalable implementation of the Expectation-Maximization (EM) algorithm. The database community has focused on distance-based clustering schemes and methods have been developed to cluster either numerical or categorical data. Unlike distancebased algorithms (such as K-Means), EM constructs proper statistical models of the underlying data source and naturally generalizes to cluster databases containing both discrete-valued and continuous-valued data. The scalable method is based on a decomposition of the basic statistics the algorithm needs: identifying regions of the data that...
475|Averaging, Maximum Penalized Likelihood and Bayesian Estimation for Improving Gaussian Mixture Probability Density Estimates|We apply the idea of averaging ensembles of estimators to probability density estimation.
476|Non Parametric Maximum Likelihood Estimation of Features in . . .|This paper addresses the problem of estimating the support domain of a bounded point process in presence of background noise. This situation occurs for example in the detection of a mine field from aerial observations. A Maximum Likelihood Estimator for a mixture of uniform point processes is derived using a natural partition of the space defined by the data themselves: the Voronoï tessellation. The methodology is tested on simulations and compared to a model-based clustering technique.
477|Inference in model-based cluster analysis|A new approach to cluster analysis has been introduced based on parsimonious geometric modelling of the within-group covariance matrices in a mixture of multivariate normal distributions, using hierarchical agglomeration and iterative relocation. It works well and is widely used via the MCLUST software available in S-PLUS and StatLib. However, it has several limitations: there is no assessment of the uncertainty about the classification, the partition can be suboptimal, parameter estimates are biased, the shape matrix has to be specified by the user, prior group probabilities are assumed to be equal, the method for choosing the number of groups is based on a crude approximation, and no formal way of choosing between the various possible models is included. Here, we propose a new approach which overcomes all these difficulties. It consists of exact Bayesian inference via Gibbs sampling, and the calculation of Bayes factors (for choosing the model and the number of groups) from the output using the Laplace-Metropolis estimator. It works well in several real and simulated examples.  
478|Principal curve clustering with noise|was supported by ONR grants N00014-96-1-0192 and N00014-96-1-0330. The authors are Clustering on principal curves combines parametric modeling of noise with nonparametric modeling of feature shape. This is useful for detecting curvilinear features in spatial point patterns, with or without background noise. Applications of this include the detection of curvilinear mine elds from reconnaissance images, some of the points in which represent false detections, and the detection of seismic faults from earthquake catalogs. Our algorithm for principal curve clustering is in two steps: the rst is hierarchical and agglomerative (HPCC), and the second consists of iterative relocation based on the Classi cation EM algorithm (CEM-PCC). HPCC is used to combine potential feature clusters, while CEM-PCC re nes the results and deals with background noise. It is importanttohave a good starting point for the algorithm: this can be found manually or automatically using, for example, nearest neighbor clutter removal or model-based clustering. We choose the number of features and the amount of smoothing simultaneously using approximate Bayes
479|Fitting mixtures of Kent distributions to aid in joint set identification|When examining a rock mass, joint sets and their orientations can play a significant role with regard to how the rock mass will behave. To identify joint sets present in the rock mass, the orientation of individual fracture planes can be measured on exposed rock faces and the resulting data can be examined for heterogeneity. In this article, the expectation–maximization algorithm is used to fit mixtures of Kent component distributions to the fracture data to aid in the identification of joint sets. An additional uniform component is also included in the model to accommodate the noise present in the data.
480|From Massive Data Sets to Science Catalogs: Applications and Challenges|With hardware advances in scientific instruments and data gathering techniques comes the  inevitable flood of data that can render traditional approaches to science data analysis severely  inadequate. The traditional approach of manual and exhaustive analysis of a data set is no longer  feasible for many tasks ranging from remote sensing, astronomy, and atmospherics to medicine,  molecular biology, and biochemistry. In this paper we present our views as practitioners engaged  in building computational systems to help scientists deal with large data sets. We focus on  what we view as challenges and shortcomings of the current state-of-the-art in data analysis in  view of the massive data sets that are still awaiting analysis. The presentation is grounded in  applications in astronomy, planetary sciences, solar physics, and atmospherics that are currently  driving much of our work at JPL.  keywords: science data analysis, limitations of current methods, challenges for massive data sets, ...
481|Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions|This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multcriteria ratings, and a provision of more flexible and less intrusive types of recommendations. 
484|Grouplens: Applying collaborative filtering to usenet news|... a collaborative filtering system for Usenet news—a high-volume, high-turnover discussion list service on the Internet. Usenet newsgroups—the individual discussion lists—may carry hundreds of messages each day. While in theory the newsgroup organization allows readers to select the content that most interests them, in practice most newsgroups carry a wide enough spread of messages to make most individuals consider Usenet news to be a high noise information resource. Furthermore, each user values a different set of messages. Both taste and prior knowledge are major factors in evaluating news articles. For example, readers of the rec.humor newsgroup, a group designed for jokes and other humorous postings, value articles based on whether they perceive them to be funny. Readers of technical groups, such as comp.lang.c? ? value articles based
485|Probabilistic Latent Semantic Analysis|Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two--mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.
486|Active Learning with Statistical Models|For manytypes of learners one can compute the statistically &#034;optimal&#034; way to select data. We review how  these techniques have been used with feedforward neural networks [MacKay, 1992# Cohn, 1994]. We then  showhow the same principles may be used to select data for two alternative, statistically-based learning  architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural  networks are expensive and approximate, the techniques for mixtures of Gaussians and locally weighted  regression are both efficient and accurate.
487|NewsWeeder: Learning to Filter Netnews|A significant problem in many information filtering systems is the dependence on the user for the creation and maintenance of a user profile, which describes the user&#039;s interests. NewsWeeder is a netnews-filtering system that addresses this problem by letting the user rate his or her interest level for each article being read (1-5), and then learning a user profile based on these ratings. This paper describes how NewsWeeder accomplishes this task, and examines the alternative learning methods used. The results show that a learning algorithm based on the Minimum Description Length (MDL) principle was able to raise the percentage of interesting articles to be shown to users from 14% to 52% on average. Further, this performance significantly outperformed (by 21%) one of the most successful techniques in Information Retrieval (IR), termfrequency /inverse-document-frequency (tf-idf) weighting. 1
488|Improving generalization with active learning|Abstract. Active learning differs from &amp;quot;learning from examples &amp;quot; in that the learning algorithm assumes at least some control over what part of the input domain it receives information about. In some situations, active learning is provably more powerful than learning from examples alone, giving better generalization for a fixed number of training examples. In this article, we consider the problem of learning a binary concept in the absence of noise. We describe a formalism for active concept learning called selective sampling and show how it may be approximately implemented by a neural network. In selective sampling, a learner receives distribution information from the environment and queries an oracle on parts of the domain it considers &amp;quot;useful. &amp;quot; We test our implementation, called an SGnetwork, on three domains and observe significant improvement in generalization.
489|Selective sampling using the Query by Committee algorithm|We analyze the &#034;query by committee&#034; algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons.
490|Learning and Revising User Profiles: The Identification of Interesting Web Sites|. We discuss algorithms for learning and revising user profiles that can determine which World Wide Web sites on a given topic would be interesting to a user. We describe the use of a naive Bayesian classifier for this task, and demonstrate that it can incrementally learn profiles from user feedback on the interestingness of Web sites. Furthermore, the Bayesian classifier may easily be extended to revise user provided profiles. In an experimental evaluation we compare the Bayesian classifier to computationally more intensive alternatives, and show that it performs at least as well as these approaches throughout a range of different domains. In addition, we empirically analyze the effects of providing the classifier with background knowledge in form of user defined profiles and examine the use of lexical knowledge for feature selection. We find that both approaches can substantially increase the prediction accuracy.  Keywords: Information filtering, intelligent agents, multistrategy lea...
491|Eigentaste: A Constant Time Collaborative Filtering Algorithm|Eigentaste is a collaborative filtering algorithm that uses universal queries to elicit real-valued user ratings on a common set of items and applies principal component analysis (PCA) to the resulting dense subset of the ratings matrix. PCA facilitates dimensionality reduction for offline clustering of users and rapid computation of recommendations. For a database of n users, standard nearest-neighbor techniques require O(n) processing time to compute recommendations, whereas Eigentaste requires O(1) (constant) time. We compare Eigentaste to alternative algorithms using data from Jester, an online joke recommending system. Jester has collected approximately 2,500,000 ratings from 57,000 users. We use the Normalized Mean Absolute Error (NMAE) measure to compare performance of different algorithms. In the Appendix we use Uniform and Normal distribution models to derive analytic estimates of NMAE when predictions are random. On the Jester dataset, Eigentaste computes recommendations two ...
492|Learning Collaborative Information Filters|Predicting items a user would like on the basis of other users’ ratings for these items has become a well-established strategy adopted by many recommendation services on the Internet. Although this can be seen as a classification problem, algo-rithms proposed thus far do not draw on results from the ma-chine learning literature. We propose a representation for collaborative filtering tasks that allows the application of virtually any machine learning algorithm. We identify the shortcomings of current collaborative filtering techniques and propose the use of learning algorithms paired with feature extraction techniques that specifically address the limitations of previous approaches. Our best-performing algorithm is based on the singular value decomposition of an initial matrix of user ratings, exploiting latent structure that essentially eliminates the need for users to rate common items in order to become predictors for one another&#039;s preferences. We evaluate the proposed algorithm on a large database of user ratings for motion pictures and find that our approach significantly out-performs current collaborative filtering algorithms.
493|Recommendation as Classification: Using Social and Content-Based Information in Recommendation|Recommendation systems make suggestions about artifacts to a user. For instance, they may predict whether a user would be interested in seeing a particular movie. Social recomendation methods collect ratings of artifacts from many individuals and use nearest-neighbor techniques to make recommendations to a user concerning new artifacts. However, these methods do not use the significant amount of other information that is often available about the nature of each artifact --- such as cast lists or movie reviews, for example. This paper presents an inductive learning approach to recommendation that is able to use both ratings information and other forms of information about each artifact in predicting user preferences. We show that our method outperforms an existing social-filtering method in the domain of movie recommendations on a dataset of more than 45,000 movie ratings collected from a community of over 250 users.  Introduction  Recommendations are a part of everyday life. We usually...
494|Latent Semantic Models for Collaborative filtering |Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, that is, a database of available user preferences. In this article, we describe a new family of model-based algorithms designed for this task. These algorithms rely on a statistical modelling technique that introduces latent class variables in a mixture model setting to discover user communities and prototypical interest profiles. We investigate several variations to deal with discrete and continuous response variables as well as with different objective functions. The main advantages of this technique over standard memory-based methods are higher accuracy, constant time prediction, and an explicit and compact model representation. The latter can also be used to mine for user communitites. The experimental evaluation shows that substantial improvements in accucracy over existing methods and published results can be obtained.
495|Content-Boosted Collaborative Filtering for Improved Recommendations|Most recommender systems use Collaborative Filtering or Content-based methods to predict new items of interest for a user. While both methods have their own advantages, individually they fail to provide good recommendations in many situations. Incorporating components from both methods, a hybrid recommender system can overcome these shortcomings.
496|Content-Based Book Recommending Using Learning for Text Categorization|Recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user&#039;s likes and dislikes. Most existing recommender systems use collaborative filtering methods that base recommendations on other users&#039; preferences. By contrast, content-based methods use information about an item itself to make suggestions. This approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations. We describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization. Initial experimental results demonstrate that this approach can produce accurate recommendations.
497|Multicriteria Optimization|n Using some real-world examples I illustrate the important role of multiobjective optimization in decision making and its interface with preference handling. I explain what optimization in the presence of multiple objectives means and discuss some of the most common methods of solving multiobjective optimization problems using transformations to single-objective optimization problems. Finally, I address linear and combinatorial optimization problems with multiple objectives and summarize techniques for solving them. Throughout the article I
498|Heterogeneous uncertainty sampling for supervised learning|Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger. 1
499|Item-Based Top-N Recommendation Algorithms|... In this paper we present one such class of model-based recommendation algorithms that first determines the similarities between the various items and then uses them to identify the set of items to be recommended. The key steps in this class of algorithms are (i) the method used to compute the similarity between the items, and (ii) the method used to combine these similarities in order to compute the similarity between a basket of items and a candidate recommender item. Our experimental evaluation on eight real datasets shows that these item-based algorithms are up to two orders of magnitude faster than the traditional user-neighborhood based recommender systems and provide recommendations with comparable or better quality
500|Combining collaborative filtering with personal agents for better recommendations|Information filtering agents and collaborative filtering both attempt to alleviate information overload by identifying which items a user will find worthwhile. Information filtering (IF) focuses on the analysis of item content and the development of a personal user interest profile. Collaborative filtering (CF) focuses on identification of other users with similar tastes and the use of their opinions to recommend items. Each technique has advantages and limitations that suggest that the two could be beneficially combined. This paper shows that a CF framework can be used to combine personal IF agents and the opinions of a community of users to produce better recommendations than either agents or users can produce alone. It also shows that using CF to create a personal combination of a set of agents produces better results than either individual agents or other combination mechanisms. One key implication of these results is that users can avoid having to select among agents; they can use them all and let the CF framework select the best ones for them.
501|Incorporating Contextual Information in Recommender Systems Using a Multidimensional Approach|The paper presents a multidimensional (MD) approach to recommender systems that can provide recommendations based on additional contextual information besides the typical information on users and items used in most of the current recommender systems. This approach supports multiple dimensions, extensive profiling, and hierarchical aggregation of recommendations. The paper also presents a multidimensional rating estimation method capable of selecting two-dimensional segments of ratings pertinent to the recommendation context and applying standard collaborative filtering or other traditional two-dimensional rating estimation techniques to these segments. A comparison of the multidimensional and two-dimensional rating estimation approaches is made, and the tradeoffs between the two are studied. Moreover, the paper introduces a combined rating estimation method that identifies the situations where the MD approach outperforms the standard two-dimensional approach and uses the MD approach in those situations and the standard two-dimensional approach elsewhere. Finally, the paper presents a pilot empirical study of the combined approach, using a multidimensional movie recommender system that was developed for implementing this approach and testing its performance. 1 1.
502|Collaborative Filtering by Personality Diagnosis: A Hybrid Memory- and Model-Based Approach|The growth of Internet commerce has stimulated  the use of collaborative filtering (CF) algorithms  as recommender systems. Such systems leverage  knowledge about the known preferences of  multiple users to recommend items of interest to  other users. CF methods have been harnessed  to make recommendations about such items as  web pages, movies, books, and toys. Researchers  have proposed and evaluated many approaches  for generating recommendations. We describe  and evaluate a new method called personality  diagnosis (PD). Given a user&#039;s preferences for  some items, we compute the probability that he  or she is of the same &#034;personality type&#034; as other  users, and, in turn, the probability that he or she  will like new items. PD retains some of the advantages  of traditional similarity-weighting techniques  in that all data is brought to bear on each  prediction and new data can be added easily and  incrementally. Additionally, PD has a meaningful  probabilistic interpretation, which ma...
503|Clustering Methods for Collaborative Filtering|Grouping people into clusters based on the items they have purchased allows accurate recommendations of new items for purchase: if you and I have liked many of the same movies, then I will probably enjoy other movies that you like. Recommending items based on similarity of interest (a.k.a. collaborative filtering) is attractive for many domains: books, CDs, movies, etc., but does not always work well. Because data are always sparse -- any given person has seen only a small fraction of all movies -- much more accurate predictions can be made by grouping people into clusters with similar movies and grouping movies into clusters which tend to be liked by the same people. Finding optimal clusters is tricky because the movie groups should be used to help determine the people groups and visa versa. We present a formal statistical model of collaborative filtering, and compare different algorithms for estimating the model parameters including variations of K-means clustering and Gibbs Sampling. This...
504|E-Commerce Recommendation Applications|Recommender systems are being used by an ever-increasing number of E-commerce sites to help consumers find products to purchase. What started as a novelty has turned into a serious business tool. Recommender systems use product knowledge -- either hand-coded knowledge provided by experts or &#034;mined&#034; knowledge learned from the behavior of consumers -- to guide consumers through the often-overwhelming task of locating products they will like. In this article we present an explanation of how recommender systems are related to some traditional database analysis techniques. We examine how recommender systems help E-commerce sites increase sales and analyze the recommender systems at six market-leading sites. Based on these examples, we create a taxonomy of recommender systems, including the inputs required from the consumers, the additional knowledge required from the database, the ways the recommendations are presented to consumers, the technologies used to create the recommendations, and t...
505|The digitization of word of mouth: Promise and challenges of online feedback mechanisms|Online feedback mechanisms harness the bidirectional communication capabilities of the Internet to engineer large-scale, word-of-mouth networks. Best known so far as a technology for building trust and fostering cooperation in online marketplaces, such as eBay, these mechanisms are poised to have a much wider impact on organizations. Their growing popularity has potentially important implications for a wide range of management activities such as brand building, customer acquisition and retention, product development, and quality assurance. This paper surveys our progress in understanding the new possibilities and challenges that these mechanisms represent. It discusses some important dimensions in which Internet-based feedback mechanisms differ from traditional word-of-mouth networks and surveys the most important issues related to their design, evaluation, and use. It provides an overview of relevant work in game theory and economics on the topic of reputation. It discusses how this body of work is being extended and combined with insights from computer science, management science, sociology, and psychology to take into consideration the special properties of online environments. Finally, it identifies opportunities that this new area presents for operations research/management science (OR/MS) research.
506|Horting Hatches an Egg: A New Graph-Theoretic Approach to Collaborative Filtering|This paper introduces a new and novel approach to ratingbased collaborative filtering. The new technique is most appropriate for e-commerce merchants offering one or more groups of relatively homogeneous items such as compact disks, videos, books, software and the like. In contrast with other known collaborative filtering techniques, the new algorithm is graph-theoretic, based on the twin new concepts of horting and predictability. As is demonstrated in this paper, the technique is fast, scalable, accurate, and requires only a modest learning curve. It makes use of a hierarchical classification scheme in order to introduce context into the rating process, and uses so-called creative links in order to find surprising and atypical items to recommend, perhaps even items which cross the group boundaries. The new technique is one of the key engines of the Intelligent Recommendation Algorithm (IRA) project, now being developed at IBM Research. In addition to several other recommendation engines, IRA contains a situation analyzer to determine the most appropriate mix of engines for a particular e-commerce merchant, as well as an engine for optimizing the placement of advertisements.
507|Discovery and Evaluation of Aggregate Usage Profiles for Web Personalization|Web usage mining, possibly used in conjunction with standard approaches to personalization such as collaborative filtering, can help address some of the shortcomings of these techniques, including reliance on subjective user ratings, lack of scalability, and poor performance in the face of high-dimensional and sparse data. However, the discovery of patterns from usage data by itself is not sufficient for performing the personalization tasks. The critical step is the effective derivation of good quality and useful (i.e., actionable) &#034;aggregate usage profiles&#034; from these patterns. In this paper we present and experimentally evaluate two techniques, based on clustering of user transactions and clustering of pageviews, in order to discover overlapping aggregate profiles that can be effectively used by recommender systems for real-time Web personalization. We evaluate these techniques both in terms of the quality of the individual profiles generated, as well as in the context of providing recommendations as an integrated part of a personalization engine. In particular, our results indicate that using the generated aggregate profiles, we can achieve effective personalization at early stages of users&#039; visits to a site, based only on anonymous clickstream data and without the benefit of explicit input by these users or deeper knowledge about them.
508|Applying Associative Retrieval Techniques to Alleviate the Sparsity Problem in Collaborative Filtering|this article, we propose to deal with this sparsity problem by applying an associative retrieval framework and related spreading activation algorithms to explore transitive associations among consumers through their past transactions and feedback. Such transitive associations are a valuable source of information to help infer consumer interests and can be explored to deal with the sparsity problem. To evaluate the effectiveness of our approach, we have conducted an experimental study using a data set from an online bookstore. We experimented with three spreading activation algorithms including a constrained Leaky Capacitor algorithm, a branch-and-bound serial symbolic search algorithm, and a Hopfield net parallel relaxation search algorithm. These algorithms were compared with several collaborative filtering approaches that do not consider the transitive associations: a simple graph search approach, two variations of the user-based approach, and an item-based approach. Our experimental results indicate that spreading activation-based approaches significantly outperformed the other collaborative filtering methods as measured by recommendation precision, recall, the F-measure, and the rank score. We also observed the over-activation effect of the spreading activation approach, that is, incorporating transitive associations with past transactional data that is not sparse may &#034;dilute&#034; the data used to infer user preferences and lead to degradation in recommendation performance
509|MovieLens Unplugged: Experiences with an Occasionally Connected Recommender System|Recommender systems have changed the way people shop online. Recommender systems on wireless mobile devices may have the same impact on the way people shop in stores. We present our experience with implementing a recommender system on a PDA that is occasionally connected to the network. This interface helps users of the MovieLens movie recommendation service select movies to rent, buy, or see while away from their computer. The results of a nine month field study show that although there are several challenges to overcome, mobile recommender systems have the potential to provide value to their users today.
510|Implicit Feedback for Recommender System|Can implicit feedback substitute for explicit ratings in recommender systems? If so, we could avoid the difficulties associated with gathering explicit ratings from users. How, then, can we capture useful information unobtrusively, and how might we use that information to make recommendations? In this paper we identify three types of implicit feedback and suggest two strategies for using implicit feedback to make recommendations.
511|Collaborative Filtering via Gaussian Probabilistic Latent Semantic Analysis|Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, i.e. a database of available user preferences. In this paper, we describe a new model-based algorithm designed for this task, which is based on a generalization of probabilistic latent semantic analysis to continuous-valued response variables. More specifically, we assume that the observed user ratings can be modeled as a mixture of user communities or interest groups, where users may participate probabilistically in one or more groups. Each community is characterized by a Gaussian distribution on the normalized ratings for each item. The normalization of ratings is performed in a user-specific manner to account for variations in absolute shift and variance of ratings. Experiments on the EachMovie data set show that the proposed approach compares favorably with other collaborative filtering techniques.
512|Flexible Mixture Model for Collaborative Filtering|This paper presents a flexible mixture model  (FMM) for collaborative filtering. FMM extends  existing partitioning/clustering algorithms for  collaborative filtering by clustering both users  and items together simultaneously without  assuming that each user and item should only  belong to a single cluster. Furthermore, with the  introduction of `preference&#039; nodes, the proposed  framework is able to explicitly model how users  rate items, which can vary dramatically, even  among the users with similar tastes on items.
514|Combining Content and Collaboration in Text Filtering|We describe a technique for combining collaborative input and document content for text filtering. This technique uses latent semantic indexing to create a collaborative view of a collection of user profiles. The profiles themselves are term vectors constructed from documents deemed relevant to the user&#039;s information need. Using standard text collections, this approach performs quite favorably compared to other content-based approaches.  1 Introduction  Filtering is a process of comparing an incoming document stream to a profile of a user&#039;s interests and recommending the documents according to that profile [ Belkin and Croft, 1992 ] . A simple approach for filtering textual content might be to look at each document&#039;s similarity to an average of known relevant documents. Collaborative filtering takes into account the similarities and differences among the profiles of several users in determining how to recommend a document. Typically, collaborative filtering is done by correlating users...
515|Recommendation Systems: A Probabilistic Analysis|A recommendation system tracks past actions of a group of users to make recommendations to individual members of the group. The growth of computer-mediated marketing and commerce has led to increased interest in such systems. We introduce a simple analytical framework for recommendation systems, including a basis for defining the utility of such a system. We perform probabilistic analyses of algorithmic methods within this framework. These analyses yield insights into how much utility can be derived from the memory of past actions and on how this memory can be exploited. 1. Introduction  Collaborative filtering (sometimes known as a recommendation  system) is a process by which information on the preferences and actions of a group of users is tracked by a system which then, based on the patterns it observes, tries to make useful recommendations to individual users [10, 12, 18, 19, 20, 22, 23]. For instance, a book recommendation system might recommend Jules Verne to someone interested ...
516|Bayesian Mixed-Effects Models for Recommender Systems|We propose a Bayesian methodology for recommender systems that incorporates user ratings, user features, and item features in a single unified framework. In principle our approach should address the cold-start issue and can address both scalability issues as well as sparse ratings. However, our early experiments have shown mixed results.  1 Introduction  Recommender systems have emerged as an important application area and have been the focus of considerable recent academic and commercial interest. The 1997 special issue of the Communications of the ACM [14] contains some key papers. Other important contributions include [2], [4], [8], [13], [16], [9], [1], [12], and [15]. In addition, many online retailers are using this technology to recommend new items to their customers, based on what they have bought in the past. Currently, most recommender systems are either  content-based or collaborative, depending on the type of information that the system uses to recommend items to a user. Co...
517|The TREC-6 Filtering Track: Description and Analysis|This article details the experiments conducted in the TREC-6 filtering track. The filtering track is an extension of the routing track which adds time sequencing of the document stream and set-based evaluation strategies which simulate immediate distribution of the retrieved documents. It also introduces an adaptive filtering subtrack which is designed to simulate on-line or sequential filtering of documents. In addition to motivating the task and describing the practical details of participating in the track, this document includes a detailed graphical presentation of the experimental results and attempts to analyze and explain the observed patterns. The final section suggests some ways to extend the current research in future experiments. 1 Introduction  There is increasing evidence that text filtering will become a critical tool in searching and managing the flow of data in the information age. New companies are appearing daily which offer push services or intelligent agents centere...
518|Expert-driven validation of rule-based user models in personalization 9 |Abstract. In many e-commerce applications, ranging from dynamic Web content presentation, to personalized ad targeting, to individual recommendations to the customers, it is important to build personalized profiles of individual users from their transactional histories. These profiles constitute models of individual user behavior and can be specified with sets of rules learned from user transactional histories using various data mining techniques. Since many discovered rules can be spurious, irrelevant, or trivial, one of the main problems is how to perform post-analysis of the discovered rules, i.e., how to validate user profiles by separating “good ” rules from the “bad.” This validation process should be done with an explicit participation of the human expert. However, complications may arise because there can be very large numbers of rules discovered in the applications that deal with many users, and the expert cannot perform the validation on a rule-by-rule basis in a reasonable period of time. This paper presents a framework for building behavioral profiles of individual users. It also introduces a new approach to expert-driven validation of a very large number of rules pertaining to these users. In particular, it presents several types of validation operators, including rule grouping, filtering, browsing, and redundant rule elimination operators, that allow a human expert validate many individual rules at a time. By iteratively applying such operators, the human expert can validate a significant part of all the initially discovered rules in an acceptable time period. These validation operators were implemented as a part of a one-to-one profiling system. The paper also presents
519|Privacy risks in recommender systems|this article, we can state some general guidelines. Like most problems in computer security, the ideal deterrents are better awareness of the issues and more openness in how systems operate in the marketplace. In particular, individual sites should clearly state the policies and methodologies they employ with recommender systems, including the role played by straddlers in their data sets and system designs. This is especially true for sites with multiple homogeneous networks (as in Figures 6c and 6d).   By conveying benefits and risks to users intuitively, recommender systems could achieve greater acceptance. We envisage three general ways to highlight the implications of our analyses. # Present plots of benefit and risk versus usermodifiable parameters such as ratings, w, and l (if the algorithm allows their direct specification) to allow users to make informed choices about their levels of involvement
520|Combining Usage, Content, and Structure Data to Improve Web Site Recommendation|Web recommender systems anticipate the needs of web users  and provide them with recommendations to personalize their navigation.
521|A Bayesian Model for Collaborative Filtering|Consider the general setup where a set of items have been partially rated by a set of judges, in the sense that not every item has been rated by every judge. For this setup, we propose a Bayesian approach for the problem of predicting the missing ratings from the observed ratings. This approach incorporates similarity by assuming the set of judges can be partitioned into groups which share the same ratings probability distribution. This leads to a predictive distribution of missing ratings based on the posterior distribution of the groupings and associated ratings probabilities. Markov chain Monte Carlo methods and a hybrid search algorithm are then used to obtain predictions of the missing ratings. 1
522|A maximum entropy approach to collaborative filtering in dynamic, sparse, high-dimensional domains|We develop a maximum entropy (maxent) approach to generating recommendations in the context of a user’s current navigation stream, suitable for environments where data is sparse, highdimensional, and dynamic—conditions typical of many recommendation applications. We address sparsity and dimensionality reduction by first clustering items based on user access patterns so as to attempt to minimize the apriori probability that recommendations will cross cluster boundaries and then recommending only within clusters. We address the inherent dynamic nature of the problem by explicitly modeling the data as a time series; we show how this representational expressivity fits naturally into a maxent framework. We conduct experiments on data from ResearchIndex, a popular online repository of over 470,000 computer science documents. We show that our maxent formulation outperforms several competing algorithms in offline tests simulating the recommendation of documents to ResearchIndex users. 1
523|Using Probabilistic Relational Models for Collaborative Filtering|Recent projects in collaborative filtering and information filtering address the task of inferring user preference relationships for products or information. The data on which these inferences are based typically consists of pairs of people and items. The items may be information sources (such as web pages or newspaper articles) or products (such as books, software, movies or CDs). We are interested in making recommendations or predictions. Traditional approaches to the problem derive from classical algorithms in statistical pattern recognition and machine learning. The majority of these approaches assume a &#034;flat&#034; data representation for each object, and focus on a single dyadic relationship between the objects. In this paper, we examine a richer model that allows us to reason about many different relations at the same time. We build on the recent work on probabilistic relational models (PRMs), and describe how PRMs can be applied to the task of collaborative filtering. PRMs allow us t...
524|Characterization and construction of radial basis functions|We review characterizations of (conditional) positive deniteness and show how they apply to the theory of radial basis functions. We then give complete proofs for the (conditional) positive deniteness of all practically relevant basis functions. Furthermore, we show how some of these characterizations may lead to construction tools for positive denite functions. Finally, we give new construction techniques based on discrete methods which lead to non-radial, even nontranslation invariant, local basis functions.  
525|Collaborative Learning for Recommender Systems|Recommender systems use ratings from users on  items such as movies and music for the purpose  of predicting the user preferences on items that  have not been rated. Predictions are normally  done by using the ratings of other users of the  system, by learning the user preference as a function  of the features of the items or by a combination  of both these methods.
526|Book Recommending using Text Categorization with Extracted Information|Content-based recommender systems suggest documents,  items, and services to users based on learning  a profile of the user from rated examples containing  information about the given items. Text categorization  methods are very useful for this task but generally  rely on unstructured text. We have developed a bookrecommending  system that utilizes semi-structured information  about items gathered from the web using  simple information extraction techniques. Initial experimental  results demonstrate that this approach can  produce fairly accurate recommendations.
527|Memory-Based Weighted-Majority Prediction For Recommender Systems|Recommender Systems are learning systems that make use of data representing multi-user preferences over items (e.g. Vote [user, item] matrix), to try to predict the preference towards new items or products regarding a particular user. User preferences are in fact the learning target functions. The main objective of the system is to filter items according to the predicted preferences and present to the user the options that are most attractive to him; i.e. he would probably like the most. We study Recommender Systems viewed as a pool of independent prediction algorithms, one per every user, in situations in which each learner faces a sequence of trials, with a prediction to make in each step. The goal is to make as few mistakes as possible. We are interested in the case that each learner has reasons to believe that there exists some other target functions in the pool that consistently behaves similar, neutral or opposite to the target function it is trying to learn. The learner doesn&#039;t ...
528|Customer Lifetime Value Modeling and Its Use for Customer Retention Planning|We present and discuss the important business problem of estimating the effect of retention efforts on the Lifetime Value of a customer in the Telecommunications industry. We discuss the components of this problem, in particular customer value and length of service (or tenure) modeling, and present a novel segment-based approach, motivated by the segment-level view marketing analysts usually employ. We then describe how we build on this approach to estimate the effects of retention on Lifetime Value. Our solution has been successfully implemented in Amdocs&#039; Business Insight (BI) platform, and we illustrate its usefulness in real-world scenarios.
529|Preference-based Graphic Models for Collaborative Filtering|Collaborative filtering is a very useful general technique for exploiting the preference patterns of a group of users to predict the utility of items to a particular user. Previous research has studied several probabilistic graphic models for collaborative filtering with promising results. However, while these models have succeeded in capturing the similarity among users and items, none of them has considered the fact that users with similar interests in items can have very different rating patterns; some users tend to assign a higher rating to all items than other users. In this paper, we propose and study two new graphic models that address the distinction between user preferences and ratings. In one model, called the decoupled model, we introduce two different variables to decouple a user’s preferences from his/her ratings. In the other, called the preference model, we model the orderings of items preferred by a user, rather than the user’s numerical ratings of items. Empirical study over two datasets of movie ratings shows that, due to its appropriate modeling of the distinction between user preferences and ratings, the proposed decoupled model significantly outperforms all the five existing approaches that we compared with. The preference model, however, performs much worse than the decoupled model, suggesting that while explicit modeling of the underlying user preferences is very important for collaborative filtering, we can not afford ignoring the rating information completely. 1.
530|Adaptive lightweight text filtering|Abstract. We present a lightweight text filtering algorithm intended for use with personal Web information agents. Fast response and low resource usage were the key design criteria, in order to allow the algorithm to run on the client side. The algorithm learns adaptive queries and dissemination thresholds for each topic of interest in its user profile. We describe a factorial experiment used to test the robustness of the algorithm under different learning parameters and more importantly, under limited training feedback. The experiment borrows from standard practice in TREC by using TREC-5 data to simulate a user reading and categorizing documents. Results indicate that the algorithm is capable of achieving good filtering performance, even with little user feedback. 1
531|A theory of memory retrieval|A theory of memory retrieval is developed and is shown to apply over a range of experimental paradigms. Access to memory traces is viewed in terms of a resonance metaphor. The probe item evokes the search set on the basis of probe-memory item relatedness, just as a ringing tuning fork evokes sympathetic vibrations in other tuning forks. Evidence is accumulated in parallel from each probe-memory item comparison, and each comparison is modeled by a continuous random walk process. In item recognition, the decision process is self-terminating on matching comparisons and exhaustive on nonmatching comparisons. The mathematical model produces predictions about accuracy, mean reaction time, error latency, and reaction time distributions that are in good accord with experimental data. The theory is applied to four item recognition paradigms (Sternberg, prememorized list, study-test, and continuous) and to speed-accuracy paradigms; results are found to provide a basis for comparison of these paradigms. It is noted that neural network models can be interfaced to the retrieval theory with little difficulty and that semantic memory models may benefit from such a retrieval scheme.  
532|Distinctive features, categorical perception, and probability learning: some applications of a neural model|A previously proposed model for memory based on neurophysiological considerations is reviewed. We assume that (a) nervous system activity is usefully represented as the set of simultaneous individual neuron activities in a group of neurons; (b) different memory traces make use of the same synapses; and (c) synapses associate two patterns of neural activity by incrementing synaptic connectivity proportionally to the product of pre- and postsynaptic activity, forming a matrix of synaptic connectivities. We extend this model by (a) introducing positive feedback of a set of neurons onto itself and (b) allowing the individual neurons to saturate. A hybrid model, partly analog and partly binary, arises. The system has certain characteristics reminiscent of analysis by distinctive features. Next, we apply the model to &amp;quot;categorical perception. &amp;quot; Finally, we discuss probability learning. The model can predict overshooting, recency data, and probabilities occurring in systems with more than two events with reasonably good accuracy. In the beginner&#039;s mind there are many possibilities, but in the expert&#039;s there are few. —Shunryu Suzuki 1970 I.
533|Retrieval processes in recognition memory|A method of analyzing reaction time data in recognition memory is presented, which uses an explicit model of latency distributions. This distributional method allows us to distinguish between processes in a way that the traditional measure, mean latency, can not. The behavior of latency distributions is described, and four experiments are reported that show how recognition accuracy and latency vary with independent variables such as study and test position, rate of presentation, and list length. These data are used to develop and test the empirical model. The resulting analyses, together with functional relationships derived from the experimental data, are used to test several theories of recognition memory. The theories examined all show problems in light of these stringent tests, and general properties required by a model to account for the data are suggested. As well as arguing for distributional analyses of reaction time data, this paper presents a wide range of phenomena that any theory of recognition memory must explain. Over the last few years, researchers have been developing theories of recognition memory based not only on accuracy measures but also on latency measures. In this article, we consider latency measures in recognition memory. Results from four experiments are presented, and an empirical model for latency distributions is developed. Latency distributions are shown to provide much more information than can be obtained from mean latency, the most common dependent variable in reaction time measurements. From this, a strong case is made for the study of distributional properties by showing how some current theories are inadequate or wrong when examined in the light of distributional analyses. These recent theories are further evaluated using functional relationships extracted from results of the four experiments presented.
534|A threshold theory for simple detection experiments |The two-state &#034;high &#034; threshold model is generalized by assuming that (with low probability) the threshold may be exceeded when there is no stimulus. Existing Yes-No data (that rejected the high threshold theory) are compatible with the resulting isosensitivity (ROC) curves, namely, 2 line segments that intersect at the true threshold prob-abilities. The corresponding 2-alternative forced-choice curve is a 45° line through this intersection. A simple learning process is suggested to predict S&#039;s location along these curves, asymptotic means are derived, and comparisons are made with data. These asymptotic biases are coupled with the von Bdk&amp;y-Stevens neural quantum model to show how the theoretical linear psychometric functions are distorted into nonsymmetric, nonlinear response curves. A classic postulate of psychophysics is that some stimuli or differences between stimuli never manage to affect the central decision making centers; others, of course, do. In a phrase, peripheral thresholds were assumed to exist. At least three types have been distinguished: absolute, difference, and detection. It is not, however, clear that there is any real difference among them. Absolute thresholds seem to be the same as detection ones except that the only noise is internal, and many difference threshold experiments differ from de-tection experiments only in the nature of the background stimulus, e.g., a pure tone or noise. Recently the literal interpretation of the threshold postulate has been
535|Ontology Development 101: A Guide to Creating Your First Ontology|In recent years the development of ontologies—explicit formal specifications of the terms in the domain and relations among them (Gruber 1993)—has been moving from the realm of Artificial-Intelligence laboratories to the desktops of domain experts. Ontologies have become common on the World-Wide Web. The ontologies on the Web range from large taxonomies categorizing Web sites (such as on Yahoo!) to categorizations of products for sale and their features (such as on Amazon.com). The WWW Consortium (W3C) is developing the Resource Description Framework (Brickley and Guha 1999), a language for encoding knowledge on Web pages to make it understandable to electronic agents searching for information. The Defense Advanced Research Projects Agency (DARPA), in conjunction with the W3C, is developing DARPA Agent Markup Language (DAML) by extending RDF with more expressive constructs aimed at facilitating agent interaction on the Web (Hendler and McGuinness 2000). Many disciplines now develop standardized ontologies that domain experts can use to share and annotate information in their fields. Medicine, for example, has produced large, standardized, structured vocabularies such as SNOMED (Price and Spackman 2000) and the semantic network of the Unified Medical Language System (Humphreys and Lindberg 1993). Broad general-purpose ontologies are
536|Methodology for the Design and Evaluation of Ontologies|This paper describes the methodology used in the Enterprise Integration Laboratory for the design and evaluation of integrated ontologies, including the proposal of new ontologies and the extension of existing ontologies (see Figure 1). We illustrate these ideas with examples from our activity and organisation ontologies. 2 Motivating Scenarios
537|Living with CLASSIC: When and How to Use a KL-ONE-Like Language|classic is a recently-developed knowledge representation system that follows the  paradigm originally set out in the kl-one system: it concentrates on the definition  of structured concepts, their organization into taxonomies, the creation and manipulation  of individual instances of such concepts, and the key inferences of subsumption  and classification. Rather than simply presenting a description of classic, we complement  a brief system overview with a discussion of how to live within the confines  of a limited object-oriented deductive system. By analyzing the representational  strengths and weaknesses of classic, we consider the circumstances under which it  is most appropriate to use (or not use) it. We elaborate a knowledge-engineering  methodology for building kl-one-style knowledge bases, with emphasis on the modeling  choices that arise in the process of describing a domain. We also address some  of the key difficult issues encountered by new users, including primitive vs. d...
538|Reusable Ontologies, Knowledge-Acquisition Tools, and Performance Systems: PROTÉGÉ-II Solutions to Sisyphus-2|This paper describes how we applied the PROTG-II architecture to build a knowledgebased system that configures elevators. The elevator-configuration task was solved originally with a system that employed the propose-and-revise problem-solving method (VT; Marcus, Stout &amp; McDermott, 1988). A variant of this task, here named the Sisyphus-2 problem, is used by the knowledge-acquisition community for comparative studies. PROTG-II is a knowledge-engineering environment that focuses on the use of reusable ontologies and problem-solving methods to generate task-specific knowledge-acquisition tools and executable problem solvers. The main goal of this paper is to describe in detail how we used PROTG-II to model the elevator-configuration task. This description provides a starting point for comparison with other frameworks that use abstract problem-solving methods. Starting from a detailed description of the elevator-configuration knowledge (Yost, 1992), we analyzed the domain knowledge and developed a general, reusable domain ontology. We selected, from PROTG-II&#039;s library of preexisting methods, a propose-and-revise method based on chronological backtracking. We then configured this method to solve the elevator-configuration task in a knowledge-based system named ELVIS. We entered domain-specific knowledge about elevator configuration into the knowledge base with the help of a task-specific knowledge-acquisition tool that was generated from the ontologies. After we constructed mapping relations to connect the domain and method ontologies, PROTG-II generated the executable problem solver. We have found that the development of ELVIS has provided a valuable test case for evaluating  PROTG-II&#039;s suite of system-building tools.
539|Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems|Abstract—Many problems in signal processing and statistical inference involve finding sparse solutions to under-determined, or ill-conditioned, linear systems of equations. A standard approach consists in minimizing an objective function which includes a quadratic (squared l2) error term combined with a sparseness-inducing (l1) regularization term.Basis pursuit, the least absolute shrinkage and selection operator (LASSO), waveletbased deconvolution, and compressed sensing are a few wellknown examples of this approach. This paper proposes gradient projection (GP) algorithms for the bound-constrained quadratic programming (BCQP) formulation of these problems. We test variants of this approach that select the line search parameters in different ways, including techniques based on the Barzilai-Borwein method. Computational experiments show that these GP approaches perform well in a wide range of applications, often being significantly faster (in terms of computation time) than competing methods. Although the performance of GP methods tends to degrade as the regularization term is de-emphasized, we show how they can be embedded in a continuation scheme to recover their efficient practical performance. A. Background I.
540|Convex Analysis|In this book we aim to present, in a unified framework, a broad spectrum of mathematical theory that has grown in connection with the study of problems of optimization, equilibrium, control, and stability of linear and nonlinear systems. The title Variational Analysis reflects this breadth. For a long time, ‘variational ’ problems have been identified mostly with the ‘calculus of variations’. In that venerable subject, built around the minimization of integral functionals, constraints were relatively simple and much of the focus was on infinite-dimensional function spaces. A major theme was the exploration of variations around a point, within the bounds imposed by the constraints, in order to help characterize solutions and portray them in terms of ‘variational principles’. Notions of perturbation, approximation and even generalized differentiability were extensively investigated. Variational theory progressed also to the study of so-called stationary points, critical points, and other indications of singularity that a point might have relative to its neighbors, especially in association with existence theorems for differential equations.
541|Regression Shrinkage and Selection Via the Lasso|We propose a new method for estimation in linear models. The &#034;lasso&#034; minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly zero and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described. Keywords: regression, subset selection, shrinkage, quadratic programming. 1 Introduction Consider the usual regression situation: we h...
542|Compressed sensing|We study the notion of Compressed Sensing (CS) as put forward in [14] and related work [20, 3, 4]. The basic idea behind CS is that a signal or image, unknown but supposed to be compressible by a known transform, (eg. wavelet or Fourier), can be subjected to fewer measurements than the nominal number of pixels, and yet be accurately reconstructed. The samples are nonadaptive and measure ‘random ’ linear combinations of the transform coefficients. Approximate reconstruction is obtained by solving for the transform coefficients consistent with measured data and having the smallest possible `1 norm. We perform a series of numerical experiments which validate in general terms the basic idea proposed in [14, 3, 5], in the favorable case where the transform coefficients are sparse in the strong sense that the vast majority are zero. We then consider a range of less-favorable cases, in which the object has all coefficients nonzero, but the coefficients obey an `p bound, for some p ? (0, 1]. These experiments show that the basic inequalities behind the CS method seem to involve reasonable constants. We next consider synthetic examples modelling problems in spectroscopy and image pro-
543|ATOMIC DECOMPOSITION BY BASIS PURSUIT|The Time-Frequency and Time-Scale communities have recently developed a large number of overcomplete waveform dictionaries -- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the Method of Frames (MOF), Matching Pursuit (MP), and, for special dictionaries, the Best Orthogonal Basis (BOB). Basis Pursuit (BP) is a principle for decomposing a signal into an &#034;optimal&#034; superposition of dictionary elements, where optimal means having the smallest l 1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP and BOB, including better sparsity, and super-resolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation de-noising, and multi-scale edge denoising. Basis Pursuit in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.
544|Robust Uncertainty Principles: Exact Signal Reconstruction From Highly Incomplete Frequency Information|This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal and a randomly chosen set of frequencies. Is it possible to reconstruct from the partial knowledge of its Fourier coefficients on the set? A typical result of this paper is as follows. Suppose that is a superposition of spikes @ Aa @ A @ A obeying @?? ? A I for some constant H. We do not know the locations of the spikes nor their amplitudes. Then with probability at least I @ A, can be reconstructed exactly as the solution to the I minimization problem I aH @ A s.t. ” @ Aa ”  @ A for all
545|Nonlinear total variation based noise removal algorithms|A constrained optimization type of numerical algorithm for removing noise from images is presented. The total variation of the image is minimized subject to constraints involving the statistics of the noise. The constraints are imposed using Lagrange multipliers. The solution is obtained using the gradient-projection method. This amounts to solving a time dependent partial differential equation on a manifold determined by the constraints. As t--- ~ 0o the solution converges to a steady state which is the denoised image. The numerical algorithm is simple and relatively fast. The results appear to be state-of-the-art for very noisy images. The method is noninvasive, yielding sharp edges in the image. The technique could be interpreted as a first step of moving each level set of the image normal to itself with velocity equal to the curvature of the level set divided by the magnitude of the gradient of the image, and a second step which projects the image back onto the constraint set. 
546|Near Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?|Suppose we are given a vector f in RN. How many linear measurements do we need to make about f to be able to recover f to within precision ? in the Euclidean (l2) metric? Or more exactly, suppose we are interested in a class F of such objects— discrete digital signals, images, etc; how many linear measurements do we need to recover objects from this class to within accuracy ?? This paper shows that if the objects of interest are sparse or compressible in the sense that the reordered entries of a signal f ? F decay like a power-law (or if the coefficient sequence of f in a fixed basis decays like a power-law), then it is possible to reconstruct f to within very high accuracy from a small number of random measurements. typical result is as follows: we rearrange the entries of f (or its coefficients in a fixed basis) in decreasing order of magnitude |f | (1)  = |f | (2)  =... = |f | (N), and define the weak-lp ball as the class F of those elements whose entries obey the power decay law |f | (n)  = C · n -1/p. We take measurements <f, Xk>, k = 1,..., K, where the Xk are N-dimensional Gaussian
547|Least angle regression |The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising
548|De-Noising By Soft-Thresholding|Donoho and Johnstone (1992a) proposed a method for reconstructing an unknown function f on [0; 1] from noisy data di = f(ti)+ zi, iid i =0;:::;n 1, ti = i=n, zi N(0; 1). The reconstruction fn ^ is de ned in the wavelet domain by translating all the empirical wavelet coe cients of d towards 0 by an amount p 2 log(n)  = p n. We prove two results about that estimator. [Smooth]: With high probability ^ fn is at least as smooth as f, in any of a wide variety of smoothness measures. [Adapt]: The estimator comes nearly as close in mean square to f as any measurable estimator can come, uniformly over balls in each of two broad scales of smoothness classes. These two properties are unprecedented in several ways. Our proof of these results develops new facts about abstract statistical inference and its connection with an optimal recovery model.
549|Greed is Good: Algorithmic Results for Sparse Approximation|This article presents new results on using a greedy algorithm, orthogonal matching pursuit (OMP), to solve the sparse approximation problem over redundant dictionaries. It provides a sufficient condition under which both OMP and Donoho’s basis pursuit (BP) paradigm can recover the optimal representation of an exactly sparse signal. It leverages this theory to show that both OMP and BP succeed for every sparse input signal from a wide class of dictionaries. These quasi-incoherent dictionaries offer a natural generalization of incoherent dictionaries, and the cumulative coherence function is introduced to quantify the level of incoherence. This analysis unifies all the recent results on BP and extends them to OMP. Furthermore, the paper develops a sufficient condition under which OMP can identify atoms from an optimal approximation of a nonsparse signal. From there, it argues that OMP is an approximation algorithm for the sparse problem over a quasi-incoherent dictionary. That is, for every input signal, OMP calculates a sparse approximant whose error is only a small factor worse than the minimal error that can be attained with the same number of terms. 
550|The Dantzig Selector: Statistical Estimation When p Is Much Larger Than n|In many important statistical applications, the number of variables or parameters p is much larger than the number of observations n. Suppose then that we have observations y = Xß + z, where ß ? Rp is a parameter vector of interest, X is a data matrix with possibly far fewer rows than columns, n « p, and the zi’s are i.i.d. N(0,s2). Is it possible to estimate ß reliably based on the noisy data y? To estimate ß, we introduce a new estimator—we call it the Dantzig selector—which is a solution to the l1-regularization problem min ˜ß?R p ? ˜ß?l1 subject to ?X * r?l 8  = (1 + t-1 v) 2logp · s, where r is the residual vector y - X ˜ß and t is a positive scalar. We show that if X obeys a uniform uncertainty principle (with unit-normed columns) and if the true parameter vector ß is sufficiently sparse (which here roughly guarantees that the model is identifiable), then with very large probability,
551|The Extended Linear Complementarity Problem|We consider an extension of the horizontal linear complementarity problem, which we call the extended linear complementarity problem (XLCP). With the aid of a natural bilinear program, we establish various properties of this extended complementarity problem; these include the convexity of the bilinear objective function under a monotonicity assumption, the polyhedrality of the solution set of a monotone XLCP, and an error bound result for a nondegenerate XLCP. We also present a finite, sequential linear programming algorithm for solving the nonmonotone XLCP.  
553|LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares|An iterative method is given for solving Ax ~ffi b and minU Ax- b 112, where the matrix A is large and sparse. The method is based on the bidiagonalization procedure of Golub and Kahan. It is analytically equivalent to the standard method of conjugate gradients, but possesses more favorable numerical properties. Reliable stopping criteria are derived, along with estimates of standard errors for x and the condition number of A. These are used in the FORTRAN implementation of the method, subroutine LSQR. Numerical tests are described comparing I~QR with several other conjugate-gradient algorithms, indicating that I~QR is the most reliable algorithm when A is ill-conditioned. Categories and Subject Descriptors: G.1.2 [Numerical Analysis]: ApprorJmation--least squares approximation; G.1.3 [Numerical Analysis]: Numerical Linear Algebra--linear systems (direct and
554|An Algorithm for Total Variation Minimization and Applications| We propose an algorithm for minimizing the total variation of an image, and provide a proof of convergence. We show applications to image denoising, zooming, and the computation of the mean curvature motion of interfaces. 
555|Interior-point Methods|The modern era of interior-point methods dates to 1984, when Karmarkar proposed his algorithm for linear programming. In the years since then, algorithms and software for linear programming have become quite sophisticated, while extensions to more general classes of problems, such as convex quadratic programming, semidefinite programming, and nonconvex and nonlinear problems, have reached varying levels of maturity. We review some of the key developments in the area, including comments on both the complexity theory and practical algorithms for linear programming, semidefinite programming, monotone linear complementarity, and convex programming over sets that can be characterized by self-concordant barrier functions. 
556|Just Relax: Convex Programming Methods for Identifying Sparse Signals in Noise|This paper studies a difficult and fundamental problem that arises throughout electrical engineering, applied mathematics, and statistics. Suppose that one forms a short linear combination of elementary signals drawn from a large, fixed collection. Given an observation of the linear combination that has been contaminated with additive noise, the goal is to identify which elementary signals participated and to approximate their coefficients. Although many algorithms have been proposed, there is little theory which guarantees that these algorithms can accurately and efficiently solve the problem. This paper studies a method called convex relaxation, which attempts to recover the ideal sparse signal by solving a convex program. This approach is powerful because the optimization can be completed in polynomial time with standard scientific software. The paper provides general conditions which ensure that convex relaxation succeeds. As evidence of the broad impact of these results, the paper describes how convex relaxation can be used for several concrete signal recovery problems. It also describes applications to channel coding, linear regression, and numerical analysis.
557|Stable recovery of sparse overcomplete representations in the presence of noise| Overcomplete representations are attracting interest in signal processing theory, particularly due to their potential to generate sparse representations of signals. However, in general, the problem of finding sparse representations must be unstable in the presence of noise. This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system. Considering an ideal underlying signal that has a sufficiently sparse representation, it is assumed that only a noisy version of it can be observed. Assuming further that the overcomplete system is incoherent, it is shown that the optimally sparse approximation to the noisy data differs from the optimally sparse decomposition of the ideal noiseless signal by at most a constant multiple of the noise level. As this optimal-sparsity method requires heavy (combinatorial) computational effort, approximation algorithms are considered. It is shown that similar stability is also available using the basis and the matching pursuit algorithms. Furthermore, it is shown that these methods result in sparse approximation of the noisy data that contains only terms also appearing in the unique sparsest representation of the ideal noiseless sparse signal. 
558|An EM Algorithm for Wavelet-Based Image Restoration|This paper introduces an expectation-maximization (EM) algorithm for image restoration (deconvolution) based on a penalized likelihood formulated in the wavelet domain. Regularization is achieved by promoting a reconstruction with low-complexity, expressed in terms of the wavelet coecients, taking advantage of the well known sparsity of wavelet representations. Previous works have investigated wavelet-based restoration but, except for certain special cases, the resulting criteria are solved approximately or require very demanding optimization methods. The EM algorithm herein proposed combines the efficient image representation oered by the discrete wavelet transform (DWT) with the diagonalization of the convolution operator obtained in the Fourier domain. The algorithm alternates between an E-step based on the fast Fourier transform (FFT) and a DWT-based M-step, resulting in an ecient iterative process requiring O(N log N) operations per iteration. Thus, it is the  rst image restoration algorithm that optimizes a wavelet-based penalized likelihood criterion and has computational complexity comparable to that of standard wavelet denoising or frequency domain deconvolution methods. The convergence behavior of the algorithm is investigated, and it is shown that under mild conditions the algorithm converges to a globally optimal restoration. Moreover, our new approach outperforms several of the best existing methods in benchmark tests, and in some cases is also much less computationally demanding.
559|Signal reconstruction from noisy random projections|Recent results show that a relatively small number of random projections of a signal can contain most of its salient information. It follows that if a signal is compressible in some orthonormal basis, then a very accurate reconstruction can be obtained from random projections. We extend this type of result to show that compressible signals can be accurately recovered from random projections contaminated with noise. We also propose a practical iterative algorithm for signal reconstruction, and briefly discuss potential applications to coding, A/D conversion, and remote wireless sensing. Index Terms sampling, signal reconstruction, random projections, denoising, wireless sensor networks
560|Nonmonotone spectral projected gradient methods on convex sets|Abstract. Nonmonotone projected gradient techniques are considered for the minimization of differentiable functions on closed convex sets. The classical projected gradient schemes are extended to include a nonmonotone steplength strategy that is based on the Grippo–Lampariello–Lucidi nonmonotone line search. In particular, the nonmonotone strategy is combined with the spectral gradient choice of steplength to accelerate the convergence process. In addition to the classical projected gradient nonlinear path, the feasible spectral projected gradient is used as a search direction to avoid additional trial projections during the one-dimensional search process. Convergence properties and extensive numerical results are presented.
561|Why simple shrinkage is still relevant for redundant representations|Abstract—Shrinkage is a well known and appealing denoising technique, introduced originally by Donoho and Johnstone in 1994. The use of shrinkage for denoising is known to be optimal for Gaussian white noise, provided that the sparsity on the signal’s representation is enforced using a unitary transform. Still, shrinkage is also practiced with nonunitary, and even redundant representations, typically leading to very satisfactory results. In this correspondence we shed some light on this behavior. The main argument in this work is that such simple shrinkage could be interpreted as the first iteration of an algorithm that solves the basis pursuit denoising (BPDN) problem. While the desired solution of BPDN is hard to obtain in general, we develop a simple iterative procedure for the BPDN minimization that amounts to stepwise shrinkage. We demonstrate how the simple shrinkage emerges as the first iteration of this novel algorithm. Furthermore, we show how shrinkage can be iterated, turning into an effective algorithm that minimizes the BPDN via simple shrinkage steps, in order to further strengthen the denoising effect. Index Terms—Basis pursuit, denoising, frame, overcomplete, redundant, sparse representation, shrinkage, thresholding.
562|Spectral bounds for sparse PCA: Exact and greedy algorithms|Sparse PCA seeks approximate sparse “eigenvectors ” whose projections capture the maximal variance of data. As a cardinality-constrained and non-convex optimization problem, it is NP-hard and yet it is encountered in a wide range of applied fields, from bio-informatics to finance. Recent progress has focused mainly on continuous approximation and convex relaxation of the hard cardinality constraint. In contrast, we consider an alternative discrete spectral formulation based on variational eigenvalue bounds and provide an effective greedy strategy as well as provably optimal solutions using branch-and-bound search. Moreover, the exact methodology used reveals a simple renormalization step that improves approximate solutions obtained by any continuous method. The resulting performance gain of discrete algorithms is demonstrated on real-world benchmark data and in extensive Monte Carlo evaluation trials. 1
563|WARM-START STRATEGIES IN INTERIOR-POINT METHODS FOR LINEAR PROGRAMMING | We study the situation in which, having solved a linear program with an interior-point method, we are presented with a new problem instance whose data is slightly perturbed from the original. We describe strategies for recovering a &#034;warm-start&#034; point for the perturbed problem instance from the iterates of the original problem instance. We obtain worst-case estimates of the number of iterations required to converge to a solution of the perturbed instance from the warm-start points, showing that these estimates depend on the size of the perturbation and on the conditioning and other properties of the problem instances.
564|Image denoising with shrinkage and redundant representations|Shrinkage is a well known and appealing denoising technique. The use of shrinkage is known to be optimal for Gaussian white noise, provided that the sparsity on the signal’s representation is enforced using a unitary transform. Still, shrinkage is also practiced successfully with nonunitary, and even redundant representations. In this paper we shed some light on this behavior. We show that simple shrinkage could be interpreted as the first iteration of an algorithm that solves the basis pursuit denoising (BPDN) problem. Thus, this work leads to a novel iterative shrinkage algorithm that can be considered as an effective pursuit method. We demonstrate this algorithm, both on synthetic data, and for the image denoising problem, where we learn the image prior parameters directly from the given image. The results in both cases are superior to several popular alternatives. 1
566|More on sparse representations in arbitrary bases|Abstract: The purpose of this contribution is to generalize some recent results on sparse representations of signals in redundant bases. The question that is considered is the following: let A be a known (n, m) matrix with m&gt; n, one observes b = AX where X is known to have p &lt; n nonzero components, under which conditions on A and p is it possible to recover X by solving a convex optimization problem such as a linear or quadratic program? The solution is known when A is the concatenation of two unitary matrices, we extend it to arbitrary matrices.
568|On the convergence properties of the projected gradient method for convex optimization |Abstract. When applied to an unconstrained minimization problem with a convex objective, the steepest descent method has stronger convergence properties than in the noncovex case: the whole sequence converges to an optimal solution under the only hypothesis of existence of minimizers (i.e. without assuming e.g. boundedness of the level sets). In this paper we look at the projected gradient method for constrained convex minimization. Convergence of the whole sequence to a minimizer assuming only existence of solutions has also been already established for the variant in which the stepsizes are exogenously given and square summable. In this paper, we prove the result for the more standard (and also more efficient) variant, namely the one in which the stepsizes are determined through an Armijo search. Mathematical subject classification: 90C25, 90C30. Key words: projected gradient method, convex optimization, quasi-Fejér convergence.
569|Learning Stochastic Logic Programs|Stochastic Logic Programs (SLPs) have been shown to  be a generalisation of Hidden Markov Models (HMMs),  stochastic context-free grammars, and directed Bayes&#039;  nets. A stochastic logic program consists of a set of  labelled clauses p:C where p is in the interval [0,1] and  C is a first-order range-restricted definite clause. This  paper summarises the syntax, distributional semantics  and proof techniques for SLPs and then discusses how a  standard Inductive Logic Programming (ILP) system,  Progol, has been modied to support learning of SLPs.  The resulting system 1) nds an SLP with uniform  probability labels on each definition and near-maximal  Bayes posterior probability and then 2) alters the probability  labels to further increase the posterior probability.  Stage 1) is implemented within CProgol4.5, which  differs from previous versions of Progol by allowing  user-defined evaluation functions written in Prolog. It  is shown that maximising the Bayesian posterior function  involves nding SLPs with short derivations of the  examples. Search pruning with the Bayesian evaluation  function is carried out in the same way as in previous  versions of CProgol. The system is demonstrated with  worked examples involving the learning of probability  distributions over sequences as well as the learning of  simple forms of uncertain knowledge.  
570|Learning logical definitions from relations| This paper describes FOIL, a system that learns Horn clauses from data expressed as relations. FOIL is based on ideas that have proved effective in attribute-value learning systems, but extends them to a first-order formalism. This new system has been applied successfully to several tasks taken from the machine learning literature.
571|Inverse entailment and Progol|This paper firstly provides a re-appraisal of the development of  techniques for inverting deduction, secondly introduces Mode-Directed Inverse  Entailment (MDIE) as a generalisation and enhancement of previous approaches  and thirdly describes an implementation of MDIE in the Progol system. Progol is  implemented in C and available by anonymous ftp. The re-assessment of previous  techniques in terms of inverse entailment leads to new results for learning from  positive data and inverting implication between pairs of clauses. 
572|Qualitative Simulation|Qualitative simulation predicts the set of possible behaviors...
573|Clausal Discovery|  The clausal discovery engine Claudien is presented. Claudien is an inductive logic programming engine that fits in the descriptive data mining paradigm. Claudien addresses characteristic induction from interpretations, a task which is related to existing formalisations of induction in logic. In characteristic induction from interpretations, the regularities are represented by clausal theories, and the data using Herbrand interpretations. Because Claudien uses clausal logic to represent hypotheses, the regularities induced typically involve multiple relations or predicates. Claudien also employs a novel declarative bias mechanism to define the set of clauses that may appear in a hypothesis.
574|Generating Production Rules From Decision Trees|Many inductive knowledge acquisition algorithms generate classifiers in the form of decision trees. This paper describes a technique for transforming such trees to small sets of production rules, a common formalism for expressing knowledge in expert systems. The method makes use of the training set of cases from which the decision tree was generated, first to generalize and assess the reliability of individual rules extracted from the tree, and subsequently to refine the collection of rules as a whole. The final set of production rules is usually both simpler than the decision tree from which it was obtained, and more accurate when classifying unseen cases. Transformation to production rules also provides a way of combining different decision trees for the same classification domain.
575|Relational bayesian networks|A new method is developed to represent probabilistic relations on multiple random events. Where previously knowledge bases containing probabilistic rules were used for this purpose, here a probabilitydistributionover the relations is directly represented by a Bayesian network. By using a powerful way of specifying conditional probability distributions in these networks, the resulting formalism is more expressive than the previous ones. Particularly, it provides for constraints on equalities of events, and it allows to define complex, nested combination functions. 1
576|Applications of Machine Learning and Rule Induction|An important area of application for machine learning is in automating the acquisition of knowledge bases required for expert systems. In this paper, we review the major paradigms for machine learning, including neural networks, instance-based methods, genetic learning, rule induction, and analytic approaches. We consider rule induction in greater detail and review some of its recent applications, in each case stating the problem, how rule induction was used, and the status of the resulting expert system. In closing, we identify the main stages in fielding an applied learning system and draw some lessons from successful applications. Introduction  Machine learning is the study of computational methods for improving performance by mechanizing the acquisition of knowledge from experience. Expert performance requires much domainspecific knowledge, and knowledge engineering has produced hundreds of AI expert systems that are now used regularly in industry. Machine learning aims to provide ...
577|Mutagenesis: ILP experiments in a non-determinate biological domain|This paper describes the use of Inductive Logic Programming as a scientific  assistant. In particular, it details the application of the ILP system  Progol to discovering structural features that can result in mutagenicity in  small molecules. To discover these concepts, Progol only had access to the  atomic and bond structure of the molecules. With such a primitive description  and no further assistance from chemists, Progol corroborated some  existing knowledge and proposed a new structural alert for mutagenicity in  compounds. In the process, the experiments act as a case study in which,  even with extremely limited background knowledge, an Inductive Logic  Programming tool firstly, complements a complex statistical model developed  by skilled chemists, and secondly, continues to provide understandable  theories when the statistical model fails. The experiments also constitute  the first demonstrations of a prototype of the Progol system. Progol  allows the construction of hypotheses with bounded non-determinacy by  performing a best-first search within the subsumption lattice. The results  here provide evidence that such searches are both viable and desirable.  1 
578|Learning concepts by asking questions|Tw o important issues in machine learning are explored: the role that memory plays in acquiring new concepts; and the extent to which the learner can take anactive part in acquiring these concepts. This chapter describes a program, called Marvin, which uses concepts it has learned previously to learn new concepts. The program forms hypotheses about the concept being learned and tests the hypotheses by asking the trainer questions. Learning begins when the trainer shows Marvin an example of the concept to be learned. The program determines which objects in the example belong to concepts stored in the memory. A description of the new concept is formed by using the information obtained from the memory to generalize the description of the training example. The generalized description is tested when the program constructs new examples and shows these to the trainer, asking if they belong to the target concept. 1.
579|Logical Depth and Physical Complexity|Some mathematical and natural objects (a random sequence, a sequence  of zeros, a perfect crystal, a gas) are intuitively trivial, while  others (e.g. the human body, the digits of #) contain internal evidence  of a nontrivial causal history. We formalize this
580|Biochemical knowledge discovery using Inductive Logic Programming|Machine Learning algorithms are being increasingly used for knowledge discovery tasks.  Approaches can be broadly divided by distinguishing discovery of procedural from that of  declarative knowledge. Client requirements determine which of these is appropriate. This  paper discusses an experimental application of machine learning in an area related to drug  design. The bottleneck here is in finding appropriate constraints to reduce the large number of  candidate molecules to be synthesisedand tested. Such constraints canbe viewed as declarative  specifications of the structural elements necessary for high medicinal activity and low toxicity.  The first-order representation used within Inductive Logic Programming (ILP) provides an  appropriate description language for such constraints. Within this application area knowledge  accreditation requires not only a demonstration of predictive accuracy but also, and crucially, a  certification of novel insight into the structural chemistry. Thi...
581|Uncertainty, Belief, and Probability|: We introduce a new probabilistic approach to dealing with uncertainty, based on the observation that probability theory does not require that every event be assigned a probability. For a nonmeasurable event (one to which we do not assign a probability), we can talk about only the inner measure and outer measure of the event. In addition to removing the requirement that every event be assigned a probability, our approach circumvents other criticisms of probability-based approaches to uncertainty. For example, the measure of belief in an event turns out to be represented by an interval (defined by the inner and outer measure), rather than by a single number. Further, this approach allows us to assign a belief (inner measure) to an event E without committing to a belief about its negation :E (since the inner measure of an event plus the inner measure of its negation is not necessarily one). Interestingly enough, inner measures induced by probability measures turn out to correspond in a ...
582|Learning Qualitative Models of Dynamic Systems|A technique is described for learning qualitative models of dynamic systems. The QSIM formalism is used as a representation for learned qualitative models. The problem of learning QSIM-type models is formulated in logic, and the GOLEM learning program is used for induction. An experiment in learning a qualitative model of the connected containers system, also called U-tube, is described in detail. 1 Introduction  It has been shown that qualitative models are better suited for several tasks than the traditional quantitative, or numerical models. These tasks include diagnosis (e.g. Bratko, Mozetic and Lavrac 1989), generating explanation of the system&#039;s behaviour (e.g. Forbus and Falkenheiner 1990) and designing novel devices from first principles (e.g. Williams 1990). We believe that system identification, a fundamental problem in the theory of dynamic systems, is also a task that is done easier at the qualitative level. This paper presents a case study in how this can be done using a l...
583|Loglinear models for first-order probabilistic reasoning|Recent work on loglinear models in probabilistic constraint logic programming is applied to firstorder probabilistic reasoning. Probabilities are defined directly on the proofs of atomic formulae, and by marginalisation on the atomic formulae themselves. We use Stochastic Logic Programs (SLPs) composed of labelled and unlabelled definite clauses to define the proof probabilities. We have a conservative extension of first-order reasoning, so that, for example, there is a one-one mapping between logical and random variables. We show how, in this framework, Inductive Logic Programming (ILP) can be used to induce the features of a loglinear model from data. We also compare the presented framework with other approaches to first-order probabilistic reasoning.
584|A Strategy for Constructing New Predicates in First Order Logic|There is increasing interest within the Machine Learning community in systems which automatically reformulate their problem representation by defining and constructing new predicates. A previous paper discussed such a system, called CIGOL, and gave a derivation for the mechanism of inverting individual steps in first order resolution proofs. In this paper we describe an enhancement to CIGOL&#039;s learning strategy which strongly constrains the formation of new concepts and hypotheses. The new strategy is based on results from algorithmic information theory. Using these results it is possible to compute the probability that the simplifications produced by adopting new concepts or hypotheses are not based on chance regularities within the examples. This can be derived from the amount of information compression produced by replacing the examples with the hypothesised concepts. CIGOL&#039;s improved performance, based on an approximation of this strategy, is demonstrated by way of the automatic &#034;di...
585|The Role of Databases in Knowledge-Based Systems|This paper explores the requirements for database techniques in the construction of  knowledge-based systems. Three knowledge-based systems are reviewed: XCN/R1,  ISIS and Callisto in order to ascertain database requirements. These requirements result  in the introduction of the Organization level, an extension to the symbol and knowledge  levels introduced by Newell. An implementation of these requirements is explored in the  SRL knowledge representation and problem-solving system.
586|The Cyclical Behavior of Equilibrium Unemployment and Vacancies|This paper argues that a broad class of search models cannot generate the observed business-cycle-frequency fluctuations in unemployment and job vacancies in response to shocks of a plausible magnitude. In the U.S., the vacancy-unemployment ratio is 20 times as volatile as average labor productivity, while under weak assumptions, search models predict that the vacancy-unemployment ratio and labor productivity have nearly the same variance. I establish this claim both using analytical comparative statics in a very general deterministic search model and using simulations of a stochastic version of the model. I show that a shock that changes average labor productivity primarily alters the present value of wages, generating only a small movement along a downward sloping Beveridge curve (unemployment-vacancy locus). A shock to the job destruction rate generates a counterfactually positive correlation between unemployment and vacancies. In both cases, the shock is only slightly amplified and the model exhibits virtually no propagation. I reconcile these findings with an existing literature and argue that the source of the model’s failure is lack of wage rigidity, a consequence of the assumption that wages are determined by Nash bargaining. * This is a major revision of ‘Equilibrium Unemployment Fluctuations’. I thank Daron Acemoglu, Olivier
587|Job Destruction and Propagation of Shocks |This paper considers propagation of aggregate shocks in a dynamic general-equilibrium model with labor-market matching and endogenous job destruction. Cyclical fluctuations in the job-destruction rate magnify the output effects of shocks, as well as making them much more persistent. Interactions between capital adjust-ment and the job-destruction rate play an important role in generating persistence. Propagation effects are shown to be quantitatively substantial when the model is calibrated using job-flow data. Incorporating costly capital adjustment leads to significantly greater propagation. (JEL E24, E32) It has been well documented that the cyclical adjustment of labor input chiefly represents move-ment of workers into and out of employment, rather than adjustment of hours at given jobs. Thus, in understanding business cycles, it is cen-trally important to understand the formation and breakdown of employment relationships. The na-
588|Measuring the Cyclicality of Real Wages: How Important Is Composition Bias?&#034; NBER Working Paper No |In the period since the 1960s, as in other periods, aggregate time series on real wages have displayed only modest cyclicality. Macroeconomists therefore have described weak cyclicality of real wages as a salient feature of the business cycle. Contrary to this conventional wisdom, our analysis of longitudinal microdata indicates that real wages have been substantially procyclical since the 1960s. We show that the true procyclicality of real wages is obscured in aggregate time series because of a composition bias: the aggregate statistics are constructed in a way that gives more weight to low-skill workers during expansions than during recessions. I.
589|2001): “Pricing and matching with frictions |Suppose that n buyers each want one unit and m sellers each have one or more units of a good. Sellers post prices, and then buyers choose sellers. In symmetric equilibrium, similar sellers all post one price, and buyers randomize. Hence, more or fewer buyers may arrive than a seller can accommodate. We call this frictions. We solve for prices and the endogenous matching function for finite n and m and consider the limit as n and m grow. The matching function displays decreasing returns but converges to constant returns. We argue that the standard matching function in the literature is misspecified and discuss implications for the Beveridge curve. I.
590|Equilibrium Unemployment|A search-theoretic model of equilibrium unemployment is constructed and shown  to be consistent with the key regularities of the labor market and business cycle.
591|Changes in Unemployment Duration and Labor Force Attachment|This paper accounts for the observed increase in unemployment duration  relative to the unemployment rate in the U.S. over the past thirty years, typified  by the record low level of short-term unemployment. We show that part of the  increase is due to changes in how duration is measured, a consequence of the  1994 Current Population Survey redesign. Another part is due to the passage  of the baby boomers into their prime working years. After accounting for these  shifts, most of the remaining increase in unemployment duration relative to the  unemployment rate is concentrated among women, whose unemployment rate  has fallen sharply in the last two decades while their unemployment duration  has increased. Using labor market transition data, we show that this is a  consequence of the increase in women&#039;s labor force attachment.  # We are grateful to Giuseppe Bertola, Robert Solow, David Weiman, and participants in the Sustainable Employment Initiative conference for their comments and to Fran Horvath, Randy Ilg, Rowena Johnson, Bob McIntire, and Anne Polivka at the Bureau of Labor Statistics (BLS) for their help compiling the data. Ron Tucker of the Census Bureau and Clyde Tucker of the BLS provided useful information concerning the Current Population Survey redesign. Joydeep Roy provided valuable research assistance. Shimer acknowledges financial support from National Science Foundation grant SES-0079345 and the hospitality of the University of Chicago while part of this paper was written. Please address correspondence to shimer@princeton.edu.  1 
592|Contractual Fragility, Job Destruction and Business Cycles|. We develop a theory of labor contracting in which negative  productivity shocks lead to costly job loss, despite unlimited possibilities  for renegotiating wage contracts. Such fragile contracts emerge from ...rms&#039;  tradeos between robustness of incentives in ongoing employment relationships  and costly speci...c investment. In a matching market equilibrium, contractual  fragility serves as a powerful mechanism for propagating underlying productivity  shocks: in our benchmark calibration, i.i.d. shocks are magni...ed seven times  in their eect on aggregate output, and the eect is highly persistent. We also  explore novel motivations for government policies that strengthen employment  relationships.  1. Introduction  Popular discussion of recessions emphasizes the high costs borne by workers as a consequence of increased job loss. This view of recessions has in turn been amply documented by empirical research. Blanchard and Diamond (1990), for example, ...nd that gross #ows of workers...
593|The Roaring Nineties: Can Full Employment be Sustained|ences, a measure of rents, declined in a two-step sequence with a pattern and timing similar to movements in the Beveridge curve-a measure of matching efficiency. These comovements also match in some important ways the spotty data on the adoption of innovative work practices. This last point-parallel timing-is a key criterion of explanatory success. Any full account of the decline in the U.S. NAIRU should explain the timing and cause of the discrete inward shifts in the Beveridge curve that took place in
594|The Beveridge Curve, Job Creation and the Propagation of Shocks|This paper proposes modifications to the popular model of equilibrium unemployment by Mortensen and Pissarides [30]. I augment the model by introducing (1) costly planning for brand-new jobs, and (2) the option to mothball preexisting jobs; to develop new jobs requires a time-consuming planning process, whereas firms with preexisting jobs are allowed to mothball (temporarily freeze) their jobs, and to reactivate them with no planning lags. These modifications greatly improve the model’s ability to replicate the Beveridge curve as well as observed correlations between vacancies and job creation. It is also shown that persistent behavior of vacancies in the model serves to enhance the model’s propagation mechanism.  
595|Modern theory of unemployment fluctuations: Empirics and policy applications |Strong and widely accepted evidence shows that the natural rate of unemployment varies over time with substantial amplitude. The frictions in the labor market that account for positive normal levels of unemployment are not simple and mechanical. Instead, as a rich modern body of theory demonstrates, the natural rate of unemployment is an equilibrium in which the volumes of job-seeking by workers and worker-seeking by employers reach a balance controlled by fundamental determinants of the relative prices of the two activities. In recessions, unemployment rises, and job vacancies fall. The natural explanation is an economywide fall in labor demand. But a compelling model that generates a fall in labor demand without a counterfactual fall in productivity has eluded theorists to date. Nonetheless, policymakers have appropriately adopted the view that the natural rate varies over time and is not a simple benchmark for setting monetary instruments.
596|Implementing data cubes efficiently|Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensional data cubes. Each cell of the data cube is a view consisting of an aggregation of interest, like total sales. The values of many of these cells are dependent on the values of other cells in the data cube..A common and powerful query optimization technique is to materialize some or all of these cells rather than compute them from raw data each time. Commercial systems differ mainly in their approach to materializing the data cube. In this paper, we investigate the issue of which cells (views) to materialize when it is too expensive to materialize all views. A lattice framework is used to express dependencies among views. We present greedy algorithms that work off this lattice and determine a good set of views to materialize. The greedy algorithm performs within a small constant factor of optimal under a variety of models. We then consider the most common case of the hypercube lattice and examine the choice of materialized views for hypercubes in detail, giving some good tradeoffs between the space used and the average time to answer a query. 1
597|Data cube: A relational aggregation operator generalizing group-by, cross-tab, and sub-totals|Abstract. Data analysis applications typically aggregate data across many dimensions looking for anomalies or unusual patterns. The SQL aggregate functions and the GROUP BY operator produce zero-dimensional or one-dimensional aggregates. Applications need the N-dimensional generalization of these operators. This paper defines that operator, called the data cube or simply cube. The cube operator generalizes the histogram, crosstabulation, roll-up, drill-down, and sub-total constructs found in most report writers. The novelty is that cubes are relations. Consequently, the cube operator can be imbedded in more complex non-procedural data analysis programs. The cube operator treats each of the N aggregation attributes as a dimension of N-space. The aggregate of a particular set of attribute values is a point in this space. The set of points forms an N-dimensional cube. Super-aggregates are computed by aggregating the N-cube to lower dimensional spaces. This paper (1) explains the cube and roll-up operators, (2) shows how they fit in SQL, (3) explains how users can define new aggregate functions for cubes, and (4) discusses efficient techniques to compute the cube. Many of these features are being added to the SQL Standard.
598|Query evaluation techniques for large databases|Database management systems will continue to manage large data volumes. Thus, efficient algorithms for accessing and manipulating large sets and sequences will be required to provide acceptable performance. The advent of object-oriented and extensible database systems will not solve this problem. On the contrary, modern data models exacerbate it: In order to manipulate large sets of complex objects as efficiently as today’s database systems manipulate simple records, query processing algorithms and software will become more complex, and a solid understanding of algorithm and architectural issues is essential for the designer of database management software. This survey provides a foundation for the design and implementation of query execution facilities in new database management systems. It describes a wide array of practical query evaluation techniques for both relational and post-relational database systems, including iterative execution of complex query evaluation plans, the duality of sort- and hash-based set matching algorithms, types of parallel query execution and their implementation, and special operators for emerging database application domains.
599|Multi-table joins through bitmapped join indices|This technical note shows how to combine some well-known techniques to create a method that will efficiently execute common multi-table joins. We concentrate on a commonly occurring type of join known as a star-join, although the method presented will generalize to any type of multi-table join. A star-join consists of a central detail table with large cardinality, such as an orders table (where an order row contains a single purchase) with foreign keys that join to descriptive tables, such as customers, products, and (sales) agents. The method presented in this note uses join indices with compressed bitmap representations, which allow predicates restricting columns of descriptive tables to determine an answer set (or foundsef) in the central detail table; the method uses different predicates on different descriptive tables in combination to restrict the detail table through compressed bitmap representations of join indices, and easily completes the join of the fully restricted detail table rows back to the descriptive tables. We outline realistic examples where the combination of these techniques yields substantial performance improvements over alternative, more traditional query evaluation plans. 1.
600|Aggregate-Query Processing in Data Warehousing Environments|In this paper we introduce generalized projections (GPs), an extension of duplicate eliminating projections, that capture aggregations, groupbys, duplicate-eliminating projections (distinct), and duplicate-preserving projections in a common unified framework. Using GPs we extend well known and simple algorithms for SQL queries that use distinct projections to derive algorithms for queries using aggregations like sum, max, min, count, and avg. We develop powerful query rewrite rules for aggregate queries that unify and extend rewrite rules previously known in the literature. We then illustrate the power of our approach by solving a very practical and important problem in data warehousing: how to answer an aggregate query on base tables using materialized aggregate views (summary tables).
601|Including Group-By in Query Optimization|In existing relational database systems, processing of group-by and computation of aggregate functions are always postponed until all joins are performed. In this paper, we present transformations that make it possible to push group-by operation past one or more joins and can potentially reduce the cost of processing a query significantly. Therefore, the placement of group-by should be decided based on cost estimation. We explain how the traditional System-R style optimizers can be modified by incorporating the greedy conservative heuristic that we developed. We prove that applications of greedy conservative heuristic produce plans that are better (or no worse) than the plans generated by a traditional optimizer. Our experimental study shows that the extent of improvement in the quality of plans is significant with only a modest increase in optimization cost. Our technique also applies to optimization of Select Distinct queries by pushing down duplicate elimination in a cost-based fas...
602|Eager Aggregation and Lazy Aggregation|Efficient processing of aggregation queries is essential for decision support applications. This paper describes a class of query transformations, called eager aggregation and lazy aggregation, that allows a query optimizer to move group-by operations up and down the query tree. Eager aggregation partially pushes a group-by past a join. After a group-by is partially pushed down, we still need to perform the original group-by in the upper query block. Eager aggregation reduces the number of input rows to the join and thus may result in a better overall plan. The reverse transformation, lazy aggregation, pulls a group-by above a join and combines two group-by operations into one. This transformation is typically of interest when an aggregation query references a grouped view (a view containing a group-by). Experimental results show that the technique is very beneficial for queries in the TPC-D benchmark. 1 Introduction Aggregation is widely used in decision support systems. All queries...
603|A new approach to the maximum flow problem|  All previously known efficient maximum-flow algorithms work by finding augmenting paths, either one path at a time (as in the original Ford and Fulkerson algorithm) or all shortest-length augmenting paths at once (using the layered network approach of Dinic). An alternative method based on the preflow concept of Karzanov is introduced. A preflow is like a flow, except that the total amount flowing into a vertex is allowed to exceed the total amount flowing out. The method maintains a preflow in the original network and pushes local flow excess toward the sink along what are estimated to be shortest paths. The algorithm and its analysis are simple and intuitive, yet the algorithm runs as fast as any other known method on dense. graphs, achieving an O(n³) time bound on an n-vertex graph. By incorporating the dynamic tree data structure of Sleator and Tarjan, we obtain a version of the algorithm running in O(nm log(n²/m)) time on an n-vertex, m-edge graph. This is as fast as any known method for any graph density and faster on graphs of moderate density. The algorithm also admits efticient distributed and parallel implementations. A parallel implementation running in O(n²log n) time using n processors and O(m) space is obtained. This time bound matches that of the Shiloach-Vishkin algorithm, which also uses n processors but requires O(n²) space.
604|A Structural Theory of Explanation-Based Learning|The impact of Explanation-Based Learning (EBL) on problem-solving efficiency varies greatly from one problem space to another. In fact, seemingly minute modifications to problem space encoding can drastically alter EBL&#039;s impact. For example, while prodigy/ebl  (a state-of-the-art EBL system) significantly speeds up the prodigy problem solver in the Blocksworld, prodigy/ebl actually slows prodigy down in a representational variant of the Blocksworld constructed by adding a single, carefully chosen, macro-operator to the Blocksworld operator set. Although EBL has been tested experimentally, no theory has been put forth that accounts for such phenomena. This paper presents such a theory. The theory exhibits a correspondence between a graph representation of problem spaces and the proofs used by EBL systems to generate search-control knowledge. The theory relies on this correspondence to account for the variations in EBL&#039;s impact. This account is validated by static, a program that extract...
605|Generating Parallel Execution Plans with a Partial-Order Planner|Many real-world planning problems require generating  plans that maximize the parallelism inherentina  problem. There are a number of partial-order planners  that generate such plans# however, in most of  these planners it is unclear under what conditions the  resulting plans will be correct and whether the planner  can even find a plan if one exists. This paper identifies  the underlying assumptions about when a partial plan  can be executed in parallel, defines the classes of parallel  plans that can be generated by different partial-order  planners, and describes the changes required to  turn ucpop into a parallel execution planner. In addition,  we describe how this planner can be applied to  the problem of query access planning, where parallel  execution produces substantial reductions in overall  execution time.  
606|The Need for Different Domain-Independent Heuristics|PRODIGY&#039;s planning algorithm uses domain-independent search heuristics. In this paper, we support our belief that there is no single search heuristic that performs more efficiently than others for all problems or in all domains. The paper presents three different domin-independent search heuristics of increasing complexity. We run PRODIGY with these heuristics in a series of artificial domains (introduced in (Barrett &amp; Weld 1994)) where in act one of the heuristics performs more efficiently than the others. However, we introduce an additional simple domain where the apparently worst heuristic outperforms the other two. The results we obtained...
607|Linkability: Examining Causal Link Commitments in Partial-Order Planning|Recently, several researchers have demonstrated domains where partially-ordered planners outperform totally-ordered planners. In (Barrett &amp; Weld 1994), Barrett and Weld build a series of artificial domains exploring the concepts of trivial and laborious serializability, in which a partially-ordered planner, SNLP, consistently outperforms two totally-ordered planners. In this paper, we demonstrate that...
608|Comparison of Methods for Improving Search Efficiency in a Partial-Order Planner|The search space in partial-order planning grows quickly with the number of subgoals and initial conditions, as well as less countable factors such as operator ordering and subgoal interactions. For partial-order planners to solve more than simple problems, the expansion of the search space will need to be controlled. This paper presents four new approaches to controlling search space expansion by exploiting commonalities in emerging plans. These approaches are described in terms of their algorithms, their effect on the completeness and correctness of the underlying planner and their expected performance. The four new and two existing approaches are compared on several metrics of search space and planning overhead. 1 Improving Search Efficiency in Planners  Partial order planning is becoming a common method of planning. Unfortunately, but hardly unexpectedly, the search space in partial order planning expands quickly as the problem size increases. Unfortunately, but less expectedly, se...
609|Dropout from higher education: A theoretical synthesis of recent research|Despite the very extensive literature on dropout from higher education, much remains unknown about the nature of the dropout process. In large measure, the failure of past research to delineate more clearly the multiple characteristics of dropout can be traced to two major shortcomings; namely, inadequate atten-tion given to questions of definition and to the development of theoretical models that seek to explain, not simply to describe, the processes that bring individuals to leave institutions of higher education. With regard to the former, inadequate attention given to definition has often led researchers to lump together, under the rubric of dropout, forms of leaving behavior that are very differ-ent in character. It is not uncommon to find, for instance, research on dropout that fails to distinguish dropout resulting from academic failure from that which is the outcome of voluntary withdrawal. Nor is it uncommon to find permanent dropouts placed together with persons whose leaving may be temporary in I am indebted to my research assistant, John B. Cullen, for having made an extensive literature search and compiling summaries of the literature for me. I am also indebted to Professors Peter Moock, to John Weidman, and to an unknown reviewer for their insightful comments on an earlier draft of this paper. The work reported here overlaps to a large extent work performed for the Office of
610|Attrition among college students|In the fall of 1961 a study of all entering freshmen students at a national sample of 248 colleges and universities was conducted at the National Merit Scholarship Corporation (Astin, 1965). The 127,212 students who participated in the study provided informa-tion on their socioeconomic backgrounds, high school activities and achievements, and educational and vocational aspirations.1 In the summer of 1965 the Office of Research of the American Council on Education2 followed up randomly selected samples of students from each of 246 colleges and universities included in the 1961 survey.3 Questionnaires were mailed to 60,078 of the original 127,212 students—or approximately 250 students per institution.4
611|Statistical mechanics of complex networks |Complex networks describe a wide range of systems in nature and society, much quoted examples including the cell, a network of chemicals linked by chemical reactions, or the Internet, a network of routers and computers connected by physical links. While traditionally these systems were modeled as random graphs, it is increasingly recognized that the topology and evolution of real
614|Learning to rank using gradient descent|We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine. 1.
615|IR evaluation methods for retrieving highly relevant documents|This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in moderu large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (In- Query ) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous rele- vance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods. 1. 
616|Classification by pairwise coupling|We discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the nature of the class probability estimates that arise, and examine the performance of the procedure in real and simulated datasets. Classifiers used include linear discriminants, nearest neighbors, and the support vector machine. 
617|Boosting Algorithms as Gradient Descent|Much recent attention, both experimental and theoretical, has been focussed on classification algorithms which produce voted combinations of classifiers. Recent theoretical work has shown that the impressive generalization performance of algorithms like AdaBoost can be attributed to the classifier having large margins on the training data. We present an abstract algorithm for finding linear combinations of functions that minimize arbitrary cost functionals (i.e functionals that do not necessarily depend on the margin). Many existing voting methods can be shown to be special cases of this abstract algorithm. Then, following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on
618|Log-Linear Models for Label Ranking|Label ranking is the task of inferring a total order over a predefined set of  labels for each given instance. We present a general framework for batch  learning of label ranking functions from supervised data. We assume that  each instance in the training data is associated with a list of preferences  over the label-set, however we do not assume that this list is either complete  or consistent. This enables us to accommodate a variety of ranking  problems. In contrast to the general form of the supervision, our goal is  to learn a ranking function that induces a total order over the entire set  of labels. Special cases of our setting are multilabel categorization and  hierarchical classification. We present a general boosting-based learning  algorithm for the label ranking problem and prove a lower bound on the  progress of each boosting iteration. The applicability of our approach is  demonstrated with a set of experiments on a large-scale text corpus.
619|Online ranking/collaborative filtering using the perceptron algorithm|In this paper we present a simple to implement truly online large margin version of the Perceptron ranking (PRank) algorithm, called the OAP-BPM (Online Aggregate Prank-Bayes Point Machine) algorithm, which finds a rule that correctly ranks a given training sequence of instance and target rank pairs. PRank maintains a weight vector and a set of thresholds to define a ranking rule that maps each instance to its respective rank. The OAP-BPM algorithm is an extension of this algorithm by approximating the Bayes point, thus giving a good generalization performance. The Bayes point is approximated by averaging the weights and thresholds associated with several PRank algorithms run in parallel. In order to ensure diversity amongst the solutions of the PRank algorithms we randomly subsample the stream of incoming training examples. We also introduce two new online versions of Bagging and the voted Perceptron using the same randomization trick as OAP-BPM, hence are referred to as OAP with extension-Bagg and-VP respectively. A rank learning experiment was conducted on a synthetic data set and collaborative filtering experiments on a number of real world data sets were conducted, showing that OAP-BPM has a better performance compared to PRank and a pure online regression algorithm, albeit with a higher computational cost, though is not too prohibitive. 1.
620|Probabilistic Visual Learning for Object Representation|We present an unsupervised technique for visual learning which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for unimodal distributions) and a Mixture-of-Gaussians model (for multimodal distributions). These probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition and coding. Our learning technique is applied to the probabilistic visual modeling, detection, recognition, and coding of human faces and non-rigid objects such as hands.
621|Snakes: Active contour models|A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the cap-ture region surrounding a feature. Snakes provide a unified account of a number of visual problems, in-cluding detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.
622|Active Shape Models -- &#034;Smart Snakes&#034;|We describe &#039;Active Shape Models&#039; which iteratively adapt to refine estimates of the pose, scale and shape of models of image objects. The method uses flexible models derived from sets of training examples. These models, known as Point Distribution Models, represent objects as sets of labelled points. An initial estimate of the location of the model points in an image is improved by attempting to move each point to a better position nearby. Adjustments to the pose variables and shape parameters are calculated. Limits are placed on the shape parameters ensuring that the example can only deform into shapes conforming to global constraints imposed by the training set. An iterative procedure deforms the model example to find the best fit to the image object. Results of applying the method are described. The technique is shown to be a powerful method for refining estimates of object shape and location. 
623|Modal Matching for Correspondence and Recognition|Modal matching is a new method for establishing correspondences and computing canonical descriptions. The method is based on the idea of describing objects in terms of generalized symmetries, as defined by each object&#039;s eigenmodes. The resulting modal description is used for object recognition and categorization, where shape similarities are expressed as the amounts of modal deformation energy needed to align the two objects. In general, modes provide a global-to-local ordering of shape deformation and thus allow for selecting which types of deformations are used in object alignment and comparison. In contrast to previous techniques, which required correspondence to be computed with an initial or prototype shape, modal matching utilizes a new type of finite element formulation that allows for an object&#039;s eigenmodes to be computed directly from available image information. This improved formulation provides greater generality and accuracy, and is applicable to data of any dimensionality. Correspondence results with 2-D contour and point feature data are shown, and recognition experiments with 2-D images of hand tools and airplanes are described.
624|Closed-Form Solutions for Physically Based Shape Modeling and Recognition| We present a closed-form, physically based solution for recovering a 3-D solid model from collections of 3-D surface measurements. Given a sufficient number of independent mea-surements, the solution is overconstrained and unique except for rotational symmetries. We then present a physically based object-recognition method that allows simple, closed-form comparisons of recovered 3-D solid models. The performance of these methods is evaluated using both synthetic range data with various signal-to-noise ratios and using laser rangefinder data. 
625|Human Face Detection in Visual Scenes|We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image, and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training the networks, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with other state-of-the-art face detection systems are presented; our system has better performance in terms of detection and false-positive rates. This work was partially supported by a grant from Siemens Corporate Research, Inc., by the Department of the Army, Army Research Office under grant number DAAH04-94-G-0006, and by the Office of Naval Research under grant number N00014-95-1-0591. This work was started while Shumeet Balu...
626|Face Recognition using View-Based and Modular Eigenspaces|In this paper we describe experiments using eigenfaces for recognition and interactive search in the FERET face database. A recognition accuracy of 99.35% is obtained using frontal views of 155 individuals. This figure is consistent with the 95% recognition rate obtained previously on a much larger database of 7,562 &#034;mugshots&#034; of approximately 3,000 individuals, consisting of a mix of all age and ethnic groups. We also demonstrate that we can automatically determine head pose without significantly lowering recognition accuracy; this is accomplished by use of a viewbased multiple-observer eigenspace technique. In addition, a modular eigenspace description is used which incorporates salient facial features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields slightly higher recognition rates as well as a more robust framework for face recognition. In addition, a robust and automatic feature detection technique using eigentemplates is demonstra...
627|Surface Learning with Applications to Lipreading|Most connectionist research has focused on learning mappings from one space to another (eg. classification and regression). This paper introduces the more general task of learning constraint surfaces. It describes a simple but powerful architecture for learning and manipulating nonlinear surfaces from data. We demonstrate the technique on low dimensional synthetic surfaces and compare it to nearest neighbor approaches. We then show its utility in learning the space of lip images in a system for improving speech recognition by lip reading. This learned surface is used to improve the visual tracking performance during recognition.  
628|Human Face Recognition and the Face Image Set&#039;s Topology|If we consider an n x n image as an n  2  dimensional vector, then images of faces can be considered as points in this n  2  -dimensional image space. Our previous studies of physical transformations of the face, including translation, small rotations and illumination changes, showed that the set of face images consists of relatively simple connected sub-regions in image space [1]. Consequently linear matching techniques can be used to obtain reliable face recognition. However for more general transformations, such as large rotations or scale changes, the face subregions become highly non-convex. We have therefore developed a scale-space matching technique that allows us to take advantage of knowledge about important geometrical transformations and about the topology of the face subregion in image space. While recognition of faces is the focus of this paper, the algorithm is sufficiently general to be applicable to a large variety of object recognition tasks.  List of Symbols  ffi: Gr...
629|Automating the Hunt for Volcanoes on Venus|Our long-term goal is to develop a trainable tool for locating patterns of interest in large image databases. Toward this goal we have developed a prototype system, based on classical filtering and statistical pattern recognition techniques, for automatically locating volcanoes in the Magellan SAR database of Venus. Training for the specific volcano-detection task is obtained by synthesizing feature templates (via normalization and principal components analysis) from a small number of examples provided by experts. Candidate regions identified by a focus of attention (FOA) algorithm are classified based on correlations with the feature templates. Preliminary tests show performance comparable to trained human observers. 1 Introduction  Many geological studies use surface features to deduce processes that have occurred on a planet. The recent JPL Magellan mission, which was successful in imaging over 95% of the surface of Venus with synthetic aperture radar (SAR), has provided planetary s...
630|On Comprehensive Visual Learning|1  Comprehensive visual learning is the treatment of theories and techniques for computer vision systems to automatically learn to understand comprehensive visual information with minimal human-imposed rules about the visual world. This article discusses some major performance difficulties encountered by currently prevailing approaches to computer vision and introduces the promising direction of comprehensive learning towards overcoming these difficulties. It also indicates why the direction may have a profound impact on the performance of computer vision algorithms for real world problems. Some example techniques for comprehensive visual learning are presented.  1 Introduction  An image of a real-world scene depends on a series of factors, illumination, object shape, surface reflectance, viewing geometry, sensor type, etc. The image is a result of compound interactions among these factors. In the real world, change in these factors is ubiquitous and mostly is not known a priori. This ...
631|Eraser: a dynamic data race detector for multithreaded programs|Multi-threaded programming is difficult and error prone. It is easy to make a mistake in synchronization that produces a data race, yet it can be extremely hard to locate this mistake during debugging. This paper describes a new tool, called Eraser, for dynamically detecting data races in lock-based multi-threaded programs. Eraser uses binary rewriting techniques to monitor every shared memory reference and verify that consistent locking behavior is observed. We present several case studies, including undergraduate coursework and a multi-threaded Web search engine, that demonstrate the effectiveness of this approach. 1
632|Time, Clocks, and the Ordering of Events in a Distributed System|The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.  
633|Atom: A system for building customized program analysis tools|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
634|Monitors: An Operating System Structuring Concept|This is a digitized copy derived from an ACM copyrighted work. It is not guaranteed to be an accurate copy of the author&#039;s original work. This paper develops Brinch-Hansen&#039;s concept of a monitor as a method of structuring an operating system. It introduces a form of synchronization, describes a possible rnctltotl of implementation in terms of semaphorcs and gives a suitable proof rule. Illustrative examples include a single rcsourcc scheduler, a bounded buffer, an alarm clock, a buffer pool, a disk head optimizer, and a version of the problem of readers and writers. Key Words and Phrases: monitors, operating systems,schcduling, mutual exclusion, synchronization, system implementation langua yes, structured multiprogramming CR Categories:
635|Extensibility, safety and performance in the SPIN operating system|This paper describes the motivation, architecture and performance of SPIN, an extensible operating system. SPIN provides an extension infrastructure, together with a core set of extensible services, that allow applications to safely change the operating system&#039;s interface and implementation. Extensions allow an application to specialize the underlying operating system in order to achieve a particular level of performance and functionality. SPIN uses language and link-time mechanisms to inexpensively export ne-grained interfaces to operating system services. Extensions are written in a type safe language, and are dynamically linked into the operating system kernel. This approach o ers extensions rapid access to system services, while protecting the operating system code executing within the kernel address space. SPIN and its extensions are written in Modula-3 and run on DEC Alpha workstations. 1
636|Petal: Distributed Virtual Disks|The ideal storage system is globally accessible, always available, provides unlimited performance and capacity for a large number of clients, and requires no management. This paper describes the design, implementation, and performance of Petal, a system that attempts to approximate this ideal in practice through a novel combination of features. Petal consists of a collection of networkconnected servers that cooperatively manage a pool of physical disks. To a Petal client, this collection appears as a highly available block-level storage system that provides large abstract containers called virtual disks. A virtual disk is globally accessible to all Petal clients on the network. A client can create a virtual disk on demand to tap the entire capacity and performance of the underlying physical resources. Furthermore, additional resources, such as servers and disks, can be automatically incorporated into Petal. We have an initial Petal prototype consisting of four 225 MHz DEC 3000/700 work...
638|Shasta: A LowOverhead Software-Only Approach to Fine-Grain Shared Memory|Digital Equipment Corporation This paper describes Shasta, a system that supports a shared address space in software on clusters of computers with physically distributed memory. A unique aspect of Shasta compared to most other software distributed shared memory systems is that shared data can be kept coherent at a fine granularity. In addition, the system allows the coherence granularity to vary across different shared data structures in a single application. Shasta implements the shared address space by transparently rewriting the application executable to intercept loads and stores. For each shared load or store, the inserted code checks to see if the data is available locally and communicates with other processors if necessary. The system uses numerous techniques to reduce the run-time overhead of these checks. Since Shasta is implemented entirely in software,
639|Experience with Processes and Monitors in Mesa|The use of monitors for describing concurrency has been much discussed in the literature. When monitors are used in real systems of any size, however, a number of problems arise which have not been adequately dealt with: the semantics of nested monitor calls; the various ways of defining the meaning of WAIT; priority scheduling; handling of timeouts, aborts and other exceptional conditions; interactions with process creation and destruction; monitoring large numbers of small objects. These problems are addressed by the facilities described here for concurrent programming in Mesa. Experience with several substantial applications gives us some confidence in the validity of our solutions.
640|On-the-fly Detection of Data Races for Programs with Nested Fork-Join Parallelism|Detecting data races in shared-memory parallel programs is an important debugging problem. This paper presents a new protocol for run-time detection of data races in executions of shared-memory programs with nested fork-join parallelism and no other interthread synchronization. This protocol has significantly smaller worst-case run-time overhead than previous techniques. The worst-case space required by our protocol when monitoring an execution of a program P is O(V N), where V is the number of shared variables in P , and N is the maximum dynamic nesting of parallel constructs in P &#039;s execution. The worst-case time required to perform any monitoring operation is O(N). We formally prove that our new protocol always reports a non-empty subset of the data races in a monitored program execution and describe how this property leads to an e#ective debugging strategy.
641|Online Data-Race Detection via Coherency Guarantees|We present the design and evaluation of an on-thefly data-race-detection technique that handles applications written for the lazy release consistent (LRC) shared memory model. We require no explicit association between synchronization and shared memory. Hence, shared accesses have to be tracked and compared at the minimum granularity of data accesses, which is typically a single word.  The novel aspect of this system is that we are able to leverage information used to support the underlying memory abstraction to perform on-the-fly data-race detection, without compiler support. Our system consists of a minimally modified version of the CVM distributed shared memory system, and instrumentation code inserted by the ATOM code re-writer.  We present an experimental evaluation of our technique by using our system to look for data races in four unaltered programs. Our system correctly found read-write data races in a program that allows unsynchronized read access to a global tour bound, and a write-write race in a program from a standard benchmark suite. Overall, our mechanism reduced program performance by approximately a factor of two.   
642|Compile-time Support for Efficient Data Race Detection in Shared-Memory Parallel Programs|Data race detection strategies based on software runtime monitoring are notorious for dramatically inflating execution times of shared-memory parallel programs. Without significant reductions in the execution overhead incurred when using these techniques, there is little hope that they will be widely used. A promising approach to this problem is to apply compile-time analysis to identify variable references that need not be monitored at run time because they will never be involved in a data race. In this paper we describe eraser, a data race instrumentation tool that uses aggressive program analysis to prune the number of references to be monitored. To quantify the effectiveness of our analysis techniques, we compare the overhead of race detection with three levels of compile-time analysis ranging from little analysis to aggressive interprocedural analysis for a suite of test programs. For the programs tested, using interprocedural analysis and dependence analysis dramatically reduced ...
643|Learning the Kernel Matrix with Semi-Definite Programming|Kernel-based learning algorithms work by embedding the data into a Euclidean space, and  then searching for linear relations among the embedded data points. The embedding is performed  implicitly, by specifying the inner products between each pair of points in the embedding  space. This information is contained in the so-called kernel matrix, a symmetric and positive  definite matrix that encodes the relative positions of all points. Specifying this matrix amounts  to specifying the geometry of the embedding space and inducing a notion of similarity in the  input space---classical model selection problems in machine learning. In this paper we show how  the kernel matrix can be learned from data via semi-definite programming (SDP) techniques. When applied
644|Online Learning with Kernels|  Kernel based algorithms such as support vector machines have achieved considerable success in various problems in the batch setting where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper we consider online learning in a Reproducing Kernel Hilbert Space. By considering classical stochastic gradient descent within a feature space, and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst case loss bounds and moreover we show the convergence of the hypothesis to the minimiser of the regularised risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection. In addition
645|Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones|SeDuMi is an add-on for MATLAB, that lets you solve optimization problems with linear, quadratic and semidefiniteness constraints. It is possible to have complex valued data and variables in SeDuMi. Moreover, large scale optimization problems are solved efficiently, by exploiting sparsity. This paper describes how to work with this toolbox.
647|Arcing Classifiers|Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging (Breiman [1996a] ) Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire [1995,1996] propose an algorithm the basis of which is to adaptively resample and combine  (hence the acronym--arcing) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets....
648|On kernel-target alignment|Editor: Kernel based methods are increasingly being used for data modeling because of their conceptual simplicity and outstanding performance on many tasks. However, the kernel function is often chosen using trial-and-error heuristics. In this paper we address the problem of measuring the degree of agreement between a kernel and a learning task. A quantitative measure of agreement is important from both a theoretical and practical point of view. We propose a quantity to capture this notion, which we call Alignment. We study its theoretical properties, and derive a series of simple algorithms for adapting a kernel to the labels and vice versa. This produces a series of novel methods for clustering and transduction, kernel combination and kernel selection. The algorithms are tested on two publicly available datasets and are shown to exhibit good performance.
649|DETERMINANT MAXIMIZATION WITH LINEAR MATRIX INEQUALITY CONSTRAINTS   | The problem of maximizing the determinant of a matrix subject to linear matrix inequalities arises in many fields, including computational geometry, statistics, system identification, experiment design, and information and communication theory. It can also be considered as a generalization of the semidefinite programming problem. We give an overview of the applications of the determinant maximization problem, pointing out simple cases where specialized algorithms or analytical solutions are known. We then describe an interior-point method, with a simplified analysis of the worst-case complexity and numerical results that indicate that the method is very efficient, both in theory and in practice. Compared to existing specialized algorithms (where they are available), the interior-point method will generally be slower; the advantage is that it handles a much wider variety of problems.  
650|Diffusion kernels on graphs and other discrete input spaces|The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels called diffusion kernels, which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space.
651|Empirical margin distributions and bounding the generalization error of combined classifiers|Dedicated to A.V. Skorohod on his seventieth birthday We prove new probabilistic upper bounds on generalization error of complex classifiers that are combinations of simple classifiers. Such combinations could be implemented by neural networks or by voting methods of combining the classifiers, such as boosting and bagging. The bounds are in terms of the empirical distribution of the margin of the combined classifier. They are based on the methods of the theory of Gaussian and empirical processes (comparison inequalities, symmetrization method, concentration inequalities) and they improve previous results of Bartlett (1998) on bounding the generalization error of neural networks in terms of l1-norms of the weights of neurons and of Schapire, Freund, Bartlett and Lee (1998) on bounding the generalization error of boosting. We also obtain rates of convergence in Lévy distance of empirical margin distribution to the true margin distribution uniformly over the classes of classifiers and prove the optimality of these rates.
652|Text Categorization by Boosting Automatically Extracted Concepts|Term-based representations of documents have found widespread use in information retrieval. However, one of the main shortcomings of such methods is that they largely disregard lexical semantics and, as a consequence, are not sufficiently robust with respect to variations in word usage. In this paper we investigate the use of concept-based document representations to supplement word- or phrase-based features. The utilized concepts are automatically extracted from documents via probabilistic latent semantic analysis. We propose to use AriaBoost to optimally combine weak hypotheses based on both types of features. Experimental results on standard benchmarks confirm the validity of our approach, showing that AriaBoost achieves consistent improvements by including additional semantic features in the learned ensemble.
654|Support Vector Machines for Text Categorization Based on Latent Semantic Indexing|Text Categorization(TC) is an important component in  many information organization and information  management tasks. Two key issues in TC are feature  coding and classifier design. In this paper Text  Categorization via Support Vector Machines(SVMs)  approach based on Latent Semantic Indexing(LSI) is  described. Latent Semantic Indexing[1][2] is a method for  selecting informative subspaces of feature spaces with the  goal of obtaining a compact representation of document. Support Vector Machines[3] are powerful machine  learning systems, which combine remarkable performance with an elegant theoretical framework. The SVMs well  fits the Text Categorization task due to the special  properties of text itself. Experiments show that the  LSI+SVMs frame improves clustering performance by  focusing attention of Support Vector Machines onto  informative subspaces of the feature spaces.
655|Convex tuning of the soft margin parameter|In order to deal with known limitations of the hard margin support vector machine (SVM) for binary classication | such as overtting and the fact that some data sets are not linearly separable |, a soft margin approach has been proposed in literature [2, 4, 5]. The soft margin SVM allows training data to be misclassied to a certain extent, by introducing slack variables and penalizing the cost function with an error term, i.e., the 1-norm or 2-norm of the corre-sponding slack vector. A regularization parameter C trades o  the importance of maximizing the margin versus minimizing the error. While the 2-norm soft margin algorithm itself is well understood, and a generalization bound is known [4, 5], no computationally tractable method for tuning the soft margin parameter C has been proposed so far. In this report we present a convex way to optimize C for the 2-norm soft margin SVM, by maximizing this generalization bound. The resulting problem is a quadratically constrained quadratic programming (QCQP) problem, which can be solved in polynomial time O(l3) with l the number of training samples. 1
656|An Overview of AspectJ|AspectJ-TM is a simple and practical aspect-oriented extension to Java-TM. With just a few new constructs, AspectJ provides support for modular implementation of a range of crosscutting concerns. In AspectJ&#039;s dynamic join point model, join points are well-defined points in the execution of the program
657|On the Criteria To Be Used in Decomposing Systems into Modules|This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a “modularization ” is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.
658|Direct manipulation: a step beyond programming languages|Direct manipulation systems offer the satisfying experience of operating on visible objects. The computer becomes transparent, and users can concentrate on their tasks.
659|Usability Analysis of Visual Programming Environments: a `cognitive dimensions&#039; framework|The cognitive dimensions framework is a broad-brush evaluation technique for interactive  devices and for non-interactive notations. It sets out a small vocabulary of terms designed to  capture the cognitively-relevant aspects of structure, and shows how they can be traded off  against each other. The purpose of this paper is to propose the framework as an evaluation  technique for visual programming environments. We apply it to two commercially-available  dataflow languages (with further examples from other systems) and conclude that it is effective  and insightful; other HCI-based evaluation techniques focus on different aspects and  would make good complements. Insofar as the examples we used are representative, current  VPLs are successful in achieving a good `closeness of match&#039;, but designers need to consider  the `viscosity&#039; (resistance to local change) and the `secondary notation&#039; (possibility of conveying  extra meaning by choice of layout, colour, etc.). 
660|D: A LANGUAGE FRAMEWORK FOR DISTRIBUTED PROGRAMMING|Two of the most important issues in distributed systems are the synchronization of concurrent threads and the application-level data transfers between execution spaces. At the design level, addressing these issues typically requires analyzing the components under a different perspective than is required to analyze the functionality. Very often, it also involves analyzing several components at the same time, because of the way those two issues cross-cut the units of functionality. At the implementation level, existing programming languages fail to provide adequate support for programming in terms of these different and cross-cutting perspectives. The result is that the programming of synchronization and remote data transfers ends up being tangled throughout the components code in more or less arbitrary ways. This thesis presents a language framework called D that untangles the implementation of synchronization
661|Adaptive Plug-and-Play Components for Evolutionary Software Development|In several works on design methodologies, design patterns, programming language design  and application frameworks, the need for adequate higher-level program entities that capture  the patterns of collaboration between several classes has been recognized. The idea is that  in general the unit of reuse is not a single class, but a slice of behavior affecting a set of collaborating  classes. The absence of large-scale components for expressing these collaborations  makes object-oriented programs more difficult to maintain and reuse, because functionality  is spread over several methods and it becomes difficult to get the &#034;big picture&#034;. In this paper,  we propose Adaptive Plug and Play Components to serve this need. These components are  designed such that they not only facilitate the construction of complex software by making  the collaborations explicit, but they do so in a manner that supports the evolutionary nature  of both structure and behavior.  1 Introduction  The step from proc...
662|Hyper/J™: Multi-Dimensional Separation of Concerns for Java™|Hyper/J supports a new approach to constructing, integrating and evolving software, called multi-dimensional separation of concerns. Developers can decompose and organize code and other artifacts according to multiple, arbitrary criteria (concerns) simultaneously—even after the software has been implemented—and synthesize or integrate the pieces into larger-scale components and systems. Hyper/J facilitates several common development and evolution activities non-invasively, including: adaptation and customization, mix-and-match of features, reconciliation and integration of multiple domain models, reuse, product line management, extraction or replacement of existing parts of software, and on-demand remodularization. Hyper/J works with standard Java software, not requiring special compilers or environments. This demonstration will show it in action in a number of software engineering scenarios at different stages of the software lifecycle.  
663|Hybrid Group Reflective Architecture for Object-Oriented Concurrent Reflective Programming|The benefits of computational reflection are the abilities to reason and alter the  dynamic behavior of computation from within the language framework. This is more  beneficial in concurrent/distributed computing, where the complexity of the system is  much greater compared to sequential computing; we have demonstrated various benefits  in our past research of Object-Oriented Concurrent Reflective (OOCR) architectures.  Unfortunately, attempts to formulate reflective features provided in practical reflective  systems, such as resource management, have led to some difficulties in maintaining  the linguistic lucidity necessary in computational reflection. The primary reason is  that previous OOCR architectures lack the ingredients for group-wide object coordination.  We present a new OOCR language with a hybrid group reflective architecture,  ABCL/R2, whose key features are the notion of heterogeneous object groups and coordinated  management of group shared resources. We describe and gi...
664|Aspect-Oriented Logic Meta Programming|We propose to use a logic meta-system as a general framework  for aspect-oriented programming. We illustrate our approach with  the implementation of a simpli#ed version of the cool aspect language  for expressing synchronization of Java programs. Using this case as an  example we illustrate the principle of aspect-orientedlogic meta programming   and how it is useful for implementing weavers on the one hand and  on the other hand also allows users of aop to #ne-tune, extend and adapt  an aspect language to their speci#c needs.
665|RbCl: A Reflective Object-Oriented Concurrent Language without a Run-time Kernel|We propose a reflective object-oriented concurrent language RbCl which has no run-time kernel. That is to say, all the behavior of RbCl except for what is restricted by the operating system and hardware can be modified/extended by the user. RbCl runs efficiently in a distributed environment and is intended for practical use. The execution of an RbCl program is performed by a metasystem that consists of metalevel objects. All the features of RbCl including concurrent execution, inter-node communication, and even reflective facilities themselves are realized by the metalevel objects, which are modifiable and extendable. Important metalevel objects are called system objects, that are registered in system object tables. The user can change the behavior of the metasystem by replacing elements of system object tables with user-defined objects. RbCl also provides a novel feature called linguistic symbiosis for metalevel objects. All the metalevel objects in the initial RbCl metasystem are act...
666|Implicit Context: Easing Software Evolution and Reuse|Software systems should consist of simple, conceptually clean software components interacting along narrow, well-defined paths. All too often, this is not reality: complex components end up interacting for reasons unrelated to the functionality they provide. We refer to knowledge within a component that is not conceptually required for the individual behaviour of that component as extraneous embedded knowledge (EEK). EEK creeps into a system in many forms, including dependences upon particular names and the passing of extraneous parameters. This paper proposes the use of implicit context as a means for reducing EEK in systems by combining a mechanism to reflect upon what has happened in a system, through queries on the call history, with a mechanism for altering calls to and from a component. We demonstrate the benefits of implicit context by describing its use to reduce EEK in the Java  TM Swing library.  
667|Exploring an Aspect-Oriented Approach to Operating System Code|This paper presents the initial results of our experiment using an aspect-oriented approach to simplify operating system code. The intuition behind this work is that complexity comes from crosscutting concerns. In this experiment, we re-implemented prefetching for page fault handling and file system read requests using a hypothetical language, AspectC -- a variant of AspectJ for C -- and hand-compiled the code to C. We believe that the aspect-oriented code is easier to understand, and in particular that it is easier to understand the interaction between the virtual memory and file system prefetching mechanisms.
668|Multiple sequence alignment with the Clustal series of programs|The Clustal series of programs are widely used in molecular biology for the multiple alignment of both nucleic acid and protein sequences and for preparing phylogenetic trees. The popularity of the programs depends on a number of factors, including not only the accuracy of the results, but also the robustness, portability and user-friendliness of the programs. New features include NEXUS and FASTA format output, printing range numbers and faster tree calculation. Although, Clustal was originally developed to run on a local computer, numerous Web servers have been set up, notably at the EBI
669|The Pfam protein families database|Pfam is a large collection of protein families and domains. Over the past 2 years the number of families in Pfam has doubled and now stands at 6190 (version 10.0). Methodology improvements for searching the Pfam collection locally as well as via the web are described. Other recent innovations include modelling of discontinuous domains allow-ing Pfam domain de®nitions to be closer to those found in structure databases. Pfam is available on the web in the UK
670|Optimal alignments in linear space|Space, not time, is often the limiting factor when computing optimal sequence alignments, and a number of recent papers in the biology literature have proposed space-saving strategies. However, a 1975 computer science paper by Hirschberg presented a method that is superior to the newer proposals, both in theory and in practice. The goal of this note is to give Hirschberg’s idea the visibility it deserves by developing a linear-space version of Gotoh’s algorithm, which accommodates affine gap penalties. A portable C-software package implementing this algorithm is available on the BIONET free of charge.
671|NEXUS: an extensible file format for systematic information|Abstract.—NEXUS is a file format designed to contain systematic data for use by computer pro-grams. The goals of the format are to allow future expansion, to include diverse kinds of infor-mation, to be independent of particular computer operating systems, and to be easily processed by a program. To this end, the format is modular, with a file consisting of separate blocks, each containing one particular kind of information, and consisting of standardized commands. Public blocks (those containing information utilized by several programs) house information about taxa, morphological and molecular characters, distances, genetic codes, assumptions, sets, trees, etc.; private blocks contain information of relevance to single programs. A detailed description of commands in public blocks is given. Guidelines are provided for reading and writing NEXUS files and for extending the format. [Computer program; file format; NEXUS; systematics.] NEXUS is a file format designed to house systematic data. Although it is cur-rently in use by several computer pro-grams, including MacClade 3.07 (Maddi-
672|Location-Aided Routing (LAR) in mobile ad hoc networks  (1998) |A mobile ad hoc network consists of wireless hosts that may move often. Movement of hosts results in a change in routes, requiring some mechanism for determining new routes. Several routing protocols have already been proposed for ad hoc networks. This paper suggests an approach to utilize location information (for instance, obtained using the global positioning system) to improve performance of routing protocols for ad hoc networks. By using location information, the proposed Location-Aided Routing (LAR) protocols limit the search for a new route to a smaller “request zone” of the ad hoc network. This results in a significant reduction in the number of routing messages. We present two algorithms to determine the request zone, and also suggest potential optimizations to our algorithms.
673|Dynamic source routing in ad hoc wireless networks|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any established infrastructure or centralized administration. In such an environment, it may be necessary for one mobile host to enlist the aid of other hosts in forwarding a packet to its destination, due to the limited range of each mobile host’s wireless transmissions. This paper presents a protocol for routing in ad hoc networks that uses dynamic source routing. The protocol adapts quickly to routing changes when host movement is frequent, yet requires little or no overhead during periods in which hosts move less frequently. Based on results from a packet-level simulation of mobile hosts operating in an ad hoc network, the protocol performs well over a variety of environmental conditions such as host density and movement rates. For all but the highest rates of host movement simulated, the overhead of the protocol is quite low, falling to just 1 % of total data packets transmitted for moderate movement rates in a network of 24 mobile hosts. In all cases, the difference in length between the routes used and the optimal route lengths is negligible, and in most cases, route lengths are on average within a factor of 1.01 of optimal. 1.
674|Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers  (1994) |An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize adhoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.  
675|A Performance Comparison of Multi-Hop Wireless Ad Hoc Network Routing Protocols|An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Due to the limited transmission range of wireless network interfaces, multiple network &amp;quot;hops &amp;quot; may be needed for one node to exchange data with another across the network. In recent years, a variety of new routing protocols targeted specifically at this environment have been developed, but little performance information on each protocol and no realistic performance comparison between them is available. This paper presents the results of a detailed packet-level simulation comparing four multi-hop wireless ad hoc network routing protocols that cover a range of design choices: DSDV, TORA, DSR, and AODV. We have extended the ns-2 network simulator to accurately model the MAC and physical-layer behavior of the IEEE 802.11 wireless LAN standard, including a realistic wireless transmission channel model, and present the results of simulations of networks of 50 mobile nodes. 1
676|A Highly Adaptive Distributed Routing Algorithm for Mobile Wireless Networks|We present a new distributed routing protocol for mobile, multihop, wireless networks. The protocol is one of a family of protocols which we term &#034;link reversal&#034; algorithms. The protocol&#039;s reaction is structured as a temporally-ordered sequence of diffusing computations; each computation consisting of a sequence of directed l i nk reversals. The protocol is highly adaptive, efficient and scalable; being best-suited for use in large, dense, mobile networks. In these networks, the protocol&#039;s reaction to link failures typically involves only a localized &#034;single pass&#034; of the distributed algorithm. This capability is unique among protocols which are stable in the face of network partitions, and results in the protocol&#039;s high degree of adaptivity. This desirable behavior is achieved through the novel use of a &#034;physical or logical clock&#034; to establish the &#034;temporal order&#034; of topological change events which is used to structure (or order) the algorithm&#039;s reaction to topological changes. We refer to the protocol as the Temporally-Ordered Routing Algorithm (TORA).
677|GeoCast - Geographic Addressing and Routing|In the near future GPS will be widely used, thus allowing a broad variety of location dependent services such as direction giving, navigation, etc. In this paper we propose and evaluate a routing and addressing method to integrate geographic coordinates into the Internet Protocol to enable the creation of location dependent services. The main challenge is to integrate the concept of physical location into the current design of the Internet which relies on logical addressing.
678|Comparative Performance Evaluation of Routing Protocols for Mobile, Ad hoc Networks|We evaluate several routing protocols for mobile, wireless, ad hoc networks via packet level simulations. The protocol suite includes routing protocols specifically designed for ad hoc routing, as well as more traditional protocols, such as link state and distance vector, used for dynamic networks. Performance is evaluated with respect to fraction of packets delivered, end-to-end delay and routing load for a given traffic and mobility model. It is observed that the new generation of on-demand routing protocols use much lower routing load. However, the traditional link state and distance vector protocols provide, in general, better packet delivery and delay performance. 1. Introduction  A mobile, ad hoc network [4] is an autonomous system of mobile hosts connected by wireless links. There is no static infrastructure such as base stations. If two hosts are not within radio range, all message communication between them must pass through one or more intermediate hosts that double as router...
679|Movement-based location update and selective paging for PCS networks| This paper introduces a mobility tracking mechanism that combines a movement-based location update policy with a selective paging scheme. Movement-based location update is selected for its simplicity. It does not require ea &amp; mobile terminal to store information about the arrangement and the distance relationship of all cells. In fact, each mobile terminal only keeps a counter of the number of cells visited. A location update is performed when this counter exceeds a predefined threshold value. This scheme allows the dynamic selection of the movement threshold on a per-user basis. This is desirable as different users may have very different mobility patterns. Selective paging reduces the cost for locating a mobile terminal in the expense of an increase in the paging delay. In this paper, we propose a selective paging scheme which significantly decreases the location tracking cost under a small increase in the allowable paging delay. We introduce an analytical model for the proposed location tracking mechanism which captures the mobility and the incoming call arrival patterns of each mobile terminal. Analytical results are provided to demonstrate the cost-effectiveness of the proposed scheme under various parameters. 
680|Signal Stability based Adaptive Routing (SSA) for Ad-Hoc Mobile Networks  (1997) |Unlike static networks, ad-hoc networks have no spatial hierarchy and suffer from frequent link failures which prevent mobile hosts from using traditional routing schemes. Under these conditions, mobile hosts must find routes to destinations without the use of designated routers and also must dynamically adapt the routes to the current link conditions. This paper proposes a distributed adaptive routing protocol for finding and maintaining stable routes based on signal strength and location stability in an ad-hoc network and presents an architecture for its implementation. 1 Introduction  Mobility is becoming increasingly important for users of computing systems. Technology has made possible wireless devices and smaller, less expensive, and more powerful computers. As a result users gain flexibility and the ability to maintain connectivity to their primary computer while roaming through a large area. The number of users with portable laptops and personal communications devices is increa...
681|A Survey of Routing Techniques for Mobile Communications Networks|Mobile wireless networks pose interesting challenges for routing system design. To produce feasible routes in a  mobile wireless network, a routing system must be able to accommodate roving users, changing network topology, and fluctuating  link quality. We discuss the impact of node mobility and wireless communication on routing system design, and we survey  the set of techniques employed in or proposed for routing in mobile wireless networks.
682|Routing in Ad Hoc Networks Using a Spine|We present a two-level hierarchical routing architecture for ad hoc networks. Within each lower level cluster, we describe a self-organizing, dynamic spine  structure to (a) propagate topology changes, (b) compute updated routes in the background, and (c) provide backup routes in case of transient failures of the primary routes. We analyze and bound the worst case of movements between upper level clusters to show that this hierarchical architecture scales well with network size. 1 Introduction  Ad hoc networks are multihop networks in which mobile hosts share a scarce wireless channel. In ad hoc networks, the network topology changes frequently. Hence, routing algorithms must expend overhead either to maintain current routing and topology tables or to discover up-to-date routes. Currently, most routing algorithms for ad hoc networks are flat, that is, designed with only one level of hierarchy. These flat routing algorithms can suffer from excessive overhead as network sizes increase. I...
683|GPS-Based Addressing and Routing|In the near future GPS will be widely used, thus allowing a broad variety of location dependent services such as direction giving, navigation, etc. In this document we propose a family of protocols and addressing methods to integrate GPS into the Internet Protocol to enable the creation of location dependent services. The solutions which we present are flexible (scalable) in terms of the target accuracy of the GPS. The main challenge is to integrate the concept of physical location into the current design of the Internet which relies on logical addressing. Two solutions are presented in this draft and a third solution is sketched. Figure 1: GPS Satellites Orbiting the Earth  Contents  1 Introduction 3  1.1 Scenarios of Usage and Interface Issues : : : : : : : : : : : : : 4  2 Background 5  2.1 Related Work : : : : : : : : : : : : : : : : : : : : : : : : : : : 5 2.2 Global Positioning System (GPS) : : : : : : : : : : : : : : : : 6 2.2.1 What is GPS? : : : : : : : : : : : : : : : : : : ...
684|Location-Based Multicast in Mobile Ad Hoc Networks|Multicast distribution in mobile wireless networks is a topic that has recently begun to be explored. For multicasting, conventional protocols define a multicast group as a collection of hosts which register to a multicast group address. However, in this paper, we define a location-based multicast group which is based on a specific region (&#034;Multicast  Region&#034;) in a mobile ad hoc network (MANET). Hosts within the multicast region at a given time form the multicast group at that time. We present two algorithms for delivering packets to such a multicast group, and present simulation results. 1 Introduction  Multicast distribution in mobile wireless networks is a topic that has recently begun to be explored [22]. When an application must send the same information to more than one destination, multicasting is often used. Multicasting has played an important role in supporting multimedia applications, such as audio/video broadcasting. Multicasting is much more advantageous than multiple unic...
685|Using Location Information to Improve Routing in Ad Hoc Networks|In ad hoc network environments, any techniques to reduce high routing-related overhead are worth investigating. This brief note explains how to exploit location information to improve ad hoc routing. We also suggest some optimization approaches that can improve the performance of the protocol.    Research reported is supported in part by Texas Advanced Technology Program 009741-052-C.  1 Introduction  Many protocols for routing have been proposed for mobile wireless networks (also known as ad hoc or mobile mesh networks) [5, 6, 8, 9, 11, 12, 14, 16, 18]. As the hosts in such a network are mobile, an inherent drawback of these protocols (more precisely, drawback of the network) is that routing-related overhead tends to be high. Therefore, any techniques to reduce this overhead are worth investigating. In this brief note, we consider how to exploit location information to improve ad hoc routing. 2 Motivation and Related Works  Location information is important in mobile computing enviro...
686|FAST VOLUME RENDERING USING A SHEAR-WARP FACTORIZATION OF THE VIEWING TRANSFORMATION|Volume rendering is a technique for visualizing 3D arrays of sampled data. It has applications in areas such as medical imaging and scientific visualization, but its use has been limited by its high computational expense. Early implementations of volume rendering used brute-force techniques that require on the order of 100 seconds to render typical data sets on a workstation. Algorithms with optimizations that exploit coherence in the data have reduced rendering times to the range of ten seconds but are still not fast enough for interactive visualization applications. In this thesis we present a family of volume rendering algorithms that reduces rendering times to one second. First we present a scanline-order volume rendering algorithm that exploits coherence in both the volume data and the image. We show that scanline-order algorithms are fundamentally more efficient than commonly-used ray casting algorithms because the latter must perform analytic geometry calculations (e.g. intersecting rays with axis-aligned boxes). The new scanline-order algorithm simply streams through the volume and the image in storage order. We describe variants of the algorithm for both parallel and perspective projections and
687|Marching cubes: A high resolution 3D surface construction algorithm|We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.
688|The rendering equation|ABSTRACT. We present an integral equation which generallzes a variety of known rendering algorithms. In the course of discussing a monte carlo solution we also present a new form of variance reduction, called Hierarchical sampling and give a number of elaborations shows that it may be an efficient new technique for a wide variety of monte carlo procedures. The resulting renderlng algorithm extends the range of optical phenomena which can be effectively simulated.
689|Display of Surfaces from Volume Data|The application of volume rendering techniques to the display of surfaces from sampled scalar functions of three spatial dimensions is explored. Fitting of geometric primitives to the sampled data is not required. Images are formed by directly shading each sample and projecting it onto the picture plane. Surface shading calculations are performed at every voxel with local gradient vectors serving as surface normals. In a separate step, surface classification operators are applied to obtain a partial opacity for every voxel. Operators that detect isovalue contour surfaces and region boundary surfaces are presented. Independence of shading and classification calculations insures an undistorted visualization of 3-D shape. Non-binary classification operators insure that small or poorly defined features are not lost. The resulting colors and opacities are composited from back to front along viewing rays to form an image. The technique is simple and fast, yet displays surfaces exhibiting smooth silhouettes and few other aliasing artifacts. The use of selective blurring and super-sampling to further improve image quality is also described. Examples from two applications are given: molecular graphics and medical imaging.
690|Footprint evaluation for volume rendering|This paper presents a forward mapping rendering algo-rithm to display regular volumetric grids that may not have the same spacings in the three grid directions. It takes advantage of the fact that convolution can be thought of as distributing energy from input samples into space. The renderer calculates an image plane footprint for each data sample and uses the footprint to spread the sample&#039;s energy onto the image plane. A result of the technique is that the forward mapping algorithm can support perspective without excessive cost, and support adaptive resampling of the three-dimensional data set during image generation.
691|Volume Rendering|A technique for rendering images Of volumes containing mixtures of materials is presented. The shading model allows both the interior of a material and the boundary between materials to be colored. Image projection is performed by simulating the absorption of light along the ray path to the eye. The algorithms used are designed to avoid artifacts caused by aliasing and quantization and can be efficiently implemented on an image computer. Images from a variety of applications are shown.
692|Efficient ray tracing of volume data|Volume rendering is a technique for visualizing sampled scalar or vector fields of three spatial dimensions without fitting geometric primitives to the data. A subset of these techniques generates images by computing 2-D projections of a colored semitransparent volume, where the color and opacity at each point are derived from the data using local operators. Since all voxels participate in the generation of each image, rendering time grows linearly with the size of the dataset. This paper presents a front-to-back image-order volume-rendering algorithm and discusses two techniques for improving its performance. The first technique employs a pyramid of binary volumes to encode spatial coherence present in the data, and the second technique uses an opacity threshold to adaptively terminate ray tracing. Although the actual time saved depends on the data, speedups of an order of magnitude have been observed for datasets of useful size and complexity. Examples from two applications are given: medical imaging and molecular graphics.
693|Radiosity and Realistic Image Synthesis|this paper, such as the global distribution of radiative energy in the tree crowns, which affects the amount of light reaching the leaves and the local temperature of plant organs. The presented framework itself is also open to further research. To begin, the precise functional specification of the environment, implied by the design of the modeling framework, is suitable for a formal analysis of algorithms that capture various environmental processes. This analysis may highlight tradeoffs between time, memory, and communication complexity, and lead to programs matching the needs of the model to available system resources in an optimal manner. A deeper understanding of the spectrum of processes taking place in the environment may lead to the design of a mini-language for environment specification. Analogous to the language of L-systems for plant specification, this mini-language would simplify the modeling of various environments, relieving the modeler from the burden of low-level programming in a general-purpose language. Fleischer and Barr&#039;s work on the specification of environments supporting collisions and reaction-diffusion processes [20] is an inspiring step in this direction. Complexity issues are not limited to the environment, but also arise in plant models. They become particularly relevant as the scope of modeling increases from individual plants to groups of plants and, eventually, entire plant communities. This raises the problem of selecting the proper level of abstraction for designing plant models, including careful selection of physiological processes incorporated into the model and the spatial resolution of the resulting structures. The complexity of the modeling task can be also addressed at the level of system design, by assigning various components o...
694|Octrees for faster isosurface generation| The large size of many volume data sets often prevents visualization algorithms from providing interactive rendering. The use of hierarchical data structures can ameliorate this problem by storing summary information to prevent useless exploration of regions of little or no current interest within the volume. This paper discusses research into the use of the octree hierarchical data structure when the regions of current interest can vary during the application, and are not known a priori. Octrees are well suited to the six-sided cell structure of many volumes. A new space-efficient design is introduced for octree representations of volumes whose resolutions are not conveniently a power of two; octrees following this design are called branch-on-need octrees (BONOs). Also, a caching method is described that essentially passes information between octree neighbors whose visitation times may be quite different, then discards it when its useful life is over. Using the application of octrees to isosurface generation as a focus, space and time comparisons for octree-based versus more traditional &#034;marching&#034; methods are presented.
696|Ray Tracing Volume Densities|This paper presents new algorithms to trace objects represented by densities within a volume grid, e.g. clouds, fog, flames, dust, particle systems. We develop the light scattering equations, discuss previous methods of solu-tion, and present a new approximate solution to the full three-dimensional radiative scattering problem suitable for use in computer graphics. Additionally we review dynamical models for clouds used to make an animated movie.
697|A Polygonal Approximation to Direct Scalar Volume Rendering|One method of directly rendering a three-dimensional volume of scalar data is to project each cell in a volume onto the screen. Rasterizing a volume cell is more complex than rasterizing a polygon. A method is presented that approximates tetrahedral volume cells with hardware renderable transparent triangles. This method produces results which are visually similar to more exact methods for scalar volume rendering, but is faster and has smaller memory requirements. The method is best suited for display of smoothlychanging data.  CR Categories and Subject Descriptors: I.3.0 [Computer Graphics]: General; I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling.  Additional Key Words and Phrases: Volume rendering, scientific visualization. 1 Introduction  Display of three-dimensional scalar volumes has recently become an active area of research. A scalar volume is described by some function f(x; y; z) defined over some region R of three-dimensional space. In many scientific ap...
698|Survey Of Texture Mapping|This paper appeared in IEEE Computer Graphics and Applications, Nov. 1986, pp. 56-67. An earlier version of thi aper appeared in Graphics Interface &#039;86, May 1986, pp. 207-212. This postscript version is missing all of the paste-up -
699|A language for shading and lighting calculations|A shading language provides a means to extend the shading and lighting formulae used by a rendering system. This paper discusses the design of a new shading language based on previous work of Cook and Perlin. This language has various types of shaders for light sources and surface reflectances, point and color data types, control flow constructs that support the casting of outgoing and the integration of incident light, a clearly specified interface to the rendering system using global state variables, and a host of useful built-in functions. The design issues and their impact on the implementation are also discussed. CR Categories: 1.3.3 [Computer Graphics] Picture/Image Generation- Display algorithms; 1.3.5 [Computer Graphics]
700|Fourier volume rendering|In computer graphics we have traditionally rendered images of data sets specified spatially, Here, we present a volume rendering technique that operates on a frequency domain representation of the data set and that efficiently generates line integral projections of the spatial data it represents, The motivation for this approach is that the Fourier Projection-Slice Theorem allows us to compute 2-D projections of 3-D data seta using only a 2-D slice of the data in the frequency domain. In general, these “X-ray-like ” images can be rendered at a significantly lower computational cost than images generated by current volume rendering techniques, Additionally, assurances of image accuracy can he made.
701|Fast Algorithms for Volume Ray Tracing|We examine various simple algorithms that exploit homogeneity and accumulated opacity for tracing rays through shaded volumes. Most of these methods have error criteria which allow them to trade quality for speed. The time vs. quality tradeoff for these adaptive methods is compared to fixed step multiresolution methods. These methods are also useful for general light transport in volumes. 1 Introduction  We are interested in speeding volume ray tracing computations. We concentrate on the one dimensional problem of tracing a single ray, or computing the intensity at a point from a single direction. In addition to being the kernel of a simple volume ray tracer, this computation can be used to generate shadow volumes and as an element in more general light transport problems. Our data structures will be view independent to speed the production of animations of preshaded volumes and interactive viewing. In [11] Levoy introduced two key concepts which we will be expanding on: presence accel...
702|MemSpy: Analyzing Memory System Bottlenecks in Programs|To cope with the increasing difference between processor and main memory speeds, modern computer systems use deep memory hierarchies. In the presence of such hierarchies, the performance attained by an application is largely determined by its memory reference behavior--- if most references hit in the cache, the performance is significantly higher than if most references have to go to main memory. Frequently, it is possible for the programmer to restructure the data or code to achieve better memory reference behavior. Unfortunately, most existing performance debugging tools do not assist the programmer in this component of the overall performance tuning task. This paper describes MemSpy, a prototype tool that helps programmers identify and fix memory bottlenecks in both sequential and parallel programs. A key aspect of MemSpy is that it introduces the notion of data oriented, in addition to code oriented, performance tuning. Thus, for both source level code objects and data objects, Mem...
703|The DASH Prototype: Logic Overhead and Performance|Abstract-The fundamental premise behind the DASH project is that it is feasible to build large-scale shared-memory multi-processors with hardware cache coherence. While paper studies and software simulators are useful for understanding many high-level design tradeoffs, prototypes are essential to ensure that no critical details are overlooked. A prototype provides convincing evidence of the feasibility of the design, allows one to accurately estimate both the hardware and the complexity cost of various features, and provides a platform for studying real workloads. A 48-processor prototype of the DASH multiprocessor is now operational. In this paper, we first examine the hardware overhead of directory-based cache coherence in the prototype. The data show that the overhead is only about M-15%, which appears to be a small cost for the ease of programming offered by coherent caches and the potential for higher performance. We then discuss the performance of the system and show the speedups obtained by a variety of parallel applications running on the prototype. Using a sophisticated hardware performance monitor, we also characterize the effectiveness of coherent caches and the relationship between an application’s reference behavior and its speedup. Finally, we present an evaluation of the optimizations incorporated in the DASH protocol in terms of their effectiveness on parallel applications and on atomic tests that stress the memory system.’ Index Terms- Directory-based cache coherence, implementa-tion cost, multiprocessor, parallel architecture, performance anal-
704|Feature-Based Volume Metamorphosis|Image metamorphosis, or image morphing, is a popular technique for creating a smooth transition between two images. For synthetic images, transforming and rendering the underlying three-dimensional (3D) models has a number of advantages over morphing between two pre-rendered images. In this paper we consider 3D metamorphosis applied to volume-based representations of objects. We discuss the issues which arise in volume morphing and present a method for creating morphs. Our morphing method has two components: first a warping of the two input volumes, then a blending of the resulting warped volumes. The warping component, an extension of Beier and Neely&#039;s image warping technique to 3D, is feature-based and allows fine user control, thus ensuring realistic looking intermediate objects. In addition, our warping method is amenable to an efficient approximation which gives a 50 times speedup and is computable to arbitrary accuracy. Also, our technique corrects the ghosting problem present in...
705|Volume Rendering on Scalable Shared-Memory MIMD Architectures|Volume rendering is a useful visualization technique for understanding the large amounts of data generated in a variety of scientific disciplines. Routine use of this technique is currently limited by its computational expense. We have designed a parallel volume rendering algorithm for MIMD architectures based on ray tracing and a novel task queue image partitioning technique. The combination of ray tracing and MIMD architectures allows us to employ algorithmic optimizations such as hierarchical opacity enumeration, early ray termination, and adaptive image sampling. The use of task queue image partitioning makes these optimizations efficient in a parallel framework. We have implemented our algorithm on the Stanford DASH Multiprocessor, a scalable shared-memory MIMD machine. Its single address-space and coherent caches provide programming ease and good performance for our algorithm. With only a few days of programming effort, we have obtained nearly linear speedups and near real-time frame update rates on a 48 processor machine. Since DASH is constructed from Silicon Graphics multiprocessors, our code runs on any Silicon Graphics workstation without modification.
706|Template-Based Volume Viewing|We present an efficient three-phase algorithm for volume viewing that is based on exploit- - t ing coherency between rays in parallel projection. The algorithm starts by building a ray emplate and determining a special plane for projection -- the base-plane. Parallel rays are cast t into the volume from within the projected region of the volume on the base-plane, by repeating he sequence of steps specified in the ray-template. We carefully choose the type of line to be s employed and the way the template is being placed on the base-plane in order to assure uniform ampling of the volume by the discrete rays. We conclude by describing an optimized software K  implementation of our algorithm and reporting its performance. eywords: volume rendering, ray casting, template, parallel projection 1. Introduction  Volume visualization is the process of converting complex volume data to a format that is p amenable to human understanding while maintaining the integrity and accuracy of the data. Th...
707|Volume Rendering by Adaptive Refinement|Volume rendering is a technique for visualizing sampled scalar functions of three spatial dimensions by computing 2D projections of a colored semi-transparent gel. This paper presents a volume rendering algorithm in which image quality is adaptively refined over time. An initial image is generated by casting a small number of rays into the data, less than one ray per pixel, and interpolating between the resulting colors. Subsequent images are generated by alternately casting more rays and interpolating. The usefulness of these rays is maximized by distributing them according to measures of local image complexity. Examples from two applications are given: molecular graphics and medical imaging. Key words: Volume rendering, voxel, adaptive refinement, adaptive sampling, ray tracing. 1. Introduction In this paper, we address the problem of visualizing sampled scalar functions of three spatial dimensions, henceforth referred to as volume data. We focus on a relatively new visualization tec...
708|Volume Rendering using the Fourier Projection-Slice Theorem|The Fourier projection-slice theorem states that the inverse transform of a slice extracted from the frequency domain representation of a volume yields a projection of the volume in a direction perpendicular to the slice. This theorem allows the generation of attenuation-only renderings of volume data in O (N  2  log N) time for a volume of size N  3  . In this paper, we show how more realistic renderings can be generated using a class of shading models whose terms are Fourier projections. Models are derived for rendering depth cueing by linear attenuation of variable energy emitters and for rendering directional shading by Lambertian reflection with hemispherical illumination. While the resulting images do not exhibit the occlusion that is characteristic of conventional volume rendering, they provide sufficient depth and shape cues to give a strong illusion that occlusion exists. Keywords: Volume rendering, Fourier projections, Shading models, Scientific visualization, Medical imaging...
709|A Data Distributed, Parallel Algorithm for Ray-Traced Volume Rendering|This paper presents a divide-and-conquer ray-traced volume rendering algorithm and a parallel image compositing method, along with their implementation and performance on the Connection Machine CM-5, and networked workstations. This algorithm distributes both the data and the computations to individual processing units to achieve fast, high-quality rendering of high-resolution data. The volume data, once distributed, is left intact. The processing nodes perform local raytracing of their subvolume concurrently. No communication between processing units is needed during this locally ray-tracing process. A subimage is generated by each processing unit and the #nal image is obtained by compositing subimages in the proper order, which can be determined a priori. Test results on both the CM-5 and a group of networked workstations demonstrate the practicality of our rendering algorithm and compositing method.  y  This researchwas supported in part by the National Aeronautics and Space Administration under NASA contract NAS1-19480 while the author was in residence at the Institute for Computer Application in Science and Engineering #ICASE#, NASA Langley Research Center, Hampton, VA 23681-0001.  i  1 
710|Parallel Volume Visualization on a Hypercube Architecture|A parallel solution to the visualisation of high resolution vol- ume data is presented. Based on the ray tracing (RT) visu- alization technique, the system works on a distributed memory MIMD architecture. A hybrid strategy to ray tracing parallelization is applied, using ray dataflow within an image partition approach. This strategy allows the flexible and effective management of huge dataset on architectures with limited local memory. The dataset is distributed over the nodes using a slice-partitioning technique. The simple data partition chosen implies a straighforward communications pattern of the visualization processes and this improves both software design and eJciency, while providing deadlock prevention. The partitioning technique used and the network interconnection topology allow for the efficient implementation of a statical load balancing technique through pre-rendering of a low resolution image. Details related to the practical issues involved in the parallelization of volumetric RT are discussed, with particular reference to deadlock and termi- nation issues.
711|Parallel Volume Rendering and Data Coherence|The two key issues in implementing a parallel ray-casting volume renderer are the work distribution and the data distribution. We have implemented such a renderer on the Fujitsu AP1000 using an adaptive image-space subdivision algorithm based on the worker-farm paradigm for the work distribution, and a distributed virtual memory, implemented in software, to provide the data distribution. Measurements show that this scheme works efficiently and effectively utilizes the data coherence that is inherent in volume data. Categories and Subject Descriptors: C.1.2 [Proces- sor Architectures]: Multiple Data Stream Architectures -- multiple-instruction-stream, multiple-data-stream (MIMD); I.3.1 [Computer Graphics]: Hardware Architecture -- parallel processing; I.3.7 [Computer Graphics]: ThreeDimensional Graphics and Realism -- ray tracing Key Words: Visualization, volume rendering, worker farm, image space, distributed virtual memory. 1 Introduction Volume rendering using ray-casting is a...
712|Cube-3: A Real-Time Architecture for High-Resolution Volume Visualization|This paper describes a high-performance special-purpose system, Cube-3, for displaying and manipulating high-resolution volumetric datasets in real-time. A primary goal of Cube-3 is to render 512³, 16-bit per voxel, datasets at about 30 frames per second. Cube-3 implements a ray-casting algorithm in a highly-parallel and pipelined architecture, using a 3D skewed volume memory, a modular fast bus, 2D skewed buffers, 3D interpolation and shading units, and a ray projection cone. Cube-3 will allow users to interactively visualize and investigate in real-time static (3D) and dynamic (4D) high-resolution volumetric datasets.
713|Transfer Equations in Global Illumination|The purpose of these notes is to describe some of the physical and mathematical properties of the equations occurring in global illumination. We first examine the physical assumptions that make the particle model of light an appropriate paradigm for computer graphics and then derive a balance equation for photons. In doing this we establish connections with the field of radiative transfer and its more abstract counterpart, transport theory. The resulting balance equation, known as the equation of transfer, accounts for large-scale interaction of light with participating media as well as complex reflecting surfaces. Under various simplifying assumptions the equation of transfer reduces to more conventional equations encountered in global illumination. 1 Introduction  Global illumination connotes a physically-based simulation of light appropriate for synthetic image generation. The task of such a simulation is to model the interplay of light among large-scale objects of an environment in...
714|A Lipschitz Method for Accelerated Volume Rendering|Interpolating discrete volume data into a continuous form adapts implicit surface techniques for rendering volumetric iso-surfaces. One such algorithm uses the Lipschitz condition to create an octree representation that accelerates volume rendering. Furthermore, only one preprocessing step is needed to create the Lipschitzoctree representation that accelerates rendering of isosurfaces for any threshold value.
715|Data Shaders|The process of visualizing a scientific data set requires an extensive knowledge of the domain in which the data set is created. Because an in-depth knowledge of all scientific domains is not available to the creator of visualization software, a flexible and extensible visualization system is essential in providing a productive tool to the scientist. This paper presents a shading language, based on the RenderMan shading language, that extends the shading model used to render volume data sets. Data shaders, written in this shading language, give the users of a volume rendering system a means of specifying how a volume data set is to be rendered. This flexibility is useful both as a visualization tool in the scientific community and as a research tool in the visualization community. 1 Introduction  As science is a diverse and far reaching topic, scientific visualization must be prepared to deal with diverse requirements when scientific data sets are examined, explored, and analyzed. In m...
716|Mining Frequent Patterns  without Candidate Generation: A Frequent-Pattern Tree Approach|Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist a large number of patterns and/or long patterns. In this study,  we propose a novel
frequent-pattern tree
(FP-tree) structure, which is an extended prefix-tree
structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-
based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth.
Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a condensed,
smaller data structure, FP-tree which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts
a pattern-fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a
partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for
mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance
study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns,
and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported
new frequent-pattern mining methods
717|Fast Algorithms for Mining Association Rules|We consider the problem of discovering association rules between items in a large database of sales transactions. We present two new algorithms for solving this problem that are fundamentally different from the known algorithms. Empirical evaluation shows that these algorithms outperform the known algorithms by factors ranging from three for small problems to more than an order of magnitude for large problems. We also show how the best features of the two proposed algorithms can be combined into a hybrid algorithm, called AprioriHybrid. Scale-up experiments show that AprioriHybrid scales linearly with the number of transactions. AprioriHybrid also has excellent scale-up properties with respect to the transaction size and the number of items in the database. 
718|Mining Sequential Patterns|We are given a large database of customer transactions, where each transaction consists of customer-id, transaction time, and the items bought in the transaction. We introduce the problem of mining sequential patterns over such databases. We present three algorithms to solve this problem, and empirically evaluate their performance using synthetic data. Two of the proposed algorithms, AprioriSome and AprioriAll, have comparable performance, albeit AprioriSome performs a little better when the minimum number of customers that must support a sequential pattern is low. Scale-up experiments show that both AprioriSome and AprioriAll scale linearly with the number of customer transactions. They also have excellent scale-up properties with respect to the number of transactions per customer and the number of items in a transaction.  
719|Mining Sequential Patterns: Generalizations and Performance Improvements|Abstract. The problem of mining sequential patterns was recently introduced in [3]. We are given a database of sequences, where each sequence is a list of transactions ordered by transaction-time, and each transaction is a set of items. The problem is to discover all sequential patterns with a user-speci ed minimum support, where the support of a pattern is the number of data-sequences that contain the pattern. An example of a sequential pattern is \5 % of customers bought `Foundation&#039; and `Ringworld &#039; in one transaction, followed by `Second Foundation &#039; in a later transaction&#034;. We generalize the problem as follows. First, we add time constraints that specify a minimum and/or maximum time period between adjacent elements in a pattern. Second, we relax the restriction that the items in an element of a sequential pattern must come from the same transaction, instead allowing the items to be present in a set of transactions whose transaction-times are within a user-speci ed time window. Third, given a user-de ned taxonomy (is-a hierarchy) on items, we allow sequential patterns to include items across all levels of the taxonomy. We present GSP, a new algorithm that discovers these generalized sequential patterns. Empirical evaluation using synthetic and real-life data indicates that GSP is much faster than the AprioriAll algorithm presented in [3]. GSP scales linearly with the number of data-sequences, and has very good scale-up properties with respect to the average datasequence size. 1
720|Efficiently mining long patterns from databases|We present a pattern-mining algorithm that scales roughly linearly in the number of maximal patterns embedded in a database irrespective of the length of the longest pattern. In comparison, previous algorithms based on Apriori scale exponentially with longest pattern length. Experiments on real data show that when the patterns are long, our algorithm is more efficient by an order of magnimaximal frequent itemset, Max-Miner’s output implicitly and concisely represents all frequent itemsets. Max-Miner is shown to result in two or more orders of magnitude in performance improvements over Apriori on some data-sets. On other data-sets where the patterns are not so long, the gains are more modest. In practice, Max-Miner is demonstrated to run in time that is roughly linear in the number of maximal frequent itemsets and the size of the database, irrespective of the size of the longest frequent itemset. tude or more. 1.
721|An efficient algorithm for mining association rules in large databases|Mining for a.ssociation rules between items in a large database of sales transactions has been described as an important database mining problem. In this paper we present an effi-cient algorithm for mining association rules that is fundamentally different from known al-gorithms. Compared to previous algorithms, our algorithm not only reduces the I/O over-head significantly but also has lower CPU overhead for most cases. We have performed extensive experiments and compared the per-formance of our algorithm with one of the best existing algorithms. It was found that for large databases, the CPU overhead was re-duced by as much as a factor of four and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases. 1
722|Discovering Frequent Closed Itemsets for Association Rules|In this paper, we address the problem of finding frequent itemsets in a database. Using the closed itemset lattice framework, we show that this problem can be reduced to the problem of finding frequent closed itemsets. Based on this statement, we can construct efficient data mining algorithms by limiting the search space to the closed itemset lattice rather than the subset lattice. Moreover, we show that the set of all frequent closed itemsets suffices to determine a reduced set of association rules, thus addressing another important data mining problem: limiting the number of rules produced without information loss. We propose a new algorithm, called A-Close, using a closure mechanism to find frequent closed itemsets. We realized experiments to compare our approach to the commonly used frequent itemset search approach. Those experiments showed that our approach is very valuable for dense and/or correlated data that represent an important part of existing databases.
723|Discovery of frequent episodes in event sequences|Abstract. Sequences of events describing the behavior and actions of users or systems can be collected in several domains. An episode is a collection of events that occur relatively close to each other in a given partial order. We consider the problem of discovering frequently occurring episodes in a sequence. Once such episodes are known, one can produce rules for describing or predicting the behavior of the sequence. We give efficient algorithms for the discovery of all frequent episodes from a given class of episodes, and present detailed experimental results. The methods are in use in telecommunication alarm management. Keywords: event sequences, frequent episodes, sequence analysis 1.
724|Efficient Mining of Emerging Patterns: Discovering Trends and Differences|We introduce a new kind of patterns, called emerging patterns (EPs), for knowledge discovery from databases. EPs are defined as itemsets whose supports increase significantly  from one dataset to another. EPs can capture emerging trends in timestamped databases, or useful contrasts between data classes. EPs have been proven useful: we have used them to build very powerful classifiers, which are more accurate than C4.5 and CBA, for many datasets. We believe that EPs with low to medium support, such as 1%-- 20%, can give useful new insights and guidance to experts, in even &#034;well understood&#034; applications.  The efficient mining of EPs is a challenging problem, since (i) the Apriori property no longer holds for EPs, and (ii) there are usually too many candidates for high dimensional databases or for small support thresholds such as 0.5%. Naive algorithms are too costly. To solve this problem, (a) we promote the description of large collections of itemsets using their concise borders (the pa...
725|PrefixSpan: Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth|Sequential pattern mining is an important data mining problem with broad applications. It is challenging since one may need to examine a combinatorially explosive number of possible subsequence patterns. Most of the previously developed sequential pattern mining methods follow the methodology of    which may substantially reduce the number of combinations to be examined. However,   still encounters problems when a sequence database is large and/or when sequential patterns to be mined are numerous and/or long.
726|CHARM: An efficient algorithm for closed itemset mining|The set of frequent closed itemsets uniquely determines the exact frequency of all itemsets, yet it can be orders of magnitude smaller than the set of all frequent itemsets. In this paper we present CHARM, an efficient algorithm for mining all frequent closed itemsets. It enumerates closed sets using a dual itemset-tidset search tree, using an efficient hybrid search that skips many levels. It also uses a technique called diffsets to reduce the memory footprint of intermediate computations. Finally it uses a fast hash-based approach to remove any “non-closed” sets found during computation. An extensive experimental evaluation on a number of real and synthetic databases shows that CHARM significantly outperforms previous methods. It is also linearly scalable in the number of transactions.
727|Exploratory Mining and Pruning Optimizations of Constrained Associations Rules|From the standpoint of supporting human-centered discovery of knowledge, the present-day model of mining association rules suffers from the following serious shortcom- ings: (i) lack of user exploration and control, (ii) lack of focus, and (iii) rigid notion of relationships. In effect, this model functions as a black-box, admitting little user interaction in between. We propose, in this paper, an architecture that opens up the black-box, and supports constraintbased, human-centered exploratory mining of associations. The foundation of this architecture is a rich set of con- straint constructs, including domain, class, and $QL-style aggregate constraints, which enable users to clearly specify what associations are to be mined. We propose constrained association queries as a means of specifying the constraints to be satisfied by the antecedent and consequent of a mined association.
728|CLOSET: An Efficient Algorithm for Mining Frequent Closed Itemsets|Association mining may often derive an undesirably large set of frequent itemsets and association rules. Recent studies have proposed an interesting alternative: mining frequent closed itemsets and their corresponding rules, which has the same power as association mining but substantially reduces the number of rules to be presented. In this paper, we propose an efficient algorithm, CLOSET, for mining closed itemsets, with the development of three techniques: (1) applying a compressed, frequent pattern tree FP-tree structure for mining closed itemsets without candidate generation, (2) developing a single prefix path compression technique to identify frequent closed itemsets quickly, and (3) exploring a partition-based projection mechanism for scalable mining in large databases. Our performance study shows that CLOSET is efficient and scalable over large databases, and is faster than the previously proposed methods. 1 Introduction It has been well recognized that frequent pattern minin...
729|Mining Association Rules with Item Constraints|The problem of discovering association rules has received considerable research attention and several fast algorithms for mining association rules have been developed. In practice, users are often interested in a subset of association rules. For example, they may only want rules that contain a specific item or rules that contain children of a specific item in a hierarchy. While such constraints can be applied as a postprocessing step, integrating them into the mining algorithm can dramatically reduce the execution time. We consider the problem of integrating constraints that are boolean expressions over the presence or absence of items into the association discovery algorithm. We present three integrated algorithms for mining association rules with item constraints and discuss their tradeoffs. 1. Introduction The problem of discovering association rules was introduced in (Agrawal, Imielinski, &amp; Swami 1993). Given a set of transactions, where each transaction is a set of literals (call...
730|An effective hash-based algorithm for mining association rules|In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an effective hash-based algorithm for the candidate set generation. Explicitly, the number of candidate 2-itemsets generated by the proposed algorithm is, in orders of magnitude, smaller than that by previous methods, thus resolving the performance bottleneck. Note that the generation of smaller candidate sets enables us to effectively trim the transaction database size at a much earlier stage of the iterations, thereby reducing the computational cost for later iterations significantly. Extensive simulation study is conducted to evaluate performance of the proposed algorithm. 1
731|Efficient Algorithms for Discovering Association Rules|Association rules are statements of the form &#034;for 90 % of the rows of the relation, if the row has value 1 in the columns in set W , then it has 1 also in column B&#034;. Agrawal, Imielinski, and Swami introduced the problem of mining association rules from large collections of data, and gave a method based on successive passes over the database. We give an improved algorithm for the problem. The method is based on careful combinatorial analysis of the information obtained in previous passes; this makes it possible to eliminate unnecessary candidate rules. Experiments on a university course enrollment database indicate that the method outperforms the previous one by a factor of 5. We also show that sampling is in general a very efficient way of finding such rules. Keywords: association rules, covering sets, algorithms, sampling. 1 Introduction Data mining (database mining, knowledge discovery in databases) has recently been recognized as a promising new field in the intersection of databa...
732|A tree projection algorithm for generation of frequent itemsets|In this paper we propose algorithms for generation of frequent itemsets by successive construction of the nodes of a lexicographic tree of itemsets. We discuss di erent strategies in generation and traversal of the lexicographic tree such as breadth- rst search, depth- rst search or a combination of the two. These techniques provide di erent trade-o s in terms of the I/O, memory and computational time requirements. We use the hierarchical structure of the lexicographic tree to successively project transactions at each node of the lexicographic tree, and use matrix counting on this reduced set of transactions for nding frequent itemsets. We tested our algorithm on both real and synthetic data. We provide an implementation of the tree projection method which is up to one order of magnitude faster than other recent techniques in the literature. The algorithm has a well structured data access pattern which provides data locality and reuse of data for multiple levels of the cache. We also discuss methods for parallelization of the
733|Efficient mining of partial periodic patterns in time series database|Partial periodicity search, i.e., search for partial periodic patterns in time-series databases, is an interesting data mining problem. Previous studies on periodicity search mainly consider finding full periodic patterns, where every point in time contributes (precisely or approximately) to the periodicity. However, partial periodicity is very common in practice since it is more likely that only some of the time episodes may exhibit periodic patterns. We present several algorithms for efficient mining of partial periodic patterns, by exploring some interesting properties related to partial periodicity, such as the Apriori property and the max-subpattern hit set property, and by shared mining of multiple periods. The max-subpattern hit set property is a vital new property which allows us to derive the counts of all frequent patterns from a relatively small subset of patterns existing in the time series. We show that mining partial periodicity needs only two scans over the time series database, even for mining multiple periods. The performance study shows our proposed methods are very efficient in mining long periodic patterns.
734|Clustering association rules|We consider the problem of clustering two-dimensional as-sociation rules in large databases. We present a geometric-based algorithm, BitOp, for performing the clustering, em-bedded within an association rule clustering system, ARCS. Association rule clustering is useful when the user desires to segment the data. We measure the quality of the segment-ation generated by ARCS using the Minimum Description Length (MDL) principle of encoding the clusters on several databases including noise and errors. Scale-up experiments show that ARCS, using the BitOp algorithm, scales linearly with the amount of data. 1
735|Integrating association rule mining with relational database systems: Alternatives and implications |Abstract. Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR). Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache option is superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache and the SQL-OR approaches incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache. The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis of qualitative factors like automatic parallelization, development ease, portability and inter-operability. As a byproduct of this study, we identify some primitives for native support in database systems for decision-support applications. Keywords: mining system architecture, association rule mining, database mining, mining algorithms in SQL
736|Mining Frequent Itemsets with Convertible Constraints|Recent work has highlighted the importance of the constraint-based mining paradigm in the context of frequent itemsets, associations, correlations, sequential patterns, and many other interesting patterns in large databases. In this paper, we study constraints which cannot be handled with existing theory and techniques. For example,, ,  ( can contain items of arbitrary values) &#034;!$ # %&#039;&amp;) ( , are customarily regarded as “tough ” constraints in that they cannot be pushed inside an algorithm such as Apriori. We develop a notion of convertible constraints and systematically analyze, classify, and characterize this class. We also develop techniques which enable them to be readily pushed deep inside the recently developed FP-growth algorithm for frequent itemset mining. Results from our detailed experiments show the effectiveness of the techniques developed. 1.
737|Metarule-Guided Mining of Multi-Dimensional Association Rules|In this paper, we employ a novel approach to  metarule-guided, multi-dimensional association  rule mining which explores a data cube structure.
738|H-Mine: Hyper-Structure Mining of Frequent Patterns in Large Databases|Methods for efficient mining of frequent patterns have been studied extensively by many researchers. However, the previously proposed methods still encounter some performance bottlenecks when mining databases with different data characteristics, such as dense vs. sparse, long vs. short patterns, memory-based vs. disk-based, etc.
739|Efficient Mining of Constrained Correlated Sets|In this paper, we study the problem of efficiently computing correlated itemsets satisfying given constraints. We call them valid correlated itemsets. It turns out constraints can have subtle interactions with correlated itemsets, depending on their underlying properties. We show that in general the set of minimal valid correlated itemsets does not coincide with that of minimal correlated itemsets that are valid, and characterize classes of constraints for which these sets coincide. We delineate the meaning of these two spaces and give algorithms for computing them. We also give an analytical evaluation of their performance and validate our analysis with a detailed experimental evaluation.  
740|A New Method for Solving Hard Satisfiability Problems|We introduce a greedy local search procedure called GSAT for solving propositional satisfiability problems. Our experiments show that this procedure can be used to solve hard, randomly generated problems that are an order of magnitude larger than those that can be handled by more traditional approaches such as the Davis-Putnam procedure or resolution. We also show that GSAT can solve structured satisfiability problems quickly. In particular, we solve encodings of graph coloring problems, N-queens, and Boolean induction. General application strategies and limitations of the approach are also discussed.  GSAT is best viewed as a model-finding procedure. Its good performance suggests that it may be advantageous to reformulate reasoning tasks that have traditionally been viewed as theorem-proving problems as model-finding tasks.  
741|The complexity of theorem-proving procedures|It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be “reduced” to the problem of determining whether a given propositional formula is a tautology. Here “reduced ” means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed. Throughout this paper, a set of strings 1 means a set of strings on some fixed, large, finite alphabet S. This alphabet is large enough to include symbols for all sets described here. All Turing machines are deterministic recognition devices, unless the contrary is explicitly stated.
742|Model Checking vs. Theorem Proving: A Manifesto| We argue that rather than representing an agent&#039;s knowledge as a collection of formulas, and then doing theorem proving to see if a given formula follows from an agent&#039;s knowledge base, it may be more useful to represent this knowledge by a semantic model, and then do model checking to see if the given formula is true in that model. We discuss how to construct a model that represents an agent&#039;s knowledge in a number of different contexts, and then consider how to approach the model-checking problem.
743|A Continuous Approach to Inductive Inference|In this paper we describe an interior point mathematical programming approach to inductive inference. We list several versions of this problem and study in detail the formulation based on hidden Boolean logic. We consider the problem of identifying a hidden Boolean function  F : f0; 1g  n  ! f0; 1g using outputs obtained by applying a limited number of random inputs to the hidden function. Given this input-output sample, we give a method to synthesize a Boolean function that describes the sample. We pose the Boolean Function Synthesis Problem as a particular type of Satisfiability Problem. The Satisfiability Problem is translated into an integer programming feasibility problem, that is solved with an interior point algorithm for integer programming. A similar integer programming implementation has been used in a previous study to solve randomly generated instances of the Satisfiability Problem. In this paper we introduce a new variant of this algorithm, where the Riemannian metric used...
744|The Complexity of Automated Reasoning|This thesis explores the relative complexity of proofs produced by the automatic theorem proving procedures of analytic tableaux, linear resolution, the connection method, tree resolution and the Davis-Putnam procedure. It is shown that tree resolution simulates the improved tableau procedure and that SL-resolution and the connection method are equivalent to restrictions of the improved tableau method. The theorem by Tseitin that the Davis-Putnam Procedure cannot be simulated by tree resolution is given an explicit and simplified proof. The hard examples for tree resolution are contradictions constructed from simple Tseitin graphs.
745|Comparison of Multiobjective Evolutionary Algorithms: Empirical Results|In this paper, we provide a systematic comparison of various evolutionary approaches to multiobjective optimization using six carefully chosen test functions. Each test function involves a particular feature that is known to cause difficulty in the evolutionary optimization process, mainly in converging to the Pareto-optimal front (e.g., multimodality and deception). By investigating these different problem features separately, it is possible to predict the kind of problems to which a certain technique is or is not well suited. However, in contrast to what was suspected beforehand, the experimental results indicate a hierarchy of the algorithms under consideration. Furthermore, the emerging effects are evidence that the suggested test functions provide sufficient complexity to compare multiobjective optimizers. Finally, elitism is shown to be an important factor for improving evolutionary multiobjective search.
746|A Fast and Elitist Multi-Objective Genetic Algorithm: NSGA-II|Multi-objective evolutionary algorithms which use non-dominated sorting and sharing have been mainly criticized for their (i) O(MN computational complexity (where M is the number of objectives and N is the population size), (ii) non-elitism approach, and (iii) the need for specifying a sharing parameter. In this paper, we suggest a non-dominated sorting based multi-objective evolutionary algorithm (we called it the Non-dominated Sorting GA-II or NSGA-II) which alleviates all the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN ) computational complexity is presented. Second, a selection operator is presented which creates a mating pool by combining the parent and child populations and selecting the best (with respect to fitness and spread) N solutions. Simulation results on a number of difficult test problems show that the proposed NSGA-II, in most problems, is able to find much better spread of solutions and better convergence near the true Pareto-optimal front compared to PAES and SPEA - two other elitist multi-objective EAs which pay special attention towards creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems eciently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint non-linear problem, are compared with another constrained multi-objective optimizer and much better performance of NSGA-II is observed. Because of NSGA-II&#039;s low computational requirements, elitist approach, parameter-less niching approach, and simple constraint-handling strategy, NSGA-II should find increasing applications in the coming years.
747|A Fast Elitist Non-Dominated Sorting Genetic Algorithm for Multi-Objective Optimization: NSGA-II|Multi-objective evolutionary algorithms which use non-dominated sorting  and sharing have been mainly criticized for their  (i)  -4  computational  complexity (where    is the number of objectives and    is the population size),  (ii) non-elitism approach, and (iii) the need for specifying a sharing parameter. In  this paper, we suggest a non-dominated sorting based multi-objective evolutionary  algorithm (we called it the Non-dominated Sorting GA-II or NSGA-II) which  alleviates all the above three difficulties. Specifically, a fast non-dominated sorting  approach with    computational complexity is presented. Second, a  selection operator is presented which creates a mating pool by combining the  parent and child populations and selecting the best (with respect to fitness and  spread)    solutions. Simulation results on five difficult test problems show that  the proposed NSGA-II is able to find much better spread of solutions in all problems  compared to PAES---another elitist multi-objective EA which pays special  attention towards creating a diverse Pareto-optimal front. Because of NSGA-II&#039;s  low computational requirements, elitist approach, and parameter-less sharing approach,  NSGA-II should find increasing applications in the years to come.
748|Genetic Algorithms for Multiobjective Optimization: Formulation, Discussion and Generalization|The paper describes a rank-based fitness assignment method for Multiple Objective Genetic Algorithms (MOGAs). Conventional niche formation methods are extended to this class of multimodal problems and theory for setting the niche size is presented. The fitness assignment method is then modified to allow direct intervention of an external decision maker (DM). Finally, the MOGA is generalised further: the genetic algorithm is seen as the optimizing element of a multiobjective optimization loop, which also comprises the DM. It is the interaction between the two that leads to the determination of a satisfactory solution to the problem. Illustrative results of how the DM can interact with the genetic algorithm are presented. They also show the ability of the MOGA to uniformly sample regions of the trade-off surface.
749|Multiobjective Optimization Using Nondominated Sorting in Genetic Algorithms|In trying to solve multiobjective optimization problems, many traditional methods scalarize  the objective vector into a single objective. In those cases, the obtained solution is highly  sensitive to the weight vector used in the scalarization process and demands the user to have  knowledge about the underlying problem. Moreover, in solving multiobjective problems, designers  may be interested in a set of Pareto-optimal points, instead of a single point. Since genetic  algorithms(GAs) work with a population of points, it seems natural to use GAs in multiobjective  optimization problems to capture a number of solutions simultaneously. Although a vector  evaluated GA (VEGA) has been implemented by Schaffer and has been tried to solve a number  of multiobjective problems, the algorithm seems to have bias towards some regions. In this  paper, we investigate Goldberg&#039;s notion of nondominated sorting in GAs along with a niche and  speciation method to find multiple Pareto-optimal points sim...
750|An Overview of Evolutionary Algorithms in Multiobjective Optimization|The application of evolutionary algorithms (EAs) in multiobjective optimization is currently receiving growing interest from researchers with various backgrounds. Most research in this area has understandably concentrated on the selection stage of EAs, due to the need to integrate vectorial performance measures with the inherently scalar way in which EAs reward individual performance, i.e., number of offspring. In this review, current multiobjective evolutionary approaches are discussed, ranging from the conventional analytical aggregation of the different objectives into a single function to a number of populationbased approaches and the more recent ranking schemes based on the definition of Pareto-optimality. The sensitivity of different methods to
751|Evolutionary Algorithms for Multiobjective Optimization|Multiple, often conflicting objectives arise naturally in most real-world optimization scenarios. As evolutionary algorithms possess several characteristics due to which they are well suited to this type of problem, evolution-based methods have been used for multiobjective optimization for more than a decade. Meanwhile evolutionary multiobjective optimization has become established as a separate subdiscipline combining the fields of evolutionary computation and classical multiple criteria decision making. In this paper, the basic principles of evolutionary multiobjective optimization are discussed from an algorithm design perspective. The focus is on the major issues such as fitness assignment, diversity preservation, and elitism in general rather than on particular algorithms. Different techniques to implement these strongly related concepts will be discussed, and further important aspects such as constraint handling and preference articulation are treated as well. Finally, two applications will presented and some recent trends in the field will be outlined.  
752|Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art|Solving optimization problems with multiple (often conflicting) objectives is, generally, a  very difficult goal. Evolutionary algorithms (EAs) were initially extended and applied during  the mid-eighties in an attempt to stochastically solve problems of this generic class. During  the past decade, a variety of multiobjective EA (MOEA) techniques have been proposed  and applied to many scientific and engineering applications. Our discussion&#039;s intent is  to rigorously define multiobjective optimization problems and certain related concepts,  present an MOEA classification scheme, and evaluate the variety of contemporary MOEAs.  Current MOEA theoretical developments are evaluated; specific topics addressed include  fitness functions, Pareto ranking, niching, fitness sharing, mating restriction, and secondary  populations. Since the development and application of MOEAs is a dynamic and rapidly  growing activity, we focus on key analytical insights based upon critical MOEA evaluation  of c...
753|A Niched Pareto Genetic Algorithm for Multiobjective Optimization|Many, if not most, optimization problems have multiple objectives. Historically, multiple objectives have been combined ad hoc to form a scalar objective function, usually through a linear combination (weighted sum) of the multiple attributes, or by turning objectives into constraints. The genetic algorithm (GA), however, is readily modified to deal with multiple objectives by incorporating the concept of Pareto domination in its selection operator, and applying a niching pressure to spread its population out along the Pareto optimal tradeoff surface. We introduce the Niched Pareto GA as an algorithm for finding the Pareto optimal set. We demonstrate its ability to find and maintain a diverse &#034;Pareto optimal population&#034; on two artificial problems and an open problem in hydrosystems.
754|A Comprehensive Survey of Evolutionary-Based Multiobjective Optimization Techniques|. This paper presents a critical review of the most important evolutionary-based multiobjective optimization techniques developed over the years, emphasizing the importance of analyzing their Operations Research roots as a way to motivate the development of new approaches that exploit the search capabilities of evolutionary algorithms. Each technique is briefly described mentioning its advantages and disadvantages, their degree of applicability and some of their known applications. Finally, the future trends in this discipline and some of the open areas of research are also addressed.  Keywords: multiobjective optimization, multicriteria optimization, vector optimization, genetic algorithms, evolutionary algorithms, artificial intelligence. 1 Introduction  Since the pioneer work of Rosenberg in the late 60s regarding the possibility of using genetic-based search to deal with multiple objectives, this new area of research (now called evolutionary multiobjective optimization) has grown c...
755|Genetic Algorithms, Noise, and the Sizing of Populations|This paper considers the effect of stochasticity on the quality of convergence of genetic algorithms  (GAs). In many problems, the variance of building-block fitness or so-called collateral noise is the  major source of variance, and a population-sizing equation is derived to ensure that average signal-to-collateral-noise ratios are favorable to the discrimination of the best building blocks required to  solve a problem of bounded deception. The sizing relation is modified to permit the inclusion of other  sources of stochasticity, such as the noise of selection, the noise of genetic operators, and the explicit  noise or nondeterminism of the objective function. In a test suite of five functions, the sizing relation  proves to be a conservative predictor of average correct convergence, as long as all major sources  of noise are considered in the sizing calculation. These results suggest how the sizing equation may  be viewed as a coarse delineation of a boundary between what a physicist might call two distinct  phases of GA behavior. At low population sizes the GA makes many errors of decision, and the  quality of convergence is largely left to the vagaries of chance or the serial fixup of flawed results  through mutation or other serial injection of diversity. At large population sizes, GAs can reliably  discriminate between good and bad building blocks, and parallel processing and recombination of  building blocks lead to quick solution of even difficult deceptive problems. Additionally, the paper  outlines a number of extensions to this work, including the development of more refined models of  the relation between generational average error and ultimate convergence quality, the development  of online methods for sizing populations via the estimation of population-s...
756|The Gambler&#039;s Ruin Problem, Genetic Algorithms, and the Sizing of Populations|This paper presents a model for predicting the convergence quality of genetic algorithms. The model incorporates previous knowledge about decision making in genetic algorithms and the initial supply of building blocks in a novel way. The result is an equation that accurately predicts the quality of the solution found by a GA using a given population size. Adjustments for different selection intensities are considered and computational experiments demonstrate the effectiveness of the model. I. Introduction The size of the population in a genetic algorithm (GA) is a major factor in determining the quality of convergence. The question of how to choose an adequate population size for a particular domain is difficult and has puzzled GA practitioners for a long time. Hard questions are better approached using a divide-and-conquer strategy and the population sizing issue is no exception. In this case, we can identify two factors that influence convergence quality: the initial supply of build...
757|Multiobjective Optimization and Multiple Constraint Handling with Evolutionary Algorithms-Part I: A Unified Formulation|In optimization, multiple objectives and constraints cannot be handled independently of the underlying optimizer. Requirements such as continuity and differentiability of the cost surface add yet another conflicting element to the decision process. While ``better&#039;&#039; solutions should be rated higher than ``worse&#039;&#039; ones, the resulting cost landscape must also comply with such requirements. Evolutionary algorithms (EAs), which have found application in many areas not amenable to optimization by other methods, possess many characteristics desirable in a multiobjective optimizer, most notably the concerted handling of multiple candidate solutions. However, EAs are essentially unconstrained search techniques which require the assignment of a scalar measure of quality, or fitness, to such candidate solutions. After reviewing current evolutionary approaches to multiobjective and constrained optimization, the paper proposes that fitness assignment be interpreted as, or at least related to, a multicriterion decision process. A suitable decision making framework based on goals and priorities is subsequently formulated in terms of a relational operator, characterized, and shown to encompass a number of simpler decision strategies. Finally, the ranking of an arbitrary number of candidates is considered. The effect of preference changes on the cost surface seen by an EA is illustrated graphically for a simple problem. The paper concludes with the formulation of a multiobjective genetic algorithm based on the proposed decision strategy. Niche formation techniques are used to promote diversity among preferable candidates, and progressive articulation of preferences is shown to be possible as long as the genetic algorithm can recover from abrupt changes in the cost landscape.
758|Multiobjective Optimization Using Evolutionary Algorithms - A Comparative Case Study|. Since 1985 various evolutionary approaches to multiobjective optimization have been developed, capable of searching for multiple solutions concurrently in a single run. But the few comparative studies of different methods available to date are mostly qualitative and restricted to two approaches. In this paper an extensive, quantitative comparison is presented, applying four multiobjective evolutionary algorithms to an extended 0/1 knapsack problem.  1 Introduction  Many real-world problems involve simultaneous optimization of several incommensurable and often competing objectives. Usually, there is no single optimal solution, but rather a set of alternative solutions. These solutions are optimal in the wider sense that no other solutions in the search space are superior to them when all objectives are considered. They are known as Pareto-optimal solutions. Mathematically, the concept of Pareto-optimality can be defined as follows: Let us consider, without loss of generality, a multio...
759|Simulated Binary Crossover for Continuous Search Space|The success of binary-coded genetic algorithms (GAs) in problems having discrete search space largely depends on the coding used to represent the problem variables and on the crossover operator that propagates building-blocks from parent strings to children strings. In solving optimization problems having continuous search space, binary-coded GAs discretize the search space by using a coding of the problem variables in binary strings. However, the coding of real-valued variables in finite-length strings causes a number of difficulties---inability to achieve arbitrary precision in the obtained solution, fixed mapping of problem variables, inherent Hamming cliff problem associated with the binary coding, and processing of Holland&#039;s schemata in continuous search space. Although, a number of real-coded GAs are developed to solve optimization problems having a continuous search space, the search powers of these crossover operators are not adequate. In this paper, the search power of a cross...
760|Multi-Objective Genetic Algorithms: Problem Difficulties and Construction of Test Problems|In this paper, we study the problem features that may cause a multi-objective genetic  algorithm (GA) difficulty in converging to the true Pareto-optimal front. Identification  of such features helps us develop difficult test problems for multi-objective optimization.  Multi-objective test problems are constructed from single-objective optimization  problems, thereby allowing known difficult features of single-objective problems (such as  multi-modality, isolation, or deception) to be directly transferred to the corresponding  multi-objective problem. In addition, test problems having features specific to multiobjective  optimization are also constructed. More importantly, these difficult test problems  will enable researchers to test their algorithms for specific aspects of multi-objective  optimization.  Keywords  Genetic algorithms, multi-objective optimization, niching, pareto-optimality, problem difficulties,  test problems.  1 Introduction  After a decade since the pioneering wor...
761|Multiobjective Optimization Using the Niched Pareto Genetic Algorithm|Many, if not most, optimization problems have multiple objectives. Historically, multiple objectives (i.e., attributes or criteria) have been combined ad hoc to form a scalar objective function, usually through a linear combination (weighted sum) of the multiple attributes, or by turning objectives into constraints. The most recent development in the field of decision analysis has yielded a rigorous technique for combining attributes multiplicatively (thereby incorporating nonlinearity), and for handling uncertainty in the attribute values. But MultiAttribute Utility Analysis (MAUA) provides only a mapping from a vector-valued objective function to a scalar-valued function, and does not address the difficulty of searching large problem spaces. Genetic algorithms (GAs), on the other hand, are well suited to searching intractably large, poorly understood problem spaces, but have mostly been used to optimize a single objective. The direct combination of MAUA and GAs is a logical next step...
762|On the Performance Assessment and Comparison of Stochastic Multiobjective Optimizers|Abstract. This work proposes a quantitative, non-parametric interpretation of statistical performance of stochastic multiobjective optimizers, including, but not limited to, genetic algorithms. It is shown that, according to this interpretation, typical performance can be defined in terms analogous to the notion of median for ordinal data, as can other measures analogous to other quantiles. Non-parametric statistical test procedures are then shown to be useful in deciding the relative performance of different multiobjective optimizers on a given problem. Illustrative experimental results are provided to support the discussion. 1
763|On a Multi-Objective Evolutionary Algorithm and Its Convergence to the Pareto Set|Although there are many versions of evolutionary algorithms that are tailored to multi-criteria optimization, theoretical results are apparently not yet available. Here, it is shown that results known from the theory of evolutionary algorithms in case of single criterion optimization do not carry over to the multi-criterion case. At first, three different step size rules are investigated numerically for a selected problem with two conflicting objectives. The empirical results obtained by these experiments lead to the observation that only one of these step size rules may have the property to ensure convergence to the Pareto set. A theoretical analysis finally shows that a special version of an evolutionary algorithm with this step size rule converges with probability one to the Pareto set for the test problem under consideration. 
764|P.: A Spatial Predator-Prey Approach to Multi-Objective Optimization: A Preliminary Study|Abstract. This paper presents a novel evolutionary approach of approximating the shape of the Pareto-optimal set of multi-objective optimization problems. The evolutionary algorithm (EA) uses the predator-prey model from ecology. The prey are the usual individuals of an EA that represent possible solutions to the optimization task. They are placed at vertices of a graph, remain stationary, reproduce, and are chased by predators that traverse the graph. The predators chase the prey only within its current neighborhood and according to one of the optimization criteria. Because there are several predators with different selection criteria, those prey individuals, which perform best with respect to all objectives, are able to produce more descendants than inferior ones. As soon as a vertex for the prey becomes free, it is refilled by descendants from alive parents in the usual way of EA, i.e., by inheriting slightly altered attributes. After a while, the prey concentrate at Pareto-optimal positions. The main objective of this preliminary study is the answer to the question whether the predator-prey approach to multi-objective optimization works at all. The performance of this evolutionary algorithm is examined under several step-size adaptation rules. 1
765|Evolutionary Computation and Convergence to a Pareto Front|Research into solving multiobjective optimization problems (MOP) has as one of its an overall goals that of developing and defining foundations of an Evolutionary Computation (EC)-based MOP theory. In this paper, we introduce relevant MOP concepts, and the notion of Pareto optimality, in particular. Specific notation is defined and theorems are presented ensuring Paretobased Evolutionary Algorithm (EA) implementations are clearly understood. Then, a specific experiment investigating the convergence of an arbitrary EA to a Pareto front is presented. This experiment gives a basis for a theorem showing a specific multiobjective EA statistically converges to the Pareto front. We conclude by using this work to justify further exploration into the theoretical foundations of EC-based MOP solution methods.  1 Introduction  Our research focuses on solving scientific and engineering multiobjective optimization problems (MOPs), contributing to the overall goal of developing and defining foundatio...
766|Continuum structural topology design with genetic algorithms|The genetic algorithm (GA), an optimization technique based on the theory of natural selection, is applied to structural topology design problems. After reviewing the GA and previous research in structural topology optimization, we describe a binary material/void design representation that is encoded in GA chromosome data structures. This representation is intended to approximate a material continuum as opposed to discrete truss structures. Four examples, showing the broad utility of the approach and representation, are then presented. A fifth example suggests an alternate representation that allows continuously-variable material density. Concluding discussion suggests recommended uses of the technique and describes ongoing and possible future work. Ó 2000 Elsevier Science S.A. All rights reserved.
767|Freenet: A Distributed Anonymous Information Storage and Retrieval System|We describe Freenet, an adaptive peer-to-peer network application  that permits the publication, replication, and retrieval of data  while protecting the anonymity of both authors and readers. Freenet operates  as a network of identical nodes that collectively pool their storage  space to store data files and cooperate to route requests to the most  likely physical location of data. No broadcast search or centralized location  index is employed. Files are referred to in a location-independent  manner, and are dynamically replicated in locations near requestors and  deleted from locations where there is no interest. It is infeasible to discover  the true origin or destination of a file passing through the network,  and difficult for a node operator to determine or be held responsible for  the actual physical contents of her own node.
769|Error and attack tolerance of complex networks|Many complex systems display a surprising degree of tolerance against errors. For example, relatively simple organisms grow, persist and reproduce despite drastic pharmaceutical or environmental interventions, an error tolerance attributed to the robustness of the underlying metabolic network [1]. Complex communication networks [2] display a surprising degree of robustness: while key components regularly malfunction, local failures rarely lead to the loss of the global information-carrying ability of the network. The stability of these and other complex systems is often attributed to the redundant wiring of the functional web defined by the systems’ components. In this paper we demonstrate that error tolerance is not shared by all redundant systems, but it is displayed only by a class of inhomogeneously wired networks, called scale-free networks. We find that scale-free networks, describing a number of systems, such as the World Wide Web (www) [3–5], Internet [6], social networks [7] or a cell [8], display an unexpected degree of robustness, the ability of their nodes to communicate being unaffected by even unrealistically high failure rates. However,
770|Private Information Retrieval| Publicly accessible databases are an indispensable resource for retrieving up to date information. But they also pose a significant risk to the privacy of the user, since a curious database operator can follow the user&#039;s queries and infer what the user is after. Indeed, in cases where the users &#039; intentions are to be kept secret, users are often cautious about accessing the database. It can be shown that when accessing a single database, to completely guarantee the privacy of the user, the whole database should be downloaded, namely n bits should be communicated (where n is the number of bits in the database). In this work, we investigate whether by replicating the database, more efficient solutions to the private retrieval problem can be obtained. We describe schemes that enable a user to access k replicated copies of a database (k * 2) and privately retrieve information stored in the database. This means that each individual database gets no information on the identity of the item retrieved by the user. Our schemes use the replication to gain substantial saving. In particular, we have ffl A two database scheme with communication complexity of O(n1=3). ffl A scheme for a constant number, k, of databases with communication complexity O(n1=k). ffl A scheme for 13 log2 n databases with polylogarithmic (in n) communication complexity.
771|The Free Haven Project: Distributed Anonymous Storage Service|We present a design for a system of anonymous storage which resists the attempts of powerful adversaries to find or destroy any stored data. We enumerate distinct notions of anonymity for each party in the system, and suggest a way to classify anonymous systems based on the kinds of anonymity provided. Our design ensures the availability of each document for a publisher-specified lifetime. A reputation system provides server accountability by limiting the damage caused from misbehaving servers. We identify attacks and defenses against anonymous storage services, and close with a list of problems which are currently unsolved.
772|Publius: A robust, tamper-evident, censorship-resistant, web publishing system|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
773|Onion Routing for Anonymous and Private Internet Connections|this article&#039;s publication, the prototype network is processing more than 1 million Web connections per month from more than six thousand IP addresses in twenty countries and in all six main top level domains. [7] Onion Routing operates by dynamically building anonymous connections within a network of real-time Chaum Mixes [3]. A Mix is a store and forward device that accepts a number of fixed-length messages from numerous sources, performs cryptographic transformations on the messages, and then forwards the messages to the next destination in a random order. A single Mix makes tracking of a particular message either by specific bit-pattern, size, or ordering with respect to other messages difficult. By routing through numerous Mixes in the network, determining who is talking to whom becomes even more difficult. Onion Routing&#039;s network of core onion-routers (Mixes) is distributed, faulttolerant, and under the control of multiple administrative domains, so no single onion-router can bring down the network or compromise a user&#039;s privacy, and cooperation between compromised onion-routers is thereby confounded.
774|Web MIXes: A system for anonymous and unobservable Internet access|We present the architecture, design issues and functions of a MIX-based system for anonymous and unobservable real-time Internet access. This system prevents trac analysis as well as ooding attacks. The core technologies include an adaptive, anonymous, time/volumesliced channel mechanism and a ticket-based authentication mechanism. The system also provides an interface to inform anonymous users about their level of anonymity and unobservability.
775|The Eternity Service|The Internet was designed to provide a communications channel that is as resistant to denial of service attacks as human ingenuity can make it. In this note, we propose the construction of a storage medium with similar properties. The basic idea is to use redundancy and scattering techniques to replicate data across a large set of machines (such as the Internet), and add anonymity mechanisms to drive up the cost of selective service denial attacks. The detailed design of this service is an interesting scientific problem, and is not merely academic: the service may be vital in safeguarding individual rights against new threats posed by the spread of electronic publishing.
776|Anonymous Web Transactions with Crowds|This article presents a system called Crowds  that enables the retrieval of information over the  Web without revealing so much potentially private  information to several parties. The goal of  Crowds is to make browsing anonymous, so that  information about either the user or what information  he or she retrieves is hidden from Web  servers and other parties. Crowds prevents a  Web server from learning any potentially identifying  information about the user, including even  the user&#039;s IP address or domain name. Crowds  also prevents Web servers from learning a variety  of other information, such as the page that  referred the user to its site or the user&#039;s computing  platform
777|A Prototype Implementation of Archival Intermemory|An Archival Intermemory solves the problem of highly survivable digital data storage in the spirit of the Internet. In this paper we describe a prototype implementation of Intermemory, including an overall system architecture and implementations of key system components. The result is a working Intermemory that tolerates up to 17 simultaneous node failures, and includes a Web gateway for browser-based access to data. Our work demonstrates the basic feasibility of Intermemory and represents significant progress towards a deployable system.
778|Project &#034;Anonymity and Unobservability in the Internet&#034;|. It is a hard problem to achieve anonymity for real-time services in the Internet (e.g. Web access). All existing concepts fail when we assume a very strong attacker model (i.e. an attacker is able to observe all communication links). We also show that these attacks are realworld attacks. This paper outlines alternative models which mostly render these attacks useless. Our present work tries to increase the efficiency of these measures.  1 The perfect system  1.1 Attacks  The perfect anonymous communication system has to prevent the following attacks:  1.  Message coding attack: If messages do not change their coding during transmission they can be linked or traced.  2.  Timing attack: An opponent can observe the duration of a specific communication by linking its possible endpoints and waiting for a correlation between the creation and/or release event at each possible endpoint.  3.  Message volume attack: The amount of transmitted data (i.e. the message length) can be observed. Thus...
779|TAZ Servers and the Rewebber Network: Enabling Anonymous Publishing on the World Wide Web|The World Wide Web has recently matured enough to provide everyday users with an extremely cheap publishing mechanism. However, the current WWW architecture makes it fundamentally difficult to provide content without identifying yourself. We examine the problem of anonymous publication on the WWW, propose a design suitable for practical deployment, and describe our implementation. Some key features of our design include universal accessibility by pre-existing clients, short persistent names, security against social, legal, and political pressure, protection against abuse, and good performance.
780|The Free Haven Project: Design and Deployment of an Anonymous Secure Data Haven|The Free Haven Project aims to deploy a system for distributed data storage which is robust against attempts by powerful adversaries to find and destroy stored data. Free Haven uses a mixnet for communication, and it emphasizes distributed, reliable, and anonymous storage over efficient retrieval. We provide an outline of a formal definition of anonymity, to help characterize the protection that Free Haven provides and to help compare related services. We also provide some background from case law about anonymous speech and anonymous publication, and examine some of the ethical and moral implications of an anonymous publishing service. In addition, we describe a variety of attacks on the system and ways of protecting against these attacks. Some of the problems Free Haven addresses include providing sufficient accountability without sacrificing anonymity, building trust between servers based entirely on their observed behavior, and providing user interfaces that will make the system easy for end-users.
781|Wrappers for Feature Subset Selection|In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach andshow a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes. 
782|Induction of Decision Trees| The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.  
783|PROBABILITY INEQUALITIES FOR SUMS OF BOUNDED RANDOM VARIABLES|Upper bounds are derived for the probability that the sum S of n independent random variables exceeds its mean ES by a positive number nt. It is assumed that the range of each summand of S is bounded or bounded above. The bounds for Pr(S-ES&gt; nt) depend only on the endpoints of the ranges of the smumands and the mean, or the mean and the variance of S. These results are then used to obtain analogous inequalities for certain sums of dependent random variables such as U statistics and the sum of a random sample without replacement from a finite population. 
784|Instance-based learning algorithms|Abstract. Storing and using specific instances improves the performance of several supervised learning algorithms. These include algorithms that learn decision trees, classification rules, and distributed networks. However, no investigation has analyzed algorithms that use only specific instances to solve incremental learning tasks. In this paper, we describe a framework and methodology, called instance-based learning, that generates classification predictions using only specific instances. Instance-based learning algorithms do not maintain a set of abstractions derived from specific instances. This approach extends the nearest neighbor algorithm, which has large storage requirements. We describe how storage requirements can be significantly reduced with, at most, minor sacrifices in learning rate and classification accuracy. While the storage-reducing algorithm performs well on several realworld databases, its performance degrades rapidly with the level of attribute noise in training instances. Therefore, we extended it with a significance test to distinguish noisy instances. This extended algorithm&#039;s performance degrades gracefully with increasing noise levels and compares favorably with a noise-tolerant decision tree algorithm.
785|A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection|We review accuracy estimation methods and compare the two most common methods: cross-validation and bootstrap. Recent experimental results on artificial data and theoretical results in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone-out cross-validation. We report on a largescale experiment -- over half a million runs of C4.5 and a Naive-Bayes algorithm -- to estimate the effects of different parameters on these algorithms on real-world datasets. For cross-validation, we vary the number of folds and whether the folds are stratified or not; for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold stratified cross validation, even if computation power allows using more folds. 
786|The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain|If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the
787|Irrelevant Features and the Subset Selection Problem|We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets.
788|Supervised and unsupervised discretization of continuous features|Many supervised machine learning algorithms require a discrete feature space. In this paper, we review previous work on continuous feature discretization, identify de n-ing characteristics of the methods, and conduct an empirical evaluation of several methods. We compare binning, an unsupervised discretization method, to entropy-based and purity-based methods, which are supervised algorithms. We found that the performance of the Naive-Bayes algorithm signi cantly improved when features were discretized using an entropy-based method. In fact, over the 16 tested datasets, the discretized version of Naive-Bayes slightly outperformed C4.5 on average. We also show that in some cases, the performance of the C4.5 induction algorithm signi cantly improved if features were discretized in advance ? in our experiments, the performance never signi cantly degraded, an interesting phenomenon considering the fact that C4.5 is capable of locally discretizing features. 1
789|Neural network ensembles, cross validation, and active learning|Learning of continuous valued functions using neural network en-sembles (committees) can give improved accuracy, reliable estima-tion of the generalization error, and active learning. The ambiguity is defined as the variation of the output of ensemble members aver-aged over unlabeled data, so it quantifies the disagreement among the networks. It is discussed how to use the ambiguity in combina-tion with cross-validation to give a reliable estimate of the ensemble generalization error, and how this type of ensemble cross-validation can sometimes improve performance. It is shown how to estimate the optimal weights of the ensemble members using unlabeled data. By a generalization of query by committee, it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme. 1
790|Estimating Attributes: Analysis and Extensions of RELIEF|. In the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies among them. Kira and Rendell (1992a,b) developed an algorithm called RELIEF, which was shown to be very efficient in estimating attributes. Original RELIEF can deal with discrete and continuous attributes and is limited to only two-class problems. In this paper RELIEF is analysed and extended to deal with noisy, incomplete, and multi-class data sets. The extensions are verified on various artificial and one well known real-world problem. 1 Introduction  This paper deals with the problem of estimating the quality of attributes with strong dependencies to other attributes which seems to be the key issue of machine learning in general. Namely, for particular problems (e.q. parity problems of higher degrees) the discovering of dependencies between attributes may be unfeasible due to combinatorial explosion. In such cases efficient heuris...
791|An analysis of Bayesian classifiers|In this paper we present anaverage-case analysis of the Bayesian classifier, a simple induction algorithm that fares remarkably well on many learning tasks. Our analysis assumes a monotone conjunctive target concept, and independent, noise-free Boolean attributes. We calculate the probability that the algorithm will induce an arbitrary pair of concept descriptions and then use this to compute the probability of correct classification over the instance space. The analysis takes into account the number of training instances, the number of attributes, the distribution of these attributes, and the level of class noise. We also explore the behavioral implications of the analysis by presenting
792| 	 Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier   |The simple Bayesian classifier (SBC) is commonly thought to assume that attributes are independent given the class, but this is apparently contradicted by the surprisingly good performance it exhibits in many domains that contain clear attribute dependences. No explanation for this has been proposed so far. In this paper we show that the SBC does not in fact assume attribute independence, and can be optimal even when this assumption is violated by a wide margin. The key to this finding lies in the distinction between classification and probability estimation: correct classification can be achieved even when the probability estimates used contain large errors. We show that the previously-assumed region of optimality of the SBC is a second-order infinitesimal fraction of the actual one. This is followed by the derivation of several necessary and several sufficient conditions for the optimality of the SBC. For example, the SBC is optimal for learning arbitrary conjunctions and disjunctions, even though they violate the independence assumption. The paper also reports empirical evidence of the SBC&#039;s competitive performance in domains containing substantial degrees of attribute dependence.
793|Induction of Selective Bayesian Classifiers|In this paper, we examine previous work on the naive Bayesian classifier and review its limitations, which include a sensitivity to correlated features. We respond to this problem by embedding the naive Bayesian induction scheme within an algorithm that carries out a greedy search through the space of features. We hypothesize that this approach will improve asymptotic accuracy in domains that involve correlated features without reducing the rate of learning in ones that do not. We report experimental results on six natural domains, including comparisons with decision-tree induction, that support these hypotheses. In closing, we discuss other approaches to extending naive Bayesian classifiers and outline some directions for future research.
794|Learning With Many Irrelevant Features|In many domains, an appropriate inductive bias is the MIN-FEATURES bias, which prefers consistent hypotheses definable over as few features as possible. This paper defines and studies this bias. First, it is shown that any learning algorithm implementing the MIN-FEATURES bias requires \Theta(  1  ffl ln  1  ffi +  1  ffl [2  p  +  p ln n]) training examples to guarantee PAC-learning a concept having p relevant features out of n available features. This bound is only logarithmic in the number of irrelevant features. The paper also presents a quasi-polynomial time algorithm, FOCUS, which implements MIN-FEATURES. Experimental studies are presented that compare FOCUS to the ID3 and FRINGE algorithms. These experiments show that--- contrary to expectations---these algorithms do not implement good approximations of MIN-FEATURES. The coverage, sample complexity, and generalization performance of FOCUS is substantially better than either ID3 or FRINGE on learning problems where the MIN-FEATURE...
795|Training a 3-Node Neural Network is NP-Complete| We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for this network so that it produces output consistent with a given set of training examples. We extend the result to other simple networks. We also present a network for which training is hard but where switching to a more powerful representation makes training easier. These results suggest that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. They also suggest the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with nonlinear functions such as sigmoids.
796|Greedy Attribute Selection|Many real-world domains bless us with a wealth of attributes to use for learning. This blessing is often a curse: most inductive methods generalize worse given too many attributes than if given a  good subset of those attributes. We examine this problem for two learning tasks taken from a calendar scheduling domain. We show that ID3/C4.5 generalizes poorly on these tasks if allowed to use all available attributes. We examine five greedy hillclimbing procedures that search for attribute sets that generalize well with ID3/C4.5. Experiments suggest hillclimbing in attribute space can yield substantial improvements in generalization performance. We present a caching scheme that makes attribute hillclimbing more practical computationally. We also compare the results of hillclimbing in attribute space with FOCUS and RELIEF on the two tasks. 1 INTRODUCTION  As machine learning is applied to real-world tasks, difficulties arise that do not occur in simpler textbook experiments. One such diffic...
797|Data Mining using MLC++: A Machine Learning Library in C++|Data mining algorithmsincluding machine learning, statistical analysis, and pattern recognition techniques can greatly improve our understanding of data warehouses that are now becoming more widespread. In this paper, we focus on classification algorithms and review the need for multiple classification algorithms. We describe a system called  MLC++ , which was designed to help choose the appropriate classification algorithm for a given dataset by making it easy to compare the utility of different algorithms on a specific dataset of interest. MLC  ++ not only provides a workbench for such comparisons, but also provides a library of C  ++  classes to aid in the development of new algorithms, especially hybrid algorithms and multi-strategy algorithms. Such algorithms are generally hard to code from scratch. We discuss design issues, interfaces to other programs, and visualization of the resulting classifiers. 1 Introduction  Data warehouses containing massive amounts of data have been b...
798|The Power of Decision Tables|. We evaluate the power of decision tables as a hypothesis space for supervised learning algorithms. Decision tables are one of the simplest hypothesis spaces possible, and usually they are easy to understand. Experimental results show that on artificial and real-world domains containing only discrete features, IDTM, an algorithm inducing decision tables, can sometimes outperform state-of-the-art algorithms such as C4.5. Surprisingly, performance is quite good on some datasets with continuous features, indicating that many datasets used in machine learning either do not require these features, or that these features have few values. We also describe an incremental method for performing crossvalidation that is applicable to incremental learning algorithms including IDTM. Using incremental cross-validation, it is possible to cross-validate a given dataset and IDTM in time that is linear in the number of instances, the number of features, and the number of label values. The time for incre...
799|Efficient algorithms for minimizing cross validation error|Model selection is important in many areas of supervised learning. Given a dataset and a set of models for predicting with that dataset, we must choose the model which is expected to best predict future data. In some situations, such as online learning for control of robots or factories, data is cheap and human expertise costly. Cross validation can then be a highly effective method for automatic model selection. Large scale cross validation search can, however, be computationally expensive. This paper introduces new algorithms to reduce the computational burden of such searches. We show how experimental design methods can achieve this, using a technique similar to a Bayesian version of Kaelbling’s Interval Estimation. Several improvements are then given, including (1) the use of blocking to quickly spot near-identical models, and (2) schemata search: a new method for quickly finding families of relevant features. Experiments are presented for robot data and noisy synthetic datasets. The new algorithms speed up computation without sacrificing reliability, and in some cases are more reliable than conventional techniques. 1
800|Learning classification trees|Algorithms for learning cIassification trees have had successes in ar-tificial intelligence and statistics over many years. This paper outlines how a tree learning algorithm can be derived using Bayesian statis-tics. This iutroduces Bayesian techniques for splitting, smoothing, and tree averaging. The splitting rule is similar to QuinIan’s information gain, while smoothing and averaging replace pruning. Comparative ex-periments with reimplementations of a minimum encoding approach, Quinlan’s C4 (1987) and Breiman et aL’s CART (1984) show the full Bayesian algorithm produces more accurate predictions than versions
801|A Comparative Evaluation of  Sequential Feature Selection Algorithms|Several recent machine learning publications demonstrate the utility of using feature  selection algorithms in supervised learning tasks. Among these, scqucnlial feature  s1ion algorithms are receiving attention. The most frequently studied variants  of these algorithms are forward and backward sequential selection. Many studies  on supervised learning with sequential feature selection report applications of these  algorithms, but do not consider variants of them that might be more appropriate  for some performance tasks. This paper reports positive empirical results on such  variants, and argues for their serious consideration in similar learning tasks.
802|Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation|Selecting a good model of a set of input points by cross validation is a computationally intensive process, especially if the number of possible models or the number of training points is high. Techniques such as gradient descent are helpful in searching through the space of models, but problems such as local minima, and more importantly, lack of a distance metric between various models reduce the applicability of these search methods. Hoeffding Races is a technique for finding a good model for the data by quickly discarding bad models, and concentrating the computational effort at differentiating between the better ones. This paper focuses on the special case of leave-one-out cross validation applied to memorybased learning algorithms, but we also argue that it is applicable to any class of model selection problems. 1 Introduction  Model selection addresses &#034;high level&#034; decisions about how best to tune learning algorithm architectures for particular tasks. Such decisions include which...
803|Learning Boolean Concepts in the Presence of Many Irrelevant Features|In many domains, an appropriate inductive bias is the MIN-FEATURES bias, which prefers consistent hypotheses definable over as few features as possible. This paper defines and studies this bias in Boolean domains. First, it is shown that any learning algorithm implementing the MIN-FEATURES bias requires \Theta(  1  ffl ln  1  ffi +  1  ffl [2  p  + p ln n])  training examples to guarantee PAC-learning a concept having p relevant features out of n available features. This bound is only logarithmic in the number of irrelevant features. For implementing the MIN-FEATURES bias, the paper presents five algorithms that identify a subset of features sufficient to construct a hypothesis consistent with the training examples. FOCUS-1 is a straightforward algorithm that returns a minimal and sufficient subset of features in quasi-polynomial time. FOCUS-2 does the same task as FOCUS-1 but is empirically shown to be substantially faster than FOCUS-1. Finally, the Simple-Greedy, Mutual-Information-G...
804|Wrappers For Performance Enhancement And Oblivious Decision Graphs|In this doctoral dissertation, we study three basic problems in machine learning and two new hypothesis spaces with corresponding learning algorithms. The problems we investigate are: accuracy estimation, feature subset selection, and parameter tuning. The latter two problems are related and are studied under the wrapper approach. The hypothesis spaces we investigate are: decision tables with a default majority rule (DTMs) and oblivious read-once decision graphs (OODGs).
805|Using Decision Trees to Improve Case-Based Learning|This paper shows that decision trees can be used to improve the performance of casebased learning (CBL) systems. We introduce a performance task for machine learning systems called semi-flexible prediction that lies between the classification task performed by decision tree algorithms and the flexible prediction task performed by conceptual clustering systems. In semi-flexible prediction, learning should improve prediction of a specific set of features known a priori rather than a single known feature (as in classification) or an arbitrary set of features (as in conceptual clustering). We describe one such task from natural language processing and present experiments that compare solutions to the problem using decision trees, CBL, and a hybrid approach that combines the two. In the hybrid approach, decision trees are used to specify the features to be included in k-nearest neighbor case retrieval. Results from the experiments show that the hybrid approach outperforms both the decision ...
806|On biases in estimating multi-valued attributes|We analyse the biases of eleven measures for estimating the quality of the multi-valued attributes. The values of information gain, J-measure, gini-index, and relevance tend to linearly increase with the number of values of an attribute. The values of gain-ratio, distance measure, Relief, and the weight of evidence decrease for informative attributes and increase for irrelevant attributes. The bias of the statistic tests based on the chi-square distribution is similar but these functions are not able to discriminate among the attributes of different quality. We also introduce a new function based on the MDL principle whose value slightly decreases with the increasing number of attribute’s values. 1
808|Oversearching and Layered Search in Empirical Learning|When learning classifiers, more extensive search for rules is shown to lead to lower predictive accuracy on many of the real-world domains investigated. This counter-intuitive result is particularly relevant to recent systematic search methods that use risk-free pruning to achieve the same outcome as exhaustive search. We propose an iterated search method that commences with greedy search, extending its scope at each iteration until a stopping criterion is satisfied. This layered search is often found to produce theories that are more accurate than those obtained with either greedy search or moderately extensive beam search. 1 Introduction  Mitchell [1982] observes that the generalization implicit in learning from examples can be viewed as a search over the space of possible theories. From this perspective, most machine learning methods carry out a series of local searches in the vicinity of the current theory, selecting at each step the most promising improvement. Covering algorithms ...
809|Selecting a Classification Method by Cross-Validation|If we lack relevant problem-specific knowledge, cross-validation methods may be used to select a classification method empirically. We examine this idea here to show in what senses cross-validation does and does not solve the selection problem. As illustrated empirically, cross-validation may lead to higher average performance than application of any single classification strategy and it also cuts the risk of poor performance. On the other hand, cross-validation is no more or  less a form of bias than simpler strategies and applying it appropriately ultimately depends in the same way on prior knowledge. In fact, cross-validation may be seen as a way of applying partial information about the applicability of alternative classification strategies. Keywords: Cross-validation, classification, decision trees, neural networks.  1 Introduction Machine learning researchers and statisticians have produced a host of approaches to the problem of classification including methods for inducing rul...
810|Feature Selection for Case-Based Classification of Cloud Types: An Empirical Comparison|Accurate weather prediction is crucial for many activities, including Naval operations. Researchers within the meteorological division of the Naval Research Laboratory have developed and fielded several expert systems for problems such as fog and turbulence forecasting, and tropical storm movement. They are currently developing an automated system for satellite image interpretation, part of which involves cloud classification. Their cloud classification database contains 204 high-level features, but contains only a few thousand instances. The predictive accuracy of classifiers can be improved on this task by employing a feature selection algorithm. We explain why non-parametric case-based classifiers are excellent choices for use in feature selection algorithms. We then describe a set of such algorithms that use case-based classifiers, empirically compare them, and introduce novel extensions of backward sequential selection that allows it to scale to this task. Several of the approache...
811|Searching for Dependencies in Bayesian Classifiers|Naive Bayesian classifiers which make independence assumptions perform remarkably well on some data sets but poorly on others. We explore ways to improve the Bayesian classifier by searching for dependencies among attributes. We propose and evaluate two algorithms for detecting dependencies among attributes and show that the backward sequential elimination and joining algorithm provides the most improvement over the naive Bayesian classifier. The domains on which the most improvement occurs are those domains on which the naive Bayesian classifier is significantly less accurate than a decision tree learner. This suggests that the attributes used in some common databases are not independent conditioned on the class and that the violations of the independence assumption that affect the accuracy of the classifier can be detected from training data. 23.1 Introduction The Bayesian classifier (Duda
812|Automatic Parameter Selection by Minimizing Estimated Error|We address the problem of finding the parameter settings that will result in optimal performance of a given learning algorithm using a particular dataset as training data. We describe a &#034;wrapper&#034; method, considering determination of the best parameters as a discrete function optimization problem. The method uses best-first search and crossvalidation to wrap around the basic induction algorithm: the search explores the space of parameter values, running the basic algorithm many times on training and holdout sets produced by cross-validation to get an estimate of the expected error of each parameter setting. Thus, the final selected parameter settings are tuned for the specific induction algorithm and dataset being studied. We report experiments with this method on 33 datasets selected from the UCI and StatLog collections using C4.5 as the basic induction algorithm. At a 90% confidence level, our method improves the performance of C4.5 on nine domains, degrades performance on one, and is...
813|Lookahead and Pathology in Decision Tree Induction|The standard approach to decision tree induction is a top-down, greedy algorithm that makes locally optimal, irrevocable decisions at each node of a tree. In this paper, we study an alternative approach, in which the algorithms use limited lookahead to decide what test to use at a node. We systematically compare, using a very large number of decision trees, the quality of decision trees induced by the greedy approach to that of trees induced using lookahead. The main results of our experiments are: (i) the greedy approach produces trees that are just as accurate as trees produced with the much more expensive lookahead step; and (ii) decision tree induction exhibits pathology, in the sense that lookahead can produce trees that are both larger and less accurate than trees produced without it. 1. Introduction  The standard algorithm for constructing decision trees from a set of examples is greedy induction --- a tree is induced top-down with locally optimal choices made at each node, with...
814|Hybrid learning using genetic algorithms and decision trees for pattern classification|This paper introduces a hybrid learning methodology that integrates genetic algorithms (GAs) and decision tree learning (ID3) in order to evolve optimal subsets of discriminatory features for robust pattern classification. A GA is used to search the space of all possible subsets of a large set of candidate discrimination features. For a given feature subset, ID3 is invoked to produce a decision tree. The classification performance of the decision tree on unseen data is used as a measure of fitness for the given feature set, which, in turn, is used by the GA to evolve better feature sets. This GA-ID3 process iterates until a feature subset is found with satisfactory classification performance. Experimental results are presented which illustrate the feasibility of our approach on difficult problems involving recognizing visual concepts in satellite and facial image data. The results also show improved classification performance and reduced description complexity when compared against standard methods for feature selection. 1
815|Feature subset selection as search with probabilistic estimates|Irrelevant features and weakly relevant features may reduce the comprehensibility and accuracy of concepts induced by supervised learning algorithms. We formulate search problem with probabilistic estimates. Searching a space using an evaluation function that is a random variable requires trading off accuracy of estimates for increased state exploration. We show how recent feature subset selection algorithms in the machine learning literature fit into this search problem as simple hill climbing approaches, and conduct a small experiment using a best-first search technique. 1
816|Inductive Policy: The Pragmatics of Bias Selection|This paper extends the currently accepted model of inductive bias by identifying six categories of bias and separates inductive bias from the policy for its selection (the inductive policy). We analyze existing &amp;quot;blas selection &amp;quot; systems, examining the similarities and differences in their inductive policies, and idemify three techniques useful for building inductive policies. We then present a framework for representing and automaticaIly selecting a wide variety of biases and describe experiments with an instantiation of the framework addressing various pragmatic tradeoffs of time, space, accuracy, and the cost oferrors. The experiments show that a common framework can be used to implement policies for a variety of different types of blas selection, such as parameter selection, term selection, and example selection, using similar techniques. The experiments also show that different tradeoffs can be made by the implementation of different policies; for example, from the same data different rule sets can be learned based on different tradeoffs of accuracy versus the cost of erroneous predictions.
817|Oblivious Decision Trees and Abstract Cases|In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.
818|Useful Feature Subsets and Rough Set Reducts|In supervised classification learning, one attempts to induce a classifier that correctly predicts the label of novel instances. We demonstrate that by choosing a useful subset of features for the indiscernibility relation, an induction algorithm based on simple decision table can have high prediction accuracy on artificial and real-world datasets. We show that useful feature subsets are not necessarily maximal independent sets (relative reducts) with respect to the label, and that, in practical situations, using a subset of the relative core features may lead to superior performance. 1 Introduction In supervised classification learning, one is given a training set containing labelled instances (examples) . Each labelled instance contains a list of feature values (attribute values) and a discrete label value. The induction task is to build a classifier that will correctly predict the label of novel instances. Common classifiers are decision trees, neural networks, and nearest-neighbor...
819|A Quantitative Study of Hypothesis Selection|The hypothesis selection problem (or the k-  armed bandit problem) is central to the realization  of many learning systems. This  paper studies the minimization of sampling  cost in hypothesis selection under a probably  approximately optimal (PAO) learning  framework. Hypothesis selection algorithms  could be exploration-oriented or exploitationoriented.
820|Learning Bayesian Networks Using Feature Selection|This paper introduces a novel enhancement for learning Bayesian networks with a bias for small, high-predictive-accuracy networks. The new approach selects a subset of features which maximizes predictive accuracy prior to the network learning phase. We examine explicitly the effects of two aspects of the algorithm, feature selection and node ordering. Our approach generates networks which are computationally simpler to evaluate and which display predictive accuracy comparable to that of Bayesian networks which model all attributes. 1 INTRODUCTION  Bayesian networks are being increasingly recognized as an important representation for probabilistic reasoning. For many domains, the need to specify the probability distributions for a Bayesian network is considerable, and learning these probabilities from data using an algorithm like K2 [8]  1  could alleviate such specification difficulties. We describe an extension to the Bayesian network learning approaches introduced in K2. Rather than ...
821|Probabilistic Hill-Climbing: Theory and Applications|Many learning systems search through a space of possible performance elements, seeking an element with high expected utility. As the task of finding the globally optimal element is usually intractable, many practical learning systems use hill-climbing to find a local optimum. Unfortunately, even this is difficult, as it depends on the distribution of problems, which is typically unknown. This paper addresses the task of approximating this hill-climbing search when the utility function can only be estimated by sampling. We present an algorithm that returns an element that is, with provably high probability, essentially a local optimum. We then demonstrate the generality of this algorithm by sketching three meaningful applications, that respectively find an element whose efficiency, accuracy or completeness is nearly optimal. These results suggest approaches to solving the utility problem from explanation-based learning, the multiple extension problem from nonmonotonic reasoning and the ...
822|Self-Similarity Through High-Variability: Statistical Analysis of Ethernet LAN Traffic at the Source Level|A number of recent empirical studies of traffic measurements from a variety of working packet networks have convincingly demonstrated that actual network traffic is self-similar or long-range dependent in nature (i.e., bursty over a wide range of time scales) -- in sharp contrast to commonly made traffic modeling assumptions. In this paper, we provide a plausible physical explanation for the occurrence of self-similarity in LAN traffic. Our explanation is based on new convergence results for processes that exhibit high variability  (i.e., infinite variance) and is supported by detailed statistical analyses of real-time traffic measurements from Ethernet LAN&#039;s at the level of individual sources. This paper is an extended version of [53] and differs from it in significant ways. In particular, we develop here the mathematical results concerning the superposition of strictly alternating ON/OFF sources. Our key mathematical result states that the superposition of many ON/OFF sources (also k...
823|Self-Similarity in World Wide Web Traffic: Evidence and Possible Causes|Recently the notion of self-similarity has been shown to apply to wide-area and local-area network traffic. In this paper we examine the mechanisms that give rise to the self-similarity of network traffic. We present a hypothesized explanation for the possible self-similarity of traffic by using a particular subset of wide area traffic: traffic due to the World Wide Web (WWW). Using an extensive set of traces of actual user executions of NCSA Mosaic, reflecting over half a million requests for WWW documents, we examine the dependence structure of WWW traffic. While our measurements are not conclusive, we show evidence that WWW traffic exhibits behavior that is consistent with self-similar traffic models. Then we show that the self-similarity insuch traffic can be explained based on the underlying distributions of WWW document sizes, the effects of caching and user preference in le transfer, the effect of user &#034;think time&#034;, and the superimposition of many such transfers in a local area network. To do this we rely on empirically measured distributions both from our traces and from data independently collected at over thirty WWW sites. 
824|Experimental Queueing Analysis with Long-Range Dependent Packet Traffic|Recent traffic measurement studies from a wide range of working packet networks have convincingly established the presence of significant statistical features that are characteristic of fractal traffic processes, in the sense that these features span many time scales. Of particular interest in packet traffic modeling is a property called long-range dependence, which is marked by the presence of correlations that can extend over many time scales. In this paper, we demonstrate empirically that, beyond its statistical significance in traffic measurements, long-range dependence has considerable impact on queueing performance, and is a dominant characteristic for a number of packet traffic engineering problems. In addition, we give conditions under which the use of compact and simple traffic models that incorporate long-range dependence in a parsimonious manner (e.g.,  fractional Brownian motion) is justified and can lead to new insights into the traffic management of high-speed networks. 1...
825|A measurement-based admission control algorithm for integrated services packet networks|Many designs for integrated service networks offer a bounded delay packet delivery service to support real-time applications. To provide bounded delay service, networks must use admission control to regulate their load. Previous work on admission control mainly focused on algorithms that compute the worst case theoretical queueing delay to guarantee an absolute delay bound for all packets. In this paper we describe a measurement-based admission control algorithm for predictive service, which allows occasional delay violations. We have tested our algorithm through simulations on a wide variety of network topologies and driven with various source models, including some that exhibit long-range dependence, both in themselves and in their aggregation. Our simulation results suggest that, at least for the scenarios studied here, the measurement-based approach combined with the relaxed service commitment of predictive service enables us to achieve a high
826|Proof of a Fundamental Result in Self-Similar Traffic Modeling|We state and prove the following key mathematical result in self-similar traffic modeling: the superposition of many ON/OFF sources (also known as packet trains) with strictly alternating ON- and OFF-periods and whose  ON-periods or OFF-periods exhibit the Noah Effect (i.e., have high variability or infinite variance) can produce aggregate network traffic that exhibits the Joseph Effect (i.e., is self-similar or long-range dependent). There is, moreover, a simple relation between the parameters describing the intensities of the Noah Effect (high variability) and the Joseph Effect (self-similarity). This provides a simple physical explanation for the presence of self-similar traffic patterns in modern high-speed network traffic that is consistent with traffic measurements at the source level. We illustrate how this mathematical result can be combined with modern high-performance computing capabilities to yield a simple and efficient linear-time algorithm for generating self-similar traf...
827|Large Deviations and Overflow Probabilities for the General Single-Server Queue, With Applications|We consider from a thermodynamic viewpoint queueing systems where the workload process is assumed to have an associated large deviation principle with arbitrary scaling: there exist increasing scaling functions (a t ; v t ; t 2  R+ ) and a rate function I  such that if (W t ; t 2 R+ ) denotes the workload process, then lim  t!1  v  \Gamma1  t log P (W t =a t ? w) = \GammaI (w) on the continuity set of I . In the case that a t = v t = t it has been argued heuristically, and recently proved in a fairly general context (for discrete time models) by Glynn and Whitt [8], that the queue-length distribution (that is, the distribution of supremum of the workload process Q = sup t0 W t ) decays exponentially: P (Q ? b) ¸ e  \Gammaffib and the decay rate ffi is directly related to the rate function I . We establish conditions for a more general result to hold, where the scaling functions are not necessarily linear in t: we find that the queue-length distribution has an exponential tail only if l...
828|Packet Trains: Measurements and a New Model for Computer Network Traffic|Traffic measurements on a ring local area computer network
at the Massachusetts Institute of Technology are presented. The analysis of the arrival pattern shows that the arrival processes are neither Poisson nor compound Poisson. An alternative model called “packet train” is proposed.

In the train model, the traffic on the network consists of a number of packet streams between various pairs of nodes tohne network. Each node-pair stream (or node-pair process, as we call them) consists of a number of trains. Each train consists of a numbero f packets (or cars) going in either direction (from node A to B or from node B to A). The
intercar gap is large (compared to packet transmission time) and random. The intertrain time is even larger. The Poisson and the compound Poisson arrivals are shown to sbpee cial cases of the train arrival model. Another important observation is that the packet arrivals exhibit a
“source locality.” If a packet is seen on the network going from A to B, the probability of the next packet going from A to B or from B to A is very high. Implications of the train arrivals and of source locality on the design
of bridges, gateways, and reservation protocols are discussed. A number of open problems requiring development of analysis techniques for systems with train arrival processes are also described.
829|Compilation and review|This report, delivered under Modification 2 to Interagency Agreement SAA2-402113 between NASA and the U.S. Forest Service, has been approved for public release. TABLE OF CONTENTS Section
830|An Empirical Workload Model for Driving Wide-Area TCP/IP Network Simulations|We present an artificial workload model of wide-area internetwork traffic. The model can be used to drive simulation experiments of communication protocols and flow and congestion control experiments. The model is based on analysis of wide-area TCP/IP traffic collected from  one industrial and two academic networks. The artificial workload model uses both detailed knowledge and measured characteristics of the user application programs responsible for the traffic. Observations drawn from our measurements contradict some commonly held beliefs regarding wide-area TCP/IP network traffic.
831|Characterizing the Variability of Arrival Processes with Indices of Dispersion|We propose to characterize the burstiness of packet arrival processes with indices of  dispersion for intervals and for counts. These indices, which are functions of the  variance of intervals and counts, are relatively straightforward to estimate and convey  much more information than simpler indices, such as the coefficient of variation,  that are often used to describe burstiness quantitatively.
832|Xgobi: Interactive Dynamic Graphics In The X Window System With A Link To S|Data analysts perform a wide variety of computational tasks in the course of an analysis, and they should be able to do them all on the same platform. XGobi helps to make this possible by bringing state-of-the-art dynamic graphic methods for the display and manipulation of scatter plots to the UNIX  R fl  workstation, and by linking these methods to the S data analysis environment. XGobi is implemented in the X Window System  TM  , which offers portability across a wide variety of workstations, X terminals, and personal computers, as well as the ability to run smoothly across a network. A user can run XGobi and S simultaneously, making it possible to use each program&#039;s functions on the same data. Using a highly interactive direct-manipulation design, XGobi offers an array of familiar scatterplot tools. Users can view pairwise plots, three-dimensional rotations and grand tour sequences. Views of the data can be reshaped, and points can be identified or brushed. Projection coefficients a...
833|Is Network Traffic Self-Similar Or Multifractal?|This note addresses the question of whether self-similar processes are sufficient to model packet network traffic, or whether a broader class of multifractal processes is needed. By using the absolute moments of aggregate traffic measurements, we conclude that measured local-area network (LAN) and wide-area network (WAN) traffic traces, with the sample means subtracted, are well modelled by random processes that are either exactly or asymtotically self-similar. 1 Introduction  Self-similarity in modern packet network traffic has recently been the focus of much attention, prompted by the results of Leland, Taqqu, Willinger, Wilson [6] (see also [13]), who examined several high-resolution Ethernet local-area network (LAN) traffic traces and found them to be consistent with statistical self-similarity. Similar findings were reported by Paxson and Floyd [7] for wide-area network (WAN) traffic measurements. This note addresses the question of whether for a more accurate description of these...
834|Heavy Tail Modeling And Teletraffic Data|. Huge data sets from the teletraffic industry exhibit many non-standard characteristics such as heavy tails and long range dependence. Various estimation methods for heavy tailed time series with positive innovations are reviewed. These include parameter estimation and model identification methods for autoregressions and moving averages. Parameter estimation methods include those of Yule-Walker and the linear programming estimators of Feigin and Resnick as well estimators for tail heaviness such as the Hill estimator and the qq-estimator. Examples are given using call holding data and inter-arrivals between packet transmissions on a computer network. The limit theory makes heavy use of point process techniques and random set theory. 1. Introduction  Classical queuing and network stochastic models contain simplifying assumptions guaranteeing the Markov property and insuring analytical tractability. Frequently inter-arrivals and service times are assumed to be iid and typically underlyi...
835|Economies of Scale in Queues With Sources Having Power-Law Large Deviation Scalings.|We analyse the queue Q  L  at a multiplexer with L sources which may display long-range dependence. This includes, for example, sources modelled by fractional Brownian Motion (fBM). The workload processes W due to each source are assumed to have large deviation properties of the form P [W t =a(t) ? x] ß e  \Gammav(t)K(x)  for appropriate scaling functions a and v, and ratefunction  K. Under very general conditions, lim  L!1  L  \Gamma1  log P [Q  L  ? Lb] = \GammaI (b) provided the offered load is held constant, where the shape function I is expressed in terms of the cumulant generating functions of the input traffic. For power-law scalings v(t) = t  v  ,  a(t) = t  a  (such as occur in fBM) we analyse the asymptotics of the shape function: lim  b!1  b  \Gammau=a  i  I(b) \Gamma ffi b  v=a  j  =  u for some exponent u and constant  depending on the sources. This demonstrates the economies of scale available through the multiplexing of a large number of such sources, by comparison with ...
836|Fractal Renewal Processes Generate 1/f Noise|f  D  noise occurs in an impressive variety of physical systems, and numerous complex theories have been proposed to explain it. We construct two relatively simple renewal processes whose power spectral densities vary as 1=f  D  : 1) a standard renewal point process, with 0 ! D ! 1; and 2) a finite-valued alternating renewal process, with 0 ! D ! 2. The resulting event number statistics, coincidence rates, minimal coverings, and autocorrelation functions are shown also to follow power-law forms. These fractal characteristics derive from interevent-time probability density functions which themselves decay in a power-law fashion. A number of applications are considered: trapping in amorphous semiconductors, electronic burst noise, movement in systems with fractal boundaries, the digital generation of 1=f  D  noise, and ionic currents in cell membranes. 05.40.+j, 02.50.+s, 72.20.Jv, 72.80.Ng, 87.10.+e Typeset using REVT E X 1  I. 
837|Heavy-Tailed ON/OFF Source Behavior and Self-Similar Traffic| Recent traffic measurement studies suggest that the self-similarity observed in packet traffic arises from aggregating individual sources which behave in an ON/OFF manner with heavy-tailed sojourn times in one or both of the states. In this paper, we investigate the connection between general ON/OFF behavior, self-similarity and queueing performance. We use chaotic maps to model general ON/OFF behavior with combinations of heavy tailed and light tailed sojourn time behavior. We present results which show that chaotic maps which capture the heavytailed sojourn time behavior in the OFF and/or ON states generate traffic that is asymptotically self-similar. However, the resulting queue length distribution decays as a power law with the heavy ON source, and as an exponential with the light ON source, even though both processes exhibit identical 1/f noise behavior. To resolve this apparent paradox, we consider aggregates of ON and OFF sources, and show that the nature of the ON period is le...
838|The QQ-Estimator And Heavy Tails|. A common visual technique for assessing goodness of fit and estimating location and scale is the qq--plot. We apply this technique to data from a Pareto distribution and more generally to data generated by a distribution with a heavy tail. A procedure for assessing the presence of heavy tails and for estimating the parameter of regular variation is discussed which can supplement other standard techniques such as the Hill plot. 1. Introduction.  A graphical technique called the qq-plot is a commonly used method of visually assessing goodness of fit and of estimating location and scale parameters. The method is standard and ubiquitious in various forms. See for example Rice (1988) and Castillo (1988). The method is based on the following simple observation: If  U 1;n  U 2;n  : : : U n;n  are the order statistics from n iid observations which are uniformly distributed on [0; 1], then by symmetry E(U i+1;n \Gamma U i;n ) = 1  n + 1 and hence  EU i;n =  i n + 1  :  Thus since U i;n should...
839|Face Recognition Based on Fitting a 3D Morphable Model|Abstract—This paper presents a method for face recognition across variations in pose, ranging from frontal to profile views, and across a wide range of illuminations, including cast shadows and specular reflections. To account for these variations, the algorithm simulates the process of image formation in 3D space, using computer graphics, and it estimates 3D shape and texture of faces from single images. The estimate is achieved by fitting a statistical, morphable model of 3D faces to images. The model is learned from a set of textured 3D scans of heads. We describe the construction of the morphable model, an algorithm to fit the model to images, and a framework for face identification. In this framework, faces are represented by model parameters for 3D shape and texture. We present results obtained with 4,488 images from the publicly available CMU-PIE database and 1,940 images from the FERET database. Index Terms—Face recognition, shape estimation, deformable model, 3D faces, pose invariance, illumination invariance. æ 1
840|An iterative image registration technique with an application to stereo vision|Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is faster because it examines far fewer potential matches between the images than existing techniques. Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show show our technique can be adapted for use in a stereo vision system. 2. The registration problem The translational image registration problem can be characterized as follows: We are given functions F(x) and G(x) which give the respective pixel values at each location x in two images, where x is a vector. We wish to find the disparity vector h which minimizes some measure of the difference between F(x + h) and G(x), for x in some region of interest R. (See figure 1). 1.
841|Determining Optical Flow|Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image. 
842|Active Appearance Models|AbstractÐWe describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors. Index TermsÐAppearance models, deformable templates, model matching. 1
844|From Few to many: Illumination cone models for face recognition under variable lighting and pose|We present a generative appearance-based method for recognizing human faces under variation in lighting and viewpoint. Our method exploits the fact that the set of images of an object in fixed pose, but under all possible illumination conditions, is a convex cone in the space of images. Using a small number of training images of each face taken with different lighting directions, the shape and albedo of the face can be reconstructed. In turn, this reconstruction serves as a generative model that can be used to render—or synthesize—images of the face under novel poses and illumination conditions. The pose space is then sampled, and for each pose the corresponding illumination cone is approximated by a low-dimensional linear subspace whose basis vectors are estimated using the generative model. Our recognition algorithm assigns to a test image the identity of the closest approximated illumination cone (based on Euclidean distance within the image space). We test our face recognition method on 4050 images from the Yale Face Database B; these images contain 405 viewing conditions (9 poses ¢ 45 illumination conditions) for 10 individuals. The method performs almost without error, except on the most extreme lighting directions, and significantly outperforms popular recognition methods that do not use a generative model.
845|Fitting Parameterized Three-Dimensional Models to Images|Model-based recognition and motion tracking depends upon the ability to solve  for projection and model parameters that will best fit a 3-D model to matching  2-D image features. This paper extends current methods of parameter solving to  handle objects with arbitrary curved surfaces and with any number of internal parameters  representing articulations, variable dimensions, or surface deformations. Numerical
846|Linear Object Classes and Image Synthesis From a Single Example Image|Abstract—The need to generate new views of a 3D object from a single real image arises in several fields, including graphics and object recognition. While the traditional approach relies on the use of 3D models, we have recently introduced [1], [2], [3] simpler techniques that are applicable under restricted conditions. The approach exploits image transformations that are specific to the relevant object class, and learnable from example views of other “prototypical ” objects of the same class. In this paper, we introduce such a technique by extending the notion of linear class proposed by Poggio and Vetter. For linear object classes, it is shown that linear transformations can be learned exactly from a basis set of 2D prototypical views. We demonstrate the approach on artificial objects and then show preliminary evidence that the technique can effectively “rotate ” highresolution face images from a single 2D view. Index Terms—3D object recognition, rotation invariance, deformable templates, image synthesis. 1
847|Face identification across different poses and illumination with a 3D morphable model|We present a novel approach for recognizing faces in im-ages taken from different directions and under different il-lumination. The method is based on a 3D morphable face model that encodes shape and texture in terms of model pa-rameters, and an algorithm that recovers these parameters from a single image of a face. For face identification, we use the shape and texture parameters of the model that are separated from imaging parameters, such as pose and illu-mination. In addition to the identity, the system provides a measure of confidence. We report experimental results for more than 4000 images from the publicly available CMU-PIE database. 1
848|Automatic face identification system using flexible appearance models|We describe the use of flexible models for representing the shape and grey-level appearance of human faces. These models are controlled by a small number of parameters which can be used to code the overall appearance of a face for image compression and classification purposes. The model parameters control both inter-class and within-class variation. Discriminant analysis techniques are employed to enhance the effect of those parameters affecting inter-class variation, which are useful for classification. We have performed experiments on face coding and reconstruction and automatic face identification. Good recognition rates are obtained even when significant variation in lighting, expression and 3D viewpoint, is allowed. 
849|Multidimensional morphable models: A framework for representing and matching object classes|This thesis describes a flexible model for representing images of objects of a certain class, such as faces, and introduces a new algorithm for matching the model to novel images from the class. The model and matching algorithm are very general and can be used for many image analysis tasks. The flexible model, called a multidimensional morphable model, is learned from example images (called prototypes) of objects of a class. In the learning phase, pixelwise correspon-dences between a reference prototype and each of the other prototypes are first computed and then used to obtain shape and texture vectors associated with each prototype. The mor-phable model is then synthesized as a linear combination that spans the linear vector space determined by the prototypical shapes and textures. We next introduce an effective stochastic gradient descent algorithm that automatically matches a model to a novel image by finding the parameters that minimize the error between the image generated by the model and the novel image. Several experiments demonstrate the robustness and the broad range of applicability of the matching algorithm and the underlying
850|SFS Based View Synthesis for Robust Face Recognition|Sensitivity to variations in pose is a challenging problem in face recognition using appearance-based methods. More specifically, the appearance of a face changes dramatically when viewing and/or lighting directions change. Various approaches have been proposed to solve this difficult problem. They can be broadly divided into three classes: 1) multiple image based methods where multiple images of various poses per person are available, 2) hybrid methods where multiple example images are available during learning but only one database image per person is available during recognition, and 3) single image based methods where no example based learning is carried out. In this paper, we present a method that comes under class 3. This method based on shape-from-shading (SFS) improves the performance of a face recognition system in handling variations due to pose and illumination via image synthesis.  
851|Eigen Light-Fields and Face Recognition Across Pose|In many face recognition tasks the pose of the probe and gallery images are different. In other cases multiple gallery or probe images may be available, each captured from a different pose. We propose a face recognition algorithm which can use any number of gallery images per subject captured at arbitrary poses, and any number of probe images, again captured at arbitrary poses. The algorithm operates by estimating the eigen light-field of the subject&#039;s head from the input gallery or probe images. Matching between the probe and gallery is then performed using the eigen light-fields. We present results on the CMU PIE and the FERET face databases.
852|Face Recognition from Unfamiliar Views: Subspace Methods and Pose Dependency|A framework for recognising human faces from unfamiliar views is described and a simple implementation of this framework evaluated. The interaction between training view and testing view is shown to compare with observations in human face recognition experiments. The ability of the system to learn from several training views, as available in video footage, is shown to improve the overall performance of the system as is the use of multiple testing images. 1 Introduction Recognising faces from previously unseen viewpoints is inherently more difficult than matching faces at the same view. Simple image comparisons such as correlation demonstrate that there is a greater difference between different viewpoints of the same subject than between different subjects at the same view which means that the recognition method used must take into account the non-linear variations of faces with viewpoint. In order to achieve recognition of previously unseen views, we require a method of relating the...
853|Distributed Computing in Practice: The Condor Experience|Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational grid. In this chapter, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course traveled by research ideas as they grow into production systems.
854|The Byzantine Generals Problem|Reliable computer systems must handle malfunctioning components that give conflicting information to different parts of the system. This situation can be expressed abstractly in terms of a group of generals of the Byzantine army camped with their troops around an enemy city. Communicating only by messenger, the generals must agree upon a common battle plan. However, one of more of them may be traitors who will try to confuse the others. The problem is to find an algorithm to ensure that the loyal generals will reach agreement. It is shown that, using only oral messages, this problem is solvable if and only if more than two-thirds of the generals are loyal; so a single traitor can confound two loyal generals. With unforgeable written messages, the problem is solvable for any number of generals and possible traitors. Applications of the solutions to reliable computer systems are then discussed.
855|Distributed Snapshots: Determining Global States of Distributed Systems|This paper presents an algorithm by which a process in a distributed system determines a global state of the system during a computation. Many problems in distributed systems can be cast in terms of the problem of detecting global states. For instance, the global state detection algorithm helps to solve an important class of problems: stable property detection. A stable property is one that persists: once a stable property becomes true it remains true thereafter. Examples of stable properties are “computation has terminated,”  “the system is deadlocked” and “all tokens in a token ring have disappeared.” The stable property detection problem is that of devising algorithms to detect a given stable property. Global state detection can also be used for checkpointing. 
856|The Protection of Information in Computer Systems|This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures--whether hardware or software--that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysis of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading. Glossary The following glossary provides, for reference, brief definitions for several terms as used in this paper in the context of protecting information in computers. Access The ability to make use of information stored in a computer system. Used frequently as a verb, to the horror of grammarians. Access control list A list of principals that are authorized to have access to some object. Authenticate To verify the identity of a person (or other agent external to the protection system) making a request.
857|Kerberos: An Authentication Service for Open Network Systems|In an open network computing environment, a workstation cannot be trusted to identify its users correctly to network services.  Kerberos provides an alternative approach whereby a trusted third-party authentication service is used to verify users’ identities. This paper gives an overview of the Kerberos authentication model as implemented for MIT’s Project Athena. It describes the protocols used by clients, servers, and Kerberos to achieve authentication. It also describes the management and replication of the database required.   The views of Kerberos as seen by the user, programmer, and administrator are described.  Finally, the role of Kerberos in the larger Athena picture is given, along with a list of applications that presently use Kerberos for user authentication.  We describe the addition of Kerberos authentication to the Sun Network File System as a case study for integrating Kerberos with an existing application.
858|A Security Architecture for Computational Grids|State-of-the-art and emerging scientific applications require fast access to large quantities of data and commensurately fast computational resources. Both resources and data are often distributed in a wide-area network with components administered locally and independently. Computations may involve hundreds of processes that must be able to acquire resources dynamically and communicate e#ciently. This paper analyzes the unique security requirements of large-scale distributed (grid) computing and develops a security policy and a corresponding security architecture. An implementation of the architecture within the Globus metacomputing toolkit is discussed.  
859|A Resource Management Architecture for Metacomputing Systems|Metacomputing systems are intended to support remote and/or  concurrent use of geographically distributed computational resources.  Resource management in such systems is complicated by five concerns  that do not typically arise in other situations: site autonomy and heterogeneous  substrates at the resources, and application requirements for policy  extensibility, co-allocation, and online control. We describe a resource  management architecture that addresses these concerns. This architecture  distributes the resource management problem among distinct local  manager, resource broker, and resource co-allocator components and defines  an extensible resource specification language to exchange information  about requirements. We describe how these techniques have been  implemented in the context of the Globus metacomputing toolkit and  used to implement a variety of different resource management strategies.  We report on our experiences applying our techniques in a large testbed,  GUSTO, incorporating 15 sites, 330 computers, and 3600 processors.  
860|Matchmaking: Distributed Resource Management for High Throughput Computing|Conventional resource management systems use a system model to describe resources and a centralized scheduler to control their allocation. We argue that this paradigm does not adapt well to distributed systems, particularly those built to support high-throughput computing. Obstacles include heterogeneity of resources, which make uniform allocation algorithms difficult to formulate, and distributed ownership, leading to widely varying allocation policies. Faced with these problems, we developed and implemented the classified advertisement (classad) matchmaking framework, a flexible and general approach to resource management in distributed environment with decentralized ownership of resources. Novel aspects of the framework include a semi-structured data model that combines schema, data, and query in a simple but powerful specification language, and a clean separation of the matching and claiming phases of resource allocation. The representation and protocols result in a robust, scalabl...
861|BEOWULF: A Parallel Workstation For Scientific Computation|Network-of-Workstations technology is applied to the challenge of implementing very high performance workstations for Earth and space science applications. The Beowulf parallel workstation employs 16 PCbased processing modules integrated with multiple Ethernet networks. Large disk capacity and high disk to memory bandwidth is achieved through the use of a hard disk and controller for each processing module supporting up to 16 way concurrent accesses. The paper presents results from a series of experiments that measure the scaling characteristics of Beowulf in terms of communication bandwidth, file transfer rates, and processing performance. The evaluation includes a computational fluid dynamics code and an N-body gravitational simulation program. It is shown that the Beowulf architecture provides a new operating point in performance to cost for high performance workstations, especially for file transfers under favorable conditions.  1 INTRODUCTION  Networks Of Workstations, or NOW [?] ...
862|Dealing with disaster: Surviving misbehaved kernel extensions|Today’s extensible operating systems allow applications to modify kernel behavior by providing mechanisms for application code to run in the kernel address space. The advantage of this approach is that it provides improved application flexibility and performance; the disadvantage is that buggy or malicious code can jeopardize the integrity of the kernel. It has been demonstrated that it is feasible to use safe languages, software fault isolation, or virtual memory protection to safeguard the main kernel. However, such protection mechanisms do not address the full range of problems, such as resource hoarding, that can arise when application code is introduced into the kernel. In this paper, we present an analysis of extension mechanisms in the VINO kernel. VINO uses software fault isolation as its safety mechanism and a lightweight transaction system to cope with resource-hoarding. We explain how these two mechanisms are sufficient to protect against a large class of errant or malicious extensions, and we quantify the overhead that this protection introduces. We find that while the overhead of these techniques is high relative to the cost of the extensions themselves, it is low relative to the benefits that extensibility brings.
863|The locus distributed operating system|LOCUS Is a distributed operating system which supports transparent access to data through a network wide fllesystem, permits automatic replication of storaget supports transparent distributed process execution, supplies a number of high reliability functions such as nested transactions, and is upward compatible with Unix. Partitioned operation of subnetl and their dynamic merge is also supported. The system has been operational for about two years at UCLA and extensive experience In its use has been obtained. The complete system architecture is outlined in this paper, and that experience is summarized. 1
864|Core algorithms of the Maui scheduler|Abstract. The Maui scheduler has received wide acceptance in the HPC community as a highly configurable and effective batch scheduler. It is currently in use on hundreds of SP, O2K, and Linux cluster systems throughout the world including a high percentage of the largest and most cutting edge research sites. While the algorithms used within Maui have proven themselves effective, nothing has been published to date documenting these algorithms nor the configurable aspects they support. This paper focuses on three areas of Maui scheduling, specifically, backfill, job prioritization, and fairshare. It briefly discusses the goals of each component, the issues and corresponding design decisions, and the algorithms enabling the Maui policies. It also covers the configurable aspects of each algorithm and the impact of various parameter selections. 1
865|A worldwide flock of Condors: load sharing among workstation clusters|Condor is a distributed batch system for sharing the workload of compute-intensive
866|Stork: Making Data Placement a First Class Citizen in the Grid|Todays scientific applications have huge data requirements which continue to increase drastically every year. These data are generally accessed by many users from all across the the globe. This implies a major necessity to move huge amounts of data around wide area networks to complete the computation cycle, which brings with it the problem of efficient and reliable data placement. The current approach to solve this problem of data placement is either doing it manually, or employing simple scripts which do not have any automation or fault tolerance capabilities. Our goal is to make data placement activities first class citizens in the Grid just like the computational jobs. They will be queued, scheduled, monitored, managed, and even check-pointed. More importantly, it will be made sure that they complete successfully and without any human interaction. We also believe that data placement jobs should be treated differently from computational jobs, since they may have different semantics and different characteristics. For this purpose, we have developed Stork, a scheduler for data placement activities in the Grid.
867|Condor Technical Summary|Condor is a software package for executing long running &#034;batch&#034; type jobs on workstations which would otherwise be idle. Major features of Condor are automatic location and allocation of idle machines, and checkpointing and migration of processes. All of these features are achieved without any modifications to the UNIX kernel whatsoever. Also, users of Condor do not need to change their source programs to run with Condor, although such programs must be specially linked. The features of Condor for both users and workstation owners along with the limitations on the kinds of jobs which may be executed by Condor are described. The mechanisms behind our implementations of checkpointing and process migration are discussed in detail. Finally, the software which detects idle machines and allocates those machines to Condor users is described along with the techniques used to configure that software to meet the demands of a particular computing site or workstation owner.  1. Introduction to the ...
868|Solving Large Quadratic Assignment Problems on Computational Grids|The quadratic assignment problem (QAP) is among the hardest combinatorial optimization problems. Some instances of size n = 30 have remained unsolved for decades. The solution of these problems requires both improvements in mathematical programming algorithms and the utilization of powerful computational platforms. In this article we describe a novel approach to solve QAPs using a state-of-the-art branch-and-bound algorithm running on a federation of geographically distributed resources known as a computational grid. Solution of QAPs of unprecedented complexity, including the nug30, kra30b, and tho30 instances, is reported.
869|Resource Management through Multilateral Matchmaking|Federated distributed systems present new challenges to resource management, which cannot be met by conventional systems that employ relatively static resource models and centralized allocators. We previously argued that Matchmaking provides an elegant and robust resource management solution for these highly dynamic environments [5]. Although powerful and flexible, multiparty policies (e.g., co-allocation) cannot be accomodated by Matchmaking. In this paper we present Gang-Matching, a multilateral matchmaking formalism to address this deficiency.
870|Interfacing Condor and PVM to harness the cycles of workstation clusters|A continuing challenge to the scientific research and engineering communities is how to fully utilize computational hardware. In particular, the proliferation of clusters of high performance workstations has become an increasingly attractive source of compute power. Developments to take advantage of this environment have previously focused primarily on managing the resources, or on providing interfaces so that a number of machines can be used in parallel to solve large problems. Both approaches are desirable, and indeed should be complementary. Unfortunately, the resource management and parallel processing systems are usually developed by independent groups, and they usually do not interact well together. To bridge this gap, we have developed a framework for interfacing these two sorts of systems. Using this framework, we have interfaced PVM, a popular system for parallel programming with Condor, a powerful resource management system. This combined system is operational, and we have ma...
871|Parrot: Transparent User-Level Middleware for Data Intensive Computing|Distributed computing continues to be an alphabet-soup of services and protocols for managing computation and storage. To live in this environment, applications require middleware that can transparently adapt standard interfaces to new distributed systems; such software is known as an interposition agent. In this paper, we present several lessons learned about interposition agents via a progressive study of design possibilities. Although performance is an important concern, we pay special attention to less tangible issues such as portability, reliability, and compatibility. We begin with a comparison of seven methods of interposition, focusing on one method, the debugger trap, that requires special techniques to achieve acceptable performance on popular operating systems. Using this method, we implement a complete interposition agent, Parrot, that splices existing remote I/O systems into the namespace of standard applications. The primary design problem of Parrot is the mapping of fixed application semantics into the semantics of the available I/O systems. We offer a detailed discussion of how errors and other unexpected conditions must be carefully managed in order to keep this mapping intact. We conclude with a evaluation of the performance of the I/O protocols employed by Parrot, and use an Andrew-like benchmark to demonstrate that semantic differences have consequences in performance. 1. 
872|Protocols and services for distributed data-intensive science|Abstract. We describe work being performed in the Globus project to develop enabling protocols and services for distributed data-intensive science. These services include: * High-performance, secure data transfer protocols based on FTP, plus a range of libraries and tools that use these protocols * Replica catalog services supporting the creation and location of file replicas in distributed systems These components leverage the substantial body of &#034;Grid &#034; services and protocols developed within the Globus project and by its collaborators, and are being used in a number of data-intensive application projects.
873|Matchmaking Frameworks for Distributed Resource Management|Federated distributed systems present new challenges to resource management. Conventional resource managers are based on a relatively static resource model and a centralized allocator that assigns resources to customers. Distributed envi-ronments, particularly those built to support high-throughput computing (HTC), are often characterized by distributed management and distributed ownership. Distributed management introduces resource heterogeneity: Not only the set of available resources, but even the set of resource types is constantly changing. Distributed ownership introduces policy heterogeneity: Each resource may have its own idiosyncratic allocation policy. We propose a resource management framework based on a matchmaking paradigm to address these shortcomings. Matchmaking services enable discov-ery and exchange of goods and services in marketplaces. Agents that provide or require services advertise their presence by publishing constraints and pref-erences on the entities they would like to be matched with, as well as their own
874|Cheap cycles from the desktop to the dedicated cluster: combining opportunistic and dedicated scheduling with Condor|Clusters of commodity PC hardware running Linux are becoming widely used as computational resources. Most software for controlling clusters relies on dedicated scheduling algorithms. These algorithms assume the constant availability of resources to compute fixed schedules. Unfortunately, due to hardware and software failures, dedicated resources are not always available over the long-term. Moreover, these dedicated scheduling solutions are only applicable to certain classes of jobs, and they can only manage clusters or large SMP machines. The Condor High Throughput Computing System overcomes these limitations by combining aspects of dedicated and opportunistic scheduling into a single system. Both parallel and serial jobs are managed at the same time, allowing a simpler interface for the user and better resource utilization. This paper describes the Condor system, defines opportunistic scheduling, explains how Condor supports MPI jobs with a combination of dedicated and opportunistic scheduling, and shows the advantages gained by such an approach. An exploration of future work in these areas concludes the paper. By using both desktop workstations and dedicated clusters, Condor harnesses all available computational power to enable the best possible science at a low cost.  1 
875|Providing Resource Management Services to Parallel Applications|Because resource management (RM) services are vital to the performance of parallel applications, it is essential that parallel programming environments (PPEs) and RM systems work together. We believe that no single RM system is always the best choice for every application and every computing environment. Therefore, the interface between the PPE and the resource manager must be flexible enough to allow for customization and extension based on the environment. We present a framework for interfacing general PPEs and RM systems. This framework is based on clearly defining the responsibilities of these two components of the system. This framework has been applied to PVM, and two separate instances of RM systems have been implemented. One behaves exactly as PVM always has, while the second uses Condor to extend the set of RM services available to PVM applications. 1 Introduction To fulfill the promises of high performance computing, parallel applications must be provided with effective reso...
876|Gathering at the well: Creating communities for grid I/O|Grid applications have demanding I/O needs. Schedulers must bring jobs and data in close proximity in order to satisfy throughput, scalability, and policy requirements. Most systems accomplish this by making either jobs or data mobile. We propose a system that allows jobs and data to meet by binding execution and storage sites together into I/O communities which then participate in the wide-area system. The relationships between participants in a community may be expressed by the ClassAd framework. Extensions to the framework allow community members to express indirect relations. We demonstrate our implementation of I/O communities by improving the performance of a key high-energy physics simulation on an international distributed system. 1.
877|Bypass: A Tool for Building Split Execution Systems|Split execution is a common model for providing a friendly environment on a foreign machine. In this model, a remotely executing process sends some or all of its system calls back to a home environment for execution. Unfortunately, hand-coding split execution systems for experimentation and research is difficult and error-prone. We have built a tool, Bypass, for quickly producing portable and correct split execution systems for unmodified legacy applications. We demonstrate Bypass by using it to transparently connect a POSIX application to a simple data staging system based on the Globus toolkit. 1. Introduction The split execution model allows a process running on a foreign machine to behave as if it were running on its home machine. Split execution generally involves three software components: an application, an agent, and a shadow. Figure 1 shows these components. Kernel Agent Application Local System Calls Calls System Trapped Kernel Shadow Local System Calls Other...
878|Utilizing Widely Distributed Computational Resources Efficiently with Execution Domains|Wide-area computational grids have the potential to provide large amounts of computing capacity to the scientific community. Realizing this potential requires intelligent data management, enabling applications to harness remote computing resources with minimal remote data access overhead. We define execution domains, a framework which defines an affinity between CPU and data resources in the grid, so applications are scheduled to run on CPUs which have the needed access to datasets and storage devices. The framework also includes domain managers, agents which dynamically adjust the execution domain configuration to support the efficient execution of grid applications. In this paper, we present the execution domain framework and show how we apply it in the Condor resource management system.
879|Error Scope on a Computational Grid: Theory and Practice|Error propagation is a central problem in grid computing. We re-learned this while adding a Java feature to the Condor computational grid. Our initial experience with the system was negative, due to the large number of new ways in which the system could fail. To reason about this problem, we developed a theory of error propagation. Central to our theory is the concept of an error&#039;s scope, defined as the portion of a system that it invalidates. With this theory in hand, we recognized that the expanded system did not properly consider the scope of errors it discovered. We modified the system according to our theory, and succeeded in making it a more robust platform for distributed computing.
880|The DBC: Processing Scientific Data Over the Internet|We present the Distributed Batch Controller (DBC), a system built to support batch processing of large scientific datasets. The DBC implements a federation of autonomous workstation pools, which may be widely-distributed. Individual batch jobs are executed using idle workstations in these pools. Input data are staged to the pool before processing begins. We describe the architecture and implementation of the DBC, and present the results of experiments in which it is used to perform image compression. 1 Introduction  In this paper we present the DBC (Distributed Batch Controller), a system that processes data using widely-distributed computational resources. The DBC was built as a tool for enriching scientific data stored in two mass storage systems at NASA&#039;s Goddard Space Flight Center (GSFC). Enriching data means processing it to make it more useful. For example, satellite images may be classified according to some domain-specific criteria. These classifications can then be stored as ...
881| View Interpolation for Image Synthesis |Image-space simplifications have been used to accelerate the calculation of computer graphic images since the dawn of visual simulation. Texture mapping has been used to provide a means by which images may themselves be used as display primitives. The work reported by this paper endeavors to carry this concept to its logical extreme by using interpolated images to portray three-dimensional scenes. The special-effects technique of morphing, which combines interpolation of texture maps and their shape, is applied to computing arbitrary intermediate frames from an array of prestored images. If the images are a structured set of views of a 3D object or scene, intermediate frames derived by morphing can be used to approximate intermediate 3D transformations of the object or scene. Using the view interpolation approach to synthesize 3D scenes has two main advantages. First, the 3D representation of the scene may be replaced with images. Second, the image synthesis time is independent of the scene complexity. The correspondence between images, required for the morphing method, can be predetermined automatically using the range data associated with the images. The method is further accelerated by a quadtree decomposition and a view-independent visible priority. Our experiments have shown that the morphing can be performed at interactive rates on today’s high-end personal computers. Potential applications of the method include virtual holograms, a walkthrough in a virtual environment, image-based primitives and incremental rendering. The method also can be used to greatly accelerate the computation of motion blur and soft shadows cast by area light sources.
882|Casting Curved Shadows on Curved Surfaces|Shadowing has historically been used to increase the intelligibility of scenes in electron micros-copy and aerial survey. Various methods have been published for the determination of shadows in com-puter synthesized scenes. The display of shadows may make the shape and relative position of objects in such scenes more comprehensible; it is a tech-nique lending vividness and realism to computer an-imation. To date, algorithms for the determination of sha-dows have been restricted to scenes constructed of planar polygons. A simple algorithm is described which utilizes Z-buffer visible surface computation to display shadows cast by objects modelled of smooth surface patches. The method can be applied
883|Visibility Preprocessing For Interactive Walkthroughs|The number of polygons comprising interesting architectural models is many more than can be rendered at interactive frame rates. However, due to occlusion by opaque surfaces (e.g., walls), only a small fraction of atypical model is visible from most viewpoints. We describe a method of visibility preprocessing that is efficient and effective for axis-aligned or axial architectural models, A model is subdivided into rectangular cc//.$whose boundaries coincide with major opaque surfaces, Non-opaque p(~rtc~/.rare identified rm cell boundaries. and used to form ana~ju{~’n~y,q)f~/&gt;//con nectingthe cells nfthesubdivisicm. Next. theccl/-r/~-cc/ / visibility is computed for each cell of the subdivisirrn, by linking pairs of cells between which unobstructed.si,q/~t/inr. ~exist. During an interactive ww/krhrm/,q/~phase, an observer with a known ~~sition and\it)M~~~)~t&gt;mov esthrc&gt;ughthe model. At each frame, the cell containingthe observer is identified, and the contents {]fp{&gt;tentially visible cells are retrieved from storage. The set of potentially visible cells is further reduced by culling it against the observer’s view cone, producing the ~)yt&gt;-r~]-t(&gt;// \ i,$ibi/ify, The contents of the remaining visible cells arc then sent to a graphics pipeline for hidden-surface removal and rendering, Tests on moderately complex 2-D and 3-D axial models reveal substantially reduced rendering loads.
884|A progressive refinement approach to fast radiosity image generation|A reformulated radiosity algorithm is presented that produces initial images in time linear to the number of patches. The enormous memory costs of the radiosity algorithm are also elim-inated by computing form-factors on-the-fly. The technique is based on the approach of rendering by progressive refinement. The algorithm provides a useful solution almost immediately which progresses gracefully and continuously to the complete radiosity solution. In this way the competing demands of real-ism and interactivity are accommodated. The technique brings the use of radiosity for interactive rendering within reach and has implications for the use and development of current and future graphics workstations.
885|Towards image realism with interactive update rates in complex virtual building environments|Two strategies, pre-computation before display and adaptive refinement during display, are used to combine interactivity with high image quality in a virtual building simulation. Pre-computation is used in two ways. The hidden-surface problem is partially solved by automatically pre-computing potentially visible sets of the model for sets of related viewpoints. Rendering only the potentially visible subset associated with the current viewpoint, rather than the entire model, produces significant speedups on real building models. Solutions for the radiosity lighting model are pre-computed for up to twenty different sets of lights. Linear combinations of these solutions can be manipulated in real time. We use adaptive refinement to trade image realism for interactivity as the situation requires. When the user is stationary we replace a coarse model using few polygons with a more detailed model. Image-level linear interpolation smooths the transition between differing levels of image realism.
886|Incremental Radiosity: An Extension of Progressive Radiosity to an Interactive Image Synthesis System|Apple Computer, Inc. Traditional radiosity methods can compute the illumination for a scene independent of the view position. However, if any part of the scene geometry is changed, the radiosity process will need to be repeated from scratch. Since the radiosity
887|A Novel Approach to Graphics|We propose a new, memory-based approach to graphic animation of 2D and 3D objects. Instead of the standard paradigm in computer graphics of 3D modeling, physical simulation and rendering we prop- ose to use a set of 2D views to synthesize a network that generates desired images and image sequences. The approach is based on a new approximation technique which can be regarded as a scheme for learning from examples. c fl Massachusetts Institute of Technology, 1993 This memo is a slightly modified version of an unpublished manuscript (May, 1990) that represents the basis for a patent application on &#034;Memory Based Method and Apparatus for Computer Graphics,&#034; filed by the Massachusetts Institute of Technology and the Istituto Trentino di Cultura, Trento, Italy, on 13 January, 1992, No. 07/819,767. The memo describes research done at I.R.S.T. in 1990. Support for the A.I. Laboratory&#039;s artificial intelligence research is provided by ONR contract N00014--91--J--4038. Tomaso Poggio is supported b...
888|OPTICS: Ordering Points To Identify the Clustering Structure|Cluster analysis is a primary method for database mining. It is either used as a stand-alone tool to get insight into the distribution of a data set, e.g. to focus further analysis and data processing, or as a preprocessing step for other algorithms operating on the detected clusters. Almost all of the well-known clustering algorithms require input parameters which are hard to determine but have a significant influence on the clustering result. Furthermore, for many real-data sets there does not even exist a global parameter setting for which the result of the clustering algorithm describes the intrinsic clustering structure accurately. We introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings. It is a versatile basis for both automatic and interactive cluster analysis. We show how to automatically and efficiently extract not only ‘traditional ’ clustering information (e.g. representative points, arbitrary shaped clusters), but also the intrinsic clustering structure. For medium sized data sets, the cluster-ordering can be represented graphically and for very large data sets, we introduce an appropriate visualization technique. Both are suitable for interactive exploration of the intrinsic clustering structure offering additional insights into the distribution and correlation of the data.
889|Some methods for classification and analysis of multivariate observations|The main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called &#039;k-means, &#039; appears to give partitions which are reasonably
890|A density-based algorithm for discovering clusters in large spatial databases with noise|Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.
891|Automatic Subspace Clustering of High Dimensional Data|Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate clusters in large high dimensional datasets.
892|Efficient and Effective Clustering Methods for Spatial Data Mining|Spatial data mining is the discovery of interesting relationships and characteristics that may exist implicitly in spatial databases. In this paper, we explore whether clustering methods have a role to play in spatial data mining. To this end, we develop a new clustering method called CLARANS which is based on randomized search. We also de- velop two spatial data mining algorithms that use CLARANS. Our analysis and experiments show that with the assistance of CLARANS, these two algorithms are very effective and can lead to discoveries that are difficult to find with current spatial data mining algorithms.
893|BIRCH: an efficient data clustering method for very large databases|Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely st,udied problems in this area is the identification of clusters, or deusel y populated regions, in a multi-dir nensional clataset. Prior work does not adequately address the problem of large datasets and minimization of 1/0 costs. This paper presents a data clustering method named Bfll (;”H (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and clynamicall y clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i. e., available memory and time constraints). BIRCH can typically find a goocl clustering with a single scan of the data, and improve the quality further with a few aclditioual scans. BIRCH is also the first clustering algorithm proposerl in the database area to handle “noise) ’ (data points that are not part of the underlying pattern) effectively. We evaluate BIRCH’S time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIR (;’H versus CLARA NS, a clustering method proposed recently for large datasets, and S11OW that BIRCH is consistently 1
894|An Efficient Approach to Clustering in Large Multimedia Databases with Noise|Several clustering algorithms can be applied to clustering in large multimedia databases. The effectiveness and efficiency of the existing algorithms, however, is somewhat limited, since clustering in multimedia databases requires clustering high-dimensional feature vectors and since multimedia databases often contain large amounts of noise. In this paper, we therefore introduce a new algorithm to clustering in large multimedia databases called DENCLUE (DENsitybased CLUstEring). The basic idea of our new approachis  to model the overall point density analytically as the sum of influence functions of the data points. Clusters can then be identified by determining density-attractors and clusters of arbitrary shape can be easily described by a simple equation based on the overall density function. The advantages of our new approach are (1) it has a firm mathematical basis, (2) it has good clustering properties in data sets with large amounts of noise, (3) it allows a compact mathematical ...
895|Wavecluster: A multi-resolution clustering approach for very large spatial databases|Many applications require the management of spatial data. Clustering large spatial databases is an important problem which tries to find the densely populated regions in the feature space to be used in data mining, knowledge discovery, or efficient information retrieval. A good clustering approach should be efficient and detect clusters of arbitrary shape. It must be insensitive to the outliers (noise) and the order of input data. We pro-pose WaveCluster, a novel clustering approach based on wavelet transforms, which satisfies all the above requirements. Using multi-resolution property of wavelet transforms, we can effectively identify arbitrary shape clus-ters at different degrees of accuracy. We also demonstrate that WaveCluster is highly effi-cient in terms of time complexity. Experi-mental results on very large data sets are pre-sented which show the efficiency and effective-ness of the proposed approach compared to the other recent clustering methods.
896|A Fast Clustering Algorithm to Cluster Very Large Categorical Data Sets in Data Mining |Partitioning a large set of objects into homogeneous clusters is a fundamental operation in data mining. The k-means algorithm is best suited for implementing this operation because of its efficiency in clustering large data sets. However, working only on numeric values limits its use in data mining because data sets in data mining often contain categorical values. In this paper we present an algorithm, called k-modes, to extend the k-means paradigm to categorical domains. We introduce new dissimilarity measures to deal with categorical objects, replace means of clusters with modes, and use a frequency based method to update modes in the clustering process to minimise the clustering cost function. Tested with the well known soybean disease data set the algorithm has demonstrated a very good classification performance. Experiments on a very large health insurance data set consisting of half a million records and 34 categorical attributes show that the algorithm is scalable in terms of both the number of clusters and the number of records.  
897|Knowledge Discovery in Large Spatial Databases: Focusing Techniques for Efficient Class Identification|. Both, the number and the size of spatial databases are rapidly growing because of the large amount of data obtained from satellite images, X-ray crystallography or other scientific equipment. Therefore, automated knowledge discovery becomes more and more important in spatial databases. So far, most of the methods for knowledge discovery in databases (KDD) have been based on relational database systems. In this paper, we address the task of class identification in spatial databases using clustering techniques. We put special emphasis on the integration of the discovery methods with the DB interface, which is crucial for the efficiency of KDD on large databases. The key to this integration is the use of a well-known spatial access method, the R*-tree. The focusing component of a KDD system determines which parts of the database are relevant for the knowledge discovery task. We present several strategies for focusing: selecting representatives from a spatial database, focusing on the re...
898|Pixel-oriented Database Visualizations|In this paper, we provide an overview of several pixel-oriented visualization techniques which have been developed over the last years to support an effective querying and exploration of large databases. Pixel-oriented techniques use each pixel of the display to visualize one data value and therefore allow the visualization of the largest amount of data possible. The techniques may be divided into query-independent techniques which directly visualize the data (or a certain portion of it) and query-dependent techniques which visualize the relevance of the data with respect to a specific query. An example for the class of query-independent techniques is the recursive pattern technique which is based on a generic recursive scheme generalizing a wide range of pixel-oriented arrangements for visualizing large databases. Examples for the class of query-dependent techniques are the generalized spiral and circle-segments techniques, which visualize the distances with respect to a database quer...
899|The BANG-Clustering System: Grid-Based Data Analysis|. For the analysis of large images the clustering of the data set is a common technique to identify correlation characteristics of the underlying value space. In this paper a new approach to hierarchical clustering of very large data sets is presented. The BANG-Clustering system presented in this paper is a novel approach to hierarchical data analysis. It is based on the BANG-Clustering method ([Sch96]) and uses a multidimensional grid data structure to organize the value space surrounding the pattern values. The patterns are grouped into blocks and clustered with respect to the blocks by a topological neighbor search algorithm. 1 Introduction  Clustering methods are extremely important for explorative data analysis, which is an important approach for the analysis of images. Previously presented algorithms can be divided into hierarchical algorithms, e.g. single-linkage, completelinkage, etc. and partitional algorithms, e.g. K-MEANS, ISODATA, etc. (see [DJ80]). All of these methods suf...
900|Image Quilting for Texture Synthesis and Transfer|We present a simple image-based method of generating novel visual appearance in which a new image is synthesized by stitching together small patches of existing images. We call this process image  quilting. First, we use quilting as a fast and very simple texture synthesis algorithm which produces surprisingly good results for a wide range of textures. Second, we extend the algorithm to perform texture transfer -- rendering an object with a texture taken from a different object. More generally, we demonstrate how an image can be re-rendered in the style of a different image. The method works directly on the images and does not require 3D information.
901|Texture Synthesis by Non-parametric Sampling|A non-parametric method for texture synthesis is proposed. The texture synthesis process grows a new image outward from an initial seed, one pixel at a time. A Markov random field model is assumed, and the conditional distribution of a pixel given all its neighbors synthesized so far is estimated by querying the sample image and finding all similar neighborhoods. The degree of randomness is controlled by a single perceptually intuitive parameter. The method aims at preserving as much local structure as possible and produces good results for a wide variety of synthetic and real-world textures. 1. Introduction Texture synthesis has been an active research topic in computer vision both as a way to verify texture analysis methods, as well as in its own right. Potential applications of a successful texture synthesis algorithm are broad, including occlusion fill-in, lossy image and video compression, foreground removal, etc. The problem of texture synthesis can be formulated as follows: let...
902|Fast texture synthesis using tree-structured vector quantization|Figure 1: Our texture generation process takes an example texture patch (left) and a random noise (middle) as input, and modifies this random noise to make it look like the given example texture. The synthesized texture (right) can be of arbitrary size, and is perceived as very similar to the given example. Using our algorithm, textures can be generated within seconds, and the synthesized results are always tileable. Texture synthesis is important for many applications in computer graphics, vision, and image processing. However, it remains difficult to design an algorithm that is both efficient and capable of generating high quality results. In this paper, we present an efficient algorithm for realistic texture synthesis. The algorithm is easy to use and requires only a sample texture as input. It generates textures with perceived quality equal to or better than those produced by previous techniques, but runs two orders of magnitude faster. This permits us to apply texture synthesis to problems where it has traditionally been considered impractical. In particular, we have applied it to constrained synthesis for image editing and temporal texture generation. Our algorithm is derived from Markov Random Field texture models and generates textures through a deterministic searching process. We accelerate this synthesis process using tree-structured vector quantization.
903|Pyramid-Based Texture Analysis/Synthesis|This paper describes a method for synthesizing images that match the texture appearanceof a given digitized sample. This synthesis is completely automatic and requires only the &#034;target&#034; texture as input. It allows generation of as much texture as desired so that any object can be covered. It can be used to produce solid textures for creating textured 3-d objects without the distortions inherent in texture mapping. It can also be used to synthesize texture mixtures, images that look a bit like each of several digitized samples. The approach is based on a model of human texture perception, and has potential to be a practically useful tool for graphics applications. 1 Introduction  Computer renderings of objects with surface texture are more interesting and realistic than those without texture. Texture mapping [15] is a technique for adding the appearance of surface detail by wrapping or projecting a digitized texture image ontoa surface. Digitized textures can be obtained from a variety ...
904|Image analogies|Figure 1 An image analogy. Our problem is to compute a new “analogous ” image B ' that relates to B in “the same way ” as A ' relates to A. Here, A, A ' , and B are inputs to our algorithm, and B ' is the output. The full-size images are shown in Figures 10 and 11. This paper describes a new framework for processing images by example, called “image analogies. ” The framework involves two stages: a design phase, in which a pair of images, with one image purported to be a “filtered ” version of the other, is presented as “training data”; and an application phase, in which the learned filter is applied to some new target image in order to create an “analogous” filtered result. Image analogies are based on a simple multiscale autoregression, inspired primarily by recent results in texture synthesis. By choosing different types of source image pairs as input, the framework supports a wide variety of “image filter ” effects, including traditional image filters, such as blurring or embossing; improved texture synthesis, in which some textures are synthesized with higher quality than by previous approaches; super-resolution, in which a higher-resolution image is inferred from a low-resolution source; texture transfer, in which images are “texturized ” with some arbitrary source texture; artistic filters, in which various drawing and painting styles are synthesized based on scanned real-world examples; and texture-by-numbers, in which realistic scenes, composed of a variety of textures, are created using a simple painting interface.
905|A Parametric Texture Model based on Joint Statistics of Complex Wavelet Coefficients|We present a universal statistical model for texture images in the context of an overcomplete complex wavelet transform. The model is parameterized by a set of statistics computed on pairs of coefficients corresponding to basis functions at adjacent spatial locations, orientations, and scales. We develop an efficient algorithm for synthesizing random images subject to these constraints, by iteratively projecting onto the set of images satisfying each constraint, and we use this to test the perceptual validity of the model. In particular, we demonstrate the necessity of subgroups of the parameter set by showing examples of texture synthesis that fail when those parameters are removed from the set. We also demonstrate the power of our model by successfully synthesizing examples drawn from a diverse collection of artificial and natural textures.
906|Synthesizing Natural Textures|We present a simple texture synthesis algorithm that is well-suited for a specific class of naturally occurring textures. This class includes quasi-repeating patterns consisting of small objects of familiar but irregular size, such as flower fields, pebbles, forest undergrowth, bushes and tree branches. The algorithm starts from a sample image and generates a new image of arbitrary size the appearance of which is similar to that of the original image. This new image does not change the basic spatial frequencies the original image; instead it creates an image that is a visually similar, and is of a size set by the user. This method is fast and its implementation is straightforward. We extend the algorithm to allow direct user input for interactive control over the texture synthesis process. This allows the user to indicate large-scale properties of the texture appearance using a standard painting-style interface, and to choose among various candidate textures the algorithm can create by performing different number of iterations.
907|Multiresolution Sampling Procedure for Analysis and Synthesis of Texture Images|This paper outlines a technique for treating input texture images as probability density estimators from which new textures, with similar appearance and structural properties, can be sampled. In a two-phase process, the input texture is first analyzed by measuring the joint occurrence of texture discrimination features at multiple resolutions. In the second phase, a new texture is synthesized by sampling successive spatial frequency bands from the input texture, conditioned on the similar joint occurrence of features at lower spatial frequencies. Textures synthesized with this method more successfully capture the characteristics of input textures than do previous techniques.  1 Introduction  Synthetic texture generation has been an increasingly active research area in computer graphics. The primary approach has been to develop specialized procedural models which emulate the generative process of the texture they are trying to mimic. For example, models based on reaction-diffusion inter...
908|Lapped Textures |Figure 1: Four different textures pasted on the bunny model. The last picture illustrates changing local orientation and scale on the body. We present a method for creating texture over an arbitrary surface mesh using an example 2D texture. The approach is to identify interesting regions (texture patches) in the 2D example, and to repeatedly paste them onto the surface until it is completely covered. We call such a collection of overlapping patches a lapped texture. It is rendered using compositing operations, either into a traditional global texture map during a preprocess, or directly with the surface at runtime. The runtime compositing approach avoids resampling artifacts and drastically reduces texture memory requirements. Through a simple interface, the user specifies a tangential vector field over the surface, providing local control over the texture scale, and for anisotropic textures, the orientation. To paste a texture patch onto the surface, a surface patch is grown and parametrized over texture space. Specifically, we optimize the parametrization of each surface patch such that the tangential vector field aligns everywhere with the standard frame of the texture patch. We show that this optimization is solved efficiently as a sparse linear system.
909|Orientable Textures for Image-Based Pen-and-Ink Illustration  |We present an interactive system for creating pen-and-ink-style line drawings from greyscale images in which the strokes of the renderedillustrationfollowthefeaturesoftheoriginalimage. The user, via new interaction techniquesforeditingadirectionfield, specifies anorientationforeachregionoftheimage; thecomputerdrawsorientedstrokes, basedonauser-specifiedsetofexamplestrokes,that achieve the same tone as the image via a new algorithm that compares anadaptively-blurred version ofthecurrent illustrationtothe targettoneimage. By aligning the direction field with surface orientations of the objects in the image, the user can create textures that appear attached to those objects instead of merely conveying their darkness. The result is a more compelling pen-and-ink illustration thanwas previously possible from 2D reference imagery.  
911|Mosaics of Scenes with Moving Objects|Image mosaics are useful for a variety of tasks in vision and computer graphics. A particularly convenient way to generate mosaics is by `stitching&#039; together many ordinary photographs. Existing algorithms focus on capturing static scenes. This paper presents a complete system for creating visually pleasing mosaics in the presence of moving objects. There are three primary contributions. The first component of our system is a registration method that remains unbiased by movement--- the Mellin transform is extended to register images related by a projective transform. Second, an efficient method for finding a globally consistent registration of all images is developed. By solving a linear system of equations, derived from many pairwise registration matrices, we find an optimal global registration. Lastly, a new method of compositing images is presented. Blurred areas due to moving objects are avoided by segmenting the mosaic into disjoint regions and sampling pixels in each region from a...
912|Novel Cluster-Based Probability Model for Texture Synthesis, Classification, and Compression|We present a new probabilistic modeling technique for high-dimensional vector sources, and consider its application to the problems of texture synthesis, classification, and compression. Our model combines kernel estimation with clustering, to obtain a semiparametric probability mass function estimate which summarizes --- rather than contains --- the training data. Because the model is cluster based, it is inferable from a limited set of training data, despite the model&#039;s high dimensionality. Moreover, its functional form allows recursive implementation that avoids exponential growth in required memory as the number of dimensions increases. Experimental results are presented for each of the three applications considered.  1. INTRODUCTION  In many information processing tasks individual data samples exhibit a great deal of statistical interdependence, and should be treated jointly (e.g., in vectors) rather than separately. For some tasks this requires modeling vectors probabilistically....
913|A Non-Hierarchical Procedure for Re-Synthesis of Complex Textures|A procedure is described for synthesizing an image with the same texture as a given input image.
914|Multi-color and artistic dithering|A multi-color dithering algorithm is proposed, which converts a barycentric combination of color intensities into a multi-color nonoverlapping surface coverage. Multi-color dithering is a generalization of standard bi-level dithering. Combined with tetrahedral color separation, multi-color dithering makes it possible to print images made of a set of non-standard inks. In contrast to most previous color halftoning methods, multi-color dithering ensures by construction that the different selected basic colors are printed side by side. Multi-color dithering is applied to generate color images whose screen dots are made of artistic shapes (letters, symbols, ornaments, etc.). Two dither matrix postprocessing techniques are developed, one for enhancing the visibility of screen motives and one for the local equilibration of large dither matrices. The dither matrix equilibration process corrects disturbing local intensity variations by taking dot gain and the human visual system transfer function into account. Thanks to the combination of the presented techniques, high quality images can be produced, which incorporate at the micro level the desired artistic screens and at the macro level the full color image. Applications include designs for advertisements and posters as well as security printing. Multi-color dithering also offers new perspectives for printing with special inks, such as fluorescent and metallic inks.
915|Efficient Variants of the ICP Algorithm|The ICP (Iterative Closest Point) algorithm is widely used for geometric alignment of three-dimensional models when an initial estimate of the relative pose is known. Many variants of ICP have been proposed, affecting all phases of the algorithm from the selection and matching of points to the minimization strategy. We enumerate and classify many of these variants, and evaluate their effect on the speed with which the correct alignment is reached. In order to improve convergence for nearly-flat meshes with small features, such as inscribed surfaces, we introduce a new variant based on uniform sampling of the space of normals. We conclude by proposing a combination of ICP variants optimized for high speed. We demonstrate an implementation that is able to align two range images in a few tens of milliseconds, assuming a good initial guess. This capability has potential application to real-time 3D model acquisition and model-based tracking.
916|Closed-form solution of absolute orientation using unit quaternions|Finding the relationship between two coordinate systems using pairs of measurements of the coordinates of a number of points in both systems is a classic photogrammetric task. It finds applications in stereophotogrammetry and in robotics. I present here a closed-form solution to the least-squares problem for three or more points. Currently various empirical, graphical, and numerical iterative methods are in use. Derivation of the solution is simplified by use of unit quaternions to represent rotation. I emphasize a symmetry property that a solution to this problem ought to possess. The best translational offset is the difference between the centroid of the coordinates in one system and the rotated and scaled centroid of the coordinates in the other system. The best scale is equal to the ratio of the root-mean-square deviations of the coordinates in the two systems from their respective centroids. These exact results are to be preferred to approximate methods based on measurements of a few selected points. The unit quaternion representing the best rotation is the eigenvector associated with the most positive eigenvalue of a symmetric 4 X 4 matrix. The elements of this matrix are combinations of sums of products of corresponding coordinates of the points. 1.
917|Registering Multiview Range Data to Create 3D Computer Objects|This research deals with the problem of range image registration for the purpose of building surface models of three-dimensional objects. The registration task involves finding the translation and rotation parameters which properly align overlapping views of the object so as to reconstruct from these partial surfaces, an integrated surface representation of the object. The approach taken is to express the registration task as an optimization problem. We define a function which measures the quality of the alignment between the partial surfaces contained in two range images as produced by a set of motion parameters . This function computes a sum of Euclidean distances between a set of control points on one of the surfaces to corresponding points on the other. The strength of this approach resides in the method used to determine point correspondences across range images. It is based on reversing the rangefinder calibration process, resulting in a set of equations which can be used to dire...
918|RANSAC-based DARCES: A New Approach for Fast Automatic Registration of Partially Overlapping Range Images |Registration of two partially-overlapping range images taken from different views is an important task in 3D computer vision. In general, if there is no initial knowledge about the poses of these two views, the information used for solving the 3D registration problem is mainly the 3D shape of the common parts of the two partially-overlapping data sets.
919|Fast global registration of 3D sampled surfaces using a multi-Z-buffer technique|We present a new method for the global registration of several overlapping 3D surfaces sampled on an object. The method is based on the ICP algorithm and on a segmentation of the sampled points in an optimized set of z-buffers. This multi-z-buffer technique provides a 3D space partitioning which greatly accelerates the search of the nearest neighbours in the establishment of the point-to-point correspondence between overlapping surfaces. Then a randomized iterative registration is processed on the surface set. We have tested an implementation of this technique on real sampled surfaces. It appears to be rapid, accurate and robust, especially in the case of highly curved objects. 1.
920|Surface Registration by Matching Oriented Points|For registration of 3-D free-form surfaces we have developed a representation which requires no knowledge of the transformation between views. The representation comprises descriptive images associated with oriented points on the surface of an object. Constructed using single point bases, these images are data level shape descriptions that are used for efficient matching of oriented points. Correlation of images is used to establish point correspondences between two views; from these correspondences a rigid transformation that aligns the views is calculated. The transformation is then refined and verified using a modified iterative closest point algorithm. To demonstrate the generality of our approach, we present results from multiple sensing domains.  1. Introduction  Surface registration is the process that aligns 3-D data sets acquired from different view points or at different times. A common application of surface registration is to spatially reconcile multiple views of a scene in...
921|Optimal Registration of Object Views Using Range Data|This paper deals with robust registration of object views in the presence of uncertainties and noise in depth data. Errors in registration of multiple views of a 3D object severely affect view integration during automatic construction of object models. We derive a minimum variance estimator (MVE) for computing the view transformation parameters accurately from range data of two views of a 3D object. The results of our experiments show that view transformation estimates obtained using MVE are significantly more accurate than those computed with an unweighted error criterion for registration.  Key words: Image registration, view transformation estimation, view integration, automatic object modeling, 3D free-form objects, range data. 1 Introduction  An important issue in the design of 3D object recognition systems is building models of physical objects. Object models are extensively used for synthesizing and predicting object appearances from desired viewpoints and also for recognizing th...
922|Registration and Integration of Multiple Object Views for 3D Model Construction|Automatic 3D object model construction is important in  applications ranging from manufacturing to entertainment, since CAD models of existing objects may be either unavailable or unusable. We describe a prototype system for automatically registering and  integrating multiple views of objects from range data. The results can  then be used to construct geometric models of the objects. New  techniques for handling key problems such as robust estimation of  transformations relating multiple views and seamless integration of  registered data to form an unbroken surface have been proposed and implemented in the system. Experimental results on real surface data  acquired using a digital interferometric sensor as well as a laser range scanner demonstrate the good performance of our system.
923|Registration of 3-D Partial Surface Models Using Luminance and Depth Information|Textured surface models of three-dimensional objects are gaining importance in computer graphics applications. These models often have to be merged from several overlapping partial models which have to be registered (i.e. the relative transformation between the partial models has to be determined) prior to the merging process. In this paper a method is presented that makes use of both camera-based depth information (e.g. from stereo) and the luminance image. The luminance information is exploited to determine corresponding point sets on the partial surfaces using an optical flow approach. Quaternions are then employed to determine the transformation between the partial models which minimizes the sum of the 3-D Euclidian distances between the corresponding point sets. In order to find corresponding points on the partial surfaces luminance information is linearized. The procedure is iterated until convergence is reached. In contrast to only using depth information, employing luminance sp...
924|Geometrical Cloning 3D Objects via Simultaneous Registration of Multiview Range Images|In this paper, we present a method for the reconstruction of real world objects from multiple range images. One ma-jor contribution of our approach is the simultaneous reg-istration of all range images acquired from different scan-ner views. Thus, registration errors are not accumulated, and it is even possible to reconstruct large objects from an arbitrary number of small range images. The registration process is based on a least-squares approach where a dis-tance metric between the overlapping range images is min-imized. A resolution hierarchy accelerates the registration substantially. After registration, a volumetric model of the object is carved out. This step is based on the idea that no part of the object can lie between the measured surface and the camera of the scanner. With the marching cube algo-rithm a polygonal representation is generated. The accu-racy of this polygonal mesh is improved by moving the ver-tices of the mesh onto the surface implicitly defined by the registered range images. 1.
926|Querying Heterogeneous Information Sources Using Source Descriptions|We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can ask). (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an imp...
927|Answering queries using views|We consider the problem of computing answers to queries by using materialized views. Aside from its potential in optimizing query evaluation, the problem also arises in applications such as Global Information Systems, Mobile Computing and maintaining physical data independence. We consider the problem of nding a rewriting of a query that uses the materialized views, the problem of nding minimal rewritings, and nding complete rewritings (i.e., rewritings that use only the views). We show that all the possible rewritings can be obtained by considering containment mappings from the views to the query, and that the problems we consider are NP-complete when both the query and the views are conjunctive and don&#039;t involve built-in comparison predicates. We show that the problem has two independent sources of complexity (the number of possible containment mappings, and the complexity of deciding which literals from the original query can be deleted). We describe a polynomial time algorithm for nding rewritings, and show that under certain conditions, it will nd the minimal rewriting. Finally, we analyze the complexity of the problems when the queries and views may be disjunctive and involve built-in comparison predicates. 1
928|Retrieving And Integrating Datafrom Multiple Information Sources|With the current explosion of data, retrieving and integrating information  from various sources is a critical problem. Work in multidatabase systems  has begun to address this problem, but it has primarily focused on methods  for communicating between databases and requires significant effort for each  new database added to the system. This paper describes a more general  approach that exploits a semantic model of a problem domain to integrate  the information from various information sources. The information sources  handled include both databases and knowledge bases, and other information  sources (e.g., programs) could potentially be incorporated into the system.  This paper describes how both the domain and the information sources are  modeled, shows how a query at the domain level is mapped into a set of  queries to individual information sources, and presents algorithms for automatically  improving the efficiency of queries using knowledge about both the  domain and the informat...
929|A Softbot-Based Interface to the Internet|this article, we focus on the ideas underlying the softbot-based interface.
930|Optimizing Queries with Materialized Views|While much work has addressed the problem of maintaining materialized views, the important question of optimizing queries in the presence of materialized views has not been resolved. In this paper, we analyze the optimization question and provide a comprehensive and efficient solution. Our solution has the desirable property that it is a simple generalization of the traditional query optimization algorithm. 1 Introduction  The idea of using materialized views for the benefit of improved query processing has been proposed in the literature more than a decade ago. In this context, problems such as definition of views, composition of views, maintenance of views [BC79, KP81, SI84, BLT86, CW91, Rou91, GMS93] have been researched but one topic has been conspicuous by its absence. This concerns the problem of the judicious use of materialized views in answering a query. It may seem that materialized views should be used to evaluate a query whenever they are applicable. In fact, blind applicat...
931|Data Model and Query Evaluation in Global Information Systems|. Global information systems involve a large number of information sources distributed over computer networks. The variety of information sources and disparity of interfaces makes the task of easily locating and efficiently accessing information over the network very cumbersome. We describe an architecture for global information systems that is especially tailored to address the challenges raised in such an environment, and distinguish our architecture from architectures of multidatabase and distributed database systems. Our architecture is based on presenting a conceptually unified view of the information space to a user, specifying rich descriptions of the contents of the information sources, and using these descriptions for optimizing queries posed in the unified view. The contributions of this paper include: (1) we identify aspects of site descriptions that are useful in query optimization; (2) we describe query optimization techniques that minimize the number of information source...
932|Answering queries using templates with binding patterns|When integrating heterogeneous information re-sources, it is often the case that the source is rather limited in the kinds of queries it can answer. If a query is asked of the entire system, we have a new kind of optimization problem, in which we must try to express the given query in terms of the lim-ited query templates that this source can answer. For the case of conjunctive queries, we show how to decide with a nondeterministic polynomial-time al-gorithm whether the given query can be answered. We then extend our results to allow arithmetic comparisons in the given query and in the tem-plates.
933|Query Caching and Optimization in Distributed Mediator Systems|Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source&#039;s performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a costbased optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants  mechanism, which s...
934|A Query Translation Scheme for Rapid Implementation of Wrappers|Wrappers provide access to heterogeneous information sources by converting application queries into source specific queries or commands. In this paper we present a wrapper implementation toolkit that facilitates rapid development of wrappers. We focus on the query translation component of the toolkit, called the converter. The converter takes as input a Query Description and Translation Language (QDTL) description of the queries that can be processed by the underlying source. Based on this description the converter decides if an application query is (a) directly supported, i.e., it can be translated to a query of the underlying system following instructions in the QDTL description; (b) logically supported, i.e., logically equivalent to a directly supported query; (c) indirectly supported, i.e., it can be computed by applying a filter,  automatically generated by the converter, to the result of a directly supported query. 1 Introduction  A wrapper or translator [C  +  94, PGMW95] is a s...
935|Distributed Active Catalogs and Meta-Data Caching in Descriptive Name Services|Today&#039;s global internetworks challenge the ability of name services and other information services to locate data quickly. We introduce a distributed active catalog and meta-data caching for optimizing queries in this environment. Our active catalog constrains the search space for a query by returning a list of data repositories where the answer to the query is likely to be found. Meta-data caching improves performance by keeping frequently used characterizations of the search space close to the user, and eliminating active catalog communication and processing costs. When searching for query responses, our techniques contact only the small percentage of the data repositories with actual responses, resulting in search times of a few seconds. We implemented a distributed active catalog and meta-data caching in a prototype descriptive name service called &#034;Nomenclator. &#034; We present performance results for Nomenclator in a search space of 1000 data repositories. 1. Introduction  Users canno...
936|Using Heterogeneous Equivalences for Query Rewriting in Multidatabase Systems|In order to have significant practical impact on future information systems, multidatabase management systems (MDBMS) must be both flexible and efficient. We consider a MDBMS with a common object-oriented model, based on the ODMG standard, and local databases that may be relational or object-oriented. In this context, query rewriting (for optimization) is made difficult by schematic discrepancy, and the need to model mapping information between the multidatabase and local schemas. We address the flexibility issue by representing the mappings from a local schema to the multidatabase schema, as a set of heterogeneous object equivalences, in a declarative language. Efficiency is obtained by exploiting these equivalences to rewrite multidatabase OQL queries into equivalent, simplified queries on the local schemas. 1 Introduction  The advent of open systems is increasingly stimulating the development of information systems which can provide high-level integration of heterogeneous informatio...
937|The Identification and Resolution of Semantic Heterogeneity in Multidatabase Systems|This paper describes several aspects of the  Remote--Exchange project at USC, which focuses on the controlled sharing and exchange of information among autonomous, heterogeneous database systems. The spectrum of heterogeneity which may exist among the components in a federation of database systems is examined, and an approach to accommodating such heterogeneity is described. An overview of the Remote--Exchange experimental system is provided. 1 Introduction  Consider an environment consisting of a collection of data/knowledge bases and their supporting systems, and in which it is desired to accommodate the controlled sharing and exchange of information among the collection. We shall refer to this as the (interconnected) autonomous heterogeneous database environment. Such environments are extremely common in various application domains, including office information systems, computer-integrated manufacturing systems (with computer-aided design as a subset), personal computing, business a...
938|Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces|A new motion planning method for robots in static workspaces is presented. This method proceeds in two phases: a learning phase and a query phase. In the learning phase, a probabilistic roadmap is constructed and stored as a graph whose nodes correspond to collision-free configurations and whose edges correspond to feasible paths between these configurations. These paths are computed using a simple and fast local planner. In the query phase, any given start and goal configurations of the robot are connected to two nodes of the roadmap; the roadmap is then searched for a path joining these two nodes. The method is general and easy to implement. It can be applied to virtually any type of holonomic robot. It requires selecting certain parameters (e.g., the duration of the learning phase) whose values depend on the scene, that is the robot and its workspace. But these values turn out to be relatively easy to choose, Increased efficiency can also be achieved by tailoring some components of the method (e.g., the local planner) to the considered robots. In this paper the method is applied to planar articulated robots with many degrees of freedom. Experimental results show that path planning can be done in a fraction of a second on a contemporary workstation (=150 MIPS), after learning for relatively short periods of time (a few dozen seconds)
939|Planning Motions with Intentions|We apply manipulation planning to computer animation. A new path planner is presented that automatically computes the collision-free trajectories for several cooperating arms to manipulate a movable object between two configurations. This implemented planner is capable of dealing with complicated tasks where regrasping is involved. In addition, we present a new inverse kinematics algorithm for the human arms. This algorithm is utilized by the planner for the generation  of realistic human arm motions as they manipulate objects. We view our system as a tool for facilitating the production of animation.
940|A Probabilistic Learning Approach to Motion Planning|In this paper a new paradigm for robot motion planning is proposed.  We split the motion planning process into two phases: the learning phase  and the query phase. In the learning phase we construct a probabilistic  roadmap in configuration space. This roadmap is a graph where nodes  correspond to randomly chosen configurations in free space and edges correspond  to simple collision-free motions between the nodes. These simple  motions are computed using a fast local method. The longer we learn, the  denser the roadmap becomes and the better it is for motion planning. In  the query phase we can use this roadmap to find paths between different  pairs of configurations. If a possible path is not found one can always extend  the roadmap by learning further. This gives a very flexible scheme in  which learning time and success for queries can be balanced.  We will demonstrate the power of the paradigm by applying it to various  instances of motion planning : free flying planar robots, plan...
941|Real-Time Robot Motion Planning Using Rasterizing Computer Graphics Hardware|We present a real-time robot motion planner that is fast andcomplete to a resolution. The technique is guaranteed to find a path if one exists at the resolution, and all paths returned are safe. The planner can handle any polyhedral geometry of robot and obstacles, including disjoint and highly concave unions of polyhedra. The planner uses standard graphics hardware to rasterize configuration space obstacles into a series of bitmap slices, and then uses dynamic programming to create a navigation function (a discrete vector-valued function) and to calculate paths in this rasterized space. The motion paths which the planner produces are minimal with respect to an L 1 (Manhattan) distance metric that includes rotation as well as translation. Several examples are shown illustrating the competence of the planner at generating planar rotational and translational plans for complex two and three dimensional robots. Dynamic motion sequences, including complicated and non-obvious backtracking so...
942|Randomized Query Processing in Robot Path Planning (Extended Abstract)  (1995) |The subject of this paper is the analysis of a randomized preprocessing scheme that has been used for query processing in robot path planning. The attractiveness of the scheme stems from its general applicability to virtually any path-planning problem, and its empirically observed success. In this paper we initiate a theoretical basis for explaining this empirical success. Under a simple assumption about the configuration space, we show that it is possible to perform preprocessing following which queries can be answered quickly. En route, we consider related problems on graph connectivity in the evasiveness model, and art-gallery theorems.
943|A Random Approach to Motion Planning|The motion planning problem asks for determining a collision-free path  for a robot amidst a set of obstacles. In this paper we present a new approach  for solving this problem, based on the construction of a random network of  possible motions, connecting the source and goal configuration of the robot.
944|Motion planning with six degrees of freedom by multistrategic bidirectional heuristic free-space enumeration|Abstract-This paper presents a general and efficient method that uses a configuration space for planning a collision-free path among known stationary obstacles for an arbitrarily moving object with six degrees of freedom. The basic approach taken in this method is to restrict the free space concerning path planning and to avoid executing unnecessary collision detections. The six-dimensional configuration space is equally quantized into cells by placing a regular grid, and the cells concerning path planning are enumerated by simultaneously executing multiple search strategies. Search strategies of different characteristics are defined by assigning different values to the coefficients of heuristic functions. The efficiency of each search strategy is evaluated during free-space enumeration, and a more promising one is automatically selected and is preferentially executed. The total number of necessary collision detections for free-space enumeration mainly depends on the most efficient search strategy among the evaluated strategies. Therefore, the free-space cells are efficiently enumerated for an arbitrary moving object in all kinds of working environments. This method has been implemented and has been applied to several examples that have different characteristics. I.
945|Randomized Preprocessing of Configuration Space for Path Planning: Articulated Robots|This paper presents a new approach to path planning for robots with many degrees of freedom (dof) operating in known static environments. The approach consists of a preprocessing and a planning stage. Preprocessing, which is done only once for a given environment, generates a network of randomly, but properly selected, collision-free configurations (nodes). Planning then connects any given initial and final configurations of the robot to two nodes of the network and computes a path through the network between these two nodes. Experiments show that after paying the preprocessing cost (on the order of hundreds of seconds), planning is extremely fast (on the order of a fraction of a second for many difficult examples involving a 10-dof robot). The approach is particularly attractive for many-dof robots which have to perform many successive point-to-point motions in the same environment.
946|Computation of Configuration-Space Obstacles Using the Fast Fourier Transform|This paper presents a new method for computing the configuration-space  map of obstacles that is used in motion-planning algorithms. The method de-  rives from the observation that, when the robot is a rigid object that can only  translate, the configuration space is a convolution of the workspace and the  robot. This convolution is computed with the use of the Fast Fourier Trans-  form (FFT) algorithm. The method is particularly promising for workspaces  with many and/or complicated obstacles, or when the shape of the robot is  not simple. It is an inherently parallel method that can significantly benefit  from existing experience and hardware on the FFT.
947|Focused crawling: a new approach to topic-specific Web resource discovery|The rapid growth of the World-Wide Web poses unprecedented scaling challenges for general-purpose crawlers and search engines. In this paper we describe a new hypertext resource discovery system called a Focused Crawler. The goal of a focused crawler is to selectively seek out pages that are relevant to a pre-defined set of topics. The topics are specified not using keywords, but using exemplary documents. Rather than collecting and indexing all accessible Web documents to be able to answer all possible ad-hoc queries, a focused crawler analyzes its crawl boundary to find the links that are likely to be most relevant for the crawl, and avoids irrelevant regions of the Web. This leads to significant savings in hardware and network resources, and helps keep the crawl more up-to-date.  To achieve such goal-directed crawling, we designed two hypertext mining programs that guide our crawler: a classifier that evaluates the relevance of a hypertext document with respect to the focus topics, ...
948|The Anatomy of a Large-Scale Hypertextual Web Search Engine|In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/

To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date.

Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.
949|Authoritative Sources in a Hyperlinked Environment|The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of contexts on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authoritative ” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages ” that join them together in the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristics for link-based analysis.
950|Text Classification from Labeled and Unlabeled Documents using EM|  This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available. We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve ...
951|Improved algorithms for topic distillation in a hyperlinked environment|Abstract This paper addresses the problem of topic distillation on the World Wide Web, namely, given a typical user query to find quality documents related to the query topic. Connectivity analysis has been shown to be useful in identifying high quality pages within a topic specific graph of hyperlinked documents. The essence of our approach is to augment a previous connectivity analysis based algorithm with content analysis. We identify three problems with the existing approach and devise algorithms to tackle them. The results of a user evaluation are reported that show an improvement of precision at 10 documents by at least 45 % over pure connectivity analysis. 1
952|Enhanced Hypertext Categorization Using Hyperlinks|A major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured search using topic taxonomies, circumvents keyword ambiguity, and improves the quality of search and profile-based routing and filtering. Therefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain highquality semantic clues that are lost upon a purely termbased classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a  relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics. We experimented ...
953|Efficient Crawling Through URL Ordering|In this paper we study in what order a crawler should visit the URLs it has seen, in order to obtain more “important” pages first. Obtaining important pages rapidly can be very useful when a crawler cannot visit the entire Web in a reasonable amount of time. We define several importance metrics, ordering schemes, and performance evaluation measures for this problem. We also experimentally evaluate the ordering schemes on the Stanford University Web. Our results show that a crawler with a good ordering scheme can obtain important pages significantly faster than one without.
954|Automatic Resource Compilation by Analyzing Hyperlink Structure and Associated Text|We describe the design, prototyping and evaluation of ARC, a system for automatically compiling a list of authoritative web resources on any (sufficiently broad) topic. The goal of ARC is to compile resource lists similar to those provided by Yahoo! or Infoseek. The fundamental difference is that these services construct lists either manually or through a combination of human and automated effort, while ARC operates fully automatically. We describe the evaluation of ARC, Yahoo!, and Infoseek resource lists by a panel of human users. This evaluation suggests that the resources found by ARC frequently fare almost as well as, and sometimes better than, lists of resources that are manually compiled or classified into a topic. We also provide examples of ARC resource lists for the reader to examine.
955|Analysis of a very large AltaVista query log|In this paper we present an analysis of a 280 GB AltaVista Search Engine query log consisting of approximately 1 billion entries for search requests over a period of six weeks. This represents approximately 285 million user sessions, each an attempt to fill a single information need. We present an analysis of individual queries, query duplication, and query sessions. Furthermore we present results of a correlation analysis of the log entries, studying the interaction of terms within queries. Our data supports the conjecture that web users differ significantly from the user assumed in the standard information retrieval literature. Specifically, we show that web users type in short queries, mostly look at the first 10 results only, and seldom modify the query. This suggests that traditional information retrieval techniques might not work well for answering web search requests. The correlation analysis showed that the most highly correlated items are constituents of phrases. This result indicates it may be useful for search engines to consider search terms as parts of phrases even if the user did not explicitly specify them as such. 1
956|Searching the World Wide Web|Permissions: Requests for permissions to reproduce figures, tables, or portions of articles originally published in Circulation can be obtained via RightsLink, a service of the Copyright Clearance Center, not the Editorial Office. Once the online version of the published article for which permission is being requested is located, click Request Permissions in the middle column of the Web page under Services. Further information about this process is available in the Permissions and Rights Question and Answer document. Reprints: Information about reprints can be found online at:
957|Finding related pages in the World Wide Web|When using traditional search engines, users have to formulate queries to describe their information need. This paper discusses a different approach toweb searching where the input to the search process is not a set of query terms, but instead is the URL of a page, and the output is a set of related web pages. A related web page is one that addresses the same topic as the original page. For example, www.washingtonpost.com is a page related to www.nytimes.com, since both are online newspapers. We describe two algorithms to identify related web pages. These algorithms use only the connectivity information in the web (i.e., the links between pages) and not the content of pages or usage information. We haveimplemented both algorithms and measured their runtime performance. To evaluate the e ectiveness of our algorithms, we performed a user study comparing our algorithms with Netscape&#039;s \What&#039;s Related &#034; service [12]. Our study showed that the precision at 10 for our two algorithms are 73 % better and 51 % better than that of Netscape, despite the fact that Netscape uses both content and usage pattern information in addition to connectivity information.
958|Strong regularities in World Wide Web surfing|One of the most common modes of accessing information in the World Wide Web (WWW) is surfing from one document to another along hyperlinks. Several large empirical studies have revealed common patterns of surfing behavior. A model which assumes that users make a sequence of decisions to proceed to another page, continuing as long as the value of the current page exceeds some threshold, yields the probability distribution for the number of pages, or depth, that a user visits within a Web site. This model was verified by comparing its predictions with detailed measurements of surfing patterns. It also explains the observed Zipf-like distributions in page hits observed at WWW sites. Huberman et al 1The exponential growth of World Wide Web (WWW) is making it the standard information system for an increasing segment of the world&#039;s population. From electronic commerce and information resource to entertainment, the Web allows inexpensive and fast access to unique and novel services provided by individuals and institutions scattered throughout the world (1).
959|Scalable Feature Selection, Classification and Signature Generation for Organizing Large Text Databases Into Hierarchical Topic Taxonomies|We explore how to organize large text databases hierarchically by topic to aid better searching, browsing and filtering. Many corpora, such as internet directories, digital libraries, and patent databases are manually organized into topic hierarchies, also called taxonomies. Similar to indices for relational data, taxonomies make search and access more efficient. However, the exponential growth in the volume of on-line textual information makes it nearly impossible to maintain such taxonomic organization for large, fast-changing corpora by hand. We describe an automatic system that starts with a small sample of the corpus in which topics have been assigned by hand, and then updates the database with new documents as the corpus grows, assigning topics to these new documents with high speed and accuracy. To do this, we use techniques from statistical pattern recognition to efficiently separate the feature words, or...
960|Web Search Using Automatic Classification|We study the automatic classification of Web documents into pre-specified categories, with the objective of increasing the precision of Web search. We describe experiments in which we classify documents into high-level categories of the Yahoo! taxonomy, and a simple search architecture and implementation using this classification. The validation of our classification experiments offers interesting insights into the power of such automatic classification, as well as into the nature of Web content. Our research indicates that Web classification and search tools must compensate for artifices such as Web spamming that have resulted from the very existence of such tools. Keywords: Automatic classification, Web search tools, Web spamming, Yahoo! categories.
961|Learning from hotlists and coldlists: Towards a WWW information filtering and seeking agent|We describe a software agent that learns to find information on the World Wide Web (WWW), deciding what new pages might interest a user. The agent maintains a separate hotlist (for links that were interesting) and coldlist (for links that were not interesting) for each topic. By analyzing the information immediately accessible from each link, the agent learns the types of information the user is interested in. This can be used to inform the user when a new interesting page becomes available or to order the user&#039;s exploration of unseen existing links so that the more promising ones are investigated first. We compare four different learning algorithms on this task. We describe an experiment in which a simple Bayesian classifier acquires a user profile that agrees with a user&#039;s judgment over 90% of the time.
962|Surfing the Web Backwards|From a user’s perspective, hypertext links on the web form a directed graph between distinct information sources. We investigate the effects of discovering “backlinks ” from web resources, namely links pointing to the resource. We describe tools for backlink navigation on both the client and server side, using an applet for the client and a module for the Apache web server. We also discuss possible extensions to the HTTP protocol to facilitate the collection and navigation of backlink information in the world wide web. 1
963|Learning Probabilistic User Profiles: Applications to Finding Interesting Web Sites, Notifying Users of Relevant Changes to Web Pages, and Locating Grant Opportunities.|this article are:
964|Automatic Acquisition of Hyponyms from Large Text Corpora|We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidante of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also he acquirable iu this way. A subset of the acquisitiou algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.
965|WordNet: An on-line lexical database|WordNet is an on-line lexical reference system whose design is inspired by current
966|A practical part-of-speech tagger|We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment.
967|Noun Classification From Predicate.argument Structures|A method of determining the similarity of nouns on the basis of a metric derived from the distribution  of subject, verb and object in a large text corpus is described. The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification.
968|Automatic Acquisition Of Subcategorization Frames From Untagged Text|that takes a raw, untagged text corpus as its  only input (no open-class dictionary) and generates  a partial list of verbs occurring in the text  and the subcategorization frames (SFs) in which  they occur. Verbs are detected by a novel technique  based on the Case Filter of Rouvret and  Vergnaud (1980). The completeness of the output  list increases monotonically with the total number  of occurrences of each verb in the corpus. Fakse  positive rates are one to three percent of observations.
969|Noun Homograph Disambiguation Using Local Context in Large Text Corpora|This paper describes an accurate, relatively inexpensive method for the disambiguation of noun homographs using large text corpora. The algorithm checks the context surrounding the target noun against that of previously observed instances and chooses the sense for which the most evidence is found, where evidence consists of a set of orthographic, syntactic, and lexical features. Because the sense distinctions made are coarse, the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms. An implementation of the algorithm is described which, starting with a small set of hand-labeled instances, improves its results automatically via unsupervised training. The approach is compared to other attempts at homograph disambiguation using both machine readable dictionaries and unrestricted text and the use of training instances is determined to be a crucial difference. 1 Introduction  Large text corpora and the computational resources to handle them have ...
970|Automatically Extracting And Representing Collocations For Language Generation|... this paper, focusing on the acquisition problem. We describe a program, Xtract, that automatically acquires a range of collocations from large textual corpora and we describe how they can be repreSented in a flexible lexicon using a unification bed formalism
971|Disambiguating Prepositional Phrase Attachments by Using On-Line Dictionary Definitions|... This paper sustains that claim by describing a set of computational tools and techniques used to disambiguate prepositional phrase attachments in English sentences, by accessing on-line dictionary definitions. Such techniques offer hope for eliminating the time-consuming, and often incomplete, hand coding of semantic information that has been conventional in natural language understanding systems.
972|Semantically significant patterns in dictionary definitions|Natural language processing systems need large lexicons containing explicit information about lexical-semantlc relationships, selection restrictions, and verb categories. Because the labor involved in constructing such lexicons by hand is overwhelming, we have been trying to construct lexical entries automatically from information available in the machine-readable version of
973|Extraction of Semantic Information from an Ordinary English Dictionary and its Evaluation|U Collective + &#039;O&#039; V P&#039; + W &#039;T&#039; + T X &#039;T&#039; + Y &#039;T&#039; + &#039;Q&#039; Z UNMARKED  total 957 836  26 15  359 181  27 21  257 187  453 314  111 79  3457 2426  42 26  5794 3927  2 2  631 464  875 603  2144 1436  69 42  758 593  23 14  4 3  1291 887  16577 9668  789 398  20 15  103 61  97 108  4t 18  415 199  43560 249(16 p&#039;lan  life  mothe an. developerl under the influence  the nous related to &#039;Anima;;e&#039;,  Nouns rduted to the concept animate have a relatively simple structure bt the thesaurus, s aimale is ofteu us am an xample of ;t theqmrtts.like system. ]qxmnple of tim words mark as %rfimte (Q)&#039; m*d related mmm, especially marked ms &#039;plant +  The, prodaced thesgnras contains laol than 60% of the words ;g&#039;t-Nd i simple concepts, such &#039;pD, nt&#039; (table 10), &#039;mtinml&#039;, anal ;}l..lgo (pers0u ht definitions)&#039;, in correct positions. As shown in t,ble 10 for example 845 words ure traversed kom  &#039;lLble 10: Nouns Related to (Living) Thing aml Plant  (hving) thing - plu; (P)  A 2  1) 2 P 370  Q othe 270  total 645  i, la&#039;,ai in the prodhoed &#039;thesaurus; 370 words (62.4%) of these winds are n&#039;.arked t,.s &#039;Plmtt?  , &#039;owever, the produc thesaurus does not captm dionactive co*ceFs sa&lt;h as %nimal or plant (V)&#039; corrtl. In the definition of c&#039;roasbreed (tgble 9), the produced thanrus only u lant as :, key noun aud ingor animal. This h a typical problem ht the current produced hmmas. Note thirst the distinction between mte (Q)&#039; and &#039;animal m. pl.n (&#039;V)&#039; (animate wiChoat human) .nm o be dimcult for ht leico;;rphek-s: bed is marked as Q; crossbreed, however, is z,m&#039;kcd as V lot exple.
974|Acquiring Lexical Knowledge from Text: A Case Study|Language acquisition addresses two important text processing issues. The immediate problem is understanding a text in spite of the existence of lexical gaps. The long term issue is that the understander must incorporate new words into its lexicon for future use. This paper describes an approach to constructing new lexical entries in a gradual process by analyzing a sequence of example texts. This approach permits the graceful tolerance of new words while enabling the automated extension of the lexicon. Each new acquired lexeme starts as a set of assumptions derived from the analysis of each word in a textual context. A variety of knowledge sources, including
975|Processing Dictionary Definitions With Phrasal Pattern Hierarchies|This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of  phrasal patterns. An experimental system embodying this mechanism has been implemented for  processing definitions from the Longman Dictionary of Contemporary English. A property of this  dictionary, exploited by the system, is that it uses a restricted vocabulary in its word sense definitions
976|Where the REALLY Hard Problems Are|It is well known that for many NP-complete problems, such as K-Sat, etc., typical cases are easy to solve; so that computationally hard cases must be rare (assuming P != NP). This paper shows that NP-complete problems can be summarized by at least one &#034;order parameter&#034;, and that the hard problems occur at a critical value of such a parameter. This critical value separates two regions of characteristically different properties. For example, for K-colorability, the critical value separates overconstrained from underconstrained random graphs, and it marks the value at which the probability of a solution changes abruptly from near 0 to near 1. It is the high density of well-separated almost solutions (local minima) at this boundary that cause search algorithms to &#034;thrash&#034;. This boundary is a type of phase transition and we show that it is preserved under mappings between problems. We show that for some P problems either there is no phase transition or it occurs for bounded N (and so bound...
977|Almost all k-colorable graphs are easy to color|We describe a simple and e	cient heuristic algorithm for the graph coloring problem and show that for all k    it nds an optimal coloring for almost all kcolorable graphs  We also show that an algorithm proposed by Brelaz and justi ed on experimental grounds optimally colors almost all kcolorable graphs  E	cient implementations of both algorithms are given  The rst one runs in O
nm log k  time where n is the number of vertices and m the number of edges  The new implementation of Brelazs algorithm runs in O
m logn time  We observe that the popular greedy heuristic works poorly on kcolorable graphs
978|T.Hogg: Phase Transition in|From birth, children live in a sexual world, and the ways they are touched and treated send messages about their worth and about being loved. Voice and body language convey feelings about intimacy and relationship. Words and actions impart values about sexuality, sexual orientation, responsibility, and gender roles. Parents, television and films, religious leaders, musicians and actors, politicians, peers, and advertisers send messages about sexuality. We want our children to have healthy, rewarding lives, to like themselves, and to develop loving, mutually supportive relationships. We want them to act responsibly and to make choices that arise from the values they hold. Unfortunately, in many homes, across many cultures, adults are embarrassed about sexuality and fail to let their children know that sexual expression is integral to loving, committed, mutually supportive, intimate relationships. Research shows that when parents approach their role as sex educators in positive, affirming ways, young people are better able to make sexually healthy decisions and to build loving relationships. Parents who respond honestly to questions, provide resources, express their feelings and values, and portray sexuality and the need for intimacy as integral elements of life rear youth who respect themselves and behave responsibly. In 2001, Advocates for Youth launched the Rights.
979|Photo tourism: Exploring photo collections in 3D|Figure 1: Our system takes unstructured collections of photographs such as those from online image searches (a) and reconstructs 3D points and viewpoints (b) to enable novel ways of browsing the photos (c). We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.
980|Distinctive Image Features from Scale-Invariant Keypoints|This paper presents a method for extracting distinctive invariant features from  images, which can be used to perform reliable matching between different images  of an object or scene. The features are invariant to image scale and rotation,  and are shown to provide robust matching across a a substantial range  of affine distortion, addition of noise, change in 3D viewpoint, and change in  illumination. The features are highly distinctive, in the sense that a single feature  can be correctly matched with high probability against a large database of  features from many images. This paper also describes an approach to using  these features for object recognition. The recognition proceeds by matching  individual features to a database of features from known objects using a fast  nearest-neighbor algorithm, followed by a Hough transform to identify clusters  belonging to a single object, and finally performing verification through leastsquares  solution for consistent pose parameters. This approach to recognition  can robustly identify objects among clutter and occlusion while achieving near  real-time performance.
982|Video google: A text retrieval approach to object matching in videos|We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieval is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching on two full length feature films. 1.
983|Light Field Rendering|A number of techniques have been proposed for flying through scenes by redisplaying previously rendered or digitized views. Techniques have also been proposed for interpolating between views by warping input images, using depth information or correspondences between multiple images. In this paper, we describe a simple and robust method for generating new views from arbitrary camera positions without depth information or feature matching, simply by combining and resampling the available images. The key to this technique lies in interpreting the input images as 2D slices of a 4D function - the light field. This function completely characterizes the flow of light through unobstructed space in a static scene with fixed illumination. We describe a 
985|The Lumigraph|This paper discusses a new method for capturing the complete appearanceof both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function, which we call a Lumigraph. The Lumigraph is a subset of the complete plenoptic function that describes the flow of light at all positions in all directions. With the Lumigraph, new images of the object can be generated very quickly, independent of the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of samples, the construction of the Lumigraph, and the subsequent rendering of images from this new representation. 1
986|An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions|Consider a set S of n data points in real d-dimensional space, R d , where distances are measured using any Minkowski metric. In nearest neighbor searching we preprocess S into a data structure, so that given any query point q 2 R d , the closest point of S to q can be reported quickly. Given any positive real ffl, a data point p is a (1 + ffl)-approximate nearest neighbor of q if its distance from q is within a factor of (1 + ffl) of the distance to the true nearest neighbor. We show that it is possible to preprocess a set of n points in R d in O(dn log n) time and O(dn) space, so that given a query point q 2 R d , and ffl ? 0, a (1 + ffl)-approximate nearest neighbor of q can be computed in O(c d;ffl log n) time, where c d;ffl d d1 + 6d=ffle d is a factor depending only on dimension and ffl. In general, we show that given an integer k 1, (1 + ffl)-approximations to the k nearest neighbors of q can be computed in additional O(kd log n) time.
987|Plenoptic Modeling: An Image-Based Rendering System|Image-based rendering is a powerful new approach for generating real-time photorealistic computer graphics. It can provide convincing animations without an explicit geometric representation. We use the “plenoptic function” of Adelson and Bergen to provide a concise problem statement for image-based rendering paradigms, such as morphing and view interpolation. The plenoptic function is a parameterized function for describing everything that is visible from a given point in space. We present an image-based rendering system based on sampling, reconstructing, and resampling the plenoptic function. In addition, we introduce a novel visible surface algorithm and a geometric invariant for cylindrical projections that is equivalent to the epipolar constraint defined for planar projections.
988|Labeling Images with a Computer Game|We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people  play the game they help determine the contents of images  by providing meaningful labels for them. If the game is  played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the  Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users  block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don&#039;t work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.
989|LabelMe: A Database and Web-Based Tool for Image Annotation|  We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.
990|A Touring Machine: Prototyping 3D Mobile Augmented Reality Systems for Exploring the Urban Environment|We describe a prototype system that combines together the overlaid 3D graphics of augmented reality with the untethered freedom of mobile computing. The goal is to explore how these two technologies might together make possible wearable computer systems that can support users in their everyday interactions with the world. We introduce an application that presents information about our university’s campus, using a head-tracked, see-through, headworn, 3D display, and an untracked, opaque, handheld, 2D display with stylus and trackpad. We provide an illustrated explanation of how our prototype is used, and describe our rationale behind designing its software infrastructure and selecting the hardware on which it runs.
991|A comparison of affine region detectors|The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris [24, 34] and Hessian points [24], as proposed by Mikolajczyk and Schmid and by Schaffalitzky and Zisserman; a detector of ‘maximally stable extremal regions’, proposed by Matas et al. [21]; an edge-based region detector [45] and a detector based on intensity extrema [47], proposed by Tuytelaars and Van Gool; and a detector of ‘salient regions’, proposed by Kadir, Zisserman and Brady [12]. The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression. The objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework. 1
992|Unstructured lumigraph rendering|We describe an image based rendering approach that generalizes many image based rendering algorithms currently in use including light field rendering and view-dependent texture mapping. In particular it allows for lumigraph style rendering from a set of input cameras that are not restricted to a plane or to any specific manifold. In the case of regular and planar input camera positions, our algorithm reduces to a typical lumigraph approach. In the case of fewer cameras and good approximate geometry, our algorithm behaves like view-dependent texture mapping. Our algorithm achieves this flexibility because it is designed to meet a set of desirable goals that we describe. We demonstrate this flexibility with a variety of examples. Keyword Image-Based Rendering 1
993|View morphing|Image morphing techniques can generate compelling 2D transitions between images. However, differences in object pose or viewpoint often cause unnatural distortions in image morphs that are difficult to correct manually. Using basic principles of projective geometry, this paper introduces a simple extension to image morphing that correctly handles 3D projective camera and scene transformations. The technique, called view morphing, works by prewarping two images prior to computing a morph and then postwarping the interpolated images. Because no knowledge of 3D shape is required, the technique may be applied to photographs and drawings, as well as rendered scenes. The ability to synthesize changes both in viewpoint and image structure affords a wide variety of interesting 3D effects via simple image transformations.
994|Constrained Delaunay triangulations|Given a set of n vertices in the plane together with a set of noncrossing edges, the constrained Delaunay triangulation (CDT) is the triangulation of the vertices with the following properties: (1) the prespecified edges are included in the triangulation, and (2) it is as close as possible to the Delaunay triangulation. We show that the CDT can be built in optimal O(n log n) time using a divide-and-conquer technique. This matches the time required to build an arbitrary (unconstrained) Delaunay triangulation and the time required to build an arbitrary constrained (nonDelaunay) triangulation. CDTs, because of their relationship with Delaunay triangulations, have a number of properties that should make them useful for the finite-element method. Applications also include motion planning in the presence of polygonal obstacles in the plane and constrained Euclidean minimum spanning trees, spanning trees subject to the restriction that some edges are prespecified. I’wnishi0tt to copy without tix all or part of thk material is granlcd provided thal IIIC wpics arc not nude or distributed li)r direct commercial advanlagc, the ACM copyright wficc and the title of lhc publication and its date appear. and notice is given that copying is hy permission ol the Association Car Computing Machinery. ‘To copy otherwise. or to republish. requires a fee and/or specific permission.
995|How Do People Manage Their Digital Photographs|In this paper we present and discuss the findings of a study that investigated how people manage their collections of digital photographs. The six-month, 13-participant study included interviews, questionnaires, and analysis of usage statistics gathered from an instrumented digital photograph management tool called Shoebox. Alongside simple browsing features such as folders, thumbnails and timelines, Shoebox has some advanced multimedia features: content-based image retrieval and speech recognition applied to voice annotations. Our results suggest that participants found their digital photos much easier to manage than their non-digital ones, but that this advantage was almost entirely due to the simple browsing features. The advanced features were not used very often and their perceived utility was low. These results should help to inform the design of improved tools for managing personal digital photographs.
996|Video Indexing Based on Mosaic Representations|Video is a rich source of information. It provides visual information about scenes. However, this information is implicitly buried inside the raw video data, and is provided with the cost of very high temporal redundancy. While the standard sequential form of video storage is adequate for viewing in a &#034;movie mode&#034;, it fails to support rapid access to information of interest that is required in many of the emerging applications of video. This paper presents an approach for efficient access, use and manipulation of video data. The video data is first transformed from its sequential and redundant frame-based representation in which the information about the scene is distributed over many frames, to an explicit and compact scene-based representation, to which each frame can be directly related. This compact reorganization of the video data supports  non-linear browsing and efficient indexing to provide rapid access directly to information of interest. The paper describes a new set of metho...
997|Modelling and interpretation of architecture from several images |The modelling of 3-dimensional (3D) environments has become a requirement for many applications in engineering design, virtual reality, visualisation and entertainment. However the scale and complexity demanded from such models has risen to the point where the acquisition of 3D models can require a vast amount of specialist time and equipment. Because of this much research has been undertaken in the computer vision community into automating all or part of the process of acquiring a 3D model from a sequence of images. This thesis focuses specifically on the automatic acquisition of architectural models from short image sequences. An architectural model is defined as a set of planes corresponding to walls which contain a variety of labelled primitives such as doors and windows. As well as a label defining its type, each primitive contains parameters defining its shape and texture. The key advantage of this representation is that the model defines not only geometry and texture, but also an interpretation of the scene. This is crucial as it enables reasoning about the scene; for instance, structure and texture can be inferred in areas of the model which are unseen in any
998|Image alignment and stitching: a tutorial|This tutorial reviews image alignment and image stitching algorithms. Image alignment algorithms can discover the correspondence relationships among images with varying degrees of overlap. They are ideally suited for applications such as video stabilization, summarization, and the creation of panoramic mosaics. Image stitching algorithms take the alignment estimates produced by such registration algorithms and blend the images in a seamless manner, taking care to deal with potential problems such as blurring or ghosting caused by parallax and scene movement as well as varying image exposures. This tutorial reviews the basic motion models underlying alignment and stitching algorithms, describes effective direct (pixel-based) and feature-based alignment algorithms, and describes blending algorithms used to produce
999|The design and implementation of a generic sparse bundle adjustment software package based on the levenberg-marquardt algorithm|The most recent revision of this document will always be found at
1000|Automatic Line matching across views|HAL is a multi-disciplinary open access archive for the deposit and dissemination of sci-entific research documents, whether they are pub-lished or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L’archive ouverte pluridisciplinaire HAL, est destine´e au de´po^t et a ` la diffusion de documents scientifiques de niveau recherche, publie´s ou non, e´manant des e´tablissements d’enseignement et de recherche franc¸ais ou e´trangers, des laboratoires publics ou prive´s.
1001|Temporal event clustering for digital photo collections |Organizing digital photograph collections according to events such as holiday gatherings or vacations is a common practice among photographers. To support photographers in this task, we present similarity-based methods to cluster digital photos by time and image content. The approach is general and unsupervised, and makes minimal assumptions regarding the structure or statistics of the photo collection. We present several variants of an automatic unsupervised algorithm to partition a collection of digital photographs based either on temporal similarity alone, or on temporal and content-based similarity. First, interphoto similarity is quantified at multiple temporal scales to identify likely event clusters. Second, the final clusters are determined according to one of three clustering goodness criteria. The clustering criteria trade off computational complexity and performance. We also describe a supervised clustering method based on learning vector quantization. Finally, we review the results of an experimental evaluation of the proposed algorithms and existing approaches on two test collections.
1002|Automatic organization for digital photographs with geographic coordinates|We describe PhotoCompas, a system that utilizes the time and location information embedded in digital photographs to automatically organize a personal photo collection. PhotoCompas produces browseable location and event hierarchies for the collection. These hierarchies are created using algorithms that interleave time and location to produce an organization that mimics the way people think about their photo collections. In addition, the algorithm annotates the generated hierarchy with geographical names. We tested our approach in case studies of three real-world collections and verified that the results are meaningful and useful for the collection owners.
1003|Calibrated, Registered Images of an Extended Urban Area|We describe a dataset of several thousand calibrated, time-stamped, geo-referenced, high  dynamic range color images, acquired under uncontrolled, variable illumination conditions in  an outdoor region spanning several hundred meters. The image data is grouped into several  regions which have little mutual inter-visibility. For each group, the calibration data is globally  consistent on average to roughly five centimeters and 0.1 # , or about four pixels of epipolar  registration. All image, feature and calibration data is available for interactive inspection and  downloading at http://city.lcs.mit.edu/data.
1004|From where to what: Metadata sharing for digital photographs with geographic coordinates|Abstract. We describe LOCALE, a system that allows cooperating information systems to share labels for photographs. Participating photographs are enhanced with a geographic location stamp – the latitude and longitude where the photograph was taken. For a photograph with no label, LOCALE can use the shared information to assign a label based on other photographs that were taken in the same area. LOCALE thus allows (i) text search over unlabeled sets of photos, and (ii) automated label suggestions for unlabeled photos. We have implemented a LOCALE prototype where users cooperate in submitting labels and locations, enhancing search quality for all users in the system. We ran an experiment to test the system in centralized and distributed settings. The results show that the system performs search tasks with surprising accuracy, even when searching for specific landmarks. 1
1005|Interactive Design of Multi-Perspective Images For Visualizing Urban Landscapes|Multi-perspective images are a useful way to visualize extended, roughly planar scenes such as landscapes or city blocks. However, constructing effective multi-perspective images is something of an art. In this paper, we describe an interactive system for creating multi-perspective images composed of serially blended cross-slits images. Beginning with a sideways-looking video of the scene as might be captured from a moving vehicle, we allow the user to interactively specify a set of cross-slits cameras, possibly with gaps between them. In each camera, one of the slits is defined to be the camera path, which is typically horizontal, and the user is left to choose the second slit, which is typically vertical. The system then generates intermediate views between these cameras using a novel interpolation scheme, thereby producing a multi-perspective image with no seams. The user can also choose the picture surface in space onto which viewing rays are projected, thereby establishing a parameterization for the image. We show how the choice of this surface can be used to create interesting visual effects. We demonstrate our system by constructing multi-perspective images that summarize city blocks, including corners, blocks with deep plazas and other challenging urban situations.
1006|A System Architecture for Ubiquitous Video |Realityflythrough is a telepresence/tele-reality system that works in the dynamic, uncalibrated environments typically associated with ubiquitous computing. By harnessing networked mobile video cameras, it allows a user to remotely and immersively explore a physical space. RealityFlythrough creates the illusion of complete live camera coverage in a physical environment. This paper describes the architecture of RealityFlythrough, and evaluates it along three dimensions: (1) its support of the abstractions for infinite camera coverage, (2) its scalability, and (3) its robustness to changing user requirements. 1
1007|A System for Automatic Pose-Estimation from a Single Image in a City Scene|We describe an automatic system for pose-estimation from a single image in a city scene. Each building has a model consisting of a number of parallel planes associated with it. The homographies for the best match of the planes to the image is estimated automatically for each of the possible buildings. We show how the estimation of homographies can be done effectively by reducing the search space and using fast convolution. The model having the best match is then used to determine the position and orientation of the camera. The results
1008|Sea of Images|A long-standing research problem in computer graphics is to reproduce the visual experience of walking through a large photorealistic environment interactively. On one hand, traditional geometry-based rendering systems fall short of simulating the visual realism of a complex environment. On the other hand, image-based rendering systems have to date been unable to capture and store a sampled representation of a large environment with complex lighting and visibility effects.
1009|Spectral Partitioning for Structure from Motion |We propose a spectral partitioning approach for large-scale optimization problems, specifically structure from motion. In structure from motion, partitioning methods reduce the problem into smaller and better conditioned subproblems which can be efficiently optimized. Our partitioning method uses only the Hessian of the reprojection error and its eigenvectors. We show that partitioned systems that preserve the eigenvectors corresponding to small eigenvalues result in lower residual error when optimized. We create partitions by clustering the entries of the eigenvectors of the Hessian corresponding to small eigenvalues. This is a more general technique than relying on domain knowledge and heuristics such as bottom-up structure from motion approaches. Simultaneously, it takes advantage of more information than generic matrix partitioning algorithms.
1010|Interactive Image-Based Rendering Using Feature Globalization|Image-based rendering (IBR) systems enable virtual walkthroughs of photorealistic environments by warping and combining reference images to novel viewpoints under interactive user control. A significant challenge in such systems is to automatically compute image correspondences that enable accurate image warping.
1011|Eye movements in reading and information processing: 20 years of research|Recent studies of eye movements in reading and other information processing tasks, such as music reading, typing, visual search, and scene perception, are reviewed. The major emphasis of the review is on reading as a specific example of cognitive processing. Basic topics discussed with respect to reading are (a) the characteristics of eye movements, (b) the perceptual span, (c) integration of information across saccades, (d) eye movement control, and (e) individual differences (including dyslexia). Similar topics are discussed with respect to the other tasks examined. The basic theme of the review is that eye movement data reflect moment-to-moment cognitive processes in the various tasks examined. Theoretical and practical considerations concerning the use of eye movement data are also discussed. Many studies using eye movements to investigate cognitive processes have appeared over the past 20 years. In an earlier review, I (Rayner, 1978b) argued that since the mid-1970s we have been in a third era of eye movement research and that the success of research in the current era would depend on the ingenuity of researchers in designing interesting and informative
1012|What one intelligence test measures: A theoretical account of the processing|Raven Progressive
1013|Scene perception: detecting and judging objects undergoing relational violations|Five classes of relations between an object and its setting can characterize the organization of objects into real-world scenes. The relations are (1) Interposition (objects interrupt their background), (2) Support (objects tend to rest on surfaces), (3) Probability (objects tend to be found in some scenes but not others), (4) Position (given an object is probable in a scene, it often is found in some positions and not others), and (5) familiar Size (objects have a limited set of size relations with other objects). In two experiments subjects viewed brief (150 msec) presentations of slides of scenes in which an object in a cued location in the scene was either in a normal relation to its background or violated from one to three of the relations. Such objects appear to (1) have the background pass through them, (2) float in air, (3) be unlikely in that particular scene, (4) be in an inappropriate position, and (5) be too large or too small relative to the other objects in the scene. In Experiment I, subjects attempted to determine whether the cued object corresponded to a target object which had been specified in advance by name. With the exception of the Interposition violation, violation costs were incurred in that the
1014|An analysis of the saccadic system by means of double step stimuli|Abatrac-The characteristics of saccadic reactions to double steps of a target were analysed as a function of the time lapse between the second target step and the onset of the response. The analysis suggests that goal-directed saccades are prepared in two steps; first a decision as to their direction is taken which requires a randomly varying time, and subsequently their amplitude is calculated as a time average of the fixation error. In addition. the analysis demonstrates that the preparatory processes of two different saccades may overlap in time (“parallel programming”) and that. although reacting in a discontinuous manner, the saccadic system continuously processes the afferent visual information. A conceptual model based on an internal predictive feedback pathway and on a non-linear decision mechanism is proposed that accounts for the observed behaviour. (Trunslated abstract at end of paper) 1. INTRODUCIION The saccadic branch of the human oculomotor system has been studied quite extensively with methods adapted from control theory, and several models have been proposed to account for the observed behaviour
1015|Eye fixations and memory for emotional events|Subjects watched either an emotional, neutral, or unusual sequence of slides containing 1 critical slide in the middle. Experiments 1 and 2 allowed only a single eye fixation on the critical slide by presenting it for 180 ms (Experiment 1) or 150 ms (Experiment 2). Despite this constraint, memory for a central detail was better for the emotional condition. In Experiment 3, subjects were allowed 2.70 s to view the critical slide while their eye movements were monitored. When subjects who had devoted the same number of fixations were compared, memory for the central detail of the emotional slide was again better. The results suggest that enhanced memory for detail information of an emotional event does not occur solely because more attention is devoted to the emotional information. The purpose of this series of studies was to examine the role of attention and focusing patterns in memory for emotional versus neutral events. We use the term emotional events in this paper to refer to scenes that have unpleasant visual features (e.g., blood) and that have the potential to evoke negative emotional feelings in the viewer. How well are details
1016|Bayesian Network Classifiers|Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods for feature selection.
1017|A Bayesian method for the induction of probabilistic networks from data|This paper presents a Bayesian method for constructing probabilistic networks from databases. In particular, we focus on constructing Bayesian belief networks. Potential applications include computer-assisted hypothesis testing, automated scientific discovery, and automated construction of probabilistic expert systems. We extend the basic method to handle missing data and hidden (latent) variables. We show how to perform probabilistic inference by averaging over the inferences of multiple belief networks. Results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases. Finally, we relate the methods in this paper to previous work, and we discuss open problems.
1018|Learning Bayesian networks: The combination of knowledge and statistical data|We describe scoring metrics for learning Bayesian networks from a combination of user knowledge and statistical data. We identify two important properties of metrics, which we call event equivalence and parameter modularity. These properties have been mostly ignored, but when combined, greatly simplify the encoding of a user’s prior knowledge. In particular, a user can express his knowledge—for the most part—as a single prior Bayesian network for the domain. 1
1019|Approximating discrete probability distributions with dependence trees|A method is presented to approximate optimally an n-dimensional discrete probability distribution by a product of second-order distributions, or the distribution of the first-order tree dependence. The problem is to find an optimum set of n-1 first order dependence relationship among the n variables. It is shown that the procedure derived in this paper yields an approximation of a minimum difference in information. It is further shown that when this procedure is applied to empirical observations from an unknown distribution of tree dependence, the procedure is the maximum-likelihood estimate of the distribution.
1020|Estimating Continuous Distributions in Bayesian Classifiers|When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Mateo, 1995 1 Introduction In rec...
1021|A Tutorial on Learning Bayesian Networks|We examine a graphical representation of uncertain knowledge called a Bayesian network. The representation is easy to construct and interpret, yet has formal probabilistic semantics making it suitable for statistical manipulation. We show how we can use the representation to learn new knowledge by combining domain knowledge with statistical data. 1 Introduction  Many techniques for learning rely heavily on data. In contrast, the knowledge encoded in expert systems usually comes solely from an expert. In this paper, we examine a knowledge representation, called a Bayesian network, that lets us have the best of both worlds. Namely, the representation allows us to learn new knowledge by combining expert domain knowledge and statistical data. A Bayesian network is a graphical representation of uncertain knowledge that most people find easy to construct and interpret. In addition, the representation has formal probabilistic semantics, making it suitable for statistical manipulation (Howard,...
1022|Learning Bayesian Networks With Local Structure|.  We examine a novel addition to the known methods for learning Bayesian networks from data that improves the quality of the learned networks. Our approach explicitly represents and learns the local structure in the conditional probability distributions (CPDs) that quantify these networks. This increases the space of possible models, enabling the representation of CPDs with a variable number of parameters. The resulting learning procedure induces models that better emulate the interactions present in the data. We describe the theoretical foundations and practical aspects of learning local structures and provide an empirical evaluation of the proposed learning procedure. This evaluation indicates that learning curves characterizing this procedure converge faster, in the number of training instances, than those of the standard procedure, which ignores the local structure of the CPDs. Our results also show that networks learned with local structures tend to be more complex (in terms of a...
1023|Theory Refinement on Bayesian Networks|Theory refinement is the task of updating a domain theory in the light of new cases, to be done automatically or with some expert assistance. The problem of theory refinement under uncertainty is reviewed here in the context of Bayesian statistics, a theory of belief revision. The problem is reduced to an incremental learning task as follows: the learning system is initially primed with a partial theory supplied by a domain expert, and thereafter maintains its own internal representation of alternative theories which is able to be interrogated by the domain expert and able to be incrementally refined from data. Algorithms for refinement of Bayesian networks are presented to illustrate what is meant by &#034;partial theory&#034;, &#034;alternative theory representation &#034;, etc. The algorithms are an incremental variant of batch learning algorithms from the literature so can work well in batch and incremental mode. 1 Introduction Theory refinement is the task of updating a domain theory in the light of...
1024|Learning Bayesian belief networks: An approach based on the MDL principle|A new approach for learning Bayesian belief networks from raw data is presented. The approach is based on Rissanen&#039;s Minimal Description Length (MDL) principle, which is particularly well suited for this task. Our approach does not require any prior assumptions about the distribution being learned. In particular, our method can learn unrestricted multiply-connected belief networks. Furthermore, unlike other approaches our method allows us to tradeo accuracy and complexity in the learned model. This is important since if the learned model is very complex (highly connected) it can be conceptually and computationally intractable. In such a case it would be preferable to use a simpler model even if it is less accurate. The MDL principle o ers a reasoned method for making this tradeo. We also show that our method generalizes previous approaches based on Kullback cross-entropy. Experiments have been conducted to demonstrate the feasibility of the approach. Keywords: Knowledge Acquisition ? Bayes Nets ? Uncertainty Reasoning. 1
1025|On bias, variance, 0/1-loss, and the curse-of-dimensionality|Abstract. The classification problem is considered in which an output variable y assumes discrete values with respective probabilities that depend upon the simultaneous values of a set of input variables x ={x1,...,xn}.At issue is how error in the estimates of these probabilities affects classification error when the estimates are used in a classification rule. These effects are seen to be somewhat counter intuitive in both their strength and nature. In particular the bias and variance components of the estimation error combine to influence classification in a very different way than with squared error on the probabilities themselves. Certain types of (very high) bias can be canceled by low variance to produce accurate classification. This can dramatically mitigate the effect of the bias associated with some simple estimators like “naive ” Bayes, and the bias induced by the curse-of-dimensionality on nearest-neighbor procedures. This helps explain why such simple methods are often competitive with and sometimes superior to more sophisticated ones for classification, and why “bagging/aggregating ” classifiers can often improve accuracy. These results also suggest simple modifications to these procedures that can (sometimes dramatically) further improve their classification performance.
1026|Learning Bayesian Networks is NP-Complete|Algorithms for learning Bayesian networks from data havetwo components: a scoring metric and a  search procedure. The scoring metric computes a score reflecting the goodness-of-fit of the structure  to the data. The search procedure tries to identify network structures with high scores. Heckerman  et al. (1995) introduce a Bayesian metric, called the BDe metric, that computes the relative posterior  probabilityofanetwork structure given data. In this paper, we show that the search problem of  identifying a Bayesian network---among those where each node has at most K parents---that has a  relative posterior probability greater than a given constant is NP-complete, when the BDe metric  is used.  12.1 
1027|A Guide to the Literature on Learning Probabilistic Networks From Data|This literature review discusses different methods under the general rubric of learning Bayesian networks from data, and includes some overlapping work on more general probabilistic networks. Connections are drawn between the statistical, neural network, and uncertainty communities, and between the different methodological communities, such as Bayesian, description length, and classical statistics. Basic concepts for learning and Bayesian networks are introduced and methods are then reviewed. Methods are discussed for learning parameters of a probabilistic network, for learning the structure, and for learning hidden variables. The presentation avoids formal definitions and theorems, as these are plentiful in the literature, and instead illustrates key concepts with simplified examples.  Keywords--- Bayesian networks, graphical models, hidden variables, learning, learning structure, probabilistic networks, knowledge discovery. I. Introduction  Probabilistic networks or probabilistic gra...
1028|Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables|We discuss Bayesian methods for learning Bayesian networks when data sets are incomplete. In particular, we examine asymptotic approximations for the marginal likelihood of incomplete data given a Bayesian network. We consider the Laplace approximation and the less accurate but more efficient BIC/MDL approximation. We also consider approximations proposed by Draper (1993) and Cheeseman and Stutz (1995). These approximations are as efficient as BIC/MDL, but their accuracy has not been studied in any depth. We compare the accuracy of these approximations under the assumption that the Laplace approximation is the most accurate. In experiments using synthetic data generated from discrete naive-Bayes models having a hidden root node, we find that (1) the BIC/MDL measure is the least accurate, having a bias in favor of simple models, and (2) the Draper and CS measures are the most accurate. 1
1029|Adaptive Probabilistic Networks with Hidden Variables|. Probabilistic networks (also known as Bayesian belief networks) allow a compact description of complex stochastic relationships among several random variables. They are rapidly becoming the tool of choice for uncertain reasoning in artificial intelligence. In this paper, we investigate the problem of learning probabilistic networks with known structure and hidden variables. This is an important problem, because structure is much easier to elicit from experts than numbers, and the world is rarely fully observable. We present a gradient-based algorithmand show that the gradient can be computed locally, using information that is available as a byproduct of standard probabilistic network inference algorithms. Our experimental results demonstrate that using prior knowledge about the structure, even with hidden variables, can significantly improve the learning rate of probabilistic networks. We extend the method to networks in which the conditional probability tables are described using a ...
1031|Learning Belief Networks in the Presence of Missing Values and Hidden Variables|In recent years there has been a flurry of works on learning probabilistic belief networks. Current state of the art methods have been shown to be successful for two learning scenarios: learning both network structure and parameters from  complete data, and learning parameters for a fixed network from incomplete data---that is, in the presence of missing values or hidden variables. However, no method has yet been demonstrated to effectively learn network structure from incomplete data. In this paper, we propose a new method for learning network structure from incomplete data. This method is based on an extension of the  Expectation-Maximization (EM) algorithm for model selection problems that performs search for the best structure inside the EM procedure. We prove the convergence of this algorithm, and adapt it for learning belief networks. We then describe how to learn networks in two scenarios: when the data contains missing values, and in the presence of hidden variables. We provide...
1032|MLC++: a machine learning library in C++|We present MLC++, a library of C++ classes and tools for supervised Machine Learning. While MLC++ provides general learning algorithms that can be used by end users, the main objective is to provide researchers and experts with a wide variety of tools that can accelerate algorithm development, increase software reliability, provide comparison tools, and display information visually. More than just a collection of existing algorithms, MLC++ is an attempt to extract commonalities of algorithms and decompose them for a unified view that is simple, coherent, and extensible. In this paper we discuss the problems MLC++ aims to solve, the design of MLC++, and the current functionality.  
1033|Building Classifiers using Bayesian Networks|Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state of the art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we examine and evaluate approaches for inducing classifiers from data, based on recent results in the theory of learning Bayesian networks. Bayesian networks are factored representations of probability distributions that generalize the naive Bayes classifier and explicitly represent statements about independence. Among these approacheswe single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness which are characteristic of naive Bayes. We experimentally tested these approaches using benchmark problems from...
1034|Discretizing Continuous Attributes While Learning Bayesian Networks|We introduce a method for learning Bayesian networks that handles the discretization of continuous variables as an integral part of the learning process. The main ingredient in this method is a new metric based on the Minimal Description Length principle for choosing the threshold values for the discretization while learning the Bayesian network structure. This score balances the complexity of the learned discretization and the learned network structure against how well they model the training data. This ensures that the discretization of each variable introduces just enough intervals to capture its interaction with adjacent variables in the network. We formally derive the new metric, study its main properties, and propose an iterative algorithm for learning a discretization policy. Finally, we illustrate its behavior in applications to supervised learning. 1 INTRODUCTION  Bayesian networks provide efficient and effective representation of the joint probability distribution over a set ...
1035|Efficient Learning of Selective Bayesian Network Classifiers|In this paper, we present a computationally efficient method for inducing  selective Bayesian network classifiers. Our approach is to use informationtheoretic metrics to efficiently select a subset of attributes from which to learn the classifier. We explore three conditional, information-theoretic metrics that are extensions of metrics used extensively in decision tree learning, namely Quinlan&#039;s gain and gain ratio metrics and Mantaras&#039;s distance metric. We experimentally show that the algorithms based on gain ratio and distance metric learn selective Bayesian networks that have predictive accuracies as good as or better than those learned by existing selective Bayesian network induction approaches (K2-AS), but at a significantly lower computational cost. We prove that the subset-selection phase of these information-based algorithms has polynomial complexity as compared to the worst-case exponential time complexity of the corresponding phase in K2-AS. We also compare the performance o...
1036|Learning Bayesian Networks: A unification for discrete and Gaussian domains|We examine Bayesian methods for learning Bayesian networks from a combination of prior knowledge and statistical data. In particular, we unify the approaches we presented at last year&#039;s conference for discrete and Gaussian domains. We derive a general Bayesian scoring metric, appropriate for both domains. We then use this metric in combination with well-known statistical facts about the Dirichlet and normal{Wishart distributions to derive our metrics for discrete and Gaussian domains.  
1037|A Comparison of Induction Algorithms for Selective and non-Selective Bayesian Classifiers|In this paper we present a novel induction algorithm for Bayesian networks. This selective Bayesian network classifier selects a subset of attributes that maximizes predictive accuracy prior to the network learning phase, thereby learning Bayesian networks with a bias for small, high-predictive-accuracy networks. We compare the performance of this classifier with selective and non-selective naive Bayesian classifiers. We show that the selective Bayesian network classifier performs significantly better than both versions of the naive Bayesian classifier on almost all databases analyzed, and hence is an enhancement of the naive Bayesian classifier. Relative to the non-selective Bayesian network classifier, our selective Bayesian network classifier generates networks that are computationally simpler to evaluate and that display predictive accuracy comparable to that of Bayesian networks which model all features. 1 INTRODUCTION  Bayesian induction methods have proven to be an important cla...
1038|Histograms of Oriented Gradients for Human Detection|We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds. 1
1039|Making Large-Scale SVM Learning Practical|Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, off-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.  
1040|A PERFORMANCE EVALUATION OF LOCAL DESCRIPTORS|In this paper we compare the performance of descriptors computed for local interest regions, as for example extracted by the Harris-Affine detector [32]. Many different descriptors have been proposed in the literature. However, it is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [3], steerable filters [12], PCA-SIFT [19], differential invariants [20], spin images [21], SIFT [26], complex filters [37], moment invariants [43], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor, and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors.
1041|An affine invariant interest point detector|Abstract. This paper presents a novel approach for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce significant changes in the point location as well as in the scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas: 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges to affine invariant points. For matching and recognition, the image is characterized by a set of affine invariant points; the affine transformation associated with each point allows the computation of an affine invariant descriptor which is also invariant to affine illumination changes. A quantitative comparison of our detector with existing ones shows a significant improvement in the presence of large affine deformations. Experimental results for wide baseline matching show an excellent performance in the presence of large perspective transformations including significant scale changes. Results for recognition are very good for a database with more than 5000 images.
1042|The Visual Analysis of Human Movement: A Survey|The ability to recognize humans and their activities by vision is key for a machine to interact intelligently and effortlessly with a human-inhabited environment. Because of many potentially important applications, “looking at people ” is currently one of the most active application domains in computer vision. This survey identifies a number of promising applications and provides an overview of recent developments in this domain. The scope of this survey is limited to work on whole-body or hand motion; it does not include work on human faces. The emphasis is on discussing the various methodologies; they are grouped in 2-D approaches with or without explicit shape models and 3-D approaches. Where appropriate, systems are reviewed. We conclude with some thoughts about future directions. c ? 1999 Academic Press 1.
1043|PCA-SIFT: A more distinctive representation for local image descriptors|Stable local feature detection and representation is a fundamental component of many image registration and object recognition algorithms. Mikolajczyk and Schmid [14] recently evaluated a variety of approaches and identified the SIFT [11] algorithm as being the most resistant to common image deformations. This paper examines (and improves upon) the local image descriptor used by SIFT. Like SIFT, our descriptors encode the salient aspects of the image gradient in the feature point&#039;s neighborhood; however, instead of using SIFT&#039;s smoothed weighted histograms, we apply Principal Components Analysis (PCA) to the normalized gradient patch. Our experiments demonstrate that the PCAbased local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation. We also present results showing that using these descriptors in an image retrieval application results in increased accuracy and faster matching.
1044|Detecting Pedestrians Using Patterns of Motion and Appearance|This paper describes a pedestrian detection system that integrates image intensity information  with motion information. We use a detection style algorithm that scans a  detector over two consecutive frames of a video sequence. The detector is trained (using  AdaBoost) to take advantage of both motion and appearance information to detect  a walking person. Past approaches have built detectors based on motion information  or detectors based on appearance information, but ours is the first to combine both  sources of information in a single detector. The implementation described runs at about  4 frames/second, detects pedestrians at very small scales (as small as 20x15 pixels), and  has a very low false positive rate
1045|A Trainable System for Object Detection|  This paper presents a general, trainable system for object detection in unconstrained, cluttered scenes. The system derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform. This example-based learning approach implicitly derives a model of an object class by training a support vector machine classifier using a large set of positive and negative examples. We present results on face, people, and car detection tasks using the same architecture. In addition, we quantify how the representation affects detection performance by considering several alternate representations including pixels and principal components. We also describe a real-time application of our person detection system as part of a driver assistance system.
1046|Object Detection in Images by Components|  In this paper we present a component based person detection system that is capable of detecting frontal, rear and near side views of people, and partially occluded persons in cluttered scenes. The framework that is described here for people is easily applied to other objects as well. The motivation for developing a component based approach istwofold: rst, to enhance the performance of person detection systems on frontal and rear views of people and second, to develop a framework that directly addresses the problem of detecting people who are partially occluded or whose body parts blend in with the background. The data classi cation is handled by several support vector machine classi ers arranged in two layers. This architecture is known as Adaptive Combination of Classi ers (ACC). The system performs very well and is capable of detecting people even when all components of a person are not found. The performance of the system is signi cantly better than a full body
1047|Real-Time Object Detection for &#034;Smart&#034; Vehicles|This paper presents an efficient shape-based object detection method based on Distance Transforms and describes its use for real-time vision on-board vehicles. The method uses a template hierarchy to capture the variety of object shapes; efficient hierarchies can be generated offline for given shape distributions using stochastic optimization techniques (i.e. simulated annealing). Online, matching involves a simultaneous coarse-to-fine approach over the shape hierarchy and over the transformation parameters. Very large speedup factors are typically obtained when comparing this approach with the equivalent brute-force formulation; we have measured gains of several orders of magnitudes. We present experimental results on the real-time detection of traffic signs and pedestrians from a moving vehicle. Because of the highly time sensitive nature of these vision tasks, we also discuss some hardware-specific implementations of the proposed method as far as SIMD parallelism is concerned. 
1048|Efficient Matching of Pictorial Structures|A pictorial structure is a collection of parts arranged in a deformable configuration. Each part is represented using a simple appearance model and the deformable configuration is represented by spring-like connections between pairs of parts. While pictorial structures were introduced a number of years ago, they have not been broadly applied to matching and recognition problems. This has been due in part to the computational difficulty of matching pictorial structures to images. In this paper we present an efficient algorithm for finding the best global match of a pictorial structure to an image. The running time of the algorithm is optimal and it it takes only a few seconds to match a model with five to ten parts. With this improved algorithm, pictorial structures provide a practical and powerful framework for qualitative descriptions of objects and scenes, and are suitable for many generic image recognition problems. We illustrate the approach using simple models of a person and a car.
1049|Orientation histograms for hand gesture recognition|We present a method to recognize hand gestures, based on a pattern recognition technique developed by McConnell [16] employing histograms of local orientation. We use the orientation histogram as a feature vector for gesture classification and interpolation. For moving or ¨dynamic gestures ¨ , the histogram of the spatio-temporal gradients of image intensity form the analogous feature vector and may be useful for dynamic gesture recognition.
1050|Object Detection Using the Statistics of Parts| In this paper we describe a trainable object detector and its instantiations for detecting faces and cars at any size, location, and pose. To cope with variation in object orientation, the detector uses multiple classifiers, each spanning a different range of orientation. Each of these classifiers determines whether the object is present at a specified size within a fixed-size image window. To find the object at any location and size, these classifiers scan the image exhaustively. Each classifier is based on the statistics of localized parts. Each part is a transform from a subset of wavelet coefficients to a discrete set of values. Such parts are designed to capture various combinations of locality in space, frequency, and orientation. In building each classifier, we gathered the class-conditional statistics of these part values from representative samples of object and non-object images. We trained each classifier to minimize classification error on the training set by using Adaboost with Confidence-Weighted Predictions (Shapire and Singer, 1999). In detection, each classifier computes the part values within the image window and looks up their associated classconditional probabilities. The classifier then makes a decision by applying a likelihood ratio test. For efficiency, the classifier evaluates this likelihood ratio in stages. At each stage, the classifier compares the partial likelihood ratio to a threshold and makes a decision about whether to cease evaluation—labeling the input as non-object—or to continue further evaluation. The detector orders these stages of evaluation from a low-resolution to a high-resolution search of the image. Our trainable object detector achieves reliable and efficient detection of human faces and passenger cars with out-of-plane rotation.
1051|Probabilistic Methods  for Finding People|Finding people in pictures presents a particularly difficult object recognition problem. We show how to find people by finding candidate body segments, and then constructing assemblies of segments that are consistent with the constraints on the appearance of a person that result from kinematic properties. Since a reasonable model of a person requires at least nine segments, it is not possible to inspect every group, due to the huge combinatorial complexity. We propose two
1052|Computer Vision for Computer Games|The appeal of computer games may be enhanced by vision-based user inputs. The high speed and low cost requirements for near-term, mass-market game applications make system design challenging. The response time of the vision interface should be less than a video frame time and the interface should cost less than $50 U.S.
1053|Vision-based pedestrian detection: The PROTECTOR system|Abstract — This paper presents the results of the first largescale field tests on vision-based pedestrian protection from a moving vehicle. Our PROTECTOR system combines pedestrian detection, trajectory estimation, risk assessment and driver warning. The paper pursues a “system approach ” related to the detection component. An optimization scheme models the system as a succession of individual modules and finds a good overall parameter setting by combining individual ROCs using a convex-hull technique. On the experimental side, we present a methodology for the validation of the pedestrian detection performance in an actual vehicle setting. We hope this test methodology to contribute towards the establishment of benchmark testing, enabling this application to mature. We validate the PROTECTOR system using the proposed methodology and present interesting quantitative results based on tens of thousands of images from hours of driving. Although results are promising, more research is needed before such systems can be placed at the hands of ordinary vehicle drivers. I.
1054|Basic concepts and taxonomy of dependable and secure computing| This paper gives the main definitions relating to dependability, a generic concept including as special case such attributes as reliability, availability, safety, integrity, maintainability, etc. Security brings in concerns for confidentiality, in addition to availability and integrity. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability and security (faults, errors, failures), their attributes, and the means for their achievement (fault prevention, fault tolerance, fault removal, fault forecasting). The aim is to explicate a set of general concepts, of relevance across a wide range of situations and, therefore, helping communication and cooperation among a number of scientific and technical communities, including ones that are concentrating on particular types of system, of system failures, or of causes of system failures.
1055|Efficient dispersal of information for security, load balancing, and fault tolerance|Abstract. An Information Dispersal Algorithm (IDA) is developed that breaks a file F of length L =  ( F ( into n pieces F,, 1 5 i 5 n, each of length ( F, 1 = L/m, so that every m pieces suffice for reconstructing F. Dispersal and reconstruction are computationally efficient. The sum of the lengths ( F, 1 is (n/m). L. Since n/m can be chosen to be close to I, the IDA is space eflicient. IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communi-cations between processors in parallel computers. For the latter problem provably time-efftcient and highly fault-tolerant routing on the n-cube is achieved, using just constant size buffers. Categories and Subject Descriptors: E.4 [Coding and Information Theory]: nonsecret encoding schemes
1056|Understanding Fault-Tolerant Distributed Systems|We propose a small number of basic concepts that can be used to explain the architecture of fault-tolerant distributed systems and we discuss a list of architectural issues that we find useful to consider when designing or examining such systems. For each issue we present known solutions and design alternatives, we discuss their relative merits and we give examples of systems which adopt one approach or the other. The aim is to introduce some order in the complex discipline of designing and understanding fault-tolerant distributed systems.  
1057|Why Do Computers Stop And What Can Be Done About It?|An analysis of the failure statistics of a commercially available fault-tolerant system shows that administration and software are the major contributors to failure. Various approaches to software fault-tolerance are then discussed -- notably process-pairs, transactions and reliable storage. It is pointed out that faults in production software are often soft (transient) and that a transaction mechanism combined with persistent processpairs provides fault-tolerant execution -- the key to software fault-tolerance.
1058|On evaluating the performability of degradable computing systems|Abstract-If the performance of a computing system is &#034;degradable,&#034; performance and reliability issues must be dealt with simultaneously in the process of evaluating system effectiveness. For this purpose, a unified measure, called &#034;performability, &#034; is introduced and the foundations of performability modeling and evaluation are established. A critical step in the modeling process is the introduction of a &#034;capability function &#034; which relates low-level system behavior to user-oriented performance levels. A hierarchical modeling scheme is used to formulate the capability function and capability is used, in turn, to evaluate performability. These techniques are then illustrated for a specific application: the performability evaluation of an aircraft computer in the environment of an air transport mission. Index Terms-Degradable computing systems, fault-tolerant computing, hierarchical modeling, performability evaluation, performance evaluation, reliability evaluation. I.
1059|Survivable Network Systems: An Emerging Discipline|: Society is growing increasingly dependent upon large-scale,  highly distributed systems that operate in unbounded network  environments. Unbounded networks, such as the Internet, have no  central administrative control and no unified security policy. Furthermore,  the number and nature of the nodes connected to such networks cannot  be fully known. Despite the best efforts of security practitioners, no  amount of system hardening can assure that a system that is connected  to an unbounded network will be invulnerable to attack. The discipline of  survivability can help ensure that such systems can deliver essential  services and maintain essential properties such as integrity,  confidentiality, and performance, despite the presence of intrusions.  Unlike the traditional security measures that require central control or  administration, survivability is intended to address unbounded network  environments. This report describes the survivability approach to helping  assure that a syste...
1060|Failure Mode Assumptions and Assumption Coverage|. A method is proposed for the formal analysis of failure mode assumptions and for the evaluation of the dependability of systems whose design correctness is conditioned on the validity of such assumptions. Formal definitions are given for the types of errors that can affect items of service delivered by a system or component. Failure mode assumptions are then formalized as assertions on the types of errors that a component may induce in its enclosing system. The concept of assumption coverage is introduced to relate the notion of partiallyordered assumption assertions to the quantification of system dependability. Assumption coverage is shown to be extremely important in systems requiring very high dependability. It is also shown that the need to increase system redundancy to accommodate more severe modes of component failure can sometimes result in a decrease in dependability. 1 Introduction and Overview  The definition of assumptions about the types of faults, the rate at which comp...
1061|Experimenting with Quantitative Evaluation Tools for Monitoring Operational Security|: This paper presents the results of an experiment of security evaluation. The evaluation method used is based on previous work involving modeling the system as a privilege graph exhibiting the security vulnerabilities and on the computation of measures representing the difficulty for a possible attacker to exploit these vulnerabilities and defeat the security objectives of the system. A set of tools has been developed to compute such measures and has been experimented to monitor a large real system during more than a year. The experiment results are presented and the validity of the measures is discussed. Finally, the practical usefulness of such tools for operational security monitoring is shown and a comparison with other existing approaches is given. 1 Introduction  Security is an increasing worry for most computing system administrators: computing systems are more and more vital for most companies and organizations, while these systems are made more and more vulnerable by new user...
1062|Verifying and validating software requirements and design specifications|This paper presents the following guideline information on verification and validation (V&amp;V) of software requirements and design specifications: Definitions of the terms &#034;verification &#034; and &#034;validation, &#034; and an explanation of their context in the software life-cycle; A description of the basic sequence of functions performed during software requirements and design V&amp;V An explanation, with examples: of the major software requirements and design V&amp;V criteria: completeness, consistency, feasibility, and testability; An evaluation of the relative cost and effectiveness of the major software requirements and design V&amp;V techniques with respect to the above criteria; An example V&amp;V checklist for software system reliability and availability. Based on the above, the paper provides recommendations of the combination of software requirements and design V&amp;V techniques most suitable for small, medium, and large software specifications. I. OBJECTIVES The basic objectives in verification and validation (V&amp;V) of software requirements and design specifications are to identify and resolve software problems and high-risk issues early in the software life-cycle. The main reason for doing this is indicated in Figure 1, (1). It shows that savings of up to 100:1 are possible by finding and fixing problems early rather than late in the life-cycle. Besides the major cost savings, there are also significant payoffs in improved
1063|The Capability Maturity Model for Software|This paper provides an overview of the latest version of the Capability Maturity Model for Software, CMM v1.1. Based on over six years of experience with software process improvement and the contributions of hundreds of reviewers, CMM v1.1 describes the software engineering and management practices that characterize organizations as they mature their processes for developing and maintaining software. This paper stresses the need for a process maturity framework to prioritize improvement actions, describes the process maturity framework of five maturity levels and the associated structural components, and discusses future directions for the CMM.  Keywords: capability maturity model, CMM, process maturity framework, software process improvement, process capability, process performance, maturity level, key process area, software process assessment, software capability evaluation. 1  1 Introduction  After two decades of unfulfilled promises about productivity and quality gains from applyi...
1064|Implementing Fault Tolerant Applications using Reflective Object-Oriented|This paper shows how refection and object-oriented programming can be used to ease the implementation of classical fault tolerance mechanisms in distributed applications. When the underlying runtime system does not provide fault tolerance transparently, classical approaches to implementing fault tolerance mechanisms ofren imply mixing functional programming with non-functional programming (e.g. error processing mechanisms). The use of reflection improves the transparency of fault tolerance mechanisms to the programmer and more generally provides a clearer separation between functional and non-functional programming. The implementations of some classical replication techniques using a reflective approach are presented in detail and illustrated by several examples, which have been prototyped on a network of Unix workstations. Lessons learnt from our experiments are drawn and future work is discussed. 1
1065|Formal Methods and their Role in the Certification of Critical Systems|This report is based on one prepared as a chapter for the FAA Digital Systems Validation Handbook (a guide to assist FAA Certification Specialists with Advanced Technology Issues).  1  Its purpose is to explain the use of formal methods in the specification and verification of software and hardware requirements, designs, and implementations, to identify the benefits, weaknesses, and difficulties in applying these methods to digital systems used in critical applications, and to suggest factors for consideration when formal methods are offered in support of certification. The presentation concentrates on the rationale for formal methods and on their contribution to assurance for critical applications within a context such as that provided by DO-178B (the guidelines for software used on board civil aircraft)  2  ; it is intended as an introduction for those to whom these topics are new. A more technical discussion of formal methods is provided in a companion report.  3 1  Digital Systems ...
1066|Self-repairing computers|Computer systems and their &#034;organs&#034;--microprocessors, applications and communications networks--are becoming ever more powerful. But they are also becoming ever more complex and therefore more susceptible to failure. As the costs of administration, oversight and downtime expand in response, scientists and engineers in the computer industry are working to enhance the dependability of their products. Significantly, many of their efforts aim to take humans (and the errors they inevitably engender) out of the loop.
1067|Formal Specification and Verification of a Fault-Masking and Transient-Recovery Model for Digital Flight-Control Systems|We present a formal model for fault-masking and transient-recovery among the replicated computers of digital flight-control systems. We establish conditions under which majority voting causes the same commands to be sent to the actuators as those that would be sent by a single computer that suffers no failures. The model and its analysis have been subjected to formal specification and mechanically checked verification using the Ehdm system. Keywords: digital flight control systems, formal methods, formal specification and verification, proof checking, fault tolerance, transient faults, majority voting, modular redundancy  Contents 1 Introduction 1  1.1 Digital Flight-Control Systems : : : : : : : : : : : : : : : : : : : : : 2 1.2 Fault Tolerance for DFCS : : : : : : : : : : : : : : : : : : : : : : : : 3 1.3 Formal Models for DFCS : : : : : : : : : : : : : : : : : : : : : : : : 11 1.3.1 Overview of the Fault-Masking Model Employed : : : : : : : 12  2 The Fault-Masking Model 17  2.1 A M...
1068|NonStop availability in a client/server environment”, Tandem|The popularity of client/server computing has grown enonnously in the last few years. Client/server architectures have become the standard campus LAN environment at most companies, and client/server architectures are beginning to be used for mission-critical applications. Because of this computing paradigm shift, client/server availability has become a very important issue. This paper presents a predictive model of client/server availability applied to a representative client/server environment. The model is based on client/server outage data and measures client/server availability using a new metric: user outage minutes, which measures the amount of downtime for each user. User outage minutes are calculated for all outage types: physical, design, operations, and environmental failures, as well as outages due to planned reconfigurations. The model includes outages due to all client, server, and network devices in a typical client/server environment. The model has been validated by showing that its results are very similar to downtime surveys and other outage reports in the literature. The major results from the model are: • Each client in today&#039;s mission-critical client/server environment experiences an
1069|Ariadne: A secure on-demand routing protocol for ad hoc networks|An ad hoc network is a group of wireless mobile computers (or nodes), in which individual nodes cooperate by forwarding packets for each other to allow nodes to communicate beyond direct wireless transmission range. Prior research in ad hoc networking has generally studied the routing problem in a non-adversarial setting, assuming a trusted environment. In this paper, we present attacks against routing in ad hoc networks, and we present the design and performance evaluation of a new secure on-demand ad hoc network routing protocol, called Ariadne. Ariadne prevents attackers or compromised nodes from tampering with uncompromised routes consisting of uncompromised nodes, and also prevents a large number of types of Denial-of-Service attacks. In addition, Ariadne is efficient, using only highly efficient symmetric cryptographic primitives.
1070|MACAW: Media access protocol for wireless lans|In recent years, a wide variety of mobile computing devices has emerged, including portables, palmtops, and personal digit al assistants. Providing adequate network connectivity y for these devices will require a new generation of wireless LAN technology. In this paper we study media access protocols for a single channel wireless LAN being developed at Xerox Corporation’s Palo Alto Research Center. We start with the MACA media access protocol first proposed by Karn [9] and later refined by Biba [3] which uses an RTS-CTS-DATA packet exchange and binary exponential backoff. Using packet-level simulations, we examine various performance and design issues in such protocols, Our analysis leads to a new protocol, MACAW, which uses an RTS-CTS-DS-DATA-ACK message exchange and includes a significantly different backoff algorithm. 1
1071|Packet Leashes: A Defense against Wormhole Attacks in Wireless Ad Hoc Networks|Abstract — As mobile ad hoc network applications are deployed, security emerges as a central requirement. In this paper, we introduce the wormhole attack, a severe attack in ad hoc networks that is particularly challenging to defend against. The wormhole attack is possible even if the attacker has not compromised any hosts, and even if all communication provides authenticity and confidentiality. In the wormhole attack, an attacker records packets (or bits) at one location in the network, tunnels them (possibly selectively) to another location, and retransmits them there into the network. The wormhole attack can form a serious threat in wireless networks, especially against many ad hoc network routing protocols and location-based wireless security systems. For example, most existing ad hoc network routing protocols, without some mechanism to defend against the wormhole attack, would be unable to find routes longer than one or two hops, severely disrupting communication. We present a new, general mechanism, called packet leashes, for detecting and thus defending against wormhole attacks, and we present a specific protocol, called TIK, that implements leashes. I.
1072|Keying hash functions for message authentication|The use of cryptographic hash functions like MD5 or SHA for message authentication has become a standard approach inmanyInternet applications and protocols. Though very easy to implement, these mechanisms are usually based on ad hoc techniques that lack a sound security analysis. We present new constructions of message authentication schemes based on a cryptographic hash function. Our schemes, NMAC and HMAC, are proven to be secure as long as the underlying hash function has some reasonable cryptographic strengths. Moreover we show, in a quantitativeway, that the schemes retain almost all the security of the underlying hash function. In addition our schemes are e cient and practical. Their performance is essentially that of the underlying hash function. Moreover they use the hash function (or its compression function) as a black box, so that widely available library code or hardware can be used to implement them in a simple way, and replaceability of the underlying hash function is easily supported.
1073|SEAD: Secure Efficient Distance Vector Routing for Mobile Wireless Ad Hoc Networks|An ad hoc network is a collection of wireless computers (nodes), communicating among themselves over possibly multihop paths, without the help of any infrastructure such as base stations or access points. Although many previous ad hoc network routing protocols have been based in part on distance vector approaches, they have generally assumed a trusted environment. In this paper, we design and evaluate the Secure Efficient Ad hoc Distance vector routing protocol (SEAD), a secure ad hoc network routing protocol based on the design of the Destination-Sequenced Distance-Vector routing protocol. In order to support use with nodes of limited CPU processing capability, and to guard against Denial-of-Service attacks in which an attacker attempts to cause other nodes to consume excess network bandwidth or processing time, we use efficient one-way hash functions and do not use asymmetric cryptographic operations in the protocol. SEAD performs well over the range of scenarios we tested, and is robust against multiple uncoordinated attackers creating incorrect routing state in any other node, even in spite of any active attackers or compromised nodes in the network.
1074|A Secure Routing Protocol for Ad Hoc Networks|Most recent ad hoc network research has focused on providing routing services without considering security. In this paper, we detail security threats against ad hoc routing protocols, specifically examining AODV and DSR. In light of these threats, we identify three different environments with distinct security requirements. We propose a solution to one, the managed-open scenario where no network infrastructure is pre-deployed, but a small amount of prior security coordination is expected. Our protocol, ARAN, is based on certificates and successfully defeats all identified attacks.
1075|Routing in Ad Hoc Networks of Mobile Hosts|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any centralized administration or standard support services. In such an environment, it may be necessary for one mobile host to enlist the aid of others in forwarding a packet to its destination, due to the limitedpropagation range of each mobile host’s wireless transmissions. Some previous attempts have been made to use conventional routing protocols for routing in ad hoc networks, treating each mobile host as a router This position paper points out a number of problems with this design and suggests a new approach based on separate route discovery and route maintenance protocols. 1.
1076|Rushing Attacks and Defense in Wireless Ad Hoc Network Routing Protocols|In an ad hoc network, mobile computers (or nodes) cooperate to forward packets for each other, allowing nodes to communicate beyond their direct wireless transmission range. Many proposed routing protocols for ad hoc networks operate in an on-demand fashion, as on-demand routing protocols have been shown to often have lower overhead and faster reaction time than other types of routing based on periodic (proactive) mechanisms. Significant attention recently has been devoted to developing secure routing protocols for ad hoc networks, including a number of secure ondemand routing protocols, that defend against a variety of possible attacks on network routing. In this paper, we present the rushing  attack, a new attack that results in denial-of-service when used against all previous on-demand ad hoc network routing protocols. For example, DSR, AODV, and secure protocols based on them, such as Ariadne, ARAN, and SAODV, are unable to discover routes longer than two hops when subject to this attack. This attack is also particularly damaging because it can be performed by a relatively weak attacker. We analyze why previous protocols fail under this attack. We then develop Rushing Attack Prevention (RAP),a  generic defense against the rushing attack for on-demand protocols. RAP incurs no cost unless the underlying protocol fails to find a working route, and it provides provable security properties even against the strongest rushing attackers.
1077|Caching Strategies in On-Demand Routing Protocols for Wireless Ad Hoc Networks|An on-demand routing protocol for wireless ad hoc networks is one that searches for and attempts to discover a route to some destination node only when a sending node originates a data packet addressed to that node. In order to avoid the need for such a route discovery to be performed before each data packet is sent, such routing protocols must cache routes previously discovered. This paper presents an analysis of the effects of different design choices for this caching in on-demand routing protocols in wireless ad hoc networks, dividing the problem into choices of cache structure, cache capacity,and cache timeout. Our analysis is based on the Dynamic Source Routing protocol (DSR), which operates entirely on-demand. Using detailed simulations of wireless ad hoc networks of 50 mobile nodes, we studied a large number of different caching algorithms that utilize a range of design choices, and simulated each cache primarily over a set of 50 different movement scenarios drawn from 5 differ...
1078|The Effects of On-Demand Behavior in Routing Protocols for Multi-Hop Wireless Ad Hoc Networks|Abstract—A number of different routing protocols proposed for use in multi-hop wireless ad hoc networks are based in whole or in part on what can be described as on-demand behavior. By ondemand behavior, we mean approaches based only on reaction to the offered traffic being handled by the routing protocol. In this paper, we analyze the use of on-demand behavior in such protocols, focusing on its effect on the routing protocol’s forwarding latency, overhead cost, and route caching correctness, drawing examples from detailed simulation of the dynamic source routing (DSR) protocol. We study the protocol’s behavior and the changes introduced by variations on some of the mechanisms that make up the protocol, examining which mechanisms have the greatest impact and exploring the tradeoffs that exist between them. Index Terms—Communication system routing, computer network performance, dynamic source routing (DSR) protocol, wireless ad hoc networks. I.
1079|Detecting Disruptive Routers: A Distributed Network Monitoring Approach|An attractive target for a computer system attacker is the router. An attacker in control of a router can disrupt communication by dropping or misrouting packets passing through the router. We present a protocol called WATCHERS that detects and reacts to routers that drop or misroute packets. WATCHERS is based on the principle of conservation of ow in a network: all data bytes sent into a node, and not destined for that node, are expected to exit the node. WATCHERS tracks this ow, and detects routers that violate the conservation principle. We show that WATCHERS has several advantages over existing network monitoring techniques. We argue that WATCH-ERS &#039; impact on router performance and WATCHERS&#039; memory requirements are reasonable for many environments. We demonstrate that in ideal conditions WATCHERS makes no false-positive diagnoses. We also describe how WATCHERS can be tuned to perform nearly as well in realistic conditions. c 1998 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE. Kirk Bradley&#039;s current a liation is SRI International, 333
1080|Mobile Network Estimation|Mobile systems must adapt their behavior to changing network conditions. To do this, they must accurately estimate available network capacity. Producing quality estimates is challenging because network observations are noisy, particularly in mobile, ad hoc networks. Current systems depend on simple, exponentially-weighted moving average (EWMA) filters. These filters are either able to detect true changes quickly or to mask observed noise and transients, but cannot do both. In this paper, we present four filters designed to react quickly to persistent changes while tolerating transient noise. Such filters are ##### when possible, but ###### when necessary, adapting their behavior to prevailing conditions. These filters are evaluated in a variety of networking situations, including persistent and transient change, congestion, and topology changes. We find that one filter, based on techniques from ########### ####### ##### ##, provides performance superior to the other three. Compared to two EWMA filters, one agile and the other stable, it is able to offer the agility of the former in four of five scenarios and the stability of the latter in three of four scenarios.
1081|An Efficient Message Authentication Scheme for Link State Routing|We study methods for reducing the cost of secure link state routing. In secure link state routing, routers may need to verify the authenticity of many routing updates, and some routers such as border routers may need to sign many routing updates. Previous work such as public-key based schemes either is very expensive computationally or has certain limitations. This paper presents an efficient solution, based on a detection-diagnosis-recovery approach, for the link state routing update authentication problem. Our scheme is scalable to handle large networks, applicable to routing protocols that use multiple-valued cost metrics, and applicable even when link states change frequently.  
1082|Almost Optimal Hash Sequence Traversal|We introduce a novel technique for computation of consecutive preimages of hash chains. Whereas traditional techniques have a memory-times-computation complexity of O(n) per output generated, the complexity of our technique is only O(log 2 n), where n is the length of the chain.
1083|Protecting Routing Infrastructures from Denial of Service Using Cooperative Intrusion Detection|We present a solution to the denial of service problem for routing infrastructures. When a network suffers from denial of service, packets cannot reach their destinations. Existing routing protocols are not well-equipped to deal with denial of service; a misbehaving router -- which may be caused by software/hardware faults, misconfiguration, or malicious attacks -- may be able to disable entire networks. To protect network infrastructures from routers that incorrectly drop packets and misroute packets, we hypothesize failure models for routers and present protocols that detect and respond to those misbehaving routers. Based on realistic assumptions, we prove that our protocols have the following properties: (1) A well-behaved router never incorrectly claims another router as a misbehaving router; (2) If a network has misbehaving routers, one or more of them can be located; (3) Misbehaving routers will eventually be removed.
1084|Implicit source routes for on-demand ad hoc network routing|In an ad hoc network, the use of source routing has many advantages, including simplicity, correctness, and flexibility. For example, all routing decisions for a packet are made by the sender of the packet, avoiding the need for up-to-date routing information at intermediate nodes and allowing the routes used to be trivially guaranteed loopfree. It is also possible for the sender to use different routes for different packets, without requiring coordination or explicit support by the intermediate nodes. In addition, on-demand source routing has performed very strongly when compared against other proposed protocol designs. However, source routing has the disadvantage of increased per-packet overhead due to the source route header that must be present in every packet originated or forwarded. In this paper, we propose and analyze the use in ad hoc networks of implicit source routing, and show that it preserves the advantages of source routing while avoiding the associated per-packet overhead in most cases. We evaluated this technique through detailed simulations of ad hoc networks based on the Dynamic Source Routing protocol (DSR), an on-demand ad hoc network routing protocol based on source routing. Although routing packet overhead increased slightly with implicit source routing, by about 12.3%, the total number of bytes of overhead decreased substantially, by between 44 and 86%. On all other metrics evaluated, the performance of DSR either did not change significantly or actually improved somewhat, due to indirect effects of the reduced routing overhead. 1.
1085|The market for corporate control: The scientific evidence|This paper reviews much of the scientific literature on the market for corporate control. The evidence indicates that corporate takeovers generate positive gains, that target firm shareholders benefit, and that bidding firm shareholders do not lose. The gains created by corporate takeovers do not appear to come from the creation of market power. With the exception of actions that exclude potential bidders, it is difficult to find managerial actions related to corporate control that harm shareholders. Finally, we argue the market for corporate control is best viewed as an arena in which managerial teams compete for the rights to manage corporate resources. 1. The analytical perspective 1.1. Definition Corporate control is frequently used to describe many phenomena ranging from the general forces that influence the use of corporate resources (such as legal and regulatory systems and competition in product and input markets) to the control of a majority of seats on a corporation’s board of directors. We define corporate control as the rights to determine the management of
1086|Theory of the Firm: Managerial Behavior, Agency Costs and Ownership Structure|This paper integrates elements from the theory of agency, the theory of property rights and the theory of finance to develop a theory of the ownership structure of the firm. We define the concept of agency costs, show its relationship to the ‘separation and control’ issue, investigate the nature of the agency costs generated by the existence of debt and outside equity, demonstrate who bears costs and why, and investigate the Pareto optimality of their existence. We also provide a new definition of the firm, and show how our analysis of the factors influencing the creation and issuance of debt and equity claims is a special case of the supply side of the completeness of markets problem. 
1087|Separation of ownership and control|This paper analyzes the survival of organizations in which decision agents do not bear a major share of the wealth effects of their decisions. This is what the literature on large corporations calls separation of â??ownershipâ? and â??control.â? Such separation of decision and risk bearing functions is also common to organizations like large professional partnerships, financial mutuals and nonprofits. We contend that separation of decision and risk bearing functions survives in these organizations in part because of the benefits of specialization of management and risk bearing but also because of an effective common approach to controlling the implied agency problems. In particular, the contract structures of all these organizations separate the ratification and monitoring of decisions from the initiation and implementation of the decisions.
1088|Measuring security price performance|Event studies focus on the impact of particular types of firm-specific events on the prices of the affected firms ’ securities. In this paper, observed stock return data are employed to examine various methodologies which are used 111 event studies to measure security price performance. Abnormal performance is Introduced into this data. We find that a simple methodology based on the market model performs well under a wide variety of conditions. In some situations, even simpler methods which do not explicitly adjust for marketwide factors or for risk perform no worse than the market model. We also show how misuse of any of the methodologies can result in false inferences about the presence of abnormal performance. 1. introduction and summary The impact of particular types of firm-specific events (e.g., stock splits, earnings reports) on the prices of the affected firms ’ securities has been the subject of a number of studies. A major concern in those ‘event ’ studies has been to assess the extent to which security price performance around the time of the event has been abnormal-- that is, the extent to which security returns were different from those which would have been appropriate, given
1089|Organization Theory and Methodology|The foundations are being put in place for a revolution in the science of organizations. Some major analytical building blocks for the development of a theory of organizations are outlined and discussed in this paper. This development of organization theory will be hastened by increased understanding of the importance of the choice of definitions, tautologies, analytical techniques, and types of evidence. The two literatures of agency theory are briefly discussed in light of these issues. Because accounting is an integral part of the structure of every organization, the development of a theory of organizations will be closely associated with the development of a theory of accounting. This theory will explain why organizations take the form they do, why they behave as they do, and why accounting practices take the form they do. Because such positive theories as these are required for purposeful decision making, their development will provide a better scientific basis for the decisions of managers, standard-setting boards, and government regulatory bodies.
1090|The effects of capital structure changes on security prices: A study of exchange offers|Thus study considers the impact of capnal structure change announcements on securrty prices. Statrstically srgnificant price adjustments m firms ’ common stock, preferred stock and debt related to these announcements are documented and alternatlv,e causes for these price changes are examined. The evidence is consistent with both corporate tax and wealth redistrrbutron effects, There IS also evidence that firms make decisions which do not maxrmrze stockholder wealth. In additron, a new approach to testmg the sigmticance of publrc announcements on security returns IS presented. I.
1091|Convertible debt issuance, capital structure change and leverage-related information: Some new evidence, Unpublished manuscript (Amos Tuck School of Business  (1982) |This paper provides evidence on the valuation effects of convertible debt issuance. Common stockholders earn significant negative abnormal returns at the initial announcement of a convert-ible debt offering, and also at the issuance date. In contrast, the average valuation effect on common stock at the announcement of non-convertible debt offerings is only marginally negative, and is zero at issuance. The significant negative average ffect on common stock value appears not to be systematically related to either the degree of leverage change induced by the convertible debt issuance or the extent to which the proceeds from issuance are used for new investment or to refinance existing debt. If, as appears likely, the issuance of convertible debt on average increases financial everage, these results are inconsistent with evidence from other recent studies document-ing common stock price effects of the same sign as the change in leverage. The evidence suggests that convertible debt offerings convey unfavorable information about the issuing firms, but the specific nature of such information remains unidentified. 1. Introduction and
1092|The Wealth Effect Of Target Share Repurchases|This paper examines the wealth impact of share repurchases that restrict participation to a particular sub-set of a firm’s stockholders. Repurchases at a premium from insiders and small shareholders increase the wealth of non-participating stockholders and are therefore consistent with the shareholders ’ interest hypothesis. However, privately negotiated repurchases of single blocks from stockholders unaffiliated with the firm reduce the wealth of non-participating stockholders. In contrast to the evidence for general repurchases, no positive wealth effect offsets the significant repurchase premium paid to the selling stockholder. Indeed, the wealth loss to non-participating stockholders is significantly greater than the premium paid. This evidence is inconsistent with the shareholders ’ interest hypothesis and supports the hypothesis that managers in their self-interest use single block repurchases to eliminate threats to their control over the firm’s resources. 1. Introduction and
1093|Assessing Competition in the Market for Corporate Acquisitions|Comments invited. Not for quotation without the author&#039;s permission.
1094|Stock issues and investment policy when firms have information investors do not have|acknowledged. The research reported here is part of the NBER&#039;s
1095|Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews|This paper presents a simple unsupervised  learning algorithm for classifying reviews  as recommended (thumbs up) or not recommended   (thumbs down). The classification  of a review is predicted by the  average semantic orientation of the  phrases in the review that contain adjectives  or adverbs. A phrase has a positive  semantic orientation when it has good associations  (e.g., &#034;subtle nuances&#034;) and a  negative semantic orientation when it has  bad associations (e.g., &#034;very cavalier&#034;). In  this paper, the semantic orientation of a  phrase is calculated as the mutual information  between the given phrase and the  word &#034;excellent&#034; minus the mutual  information between the given phrase and  the word &#034;poor&#034;. A review is classified as  recommended if the average semantic orientation  of its phrases is positive. The algorithm  achieves an average accuracy of  74% when evaluated on 410 reviews from  Epinions, sampled from four different  domains (reviews of automobiles, banks,  movies, and travel destinations). The accuracy  ranges from 84% for automobile  reviews to 66% for movie reviews.
1096|A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge|How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched. 
1097|Word Association Norms, Mutual Information, and Lexicography|This paper will propose an objective measure based on the information theoretic notion of mutual  information, for estimating word association norms from computer readable corpora. (The standard method  of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly  and unreliable.) The proposed measure, the association ratio, estimates word association norms directly  from computer readable corpora, making it possible to estimate norms for tens of thousands of words
1098|Predicting the Semantic Orientation of Adjectives|We identify and validate from a large corpus  constraints from conjunctions on the  positive or negative semantic orientation  of the conjoined adjectives. A log-linear  regression model uses these constraints to  predict whether conjoined adjectives are  of same or different orientations, achiev-  ing 82% accuracy in this task when each  conjunction is considered independently.
1099|Some advances in transformation-based part-of-speech tagging|Most recent research in trainable part of speech taggers has explored stochastic tagging. While these taggers obtain high accuracy, linguistic information is captured indirectly, typically in tens of thousands of lexical and contextual probabilities. In (Brill 1992), a trainable rule-based tagger was described that obtained performance comparable to that of stochastic taggers, but captured relevant linguistic information in a small number of simple non-stochastic rules. In this paper, we describe a number of extensions to this rule-based tagger. First, we describe a method for expressing lexical relations in tagging that stochastic taggers are currently unable to express. Next, we show a rule-based approach to tagging unknown words. Finally, we show how the tagger can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty.
1100|M.: A simple approach to ordinal classification|Abstract. Machine learning methods for classification problems commonly assume that the class values are unordered. However, in many practical applications the class values do exhibit a natural order—for example, when learning how to grade. The standard approach to ordinal classification converts the class value into a numeric quantity and applies a regression learner to the transformed data, translating the output back into a discrete class value in a post-processing step. A disadvantage of this method is that it can only be applied in conjunction with a regression scheme. In this paper we present a simple method that enables standard classification algorithms to make use of ordering information in class attributes. By applying it in conjunction with a decision tree learner we show that it outperforms the naive approach, which treats the class values as an unordered set. Compared to special-purpose algorithms for ordinal classification our method has the advantage that it can be applied without any modification to the underlying learning scheme. 1
1101|Smokey: Automatic Recognition of Hostile Messages|Abusive messages (flames) can be both a source of frustration and a waste of time for Internet users. This paper describes some approaches to flame recognition, including a prototype system, Smokey. Smokey builds a 47-element feature vector based on the syntax and semantics of each sentence, combining the vectors for the sentences within each message. A training set of 720 messages was used by  Quinlan&#039;s C4.5 decision-tree generator to determine featurebased rules that were able to correctly categorize 64% of the flames and 98% of the non-flames in a separate test set of 460 messages. Additional techniques for greater accuracy and user customization are also discussed. Introduction  Flames are one of the current hazards of on-line communication. While some people enjoy exchanging flames, most users consider these abusive and insulting messages to be a nuisance or even upsetting. I describe Smokey, a prototype system to automatically recognize email flames. Smokey combines natural-langu...
1102|Direction-Based Text Interpretation as an Information Access Refinement|A Text-Based Intelligent System should provide more in-depth information about the contents  of its corpus than does a standard information retrieval system, while at the same time  avoiding the complexity and resource-consuming behavior of detailed text understanders. Instead  of focusing on discovering documents that pertain to some topic of interest to the user,  an approach is introduced based on the criterion of directionality (e.g., Is the agent in favor of,  neutral, or opposed to the event?). A method is described for coercing sentence meanings into  a metaphoric model such that the only semantic interpretation needed in order to determine  the directionality of a sentence is done with respect to the model. This interpretation method  is designed to be an integrated component of a hybrid information access system.  1 Introduction  In the light of the increasing availability of computer-accessible full text, an important goal of a Text-Based Intelligent System is to provide a me...
1103|Solving multiclass learning problems via error-correcting output codes|Multiclass learning problems involve nding a de nition for an unknown function f(x) whose range is a discrete set containing k&gt;2values (i.e., k \classes&amp;quot;). The de nition is acquired by studying collections of training examples of the form hx i;f(x i)i. Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of over tting avoidance techniques such as decision-tree pruning. Finally,we show that|like the other methods|the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.  
1104|Parallel Networks that Learn to Pronounce English Text|This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (;i) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations. 
1105|Connectionist Learning Procedures|A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks. 
1106|When Networks Disagree: Ensemble Methods for Hybrid Neural Networks|This paper presents a general theoretical framework for ensemble methods of constructing significantly improved regression estimates. Given a population of regression estimators, we construct a hybrid estimator which is as good or better in the MSE sense than any estimator in the population. We argue that the ensemble method presented has several properties: 1) It efficiently uses all the networks of a population - none of the networks need be discarded. 2) It efficiently uses all the available data for training without over-fitting. 3) It inherently performs regularization by smoothing in functional space which helps to avoid over-fitting. 4) It utilizes local minima to construct improved estimates whereas other neural network algorithms are hindered by local minima. 5) It is ideally suited for parallel computation. 6) It leads to a very useful and natural measure of the number of distinct estimators in a population. 7) The optimal parameters of the ensemble estimator are given in clo...
1107|Learning Machines|This book is about machines that learn to discover hidden relationships in data. A constant sfream of data bombards our senses and millions of sensory channels carry information into our brains. Brains are also learning machines that condition,
1108|Graphs over Time: Densification Laws, Shrinking Diameters and Possible Explanations|How do real graphs evolve over time? What are “normal” growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network, or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over time. Here we study a wide range of real graphs, and we observe some surprising phenomena. First, most of these graphs densify over time, with the number of edges growing superlinearly in the number of nodes. Second, the average distance between nodes often shrinks over time, in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes (like O(log n) orO(log(log n)). Existing graph generation models do not exhibit these types of behavior, even at a qualitative level. We provide a new graph generator, based on a “forest fire” spreading process, that has a simple, intuitive justification, requires very few parameters (like the “flammability” of nodes), and produces graphs exhibiting the full range of properties observed both in prior work and in the present study.  
1110|The structure and function of complex networks|Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.
1111|On Power-Law Relationships of the Internet Topology|Despite the apparent randomness of the Internet, we discover some surprisingly simple power-laws of the Internet topology. These power-laws hold for three snapshots of the Internet, between November 1997 and December 1998, despite a 45% growth of its size during that period. We show that our power-laws fit the real data very well resulting in correlation coefficients of 96% or higher. Our observations provide a novel perspective of the structure of the Internet. The power-laws describe concisely skewed distributions of graph properties such as the node outdegree. In addition, these power-laws can be used to estimate important parameters such as the average neighborhood size, and facilitate the design and the performance analysis of protocols. Furthermore, we can use them to generate and select realistic topologies for simulation purposes.  
1112|A Brief History of Generative Models for Power Law and Lognormal Distributions |Recently, I became interested in a current debate over whether file size distributions are best modelled by a power law distribution or a a lognormal distribution. In trying
1113|Trawling the Web for emerging cyber-communities|Abstract: The web harbors a large number of communities-- groups of content-creators sharing a common interest-- each of which manifests itself as a set of interlinked web pages. Newgroups and commercial web directories together contain of the order of 20000 such communities; our particular interest here is on emerging communities-- those that have little or no representation in such fora. The subject of this paper is the systematic enumeration of over 100,000 such emerging communities from a web crawl: we call our process trawling. We motivate a graph-theoretic approach to locating such communities, and describe the algorithms, and the algorithmic engineering necessary to find structures that subscribe to this notion, the challenges in handling such a huge data set, and the results of our experiment.
1114|Stochastic Models for the Web Graph|The web may be viewed as a directed graph each of whose vertices is a static HTML web page, and each of whose edges corresponds to a hyperlink from one web page to another. In this paper we propose and analyze random graph models inspired by a series of empirical observations on the web. Our graph models differ from the traditional Gn;p models in two ways: 1. Independently chosen edges do not result in the statistics (degree distributions, clique multitudes) observed on the web. Thus, edges in our model are statistically dependent on each other. 2. Our model introduces new vertices in the graph as time evolves. This captures the fact that the web is changing with time. Our results are two fold: we show that graphs generated using our model exhibit the statistics observed on the web graph, and additionally, that natural graph models proposed earlier do not exhibit them. This remains true even when these earlier models are generalized to account for the arrival of vertices over time. In particular, the sparse random graphs in our models exhibit properties that do not arise in far denser random graphs generated by Erdos-R&#039;enyi models. 
1115|The Average Distance in a Random Graph with Given Expected Degrees | Random graph theory is used to examine the “small-world phenomenon”– any two strangers are connected through a short chain of mutual acquaintances. We will show that for certain families of random graphs with given expected degrees, the average distance is almost surely of order log n / log ˜ d where ˜ d is the weighted average of the sum of squares of the expected degrees. Of particular interest are power law random graphs in which the number of vertices of degree k is proportional to 1/k ß for some fixed exponent ß. For the case of ß&gt; 3, we prove that the average distance of the power law graphs is almost surely of order log n / log ˜ d. However, many Internet, social, and citation networks are power law graphs with exponents in the range 2 &lt; ß &lt; 3 for which the power law random graphs have average distance almost surely of order log log n, but have diameter of order log n (provided having some mild constraints for the average distance and maximum degree). In particular, these graphs contain a dense subgraph, that we call the core, having n c / log log n vertices. Almost all vertices are within distance log log n of the core although there are vertices at distance log n from the core.
1116|R-MAT: A recursive model for graph mining|How does a ‘normal ’ computer (or social) network look like? How can we spot ‘abnormal ’ sub-networks in the Internet, or web graph? The answer to such questions is vital for outlier detection (terrorist networks, or illegal money-laundering rings), forecasting, and simulations (“how will a computer virus spread?”). The heart of the problem is finding the properties of real graphs that seem to persist over multiple disciplines. We list such “laws ” and, more importantly, we propose a simple, parsimonious model, the “recursive matrix ” (R-MAT) model, which can quickly generate realistic graphs, capturing the essence of each graph in only a few parameters. Contrary to existing generators, our model can trivially generate weighted, directed and bipartite graphs; it subsumes the celebrated Erdos-Rényi model as a special case; it can match the power law behaviors, as well as the deviations from them (like the “winner does not take it all ” model of Pennock et al. [21]). We present results on multiple, large real graphs, where we show that our parameter fitting algorithm (AutoMAT-fast) fits them very well. 1
1117|The NBER patent citations data file: Lessons, insights and methodological tools|This paper pulls together material from multiple research projects spanning approximately a decade, and hence it would be impossible to thank everyone who contributed to it. Still, our co-authors Rebecca Henderson and Michael Fogarty, and research assistants Meg Fernando, Abi Rubin, Guy Michaels and Michael Katz deserve special thanks. Above all, this paper – indeed much of the patent-related research of the past two decades – could never have come to pass without the inspiration, support and
1118|Identity and search in social networks|Social networks have the surprising property of being “searchable”: ordinary people are capable of directing messages through their network of acquaintances to reach a specific but distant target person in only a few steps. We present a model that offers an explanation of social network searchability in terms of recognizable personal identities defined along a number of social dimensions. Our model defines a class of searchable networks and a method for searching them that may be applicable to many network search problems including the location of data files in peer-to-peer networks, pages on the World Wide Web, and information in distributed databases. In the late 1960’s, Travers and Milgram [1] conducted an experiment in which randomly selected individuals
1119|Small-World Phenomena and the Dynamics of Information|Introduction The problem of searching for information in networks like the World Wide Web can be approached in a variety of ways, ranging from centralized indexing schemes to decentralized mechanisms that navigate the underlying network without knowledge of its global structure. The decentralized approach appears in a variety of settings: in the behavior of users browsing the Web by following hyperlinks; in the design of focused crawlers [4, 5, 8] and other agents that explore the Web&#039;s links to gather information; and in the search protocols underlying decentralized peer-to-peer systems such as Gnutella [10], Freenet [7], and recent research prototypes [21, 22, 23], through which users can share resources without a central server. In recent work, we have been investigating the problem of decentralized search in large information networks [14, 15]. Our initial motivation was an experiment that dealt directly with the search problem in a decidedly pre-Internet context: Stanley Milgram
1120|ANF: A Fast and Scalable Tool for Data Mining in Massive Graphs|Graphs are an increasingly important data source, with such important graphs as the Internet and the Web. Other familiar graphs include CAD circuits, phone records, gene sequences, city streets, social networks and academic citations. Any kind of relationship, such as actors appearing in movies, can be represented as a graph. This work presents a data mining tool, called ANF, that can quickly answer a number of interesting questions on graph-represented data, such as the following. How robust is the Internet to failures? What are the most influential database papers? Are there gender differences in movie appearance patterns? At its core, ANF is based on a fast and memory-efficient approach for approximating the complete &#034;neighbourhood function&#034; for a graph. For the Internet graph (268K nodes), ANF&#039;s highly-accurate approximation is more than 700 times faster than the exact computation. This reduces the running time from nearly a day to a matter of a minute or two, allowing users to perform ad hoc drill-down tasks and to repeatedly answer questions about changing data sources. To enable this drill-down, ANF employs new techniques for approximating neighbourhood-type functions for graphs with distinguished nodes and/or edges. When compared to the best existing approximation, ANF&#039;s approach is both faster and more accurate, given the same resources. Additionally, unlike previous approaches, ANF scales gracefully to handle disk resident graphs. Finally, we present some of our results from mining large graphs using ANF.
1121|A General Model of Web Graphs|We describe a very general model of a random graph process whose proportional degree sequence obeys a power law. Such laws have recently been observed in graphs associated with the world wide web.
1122|A Functional Approach to External Graph Algorithms|. We present a new approach for designing external graph algorithms  and use it to design simple external algorithms for computing connected components,  minimum spanning trees, bottleneck minimum spanning trees, and maximal  matchings in undirected graphs and multi-graphs. Our I/O bounds compete  with those of previous approaches. Unlike previous approaches, ours is purely  functional---without side effects---and is thus amenable to standard checkpointing  and programming language optimization techniques. This is an important  practical consideration for applications that may take hours to run.  1 Introduction  We present a divide-and-conquer approach for designing external graph algorithms, i.e., algorithms on graphs that are too large to fit in main memory. Our approach is simple to describe and implement: it builds a succession of graph transformations that reduce to sorting, selection, and a recursive bucketing technique. No sophisticated data structures are needed. We apply our t...
1123|The ‘DGX’ distribution for mining massive, skewed data|Skewed distributions appear very often in practice. Unfortunately, the traditional Zipf distribution often fails to model them well. In this paper, we propose a new probability distribution, the Discrete Gaussian Exponential (DGX), to achieve excellent fits in a wide variety of settings; our new distribution includes the Zipf distribution as a special case. We present a statistically sound method for estimating the DGX parameters based on maximum likelihood estimation (MLE). We applied DGX to a wide variety of real world data sets, such as sales data from a large retailer chain, usage data from AT&amp;T, and Internet clickstream data; in all cases, DGX fits these distributions very well, with almost a 99 % correlation coefficient in quantile-quantile plots. Our algorithm also scales very well because it requires only a single pass over the data. Finally, we illustrate the power of DGX as a new tool for data mining tasks, such as outlier detection. Keywords DGX, Zipf’s law, rank-frequency plot, frequency-count plot, maximum likelihood estimation, lognormal distribution, outlier detection
1124|Overview of the 2003 kdd cup|This paper surveys the 2003 KDD Cup, a competition held in conjunction with the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) in August 2003. The competition focused on mining the complex real-life social network inherent in the e-print arXiv (arXiv.org). We describe the four KDD Cup tasks: citation prediction, download prediction, data cleaning, and an open task. 1.
1125|Scale Independent Bibliometric Indicators|Van Raan (this issue) makes an excellent case for using bibliometric data to measure some central aspects of scientific research and to construct indicators of groups: research groups, university departments, and institutes. He claims that, next to peer review, these indicators are indispensable for evaluating research and can be used in parallel with peer review processes. By way of an example, van Raan provides a table containing nine indicators for a German medical research institute. Two of these indicators—articles (P) and citations (C)—are established proxy measures for the size of a group and the impact of its published research (Katz &amp; Hicks, 1997). The ratio between citations and articles (CPP) and the ratios between CPP and the mean Journal Citation Score and between the field-based world average and the Germany-specific world average—which are uniquely defined CPP reference values—are used to construct a set of indicators that van Raan suggests can be used to assess international research performance. This commentary focuses solely on the use of bibliometric indicators to compare international research performance. It addresses the fundamental question of whether CPP or measures like CPP can be used to accurately compare the performance of groups of different sizes. SCALING RELATIONS A scaling relation exists between two entities, x and y, if they are correlated by a power law given by the equation y = kx n, where n is the scaling factor and k is a constant. There is evidence to suggest that C and P have a scaling relation when
1126|Tandem repeats finder: a program to analyze DNA sequences|A tandem repeat in DNA is two or more contiguous, approximate copies of a pattern of nucleotides. Tandem repeats have been shown to cause human disease, may play a variety of regulatory and evolutionary roles and are important laboratory and analytic tools. Extensive knowledge about pattern size, copy number, mutational history, etc. for tandem repeats has been limited by the inability to easily detect them in genomic sequence data. In this paper, we present a new algorithm for finding tandem repeats which works without the need to specify either the pattern or pattern size. We model tandem repeats by percent identity and frequency of indels between adjacent pattern copies and use statistically based recognition criteria. We demonstrate the algorithm&#039;s speed and its ability to detect tandem repeats that have undergone extensive mutational change by analyzing four sequences: the human frataxin gene, the human b T cell receptor locus sequence and two yeast chromosomes. These sequences range in size from 3 kb up to 700 kb. A World Wide Web server interface at c3.biomath.mssm.edu/trf.html has been established for automated use of the program.  
1127|Power-law distributions in empirical data|Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the empirical detection and characterization of power laws is made difficult by the large fluctuations that occur in the tail of the distribution. In particular, standard methods such as least-squares fitting are known to produce systematically biased estimates of parameters for power-law distributions and should not be used in most circumstances. Here we describe statistical techniques for making accurate parameter estimates for power-law data, based on maximum likelihood methods and the Kolmogorov-Smirnov statistic. We also show how to tell whether the data follow a power-law distribution at all, defining quantitative measures that indicate when the power law is a reasonable fit to the data and when it is not. We demonstrate these methods by applying them to twentyfour real-world data sets from a range of different disciplines. Each of the data sets has been conjectured previously to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.
1128|A Random Graph Model for Massive Graphs|  We propose a random graph model which is a special case of sparse random graphs with given degree sequences. This model involves only a small number of parameters, called logsize and log-log growth rate. These parameters capture some universal characteristics of massive graphs. Furthermore, from these parameters, various properties of the graph can be derived. For example, for certain ranges of the parameters, we will compute the expected distribu-tion of the sizes of the connected components which almost surely occur with high probability. We will illustrate the consistency of our model with the behavior of some massive graphs derived from data in telecommunications. We will also discuss the threshold function, the giant component, and the evolution of random graphs in this model.  
1129|Collective entity resolution in relational data|Many databases contain uncertain and imprecise references to real-world entities. The absence of identifiers for the underlying entities often results in a database which contains multiple references to the same entity. This can lead not only to data redundancy, but also inaccuracies in query processing and knowledge extraction. These problems can be alleviated through the use of entity resolution. Entity resolution involves discovering the underlying entities and mapping each database reference to these entities. Traditionally, entities are resolved using pairwise similarity over the attributes of references. However, there is often additional relational information in the data. Specifically, references to different entities may cooccur. In these cases, collective entity resolution, in which entities for cooccurring references are determined jointly rather than independently, can improve entity resolution accuracy. We propose a novel relational clustering algorithm that uses both attribute and relational information for determining the underlying domain entities, and we give an efficient implementation. We investigate the impact that different relational similarity measures have on entity resolution quality. We evaluate our collective entity resolution algorithm on multiple real-world databases. We show that it improves entity resolution performance over both attribute-based baselines and over algorithms that consider relational information but do not resolve entities collectively. In addition, we perform detailed experiments on synthetically generated data to identify data characteristics that favor collective relational resolution over purely attribute-based algorithms.
1130|Where mathematics meets the Internet|The Internet has experienced a fascinating evolution in the recent past, especially since the early days of the Web, a fact well-documented not only in the trade journals, but also in the popular press. Unprecedented in its growth, unparalleled in its heterogeneity, and unpredictable or even chaotic in the behavior of its tra c, \the Internet is its own revolution&#034;, as Anthony-Michael Rutkowski, former Executive Director of the Internet Society, likes to put it.
1131|Problems with fitting to the powerlaw distribution |Abstract. This short communication uses a simple experiment to show that fitting to a power law distribution by using graphical methods based on linear fit on the log-log scale is biased and inaccurate. It shows that using maximum likelihood estimation (MLE) is far more robust. Finally, it presents a new
1132|Functional and topological characterization of protein interaction networks|The elucidation of the cell’s large-scale organization is a primary challenge for post-genomic biology, and understanding the structure of protein interaction networks offers an important starting point for such studies. We compare four available databases that approximate the protein interaction network of the yeast, Saccharomyces cerevisiae, aiming to uncover the network’s generic large-scale properties and the impact of the proteins ’ function and cellular localization on the network topology. We show how each database supports a scale-free, topology with hierarchical modularity, indicating that these features represent a robust and generic property of the protein interactions network. We also find strong correlations between the network’s structure and the functional role and subcellular localization of its protein constituents, concluding that most functional and/or localization classes appear as relatively segregated subnetworks of the full protein interaction network. The uncovered systematic differences between the four protein interaction databases reflect their relative coverage for different functional and localization classes and provide a guide for their utility in various bioinformatics studies. Keywords: Bioinformatics / Protein interaction networks / Scale-free networks 1
1133|On the bias of traceroute sampling: or, power-law degree distributions in regular graphs|Understanding the graph structure of the Internet is a crucial step for building accurate network models and designing efficient algorithms for Internet applications. Yet, obtaining this graph structure can be a surprisingly difficult task, as edges cannot be explicitly queried. For instance, empirical studies of the network of Internet Protocol (IP) addresses typically rely on indirect methods like traceroute to build what are approximately single-source, all-destinations, shortest-path trees. These trees only sample a fraction of the network’s edges, and a recent paper by Lakhina et al. found empirically that the resulting sample is intrinsically biased. Further, in simulations, they observed that the degree distribution under traceroute sampling exhibits a power law even when the underlying degree distribution is Poisson. In this paper, we study the bias of traceroute sampling mathematically and, for a very general class of underlying degree distributions, explicitly calculate the distribution that will be observed. As example applications of our machinery, we prove that traceroute sampling finds power-law degree distributions in both d-regular and Poisson-distributed random graphs. Thus, our work puts the observations of Lakhina et al. on a rigorous footing, and extends them to nearly arbitrary degree distributions.
1134|Currency and commodity metabolites: Their identification and relation to the modularity of metabolic networks|The large-scale shape and function of metabolic networks are intriguing topics of systems biology. Such networks are on one hand commonly regarded as modular (i.e. built by a number of relatively independent subsystems), but on the other hand they are robust in a way not expected of a purely modular system. To address this question we carefully discuss the partition of metabolic networks into subnetworks. The practice of preprocessing such networks by removing the most abundant substrates, “currency metabolites,” is formalized into a network-based algorithm. We study partitions for metabolic networks of many organisms and find cores of currency metabolites and modular peripheries of what we call “commodity metabolites.” The networks are found to be more modular than random networks but far from perfectly divisible into modules. We argue that cross-modular edges are the key for the robustness of metabolism. 
1135|Likelihood-Based Inference for Stochastic Models of Sexual Network Formation|Sexually-Transmitted Diseases (STDs) constitute a major public health concern. Mathematical models for the transmission dynamics of STDs indicate that heterogeneity in sexual activity level allow them to persist even when the typical behavior of the population would not support endemicity. This insight focuses attention on the distribution of sexual activity level in a population. In this paper, we develop several stochastic process models for the f&#039;ormation of sexual partnership networks. Using likelihood-based model selection procedures, we assess the fit of the different models to three large distributions of sexual partner counts: (1) Rakai, Uganda, (2) Sweden, and (3) the USA. Five of&#039; the six single-sex networks were fit best by the negative binomial model. The American women&#039;s network was best fit by a power-law model, the Yule. For most networks, several competing models fit approximately equally well. These results sug- gest three conclusions: (1) no single unitary process clearly underlies the formation of these sexual networks, (2) behavioral heterogeneity plays an essential role in network structure, (3) substantial model uncertainty exists for sexual network degree distributions. Behavioral research focused on the mechanisms of partnership f&#039;ormation will play an essential role in specifying the best model for empirical degree distributions. We discuss the limitations of inferences f&#039;rom such data, and the utility of degree-based epidemiological models more generally.
1136|Editorial: The future of power law research |Abstract. I argue that power law research must move from focusing on observation, interpretation, and modeling of power law behavior to instead considering the challenging problems of validation of models and control of systems. 1. The Problem with Power Law Research To begin, I would like to recall a humorous insight from the paper of Fabrikant, Koutsoupias, and Papadimitriou [Fabrikant et al. 01], consisting of this quote and the following footnote. “Power laws... have been termed ‘the signature of human activity’... ” 1 The study of power laws, especially in networks, has clearly exploded over the last decade, with seemingly innumerable papers and even popular books, such as Barabási’s Linked [Barabási 02] and Watts ’ Six Degrees [Watts 03]. Power laws are, indeed, everywhere. Despite this remarkable success, I believe that research into power laws in computer networks (and networks more generally) suffers from glaring deficiencies that need to be addressed by the community. Coping with these deficiencies should lead to another great burst of exciting and compelling research. To explain the problem, I would like to make an analogy to the area of string theory. String theory is incredibly rich and beautiful mathematically, with a simple and compelling basic starting assumption: the universe’s building blocks do not really correspond to (zero-dimensional) points, but to small 1 “They are certainly the product of one particular kind of human activity: looking for power laws... ” [Fabrikant et al. 01]
1137|DYNAMICS OF BAYESIAN UPDATING WITH DEPENDENT DATA AND MISSPECIFIED MODELS|Recent work on the convergence of posterior distributions under Bayesian updating has established conditions under which the posterior will concentrate on the truth, if the latter has a perfect representation within the support of the prior, and under various dynamical assumptions, such as the data being independent and identically distributed or Markovian. Here I establish sufficient conditions for the convergence of the posterior distribution in non-parametric problems even when all of the hypotheses are wrong, and the data-generating process has a complicated dependence structure. The main dynamical assumption is the generalized asymptotic equipartition (or “Shannon-McMillan-Breiman”) property of information theory. I derive a kind of large deviations principle for the posterior measure, and discuss the advantages of predicting using a combination of models known to be wrong. An appendix sketches connections between the present results and the “replicator dynamics” of evolutionary theory.  
1138|On the frequency of severe terrorist events|The online version of this article can be found at:
1139|Radial structure of the internet|The structure of the Internet at the autonomous system (AS) level has been studied by the
1140|Estimating heavy–tail exponents through max self–similarity|2 Heavy tailed data • A random variable X is said to be heavy–tailed if P{|X |  = x}  ~ L(x)x -a, as x ? 8, for some a&gt; 0 and a slowly varying function L. ? Here we focus on the simpler but important context: X = 0, a.s. and P{X&gt; x}  ~ Cx -a, as x ? 8. ? X (infinite moments) For p&gt; 0, EX p &lt; 8 if and only if p &lt; a. In particular, and 0 &lt; a = 2 ? Var(X)  = 8 0 &lt; a = 1 ? E|X |  = 8. • The estimation of the heavy–tail exponent a is an important problem with rich history. • Why do we need heavy–tail models? Every finite sample X1,..., Xn has finite sample mean, variance and all sample moments! Why consider heavy tailed models in practice?! 3 Why use heavy–tailed models? “All models are wrong, but some are useful.” George Box Let F and G be any two distributions with positive densities on (0, 8). Let ?&gt; 0 and x1,..., xn ? (0, 8) be arbitrary, then both: and PF {Xi ? (xi - ?, xi + ?), i = 1,..., n}&gt; 0 are positive! PG{Xi ? (xi - ?, xi + ?), i = 1,..., n}&gt; 0 • For a given sample, very many models apply. • The ones that continue to work as the sample grows are most suitable. We next present real data sets of Financial, Insurance and Internet data. They can be very heavy tailed. 4 Traded volumes on the Intel stock
1141|Empirical distributions of logreturns: Between the stretched exponential and the power law? Quantitative Finance |A large consensus now seems to take for granted that the distributions of empirical returns of financial time series are regularly varying, with a tail exponent b close to 3. First, we show by synthetic tests performed on time series with time dependence in the volatility with both Pareto and Stretched-Exponential distributions that for sample of moderate size, the standard generalized extreme value (GEV) estimator is quite inefficient due to the possibly slow convergence toward the asymptotic theoretical distribution and the existence of biases in presence of dependence between data. Thus it cannot distinguish reliably between rapidly and regularly varying classes of distributions. The Generalized Pareto distribution (GPD) estimator works better, but still lacks power in the presence of strong dependence. Then, we use a parametric representation of the tail of the distributions of returns of 100 years of daily return of the Dow Jones Industrial Average and over 1 years of 5-minutes returns of the Nasdaq Composite index, encompassing both a regularly varying distribution in one limit of the parameters and rapidly varying distributions of the class of the Stretched-Exponential (SE) and Log-Weibull distributions in other limits. Using the method of nested hypothesis testing (Wilks ’ theorem),
1142|Representing twentieth century space-time climate variability, part 1: development of a 1961-90 mean monthly terrestrial climatology|The construction of a 0.58 lat 3 0.58 long surface climatology of global land areas, excluding Antarctica, is described. The climatology represents the period 1961–90 and comprises a suite of nine variables: precipitation, wet-day frequency, mean temperature, diurnal temperature range, vapor pressure, sunshine, cloud cover, ground frost frequency, and wind speed. The climate surfaces have been constructed from a new dataset of station 1961–90 climatological normals, numbering between 19 800 (precipitation) and 3615 (wind speed). The station data were interpolated as a function of latitude, longitude, and elevation using thin-plate splines. The accuracy of the interpolations are assessed using cross validation and by comparison with other climatologies. This new climatology represents an advance over earlier published global terrestrial climatologies in that it is strictly constrained to the period 1961–90, describes an extended suite of surface climate variables, explicitly incorporates elevation as a predictor variable, and contains an evaluation of regional errors associated with this and other commonly used climatologies. The climatology is already being used by researchers in the areas of ecosystem modelling, climate model evaluation, and climate change impact assessment. The data are available from the Climatic Research Unit and images of all the monthly fields can be accessed via the World Wide Web. 1.
1143|Global trends of measured surface air temperature|We analyze surface air temperature data from available meteorological stations with principal focus on the period 1880-1985. The temperature changes at mid- and high latitude stations separated by less than 1000 km are shown to be highly correlated; at low latitudes the correlation falls off more rapidly with distance for nearby stations. We combine the station data in a way which is designed to provide accurate long-term variations. Error estimates are based in part on studies of how accurately the actual station distributions are able to reproduce temperature change in a global data set produced by a threedimensional general circulation model with realistic variability. We find that meaningful global temperature change can be obtained for the past century, despite the fact that the meteorological stations are confined mainly to continental and island locations. The results indicate a global warming of about 0.5ø-0.7øC in the past century, with warming of similar magnitude in both hemispheres; the northern hemisphere result is similar to that found by several other investigators. A strong warming
1144|Geographical distribution of changes in maximum and minimum temperatures|A dataset has been created containing monthly maximum and minimum temperatures over all of the world &#039; sland surface with available data. Procedures for assessing and correcting (where possible) station records for homogeneity are detailed, and a preliminary analysis is presented. Seasonal trends of maximum and minimum temperature, and diurnal range, are presented, revealing a marked ecrease indiurnal temperature range in 1981-1990 compared with 1951-1980 over a large area of the world. Statistical tests used to find the significance of local and regional trends indicate that he decreases are most significant over Asia. The trends are tentatively related to changes in mean sea level pressure and rainfall. Time series analysis of maximum and minimum temperatures forthe
1145|The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations|In this article, we attempt to distinguish between the properties of moderator and mediator variables at a number of levels. First, we seek to make theorists and researchers aware of the importance of not using the terms moderator and mediator interchangeably by carefully elaborating, both conceptually and strategically, the many ways in which moderators and mediators differ. We then go beyond this largely pedagogical function and delineate the conceptual and strategic implications of making use of such distinctions with regard to a wide range of phenomena, including control and stress, attitudes, and personality traits. We also provide a specific compendium of analytic procedures appropriate for making the most effective use of the moderator and mediator distinction, both separately and in terms of a broader causal system that includes both moderators and mediators. The purpose of this analysis is to distinguish between the properties of moderator and mediator variables in such a way as to clarify the different ways in which conceptual variables may account for differences in peoples &#039; behavior. Specifically, we differentiate between two often-confused functions of third variables: (a) the moderator function of third variables, which
1147|An evaluation of statistical approaches to text categorization|Abstract. This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.
1148|A Comparison of Two Learning Algorithms for Text Categorization|This paper examines the use of inductive learning to categorize natural language documents into predefined content categories. Categorization of text is of increasing importance in information retrieval and natural language processing systems. Previous research on automated text categorization has mixed machine learning and knowledge engineering methods, making it difficult to draw conclusions about the performance of particular methods. In this paper we present empirical results on the performance of a Bayesian classifier and a decision tree learning algorithm on two text categorization data sets. We find that both algorithms achieve reasonable performance and allow controlled tradeoffs between false positives and false negatives. The stepwise feature selection in the decision tree algorithm is particularly effective in dealing with the large feature sets common in text categorization. However, even this algorithm is aided by an initial prefiltering of features, confirming the results...
1149|OHSUMED: An interactive retrieval evaluation and new large test collection for research|A series of information retrieval experiments was earned out with a computer installed in a medical practice setting for relatively inexperienced physician end-users. Using a commercial MEDLINE product based on the vector space model, these physicians searched just as effectively as more experienced searchers using Boolean searching. The results of this experiment were subsequently used to create anew large medical test collection, which was used in experiments with the SMART ~trieval system to obtain baseline performance data as well as compare SMART with the other searchers. 1
1150|Context-Sensitive Learning Methods for Text Categorization|this article, we will investigate the performance of two recently implemented machine-learning algorithms on a number of large text categorization problems. The two algorithms considered are set-valued RIPPER, a recent rule-learning algorithm [Cohen A earlier version of this article appeared in Proceedings of the 19th Annual International ACM Conference on Research and Development in Information Retrieval (SIGIR) pp. 307--315
1151|Training Algorithms for Linear Text Classifiers|Systems for text retrieval, routing, categorization and other IR tasks rely heavily on linear classifiers. We propose that two machine learning algorithms, the Widrow-Hoff and EG algorithms, be used in training linear text classifiers. In contrast to most IR methods, theoretical analysis provides performance guarantees and guidance on parameter settings for these algorithms. Experimental data is presented showing Widrow-Hoff and EG to be more effective than the widely used Rocchio algorithm on several categorization and routing tasks.  1 Introduction  Document retrieval, categorization, routing, and filtering systems often are based on classification. That is, the IR system decides for each document which of two or more classes it belongs to, or how strongly it belongs to a class, in order to accomplish the IR task of interest. For instance, the two classes may be the documents relevant to and not relevant to a particular user, and the system may rank documents based on how likely it i...
1152|A Neural Network Approach to Topic Spotting|This paper presents an application of nonlinear neural networks to topic spotting. Neural networks allow us to model higherorder interaction between document terms and to simultaneously predict multiple topics using shared hidden features. In the context of this model, we compare two approaches to dimensionality reduction in representation: one based on term selection and another based on Latent Semantic Indexing (LSI). Two different methods are proposed for improving LSI representations for the topic spotting task. We find that term selection and our modified LSI representations lead to similar topic spotting performance, and that this performance is equal to or better than other published results on the same corpus. 1 Introduction  Topic spotting is the problem of identifying which of a set of predefined topics are present in a natural language document. More formally, given a set of n topics and a document, the task is to output for each topic the probability that the topic is prese...
1153|Towards Language Independent Automated Learning of Text Categorization Models|We describe the results of extensivemachine learning experiments on large collections of  Reuters&#039; English and German newswires. The goal of these experiments was to automatically  discover classification patterns that can be used for assignment of topics to the individual  newswires. Our results with the English newswire collection show a very large gain in  performance as compared to published benchmarks, while our initial results with the German  newswires appear very promising. We present our methodology, which seems to be insensitive  to the language of the document collections, and discuss issues related to the differences in  results that wehave obtained for the two collections.  
1154|Noise Reduction in a Statistical Approach to Text Categorization|This paper studies noise reduction for computational efficiency improvements in a statistical learning method for text categorization, the Linear Least Squares Fit (LLSF) mapping. Multiple noise reduction strategies are proposedand evaluated, including: an aggressive removal of “non-informative words ” from texts before training; the use of a truncated singular value decomposition to cut off noisy “latent semantic structures ” during training; the elimination of non-influential components in the LLSF solution (a word-concept association matrix) after training. Text collections in different domains were used for evaluation. Significant improvements in computational efficiency without losing categorization accuracy were evident in the testing results. 1
1155|Automatic Indexing Based on Bayesian Inference Networks|In this paper, a Bayesian inference network model for automatic indexing with index terms (descriptors) from a prescribed vocabulary is presented. It requires an indexing dictionary with rules mapping terms of the respective subject field onto descriptors and inverted lists for terms occuring in a set of documents of the subject field and descriptors manually assigned to these documents. The indexing dictionary can be derived automatically from a set of manually indexed documents. An application of the network model is described, followed by an indexing example and some experimental results about the indexing performance of the network model.  
1156|AIR/X - a Rule-Based Multistage Indexing System for Large Subject Fields|AIR/X is a rule-based system for indexing with terms (descriptors) from a prescribed vocabulary. For this  task, an indexing dictionary with rules for mapping terms from the text onto descriptors is required, which  can be derived automatically from a set of manually indexed documents. Based on the Darmstadt Indexing  Approach, the indexing task is divided into a description step and a decision step. First, terms (single words  or phrases) are identified in the document text. With term-descriptor rules from the dictionary, descriptor  indications are formed. The set of all indications from a document leading to the same descriptor is called a  relevance description. A probabilistic classification procedure computes indexing weights for each relevance  description. Since the whole system is rule-based, it can be adapted to different subject fields by appropriate  modifications of the rule bases. A major application of AIR/X is the AIR/PHYS system developed for a  large physics database. This application is described in more detail along with experimental results.
1157|A linear least squares fit mapping method for information retrieval from natural language texts|This paper describes a unique method for mapping natural language texts to canonical terms that identify the contents of the texts. This method learns empirical associations between free-form texts and canonical terms from human-assigned matches and determines a Linear Least Squares Fit (LLSF) mapping function which represents weighted connections between words in the texts and the canonical terms. The mapping function enables us to project an arbitrary text to the canonical term space where the &#034;transformed &#034; text is compared with the terms, and similarity scores are obtained which quantify the relevance between the the text and the terms. This approach has superior power to discover synonyms or related terms and to preserve the context sensitivity of the mapping. We achieved a rate of 84 ~ in both the recall and the precision with a testing set of 6,913 texts, outperforming other techniques including string matching (15%), morphological parsing (17%) and statistical weighting (21%). 1.
1158|An evaluation of statistical approaches to medline indexing|Whether or not high accuracy classi cation methods can be scaled tolarge applications is crucial for the ultimate usefulness of such methods in text categorization. This paper applies two statistical learning algorithms, the Linear Least Squares Fit (LLSF) mapping and a Nearest Neighbor classi er named ExpNet, to a large collection of MED-LINE documents. With the use of suitable dimensionality reduction techniques and e cient algorithms, both LLSF and ExpNet successfully scaled to this very large problem with a result signi cantly outperforming word-matching and other automatic learning methods applied to the same corpus.
1159|LogP: Towards a Realistic Model of Parallel Computation|A vast body of theoretical research has focused either on overly simplistic models of parallel  computation, notably the PRAM, or overly specific models that have few representatives in  the real world. Both kinds of models encourage exploitation of formal loopholes, rather than  rewarding development of techniques that yield performance across a range of current and  future parallel machines. This paper offers a new parallel machine model, called LogP, that  reflects the critical technology trends underlying parallel computers. It is intended to serve  as a basis for developing fast, portable parallel algorithms and to offer guidelines to machine  designers. Such a model must strike a balance between detail and simplicity in order to reveal  important bottlenecks without making analysis of interesting problems intractable. The model  is based on four parameters that specify abstractly the computing bandwidth, the communication  bandwidth, the communication delay, and the efficiency of coupling communication  and computation. Portable parallel algorithms typically adapt to the machine configuration, in  terms of these parameters. The utility of the model is demonstrated through examples that are  implemented on the CM-5.
1160|Active Messages: a Mechanism for Integrated Communication and Computation|The design challenge for large-scale multiprocessors is (1) to minimize communication overhead, (2) allow communication to overlap computation, and (3) coordinate the two without sacrificing processor cost/performance. We show that existing message passing multiprocessors have unnecessarily high communication costs. Research prototypes of message driven machines demonstrate low communication overhead, but poor processor cost/performance. We introduce a simple communication mechanism, Active Messages, show that it is intrinsic to both architectures, allows cost effective use of the hardware, and offers tremendous flexibility. Implementations on nCUBE/2 and CM-5 are described and evaluated using a split-phase shared-memory extension to C, Split-C. We further show that active messages are sufficient to implement the dynamically scheduled languages for which message driven machines were designed. With this mechanism, latency tolerance becomes a programming/compiling concern. Hardware support for active messages is desirable and we outline a range of enhancements to mainstream processors.  
1161|Performance Analysis of k-ary n-cube Interconnection Networks|Abstmct- VLSI communication networks are wire-limited. The cost of a network is not a function of the number of switches required, but rather a function of the wiring density required to construct the network. This paper analyzes commu-nication networks of varying dimension under the assumption of constant wire bisection. Expressions for the latency, average case throughput, and hot-spot throughput of k-ary n-cube networks with constant bisection are derived that agree closely with experi-mental measurements. It is shown that low-dimensional networks (e.g., tori) have lower latency and higher hot-spot throughput than high-dimensional networks (e.g., binary n-cubes) with the same bisection width. Index Terms- Communication networks, concurrent comput-ing, interconnection networks, message-passing multiprocessors, parallel processing, VLSI.
1162|Monsoon: an explicit token-store architecture|Dataflow architectures tolerate long unpredictable com-munication delays and support generation and coordi-nation of parallel activities directly in hardware, rather than assuming that program mapping will cause these issues to disappear. However, the proposed mecha-nisms are complex and introduce new mapping com-plications. This paper presents a greatly simplified ap-proach to dataflow execution, called the explicit token store (ETS) architecture, and its current realization in Monsoon. The essence of dynamic datallow execution is captured by a simple transition on state bits associ-ated with storage local to a processor. Low-level storage management is performed by the compiler in assigning nodes to slots in an activation frame, rather than dy-namically in hardware. The processor is simple, highly pipelined, and quite general. It may be viewed as a generalization of a fairly primitive von Neumann archi-tecture. Although the addressing capability is restric-tive, there is exactly one instruction executed for each action on the dataflow graph. Thus, the machine ori-ented ETS model provides new understanding of the merits and the real cost of direct execution of dataflow graphs. 1
1163|  A Comparison of Sorting Algorithms for the Connection Machine CM-2 |We have implemented three parallel sorting algorithms on the Connection Machine Supercomputer model CM-2: Batcher&#039;s bitonic sort, a parallel radix sort, and a sample sort similar to Reif and Valiant&#039;s flashsort. We have also evaluated the implementation of many other sorting algorithms proposed in the literature. Our computational experiments show that the sample sort algorithm, which is a theoretically efficient &#034;randomized&#034; algorithm, is the fastest of the three algorithms on large data sets. On a 64K-processor CM-2, our sample sort implementation can sort 32 10 6 64-bit keys in 5.1 seconds, which is over 10 times faster than the CM-2 library sort. Our implementation of radix sort, although not as fast on large data sets, is deterministic, much simpler to code, stable, faster with small keys, and faster on small data sets (few elements per processor). Our implementation of bitonic sort, which is pipelined to use all the hypercube wires simultaneously, is the least efficient of the three on large data sets, but is the most efficient on small data sets, and is considerably more space efficient. This paper analyzes the three algorithms in detail and discusses many practical issues that led us to the particular implementations.  
1164|Scans as Primitive Parallel Operations|In most parallel random-access machine (P-RAM) models, memory references are assumed to take unit time. In practice, and in theory, certain scan operations, also known as prefix computations, can executed in no more time than these parallel memory references. This paper outline an extensive study of the effect of including in the P-RAM models, such scan operations as unit-time primitives. The study concludes that the primitives improve the asymptotic running time of many algorithms by an O(lg n) factor, greatly simplify the description of many algorithms, and are significantly easier to implement than memory references. We therefore argue that the algorithm designer should feel free to use these operations as if they were as cheap as a memory reference. This paper describes five algorithms that clearly illustrate how the scan primitives can be used in algorithm design: a radix-sort algorithm, a quicksort algorithm, a minimumspanning -tree algorithm, a line-drawing algorithm and a mergi...
1165|Designing Broadcasting Algorithms in the Postal Model for Message-Passing Systems|In many distributed-memory parallel computers and high-speed communication networks, the exact structure of the underlying communication network may be ignored. These systems assume that the network creates a complete communication graph between the processors, in which passing messages is associated with communication latencies. In this paper, we explore the impact of communication latencies on the design of broadcasting algorithms for fully-connected message-passing systems. For this purpose, we introduce the postal model that incorporates a communication latency parameter   1. This parameter measures the inverse of the ratio between the time it takes an originator of a message to send the message and the time that passes until the recipient of the message receives it. We present an optimal algorithm for broadcasting one message in systems with n processors and communication latency , the running time of which is \Theta(   log n  log(+1)  ). For broadcasting m  1 messages, we first e...
1166|The Uniform Memory Hierarchy Model of Computation|The Uniform Memory Hierarchy (UMH) model introduced in this paper captures performance-relevant aspects of the hierarchical nature of computer memory. It is used to quantify architectural requirements of several algorithms and to ratify the faster speeds achieved by tuned implementations that use improved data-movement strategies. A sequential computer&#039;s memory is modelled as a sequence hM 0 ; M 1 ; :::i of increasingly large memory modules. Computation takes place in M 0 . Thus, M 0 might model a computer&#039;s central processor, while M 1 might be cache memory, M 2 main memory, and so on. For each module M U , a bus B U connects it with the next larger module M U+1 . All buses may be active simultaneously. Data is transferred along a bus in fixed-sized blocks. The size of these blocks, the time required to transfer a block, and the number of blocks that fit in a module are larger for modules farther from the processor. The UMH  model is parameterized by the rate at which the blocksizes i...
1167|Hiding Communication Costs in Bandwidth-Limited Parallel FFT Computation|This paper presentsanovel computation schedule for FFT-type computations on a bandwidth-limited parallel computer. Using P processors, we are able to process n log n an n-input FFT graph in the optimal time of by carefully interleaving inter-P processor communication steps with local computation. Our algorithm is suitable for both shared-memory and distributed memory machines and is analyzed in a simpli cation of the LogP model [5] suitable for studying bandwidth-limited parallel machines. Our parallel FFT algorithm incorporates several techniques that have long been used by parallel programmers to reduce communication costs and our analysis provides theoretical justi cation for the success of these techniques in the context of highly structured computations like FFTs. At another level, our algorithm can be viewed as an optimal simulation of large butter y networks on arbitrary machines (as modeled under LogP.) Thus, we argue that computations thought to be inherently suited to buttery
1168|Energy Conserving Routing in Wireless Ad-hoc Networks| An ad-hoc network of wireless static nodes is considered as it arises in a rapidly deployed, sensor based, monitoring system. Information is generated in certain nodes and needs to reach a set of designated gateway nodes. Each node may adjust its power within a certain range that determines the set of possible one hop away neighbors. Traffic forwarding through multiple hops is employed when the intended destination is not within immediate reach. The nodes have limited initial amounts of energy that is consumed in different rates depending on the power level and the intended receiver. We propose algorithms to select the routes and the corresponding power levels such that the time until the batteries of the nodes drain-out is maximized. The algorithms are local and amenable to distributed implementation. When there is a single power level, the problem is reduced to a maximum flow problem with node capacities and the algorithms converge to the optimal solution. When there are multiple power levels then the achievable lifetime is close to the optimal (that is computed by linear programming) most of the time. It turns out that in order to maximize the lifetime, the traffic should be routed such that the energy consumption is balanced among the nodes in proportion to their energy reserves, instead of routing to minimize the absolute consumed power.
1169|Power-Aware Routing in Mobile Ad Hoc Networks|In this paper we present a case for using new power-aware metrics for determining routes in wireless  ad hoc networks. We present five different metrics based on battery power consumption at nodes. We  show that using these metrics in a shortest-cost routing algorithm reduces the cost/packet of routing  packets by 5-30% over shortest-hop routing (this cost reduction is on top of a 40-70% reduction in energy  consumption obtained by using PAMAS, our MAC layer protocol). Furthermore, using these new metrics  ensures that the mean time to node failure is increased significantly. An interesting property of using  shortest-cost routing is that packet delays do not increase. Finally, we note that our new metrics can  be used in most traditional routing protocols for ad hoc networks.  1 
1170|Minimum energy mobile wireless networks| We describe a distributed position-based network protocol optimized for minimum energy consumption in mobile wireless networks that support peer-to-peer communications. Given any number of randomly deployed nodes over an area, we illustrate that a simple local optimization scheme executed at each node guarantees strong connectivity of the entire network and attains the global minimum energy solution for stationary networks. Due to its localized nature, this protocol proves to be self-reconfiguring and stays close to the minimum energy solution when applied to mobile networks. Simulation results are used to verify the performance of the protocol. 
1171|An Efficient Routing Protocol for Wireless Networks|We present
1172|A distributed algorithm for minimum-weight spanning trees|A distributed algorithm is presented that constructs he minimum-weight spanning tree in a connected undirected graph with distinct edge weights. A processor exists at each node of the graph, knowing initially only the weights of the adjacent edges. The processors obey the same algorithm and exchange messages with neighbors until the tree is constructed. The total number of messages required for a graph of N nodes and E edges is at most 5N log2N + 2E, and a message contains at most one edge weight plus log28N bits. The algorithm can be initiated spontaneously at any node or at any subset of nodes.
1173|Routing for Maximum System Lifetime in Wireless Ad-hoc Networks|An ad-hoc network of wireless static nodes is considered as it arises in a rapidly  deployed, sensor based, monitoring system. Information is generated in certain  nodes and needs to reach some designated gateway node. Each node may adjust  its power within a certain range that determines the set of possible one hop away  neighbors. Traffic forwarding through multiple hops is employed when the intended  destination is not within immediate reach. The nodes have limited initial amounts  of energy that are consumed in different rates depending on the power level and the  intended receiver. We propose algorithms to select the routes and the corresponding  power levels such that the time until the batteries of the nodes drain-out is  maximized. The algorithms are local and amenable to distributed implementation.
1174|Decentralized Channel Management in Scalable Multihop Spread-Spectrum Packet Radio Networks|This thesis addresses the problems of managing the transmissions of stations in a spread-spectrum packet radio network so that the system can remain effective when scaled to millions of nodes concentrated in a metropolitan area. The principal difficulty in scaling a system of packet radio stations is interference from other stations in the system. Interference comes both from nearby stations and from distant stations. Each nearby interfering station is a particular problem, because a signal received from it may be as strong as or stronger than the desired signal from some other station. Far-off interfering stations are not individually a problem, since each of their signals will be weaker, but the combined effect may be the dominant source of interference. The thesis begins with an analysis of propagation and interference models. The overall noise level in the system (mainly caused by the many distant stations) is then analyzed, and found to remain manageable even as the system scales ...
1175|Transfer of Cognitive Skill|A framework for skill acquisition is proposed that includes two major stages in the development of a cognitive skill: a declarative stage in which facts about the skill domain are interpreted and a procedural stage in which the domain knowledge is directly embodied in procedures for performing the skill. This general framework has been instantiated in the ACT system in which facts are encoded in a propositional network and procedures are encoded as productions. Knowledge compilation is the process by which the skill transits from the declarative stage to the procedural stage. It consists of the subprocesses of composition, which collapses sequences of productions into single productions, and proceduralization, which embeds factual knowledge into productions. Once proceduralized, further learning processes operate on the skill to make the productions more selective in their range of applications. These processes include generalization, discrimination, and strengthening of productions. Comparisons are made to similar concepts from past learning theories. How these learning mechanisms apply to produce the power law speedup in processing time with practice is discussed. It requires at least 100 hours of learning and practice to acquire any significant cognitive skill to a reasonable degree of proficiency. For instance, after 100 hours a student learning to program a computer has achieved only a very modest facility in the skill. Learning one&#039;s primary language takes tens of thousands of hours. The psychology of human learning has been very thin in ideas about what happens to skills under the impact of this amount of learning—and for obvious reasons. This article presents a theory about the changes in the nature of a skill over such large time scales and about the basic learning processes that are responsible.
1176|Mechanisms of skill acquisition and the law of practice|Mechanisms of skill acquisition and the law of practice
1177|A First Language|Do firms employing undocumented workers have a competitive advantage? Using administrative data from the state of Georgia, this paper investigates the incidence of undocumented worker employment across firms and how it affects firm survival. Firms are found to engage in herding behavior, being more likely to employ undocumented workers if competitors do. Rivals’ undocumented employment harms firms ’ ability to survive, while firms ’ own undocumented employment strongly enhances their survival prospects. This suggests that firms enjoy cost savings from employing lower-paid undocumented at wages less than their marginal revenue product. The herding behavior and competitive effects are found to be much weaker in geographically broad product markets, where firms have the option to shift labor-intensive production out of state or abroad. The views expressed in this paper are those of the authors and do not necessarily reflect those of the Federal Reserve Bank of Atlanta or the Federal Reserve System. Valuable research
1178|Repair Theory: A generative theory of bugs in procedural skills |This paper describes a generative theory of bugs. It claims that all bugs of D procedural skill con be derived by a highly constrained form of problem solving acting on incomplete procedures. These procedures are characterized by formal deletion operations that model incomplete learning and forgetting. The problem solver and the deletion operator have been constrained to make it impossible to derive “star-bugs”--xJgorithms that are so absurd that expert diagnosticians agree that the alogorithm will never be observed as o bug. Hence, the theory not only generates the observed bugs, it fails to generate star-bugs. The theory has been tested on on extensive doto base of bugs for multidigit subtraction that was collected with the aid of the diagnostic systems BUGGY and DEBUGGY. In addition to predicting bug occurrence, by adoption of additional hypotheses, the theory also makes predictions about the frequency and stability of bugs, as well as the occurrence of certain lotencies in processing time during testing. Arguments are given that the theory can be applied to domains other than subtraction and that it con be extended to provide a theory of procedural learning that accounts for bug acquisition. Lastly, particular care has been taken to make the theory principled so that it can not be tailored to fit ony possible data. 1.
1179|A theory of language acquisition based on general learning principles|A simulation model is described for the acquisition of the control of syntax in language generation. This model makes use of general learning principles and general principles of cognition. Language generation is modelled as a problem solving process involving prmciply the decomposition of a lo be-communicated semantic structure into a hierarchy of subunits for generation. The syntax of the language controls this decomposition. It is shown how a sentence and semantic structure can be compared to infer the decomposition that led to the sentence. The learning processes involve generalizing rules to classes of words, learning by discrimination the various contextual constraints on a rule application, and a strength process which monitors a rule&#039;s history of success and failure. This system is shown to apply to the learning of noun declensions in Latin, relative clause constructions in French, and verb auxiliary structures in English.
1180|The embryonic cell lineage of the nematode Caenorhabditis elegans|The number of nongonadal nuclei in the free-living soil nematode Caenorhabditis elegans increases from about 550 in the newly hatched larva to about 810 in the mature hermaphrodite and to about 970 in the mature male. The pattern of cell divisions which leads to this increase is essentially invariant among individuals; rigidly determined cell lineages generate a fixed number of progeny cells of strictly specified fates. These lineages range in length from one to eight sequential divisions and lead to significant developmental changes in the neuronal, muscular, hypodermal, and digestive systems. Frequently, several blast cells follow the same asymmetric program of divisions; lineally equivalent progeny of such cells generally differen-tiate into functionally equivalent cells. We have determined these cell lineages by direct observation of the divisions, migrations, and deaths of individual cells in living nematodes. Many of the cell lineages are involved in sexual maturation. At hatching, the hermaphrodite and male are almost identical morphologically; by the adult stage, gross anatomical differences are obvious. Some of these sexual differences arise from blast cells whose division patterns are initially identical in the male and in the hermaphrodite but later diverge. In the hermaphro-dite, these cells produce structures used in egg-laying and mating, whereas, in the male, they produce morphologically different structures which function before and during copulation. In addition, development of the male involves a number of lineages derived from cells which do not divide in the hermaphrodite. Similar postembryonic developmental events occur in other nematode species.
1181|Methods. In The Nematode Caenorhabditis elegans|Analysis of mig-10 expression to determine cell autonomy or nonautonomy in
1182|Color indexing|Computer vision is embracing a new research focus in which the aim is to develop visual skills for robots that allow them to interact with a dynamic, realistic environment. To achieve this aim, new kinds of vision algorithms need to be developed which run in real time and subserve the robot&#039;s goals. Two fundamental goals are determin-ing the location of a known object. Color can be successfully used for both tasks. This article demonstrates that color histograms of multicolored objects provide a robust, efficient cue for index-ing into a large database of models. It shows that color histograms are stable object representations in the presence of occlusion and over change in view, and that they can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and im-age histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm called Histogram Backprojection, which performs this task efficiently in crowded scenes. 1
1183|Computer Vision|Driver inattention is one of the main causes of traffic accidents. Monitoring a driver to detect inattention is a complex problem that involves physiological and behavioral elements. Different approaches have been made, and among them Computer Vision has the potential of monitoring the person behind the wheel without interfering with his driving. In this paper I have developed a system that can monitor the alertness of drivers in order to prevent people from falling asleep at the wheel. The other main aim of this algorithm is to have efficient performance on low quality webcam and without the use of infrared light which is harmful for the human eye. Motor vehicle accidents cause injury and death, and this system will help to decrease the amount of crashes due to fatigued drivers. The proposed algorithm will work in three main stages. In first stage the face of the driver is detected and tracked. In the second stage the facial features are extracted for further processing. In last stage the most crucial parameter is monitored which is eye’s status. In the last stage it is determined that whether the eyes are closed or open. On the basis of this result the warning is issued to the driver to take a break.
1184|A survey of general-purpose computation on graphics hardware|The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware acompelling platform for computationally demanding tasks in awide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general-purpose computation to graphics hardware. We begin with the technical motivations that underlie general-purpose computation on graphics processors (GPGPU) and describe the hardware and software developments that have led to the recent interest in this field. We then aim the main body of this report at two separate audiences. First, we describe the techniques used in mapping general-purpose computation to graphics hardware. We believe these techniques will be generally useful for researchers who plan to develop the next generation of GPGPU algorithms and techniques. Second, we survey and categorize the latest developments in general-purpose application development on graphics hardware.  
1185|FFTW: An Adaptive Software Architecture For The FFT|FFT literature has been mostly concerned with minimizing the number of floating-point operations performed by an algorithm. Unfortunately, on present-day microprocessors this measure is far less important than it used to be, and interactions with the processor pipeline and the memory hierarchy have a larger impact on performance. Consequently, one must know the details of a computer architecture in order to design a fast algorithm. In this paper, we propose an adaptive FFT program that tunes the computation automatically for any particular hardware. We compared our program, called FFTW, with over 40 implementations of the FFT on 7 machines. Our tests show that FFTW&#039;s self-optimizing approach usually yields significantly better performance than all other publicly available software. FFTW also compares favorably with machine-specific, vendor-optimized libraries. 1. INTRODUCTION The discrete Fourier transform (DFT) is an important tool in many branches of science and engineering [1] and...
1186|Modeling the Interaction of Light Between Diffuse Surfaces|A method is described which models the interaction of light between diffusely reflecting surfaces. Current light reflection models used in computer graphics do not account for the object-to-object reflection between diffuse surfaces, and thus incorrectly compute the global illumination effects. The new procedure, based on methods used in thermal engineering, includes the effects of diffuse light sources of finite area, as well as the &#034;color-bleeding&#034; effects which are caused by the diffuse reflections. A simple environment is used to illustrate these simulated effects and is presented with photographs of a physical model. The procedure is applicable to environments composed of ideal diffuse reflectors and can account for direct illumination from a variety of light sources. The resultant surface intensities are independent of observer position, and thus environments can be preprocessed for dynamic sequences.
1187|Chromium: A Stream-Processing Framework for Interactive Rendering on Clusters|We describe Chromium, a system for manipulating streams of graphics API commands on clusters of workstations. Chromium&#039;s stream filters can be arranged to create sort-first and sort-last parallel graphics architectures that, in many cases, support the same applications while using only commodity graphics accelerators. In addition, these stream filters can be extended programmatically, allowing the user to customize the stream transformations performed by nodes in a cluster. Because our stream processing mechanism is completely general, any cluster-parallel rendering algorithm can be either implemented on top of or embedded in Chromium. In this paper, we give examples of real-world applications that use Chromium to achieve good scalability on clusters of workstations, and describe other potential uses of this stream processing technology. By completely abstracting the underlying graphics architecture, network topology, and API command processing semantics, we allow a variety of applications to run in different environments.
1188|Fast Computation of Generalized Voronoi Diagrams Using Graphics Hardware|We present a new approach for computing generalized 2D and 3D Voronoi diagrams using interpolation-based polygon rasterization hardware. We compute a discrete Voronoi diagram by rendering a three dimensional distance mesh for each Voronoi site. The polygonal mesh is a bounded-error approximation of a (possibly) non-linear function of the distance between a site and a 2D planar grid of sample points. For each sample point, we compute the closest site and the distance to that site using polygon scan-conversion and the Z-buffer depth comparison. We construct distance meshes for points, line segments, polygons, polyhedra, curves, and curved surfaces in 2D and 3D. We generalize to weighted and farthest-site Voronoi diagrams, and present efficient techniques for computing the Voronoi boundaries, Voronoi neighbors, and the Delaunay triangulation of points. We also show how to adaptively refine the solution through a simple windowing operation. The algorithm has been implemented on SGI workstations and PCs using OpenGL, and applied to complex datasets. We demonstrate the application of our algorithm to fast motion planning in static and dynamic environments, selection in complex user-interfaces, and creation of dynamic mosaic effects.
1189|Reflection from Layered Surfaces due to Subsurface Scattering|The reflection of light from most materials consists of two major terms: the specular and the diffuse. Specular reflection may be modeled from first principles by considering a rough surface consisting of perfect reflectors, or micro-facets. Diffuse reflection is generally considered to result from multiple scattering either from a rough surface or from within a layer near the surface. Accounting for diffuse reflection by Lambert&#039;s Cosine Law, as is universally done in computer graphics, is not a physical theory based on first principles. This paper presents
1190|Brook for GPUs: Stream Computing on Graphics Hardware|In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming coprocessor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.
1191|GPUTeraSort: High Performance Graphics Coprocessor Sorting for Large Database Management|We present a new algorithm, GPUTeraSort, to sort billionrecord wide-key databases using a graphics processing unit (GPU) Our algorithm uses the data and task parallelism on the GPU to perform memory-intensive and computeintensive tasks while the CPU is used to perform I/O and resource management. We therefore exploit both the highbandwidth GPU memory interface and the lower-bandwidth CPU main memory interface and achieve higher memory bandwidth than purely CPU-based algorithms. GPUTera-Sort is a two-phase task pipeline: (1) read disk, build keys, sort using the GPU, generate runs, write disk, and (2) read, merge, write. It also pipelines disk transfers and achieves near-peak I/O performance. We have tested the performance of GPUTeraSort on billion-record files using the standard Sort benchmark. In practice, a 3 GHz Pentium IV PC with $265 NVIDIA 7800 GT GPU is significantly faster than optimized CPU-based algorithms on much faster processors, sorting 60GB for a penny; the best reported PennySort price-performance. These results suggest that a GPU co-processor can significantly improve performance on large data processing tasks. 1.
1192|Problem-oriented software engineering|This paper introduces a formal conceptual framework for software development, based on a problem-oriented perspective that stretches from requirements engineering through to program code. In a software problem the goal is to develop a machine—that is, a computer executing the software to be developed—that will ensure satisfaction of the requirement in the problem world. We regard development steps as transformations by which problems are moved towards software solutions. Adequacy arguments are built as problem transformations are applied: adequacy arguments both justify proposed development steps and establish traceability relationships between problems and solutions. The framework takes the form of a sequent calculus. Although itself formal, it can accommodate both formal and informal steps in development. A number of transformations are presented, and illustrated by application to small examples.  
1193|Interactive Order-Independent Transparency|this document is to enable OpenGL developers to implement this technique with NVIDIA OpenGL extensions and GeForce3 hardware. Since shadow mapping is integral to the technique a very basic introduction is provided, but the interested reader is encouraged to explore the referenced material for more detail
1194|A Multigrid Solver for Boundary Value Problems Using Programmable Graphics Hardware|We present a method for using programmable graphics hardware to solve a variety of boundary value problems. The time-evolution of such problems is frequently governed by partial differential equations, which are used to describe a wide range of dynamic phenomena including heat transfer and fluid mechanics. The need to solve these equations efficiently arises in many areas of computational science. Finite difference methods are commonly used for solving partial differential equations; we show that this approach can be mapped onto a modern graphics processor. We demonstrate an implementation of the multigrid method, a fast and popular approach to solving boundary value problems, on two modern graphics architectures. Our initial tests with available hardware show speedups of roughly 15x compared to traditional software implementation. This work presents a novel use of computer hardware and raises the intriguing possibility that we can make the inexpensive power of modern commodity graphics hardware accessible to and useful for the simulation commuuity.
1195|The Direct3D 10 system |We present a system architecture for the 4 th generation of PCclass programmable graphics processing units (GPUs). The new pipeline features significant additions and changes to the prior generation pipeline including a new programmable stage capable of generating additional primitives and streaming primitive data to memory, an expanded, common feature set for all of the programmable stages, generalizations to vertex and image memory resources, and new storage formats. We also describe structural modifications to the API, runtime, and shading language to complement the new pipeline. We motivate the design with descriptions of frequently encountered obstacles in current systems. Throughout the paper we present rationale behind prominent design choices and alternatives that were ultimately rejected, drawing on insights collected during a multi-year collaboration with application developers and hardware designers.
1196|Fast computation of database operations using graphics processors|We present new algorithms for performing fast computation of several common database operations on commodity graphics processors. Specifically, we consider operations such as conjunctive selections, aggregations, and semi-linear queries, which are essential computational components of typical database, data warehousing, and data mining applications. While graphics processing units (GPUs) have been designed for fast display of geometric primitives, we utilize the inherent pipelining and parallelism, single instruction and multiple data (SIMD) capabilities, and vector processing functionality of GPUs, for evaluating boolean predicate combinations and semi-linear queries on attributes and executing database operations efficiently. Our algorithms take into account some of the limitations of the programming model of current GPUs and perform no data rearrangements. Our algorithms have been implemented on a programmable GPU (e.g. NVIDIA’s GeForce FX 5900) and applied to databases consisting of up to a million records. We have compared their performance with an optimized implementation of CPU-based algorithms. Our experiments indicate that the graphics processor available on commodity computer systems is an effective co-processor for performing database operations.
1197|Physically-Based Visual Simulation on Graphics Hardware|In this paper, we present a method for real-time visual simulation of diverse dynamic phenomena using programmable graphics hardware. The simulations we implement use an extension of cellular automata known as the coupled map lattice (CML). CML represents the state of a dynamic system as continuous values on a discrete lattice. In our implementation we store the lattice values in a texture, and use pixel-level programming to implement simple next-state computations on lattice nodes and their neighbors. We apply these computations successively to produce interactive visual simulations of convection, reaction-diffusion, and boiling. We have built an interactive framework for building and experimenting with CML simulations running on graphics hardware, and have integrated them into interactive 3D graphics applications.
1198|Understanding the Efficiency of GPU Algorithms for Matrix-Matrix Multiplication|Utilizing graphics hardware for general purpose numerical computations has become a topic of considerable  interest. The implementation of streaming algorithms, typified by highly parallel computations with little reuse  of input data, has been widely explored on GPUs. We relax the streaming model&#039;s constraint on input reuse and  perform an in-depth analysis of dense matrix-matrix multiplication, which reuses each element of input matrices  O(n) times. Its regular data access pattern and highly parallel computational requirements suggest matrix-matrix  multiplication as an obvious candidate for efficient evaluation on GPUs but, surprisingly we find even nearoptimal  GPU implementations are pronouncedly less efficient than current cache-aware CPU approaches. We find  the key cause of this inefficiency is that the GPU can fetch less data and yet execute more arithmetic operations  per clock than the CPU when both are operating out of their closest caches. The lack of high bandwidth access to  cached data will impair the performance of GPU implementations of any computation featuring significant input  reuse.
1199|Lu-gpu: Efficient algorithms for solving dense linear systems on graphics hardware|We present a novel algorithm to solve dense linear systems using graphics processors (GPUs). We reduce matrix decomposition and row operations to a series of rasterization problems on the GPU. These include new techniques for streaming index pairs, swapping rows and columns and parallelizing the computation to utilize multiple vertex and fragment processors. We also use appropriate data representations to match the rasterization order and cache technology of graphics processors. We have implemented our algorithm on different GPUs and compared the performance with optimized CPU implementations. In particular, our implementation on a NVIDIA GeForce 7800 GPU outperforms a CPU-based ATLAS implementation. Moreover, our results show that our algorithm is cache and bandwidth efficient and scales well with the number of fragment processors within the GPU and the core GPU clock rate. We use our algorithm for fluid flow simulation and demonstrate that the commodity GPU is a useful co-processor for many scientific applications. 1
1200|Sequential point trees|Figure 1: Continuous detail levels of a Buddha generated in vertex programs on the GPU. The colors denote the LOD level used and the bars describe the selected amount of points selected for the GPU (top row) and the average CPU load required for rendering (bottom row). In this paper we present sequential point trees, a data structure that allows adaptive rendering of point clouds completely on the graphics processor. Sequential point trees are based on a hierarchical point representation, but the hierarchical rendering traversal is replaced by sequential processing on the graphics processor, while the CPU is available for other tasks. Smooth transition to triangle rendering for optimized performance is integrated. We describe optimizations for backface culling and texture adaptive point selection. Finally, we discuss implementation issues and show results.
1201|A memory model for scientific algorithms on graphics processors|We present a memory model to analyze and improve the performance of scientific algorithms on graphics processing units (GPUs). Our memory model is based on texturing hardware, which uses a 2D block-based array representation to perform the underlying computations. We incorporate many characteristics of GPU architectures including smaller cache sizes, 2D block representations, and use the 3C’s model to analyze the cache misses. Moreover, we present techniques to improve the performance of nested loops on GPUs. In order to demonstrate the effectiveness of our model, we highlight its performance on three memory-intensive scientific applications – sorting, fast Fourier transform and dense matrix-multiplication. In practice, our cache-efficient algorithms for these applications are able to achieve memory throughput of 30–50 GB/s on a NVIDIA 7900 GTX GPU. We also compare our results with prior GPU-based and CPU-based implementations on highend processors. In practice, we are able to achieve 2–5× performance improvement.
1202|Radiosity on graphics hardware|Radiosity is a widely used technique for global illumination. Typically the computation is performed offline and the result is viewed interactively. We present a technique for computing radiosity, including an adaptive subdivision of the model, using graphics hardware. Since our goal is to run at interactive rates, we exploit the computational power and programmability of modern graphics hardware. Using our system on current hardware, we have been able to compute and display a radiosity solution for a 10,000 element scene in less than one second. Key words: Graphics Hardware, Global Illumination. 1
1203|Fast and Simple 2D Geometric Proximity Queries Using Graphics Hardware|We present a new approach for computing generalized proximity information of arbitrary 2D objects using graphics hardware. Using multi-pass rendering techniques and accelerated distance computation, our algorithm performs proximity queries not only for detecting collisions, but also for computing intersections, separation distance, penetration depth, and contact points and normals. Our hybrid geometry and image-based approach balances computation between the CPU and graphics subsystems. Geometric object-space techniques coarsely localize potential intersection regions or closest features between two objects, and image-space techniques compute the low-level proximity information in these regions. Most of the proximity information is derived from a distance field computed using graphics hardware. We demonstrate the performance in collision response computation for rigid and deformable body dynamics simulations. Our approach provides proximity information at interactive rates for a variet...
1204|GPU algorithms for radiosity and subsurface scattering|All in-text	references	underlined	in	blue	are	linked	to	publications	on	ResearchGate, letting you	access	and	read	them	immediately.
1205|Fast and approximate stream mining of quantiles and frequencies using graphics processors|We present algorithms for fast quantile and frequency estimation in large data streams using graphics processors (GPUs). We exploit the high computation power and memory bandwidth of graphics processors and present a new sorting algorithm that performs ras-terization operations on the GPUs. We use sorting as the main computational component for histogram approximation and con-struction of -approximate quantile and frequency summaries. Our algorithms for numerical statistics computation on data streams are deterministic, applicable to xed or variable-sized sliding windows and use a limited memory footprint. We use GPU as a co-processor and minimize the data transmission between the CPU and GPU by taking into account the low bus bandwidth. We implemented our algorithms on a PC with a NVIDIA GeForce FX 6800 Ultra GPU and a 3:4 GHz Pentium IV CPU and applied them to large data streams consisting of more than 100 million values. We also compared the performance of our GPU-based algorithms with op-timized implementations of prior CPU-based algorithms. Overall, our results demonstrate that the graphics processors available on a commodity computer system are efcient stream-processor and useful co-processors for mining data streams.
1206|Performance and accuracy of hardware-oriented native-, emulated- and mixed-precision solvers in FEM simulations |In a previous publication, we have examined the fundamental difference between computational precision and result accuracy in the context of the iterative solution of linear systems as they typically arise in the Finite Element discretization of Partial Differential Equations (PDEs) [1]. In particular, we evaluated mixed- and emulatedprecision schemes on commodity graphics processors (GPUs), which at that time only supported computations in single precision. With the advent of graphics cards that natively provide double precision, this report updates our previous results. We demonstrate that with new co-processor hardware supporting native double precision, such as NVIDIA’s G200 architecture, the situation does not change qualitatively for PDEs, and the previously introduced mixed precision schemes are still preferable to double precision alone. But the schemes achieve significant quantitative performance improvements with the more powerful hardware. In particular, we demonstrate that a Multigrid scheme can accurately solve a common test problem in Finite Element settings with one million unknowns in less than 0.1 seconds, which is truely outstanding performance. We support these conclusions by exploring the algorithmic design space enlarged by the availability of double precision directly in the hardware. 1 Introduction and
1207|Detection of Collisions and Self-collisions Using Image-space Techniques|Image-space techniques have shown to be very efficient for collision detection in dynamic simulation and animation environments. This paper proposes a new image-space technique for efficient collision detection of arbitrarily shaped, water-tight objects. In contrast to existing approaches that do not consider self-collisions, our approach combines the image-space object representation with information on face orientation to overcome this limitation. While
1208|Applications of Pixel Textures in Visualization and Realistic Image Synthesis|With fast 3D graphics becoming more and more available even on low end platforms, the focus in developing new graphics hardware is beginning to shift towards higher quality rendering and additional functionality instead of simply higher performance implementations of the traditional graphics pipeline. On this search for improved quality it is important to identify a powerful set of orthogonal features to be implemented in hardware, which can then be flexibly combined to form new algorithms.  Pixel textures are an OpenGL extension by Silicon Graphics that fits into this category. In this paper, we demonstrate the benefits of this extension by presenting several different algorithms exploiting its functionality to achieve high quality, high performance solutions for a variety of different applications from scientific visualization and realistic image synthesis. We conclude that pixel textures are a valuable, powerful feature that should become a standard in future graphics systems.   
1209|Accelerating 3D convolution using graphics hardware|Many volume filtering operations used for image enhancement, data processing or feature detection can be written in terms of three-dimensional convolutions. It is not possible to yield interactive frame rates on todays hardware when applying such convolutions on volume data using software filter routines. As modern graph-ics workstations have the ability to render two-dimensional convo-luted images to the frame buffer, this feature can be used to accel-erate the process significantly. This way generic 3D convolution can be added as a powerful tool in interactive volume visualization toolkits.
1210|GPU-ABiSort: Optimal parallel sorting on stream architectures|In this paper, we present a novel approach for parallel sorting on stream processing architectures. It is based on adaptive bitonic sorting. For sorting n values utilizing p stream processor units, this approach achieves the optimal time complexity O((n log n)/p). While this makes our approach competitive with common sequential sorting algorithms not only from a theoretical viewpoint, it is also very fast from a practical viewpoint. This is achieved by using efficient linear stream memory accesses and by combining the optimal time approach with algorithms optimized for small input sequences. We present an implementation on modern programmable graphics hardware (GPUs). On recent GPUs, our optimal parallel sorting approach has shown to be remarkably faster than sequential sorting on the CPU, and it is also faster than previous non-optimal sorting approaches on the GPU for sufficiently large input sequences. Because of the excellent scalability of our algorithm with the number of stream processor units p (up to n / log 2 n or even n / log n units, depending on the stream architecture), our approach profits heavily from the trend of increasing number of fragment processor units on GPUs, so that we can expect further speed improvement with upcoming GPU generations.
1211|Towards Fast Non-Rigid Registration|A fast multiscale and multigrid method for the matching of images in 2D and 3D is presented. Especially in medical imaging this problem - denoted as the registration problem - is of fundamental importance in the handling of images from multiple image modalities or of image time series. The paper restricts to the simplest matching energy to be minimized, i.e., E[] =    R    jf 1   f2 j    , where f1 , f2 are the intensity maps of the two images to be matched and  is a deformation. The focus is on a robust and efficient solution strategy. Matching of
1212|Interactive Time-Dependent Tone Mapping Using Programmable Graphics Hardware|Modern graphics architectures have replaced stages of the graphics pipeline with fully programmable modules. Therefore, it is now possible to perform fairly general computation on each vertex or fragment in a scene. In addition, the nature of the graphics pipeline makes substantial computational power available if the programs have a suitable structure. In this paper, we show that it is possible to cleanly map a state-of-the-art tone mapping algorithm to the pixel processor. This allows an interactive application to achieve higher levels of realism by rendering with physically based, unclamped lighting values and high dynamic range texture maps. We also show that the tone mapping operator can easily be extended to include a time-dependent model, which is crucial for interactive behavior. Finally, we describe the ways in which the graphics hardware limits our ability to compress dynamic range efficiently, and discuss modifications to the algorithm that could alleviate these problems.
1213|Fast summed-area table generation and its applications|We introduce a technique to rapidly generate summed-area tables using graphics hardware. Summed area tables, originally introduced by Crow, provide a way to filter arbitrarily large rectangular regions of an image in a constant amount of time. Our algorithm for generating summed-area tables, similar to a technique used in scientific computing called recursive doubling, allows the generation of a summed-area table in O(log n) time. We also describe a technique to mitigate the precision requirements of summed-area tables. The ability to calculate and use summed-area tables at interactive rates enables numerous interesting rendering effects. We present several possible applications. First, the use of summed-area tables allows real-time rendering of interactive, glossy environmental reflections. Second, we present glossy planar reflections with varying blurriness dependent on a reflected object’s distance to the reflector. Third, we show a technique that uses a summed-area table to render glossy transparent objects. The final application demonstrates an interactive depth-of-field effect using summedarea tables. Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
1214|STAPL: An adaptive, generic parallel C++ library| The Standard Template Adaptive Parallel Library (STAPL) is a parallel library designed as a superset of the ANSI C++ Standard Template Library (STL). It is sequentially consistent for functions with the same name, and executes on uni- or multi-processor systems that utilize shared or distributed memory. STAPL is implemented using simple parallel extensions of C++ that currently provide a SPMD model of parallelism, and supports nested parallelism. The library is intended to be general purpose, but emphasizes irregular programs to allow the exploitation of parallelism for applications which use dynamically linked data structures such as particle transport calculations, molecular dynamics, geometric modeling, and graph algorithms. STAPL provides several different algorithms for some library routines, and selects among them adaptively at runtime. STAPL can replace STL automatically by invoking a preprocessing translation phase. In the applications studied, the performance of translated code was within 5 % of the results obtained using STAPL directly. STAPL also provides functionality to allow the user to further optimize the code and achieve additional performance gains. We present results obtained using STAPL for a molecular dynamics code and a particle transport code.  
1215|Hardware Acceleration in Commercial Databases: A Case Study of Spatial Operations|Traditional databases have focused on the issue  of reducing I/O cost as it is the bottleneck  in many operations. As databases become  increasingly accepted in areas such as Geographic  Information Systems (GIS) and Bioinformatics,  commercial DBMS need to support  data types for complex data such as spatial  geometries and protein structures. These  non-conventional data types and their associated  operations present new challenges. In  particular, the computational cost of some  spatial operations can be orders of magnitude  higher than the I/O cost. In order to improve  the performance of spatial query processing,  innovative solutions for reducing this  computational cost are beginning to emerge.
1216|Nonlinear optimization framework for image-based modeling on programmable graphics hardware|Graphics hardware is undergoing a change from fixed-function pipelines to more programmable organizations that resemble general-purpose stream processors. In this paper, we show that certain general algorithms, not normally associated with computer graphics, can be mapped to such designs. Specifically, we cast nonlinear optimization as a data streaming process that is well matched to modern graphics processors. Our framework is particularly well suited for solving image-based modeling problems since it can be used to represent a large and diverse class of these problems using a common formulation. We successfully apply this approach to two distinct image-based modeling problems: light field mapping approximation and fitting the Lafortune model to spatial bidirectional reflectance distribution functions. Comparing the performance of the graphics hardware implementation to a CPU implementation, we show more than 5-fold improvement.
1217|Generic Mesh Refinement on GPU|Many recent publications have shown that a large variety of computation involved in computer graphics can be  moved from the CPU to the GPU, by a clever use of vertex or fragment shaders. Nonetheless there is still one  kind of algorithms that is hard to translate from CPU to GPU: mesh refinement techniques. The main reason  for this, is that vertex shaders available on current graphics hardware do not allow the generation of additional  vertices on a mesh stored in graphics hardware. In this paper, we propose a general solution to generate mesh  refinement on GPU. The main idea is to define a generic refinement pattern that will be used to virtually create  additional inner vertices for a given polygon. These vertices are then translated according to some procedural  displacement map defining the underlying geometry (similarly, the normal vectors may be transformed according  to some procedural normal map). For illustration purpose, we use a tesselated triangular pattern, but many other  refinement patterns may be employed. To show its flexibility, the technique has been applied on a large variety  of refinement techniques: procedural displacement mapping, as well as more complex techniques such as curved  PN-triangles or ST-meshes.
1218|A cache-efficient sorting algorithm for database and data mining computations using graphics processors|We present a fast sorting algorithm using graphics processors (GPUs) that adapts well to database and data mining applications. Our algorithm uses texture mapping and blending functionalities of GPUs to implement an efficient bitonic sorting network. We take into account the communication bandwidth overhead to the video memory on the GPUs and reduce the memory bandwidth requirements. We also present strategies to exploit the tile-based computational model of GPUs. Our new algorithm has a memoryefficient data access pattern and we describe an efficient instruction dispatch mechanism to improve the overall sorting performance. We have used our sorting algorithm to accelerate join-based queries and stream mining algorithms. Our results indicate up to an order of magnitude improvement over prior CPU-based and GPU-based sorting algorithms. 1
1219|Computer vision signal processing on graphics processing units|In some sense, computer graphics and computer vision are inverses of one another. Special purpose computer vision hardware is rarely found in typical mass-produced personal computers, but graphics processing units (GPUs) found on most personal computers, often exceed (in number of transistors as well as in compute power) the capabilities of the Central Processing Unit (CPU). This paper shows speedups attained by using computer graphics hardware for implementation of computer vision algorithms by efficiently mapping mathematical operations of computer vision onto modern computer graphics architecture. As an example computer vision algorithm, we implement a real–time projective camera motion tracking routine on modern, GeForce FX class GPUs. Algorithms are implemented using OpenGL and the nVIDIA Cg fragment shaders. Trade–offs between computer vision requirements and GPU resources are discussed. Algorithm implementation is examined closely, and hardware bottlenecks are addressed to examine the performance of GPU architecture for computer vision. It is shown that significant speedups can be achieved, while leaving the CPU free for other signal processing tasks. Applications of our work include wearable, computer mediated reality systems that use both computer vision and computer graphics, and require realtime processing with low–latency and high throughput provided by modern GPUs. 1.
1220|Fast and reliable collision culling using graphics hardware|Figure 1: Tree with falling leaves: In this scene, leaves fall from the tree and undergo non-rigid motion. They collide with other leaves and branches. The environment consists of more than 40K triangles and 150 leaves. Our algorithm, FAR, can compute all the collisions in about 35 msec per time step. We present a reliable culling algorithm that enables fast and accurate collision detection between triangulated models in a complex environment. Our algorithm performs fast visibility queries on the GPUs for eliminating a subset of primitives that are not in close proximity. To overcome the accuracy problems caused by the limited viewport resolution, we compute the Minkowski sum of each primitive with a sphere and perform reliable 2.5D overlap tests between the primitives. We are able to achieve more effective collision culling as compared to prior object-space culling algorithms. We integrate our culling algorithm with CULLIDE [8] and use it to perform reliable GPU-based collision queries at interactive rates on all types of models, including non-manifold geometry, deformable models, and breaking objects.
1221|Solving the Euler Equations on Graphics Processing Units |Abstract. The paper describes how one can use commodity graphics cards (GPUs) as a high-performance parallel computer to simulate the dynamics of ideal gases in two and three spatial dimensions. The dynamics is described by the Euler equations, and numerical approximations are computed using state-of-the-art high-resolution finite-volume schemes. These schemes are based upon an explicit time discretisation and are therefore ideal candidates for parallel implementation. 1
1222|An in-depth look at computer performance growth|Abstract — It is a common belief that computer performance growth is over 50 % annually, or that performance doubles every 18-20 months. By analyzing publicly available results from the SPEC integer (CINT) benchmark suites, we conclude that this was true between 1985 and 1996 – the early years of the RISC paradigm. During the last 7.5 years (1996-2004), however, performance growth has slowed down to 41%, with signs of a continuing decline. Meanwhile, clock frequency has improved with about 29 % annually. The improvement in clock frequency was enabled both by an annual device speed scaling of 20 % as well as by longer pipelines with a lower gate-depth in each stage. This paper takes a fresh look at – and tries to remove the confusion about – performance scaling that exists in the computer architecture community. I.
1223|Fast Interpolated Cameras by combining a GPU based Plane Sweep with a Max-Flow Regularisation Algorithm|The paper presents a method for the high speed calculation of crude depth maps. Performance and applicability are illustrated for view interpolation based on two input video streams, but the algorithm is perfectly amenable to multi-camera environments. First a 
1224|Kohonen Feature Mapping through Graphics Hardware|This work describes the utilization of the inherent parallelism of commonly available hardware graphics accelerators for the realization of the Kohonen feature map. The result is an essential reduction of computing time compared to standard software implementations.  Keywords. Kohonen feature map, computer graphics, hardware, OpenGL , frame buffer. 1 Introduction  The Kohonen feature map (KFM) [3] is a particular kind of an artificial neural network (ANN) model, which consists of one layer of n-dimensional units  (neurons). They are fully connected with the network input. Additionally, there exist lateral connections through which a topological structure is imposed. For the standard model, the topology is a regular two-dimensional map instantiated by connections between each unit and its direct neighbors. The KFM is used for unsupervised learning tasks [2]. Through n-dimensional training samples, the units organize in a way that they match the distribution of samples in their n-dimensi...
1225|  A Relational Debugging Engine for the Graphics Pipeline |  We present a new, unified approach to debugging graphics software. We propose a representation of all graphics state over the course of program execution as a relational database, and produce a query-based framework for extracting, manipulating, and visualizing data from all stages of the graphics pipeline. Using an SQLbased query language, the programmer can establish functional relationships among all the data, linking OpenGL state to primitives to vertices to fragments to pixels. Based on the Chromium library, our approach requires no modification to or recompilation of the program to be debugged, and forms a superset of many existing techniques for debugging graphics software.
1226|A graphics hardware accelerated algorithm for nearest neighbor search|Abstract. We present a GPU algorithm for the nearest neighbor search, an important database problem. The search is completely performed using the GPU: No further post-processing using the CPU is needed. Our experimental results, using large synthetic and real-world data sets, showed that our GPU algorithm is several times faster than its CPU version. 1
1227|Toward real time fractal image compression using graphics hardware|Abstract. In this paper, we present a parallel fractal image compression using the programmable graphics hardware. The main problem of fractal compression is the very high computing time needed to encode images. Our implementation exploits SIMD architecture and inherent parallelism of recently graphic boards to speed-up baseline approach of fractal encoding. The results we present are achieved on cheap and widely available graphics boards. 1
1228|Application of the Two-Sided Depth Test to CSG Rendering|Shadow mapping is a technique for doing real-time shadowing. Recent work has shown that shadow mapping hardware can be used as a second depth test in addition to the z-test. In this paper, we explore the computational power provided by this second depth test by examining the problem of rendering objects described as CSG (Constructive Solid Geometry) expressions. We provide an algorithm that asymptotically improves the number of rendering passes required to display a CSG object by a factor of n by exploiting the two-sided depth test. Interestingly, a matching lower bound can be proved demonstrating that our algorithm is optimal.
1229|Hardware Based Wavelet Transformations|Abstract Many filtering and feature extraction algorithms usewavelet or related multiscale representations of volume data for edge detection and processing. Due tothe computational complexity of these approaches no interactive visualization of the extraction process ispossible nowadays. Using the hardware of modern graphics workstations for wavelet decomposition andreconstruction is a first important step for removing lags in the visualization cycle. 1 Introduction Feature extraction has been proven to be a usefulutility for segmentation and registration in volume visualization [6, 14]. Many edge detectionalgorithms used in this step employ wavelets or related basis functions for the internal represen-tation of the volume. Additionally, wavelets can be used for fast volume visualization [4] usingthe Fourier rendering approach [7, 13]. Wavelet decomposition and reconstruction isusually implemented by applying multiple convolution and down- / up-sampling steps to thevolume data. The convolution steps will not scale with new computer hardware as well aspure computational problems, as they are already mainly memory-bound. When using typ-ical tensor-product wavelets the complete volume data has to be accessed three times for eachwavelet filtering step.
1230|Efficient 3D Audio Processing with the GPU|Introduction  Audio processing applications are among the most computeintensive and often rely on additional DSP resources for realtime performance. However, programmable audio DSPs are in general only available to product developers. Professional audio boards with multiple DSPs usually support specific effects and products while consumer &#034;game-audio&#034; hardware still only implements fixed-function pipelines which evolve at a rather slow pace.  The widespread availability and increasing processing power of GPUs could offer an alternative solution. GPU features, like multiply-accumulate instructions or multiple execution units, are similar to those of most DSPs [3]. Besides, 3D audio rendering applications require a significant number of geometric calculations, which are a perfect fit for the GPU. Our feasibility study investigates the use of GPUs for efficient audio processing.  GPU-accelerated audio rendering  We consider a combination of two simple operations commonly used for 3D audio
1231|MANOCHA D.: Efficient relational database management using graphics processors|We present algorithms using graphics processing units (GPUs) to efficiently perform database management queries. Our algorithms use efficient data memory representations and storage models on GPUs to perform fast database computations. We present relational database algorithms that successfully exploit the high memory bandwidth and the inherent parallelism available in GPUs. We implement these algorithms on commodity GPUs and compare their performance with optimized CPU-based algorithms. We show that the GPUs can be used as a co-processor to accelerate many database and data mining queries. 1.
1232|Content-based image retrieval at the end of the early years|The paper presents a review of 200 references in content-based image retrieval. The paper starts with discussing the working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap. Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrieval sorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by: accumulative and global features, salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures is reviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable of giving by interaction. We briefly discuss aspects of system engineering: databases, system architecture, and evaluation. In the concluding section, we present our view on: the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap.
1233|The SR-tree: An Index Structure for High-Dimensional Nearest Neighbor Queries|Recently, similarity queries on feature vectors have been widely used to perform content-based retrieval of images. To apply this technique to large databases, it is required to develop multidimensional index structures supporting nearest neighbor queries e ciently. The SS-tree had been proposed for this purpose and is known to outperform other index structures such as the R*-tree and the K-D-B-tree. One of its most important features is that it employs bounding spheres rather than bounding rectangles for the shape of regions. However, we demonstrate in this paper that bounding spheres occupy much larger volume than bounding rectangles with high-dimensional data and that this reduces search efficiency. To overcome this drawback, we propose a new index structure called the SR-tree (Sphere/Rectangle-tree) which integrates bounding spheres and bounding rectangles. A region of the SR-tree is specified by the intersection of a bounding sphere and a bounding rectangle. Incorporating bounding rectangles permits neighborhoods to be partitioned into smaller regions than the SS-tree and improves the disjointness among regions. This enhances the performance on nearest neighbor queries especially for highdimensional and non-uniform data which can be practical in actual image/video similarity indexing. We include the performance test results that verify this advantage of the SR-tree and show that the SR-tree outperforms both the SS-tree and the R*-tree.   
1234|The Bayesian image retrieval system, PicHunter: Theory, implementation, and psychophysical experiments| This paper presents the theory, design principles, implementation, and performance results of PicHunter, a prototype content-based image retrieval (CBIR) system that has been developed over the past three years. In addition, this document presents the rationale, design, and results of psychophysical experiments that were conducted to address some key issues that arose during PicHunter’s development. The PicHunter project makes four primary contributions to research on content-based image retrieval. First, PicHunter represents a simple instance of a general Bayesian framework we describe for using relevance feedback to direct a search. With an explicit model of what users would do, given what target image they want, PicHunter uses Bayes’s rule to predict what is the target they want, given their actions. This is done via a probability distribution over possible image targets, rather than by refining a query. Second, an entropy-minimizing display algorithm is described that attempts to maximize the information obtained from a user at each iteration of the search. Third, PicHunter makes use of hidden annotation rather than a possibly inaccurate/inconsistent annotation structure that the user must learn and make queries in. Finally, PicHunter introduces two experimental paradigms to quantitatively evaluate the performance of the system, and psychophysical experiments are presented that support the theoretical claims.  
1235|Content-based representation and retrieval of visual media: A state-of-the-art review|This paper reviews a number of recently available techniques in contentanalysis of visual media and their application to the indexing, retrieval,abstracting, relevance assessment, interactive perception, annotation and re-use of visualdocuments. 1. Background A few years ago, the problems of representation and retrieval of visualmedia were confined to specialized image databases (geographical, medical, pilot experimentsin computerized slide libraries), in the professional applications of the audiovisualindustries (production, broadcasting and archives), and in computerized training or education. The presentdevelopment of multimedia technology and information highways has put content processing of visualmedia at the core of key application domains: digital and interactive video, large distributed digital libraries, multimedia publishing. Though the most important investments have been targeted at the information infrastructure (networks, servers, coding and compression, deliverymodels, multimedia systems architecture), a growing number of researchers have realized thatcontent processing will be a key asset in putting together successful applications. The need for contentprocessing techniques has been made evident from a variety of angles, ranging from achievingbetter quality in compression, allowing user choice of programs in video-on-demand, achieving betterproductivity in video production, providing access to large still image databases or integrating still images and video in multimedia publishing and cooperative work. Content-based retrieval of visual media and representation of visualdocuments in human-computer interfaces are based on the availability of content representationdata (time-structure for
1236|Pictoseek: combining color and shape invariant features for image retrieval |Abstract—We aim at combining color and shape invariants for indexing and retrieving images. To this end, color models are proposed independent of the object geometry, object pose, and illumination. From these color models, color invariant edges are derived from which shape invariant features are computed. Computational methods are described to combine the color and shape invariants into a unified high-dimensional invariant feature set for discriminatory object retrieval. Experiments have been conducted on a database consisting of 500 images taken from multicolored man-made objects in real world scenes. From the theoretical and experimental results it is concluded that object retrieval based on composite color and shape invariant features provides excellent retrieval accuracy. Object retrieval based on color invariants provides very high retrieval accuracy whereas object retrieval based entirely on shape invariants yields poor discriminative power. Furthermore, the image retrieval scheme is highly robust to partial occlusion, object clutter and a change in the object’s pose. Finally, the image retrieval scheme is integrated into the PicToSeek system on-line at
1237|Shape-Based Retrieval: A Case Study with Trademark Image Databases|Retrieval efficiency and accuracy are two important issues in designing a content-based database retrieval system. We propose a method for trademark image database retrieval based on object shape information that would supplement traditional text-based retrieval systems. This system achieves both the desired efficiency and accuracy using a two-stage hierarchy: in the first stage, simple and easily computable shape features are used to quickly browse through the database to generate a moderate number of plausible retrievals when a query is presented; in the second stage, the candidates from the first stage are screened using a deformable template matching process to discard spurious matches. We have tested the algorithm using hand drawn queries on a trademark database containing 1; 100 images. Each retrieval takes a reasonable amount of computation time (¸ 4-5 seconds on a Sun Sparc 20 workstation). The top most image retrieved by the system agrees with that obtained by human subjects, ...
1238|Convexity Rule for Shape Decomposition Based on Discrete Contour Evolution|We concentrate here on decomposition of 2D objects into meaningful parts of visual form,orvisual parts. It is a simple observation that convex parts of objects determine visual parts. However, the problem is that many significant visual parts are not convex, since a visual part may have concavities. We solve this problem by identifying convex parts at different stages of a proposed contour evolution method in which significant visual parts will become convex object parts at higher stages of the evolution. We obtain a novel rule for decomposition of 2D objects into visual parts, called the hierarchical convexity rule, which states that visual parts are enclosed by maximal convex (with respect to the object) boundary arcs at different stages of the contour evolution. This rule determines not only parts of boundary curves but directly the visual parts of objects. Moreover, the stages of the evolution hierarchy induce a hierarchical structure of the visual parts. The more advanced the stage of contour evolution, the more significant is the shape contribution of the obtained visual parts. c ? 1999 Academic Press Key Words: visual parts; discrete curve evolution; digital curves; digital straight line segments; total curvature; shape hierarchy; digital geometry. 1.
1239|A parallel computing approach to creating engineering concept spaces for semantic retrieval: The Illinois Digital Library Initiative project|Abstract-This research presents preliminary results generated from the semantic retrieval research component of the Illinois Digital Library Initiative (DLI) project. Using a variation of the automatic thesaurus generation techniques, to which we refer as the concept space approach, we aimed to create graphs of domain-specific concepts (terms) and their weighted co-occurrence relationships for all major engineering domains. Merging these concept spaces and providing traversal paths across different concept spaces could potentially help alleviate the vocabulary (difference) problem evident in large-scale information retrieval. We have experimented previously with such a technique for a smaller molecular biology domain (Worm Community System, with IO+ MBs of document collection) with encouraging results. In order to address the scalability issue related to large-scale information retrieval and analysis for the current Illinois DLI project, we recently conducted experiments using the concept space approach on parallel supercomputers. Our test collection included 2+ GBs of computer science and electrical engineering abstracts extracted from the INSPEC database. The concept space approach called for extensive textual and statistical analysis (a form of knowledge discovery) based on automatic indexing and cooccurrence analysis algorithms, both previously tested in the biology domain. Initial testing results using a 512-node CM-5 and a 16processor SGI Power Challenge were promising. Power Challenge was later selected to create a comprehensive computer engineering concept space of about 270,000 terms and 4,000,000+ links using 24.5 hours of CPU time. Our system evaluation involving 12 knowledgeable subjects revealed that the automatically-created computer engineering concept space generated
1240|A Knowledge-Based Approach for Retrieving Images by Content|A knowledge-based approach is introduced for retrieving images by content. It supports the answering of conceptual image queries involving similar-to predicates, spatial semantic operators, and references to conceptual terms. Interested objects in the images are represented by contours segmented from images. Image content such as shapes and spatial relationships are derived from object contours according to domain-specific image knowledge. A three-layered model is proposed for integrating image representations, extracted image features, and image semantics. With such a model, images can be retrieved based on the features and content specified in the queries. The knowledge-based query processing is based on a query relaxation technique. The image features are classified by an automatic clustering algorithm and represented by Type Abstraction Hierarchies (TAHs) for knowledge-based query processing. Since the features selected for TAH generation are based on context and user profile, and ...
1241|Geometric and Illumination Invariants for Object Recognition|We propose invariant formulations that can potentially be combined into a single system. In particular# we describe a framework for computing invariant features which are insensitiveto rigid motion# a#ne transform# changes of parameterization and scene illumination# perspective transform# and view point change. This is unlike most current research on image invariants which concentrates on either geometric or illumination invariants exclusively. The formulations are widely applicable to many popular basis representations# such as wavelets #3# 4# 24# 25## short#time Fourier analysis #13#35## and splines #2# 5#37#. Exploiting formulations that examine information about shape and color at di#erent resolution levels# the new approachisneither strictly global nor local. It enables a quasi#localized# hierarchical shape analysis which is rarely found in other known invariant techniques# such as global invariants. Furthermore# it does not require estimating high#order derivatives in computing i...
1242|Reliable and Efficient Pattern Matching Using an Affine Invariant Metric|In the field of pattern matching, there is a clear trade-off between  effectiveness, accuracy and robustness on one hand and efficiency and  simplicity on the other hand. For example, matching patterns more  effectively by using a more general class of transformations usually  results in a considerable increase of computational complexity. In this  paper, we introduce a general pattern matching approach which will be  applied to a new measure called the absolute difference. This patternsimilarity  measure is affine invariant, which stands out favourably in  practical use. The problem of finding a transformation mapping to the  minimal absolute difference, like many pattern matching problems, has  a high computational complexity. Therefore, we base our algorithm on  a hierarchical subdivision of transformation space. The method applies  to any affine group of transformations, allowing optimisations for rigid  motion. Our implementation of the method performs well in terms of  reliabilit...
1243|A novel vector-based approach to color image retrieval using a vector angular-based distance measure|Color is the characteristic which is most used for image indexing and retrieval. Due to its simplicity, the color histogram remains the most commonly used method for this task. However, the lack of good perceptual histogram similarity measures, the global color content of histograms, and the erroneous retrieval results due to gamma nonlinearity, call for improved methods. We present a new scheme which implements a recursive HSV-space segmentation technique to identify perceptually prominent color areas. The average color vector of these extracted areas are then used to build the image indices, requiring very little storage. Our retrieval is performed by implementing a combination distance measure, based on the vector angle between two vectors. Our system provides accurate retrieval results and high retrieval rate. It allows for queries based on single or multiple colors and, in addition, it allows for certain colors to be excluded in the query. This flexibility is due to our distance measure and the multidimensional query space in which the retrieval ranking of the database images is determined. Furthermore, our scheme proves to be very resistant to gamma nonlinearity providing robust retrieval results for a wide range of gamma nonlinearity values, which proves to be of great importance since, in general, the image acquisition source is unknown. c ? 1999 Academic Press I.
1244|Multiscale Texture Segmentation using Wavelet-Domain Hidden Markov Models|Wavelet-domain Hidden Markov Tree (HMT) models are powerful tools for modeling the statistical properties of wavelet transforms. By characterizing the joint statistics of the wavelet coefficients, HMTs efficiently capture the characteristics of a large class of real-world signals and images. In this paper, we apply this multiscale statistical description to the texture segmentation problem. Using the inherent tree structure of the HMT, we classify textures at various scales and then fuse these decisions into a reliable pixel-by-pixel segmentation.   1 Introduction  The goal of an image segmentation algorithm is to assign a class label to each pixel of an image based on the properties of the pixels and their relationships with their neighbors. The segmentation process is a joint detection and estimation of the class labels and shapes of regions with homogeneous behavior. For proper segmentation of images, both the large and small scale behaviors should be utilized to segment both large,...
1245|Document image database retrieval and browsing using texture analysis|A system is presented that uses texture to retrieve and browse images stored in a large document image database. A method of graphically generating a candidate search image is used that shows the visual layout and content of a target document. All images similar to this candidate are returned for the purpose of browsing orfurther query. The system is accessed using a World wide Web (Web) browser Applications include the retrieval and browsing of document images including newspapers, faxes and business letters. A system is described in this paper that allows for the retrieval of document images based on such non-text features. The system includes a graphical user interface that allows the user to specify the visual characteristics of a query document. From this description, a set of features are generated that are matched against a database of document images. We use texture to describe the types of features in the document. The target document is known only to have a certain typo of layout and content, which corresponds to a texture measure for that document. Texture in effect becomes the search key for a document. 1.
1246|Line pattern retrieval using relational histograms| This paper presents a new compact shape representation for retrieving line-patterns from large databases. The basic idea is to exploit both geometric attributes and structural information to construct a shape histogram. We realize this goal by computing the N-nearest neighbor graph for the lines-segments for each pattern. The edges of the neighborhood graphs are used to gate contributions to a two-dimensional pairwise geometric histogram. Shapes are indexed by searching for the line-pattern that maximizes the cross correlation of the normalized histogram bin-contents. We evaluate the new method on a database containing over 2,500 line-patterns each composed of hundreds of lines.  
1247|Semiotics and Agents for Integrating and Navigating through Multimedia Representations of Concepts|The purpose of this paper is two-fold. We begin by exploring the emerging trend to view multimedia information in terms of low-level and high-level components; the former being feature-based and the latter the \semantics&#034; intrinsic to what is portrayed by the media object. Traditionally, this has been viewed by employing analogies with generative linguistics (e.g. compositional semantics). Recently, a new perspective based on the semiotic tradition has been alluded to in several papers. We believe this to be a more appropriate approach. From this, we propose an approach for tackling this problem which uses an associative data structure expressing authored information together with intelligent agents acting autonomously over this structure. We then show how neural networks can be used to implement such agents. The agents act as \vehicles&#034; for bridging the gap between multimedia semantics and concrete expressions of high-level knowledge, but we suggest that traditional neural network tec...
1248|Algebraic and Geometric Tools to Compute Projective and Permutation Invariants|. This paper studies the computation of projective invariants in pairs of images from uncalibrated cameras, and presents a detailed study of the projective and permutation invariants for configurations of points and/or lines. We give two basic computational approaches, one algebraic and one geometric, and also the relations between the invariants computed by different approaches. In each case, we show how to compute invariants in projective space assuming that the points and lines have already been reconstructed in an arbitrary projective basis, and also, how to compute them directly from image coordinates in a pair of views using only point and line correspondences and the fundamental matrix. Finally, we develop combinations of those projective invariants which are insensitive to permutations of the geometric primitives of each of the basic configurations.  Introduction  Various visual or visually-guided robotics tasks may be carried out using only a projective representation which sh...
1249|The Skyline Operator|We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SQL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations (e.g., join and Top N).
1250|Combining fuzzy information from multiple systems (Extended Abstract)  (1996) |In a traditional database system, the result of a query is a set of values (those values that satisfy the query). In other data servers, such as a system with queries baaed on image content, or many text retrieval systems, the result of a query is a sorted list. For example, in the case of a system with queries based on image content, the query might aak for objects that are a particular shade of red, and the result of the query would be a sorted list of objects in the database, sorted by how well the color of the object matches that given in the query. A multimedia system must somehow synthesize both types of queries (those whose result is a set, and those whose result is a sorted list) in a consistent manner. In this paper we discuss the solution adopted by Garlic, a multimedia information system being developed at
1251|On finding the maxima of a set of vectors|ASSTRACT. Let U1, U2,..., Ud be totally ordered sets and let V be a set of n d-dimensional vectors In U ~ X Us.. X Ud. A partial ordering is defined on V in a natural way The problem of finding all maximal elements of V with respect to the partial ordering ~s considered The computational com-plexity of the problem is defined to be the number of required comparisons of two components and is denoted by Cd(n). It is tnwal that C~(n)  = n- 1 and C,~(n) &lt; O(n 2) for d _ ~ 2 In this paper we show: (1) C2(n)  = O(n logan) for d = 2, 3 and Cd(n)  ~ O(n(log2n) ~-~) for d ~ 4, (2) C,t(n)&gt; _ flog2 n!l for d _&gt; 2 KEY WORDS AND PHRASES: maxima of a set of vectors, computattonal complexity, number of com-parisons, algorithm, recurrence CR CATEaOmES. 5.25, 5,31, 5.39 1.
1252|Reducing the braking distance of an SQL query engine|In a recent paper, we proposed adding a STOP AFTER clause to SQL to permit the cardinality of a query result to be explicitly limited by query writers and query tools. We demonstrated the usefulness of having this clause, showed how to extend a traditional cost-based query optimizer to accommodate it, and demonstrated via DB2-based simulations that large performance gains are possible when STOP AFTER queries are explicitly supported by the database engine. In this paper, we present several new strategies for efficiently processing STOP AFTER queries. These strategies, based largely on the use of range partitioning techniques, offer significant additional savings for handling STOP AFTER queries that yield sizeable result sets. We describe classes of queries where such savings would indeed arise and present experimental measurements that show the benefits and tradeoffs associated with the new processing strategies. 1
1253|Duplicate record elimination in large data files |The issue of duplicate elimination for large data files in which many occurrences of the same record may appear is addressed. A comprehensive cost analysis of the duplicate elimination operation is presented. This analysis is based on a combinatorial model developed for estimating the size of intermediate runs produced by a modified merge-sort procedure. The performance of this modified merge-sort procedure is demonstrated to be significantly superior to the standard duplicate elimination technique of sorting followed by a sequential pass to locate duplicate records. The results can also be used to provide critical input to a query optimizer in a relational database system.
1254|SEEKing the Truth about Ad Hoc Join Costs|In this paper, we reexamine the results of prior work on methods for computing ad hoc joins. We  develop a detailed cost model for predicting join algorithm performance, and we use the model to develop  cost formulas for the major ad hoc join methods found in the relational database literature. We show  that various pieces of &#034;common wisdom&#034; about join algorithm performance fail to hold up when analyzed  carefully, and we use our detailed cost model to derive optimal buffer allocation schemes for each of  the join methods examined here. We show that optimizing their buffer allocations can lead to large  performance improvements, e.g., as much as a 400% improvement in some cases. We also validate our  cost model&#039;s predictions by measuring an actual implementation of each join algorithm considered. The  results of this work should be directly useful to implementors of relational query optimizers and query  processing systems.  1 Introduction  The join of two sets of tuples is a fundament...
1255|Grouping and Duplicate Elimination: Benefits of Early Aggregation|Early aggregation is a technique for speeding up the processing of GROUP BY  queries by reducing the amount of intermediate data transferred between main memory and disk. It can also be applied to duplicate elimination because duplicate elimination is equivalent to grouping with no aggregation functions. This paper describes six different algorithms for grouping and aggregation, shows how to incorporate early aggregation in each of them, and analyzes the resulting reduction in intermediate data. In addition to the grouping algorithm used, the reduction depends on several factors: the number of groups, the skew in group size distribution, the input size, and the amount of main memory available. All six algorithms considered benefit from early aggregation with grouping by hash partitioning producing the least amount of intermediate data. If the group size distribution is skewed, the overall reduction can be very significant, even with a modest amount of additional main memory. 
1256|Query Evaluation in CROQUE  -- Calculus and Algebra Coincide  | With the substantial change of declarative query languages from plain SQL to the so-called &#034;object SQLs&#034;, in particular OQL, there has surprisingly been not much change in the way problems of query representation and optimization for such languages are tackled. We identify some of the difficulties pure algebraic approaches experience when facing object models and the operations defined for them. Calculus-style formalisms suite this challenge better, but are said not to be efficiently implementable in the database context. This paper proposes a hybrid query representation and optimization approach, combining the strengths of a many-sorted query algebra and the monoid comprehension calculus. We show that efficient execution plans beyond nested-loop processing can be derived -- not only for oe-ß-1 queries -- in such a framework. The translation process accounts for queries manipulating bulk-typed values by employing various join methods of the database engine, as well as queries that us...
1258|Prospect theory: An analysis of decisions under risk|Your use of the JSTOR archive indicates your acceptance of JSTOR&#039;s Terms and Conditions of Use, available at
1259|Who is the entrepreneur? is the wrong question|Entrepreneurship is the creation of organizations. What dlfterentiates entrepreneurs trom non-entrepreneurs ls that entrepreneuns create organizations, whil-e.non-entrepreneurs dc not. In behavioral approaches to the study of entrepreneurship an entre preneur is seen as a set of activlties involved in organization creation, while in tralt approaches an entrepreneur ls a set of personality traits and characteristics. This gager argues that tralt approaches have been unfrultful and that behavioral ap proaches will be a more productive perspective for future&#039;research in entrepreneurship. r My own personal experience was that for ten years we ran a research center in entrepreneurial history; for ten ye:trs we tried to define the entrepreneur. We nevlr succeeded.iEicn of us-had some notion of it-what he thought was, for his purposes, a Geful definition. And I don&#039;t think you&#039;re going to get farther than that. (Cole, 1969, p. 17) How can we know the dancer from the dance? (Yeats, 1956) Arthur Cole&#039;s words have taken on the deeper tones of prophecy. Recent reviews
1260|First-Mover Advantages|for helpfiul discussions on earlier drafts. The Strategic Management Program at
1261|Business unit strategy, managerial characteristics, and business unit effectiveness at strategy implementation|Data from 58 strategic business units (SBUs) reveal that greater marketing/sales experience, greater willingness to take risk, and greater tolerance for ambiguity on the part of the SBU general manager contribute to effectiveness in the case of &#034;build &#034; SBUs but hamper it in the case of &#034;harvest &#034; SBUs. Despite the widespread acceptance of strategy&#039;s role in mediating an or-ganization&#039;s interaction with its environment (Andrews, 1971; Ansoff, 1965; Chandler, 1962; Child, 1972; Miles &amp; Snow, 1978), the scope of research on strategy &#034;implementation &#034; has remained quite narrow. Following Chandler (1962), the concern has been predominantly with how a firm&#039;s organizational structure and control system are, or might be, related to the degree and nature of its product and geographic diversification (Fouraker
1263|Bigtable: A distributed storage system for structured data|Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.  
1264|MapReduce: Simplified Data Processing on Large Clusters|MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.  
1265|Space/Time Trade-offs in Hash Coding with Allowable Errors|this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash- coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency
1266|The Google File System |We have designed and implemented the Google File Sys-tem, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous dis-tributed file systems, our design has been driven by obser-vations of our application workloads and technological envi-ronment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore rad-ically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our ser-vice as well as research and development efforts that require large data sets. The largest cluster to date provides hun-dreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use. Categories and Subject Descriptors D [4]: 3—Distributed file systems
1267|Parallel database systems: the future of high performance database systems|Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1.
1268|Resource Containers: A New Facility for Resource Management in Server Systems|General-purpose operating systems provide inadequate support for resource management in large-scale servers. Applications lack sufficient control over scheduling and management of machine resources, which makes it difficult to enforce priority policies, and to provide robust and controlled service. There is a fundamental mismatch between the original design assumptions underlying the resource management mechanisms of current general-purpose operating systems, and the behavior of modern server applications. In particular, the operating system’s notions of protection domain and resource principal coincide in the process abstraction. This coincidence prevents a process that manages large numbers of network connections, for example, from properly allocating system resources among those connections. We propose and evaluate a new operating system abstraction called a resource container, which separates the notion of a protection domain from that of a resource principal. Resource containers enable fine-grained resource management in server systems and allow the development of robust servers, with simple and firm control over priority policies. 1
1269|Operating System Support for Planetary-Scale Network Services|PlanetLab is a geographically distributed overlay network designed to support the deployment and evaluation of planetary-scale network services. Two high-level goals shape its design. First, to enable a large research community to share the infrastructure, PlanetLab provides distributed virtualization, whereby each service runs in an isolated slice of PlanetLab’s global resources. Second, to support competition among multiple network services, PlanetLab decouples the operating system running on each node from the networkwide services that define PlanetLab, a principle referred to as unbundled management. This paper describes how Planet-Lab realizes the goals of distributed virtualization and unbundled management, with a focus on the OS running on each node. 1
1270|Paxos made live: an engineering perspective|We describe our experience building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected algorithmic and engineering problems encountered, and the solutions we found for them. Our measurements indicate that we have built a competitive system. 1
1271|Integrating compression and execution in column-oriented database systems|Column-oriented database system architectures invite a reevaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads. In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in roworiented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.
1272|Weaving Relations for Cache Performance|Relational database systems have traditionally optimzed for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM) (a.k.a., slotted pages). Recent research, however, indicates that cache utilization and performance is becoming increasingly important on modern platforms. In this paper, we first demonstrate that in-page data placement is the key to high cache performance and that NSM exhibits low cache utilization on modern platforms. Next, we propose a new data organization model called PAX (Partition Attributes Across), that significantly improves cache performance by grouping together all values of each attribute within each page. Because PAX only affects layout inside the pages, it incurs no storage penalty and does not affect I/O behavior. According to our experimental results, when compared to NSM (a) PAX exhibits superior cache and memory bandwidth utilization, saving at least 75% of NSM&#039;s stall time due to data cache accesses, (b) range selection queries and updates on memoryresident relations execute 17-25% faster, and (c) TPC-H queries involving I/O execute 11-48% faster.
1273|Data Placement in Bubba|Thus paper examines the problem of data placement in Bubba, a highly-parallel system for data-intensive applications bemg developed at MCC “Highly-parallel” lmplres that load balancmng IS a cntlcal performance issue “Data-mtenave” means data IS so large that operatrons should be executed where the data resides As a result, data placement becomes a cntlcal performance issue In general, determmmg the optimal placement of data across processmg nodes for performance IS a difficult problem We describe our heuristic approach to solvmg the data placement problem w Bubba We then present expenmental results using a specific workload to provide msrght into the problem Several researchers have argued the benefits of deelustering (1 e, spreading each base relation over many nodes) We show that as declustermg IS increased. load balancing contmues to improve However, for transactions mvolvmg complex Joins, further declusterrng reduces throughput because of communications, startup and termmatron overhead We argue that data placement, especially declustermg, m a highly-parallel system must be considered early in the design, so that mechanrsms can be included for supportmg variable declustermg, for mmtmlzmg the most significant overheads associated with large-scale declustenng, and for gathering the required statistics.
1274|DB2 Parallel Edition|The rate of increase in database size and response time requirements has outpaced  advancements in processor and mass storage technology. One way to satisfy the increasing  demand for processing power and I/O bandwidth in database applications is to have a  number of processors, loosely or tightly coupled, serving database requests concurrently.  Technologies developed during the last decade have made commercial parallel database  systems a reality and these systems have made an inroad into the stronghold of traditionally  mainframe based large database applications. This paper describes the parallel database  project initiated at IBM Research at Hawthorne and the DB2/AIX-PE product based on  it.  1 Introduction  Large scale parallel processing technology has made giant strides in the past decade and there is no doubt that it has established a place for itself. However, almost all of the applications harnessing this technology are scientific or engineering applications. The lack of com...
1275|CONDENSATION - conditional density propagation for visual tracking|The problem of tracking curves in dense visual clutter is challenging. Kalman filtering is inadequate because it is based on Gaussian densities which, being unimodal, cannot represent simultaneous alternative hypotheses. The Condensation algorithm uses &#034;factored sampling&#034;, previously applied to the interpretation of static images, in which the probability distribution of possible interpretations is represented by a randomly generated set. Condensation uses learned dynamical models, together with visual observations, to propagate the random set over time. The result is highly robust tracking of agile motion. Notwithstanding the use of stochastic methods, the algorithm runs in near real-time. Contents 1 Tracking curves in clutter 2 2 Discrete-time propagation of state density 3 3 Factored sampling 6 4 The Condensation algorithm 8 5 Stochastic dynamical models for curve motion 10 6 Observation model 13 7 Applying the Condensation algorithm to video-streams 17 8 Conclusions 26 A Non-line...
1276|Contour Tracking By Stochastic Propagation of Conditional Density|.  In Proc. European Conf. Computer Vision, 1996, pp. 343--356, Cambridge, UK  The problem of tracking curves in dense visual clutter is  a challenging one. Trackers based on Kalman filters are of limited use;  because they are based on Gaussian densities which are unimodal, they  cannot represent simultaneous alternative hypotheses. Extensions to the  Kalman filter to handle multiple data associations work satisfactorily in  the simple case of point targets, but do not extend naturally to continuous  curves. A new, stochastic algorithm is proposed here, the Condensation   algorithm --- Conditional Density Propagation over time. It  uses `factored sampling&#039;, a method previously applied to interpretation  of static images, in which the distribution of possible interpretations is  represented by a randomly generated set of representatives. The Condensation   algorithm combines factored sampling with learned dynamical  models to propagate an entire probability distribution for object  pos...
1277|Kalman Filter-based Algorithms for Estimating Depth from Image Sequences|Using known camera motion to estimate depth from image sequences is an important problem in robot vision. Many applications of depth-from-motion, including navigation and manipulation, require algorithms that can estimate depth in an on-line, incremental fashion. This requires a representation that records the uncertainty in depth estimates and a mechanism that integrates new measurements with existing depth estimates to reduce the uncertainty over time. Kalman filtering provides this mechanism. Previous applications of Kalman filtering to depth-from-motion have been limited to estimating depth at the location of a sparse set of features. In this paper, we introduce a new, pixel-based (iconic) algorithm that estimates depth and depth uncertainty at each pixel and incrementally refines these estimates over time. We describe the algorithm and contrast its formulation and performance to that of a feature-based Kalman filtering algorithm. We compare the performance of the two approaches by analyzing their theoretical convergence rates, by conducting quantitative experiments with images of a flat poster, and by conducting qualitative experiments with images of a realistic outdoor-scene model. The results show that the new method is an effective way to extract depth from lateral camera translations. This approach can be extended to incorporate general motion and to integrate other sources of information, such as stereo. The algorithms we have developed, which combine Kalman filtering with iconic descriptions of depth, therefore can serve as a useful and general framework for low-level dynamic vision.
1278|Visual Tracking of High DOF Articulated Structures: an Application to Human Hand Tracking|. Passive sensing of human hand and limb motion is important for a wide range of applications from human-computer interaction to athletic performance measurement. High degree of freedom articulated mechanisms like the human hand are difficult to track because of their large state space and complex image appearance. This article describes a model-based hand tracking system, called DigitEyes, that can recover the state of a 27 DOF hand model from ordinary gray scale images at speeds of up to 10 Hz. 1 Introduction Sensing of human hand and limb motion is important in applications from Human-Computer Interaction (HCI) to athletic performance measurement. Current commercially available solutions are invasive, and require the user to don gloves [15] or wear targets [8]. This paper describes a noninvasive visual hand tracking system, called DigitEyes. We have demonstrated hand tracking at speeds of up to 10 Hz using line and point features extracted from gray scale images of unadorne...
1279|Learning Flexible Models from Image Sequences|The &#034;Point Distribution Model&#034;, derived by analysing the modes of variation of a set of training examples, can be a useful tool in machine vision. One of the drawbacks of this approach to date is that the training data is acquired with human intervention where fixed points must be selected by eye from example images. This is a laborious process and may lead to a non-representative set of training examples being used. A method is described for generating a similar flexible shape model automatically from real image data. A cubic B-spline is used as the shape vector for training the model. Large training sets are used to generate a robust model of the human profile. The resulting modes of variation show the potential of the model for labelling and tracking of pedestrians in realworld scenes. Furthermore, an extended model is described which incorporates the direction of motion of the human, allowing the extrapolation of direction from shape. 1 Introduction  We wish to generate a 2D flexib...
1280|Learning to Track the Visual Motion of Contours|A development of a method for tracking visual contours is described. Given an &#034;un-trained&#034; tracker, a training-motion of an object can be observed over some extended time and stored as an image sequence. The image sequence is used to learn parameters in a stochastic differential equation model. These are used, in turn, to build a tracker whose predictor imitates the motion in the training set. Tests show that the resulting trackers can be markedly tuned to desired curve shapes and classes of motions.  Contents  1 Introduction 2 2 Tracking framework 2 2.1 Curve representation : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2 2.2 Tracking as estimation over time : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 3 2.3 Rigid body transformations : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 3 2.4 Curves in motion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 7 2.5 Discrete-time model : : : : : : : : : :...
1281|D Position, Attitude and Shape Input Using Video Tracking of Hands and Lips |Recent developments in video-tracking allow the outlines of moving, natural objects in a video-camera input stream to be tracked live, at full video-rate. Previous systems have been available to do this for specially illuminated objects or for naturally illuminated but polyhedral objects. Other systems have been able to track nonpolyhedral objects in motion, in some cases from live video, but following only centroids or key-points rather than tracking whole curves. The system described here can track accurately the curved silhouettes of moving non-polyhedral objects at frame-rate, for example hands, lips, legs, vehicles, fruit, and without any special hardware beyond a desktop workstation and a video-camera and framestore. The new algorithms are a synthesis of methods in deformable models, B-spline curve representation and control theory. This paper shows how such a facility can be used to turn parts of the body --- for instance, hands and lips --- into input devices. Rigid motion of a...
1282|A Bayesian Approach to Dynamic Contours through Stochastic Sampling and Simulated Annealing|In many applications of image analysis, simply connected objects are to be located in noisy images. During the last 5-6 years active contour models have become popular for finding the contours of such objects. Connected to these models are iterative algorithms for finding the minimizing energy curves making the curves behave dynamically through the iterations. These approaches do however have several disadvantages. The numerical algorithms that are in use constraint the models that can be used. Furthermore, in many cases only local minima can be achieved.
1283|Visual interpretation of known objects in constrained scenes|Recent work on the visual interpretation of traffic scenes is described which relies heavily on  a priori knowledge of the scene and position of the camera, and expectations about the shapes of vehicles and their likely movements in the scene. Knowledge is represented in the computer as explicit 3D geometrical models, dynamic filters, and descriptions of behaviour. Model-based vision, based on reasoning with analog models, avoids many of the classical problems in visual perception: recognition is robust against changes in the image of shape, size, colour and illumination. The 3D understanding of the scene which results also deals naturally with occlusion, and allows the behaviour of vehicles to be interpreted. The experiments with machine vision raise questions about the part played by perceptual context for object recognition in natural vision, and the neural mechanisms which might serve such a role.  Vision in constrained scenes - 2 - GDS 8/3/92  1. INTRODUCTION  High-level vision i...
1284|Learning Dynamics of Complex Motions from Image Sequences|The performance of Active Contours in tracking is highly dependent on the availability of an appropriate model of shape and motion, to use as a predictor. Models can be hand-built, but it is far more effective and less time-consuming to learn them from a training set. Techniques to do this exist both for shape, and for shape and motion jointly. This paper extends the range of shape and motion models in two significant ways. The first is to model jointly the random variations in shape arising within an object-class and those occuring during object motion. The resulting algorithm is applied to tracking of plants captured by a video camera mounted on an agricultural robot. The second addresses the tracking of coupled objects such as head and lips. In both cases, new algorithms are shown to make important contributions to tracking performance.
1285|Generating spatiotemporal models from examples|Physically based vibration modes have been shown to provide a useful mechanism for describing non-rigid motions of articulated and deformable objects. The approach relies on assumptions being made about the elastic properties of an object to generate a compact set of orthogonal shape parameters which can then be used for tracking and data approximation. We present a method for automatically generating an equivalent physically based model using a training set of examples of the object deforming, tuning the elastic properties of the object to reflect how the object actually deforms. The resulting model provides a low dimensional shape description that allows accurate temporal extrapolation based on the training motions. Results are shown in which the method is applied to an automatically acquired training set of the outline of a walking pedestrian.
1286|Conditional-Mean Estimation Via Jump-Diffusion Processes in Multiple Target Tracking/Recognition|A new algorithm is presented for generating the conditional mean estimates of functions of target positions, orientations and type in recognition, and tracking of an unknown nmnber of targets and target types. Taking a Bayesian approach, a posterior measure is defined on the tracking/target parameter space by combining a narrowband sensor array manifold model with a high resolution imaging model, and a prior based on airplane dynanfics. The Newtoninn force equations governing rigid body dynamic s are utilized to form the prior density on airplane motion. The conditional mean estimates are generated using a random sampling algorithm based on jump-diffusion processes [1] i)r empirically generating MMSE estimates of functions of these random target positions, orientations, and type under the posterior measure. Results are presented on target tracking and identification from an implementation of the algorithm on a networked Silicon Graphics workstation and DECmpp/MasPar parallel machine.
1287|Cilk: An Efficient Multithreaded Runtime System|Cilk (pronounced &#034;silk&#034;) is a C-based runtime system for multithreaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the &#034;work&#034; and &#034;critical-path length&#034; of a Cilk computation can be used to model performance accurately. Consequently, a Cilk programmer can focus on reducing the computation&#039;s work and critical-path length, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of &#034;fully strict&#034; (well-structured) programs, the Cilk scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The Cilk
1288|FAST FOURIER TRANSFORMS: A TUTORIAL REVIEW AND A STATE OF THE ART|The publication of the Cooley-Tukey fast Fourier transform (FIT) algorithm in 1965 has opened a new area in digital signal processing by reducing the order of complexity of some crucial computational tasks like Fourier transform and convolution from N 2 to N log2 N, where N is the problem size. The development of the major algorithms (Cooley-Tukey and split-radix FFT, prime factor algorithm and Winograd fast Fourier transform) is reviewed. Then, an attempt is made to indicate the state of the art on the subject, showing the standing of research, open problems and implementations. 
1289|An analysis of dag-consistent distributed shared-memory algorithms|In this paper, we analyze the performance of parallel multi-threaded algorithms that use dag-consistent distributed shared memory. Specifically, we analyze execution time, page faults, and space requirements for multithreaded algorithms executed by a FP(C) workstealing thread scheduler and the BACKER algorithm for maintaining dag consistency. We prove that if the accesses to the backing store are random and independent (the BACKER algorithm actually uses hashing), the expected execution time TP(C)of a “fully strict” multithreaded computation on P processors, each with a LRU cache of C pages, is O(T1(C)=P+mCT8), where T1(C)is the total work of the computation including page faults, T 8 is its critical-path length excluding page faults, and m is the minimum page transfer time. As
1290|An algorithm for computing the mixed radix fast Fourier transform|This paper presents an algorithm for computing the fast Fourier transform, based on a method proposed by Cooley and Tukey. As in their algorithm, the dimension n of the transform is factored (if possi-ble), and n/p elementary transforms of dimension p are computed for each factor p of n. An improved method of computing a transform step corresponding to an odd factor of n is given; with this method, the number of complex multiplicatiops for an elementary transform of di-mension p is reduced from (p-1)2 to (p-1)2/4 for odd p. The fast Fourier transform, when computed in place, requires a final permuta-tion step to arrange the results in normal order. This algorithm in-cludes an efficient method for permuting the results in place. The al-gorithm is described mathematically and illustrated by a FORTRAN subroutine.
1291|A Framework for Generating Distributed-Memory Parallel Programs for Block Recursive Algorithms|A framework for synthesizing communication-efficient distributed-memory parallel programs for block recursive algorithms such as the fast Fourier transform (FFT) and Strassen’s matrix multiplication is presented. This framework is based on an algebraic representation of the algorithms, which involves the tensor (Kronecker) product and other matrix operations. This representation is useful in analyzing the communication implications of computation partitioning and data distributions. The programs are synthesized under two different target program models. These two models are based on different ways of managing the distribution of data for optimizing communication. The first model uses point-to-point interprocessor communication primitives, whereas the second model uses data redistribution primitives involving collective all-to-many communication. These two program models are shown to be suitable for different ranges of problem size. The methodology is illustrated by synthesizing communication-efficient programs for the FFT. This framework has been incorporated into the EX-TENT system for automatic generation of parallel/vector programs for block recursive algorithms. © 1996 Academic Press, Inc. 1.
1292|Automatic Generation of Prime Length FFT Programs|We describe a set of programs for circular convolution and prime length FFTs that are relatively short, possess great structure, share many computational procedures, and cover a large variety of lengths. The programs make clear the structure of the algorithms and clearly enumerate independent computational branches that can be performed in parallel. Moreover, each of these independent operations is made up of a sequence of sub-operations which can be implemented as vector/parallel operations. This is in contrast with previously existing programs for prime length FFTs: they consist of straight line code, no code is shared between them, and they can not be easily adapted for vector/parallel implementations. We have also developed a program that automatically generates these programs for prime length FFTs. This code generating program requires information only about a set of modules for computing cyclotomic convolutions. Contact Address:  Ivan W. Selesnick Electrical and Computer Engineer...
1293|The Quick Discrete Fourier Transform|This paper will look at an approach that uses symmetric properties of the basis function to remove redundancies in the calculation of discrete Fourier transform (DFT). We will develop an algorithm, called the quick Fourier transform (QFT), that will reduce the number of floating point operations necessary to compute the DFT by a factor of two or four over direct methods or Goertzel&#039;s method for prime lengths. Further apply the idea to the calculation of a DFT of length-2  M  , we construct a new O(N log N) algorithm. The algorithm can be easily modified to compute the DFT with only a subset of input points, and it will significantly reduce the number of operations when the data are real. The simple structure of the algorithm and the fact that it is well suited for DFTs on real data should lead to efficient implementations and to a wide range of applications. 1. INTRODUCTION  In the field of digital signal processing, the discrete Fourier transform (DFT) is an interesting, important, an...
1294|Locally weighted learning|This paper surveys locally weighted learning, a form of lazy learning and memorybased learning, and focuses on locally weighted linear regression. The survey discusses distance functions, smoothing parameters, weighting functions, local model structures, regularization of the estimates and bias, assessing predictions, handling noisy data and outliers, improving the quality of predictions by tuning t parameters, interference between old and new data, implementing locally weighted learning e ciently, and applications of locally weighted learning. A companion paper surveys how locally weighted learning can be used in robot learning and control.
1296|Design-adaptive nonparametric regression|Visual realism is defined as the degree an image appears to people to be a photo rather than computer generated. Can computers predict human visual realism perception?
1297|Discriminant Adaptive Nearest Neighbor Classification|Nearest neighbor classification expects the class conditional probabilities to be locally constant, and suffers from bias in high dimensions. We propose a locally adaptive form of nearest neighbor classification to try to ameliorate this curse of dimensionality. We use a local linear discriminant analysis to estimate an effective metric for computing neighborhoods. We determine the local decision boundaries from centroid information, and then shrink neighborhoods in directions orthogonal to these local decision boundaries, and elongate them parallel to the boundaries. Thereafter, any neighborhood-based classifier can be employed, using the modified neighborhoods. The posterior probabilities tend to be more homogeneous in the modified neighborhoods. We also propose a method for global dimension reduction, that combines local dimension information. In a number of examples, the methods demonstrate the potential for substantial improvements over nearest neighbor classification.  Keywords--...
1298|A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features|In the past, nearest neighbor algorithms for learning from examples have worked best in domains in which all features had numeric values. In such domains, the examples can be treated as points and distance metrics can use standard definitions. In symbolic domains, a more sophisticated treatment of the feature space is required. We introduce a nearest neighbor algorithm for learning in domains with symbolic features. Our algorithm calculates distance tables that allow it to produce real-valued distances between instances, and attaches weights to the instances to further modify the structure of feature space. We show that this technique produces excellent classification accuracy on three problems that have been studied by machine learning researchers: predicting protein secondary structure, identifying DNA promoter sequences, and pronouncing English text. Direct experimental comparisons with the other learning algorithms show that our nearest neighbor algorithm is comparable or superior ...
1299|Local Learning Algorithms|Very rarely are training data evenly distributed in the input space. Local learning algorithms attempt to locally adjust the capacity of the training system to the properties of the training set in each area of the input space. The family of local learning algorithms contains known methods, like the k-Nearest Neighbors method (kNN) or the Radial Basis Function networks (RBF), as well as new algorithms. A single analysis models some aspects of these algorithms. In particular, it suggests that neither kNN or RBF, nor non local classifiers, achieve the best compromise between locality and capacity. A careful control of these parameters in a simple local learning algorithm has provided a performance breakthrough for an optical character recognition problem. Both the error rate and the rejection performance have been significantly improved. 1 Introduction.  Here is a simple local algorithm: For each testing pattern, (1) select the few training examples located in the vicinity of the testing...
1300|Flexible Metric Nearest Neighbor Classification|The K-nearest-neighbor decision rule assigns an object of unknown class to the plurality class among the K labeled &#034;training&#034; objects that are closest to it. Closeness is usually defined in terms of a metric distance on the Euclidean space with the input measurement variables as axes. The metric chosen to define this distance can strongly effect performance. An optimal choice depends on the problem at hand as characterized by the respective class distributions on the input measurement space, and within a given problem, on the location of the unknown object in that space. In this paper new types of K-nearest-neighbor procedures are described that estimate the local relevance of each input variable, or their linear combinations, for each individual point to be classified. This information is then used to separately customize the metric used to define distance from that object in finding its nearest neighbors. These procedures are a hybrid between regular K-nearest-neighbor methods and tree-structured recursive partitioning techniques popular in statistics and machine learning.
1301|Variable bandwidth and local linear regression smoothers|In this paper we introduce an appealing nonparametric method for estimating the mean regression function. The proposed method combines the ideas of local linear smoothers and variable bandwidth. Hence, it also inherits the advantages of both approaches. We give expressions for the conditional MSE and MISE of the estimator. Minimization of the MISE leads to an explicit formula for an optimal choice of the variable bandwidth. Moreover, the merits of considering a variable bandwidth are discussed. In addition, we show that the estimator does not have boundary effects, and hence does not require modifications at the boundary. The performance of a corresponding plug-in estimator is investigated. Simulations illustrate the proposed estimation method.  
1302|Fast implementations of nonparametric curve estimators|Recent proposals for implementation of kernel based nonparametric curve estimators are seen to be faster than naive direct implementations by factors up into the hundreds. The main ideas behind two different approaches of this type are made clear. Careful speed comparisons in a variety of settings, and using a variety of machines and software is done. Various issues on computational accuracy and stability are also discussed. The fast methods are seen to be somewhat better than methods traditionally considered very fast, such as LOWESS and smoothing splines. 1
1303|On The Almost Everywhere Convergence Of Nonparametric Regression Function Estimates|this paper we find sufficient conditions on the Wni&#039;s that guarantee  E([mn(x)-m(x)]P)--)O as n--) for almost all x(/)  (1.7)  whenever E([ Y[P) oo, allp -- 1
1304|Memory-Based Neural Networks For Robot Learning|This paper explores a memory-based approach to robot learning, using memorybased neural networks to learn models of the task to be performed. Steinbuch and Taylor presented neural network designs to explicitly store training data and do nearest neighbor lookup in the early 1960s. In this paper their nearest neighbor network is augmented with a local model network, which fits a local model to a set of nearest neighbors. This network design is equivalent to a statistical approach known as locally weighted regression, in which a local model is formed to answer each query, using a weighted regression in which nearby points (similar experiences) are weighted more than distant points (less relevant experiences). We illustrate this approach by describing how it has been used to enable a robot to learn a difficult juggling task. Keywords: memory-based, robot learning, locally weighted regression, nearest neighbor, local models. 1 Introduction  An important problem in motor learning is approxim...
1305|Learning to Catch: Applying Nearest Neighbor Algorithms to Dynamic Control Tasks (Extended Abstract)  (1993) |Steven L. Salzberg  1  and David W. Aha  2 1 Introduction  Dynamic control problems are the subject of much research in machine learning (e.g., Selfridge, Sutton, &amp; Barto, 1985; Sammut, 1990; Sutton, 1990). Some of these studies investigated the applicability of various k-nearest neighbor methods (Dasarathy, 1990) to solve these tasks by modifying control strategies based on previously gained experience (e.g., Connell &amp; Utgoff, 1987; Atkeson, 1989; Moore, 1990; 1991). However, these previous studies did not highlight the fact that small changes in the design of these algorithms drastically alter their learning behavior. This paper describes a preliminary study that investigates this issue in the context of a difficult dynamic control task: learning to catch a ball moving in a three-dimensional space, an important problem in robotics research (Geng et al., 1991). Our thesis in this paper is that agents can improve substantially at physical tasks by storing experiences without explicitly...
1306|Incremental learning of local linear mappings|A new incremental network model for supervised learning is proposed. The model builds up a structure of units each of which has an associated local linear mapping (LLM). Error information obtained during training is used to determine where to insert new units whose LLMs are interpolated from their neighbors. Simulation results for several classication tasks indicate fast convergence as well as good generalization. The ability of the model to also perform function approximation is demonstrated by an example. 1
1307|Automatic Local Smoothing for Spectral Density Estimation|This article uses local polynomial techniques to fit Whittle&#039;s likelihood for spectral density estimation. Asymptotic sampling properties of the proposed estimators are derived, and adaptation of the proposed estimator to the boundary effect is noted. We show that the Whittle likelihood based estimator has advantages over the least-squares based logperiodogram. The bandwidth for the Whittle likelihood-based method is chosen by a simple adjustment of a bandwidth selector proposed in Fan and Gijbels (1995). The effectiveness of the proposed procedure is demonstrated by a few simulated and real numerical examples. Our simulation results support the asymptotic theory that the likelihood based spectral density and log-spectral density estimators are the most appealing among their peers.  KEY WORDS: Bandwidth selection, local polynomial fit, periodogram, spectral density estimation, Whittle likelihood. 1 Introduction  Spectral density estimation is useful for studying stationary time series,...
1308|Asymptotics for nonparametric regression|the error distribution function in
1309|Planning For Dynamic Motions Using A Search Tree|The generation of physics-based motion for articulated figures can be studied as a pathplanning problem through state-space. This thesis presents an algorithm that searches for control sequences that perform a desired action. The search is accelerated by making use of evaluation functions, pruning conditions, and a memory containing successful state-action pairs. Results for various low-level control problems are demonstrated, including balance for an acrobot (a two-link robot); simultaneous balance of two inverted pendulums; hopping and flipping for the acrobot; and swing-up solutions for a double pendulum on a cart. The application of this algorithm for high-level path-planning is demonstrated using the example of a hopping lamp that traverses rugged terrains. ii  Acknowledgements A well-deserved thanks goes to my supervisor, Michiel van de Panne, who has provided a wealth of stimulating ideas and energetic enthusiasm. Michiel provided the starting points for this work; his subseque...
1310|The New Science of Management Decision|Classical theories of choice emphasise decision making as a rational process. In general, these theories fail to recognise the formulation stages of a decision and typically can only be applied to problems comprising two or more measurable alternatives. In response to such limitations, numerous descriptive theories have been developed over the last forty years, intended to describe how decisions are made. This paper presents a framework that classifies descriptive theories using a theme of comparison; comparisons involving attributes, alternatives and situations. The paper also reports on research undertaken within a New Zealand local authority. Twenty three senior managers were interviewed about their decision making with the aim of comparing the responses of participants with how the descriptive decision making literature purports decisions are made. Evidence of behaviour consistent with recognised descriptive theories was also investigated. It was found that few managers exhibited behaviour consistent with what is described in the literature. The major difference appears to be the lack of decision formulation contained within most descriptive theories. Descriptive theories are, in general, theories of choice and few decisions described by participants contained a distinct choice phase.
1311|What Common Ground Exists for Descriptive, Prescriptive, and Normative Utility Theories?|Descriptive and normative modeling of decision making under risk and uncertainty have grown apart over the past decade. Psychological models attempt to accommodate the numerous violations of rationality axioms, including independence and transitivity. Meanwhile, normatively oriented decision analysts continue to insist on the applied usefulness of the subjective expected utility (SEU) model. As this gap has widened, two facts have remained largely unobserved. First, most people in real situations attempt to behave in accord with the most basic rationality principles, even though they are likely to fail in more complex situations. Second, the SEU model is likely to provide consistent and rational answers to decision problems within a given problem structure, but may not be invariant across structures. Thus, people may be more rational than the psychological literature gives them credit for, and applications of the SEU model may be susceptible to some violations of invariance principles. This paper attempts to search out the common ground between the normative, descriptive, and prescriptive modeling by exploring three types of axioms concerning structural rationality, preference rationality, and quasi-rationality. Normatively the first two are mandatory and the last, suspect. Descriptively, all have been questioned, but often the inferences involved have confounded preference and structural rationality. We propose a prescriptive view that entails full compliance with preference rationality, modifications of structural rationality, and acceptance of quasi-rationality to the extent of granting a primary role to the status quo and the decomposition of decision problems into gains and losses.
1312|Image registration methods: a survey|This paper aims to present a review of recent as well as classic image registration methods. Image registration is the process of overlaying images (two or more) of the same scene taken at different times, from different viewpoints, and/or by different sensors. The registration geometrically align two images (the reference and sensed images). The reviewed approaches are classified according to their nature (areabased and feature-based) and according to four basic steps of image registration procedure: feature detection, feature matching, mapping function design, and image transformation and resampling. Main contributions, advantages, and drawbacks of the methods are mentioned in the paper. Problematic issues of image registration and outlook for the future research are discussed too. The major goal of the paper is to provide a comprehensive reference source for the researchers involved in image registration, regardless of particular application areas.  
1313|A computational approach to edge detection|Abstract-This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to- a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge. This detection scheme uses several elongated operators at each point, and the directional operator outputs are integrated with the gradient maximum detector. Index Terms-Edge detection, feature extraction, image processing, machine vision, multiscale image analysis. I.
1314|Cubic convolution interpolation for digital image processing|Absfrucf-Cubic convolution interpolation is a new technique for resampling discrete data. It has a number of desirable features which make it useful for image processing. The technique can be performed efficiently on a digital computer. The cubic convolution interpolation function converges uniformly to the function being interpolated as the sampling increment approaches zero, With the appropriate boundary conditions and constraints on the interpolation kernel, it can be shown that the order of accuracy of the cubic convolution method is between that of linear interpolation and that of cubic splines. A one-dimensional interpolation function is derived in this paper. A separable extension of this algorithm to two dimensions is applied to image data. I
1315|Fast Fluid Registration of Medical Images|. This paper offers a new fast algorithm for non-rigid Viscous Fluid Registration of medical images that is at least an order of magnitude faster than the previous method by Christensen et al. [4]. The core algorithm in the fluid registration method is based on a linear elastic deformation of the velocity field of the fluid. Using the linearity of this deformation we derive a convolution filter which we use in a scalespace framework. We also demonstrate that the &#039;demon&#039;-based registration method of Thirion [13] can be seen as an approximation to the fluid registration method and point to possible problems. 1 Introduction  Non-rigid registration of two medical images is performed by applying global and/or local transformations to one of the images (which we will call the template  T ) in such a way that it matches the other image (the study S). It is important to understand that the aim of the transformation is to map the template completely  onto the study in such a way that informatio...
1316|An Algorithmic Overview of Surface Registration . . .|This paper presents a literature survey of automatic 3D surface registration techniques emphasizing  the mathematical and algorithmic underpinnings of the subject. The relevance of surface  registration to medical imaging is that there is much useful anatomical information in the form  of collected surface points which originate from complimentary modalities and which must be  reconciled. Surface registration
1317|Y.: Warping by radial basis functions ï?? application to facial expressions |The human face is an elastic object. A natural paradigm for rep-resenting facial expressions is to form a complete 3D model of facial muscles and tissues. However, determining the actual parameter val-ues for synthesizing and animating facial expressions is tedious; eval-uating these parameters for facial expression analysis out of grey-level images is ahead of the state of the art in computer vision. Using only 2D face images and a small number of anchor points, we show that the method of radial basis functions provides a powerful mechanism for processing facial expressions. Although constructed specically for facial expressions, our method is applicable to other elastic objects as well. 1
1318|Image Registration Based on Boundary Mapping|A new two-stage approach for nonlinear brain image registration is proposed. In the first stage, an active contour algorithm is used to establish a homothetic one-to-one map between a set of region boundaries in two images to be registered. This mapping is used in the second step: a two-dimensional transformation which is based on an elastic body deformation. This method is tested by registering magnetic resonance images to atlas images. I. Introduction  Registration of both intra-subject and inter-subject brain images has been the subject of extensive study in the medical imaging literature. The various techniques that have been proposed can be classified into three major categories: polynomial transformations, similarity-based methods, and boundary-based methods. Polynomial transformations [1, 2, 3] apply a polynomial warping and determine the coefficients of the polynomial using linear regression if a sufficient number of landmark points is provided. Numerical instabilities and the ...
1319|Degraded Image Analysis: An Invariant Approach|Analysis and interpretation of an image which was acquired by a nonideal imaging system is the key problem in many application areas. The observed image is usually corrupted by blurring, spatial degradations, and random noise. Classical methods like blind deconvolution try to estimate the blur parameters and to restore the image. In this paper, we propose an alternative approach. We derive the features for image representation which are invariant with respect to blur regardless of the degradation PSF provided that it is centrally symmetric. As we prove in the paper, there exist two classes of such features: the first one in the spatial domain and the second one in the frequency domain. We also derive so-called combined invariants, which are invariant to composite geometric and blur degradations. Knowing these features, we can recognize objects in the degraded scene without any restoration.  Index Terms---Degraded image, symmetric blur, blur invariants, image moments, combined invariant...
1320|Voxel similarity measures for 3-D serial MR brain image registration| We have evaluated eight different similarity measures used for rigid body registration of serial magnetic resonance (MR) brain scans. To assess their accuracy we used 33 clinical threedimensional (3-D) serial MR images, with deformable extradural tissue excluded by manual segmentation and simulated 3-D MR images with added intensity distortion. For each measure we determined the consistency of registration transformations for both sets of segmented and unsegmented data. We have shown that of the eight measures tested, the ones based on joint entropy produced the best consistency. In particular, these measures seemed to be least sensitive to the presence of extradural tissue. For these data the difference in accuracy of these joint entropy measures, with or without brain segmentation, was within the threshold of visually detectable change in the difference images.  
1321|Registration Techniques for Multisensor Remotely Sensed Images,” Photogrammetric Engineering|Image registration is one of the basic image processing oper-ations in remote sensing. With the increase in the number of images collected every day from different sensors, automated registration of multisensor/multispectral images has become a very important issue. A wide range of registration tech-niques has been developed for many different types of appli-cations and data. Given the diversity of the data, it i s un-likely that a single registration scheme will work satisfactorily for all different applications. A possible solu-tion is to integrate multiple registration algorithms into a rule-based artificial intelligence system so that appropriate methods for any given set of multisensor data can be auto-matically selected. The first step in the development of such an expert system for remote sensing application would be to obtain a better understanding and characterization of the various existing techniques for image registration. This is the main objective of this paper as we present a comparative study of some recent image registration methods. We empha-size in particular techniques for multisensor image data, and a brief discussion of each of the techniques is given. This comprehensive study will enable the user to select algorithms that work best for hidher particular application domain.
1322|Radial Basis Functions with Compact Support for Elastic Registration of Medical Images|. Common elastic registration schemes based on landmarks  and using radial basis functions (RBFs) such as thin-plate splines or  multiquadrics are global. Here, we introduce radial basis functions with  compact support for elastic registration of medical images. With these  basis functions the inuence of a landmark on the registration result  is limited to a circle in 2D or, respectively, to a sphere in 3D. Therefore,  the registration can be locally constrained which especially allows  to deal with rather local changes in medical images due to, e.g., tumor  resection. An important property of the used RBFs is that they are positive  denite. Thus, the solvability of the resulting system of equations is  always guaranteed. We give the theoretical background of the basis functions  with compact support and compare them with other basis functions  w.r.t. locality, solvability, and eciency. We demonstrate the applicability  of our approach for synthetic as well as for 2D and 3D tomograph...
1323|Coupling Dense and Landmark-Based Approaches for Non Rigid Registration|In this paper, we investigate the introduction of cortical constraints for non rigid inter-subject brain registration. We extract sulcal patterns with the active ribbon method, presented in [25]. An energy based registration method [21] makes it possible to incorporate the matching of cortical sulci, and express in a unified framework the local sparse similarity and the global &#034;iconic&#034; similarity. We show the objective benefits of cortical constraints on a database of 18 subjects, with global and local measures of the registration&#039;s quality.
1324|Quadratic Interpolation for Image Resampling|Nearest-neighbour, linear, and various cubic interpolation functions are frequently used in image resampling. Quadratic functions have been disregarded, largely because they have been thought to introduce phase distortions. This is shown not to be the case, and a family of quadratic functions is derived. The interpolating member of this family has visual quality close to that of the Catmull-Rom cubic, yet requires only sixty percent of the computation time.
1325|The Distribution of Target Registration Error in Rigid-body, Point-based Registration|Introduction  The point-based registration problem is as follows: given a set of homologous points in two spaces, nd a transformation that brings the points into approximate alignment. In many cases the appropriate transformations are rigid, consisting of translations and rotations. Medical applications abound in neurosurgery, for example, where the head can be treated as a rigid body [1], [2], [3], [4], [5], [6], [7]. The points, which we will call  ducial points, may be anatomical landmarks or may be produced articially by means of attached markers. In the case that we address here, the spaces are three dimensional and may consist, for example, of two MR volumes, a CT volume and an MR volume or PET volume, or, in the case of image-guided neurosurgical applications, an image volume and the physical space of the operating room itself. The rigid-body, point-based image registration problem is typically dened to be the problem of nding the translation vector an
1326|Fast algorithm for point pattern matching: Invariant to translations rotations and scale changes|Abstract--Based on 2-D cluster approach, a fast algorithm for point pattern matching is proposed to effectively solve the problems of optimal matches between two point pattern under geometrical transformation and correctly identify the missing or spurious points of patterns. Theorems and algorithms are developed to determine the matching pairs support of each point pair and its transformation parameters (scaling s and rotation 0) on a two-parameter space (s,O). Experiments are conducted both on real and synthetic data. The experimental results show that the proposed matching algorithm can handle translation, rotation, and scaling differences under noisy or distorted condition. The computational time is just about 0.5 s for 50 to 50 point matching on Sun-4 workstation. Copyright © 1997 Pattern Recognition Society. Published by Elsevier Science Ltd. Point pattern matching Affine transformation Maximum matching pairs support Hough transform Inexact matching Registration 1.
1327|Moment Forms Invariant to Rotation and Blur in Arbitrary Number of Dimensions|We present the construction of combined blur and rotation moment invariants in arbitrary number of dimensions. Moment  invariants to convolution with an arbitrary centrosymmetric filter are derived first, and then their rotationally invariant forms are found by  means of group representation theory to achieve the desired combined invariance. Several examples of the invariants are calculated  explicitly to illustrate the proposed procedure. Their invariance, robustness, and capability of using in template matching and in image  registration are demonstrated on 3D MRI data and 2D indoor images.
1328|Projection-based image registration in the presence of fixed-pattern noise|Abstract—A computationally efficient method for image regis-tration is investigated that can achieve an improved performance over the traditional two-dimensional (2-D) cross-correlation-based techniques in the presence of both fixed-pattern and temporal noise. The method relies on transforming each image in the sequence of frames into two vector projections formed by accu-mulating pixel values along the rows and columns of the image. The vector projections corresponding to successive frames are in turn used to estimate the individual horizontal and vertical components of the shift by means of a one-dimensional (1-D) cross-correlation-based estimator. While gradient-based shift esti-mation techniques are computationally efficient, they often exhibit degraded performance under noisy conditions in comparison to cross-correlators due to the fact that the gradient operation amplifies noise. The projection-based estimator, on the other hand, significantly reduces the computational complexity associated with the 2-D operations involved in traditional correlation-based shift estimators while improving the performance in the presence of temporal and spatial noise. To show the noise rejection capability of the projection-based shift estimator relative to the 2-D cross correlator, a figure-of-merit is developed and computed reflecting the signal-to-noise ratio (SNR) associated with each estimator. The two methods are also compared by means of computer simulation and tests using real image sequences. Index Terms—Fixed-pattern noise suppression, image registra-tion, motion estimation, vector projection. I.
1329|Combined Invariants To Linear Filtering And Rotation|A new class of moment-based features invariant to image rotation, translation, scaling, contrast changes and also to convolution with an unknown PSF are introduced in this paper. These features can be used for recognition of objects captured by a non-ideal imaging system of unknown position and blurring parameters. Keywords: Complex moments, convolution invariants, rotation invariants, combined invariants, invariant basis. 1. Introduction  In scene analysis, we often obtain the input information in a form of an image captured by a non-ideal imaging system. Most real cameras and other sensors can be modelled as a linear space-invariant system, where the relationship between the input f(x; y) and the acquired image g(x; y) is described as  g((x; y)) = a(f  h)(x; y) + n(x; y): (1) In the above model, h(x; y) is the point-spread function (PSF) of the system, n(x; y) is an additive random noise, a is a constant describing the overall change of contrast,  stands for a transform of spatial co...
1330|Automatic Registration of Satellite Images|Image registration is one of the basic image processing operations in remote sensing. With  the increase in the number of images collected every day from different sensors, automated registration  of multi-sensor/multi-spectral images has become an important issue. A wide range of registration techniques  has been developed for many different types of applications and data. Given the diversity of the  data, it is unlikely that a single registration scheme will work satisfactorily for all different applications.
1331|Alignment Using Distributions of Local Geometric Properties|We describe a framework for aligning images without needing to establish explicit feature correspondences. We assume that the geometry between the two images can be adequately described by an affine transformation and develop a framework that uses the statistical distribution of geometric properties of image contours to estimate the relevant transformation parameters. The estimates obtained using the proposed method are robust to illumination conditions, sensor characteristics etc since image contours are relatively invariant to these changes. Moreover, the distributional nature of our method alleviates some of the common problems due to contour fragmentation, occlusion, clutter etc. We provide empirical evidence of the accuracy and robustness of our algorithm. Finally, we demonstrate our method on both real and synthetic images, including multi-sensor image pairs.
1332|Image Registration Using A New Edge-Based Approach|A new edge--based approach for efficient image registration is proposed. The proposed approach applies wavelet transform to extract a number of feature points as the basis for registration. Each selected feature point is an edge point whose edge response is the maximum within a neighborhood. By using a line--fitting model, all the edge directions of the feature points are estimated from the edge outputs of a transformed image. In order to estimate the orientation difference between two partially overlapping images, a so--called &#034;angle histogram&#034; is calculated. From the angle histogram, the rotation angle which can be used to compensate for the difference between two target images can be decided by seeking the angle that corresponds to the maximum peak in the histogram. Based on the rotation angle, an initial matching can be performed. During the real matching process, we check each candidate pair in advance to see if it can possibly become a correct matching pair. Due to this checking,...
1333|Non-rigid Registration by Geometry-Constrained Diffusion|. Assume that only partial knowledge about a non-rigid registration  is given so that certain points, curves, or surfaces in one 3D  image map to certain certain points, curves, or surfaces in another 3D  image. We are facing the aperture problem because along the curves  and surfaces, point correspondences are not given. We will advocate the  viewpoint that the aperture and the 3D interpolation problem may be  solved simultaneously by finding the simplest displacement field. This is  obtained by a geometry-constrained diffusion which yields the simplest  displacement field in a precise sense. The point registration obtained may  be used for growth modelling, shape statistics, or kinematic interpolation.  The algorithm applies to geometrical objects of any dimensionality. We  may thus keep any number of fiducial points, curves, and/or surfaces  fixed while finding the simplest registration. Examples of inferred point  correspondences in a longitudinal growth study of the mandible are g...
1334|Robust image registration by increment sign correlation|A novel and robust statistic as a similarity measure for robust image registration is proposed. The statistic is named as Increment Sign Correlation because it is based on average evaluation of incremental tendency of brightness in adjacent pixels. It is formalized to be a binary distribution or a Gaussian distribution for a large image size through statistical analysis and modeling. By utilizing the proposed statistical model, for example, we can theoretically determine a reasonable value of threshold for verification of matching. This sign correlation also can be proved to expectedly have the constant value 0.5 for any uncorrelated images to a template image, and then the property of the constancy can be utilized to analyze the high robustness for occlusion. The good performance for the case of saturation or highlight can be proved through theoretical analysis and fundamental experiments also. A basic algorithm for image scanning, search and registration over a large scene is represented with a technique for a fast version by the branch-and-bound approach. Many experimental evidences with real images are provided and discussed. Key words: increment sign correlation; image registration; template matching; robust statistics 1
1335|Cumulated Gain-based Evaluation of IR Techniques|Modem large retrieval environments tend to overwhelm their users by their large output. Since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation to the users. In order to develop IR techniques to this direction, it is necessary to develop evaluation approaches and methods that credit IR methods for their ability to retrieve highly relevant documents. This can be done by extending traditional evaluation methods, i.e., recall and precision based on binary relevance assessments, to graded relevance assessments. Alternatively, novel measures based on graded relevance assessments may be developed. This paper proposes three novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. The first one accumulates the relevance scores of retrieved documents along the ranked result list. The second one is similar but applies a discount factor on the relevance scores in order to devaluate late-retrieved documents. The third one computes the relative-tothe -ideal performance of IR techniques, based on the cumulative gain they are able to yield. The novel measures are defined and discussed and then their use is demonstrated in a case study using TREC data - sample system run results for 20 queries in TREC-7. As relevance base we used novel graded relevance assessments on a four-point scale. The test results indicate that the proposed measures credit IR methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. The graphs based on the measures also provide insight into the performance IR techniques and allow interpretation, e.g., from the user point of ...
1336|Using Statistical Testing in the Evaluation of Retrieval Experiments|The standard strategies for evaluation based on precision and recall are examined and their relative advantages and disadvantages are discussed. In particular, it is suggested that relevance feedback be evaluated from the perspective of the user. A number of different statistical tests are described for determining if differences in performance between retrieval methods are significant. These tests have often been ignored in the past because most are based on an assumption of normality which is not strictly valid for the standard performance measures. However, one can test this assumption using simple diagnostic plots, and if it is a poor approximation, there are a number of non-parametric alternatives.  
1337|Overview of the Seventh Text REtrieval Conference TREC-7|This paper serves as an introduction to the research described in detail in the remainder of the volume. It concentrates mainly on the main task, ad hocretrieval, which is de#ned the next section. Details regarding the test collections and evaluation methodology used in TREC followinsections 3 and 4, while section 5 provides an overview of the ad hoc retrieval results. In addition to the main ad hoc task, TREC-7 contained seven #tracks,&#034; tasks that focus research on particular subproblems of text retrieval. Taken together, the tracks represent the bulk of the experiments performed in TREC-7. However, each track has its own overview paper included in the proceedings, so this paper presents only a short summary of each track in section 6. The #nal section looks forward to future TREC conferences
1338|Evaluation by Highly Relevant Documents|Given the size of the web, the search engine industry has argued that engines should be evaluated by their ability to retrieve highly relevant pages rather than all possible relevant pages. To explore the role highly relevant documents play in retrieval system evaluation, assessors for the TREC-9 web track used a three-point relevance scale and also selected best pages for each topic. The relative eectiveness of runs evaluated by dierent relevant document sets differed, con  rming the hypothesis that dierent retrieval techniques work better for retrieving highly relevant documents. Yet evaluating by highly relevant documents can be unstable since there are relatively few highly relevant documents. TREC assessors frequently disagreed in their selection of the best page, and subsequent evaluation by best page across dierent assessors varied widely. The discounted cumulative gain measure introduced by Jarvelin and Kekalainen increases evaluation stability by incorporating all relevance judgments while still giving precedence to highly relevant documents.
1339|Liberal Relevance Criteria of TREC- Counting on Negligible Documents?|Most test collections (like TREC and CLEF) for experimental research in information retrieval apply binary relevance assessments. This paper introduces a four-point relevance scale and reports the findings of a project in which TREC-7 and TREC8 document pools on 38 topics were reassessed. The goal of the reassessment was to build a subcollection of TREC for experiments on highly relevant documents and to learn about the assessment process as well as the characteristics of a multigraded relevance corpus.
1340|Sustainability Indexes |General and the common Law    3- 15
1341|A Study in Information Seeking and Retrieving. I. Background and Methodology|The objectives of the study were to conduct a series of observations and experiments under as real-life a situation as possible related to: (i) user context of questions in information retrieval; (ii) the structure and classification of questions; (iii) cognitive traits and decision making of searchers; and (iv) different searches of the same question. The study is presented in three parts: Part I presents the background ot the study and describes the models, measures, methods, procedures, and statistical analyses used. Part II is devoted to results related to users, questions, and effectiveness measures, and Part III to results related to searchers, searches, and overlap studies. A concluding summary of all results is presented in Part III. introduction Problem, Motivation, Significance Users and their questions are fundamental to all kinds of information systems, and human decisions and humansystem interactions are by far the most important variables in processes dealing with searching for and retrieval of information. These statements are true to the point of being trite. Nevertheless, it is nothing but short of amazing how relatively little knowledge and understanding in a scientific sense we have about these factors. Information retrieval
1342|The Impact of Query Structure and Query Expansion on Retrieval Performance|The effects of query structures and query expansion (QE) on retrieval performance were tested with a best match retrieval system (INQUERY    ). Query structure means the use of operators to express the relations between search keys. Eight different structures were tested, representing weak structures (averages and weighted averages of the weights of the keys) and strong structures (e.g., queries with more elaborated search key relations). QE was based on concepts, which were first selected from a conceptual model, and then expanded by semantic relationships given in the model. The expansion levels were (a) no expansion, (b) a synonym expansion, (c) a narrower concept expansion, (d) an associative concept expansion, and (e) a cumulative expansion of all other expansions. With weak structures and Boolean structured queries, QE was not very effective. The best performance was achieved with one of the strong structures at the largest expansion level.
1343|Evaluating Information Retrieval Systems Under The Challenges Of Interaction And Multidimensional Dynamic Relevance|The Laboratory Model of information retrieval (IR) evaluation has been challenged by pro- gress in research related to relevance and information seeking as well as by the growing need for accounting for interaction in evaluation. Real human users introduce non-binary, subjec- tive and dynamic relevance judgments into IR processes and affect these processes. Therefore the traditional evaluation based on the Laboratory Model is challenged for its (lack of) real- ism. This paper examines the rationale of evaluating the IR algorithms, the status of the tradi- tional evaluation, and the applicability of the proposed novel evaluation methods and meas- ures. It further points out research problems requiring attention for further advances in the area. The Laboratory Model is found limited but still useful for the specific tasks it fulfills in the development of IR algorithms.
1344|Extensions to the STAIRS Study - Empirical Evidence for the Hypothesised Ineffectiveness of Boolean Queries in Large Full-Text Databases|The STAIRS study conducted by Blair and Maron in the mid-80&#039;s is a milestone in the history of IR evaluation. Blair and Maron made strong conclusion about the inadequacy of free-text searching large databases, and their study has been widely referred in the literature to justify the problems of effectiveness in IR systems. However, some critics of the study have plausibly pointed out that the ineffectiveness conclusions were not solidly based on empirical data. This paper introduces a new theoretical and empirical approach to study the problems of high recall searching in large databases and reports the results of a case experiment. The findings verify some of the hypothetical conclusions introduced in the STAIRS study, and expands the picture of falling performance. It is shown that low precision in high recall searching is unavoidable in exact-match Boolean searching since even major concepts are often expressed implicitly in relevant documents. The author suggests that the problem could be reduced in facet-based best-match searching.
1345|Multimodality Image Registration by Maximization of Mutual Information|  A new approach to the problem of multimodality medical image registration is proposed, using a basic concept from information theory, mutual information (MI), or relative entropy, as a new matching criterion. The method presented in this paper applies MI to measure the statistical dependence or information redundancy between the image intensities of corresponding voxels in both images, which is assumed to be maximal if the images are geometrically aligned. Maximization of MI is a very general and powerful criterion, because no assumptions are made regarding the nature of this dependence and no limiting constraints are imposed on the image content of the modalities involved. The accuracy of the MI criterion is validated for rigid body registration of computed tomography (CT), magnetic resonance (MR), and photon emission tomography (PET) images by comparison with the stereotactic registration solution, while robustness is evaluated with respect to implementation issues, such as interpolation and optimization, and image content, including partial overlap and image degradation. Our results demonstrate that subvoxel accuracy with respect to the stereotactic reference solution can be achieved completely automatically and without any prior segmentation, feature extraction, or other preprocessing steps which makes this method very well suited for clinical applications.  
1346|Multi-Modal Volume Registration by Maximization of Mutual Information|A new information-theoretic approach is presented for finding the registration of volumetric medical images of differing modalities. Registration is achieved by adjustment of the relative pose until the mutual information between images is maximized. In our derivation of the registration procedure, few assumptions are made about the nature of the imaging process. As a result the algorithms are quite general and can foreseeably be used with a wide variety of imaging devices.  This approach works directly with raw images; no preprocessing or feature detection is required. As opposed to feature-based techniques, all of the information in the scan is used to evaluate the registration. This technique is however more flexible and robust than other intensity based techniques like correlation. Additionally, it has an efficient implementation that is based on stochastic approximation.  Experiments are presented that demonstrate the approach registering magnetic resonance (MR) images with comput...
1347|Voxel similarity measures for automated image registration|We present the concept of the feature space sequence: 2D distributions of voxel features of two images generated at registra-tion and a sequence of misregistrations. We provide an explanation of the structure seen in these images. Feature space sequences have been generated for a pair of MR image volumes identical apart from the addition of Gaussian noise to one, MR image volumes with and without Gadolinium enhancement, MR and PET-FDG image volumes and MR and CT image vol-umes, all of the head. The structure seen in the feature space sequences was used to devise two new measures of similarity which in turn were used to produce plots of cost versus misregistration for the 6 degrees of freedom of rigid body motion. One of these, the third order moment of the feature space histogram, was used to register the MR image volumes with and without Gadolinium enhancement. These techniques have the potential for registration accuracy to within a small fraction of a voxel or resolution element and therefore interpolation errors in image transformation can be the dominant source of error in subtracted images. We present a method for removing these errors using sinc interpolation and show how interpolation errors can be reduced by over two orders of magnitude. 1.
1348|An Error Metric for Binary Images|This paper introduces a new error metric for binary images, defined as the p-th order mean difference between thresholded distance transforms of the two images. This has a theoretical justification related to topological ideas in mathematical morphology [14, 27, 39] and random set theory [24, 29, 45, 50]. It also has some intuitive interpretations. Theoretical development of the metric will be presented in a separate paper [2] (an earlier unsuccessful attempt was in [3]). The theory is also applicable to grey-level images, but here we describe only the implementation for binary images and compare it with standard measures such as the misclassification error rate and Pratt&#039;s [1, 31] figure of merit. 1
1349|Comparison of feature-based matching of CT and MR brain images | Geometrical image features like edges and ridges in digital images may be extracted by convolving the images with appropriate derivatives of Gaussians. The choice of the convolution operator and of the parameters of the Gaussian involved defines a specific feature image. In this paper, various feature images derived from CT and MR brain images are defined and tested for usability and robustness in a correlation-based two and three dimensional matching algorithm. A number of these feature images is shown to furnish accurate matching results. The best results are obtained using gradient magnitude edgeness images.
1350|An introduction to variable and feature selection|Variable and feature selection have become the focus of much research in areas of application  for which datasets with tens or hundreds of thousands of variables are available.
1351|Gene selection for cancer classification using support vector machines |Abstract. DNA micro-arrays now permit scientists to screen thousands of genes simultaneously and determine whether those genes are active, hyperactive or silent in normal or cancerous tissue. Because these new micro-array devices generate bewildering amounts of raw data, new analytical methods must be developed to sort out whether cancer tissues have distinctive signatures of gene expression over normal tissues or other types of cancer tissues. In this paper, we address the problem of selection of a small subset of genes from broad patterns of gene expression data, recorded on DNA micro-arrays. Using available training examples from cancer and normal patients, we build a classifier suitable for genetic diagnosis, as well as drug discovery. Previous attempts to address this problem select genes with correlation techniques. We propose a new method of gene selection utilizing Support Vector Machine methods based on Recursive Feature Elimination (RFE). We demonstrate experimentally that the genes selected by our techniques yield better classification performance and are biologically relevant to cancer. In contrast with the baseline method, our method eliminates gene redundancy automatically and yields better and more compact gene subsets. In patients with leukemia our method discovered 2 genes that yield zero leaveone-out error, while 64 genes are necessary for the baseline method to get the best result (one leave-one-out error). In the colon cancer database, using only 4 genes our method is 98 % accurate, while the baseline method is only 86 % accurate.
1352|Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms|This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These tests are compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for the difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, McNemar’s test, is shown to have low type I error. The fifth test is a new test, 5 × 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 5×2 cv test is shown to be slightly more powerful than McNemar’s test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, Mc-Nemar’s test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5×2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set.  
1353|Distributional Clustering Of English Words|We describe and evaluate experimentally a method for clustering words according to their dis- tribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the an- nealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchi- cal &#034;soft&#034; clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.
1354|Selection of relevant features and examples in machine learning|In this survey, we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information. We focus on two key issues: the problem of selecting relevant features, and the problem of selecting relevant examples. We describe the advances that have been made on these topics in both empirical and theoretical work in machine learning, and we present a general framework that we use to compare different methods. We close with some challenges for future work in this area. 
1355|Support Vector Machine Classification and Validation of Cancer Tissue Samples Using Microarray Expression Data|Motivation: DNA microarray experiments generating thousands of gene expression measurements, are being used to gather information from tissue and cell samples regarding gene expression differences that will be useful in diagnosing disease. We have developed a new method to analyse this kind of data using support vector machines (SVMs). This analysis consists of both classification of the tissue samples, and an exploration of the data for mis-labeled or questionable tissue results.  Results: We demonstrate the method in detail on samples consisting of ovarian cancer tissues, normal ovarian tissues, and other normal tissues. The dataset consists of expression experiment results for 97 802 cDNAs for each tissue. As a result of computational analysis, a tissue sample is discovered and confirmed to be wrongly labeled. Upon correction of this mistake and the removal of an outlier, perfect classification of tissues is achieved, but not with high confidence. We identify and analyse a subset of genes from the ovarian dataset whose expression is highly differentiated between the types of tissues. To show robustness of the SVM method, two previously published datasets from other types of tissues or cells are analysed. The results are comparable to those previously obtained. We show that other machine learning methods also perform comparably to the SVM on many of those datasets.  Availability: The SVM software is available at http:// www. cs.columbia.edu/#bgrundy/svm.  Contact: booch@cse.ucsc.edu  
1356|The information bottleneck method|We define the relevant information in a signal x ? X as being the information that this signal provides about another signal y ? Y. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize this problem as that of finding a short code for X that preserves the maximum information about Y. That is, we squeeze the information that X provides about Y through a ‘bottleneck ’ formed by a limited set of codewords ˜X. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, ˜x) emerges from the joint statistics of X and Y. This approach yields an exact set of self consistent equations for the coding rules X ?  ˜ X and ˜ X ? Y. Solutions to these equations can be found by a convergent re–estimation method that generalizes the Blahut–Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.  
1357|Optimal Brain Damage|We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.
1358|An extensive empirical study of feature selection metrics for text classification|Machine learning for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and more accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g. Information Gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from Reuters, TREC, OHSUMED, etc. The results are analyzed from multiple goal perspectives—accuracy, F-measure, precision, and recall—since each is appropriate in different situations. The results reveal that a new feature selection metric we call ‘Bi-Normal Separation ’ (BNS), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspective, BNS was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Information Gain and Chi-Squared have correlated failures, and so they work poorly together. When choosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a member of the pair—e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin.
1359|Toward optimal feature selection|In this paper, we examine a method for feature subset selection based on Information Theory. Initially, a framework for de ning the theoretically optimal, but computationally intractable, method for feature subset selection is presented. We show that our goal should be to eliminate a feature if it gives us little or no additional information beyond that subsumed by the remaining features. In particular, this will be the case for both irrelevant and redundant features. We then give an e cient algorithm for feature selection which computes an approximation to the optimal feature selection criterion. The conditions under which the approximate algorithm is successful are examined. Empirical results are given on a number of data sets, showing that the algorithm e ectively handles datasets with a very large number of features.
1360|Feature selection for SVMs|We introduce a method of feature selection for Support Vector Machines. The method is based upon finding those features which minimize bounds on the leave-one-out error. This search can be efficiently performed via gradient descent. The resulting algorithms are shown to be superior to some standard feature selection algorithms on both toy data and real-life problems of face recognition, pedestrian detection and analyzing DNA microarray data. 1
1361|Inference for the Generalization Error|In order to compare learning algorithms, experimental results reported in the machine learning literature often use statistical tests of signicance to support the claim that a new learning algorithm generalizes better. Such tests should take into account the variability due to the choice of training set and not only that due to the test examples, as is often the case. This could lead to gross underestimation of the variance of the cross-validation estimator, and to the wrong conclusion that the new algorithm is signicantly better when it is not. We perform a theoretical investigation of the variance of a cross-validation estimator of the generalization error that takes into account the variability due to the randomness of the training set as well as test examples. Our analysis shows that all the variance estimators that are based only on the results of the cross-validation experiment must be biased. This analysis allows us to propose new estimators of this variance. We show, via simulations, that tests of hypothesis about the generalization error using those new variance estimators have better properties than tests involving variance estimators currently in use and listed in (Dietterich, 1998). In particular, the new tests have correct size and good power. That is, the new tests do not reject the null hypothesis too often when the hypothesis is true, but they tend to frequently reject the null hypothesis when the latter is false.
1362|Use of the Zero-Norm With Linear Models and Kernel Methods|We explore the use of the so-called zero-norm of the parameters of linear models in learning.
1363|A divisive information-theoretic feature clustering algorithm for text classification|High dimensionality of text can be a deterrent in applying complex learners such as Support Vector Machines to the task of text classification. Feature clustering is a powerful alternative to feature selection for reducing the dimensionality of text data. In this paper we propose a new informationtheoretic divisive algorithm for feature/word clustering and apply it to text classification. Existing techniques for such “distributional clustering ” of words are agglomerative in nature and result in (i) sub-optimal word clusters and (ii) high computational cost. In order to explicitly capture the optimality of word clusters in an information theoretic framework, we first derive a global criterion for feature clustering. We then present a fast, divisive algorithm that monotonically decreases this objective function value. We show that our algorithm minimizes the “within-cluster Jensen-Shannon divergence ” while simultaneously maximizing the “between-cluster Jensen-Shannon divergence”. In comparison to the previously proposed agglomerative strategies our divisive algorithm is much faster and achieves comparable or higher classification accuracies. We further show that feature clustering is an effective technique for building smaller class models in hierarchical classification. We present detailed experimental results using Naive Bayes and Support Vector Machines on the 20Newsgroups data set and a 3-level hierarchy of HTML documents collected from the Open Directory project (www.dmoz.org).
1364|Variable Selection Using SVM-based Criteria|We propose new methods to evaluate variable subset relevance with a view to variable selection.
1365|Dimensionality Reduction via Sparse Support Vector Machines|We describe a methodology for performing variable ranking and selection using support  vector machines (SVMs). The method constructs a series of sparse linear SVMs to generate  linear models that can generalize well, and uses a subset of nonzero weighted variables found  by the linear models to produce a final nonlinear model. The method exploits the fact that a  linear SVM (no kernels) with # 1 -norm regularization inherently performs variable selection  as a side-e#ect of minimizing capacity of the SVM model. The distribution of the linear  model weights provides a mechanism for ranking and interpreting the e#ects of variables.
1366|On the Approximability of Minimizing Nonzero Variables Or Unsatisfied Relations in Linear Systems|We investigate the computational complexity of two closely related classes of combinatorial optimization problems for linear systems which arise in various fields such as machine learning, operations research and pattern recognition. In the first class (Min ULR) one wishes, given a possibly infeasible system of linear relations, to find a solution that violates as few relations as possible while satisfying all the others. In the second class (Min RVLS) the linear system is supposed to be feasible and one looks for a solution with as few nonzero variables as possible. For both Min ULR and Min RVLS the four basic types of relational operators =,  , ? and 6= are considered. While Min RVLS with equations was known to be NP-hard  in [27], we established in [2, 5] that Min ULR with equalities and inequalities are NP-hard  even when restricted to homogeneous systems with bipolar coefficients. The latter problems have been shown hard to approximate in [8]. In this paper we determine strong bou...
1367|Grafting: Fast, Incremental Feature Selection by Gradient Descent in Function Space|We present a novel and flexible approach to the problem of feature selection, called grafting.Rather  than considering feature selection as separate from learning, grafting treats the selection of suitable  features as an integral part of learning a predictor in a regularized learning framework. To make  this regularized learning process sufficiently fast for large scale problems, grafting operates in an  incremental iterative fashion, gradually building up a feature set while training a predictor model  using gradient descent. At each iteration, a fast gradient-based heuristic is used to quickly assess  which feature is most likely to improve the existing model, that feature is then added to the model,  and the model is incrementally optimized using gradient descent. The algorithm scales linearly  with the number of data points and at most quadratically with the number of features. Grafting  can be used with a variety of predictor model classes, both linear and non-linear, and can be used  for both classification and regression. Experiments are reported here on a variant of grafting for  classification, using both linear and non-linear models, and using a logistic regression-inspired loss  function. Results on a variety of synthetic and real world data sets are presented. Finally the  relationship between grafting, stagewise additive modelling, and boosting is explored.
1368|Distributional Word Clusters vs. Words for Text Categorization|We study an approach to text categorization that combines distributional clustering of words and a  Support Vector Machine (SVM) classifier. This word-cluster representation is computed using the  recently introduced Information Bottleneck method, which generates a compact and efficient representation  of documents. When combined with the classification power of the SVM, this method  yields high performance in text categorization. This novel combination of SVM with word-cluster  representation is compared with SVM-based categorization using the simpler bag-of-words (BOW)  representation. The comparison is performed over three known datasets. On one of these datasets  (the 20 Newsgroups) the method based on word clusters significantly outperforms the word-based  representation in terms of categorization accuracy or representation efficiency. On the two other sets  (Reuters-21578 and WebKB) the word-based representation slightly outperforms the word-cluster  representation. We investigate the potential reasons for this behavior and relate it to structural  differences between the datasets.
1369|Ranking a Random Feature For Variable And Feature Selection|We describe a feature selection method that can be applied directly to models that are linear with  respect to their parameters, and indirectly to others. It is independent of the target machine. It is  closely related to classical statistical hypothesis tests, but it is more intuitive, hence more suitable  for use by engineers who are not statistics experts. Furthermore, some assumptions of classical  tests are relaxed. The method has been used successfully in a number of applications that are  briefly described.
1370|Feature Selection and Dualities in Maximum Entropy Discrimination|We present the maximum entropy discrimination  (MED) formalism as a regularization  approach with information theoretic penalties.  By extending discriminative and large  margin concepts to a probabilistic setting,  MED permits many important generalizations  to SVMs. We introduce feature selection  as a particularly critical augmentation of  the learning machine. MED derivations for  both regression and classification cases are  shown and lead to promising experimental  results. Features are pruned simultaneously  with parameter estimation to generate substantial  improvements with relatively sparse  training data. Furthermore, in the linear  model case, complexity scales linearly with  dimensionality and can remain tractable under  explicit feature expansions of non-linear  kernels. The MED formalism also accommodates  discriminant functions that arise from  generative probability models (log-likelihood  ratios) although feature selection may require  more computational effort and ap...
1371|Adaptive Scaling for Feature Selection in SVMs|This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors defining the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters defining the classifier. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.
1372|On Feature Selection: Learning with Exponentially many Irrelevant Features as Training Examples|We consider feature selection in the &#034;wrapper &#034; model of feature selection. This typically involves an NP-hard optimization problem that is approximated by heuristic search for a &#034;good&#034; feature subset. First considering the idealization where this optimization is performed exactly, we give a rigorous bound for generalization error under feature selection. The search heuristics typically used are then immediately seen as trying to achieve the error given in our bounds, and succeeding to the extent that they succeed in solving the optimization. The bound suggests that, in the presence of many &#034;irrelevant&#034; features, the main source of error in wrapper model feature selection is from &#034;overfitting &#034; hold-out or cross-validation data. This motivates a new algorithm that, again under the idealization of performing search exactly, has sample complexity (and error) that grows  logarithmically in the number of &#034;irrelevant&#034; features -- which means it can tolerate having a number of &#034;irrelevant&#034; f...
1373|A New Metric-Based Approach to Model Selection|We introduce a new approach to model selection that performs better than the standard complexitypenalization and hold-out error estimation techniques in many cases. The basic idea is to exploit the intrinsic metric structure of a hypothesis space, as determined by the natural distribution of unlabeled training patterns, and use this metric as a reference to detect whether the empirical error estimates derived from a small (labeled) training sample can be trusted in the region around an empirically optimal hypothesis. Using simple metric intuitions we develop new geometric strategies for detecting overfitting and performing robust yet responsive model selection in spaces of candidate functions. These new metric-based strategies dramatically outperform previous approaches in experimental studies of classical polynomial curve fitting. Moreover, the technique is simple, efficient, and can be applied to most function learning tasks. The only requirement is access to an auxiliary collection ...
1374|Sufficient Dimensionality Reduction|Dimensionality reduction of empirical co-occurrence data is a fundamental problem in unsupervised learning. It is also a well studied problem in statistics known as the analysis of cross-classified data. One principled approach to this problem is to represent the data in low dimension with minimal loss of (mutual) information contained in the original data. In this paper we introduce an information theoretic nonlinear method for finding such a most informative dimension reduction. In contrast with...
1375|MLPs (mono-layer polynomials and multi-layer perceptrons) for nonlinear modeling. JMLR, 3:1383–1398 (this issue  (2003) |This paper presents a model selection procedure which stresses the importance of the classic polynomial models as tools for evaluating the complexity of a given modeling problem, and for removing non-significant input variables. If the complexity of the problem makes a neural network necessary, the selection among neural candidates can be performed in two phases. In an additive phase, the most important one, candidate neural networks with an increasing number of hidden neurons are trained. The addition of hidden neurons is stopped when the effect of the round-off errors becomes significant, so that, for instance, confidence intervals cannot be accurately estimated. This phase leads to a set of approved candidate networks. In a subsequent subtractive phase, a selection among approved networks is performed using statistical Fisher tests. The series of tests starts from a possibly too large unbiased network (the full network), and ends with the smallest unbiased network whose input variables and hidden neurons all have a significant contribution to the regression estimate. This method was successfully tested against the real-world regression problems proposed at the NIPS2000 Unlabeled Data Supervised Learning Competition; two of them are included here as illustrative examples.
1376|Extensions to Metric-Based Model Selection|Metric-based methods have recently been introduced for model selection and regularization, often  yielding very significant improvements over the alternatives tried (including cross-validation). All  these methods require unlabeled data over which to compare functions and detect gross differences  in behavior away from the training points. We introduce three new extensions of the metric model  selection methods and apply them to feature selection. The first extension takes advantage of the  particular case of time-series data in which the task involves prediction with a horizon h. The idea is  to use at t the h unlabeled examples that precede t for model selection. The second extension takes  advantage of the different error distributions of cross-validation and the metric methods: crossvalidation  tends to have a larger variance and is unbiased. A hybrid combining the two model  selection methods is rarely beaten by any of the two methods. The third extension deals with the  case when unlabeled data is not available at all, using an estimated input density. Experiments are  described to study these extensions in the context of capacity control and feature subset selection.
1377|Convergence rates of the Voting Gibbs classifier, with application to Bayesian feature selection|The Gibbs classifier is a simple approximation  to the Bayesian optimal classifier in  which one samples from the posterior for the  parameter `, and then classifies using the  single classifier indexed by that parameter  vector. In this paper, we study the Voting  Gibbs classifier, which is the extension of this  scheme to the full Monte Carlo setting, in  which N samples are drawn from the posterior  and new inputs are classified by voting  the N resulting classifiers. We show that the  error of Voting Gibbs converges rapidly to the  Bayes optimal rate; in particular the relative  error decays at a rapid O(1=N) rate. We also  discuss the feature selection problem in the  Voting Gibbs context. We show that there is  a choice of prior for Voting Gibbs such that  the algorithm has high tolerance to the presence  of irrelevant features. In particular, the  algorithm has sample complexity that is logarithmic   in the number of irrelevant features.  1. 
1378|Benefitting from the Variables that Variable Selection Discards|In supervised learning variable selection is used to find a subset of the available inputs that  accurately predict the output. This paper shows that some of the variables that variable selection  discards can beneficially be used as extra outputs for inductive transfer. Using discarded input variables  as extra outputs forces the model to learn mappings from the variables that were selected as  inputs to these extra outputs. Inductive transfer makes what is learned by these mappings available  to the model that is being trained on the main output, often resulting in improved performance on  that main output.
1379|Bayesian Input Variable Selection Using Posterior Probabilities and Expected Utilities|We consider the input variable selection in complex Bayesian hierarchical models. Our goal is to  find a model with the smallest number of input variables having statistically or practically at least the  same expected utility as the full model with all the available inputs. A good estimate for the expected  utility can be computed using cross-validation predictive densities. In the case of input selection  and a large number of input combinations, the computation of the cross-validation predictive densities  for each model easily becomes computationally prohibitive. We propose to use the posterior  probabilities obtained via variable dimension MCMC methods to find out potentially useful input  combinations, for which the final model choice and assessment is done using the expected utilities.
1381|Self-efficacy: Toward a unifying theory of behavioral change|The present article presents an integrative theoretical framework to explain and to predict psychological changes achieved by different modes of treatment. This theory states that psychological procedures, whatever their form, alter the level and strength of self-efficacy. It is hypothesized that expectations of personal efficacy determine whether coping behavior will be initiated, how much effort will be expended, and how long it will be sustained in the face of obstacles and aversive experiences. Persistence in activities that are subjectively threatening but in fact relatively safe produces, through experiences of mastery, further enhancement of self-efficacy and corresponding reductions in defensive behavior. In the proposed model, expectations of personal efficacy are derived from four principal sources of information: performance accomplishments, vicarious experience, verbal persuasion, and physiological states. The more dependable the experiential sources, the greater are the changes in perceived selfefficacy. A number of factors are identified as influencing the cognitive processing of efficacy information arising from enactive, vicarious, exhortative, and emotive
1382|Institutions and Organizations |The transformation of society through broad-based
1383|Predicting Internet Network Distance with Coordinates-Based Approaches|In this paper, we propose to use coordinates-based mechanisms in a peer-to-peer architecture to predict Internet network distance (i.e. round-trip propagation and transmission delay) . We study two mechanisms. The first is a previously proposed scheme, called the triangulated heuristic, which is based on relative coordinates that are simply the distances from a host to some special network nodes. We propose the second mechanism, called Global Network Positioning (GNP), which is based on absolute coordinates computed from modeling the Internet as a geometric space. Since end hosts maintain their own coordinates, these approaches allow end hosts to compute their inter-host distances as soon as they discover each other. Moreover coordinates are very efficient in summarizing inter-host distances, making these approaches very scalable. By performing experiments using measured Internet distance data, we show that both coordinates-based schemes are more accurate than the existing state of the art system IDMaps, and the GNP approach achieves the highest accuracy and robustness among them.
1384|A Case for End System Multicast|Abstract — The conventional wisdom has been that IP is the natural protocol layer for implementing multicast related functionality. However, more than a decade after its initial proposal, IP Multicast is still plagued with concerns pertaining to scalability, network management, deployment and support for higher layer functionality such as error, flow and congestion control. In this paper, we explore an alternative architecture that we term End System Multicast, where end systems implement all multicast related functionality including membership management and packet replication. This shifting of multicast support from routers to end systems has the potential to address most problems associated with IP Multicast. However, the key concern is the performance penalty associated with such a model. In particular, End System Multicast introduces duplicate packets on physical links and incurs larger end-to-end delays than IP Multicast. In this paper, we study these performance concerns in the context of the Narada protocol. In Narada, end systems selforganize into an overlay structure using a fully distributed protocol. Further, end systems attempt to optimize the efficiency of the overlay by adapting to network dynamics and by considering application level performance. We present details of Narada and evaluate it using both simulation and Internet experiments. Our results indicate that the performance penalties are low both from the application and the network perspectives. We believe the potential benefits of transferring multicast functionality from end systems to routers significantly outweigh the performance penalty incurred. I.
1385|Topologically-aware overlay construction and server selection|  A number of large-scale distributed Internet applications could potentially benefit from some level of knowledge about the relative proximity between its participating host nodes. For example, the performance of large overlay networks could be improved if the application-level connectivity between the nodes in these networks is congruent with the underlying IP-level topology. Similarly, in the case of replicated web content, client nodes could use topological information in selecting one of multiple available servers. For such applications, one need not find the optimal solution in order to achieve significant practical benefits. Thus, these applications, and presumably others like them, do not require exact topological information and can instead use sufficiently informative hints about the relative positions of Internet hosts. In this paper, we present a binning scheme whereby nodes partition themselves into bins such that nodes that fall within a given bin are relatively close to one another in terms of network latency. Our binning strategy is simple (requiring minimal support from any measurement infrastructure), scalable (requiring no form of global knowledge, each node only needs knowledge of a small number of well-known landmark nodes) and completely distributed (requiring no communication or cooperation between the nodes being binned). We apply this binning strategy to the two applications mentioned above: overlay network construction and server selection. We test our binning strategy and its application using simulation and Internet measurement traces. Our results indicate that the performance of these applications can be significantly improved by even the rather coarse-grained knowledge of topology offered by our binning scheme.
1386|On the constancy of Internet path properties|Abstract — Many Internet protocols and operational procedures use measurements to guide future actions. This is an effective strategy if the quantities being measured exhibit a degree of constancy: that is, in some fundamental sense, they are not changing. In this paper we explore three different notions of constancy: mathematical, operational, and predictive. Using a large measurement dataset gathered from the NIMI infrastructure, we then apply these notions to three Internet path properties: loss, delay, and throughput. Our aim is to provide guidance as to when assumptions of various forms of constancy are sound, versus when they might prove misleading. I.
1387|Application-layer multicast with Delaunay triangulations| Application-layer multicast supports group applications without the need for a network-layer multicast protocol. Here, applications arrange themselves in a logical overlay network and transfer data within the overlay. In this paper, we present an application-layer multicast solution that uses a Delaunay triangulation as an overlay network topology. An advantage of using a Delaunay triangulation is that it allows each application to locally derive next-hop routing information without requiring a routing protocol in the overlay. A disadvantage of using a Delaunay triangulation is that the mapping of the overlay to the network topology at the network and data link layer may be suboptimal. We present a protocol, called Delaunay triangulation (DT protocol), which constructs Delaunay triangulation overlay networks. We present measurement experiments of the DT protocol for overlay networks with up to 10 000 members, that are running on a local PC cluster with 100 Linux PCs. The results show that the protocol stabilizes quickly, e.g., an overlay network with 10 000 nodes can be built in just over 30 s. The traffic measurements indicate that the average overhead of a node is only a few kilobits per second if the overlay network is in a steady state. Results of throughput experiments of multicast transmissions (using TCP unicast connections between neighbors in the overlay network) show an achievable throughput of approximately 15 Mb/s in an overlay with 100 nodes and 2 Mb/s in an overlay with 1000 nodes. 
1388|Locating Nearby Copies of Replicated Internet Servers|In this paper we consider the problem of choosing among a collection of replicated servers, focusing on the question of how to make choices that segregate client/server traffic according to network topology.We explore the cost and effectiveness of a variety of approaches, ranging  from those requiring routing layer support (e.g., anycast) to those that build location databases using application-level probe tools like traceroute. Weuncover a number of tradeoffs between effectiveness, network cost, ease of deployment, and portability across differenttypes of networks. We performed our experiments using a simulation parameterized by a topology collected from  7 survey sites across the United States, exploring a global collection of Network Time Protocol  servers. 
1389|An Architecture for a Global Internet Host Distance Estimation Service|There is an increasing need for Internet hosts to be able to quickly and efficiently learn the distance, in terms of metrics such as latency or bandwidth, between Internet hosts. For example, to select the nearest of multiple equal-content web servers. This paper explores technical issues related to the creation of a public infrastructure service to provide such information. In so doing, we suggests an architecture, called IDMaps, whereby Internet distance information is distributed over the Internet, using IP multicast groups, in the form of a virtual distance map. Systems listening to the groups can estimate the distance between any pair of IP addresses by running a spanning tree algorithm over the received distance map. We also present the results of experiments that give preliminary evidence supporting the architecture. This work thus lays the initial foundation for future work in this new area.
1390|The Stationarity of Internet Path Properties: Routing, Loss, and Throughput|There is much interest in using network measurements for both modeling and operational purposes. In this paper we focus on the fundamental question of the stationarity of such measurements. That is, to what extent are past measurements a good predictor of the future? We used the NIMI infrastructure and a set of public traceroute servers to capture large measurement datasets of three quantities: routing, packet loss, and TCP throughput. We apply statistical tests to attempt to develop sound characterizations of the stationarity of these data sets, and discuss several types of nonstationarity. 1 Introduction In recent years there has been a surge of interest in network measurements. These measurements have deepened our understanding of network behavior and led to more accurate and qualitatively different models of network traffic. Network measurements are also used operationally by various protocols to guide network usage. For instance, RLM [MJV96] and equation-based congestion control...
1391|Access path selection in a relational database management system|ABSTRACT: In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research&#039;Laboratory. 1.
1392|The Gamma database machine project|This paper describes the design of the Gamma database machine and the techniques employed in its implementation. Gamma is a relational database machine currently operating on an Intel iPSC/2 hypercube with 32 processors and 32 disk drives. Gamma employs three key technical ideas which enable the architecture to be scaled to 100s of processors. First, all relations are horizontally partitioned across multiple disk drives enabling relations to be scanned in parallel. Second, novel parallel algorithms based on hashing are used to implement the complex relational operators such as join and aggregate functions. Third, dataflow scheduling techniques are used to coordinate multioperator queries. By using these techniques it is possible to control the execution of very complex queries with minimal coordination- a necessity for configurations involving a very large number of processors. In addition to describing the design of the Gamma software, a thorough performance evaluation of the iPSC/2 hypercube version of Gamma is also presented. In addition to measuring the effect of relation size and indices on the response time for selection, join, aggregation, and update queries, we also analyze the performance of Gamma relative to the number of processors employed when the sizes of the input relations are kept constant (speedup) and when the sizes of the input relations are increased proportionally to the number of processors (scaleup). The speedup results obtained for both selection and join queries are linear; thus, doubling the number of processors
1393|A Performance Evaluation of Four Parallel Join Algorithms in a Shared-Nothing Multiprocessor Environment|The join operator has been a cornerstone of relational database systems since their inception. As such, much time and effort has gone into making joins efficient. With the obvious trend towards multiprocessors, attention has focused on efficiently parallelizing the join operation. In this paper we analyze and compare four parallel join algorithms. Grace and Hybrid hash represent the class of hash-based join methods, Simple hash represents a looping algorithm with hashing, and our last algorithm is the more traditional sort-merge. The Gamma database machine serves as the host for the performance comparison. Gamma’s shared-nothing architecture with commercially available components is becoming increasingly common, both in research and in industry. 1.
1394|Dynamic Query Evaluation Plans|Traditional query optimizers assume accurate knowledge of run-time parameters such as selectivities and resource availability during plan optimization, i.e., at compile-time. In reality, however, this assumption is often not justified. Therefore, the “static ” plans produced by traditional optimizers may not be optimal for many of their actual run-time invocations. Instead, we propose a novel optimization model that assigns the bulk of the optimization effort to compile-time and delays carefully selected optimization decisions until run-time. Our previous work defined the run-time primitives, “dynamic plans ” using “choose-plan” operators, for executing such delayed decisions, but did not solve the problem of constructing dynamic plans at compile-time. The present paper introduces techniques that solve this problem. Experience with a working prototype optimizer demonstrates (i) that the additional optimization and start-up overhead of dynamic plans compared to static plans is dominated by their advantage at run-time, (ii) that dynamic plans are as robust as the “brute-force” remedy of run-time optimization, i.e., dynamic plans maintain their optimality even if parameters change between compile-time and run-time, and (iii) that the start-up overhead of dynamic plans is signifmantly less than the time required for complete optimization at run-time. In other words, our proposed techniques are superior to both techniques considered to-date, namely compile-time optimization into a single static plan as well as run-time optimization. Finally, we believe that the concepts and technology described can be transferred to commercial query optimizers in order to improve the performance of embedded queries with host variables in the query predicate and to adapt to run-time system loads unpredictable at compile-time. 1.
1395|The Design Of Xprs|This paper presents an overview of the techniques we are using to build a DBMS at Berkeley that will simultaneously provide high performance and high availability in transaction processing environments, in applications with complex ad-hoc queries and in applications with large objects such as images or CAD layouts. We plan to achieve these goals using a general purpose DBMS and operating system and a shared memory multiprocessor. The hardware and software tactics which we are using to accomplish these goals are described in this paper and include a novel &#034;fast path&#034; feature, a special purpose concurrency control scheme, a two-dimensional file system, exploitation of parallelism and a novel method to efficiently mirror disks. 
1396|A benchmark of NonStop SQL release 2 demonstrating near-linear speedup and scaleup on large databases|its second release, NonStop SQL transparently and automatically implements parallelism within an SQL statement. This parallelism allows query execution speed to increase almost linearly as processors and discs are added to the system-- speedup. In addition, this parallelism can help jobs restricted to a fIxed &amp;quot;batch window&amp;quot;. When the job doubles in size, its elapsed processing time will not change ifproportionately more equipment is available to process the job-- scaleup. This paper describes the parallelism features of NonStop SQL and an audited benchmark that demonstrates these speedup and scaleup claims.
1397|Database resources of the National Center for Biotechnology Information|In addition to maintaining the GenBankÒ nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI’s Web site. NCBI resources include Entrez,
1398|The swiss-prot protein knowledgebase and its supplement trembl in 2003|The SWISS-PROT protein knowledgebase
1399|A greedy algorithm for aligning DNA sequences|For aligning DNA sequences that differ only by sequencing errors, or by equivalent errors from other sources, a greedy algorithm can be much faster than traditional dynamic programming approaches and yet produce an alignment that is guaranteed to be theoretically optimal. We introduce a new greedy alignment algorithm with particularly good performance and show that it computes the same alignment as does a certain dynamic programming algorithm, while executing over 10 times faster on appropriate data. An implementation of this algorithm is currently used in a program that assembles the UniGene database at the National Center for Biotechnology Information.
1400|Online Mendelian Inheritance in Man (OMIM), a knowledgebase of human genes and genetic disorders  (2002) |human genes and genetic disorders
1401|PatternHunter: faster and more sensitive homology search|Motivation: Genomics and proteomics studies routinely depend on homology searches based on the strategy of finding short seed matches which are then extended. The exploding genomic data growth presents a dilemma for DNA homology search techniques: increasing seed size decreases sensitivity whereas decreasing seed size slows down computation. Results: We present a new homology search algorithm &#034;PatternHunter&#034; that uses a novel seed model for increased sensitivity and new hit-processing techniques for significantly increased speed. At Blast levels of sensitivity, PatternHunter is able to find homologies between sequences as large as human chromosomes, in mere hours on a desktop. Availability: PatternHunter is available at
1402|Pfam: clans, web tools and services|Pfam is a database of protein families that currently contains 7973 entries (release 18.0). A recent development in Pfam has enabled the grouping of related families into clans. Pfam clans are described in detail, together with the new associated web pages. Improvements to the range of Pfam web tools and the first set of Pfam web services that allow programmatic access to the database and associated tools are also presented. Pfam is available on the web in the UK
1403|NCBI GEO: mining tens of millions of expression profiles—database and tools update|tools update
1404|CDD: a Conserved Domain Database for protein classification|TheConservedDomainDatabase (CDD) is the protein classification component of NCBI’s Entrez query and retrieval system. CDD is linked to other Entrez data-bases such as Proteins, Taxonomy and PubMed1, and can be accessed at
1405|SMART 5: domains in the context of genomes and networks|The Simple Modular Architecture Research Tool 10 (SMART) is an online resource
1406|The Zebrafish Information Network: the zebrafish model organism database|model organism database provides expanded support for genotypes and phenotypes
1407|BLAST: improvements for better sequence analysis|Basic local alignment search tool (BLAST) is a sequence similarity search program. The National Center for Biotechnology Information (NCBI) maintains a BLAST server with a home page at
1408|Genome Snapshot: a new resource at the Saccharomyces Genome Database (SGD) presenting an overview of the Saccharomyces cerevisiae genome. Nucleic Acids Res 34: D442–445  (2006) |15Sequencing and annotation of the entire Saccharomyces cerevisiae genome has made it possible to gain a genome-wide perspective on yeast genes and gene products. To make this information available on an ongoing basis, the Saccharomyces 20Genome Database (SGD)
1409|The Mouse Genome Database (MGD): updates and enhancements  (2006) |The Mouse Genome Database (MGD) integrates genetic and genomic data for the mouse in order to facilitate the use of the mouse as a model system for understanding human biology and disease pro-cesses. A core component of the MGD effort is the acquisition and integration of genomic, genetic, functional and phenotypic information about mouse genes and gene products. MGD works within the broader bioinformatics community to define ref-erential and semantic standards to facilitate data exchange between resources including the incorp-oration of information from the biomedical literature. MGD is also a platform for computational assess-ment of integrated biological data with the goal of identifying candidate genes associated with complex phenotypes. MGD is web accessible at
1411|As we may think|use in organizing the vast record of human knowledge. Inspired by his previous work in microfilm mass storage, Bush envisioned an information workstation—the memex—capable of storing, navigating, and annotating an entire library’s worth of information. His idea of push-button linking between documents is commonly held to be the forefather of modern hypertext. However, Bush’s vision lacked several key innovations present in today’s information workstations, including searchable, digital content and rapid information sharing on a network. Bush tells a masterful story of technology’s state of the art in 1945 and of taking its trends to their logical conclusion, revealing how his vision was both guided by and limited by that technology. This document shall explore and summarize Bush the man, the contributions and limitations of his paper “As We May Think, ” and our class discussion thereof. We begin with (1) a biography of Bush and (2) a discussion of his vision for organizing the human record. We discuss in turn Bush’s predictions on technology for (3) information acquisition, mass storage, (4) automation, and (5) information retrieval, and the culmination of those technologies in (6) the memex. We then discuss (7) the limitations of the memex and of Bush’s vision at large. 1. Bush Biography We give a brief introduction of Bush’s life 1, so as to establish his personal background motivating “As We May Think.”
1412|World-wide web: The information universe|The World-Wide Web (W 3) initiative is a practical project to bring a global information universe into existence using available technology. This article describes the aims, data model, and protocols needed to implement the “web”, and compares them with various contemporary systems. The Dream Pick up your pen, mouse or favorite pointing device and press it on a reference in this document- perhaps to the author’s name, or organization, or some related work. Suppose you are directly presented with the background material- other papers, the author’s coordinates, the organization’s address and its entire telephone directory. Suppose each of these documents has the same property of being linked to other original documents all over the world. You would have at your fingertips all you need to know about electronic publishing, high-energy physics or for that matter Asian culture. If you are reading this article on paper, you can only dream, but read on.
1413|Rethinking media richness: Towards a theory of media synchronicity|This paper describes a new theory called a theory of media synchronicity which proposes that a set of five media capabilities are important to group work, and that all tasks are composed of two fundamental communication processes (conveyance and convergence). Communication effectiveness is influenced by matching the media capabilities to the needs of the fundamental communication processes, not aggregate collections of these processes (i.e., tasks) as proposed by media richness theory. The theory also proposes that the relationships between communication processes and media capabilities will vary between established and newly formed groups, and will change over time. 1
1414|Optimal experience in work and leisure|Followed 78 adult workers for 1 week with the experience sampling method. (This method randomly samples self-reports throughout the day.) The main question was whether the quality of experience was more influenced by whether a person was at work or at leisure or more influenced by whether a person was in flow (i.e., in a condition of high challenges and skills). Results showed that all the variables measuring the quality of experience, except for relaxation and motivation, are more affected by flow than by whether the respondent is working or in leisure. Moreover, the great majority of flow experiences are reported when working, not when in leisure. Regardless of the quality of experience, however, respondents are more motivated in leisure than in work. But individuals more motivated in flow than in apathy reported more positive experiences in work. Results suggest im-plications for improving the quality of everyday life. Leisure researchers usually choose one of three different ways of denning the phenomenon they are studying. Leisure is seen as discretionary time left free from obligations (Brightbill, 1960; Kelly, 1982), the pursuit of freely chosen recreational ac-tivities (Dumazedier, 1974; Roberts, 1981), or time spent in ac-
1415|The World-Wide Web Initiative|The World-Wide Web (W3) is a way of viewing all the online information available on the Internet as a seamless, browsable continuum. Using hypertext jumps and searches, the user navigates through an information world partly hand-authored, partly computer -generated from existing databases and information systems. The web today incorporates all information from more basic information systems such as Gopher as WAIS, as well as sophisticated multimedia and hypertext information from many organizations. As a user interface tool, W3 clients provides a comprehensive point-and-click network access tools, while W3 servers provide an efficient, friendly method of providing data to real users. This paper answers some commonly asked questions about W3, such as those comparing it with other systems, and about recent developments and future directions global hypermedia will take.  I. What is the World-Wide Web?  If you haven&#039;t come across the web before, the best way to find out about it is to try ...
1416|Working Knowledge|While knowledge is viewed by many as an asset, it is often difficult  to locate particular items within a large electronic corpus. This paper presents an  agent based framework for the location of resources to resolve a specific query,  and considers the associated design issue. Aspects of the work presented complements  current research into both expertise finders and recommender systems. The  essential issues for the proposed design are scalability, together with the ability  to learn and adapt to changing resources. As knowledge is often implicit within  electronic resources, and therefore difficult to locate, we have proposed the use  of ontologies, to extract the semantics and infer meaning to obtain the results required.
1417|OntoSeek: Content-Based Access to the Web|this article, we discuss the special characteristics of online yellow pages and product catalogs, examine linguistic ontologies&#039; role in content matching,  and present OntoSeek&#039;s architecture.  Understanding yellow pages  and product catalogs  Online yellow pages locate suppliers  based on a generic natural-language (NL)  description of their products and services;  product catalogs let users select a specific  product or service offered by a certain supplier.  These repositories&#039; peculiarities, with  respect to generic Web documents, can be  roughly characterized by four parameters  (see Table 1 for their estimated values):  . vocabulary size: number of concepts necessary  to formalize all descriptions in the repository;  . description complexity: average number of concepts for one description;  . description heterogeneity: average number of semantic relations in a description with respect to the t
1418|Just Talk to Me: A Field Study of Expertise Location|Everyday, people in organizations must solve their problems to get their work accomplished. To do so, they often must find others with knowledge and information. Systems that assist users with finding such expertise are increasingly interesting to organizations and scientific communities. But, as we begin to design and construct such systems, it is important to determine what we are attempting to augment. Accordingly, we conducted a five-month field study of a medium-sized software firm. We found the participants use complex, iterative behaviors to minimize the number of possible expertise sources, while at the same time, provide a high possibility of garnering the necessary expertise. We briefly consider the design implications of the identification, selection, and escalation behaviors found during our field study.  Keywords  Expertise networks, knowledge networks, computermediated communications, expert locators, expertise location, expertise finding, information seeking, CSCW, compu...
1419|Agent Amplified Communication|We propose an agent-based framework for assisting and simplifying person-to-person communication for information gathering tasks. As an example, we focus on locating experts for any specified topic. In our approach, the informal person-to-person networks that exist within an organization are used to &#034;referral chain&#034; requests for expertise. User-agents help automate this process. The agents generate referrals by analyzing records of email communication patterns. Simulation results show that the higher responsiveness of an agent-based system can be effectively traded for the higher accuracy of a completely manual approach. Furthermore, preliminary experience with a group of users on a prototype system has shown that useful automatic referrals can be found in practice. Our experience with actual users has also shown that privacy concerns are central to the successful deployment of personal agents: an advanced agent-based system will therefore need to reason about issues involving trust an...
1420|Leveraging Corporate Skill Knowledge -- From ProPer to OntoProPer|Skill management systems serve as technical platforms for mostly, though not exclusively,  corporate-internal market places for skills and know-how. The systems are typically built on  top of a database that contains profiles of employees and applicants. Thus, the skills may be  retrieved through database queries. However, these approaches incur two major problems,  viz. the finding of approximate matches and the maintenance of skill data. In this paper we  describe two systems that leverage corporate skill knowledge by offering advanced means for  both. We present ProPer that uses means from decision theory to allow for compensate skill  matching. Then, we describe OntoProPer that combines these methods with intelligent means  for inferencing of skill data. For the latter an ontology provides background knowledge, i.e.
1421|Ontocopi: Methods and Tools for Identifying Communities of Practice|The paper describes ONTOCOPI, a tool for identifying communities of practice (COPs) by analysing ontologies of the relevant working domain. COP identification is currently a resource-heavy process largely based on interviews. ONTOCOPI attempts to uncover informal COP relations by spotting patterns in the formal relations represented in ontologies, traversing the ontology from instance to instance via selected relations. Experiments to determine particular COPs from an academic ontology are described, showing how the alteration of threshold and temporal settings, and the weights applied to the ontology&#039;s relations affect the composition of the identified COP.
1422|The Role of Artificial Intelligence Technologies in the Implementation of People-Finder Knowledge Management Systems|The development of Knowledge Management Systems  (KMS) demands that knowledge be obtained, shared, and  regulated by individuals and knowledge-sharing  organizational systems such as Knowledge Repositories.
1423|myPlanet: an ontology-driven Web-based personalised news service|In this paper we present myPlanet, an ontologydriven personalised Web-based service. We extended the existing infrastructure of the PlanetOnto news publishing system. Our concerns were mainly to provide lightweight means for ontology maintenance and ease the access to repositories of news items, a rich resource for information sharing. We reason about the information being shared by providing an ontology-driven interest-profiling tool which enable users to specify their interests. We also developed ontology-driven heuristics to find news items related to users ’ interests. This paper argues for the role of ontology-driven personalised Web-based services in information sharing.
1424|Facilitating the Online Search of Experts at NASA Using Expert Seeker People-Finder|People-Finder Systems are knowledge repositories that attempt to manage knowledge by holding pointers to experts who possess specific knowledge within an organization. This paper presents insights from the development of Expert Seeker, an organizational People-Finder KMS that will be used to locate experts at the National Aeronautics and Space Administration (NASA). This paper discusses insights and lessons learned from the development of this system, and the role of technology in automating the maintenance of the expert&#039;s profiles. Expert Seeker represents an important first step towards achieving our objective of automatically and intuitively discovering and identifying intellectual capital within the organization. While several systems in place today rely on self-assessment, we look at the potential of artificial intelligence (AI) technologies, in particular, data mining and clustering techniques, to uncover and map organizational expertise.  1 
1425|A practical guide to support vector classification|The support vector machine (SVM) is a popular classification technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but significant steps. In this guide, we propose a simple procedure which usually gives reasonable results.  
1426|LIBLINEAR: A Library for Large Linear Classification|LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.
1427|  Asymptotic Behaviors of Support Vector Machines with Gaussian Kernel   |Support vector machines (SVMs) with the Gaussian (RBF) kernel have been popular for practical use. Model selection in this class of SVMs involves two hyperparameters: the penalty parameter C and the kernel width s. This paper analyzes the behavior of the SVM classifier when these hyperparameters take very small or very large values. Our results help in a good understanding of the hyperparameter space that leads to an efficient heuristic method of searching for hyperparameter values with small generalization errors. The analysis also indicates that if complete model selection using the Gaussian kernel has been conducted, there is no need to consider linear SVM. 
1429|A Study on Sigmoid Kernels for SVM and the Training of non-PSD Kernels by SMO-type Methods|The sigmoid kernel was quite popular for support vector machines due to its origin from neural networks. However, as the kernel matrix may not be positive semidefinite (PSD), it is not widely used and the behavior is unknown. In this paper, we analyze such non-PSD kernels through the point of view of separability. Based on the investigation of parameters in different ranges, we show that for some parameters, the kernel matrix is conditionally positive definite (CPD), a property which explains its practical viability. Experiments are given to illustrate our analysis. Finally, we discuss how to solve the non-convex dual problems by SMO-type decomposition methods. Suitable modifications for any symmetric non-PSD kernel matrices are proposed with convergence proofs.
1430|From data mining to knowledge discovery in databases|¦ Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field. Across a wide variety of fields, data are
1431|The Merge/Purge Problem for Large Databases|Many commercial organizations routinely gather large numbers of databases for various marketing and business analysis functions. The task is to correlate information from different databases by identifying distinct individuals that appear in a number of different databases typically in an inconsistent and often incorrect fashion. The problem we study here is the task of merging data from multiple sources in as efficient manner as possible, while maximizing the accuracy of the result. We call this the merge/purge problem. In this paper we detail the sorted neighborhood method that is used by some to solve merge/purge and present experimental results that demonstrates this approach may work well in practice but at great expense. An alternative method based upon clustering is also presented with a comparative evaluation to the sorted neighborhood method. We show a means of improving the accuracy of the results based upon a multi-pass approach that succeeds by computing the Transitive Clos...
1432|Unexpectedness as a Measure of Interestingness in Knowledge Discovery|Organizations are taking advantage of &#034;data-mining&#034; techniques to leverage the vast amounts of data captured as they process routine transactions. Data-mining is the process of discovering hidden structure or patterns in data. However several of the pattern discovery methods in datamining systems have the drawbacks that they discover too many obvious or irrelevant patterns and that they do not leverage to a full extent valuable prior domain knowledge that managers have. This research addresses these drawbacks by developing ways to generate interesting patterns by incorporating managers&#039; prior knowledge in the process of searching for patterns in data. Specifically we focus on providing methods that generate unexpected patterns with respect to managerial intuition by eliciting managers&#039; beliefs about the domain and using these beliefs to seed the search for unexpected patterns in data. Our approach should lead to the development of decision support systems that provide managers with mor...
1433|The interestingness of deviations|gps~gte.com, matheus~gte.com One of the moet promising areas in Knowledge Discovery in Databases is the automatic analysis of changes and deviations. Several systems have recently been developed for this task. Suc ~ of these systems hinges on their ability to identify s few important and relevant deviations among the multitude of potentially interesting events:  ~ In this paper we argue that related deviations should be grouped togetherin a finding and that the interestingness of a finding is the estimated benefit from a poesible ~tion connected to it. We discuss methods for determining the estimated benefit from the impact of the deviations and the success probability of an action. Our analysis is done in the context of the Key Findings Reporter (KEFIIt), a system for discovering and explaining ~key findings &#034; in large relational databases, currently being applied to the analysis of healthcare information.
1434|The World Wide Web: quagmire or gold mine?|This article considers the question: is effective Web mining possible?
1435|Discovering Informative Patterns and Data Cleaning|We present a method for discovering informative patterns from data. With this method, large databases can be reduced to only a few representative data entries. Our framework also encompasses methods for cleaning databases containing corrupted data. Both on-line and off-line algorithms are proposed and experimentally checked on databases of handwritten images. The generality of the framework makes it an attractive candidate for new applications in knowledge discovery.  Keywords: knowledge discovery, machine learning, informative patterns, data cleaning, information gain.  4.1 
1436|Selecting Among Rules Induced from a Hurricane Database|can achieve orders of magnitude reduction in the volume of data For example, we applied a commercial tool (IXLtin) to a 1,819 record tropical storm database, yielding 161 rules. However, the human comprehension goals of Knowledge Discovery in Databases may require still more orders of magnitude. We present a rule refinement strategy, partly implemented in a Prolog program, that operationalizes &#034;interestingness &#034; into performance, simplicity, novelty, and significance. Applying the strategy to the induced rulebase yielded 10 &#034;genuinely interesting &#034; rules. I. PURPOSE OF THE STUDY At The Travelers Insurance Company, we are involved in applying statistics and artificial intelligence techniques to the solution of business problems. This work is part of an investigation into applications for Natural Hazards Research Services. The purpose of this study is not to deyelop a hurricane model or predictor. It is, rather, to assess the utility of rule induction technology and our particular rule refinement strategy. The object task of the study is to develop rules that
1437|KDD for Science Data Analysis: Issues and Examples|Tile analysis of tile massive data sets collected by scientific instruments demands automation as a pre-requisite to analysis. There is an urgent need to cre-ate an intermediate level at which scientists can oper-ate effectively; isolating them from the massive sizes and harnessing human analysis capabilities to focus on tasks in which machines do not even renmtely ap-proach humans--namely, creative data analysis, the-ory and hypothesis formation, and drawing insights into underlying phe,mmena. We give an overview of the main issues in the exploitation of scientific data.sets, present five c,~se studies where KDD tools play important and enabling roles, and conclude with fi,ture challenges for data mining and KDD techniques in science data analysis. 1
1438| 	 Active Data Mining   |We introduce an active data mining paradigm that combines the recent work in data mining with the rich literature on active database systems. In this paradigm, data is continuously mined at a desired frequency. As rules are discovered, they are added to a rulebase, and if they already exist, the history of the statistical parameters associated with the rules is updated. When the history starts exhibiting certain trends, specified as shape queries in the user-speci ed triggers, the triggers are red and appropriate actions are initiated. To be able to specify shape queries, we describe the constructs for de ning shapes, and discuss how the shape predicates are used in a query construct to retrieve rules whose histories exhibit the desired trends. We describe how this query capability is integrated into a trigger system to realize an active mining system. The system presented here has been validated using two sets of customer data.
1439|Fast Spatio-Temporal Data Mining of Large Geophysical Datasets|The important scientific challenge of understanding global climate change is one that clearly requires the application of knowledge discovery and datamining techniques on a massive scale. Advances in parallel supercomputing technology, enabling high-resolution modeling, as well as in sensor technology, allowing data capture on an unprecedented scale, conspire to overwhelm present-day analysis approaches. We present here early experiences with a prototype exploratory data analysis environment, CONQUEST, designed to provide content-based access to such massive scientific datasets. CONQUEST (CONtent-based Querying in Space and Time) employs a combination of workstations and massively parallel processors (MPP&#039;s) to mine geophysical datasets possessing a prominent temporal component. It is designed to enable complex multi-modal interactive querying and knowledge discovery, while simultaneously coping with the extraordinary computational demands posed by the scope of the datasets involved. A...
1440|Predicting Equity Returns from Securities Data|Our experiments with capital markets data suggest that the domain can be effectively modeled by classification rules induced from available historical data for the purpose of making gainful predictions for equityinvestments. New classification techniques developed at IBM Research, including minimal rule generation (R-MINI) and contextual feature analysis, seem robust enough for consistently extracting useful information from noisy domains such as financial markets. We will briefly introduce the rationale for our minimal rule generation technique, and the motivation for the use of contextual information in analyzing features. We will then describe our experience from several experiments with the S&amp;P 500 data, illustrating the general methodology, and the results of correlations and simulated managed investment based on classification rules generated by R-MINI. Wewillsketchhow the rules for classifications can be effectively used for numerical prediction, and eventually to an investment ...
1441|Graphical Models for Discovering Knowledge|There are many different ways of representing knowledge, and for each of these ways there are many different discovery algorithms. How can we compare different representations? How can we mix, match and merge representations and algorithms on new problems with their own unique requirements? This chapter introduces probabilistic modeling as a philosophy for addressing these questions and presents graphical models for representing probabilistic models. Probabilistic graphical models are a unified qualitative and quantitative framework for representing and reasoning with probabilities and independencies. 4.1 Introduction Perhaps one common element of the discovery systems described in this and previous books on knowledge discovery is that they are all different. Since the class of discovery problems is a challenging one, we cannot write a single program to address all of knowledge discovery. The KEFIR discovery system applied to health care by Matheus, Piatetsky-Shapiro, and McNeill (199...
1442|An Overview of Issues in Developing Industrial Data Mining and Knowledge Discovery Applications|This paper surveys the growing number of indu5 trial applications of data mining and knowledge discovery. We look at the existing tools, describe some representative applications, and discuss the major issues and problems for building and deploying successful applications and their adoption by business users. Finally, we examine how to assess the potential of a knowledge discovery application. 1
1443|Analyzing the Benefits of Domain Knowledge in Substructure Discovery|Discovering repetitive, interesting, and functional substructures in a structural  database improves the ability to interpret and compress the data. However, scientists  working with a database in their area of expertise often search for a predetermined  type of structure, or for structures exhibiting characteristics specic to the domain.  This paper presents methods for guiding the discovery process with domain-specic  knowledge. In this paper, the Subdue discovery system is used to evaluate the bene-  ts of using domain knowledge. The domain knowledge is incorporated into Subdue  following a single general methodology to guide the discovery process. Results show  using domain-specic knowledge improves the search for substructures which are useful  to the domain, and leads to greater compression of the data. To illustrate these bene-  ts, examples and experiments from the domain of computer programming, computer  aided design circuit, and a series of articially-generated domains...
1444|Planning Algorithms|This book presents a unified treatment of many different kinds of planning algorithms. The subject lies at the crossroads between robotics, control theory, artificial intelligence, algorithms, and computer graphics. The particular subjects covered include motion planning, discrete planning, planning under uncertainty, sensor-based planning, visibility, decision-theoretic planning, game theory, information spaces, reinforcement learning, nonlinear systems, trajectory planning, nonholonomic planning, and kinodynamic planning.
1445|Speeding Up the Convergence of Value Iteration in Partially Observable Markov Decision Processes|Partially observable Markov decision processes (POMDPs) have recently become popular  among many AI researchers because they serve as a natural model for planning under  uncertainty. Value iteration is a well-known algorithm for finding optimal policies for  POMDPs. It typically takes a large number of iterations to converge. This paper proposes  a method for accelerating the convergence of value iteration. The method has been evaluated  on an array of benchmark problems and was found to be very effective: It enabled  value iteration to converge after only a few iterations on all the test problems.  1. Introduction  POMDPs model sequential decision making problems where effects of actions are nondeterministic and the state of the world is not known with certainty. They have attracted many researchers in Operations Research and Artificial Intelligence because of their potential applications in a wide range of areas (Monahan 1982, Cassandra 1998b), one of which is planning under uncertai...
1446|An Improved Grid-Based Approximation Algorithm for POMDPs|Although a partially observable Markov decision  process (POMDP) provides an appealing model for  problems of planning under uncertainty, exact algorithms  for POMDPs are intractable. This motivates  work on approximation algorithms, and grid-based  approximation is a widely-used approach. We describe  a novel approach to grid-based approximation  that uses a variable-resolution regular grid, and  show that it outperforms previous grid-based approaches  to approximation.  1 
1447|Continuous Motion Plans for Robotic Systems with Changing Dynamic Behavior|this paper is to address motion planning for systems in which the dynamic equations describing the evolution of the system change in different regions of the state space. We adopt the control theory point of view and focus on the planning of open loop trajectories that can be used as nominal inputs for control. Systems with changing dynamic behavior are characterized by: (a) equality and inequality constraints that partition the state space into regions (discrete states); and (b) trajectories that are governed by different dynamic equations as the system traverses different regions in the state space. The motion plan therefore consists of the sequence of regions (discrete states) as well as continuous trajectory (evolution of the continuous state) within each of the regions. Since the task may require that the system trajectories and the inputs are sufficiently smooth, we formulate the motion planning problem as an optimal control problem and achieve the smoothness by specifying an appropriate cost function. We present a formal framework for describing systems with changing dynamic behavior borrowing from the literature on hybrid systems. We formulate the optimal control problem for such systems, develop a novel technique for simplifying this problem when the sequence of discrete states is known, and suggest a numerical method for dealing with inequality constraints. The approach is illustrated with two examples. We first consider the coordination between mobile manipulators carrying an object while avoiding obstacles. We show that the obstacle avoidance translates to inequality constraints on the state and the input. In this task no changes in the dynamic equations occur since no physical interaction between the manipulators and the obstacles takes place. In our second...
1448|Probabilistic models of dead-reckoning error in nonholonomic mobile robots|Abstract - In this paper, dead-reckoning error in mobile robots is studied in the context of several different models. These models are derived first in the form of stochastic differential equations (SDEs). Corresponding Fokker-Planck equations are derived, and desired probability density functions (PDFs) of robot pose are computed by using the Fourier transform for SE(2). I.
1449|Stabilization of systems with changing dynamics by means of switching| We present a framework for designing stable control schemes for systems whose dynamics change. The idea is to develop a controller for each of the regions defined by different dynamic characteristics and design a switching scheme that guarantees the stability of the overall system. We derive sufficient conditions for the stability of the switching scheme for systems evolving on a sequence of embedded manifolds. An important feature of the proposed framework is that if the conditions are satisfied by pairs of controllers adjacent in the hierarchy, the overall system will be stable. This makes the application of our results particularly straight forward. The methodology is applied to stabilization of a shimmying wheel, where changes in the dynamic behavior are due to switches between sliding and rolling. 
1450|Bayesian Interpolation|Although Bayesian analysis has been in use since Laplace, the Bayesian method of model--comparison has only recently been developed in depth.  In this paper, the Bayesian approach to regularisation and model--comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other problems.  Regularising constants are set by examining their posterior probability distribution. Alternative regularisers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. `Occam&#039;s razor&#039; is automatically embodied by this framework.  The way in which Bayes infers the values of regularising constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.  1 Data modelling and Occam&#039;s razor In science, a central task is to develop and compare models to a...
1451|A Practical Bayesian Framework for Backprop Networks|A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible: (1) objective comparisons between solutions using alternative network architectures
1452|Information-Based Objective Functions for Active Data Selection|Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed which measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness. 1 Introduction  Theories for data modelling often assume that the data is provided by a source that we do not control. However, there are two scenarios in which we are able to actively select training data. In the first, data measurements are relatively expensive or slow, and we want to know where to look next so as to learn as much as possible. According to Jaynes (1986), Bayesian reasoning was first applied to this problem two centuries ago by Laplace, who in consequence made more important discoveries...
1453|Bounds on the Sample Complexity of Bayesian Learning Using Information Theory and the VC Dimension|In this paper we study a Bayesian or average-case model of concept learning with a twofold goal: to provide more precise characterizations of learning curve (sample complexity) behavior that depend on properties of both the prior distribution over concepts and the sequence of instances seen by the learner, and to smoothly unite in a common framework the popular statistical physics and VC dimension theories of learning curves. To achieve this, we undertake a systematic investigation and comparison of two fundamental quantities in learning and information theory: the probability of an incorrect prediction for an optimal learning algorithm, and the Shannon information gain. This study leads to a new understanding of the sample complexity of learning in several existing models. 1 Introduction  Consider a simple concept learning model in which the learner attempts to infer an unknown  target concept f , chosen from a known concept class F of f0; 1g-valued functions over an instance space X....
1454|From Laplace To Supernova Sn 1987a: Bayesian Inference In Astrophysics|. The Bayesian approach to probability theory is presented as an alternative to the currently used long-run relative frequency approach, which does not offer clear, compelling criteria for the design of statistical methods. Bayesian probability theory offers unique and demonstrably optimal solutions to well-posed statistical problems, and is historically the original approach to statistics. The reasons for earlier rejection of Bayesian methods are discussed, and it is noted that the work of Cox, Jaynes, and others answers earlier objections, giving Bayesian inference a firm logical and mathematical foundation as the correct mathematical language for quantifying uncertainty. The Bayesian approaches to parameter estimation and model comparison are outlined and illustrated by application to a simple problem based on the gaussian distribution. As further illustrations of the Bayesian paradigm, Bayesian solutions to two interesting astrophysical problems are outlined: the measurement of wea...
1455|Bayesian Methods: General Background|: We note the main points of history, as a framework on which to hang many background remarks concerning the nature and motivation of Bayesian/Maximum Entropy methods. Experience has shown that these are needed in order to understand recent work and problems. A more complete account of the history, with many more details and references, is given in Jaynes (1978). The following discussion is essentially nontechnical; the aim is only to convey a little introductory &#034;feel&#034; for our outlook, purpose, and terminology, and to alert newcomers to common pitfalls of misunderstanding. HERODOTUS 2 BERNOULLI 2 BAYES 4 LAPLACE 5 JEFFREYS 6 COX 8 SHANNON 9 COMMUNICATION DIFFICULTIES 10 IS OUR LOGIC OPEN OR CLOSED? 13 DOWNWARD ANALYSIS IN STATISTICAL MECHANICS 14 CURRENT PROBLEMS 15 REFERENCES 17 ?  Presented at the Fourth Annual Workshop on Bayesian/Maximum Entropy Methods, University of Calgary, August 1984. In the Proceedings Volume, Maximum Entropy and Bayesian Methods in Applied Statistics, J. H....
1456|Bayesian Mixture Modeling by Monte Carlo Simulation|. It is shown that Bayesian inference from data modeled by a mixture distribution can feasibly be performed via Monte Carlo simulation. This method exhibits the true Bayesian predictive distribution, implicitly integrating over the entire underlying parameter space. An infinite number of mixture components can be accommodated without difficulty, using a prior distribution for mixing proportions that selects a reasonable subset of components to explain any finite training set. The need to decide on a &#034;correct&#034; number of components is thereby avoided. The feasibility of the method is shown empirically for a simple classification task.  Introduction  Mixture distributions [8, 20] are an appropriate tool for modeling processes whose output is thought to be generated by several different underlying mechanisms, or to come from several different populations. One aim of a mixture model analysis may be to identify and characterize these underlying &#034;latent classes&#034; [2, 7], either for some scient...
1457|Bayesian Analysis. I. Parameter Estimation Using Quadrature NMR Models|. In the analysis of magnetic resonance data, a great deal of prior information is available which is ordinarily not used. For example, considering high resolution NMR spectroscopy, one knows in general terms what functional form the signal will take (e.g., sum of exponentially decaying sinusoids) and that, for quadrature measurements, it will be the same in both channels except for a 90  ffi  phase shift. When prior information is incorporated into the analysis of time domain data, the frequencies, decay rate constants, and amplitudes may be estimated much more precisely than by direct use of discrete Fourier transforms. Here, Bayesian probability theory is used to estimate parameters using quadrature models of NMR data. The calculation results in an interpretation of the quadrature model fitting that allows one to understand on an intuitive level what frequencies and decay rates will be estimated and why. Introduction  Probability theory when interpreted as logic is a quantitative th...
1458|The pyramid match kernel: Discriminative classification with sets of image features|Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences – generally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This “pyramid match” computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches. 
1459|Shape Matching and Object Recognition Using Shape Contexts|We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by (1) solv- ing for correspondences between points on the two shapes, (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape con- texts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; reg- ularized thin plate splines provide a flexible class of transformation maps for this purpose. The dissimilarity between the two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning trans- form. We treat recognition in a nearest-neighbor classification framework as the problem of finding the stored prototype shape that is maximally similar to that in the image. Results are presented for silhouettes, trademarks, handwritten digits and the COIL dataset.
1460|Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories|Abstract — Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum-likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets. I.
1461|The earth mover’s distance as a metric for image retrieval|1 Introduction Multidimensional distributions are often used in computer vision to describe and summarize different features of an image. For example, the one-dimensional distribution of image intensities describes the overall brightness content of a gray-scale image, and a three-dimensional distribution can play a similar role for color images. The texture content of an image can be described by a distribution of local signal energy over frequency. These descriptors can be used in a variety of applications including, for example, image retrieval.
1462|Shape matching and object recognition using low distortion correspondence|We approach recognition in the framework of deformable shape matching, relying on a new algorithm for finding correspondences between feature points. This algorithm sets up correspondence as an integer quadratic programming problem, where the cost function has terms based on similarity of corresponding geometric blur point descriptors as well as the geometric distortion between pairs of corresponding feature points. The algorithm handles outliers, and thus enables matching of exemplars to query images in the presence of occlusion and clutter. Given the correspondences, we estimate an aligning transform, typically a regularized thin plate spline, resulting in a dense correspondence between the two shapes. Object recognition is then handled in a nearest neighbor framework where the distance between exemplar and query is the matching cost between corresponding points. We show results on two datasets. One is the Caltech 101 dataset (Fei-Fei, Fergus and Perona), an extremely challenging dataset with large intraclass variation. Our approach yields a 48 % correct classification rate, compared to Fei-Fei et al’s 16%. We also show results for localizing frontal and profile faces that are comparable to special purpose approaches tuned to faces. 1.
1463|Indexing based on scale invariant interest points|This paper presents a new method for detecting scale invariant interest points. The method is based on two recent results on scale space: 1) Interest points can be adapted to scale and give repeatable results (geometrically stable). 2) Local extrema over scale of normalized derivatives indicate the presence of characteristic local structures. Our method first computes a multi-scale representation for the Harris interest point detector. We then select points at which a local measure (the Laplacian) is maximal over scales. This allows a selection of distinctive points for which the characteristic scale is known. These points are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. For indexing, the image is characterized by a set of scale invariant points; the scale associated with each point allows the computation of a scale invariant descriptor. Our descriptors are, in addition, invariant to image rotation, to affine illumination changes and robust to small perspective deformations. Experimental results for indexing show an excellent performance up to a scale factor of 4 for a database with more than 5000 images. 1
1464|Learning to detect objects in images via a sparse, part-based representation| We study the problem of detecting objects in still, grayscale images. Our primary focus is development of a learning-based approach to the problem, that makes use of a sparse, part-based representation. A vocabulary of distinctive object parts is automatically constructed from a set of sample images of the object class of interest; images are then represented using parts from this vocabulary, together with spatial relations observed among the parts. Based on this representation, a learning algorithm is used to automatically learn to detect instances of the object class in new images. The approach can be applied to any object with distinguishable parts in a relatively fixed spatial configuration; it is evaluated here on difficult sets of real-world images containing side views of cars, and is seen to successfully detect objects in varying conditions amidst background clutter and mild occlusion. In evaluating object detection approaches, several important methodological issues arise that have not been satisfactorily addressed in previous work. A secondary focus of this paper is to highlight these issues and to develop rigorous evaluation standards for the object detection problem. A critical evaluation of our approach under the proposed standards is presented.
1465|Object recognition with features inspired by visual cortex|We introduce a novel set of features for robust object recognition. Each element of this set is a complex feature obtained by combining position- and scale-tolerant edgedetectors over neighboring positions and multiple orientations. Our system’s architecture is motivated by a quantitative model of visual cortex. We show that our approach exhibits excellent recognition performance and outperforms several state-of-the-art systems on a variety of image datasets including many different object categories. We also demonstrate that our system is able to learn from very few examples. The performance of the approach constitutes a suggestive plausibility proof for a class of feedforward models of object recognition in cortex. 1
1466|Recognition with Local Features: The Kernel Recipe|Recent developments in computer vision have shown that local features can provide efficient representations suitable for robust object recognition. Support Vector Machines have been established as powerful learning algorithms with good generalization capabilities. In this paper, we combine these two approaches and propose a general kernel method for recognition with local features. We show that the proposed kernel satisfies the Mercer condition and that it is suitable for many established local feature frameworks. Large-scale recognition results are presented on three different databases, which demonstrate that SVMs with the proposed kernel perform better than standard matching techniques on local features. In addition, experiments on noisy and occluded images show that local feature representations significantly outperform global approaches. 1.
1467|A Survey of Kernels for Structured Data|Kernel methods in general and support vector machines in particular have been successful in various learning tasks on data represented in a single table. Much ‘real-world’ data, however, is structured – it has no natural representation in a single table. Usually, to apply kernel methods to ‘real-world’ data, extensive pre-processing is performed to embed the data into a real vector space and thus in a single table. This survey describes several approaches of defining positive definite kernels on structured instances directly.  
1468|A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications|... In this paper we suggest an alternative procedure  to the Fisher kernel for systematically finding kernel functions that  naturally handle variable length sequence data in multimedia  domains. In particular for domains such as speech and images we  explore the use of kernel functions that take full advantage of well  known probabilistic models such as Gaussian Mixtures and single  full covariance Gaussian models. We derive a kernel distance  based on the Kullback-Leibler (KL) divergence between  generative models. In effect our approach combines the best of  both generative and discriminative methods and replaces the  standard SVM kernels. We perform experiments on speaker  identification/verification and image classification tasks and show  that these new kernels have the best performance in speaker  verification and mostly outperform the Fisher kernel based SVM&#039;s  and the generative classifiers in speaker identification and image  classification.
1469|A kernel between sets of vectors|In various application domains, including image recognition, it is natural to represent each example as a set of vectors. With a base kernel we can implicitly map these vectors to a Hilbert space and fit a Gaussian distribution to the whole set using Kernel PCA. We define our kernel between examples as Bhattacharyya’s measure of affinity between such Gaussians. The resulting kernel is computable in closed form and enjoys many favorable properties, including graceful behavior under transformations, potentially justifying the vector set representation even in cases when more conventional representations also exist. 1.
1470|Learning over Sets using Kernel Principal Angles|We consider the problem of learning with instances defined over a space of sets of vectors. We  derive a new positive definite kernel f (A,B) defined over pairs of matrices A,B based on the concept  of principal angles between two linear subspaces. We show that the principal angles can be  recovered using only inner-products between pairs of column vectors of the input matrices thereby  allowing the original column vectors of A,B to be mapped onto arbitrarily high-dimensional feature  spaces.
1471|SVMs for Histogram-Based Image Classification|Traditional classification approaches generalize poorly on image classification tasks, because of the high dimensionality of the feature space. This paper shows that Support Vector Machines (SVM) can generalize well on difficult image classification problems where the only features are high dimensional histograms. Heavy-tailed RBF kernels of the form K(x;y) = e  \Gammaae  P  i jx  a  i \Gammay  a  i j  b  with a  1 and b  2 are evaluated on the classification of images extracted from the Corel Stock Photo Collection and shown to far outperform traditional polynomial or Gaussian RBF kernels. Moreover, we observed that a simple remapping of the input x i ! x  a  i improves the performance of linear SVMs to such an extend that it makes them, for this problem, a valid alternative to RBF kernels.  
1472|Fast contour matching using approximate earth mover’s distance|Weighted graph matching is a good way to align a pair of shapes represented by a set of descriptive local features; the set of correspondences produced by the minimum cost matching between two shapes ’ features often reveals how similar the shapes are. However, due to the complexity of computing the exact minimum cost matching, previous algorithms could only run efficiently when using a limited number of features per shape, and could not scale to perform retrievals from large databases. We present a contour matching algorithm that quickly computes the minimum weight matching between sets of descriptive local features using a recently introduced low-distortion embedding of the Earth Mover’s Distance (EMD) into a normed space. Given a novel embedded contour, the nearest neighbors in a database of embedded contours are retrieved in sublinear time via approximate nearest neighbors search with Locality-Sensitive Hashing (LSH). We demonstrate our shape matching method on a database of 136,500 images of human figures. Our method achieves a speedup of four orders of magnitude over the exact method, at the cost of only a 4 % reduction in accuracy. 1.
1473|Mercer kernels for object recognition with local features|A new class of kernels for object recognition based on local image feature representations are introduced in this paper. These kernels satisfy the Mercer condition and incorporate multiple types of local features and semilocal constraints between them. Experimental results of SVM classifiers coupled with the proposed kernels are reported on recognition tasks with the COIL-100 database and compared with existing methods. The proposed kernels achieved competitive performance and were robust to changes in object configurations and image degradations. 1.
1474|View-Based 3d Object Recognition With Support Vector Machines|. Support Vector Machines have demonstrated excellent results in pattern recognition tasks and 3D object recognition. In this contribution, we confirm some of the results in 3D object recognition and compare it to other object recognition systems. We use di#erent pixel-level representations to perform the experiments, while we extend the setting to the more challenging and practical case when only a limited number of views of the object are presented during training. We report high correct classification of unseen views, especially considering that no domain knowledge is including into the proposed system. Finally, we suggest an active learning algorithm to reduce further the required number of training views. INTRODUCTION  Humans are able to recognize everyday 3D objects when shown previously only one - or at most a few - views of the object. In contrast, artificial systems must either been shown many views of an object (e.g. [8]) or either a lot of knowledge of object structure must ...
1475|Learning a Discriminative Classifier Using Shape Context Distances|For purpose of object recognition, we learn one discriminative classifier based on one prototype, using shape context distances as the feature vector. From multiple prototypes, the outputs of the classifiers are combined using the method called &#034;error correcting output codes&#034;. The overall classifier is tested on benchmark dataset and is shown to outperform existing methods with far fewer prototypes.
1476|Object categorization with SVM: Kernels for local features|In this paper, we propose to combine an efficient image representation based on local descriptors with a Support Vector Machine classifier in order to perform object categorization. For this purpose, we apply kernels defined on sets of vectors. After testing different combinations of kernel / local descriptors, we have been able to identify a very performant one. 1
1477|Shape Matching and Object Recognition| We approach recognition in the framework of deformable shape matching, relying on a new algorithm for finding correspondences between feature points. This algorithm sets up correspondence as an integer quadratic programming problem, where the cost function has terms based on similarity of corresponding geometric blur point descriptors as well as the geometric distortion between pairs of corresponding feature points. The algorithm handles outliers, and thus enables matching of exemplars to query images in the presence of occlusion and clutter. Given the correspondences, we estimate an aligning transform, typically a regularized thin plate spline, resulting in a dense correspondence between the two shapes. Object recognition is handled in a nearest neighbor framework where the distance between exemplar and query is the matching cost between corresponding points. We show results on two datasets. One is the Caltech 101 dataset (Li, Fergus and Perona), a challenging dataset with large intraclass variation. Our approach yields a 45 % correct classification rate in addition to localization. We also show results for localizing frontal and profile faces that are comparable to special purpose approaches tuned to faces.
1478|Algebraic Set Kernels with Application to Inference Over Local Image Representations|This paper presents a general family of algebraic positive definite similarity functions over spaces of matrices with varying column rank. The columns can represent local regions in an image (whereby images have varying number of local parts), images of an image sequence, motion trajectories in a multibody motion, and so forth. The family of set kernels we derive is based on a group invariant tensor product lifting with parameters that can be naturally tuned to provide a cook-book of sorts covering the possible ”wish lists ” from similarity measures over sets of varying cardinality. We highlight the strengths of our approach by demonstrating the set kernels for visual recognition of pedestrians using local parts representations. 1
1479|Using collaborative filtering to weave an information tapestry|predicated on the belief that information filtering can be more effective when humans are involved in the filtering process. Tapestry was designed to support both content-based filtering and collaborative filtering, which entails people collaborating to help each other perform filtering by recording their reactions to documents they read. The reactions are called annotations; they can be accessed by other people’s filters. Tapestry is intended to handle any incoming stream of electronic documents and serves both as a mail filter and repository; its components are the indexer, document store, annotation store, filterer, little box, remailer, appraiser and reader/browser. Tapestry’s client/server architecture, its various components, and the Tapestry query language are described.
1480|Mining the Network Value of Customers|One of the major applications of data mining is in helping companies determine which potential customers to market to. If the expected pro t from a customer is greater than the cost of marketing to her, the marketing action for that customer is executed. So far, work in this area has considered only the intrinsic value of the customer (i.e, the expected pro t from sales to her). We propose to model also the customer&#039;s network value: the expected pro t from sales to other customers she may inuence to buy, the customers those may inuence, and so on recursively. Instead of viewing a market as a set of independent entities, we view it as a social network and model it as a Markov random eld. We show the advantages of this approach using a social network mined from a collaborative ltering database. Marketing that exploits the network value of customers|also known as viral marketing|can be extremely eective, but is still a black art. Our work can be viewed as a step towards providing a more solid foundation for it, taking advantage of the availability of large relevant databases. Categories and Subject Descriptors H.2.8 [Database Management]: Database Applications| data mining
1481|Application of Dimensionality Reduction in Recommender System -- A Case Study|We investigate the use of dimensionality reduction to improve performance for a new class of data analysis  software called &#034;recommender systems&#034;. Recommender systems apply knowledge discovery techniques to the problem of making product recommendations during a live customer interaction. These systems are achieving widespread success in E-commerce nowadays, especially with the advent of the Internet. The tremendous growth of customers and products poses three key challenges for recommender systems in the E-commerce domain. These are: producing high quality recommendations, performing many recommendations per second for millions of customers and products, and achieving high coverage in the face of data sparsity. One successful recommender system technology is collaborative filtering , which works by matching customer preferences to other customers in making recommendations. Collaborative filtering has been shown to produce high quality recommendations, but the performance degrades with ...
1482|Footprints: History-Rich Tools for Information Foraging|Inspired by Hill and Hollan&#039;s original work [6], we have been developing a theory of interaction history and building tools to apply this theory to navigation in a complex information space. We have built a series of tools --- map, trails, annotations and signposts --- based on a physical-world navigation metaphor. These tools have been in use for over a year. Our user study involved a controlled browse task and showed that users were able to get the same amount of work done with significantly less effort.  Keywords  information navigation, information foraging, interaction history, Web browsing  INTRODUCTION  Digital information has no history. It comes to us devoid of the patina that forms on physical objects as they are used. In the non-digital world we make extensive use of these traces to guide our actions, to make choices, and to find things of importance or interest. We call this area interaction history; that is, the records of the interactions of people and objects. Physical o...
1483|Dependency networks for inference, collaborative filtering, and data visualization |We describe a graphical model for probabilistic relationships|an alternative tothe Bayesian network|called a dependency network. The graph of a dependency network, unlike aBayesian network, is potentially cyclic. The probability component of a dependency network, like aBayesian network, is a set of conditional distributions, one for each nodegiven its parents. We identify several basic properties of this representation and describe a computationally e cient procedure for learning the graph and probability components from data. We describe the application of this representation to probabilistic inference, collaborative ltering (the task of predicting preferences), and the visualization of acausal predictive relationships.
1484|Collaborative filtering with privacy via factor analysis|Collaborative filtering is valuable in e-commerce, and for direct recommendations for music, movies, news etc. But today’s systems use centralized databases and have several disadvantages, including privacy risks. As we move toward ubiquitous computing, there is a great potential for individuals to share all kinds of information about places and things to do, see and buy, but the privacy risks are severe. In this paper we introduce a peer-to-peer protocol for collaborative filtering which protects the privacy of individual data. A second contribution of this paper is a new collaborative filtering algorithm based on factor analysis which appears to be the most accurate method for CF to date. The new algorithm has other advantages in speed and storage over previous algorithms. It is based on a careful probabilistic model of user choice, and on a probabilistically sound approach to dealing with missing data. Our experiments on several test datasets show that the algorithm is more accurate than previously reported methods, and the improvements increase with the sparseness of the dataset. Finally, factor analysis with privacy is applicable to other kinds of statistical analyses of survey or questionaire data scientists (e.g. web surveys or questionaires).
1485|Information Filtering Based on User Behavior Analysis|Information filtering systems have potential power that may provide an efficient means of navigating through large and diverse data space. However, current information filtering technology heavily depends on a user’s active participation for describing the user’s interest to information items, forcing the user to accept extra load to overcome thealready loaded situation. Fumhemo~, because theuser&#039;s interests weoften expressed indiscrete fomat such as a set of keywords sometimes augmented with if-then rules, it is difficult to express ambiguous interests, which users often want to do. We propose a technique that uses user behavior monitonng to transparently capture the user’sinterest in information, andatechnique to use this interest to fikerincoming information in avery efficient way. The proposed techniques are verified to perform very well by having conducted a field experiment and a series of simulation. 1
1486|Using Filtering Agents to Improve Prediction Quality in the GroupLens Research Collaborative Filtering System|Collaborative filtering systems help address information overload by using the opinions of users in a community to make personal recommendations for documents to each user. Many collaborative filtering systems have few user opinions relative to the large number of documents available. This sparsity problem can reduce the utility of the filtering system by reducing the number of documents for which the system can make recommendations and adversely affecting the quality of recommendations.  This paper defines and implements a model for integrating content-based ratings into a collaborative filtering system. The filterbot model allows collaborative filtering systems to address sparsity by tapping the strength of content filtering techniques. We identify and evaluate metrics for assessing the effectiveness of filterbots specifically, and filtering system enhancements in general. Finally, we experimentally validate the filterbot approach by showing that even simple filterbots such as spell ...
1487|Getting to Know You: Learning New User Preferences in Recommender Systems|Recommender systems have become valuable resources for users seeking intelligent ways to search through the enormous volume of information available to them. One crucial unsolved problem for recommender systems is how best to learn about a new user. In this paper we study six techniques that collaborative filtering recommender systems can use to learn about new users. These techniques select a sequence of items for the collaborative filtering system to present to each new user for rating. The techniques include the use of information theory to select the items that will give the most value to the recommender system, aggregate statistics to select the items the user is most likely to have an opinion about, balanced techniques that seek to maximize the expected number of bits learned per presented item, and personalized techniques that predict which items a user will have an opinion about. We study the techniques thru offline experiments with a large preexisting user data set, and thru a live experiment with over 300 users. We show that the choice of learning technique significantly affects the user experience, in both the user effort and the accuracy of the resulting predictions.
1488|An empirical analysis of design choices in neighborhoodbased collaborative filtering algorithms|Abstract. Collaborative filtering systems predict a user’s interest in new items based on the recommendations of other people with similar interests. Instead of performing content indexing or content analysis, collaborative filtering systems rely entirely on interest ratings from members of a participating community. Since predictions are based on human ratings, collaborative filtering systems have the potential to provide filtering based on complex attributes, such as quality, taste, or aesthetics. Many implementations of collaborative filtering apply some variation of the neighborhood-based prediction algorithm. Many variations of similarity metrics, weighting approaches, combination measures, and rating normalization have appeared in each implementation. For these parameters and others, there is no consensus as to which choice of technique is most appropriate for what situations, nor how significant an effect on accuracy each parameter has. Consequently, every person implementing a collaborative filtering system must make hard design choices with little guidance. This article provides a set of recommendations to guide design of neighborhood-based prediction systems, based on the results of an empirical study. We apply an analysis framework that divides the neighborhood-based prediction approach into three components and then examines variants of the key parameters in each component. The three components identified are similarity computation, neighbor selection, and rating combination.
1489|Beyond Algorithms: An HCI Perspective on Recommender Systems|The accuracy of recommendations made by an online Recommender System (RS) is mostly dependent on the underlying collaborative filtering algorithm. However, the ultimate effectiveness of an RS is dependent on factors that go beyond the quality of the algorithm. The goal of an RS is to introduce users to items that might interest them, and convince users to sample those items. What design elements of an RS enable the system to achieve this goal? To answer this question, we examined the quality of recommendations and usability of three book RS (Amazon.com, RatingZone &amp; Sleeper) and three movie RS (Amazon.com, MovieCritic, Reel.com). Our findings indicate that from a user&#039;s perspective, an effective recommender system inspires trust in the system; has system logic that is at least somewhat transparent; points users towards new, not-yet-experienced items; provides details about recommended items, including pictures and community ratings; and finally, provides ways to refine recommendations by including or excluding particular genres. Users expressed willingness to provide more input to the system in return for more effective recommendations. 
1490|Variations in relevance assessments and the measurement of retrieval effectiveness|The purpose of this article is to bring attention to the prob-lem of variations in relevance assessments and the effects that these may have on measures of retrieval effective-ness. Through an analytical review of the literature, I show that despite known wide variations in relevance assess-ments in experimental test collections, their effects on the measurement of retrieval performance are almost com-pletely unstudied. I will further argue that what we know about the many variables that have been found to affect relevance assessments under experimental conditions, as well as our new understanding of psychological, situa-tional, user-based relevance, point to a single conclusion. We can no longer rest the evaluation of information re-trieval systems on the assumption that such variations do not significantly affect the measurement of information re-trieval performance. A series of thorough, rigorous, and extensive tests is needed, of precisely how, and under what conditions, variations in relevance assessments do, and do not, affect measures of retrieval performance. We need to develop approaches to evaluation that are sensi-tive to these variations and to human factors and individual differences more generally. Our approaches to evaluation must reflect the real world of real users.
1491|Let&#039;s stop pushing the envelope and start addressing it: a Reference Task Agenda for HCI|We identify a problem with the process of research in the HCI community -- an overemphasis on &#034;radical invention&#034; at the price of achieving a common research focus. Without such a focus, it is difficult to build on previous work, to compare different interaction techniques objectively, and to make progress in developing theory. These problems at the research level have implications for practice, too; as
1492|Why batch and user evaluations do not give the same results |Much system-oriented evaluation of information retrieval systems has used the Cranfield approach based upon queries run against test collections in a batch mode. Some researchers have questioned whether this approach can be applied to the real world, but little data exists for or against that assertion. We have studied this question in the context of the TREC Interactive Track. Previous results demonstrated that improved performance as measured by relevance-based metrics in batch studies did not correspond with the results of outcomes based on real user searching tasks. The experiments in this paper analyzed those results to determine why this occurred. Our assessment showed that while the queries entered by real users into systems yielding better results in batch studies gave comparable gains in ranking of relevant documents for those users, they did not translate into better performance on specific tasks. This was most likely due to users being able to adequately find and utilize relevant documents ranked further down the output list.
1493|Measuring retrieval effectiveness based on user preference of documents|The notion of user preference is adopted for the representation, interpretation and measurement of the relevance or usefulness of documents. User judgments on documents may be formally described by a weak order (i.e., user ranking) and measured using an ordinal scale. Within this framework, a new measure of system performance is suggested based on the distance between user ranking and system ranking. It only uses the relative order of documents and therefore confirms to the valid use of an ordinal scale measuring relevance. It is also applicable to multi-level relevance judgments and ranked system output. The appropriateness of the proposed measure is demonstrated through an axiomatic approach. The inherent relationships between the new measure and many existing measures provide further supporting evidence. 1 1
1494|Effectiveness of information retrieval methods|Distribution of this document is unlimited. It may be released to the Clearinghouse, Department of Commerce, for sale to the general public.
1495|Better or Just Different? On the Benefits of Designing Interactive Systems in Terms of Critical Parameters|Critical parameters are quantitative measures of performance that may be used to determine the overall ability of a design to serve its purpose. Although critical parameters figure in almost every field of design where there is a demand for progressive improvement, they do not appear to figure significantly in the design of interactive systems. As a result, systems are designed that are recognizably different from other systems but not necessarily better at doing the job intended. This paper discusses the role of critical parameters in design, and illustrates their lack of use in interactive system design by presenting a number of of examples drawn from the HCI literature. It identifies a consequent need for research to establish critical parameters for applications and to build models of the performance of designs against these parameters. Some ideas are presented on how critical parameters might be established for specific applications, and the paper concludes by summarising some of the benefits that might be gained from moving in this direction.
1496|Inferring User Interest|Recommender systems provide personalized suggestions about items that users will find interesting. Typically, recommender systems require a user interface that can determine the interest of a user and use this information to make suggestions. The common solution, explicit ratings, where users tell the system what they think about a piece of information, is well-understood and fairly precise. However, having to stop to enter explicit ratings can alter normal patterns of browsing and reading. A less intrusive method is to use implicit ratings, where a rating is obtained by a method other than obtaining it directly from the user. This research studies the correlation between various implicit ratings and the explicit rating for a single Web page, and the impact of implicit interest indicators on user privacy. We developed a Web browser that records a user&#039;s actions (implicit ratings) and the explicit rating for each page visited. The browser was used by over 70 people that browsed more than 2500 Web pages. We find that the time spent on a page, the amount of scrolling on a page and the combination of time and scrolling has a strong correlation with explicit interest, while individual scrolling methods and mouse-clicks are ineffective in predicting explicit interest.
1497|Evaluating Expertise Recommendations|Finding a person who has the experience to solve a specific precif is an imporKRI application ofr1SSWRI-1W systems to a difficultorcultRA1AWAR pr11Rr Pr11 systems have made attempts to implement solutions to thisprsRDAK but few systems haveunderC12 systematicuser evaluation. This wor descrDWA a systematic evaluation of theExperWAW Recommender (ER), a system thatrtRAKAC1R people who ar likely to haveexper1Ain a specific prcific ER and the or2-1CRICCW11 contextfor which it was designedar descredR toprAA2R a basisfor underDM-2RIC this evaluation.Pral t o conducting the evaluation, a baselineexpereRA2 showed that peoplear rpleRWKSgood at judging coworRD1- experRD when given an appr1RIM2W context. This finding prdingR a way to demonstrKW the effectiveness of ER bycompar-R ER&#039;s per-1MRIMK torRAAMM by coworARIM The evaluation, the design, andrdR2SW ar descrSRI in detail. ThereRDMD suggest that the parDMARIM-W agr with therRAKWAKRIM-W11 made by ER, and that ER significantly outpercant other experrc rperrcan systems when compared using similar metrics.
1498|An Empirical Evaluation of User Interfaces for Topic Management of Web Sites|Topic management is the task of gathering, evaluating, organizing, and sharing a set of web sites for a specific topic. Current web tools do not provide adequate support for this task. We created the TopicShop system to address this need. TopicShop includes (1) a webcrawler that discovers relevant web sites and builds site profiles, and (2) user interfaces for exploring and organizing sites. We conducted an empirical study comparing user performance with TopicShop vs. Yahoo. TopicShop subjects found over 80% more high-quality sites (where quality was determined by independent expert judgements) while browsing only 81% as many sites and completing their task in 89% of the time. The site profile data that TopicShop provides -- in particular, the number of pages on a site and the number of other sites that link to it -- was the key to these results, as users exploited it to identify the most promising sites quickly and easily.  KEYWORDS  information access, information retrieval, informat...
1499|Is Seeing Believing? How Recommender Interfaces Affect Users&#039; Opinions|Recommender systems use people&#039;s opinions about items in an information domain to help people choose other items. These systems have succeeded in domains as diverse as movies, news articles, Web pages, and wines. The psychological literature on conformity suggests that in the course of helping people make choices, these systems probably affect users&#039; opinions of the items. If opinions are influenced by recommendations, they might be less valuable for making recommendations for other users. Further, manipulators who seek to make the system generate artificially high or low recommendations might benefit if their efforts influence users to change the opinions they contribute to the recommender. We study two aspects of recommender system interfaces that may affect users&#039; opinions: the rating scale and the display of predictions at the time users rate items. We find that users rate fairly consistently across rating scales. Users can be manipulated, though, tending to rate toward the prediction the system shows, whether the prediction is accurate or not. However, users can detect systems that manipulate predictions. We discuss how designers of recommender systems might react to these findings.
1500|MovieLens Unplugged: Experiences with a recommender system on four mobile devices|Recommender systems have changed the way people shop online. Recommender systems on wireless mobile devices may have the same impact on the way people shop in stores. There are several important challenges that interface designers must overcome on mobile devices: Providing sufficient value to attract prospective wireless users, handling occasionally connected devices, privacy and security, and surmounting the physical limitations of the devices. We present our experience with the implementation of a wireless movie recommender system on a cell phone browser, an AvantGo channel, a wireless PDA, and a voice-only phone interface. These interfaces help MovieLens users select movies to rent, buy, or see while away from their computer. The results of a nine month field study show that although wireless has still not arrived for the majority of users, mobile recommender systems have the potential to provide value to their users today.
1501|Generative Models for Cold-Start Recommendations|Systems for automatically recommending items (e.g., movies, products, or information) to users are becoming increasingly important in e-commerce applications, digital libraries, and other domains where mass personalization is highly valued. Such recommender systems typically base their suggestions on (1) collaborative data encoding which users like which items, and/or (2) content data describing item features and user demographics. Systems that rely solely on collaborative data fail when operating from a cold start|that is, when recommending items (e.g., rst-run movies) that no member of the community has yet seen. We develop several generative probabilistic models that circumvent the cold-start problem by mixing content data with collaborative data in a sound statistical manner. We evaluate the algorithms using MovieLens movie ratings data, augmented with actor and director information from the Internet Movie Database. We nd that maximum likelihood learning with the expectation maximization (EM) algorithm and variants tends to over t complex models that are initialized randomly. However, by seeding parameters of the complex models with parameters learned in simpler models, we obtain greatly improved performance. We explore both methods that exploit a single type of content data (e.g., actors only) and methods that leverage multiple types of content data (e.g., both actors and directors) simultaneously.
1502|Experiments in social data mining: The topicshop system|Social data mining systems enable people to share opinions and benefit from each other’s experience. They do this by mining and redistributing information from computational records of social activity such as Usenet messages, system usage history, citations, or hyperlinks. Some general questions for evaluating such systems are: (1) is the extracted information valuable? and (2) do interfaces based on the information improve user task performance? We report here on TopicShop, a system that mines information from the structure and content of Web pages and provides an exploratory information workspace interface. We carried out experiments that yielded positive answers to both evaluation questions. First, a number of automatically computable features about Web sites do a good job of predicting expert quality judgments about sites. Second, compared to popular Web search interfaces, the TopicShop interface to this information lets users select significantly more high-quality sites, in less time and with less effort, and to organize the sites they select into personally meaningful collections more quickly and easily. We conclude by discussing how our results may be applied and considering how they touch on general issues concerning quality, expertise, and consensus.
1503|An Examination of Trust Production in Computer-Mediated Exchange|In this paper, we apply principles of trust derived mostly from interpersonal communication and human-computer interaction research to computer-mediated exchange (CME). We define key terms and synthesize relevant literature identifying four sources and seven dimensions of trust. Combining these sources and dimensions, we offer a trust taxonomy enabling trust analysis of exchange partners in CME. To demonstrate the usefulness of the taxonomy, we report on a case study in which we examined trust production methods in three exchange sites and compared the results.
1505|The magical number seven, plus or minus two: Some limits on our capacity for processing information|z Information measurement z Absolute judgments of unidimensional stimuli z Absolute judgments of multidimensional stimuli z Subitizing
1506|Explanation-Based Learning: An Alternative View|Key words: machine learning, concept acquisition, explanation-based learning Abstract. In the last issue of this journal Mitchell, Keller, and Kedar-Cabelli presented a unifying framework for the explanation-based approach to machine learning. While it works well for a number of systems, the framework does not adequately capture certain aspects of the systems under development by the explanation-based learning group at Illinois. The primary inadequacies arise in the treatment of concept operationality, organization of knowledge into schemata, and learning from observation. This paper outlines six specific problems with the previously proposed framework and presents an alternative generalization method to perform explanation-based learning of new concepts.
1508|Mapping explanation-based generalization onto Soar|Explanation-based generalization (EBG) is a powerful approach to concept formation in which a justifiable concept definition is acquired from a single training example and an underlying theory of how the example is an instance of the concept. Soar is an attempt to build a general cognitive architecture combining general learning, problem solving, and memory capabilities. It includes an independently developed learning mechanism, called chunking, that is similar to but not the same as explanation-based generalization. In this article we clarify the relationship between the explanation-based generalization framework and the Soar/chunking combination by showing how the EBG framework maps onto Soar, how several EBG conceptformation tasks are implemented in Soar, and how the Soar approach suggests answers to some of the outstanding issues in explanation-based generalization. I
1510|A universal algorithm for sequential data compression|A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source.
1511|A theory for multiresolution signal decomposition : the wavelet representation|Abstract-Multiresolution representations are very effective for analyzing the information content of images. We study the properties of the operator which approximates a signal at a given resolution. We show that the difference of information between the approximation of a signal at the resolutions 2 ’ + ’ and 2jcan be extracted by decomposing this signal on a wavelet orthonormal basis of L*(R”). In LL(R), a wavelet orthonormal basis is a family of functions (  @ w (2’ ~-n)),,,“jEZt, which is built by dilating and translating a unique function t+r (xl. This decomposition defines an orthogonal multiresolution representation called a wavelet representation. It is computed with a pyramidal algorithm based on convolutions with quadrature mirror lilters. For images, the wavelet representation differentiates several spatial orientations. We study the application of this representation to data compression in image coding, texture discrimination and fractal analysis. Index Terms-Coding, fractals, multiresolution pyramids, quadrature mirror filters, texture discrimination, wavelet transform. I I.
1512|Orthonormal bases of compactly supported wavelets|  Several variations are given on the construction of orthonormal bases of wavelets with compact support. They have, respectively, more symmetry, more regularity, or more vanishing moments for the scaling function than the examples constructed in Daubechies [Comm. Pure Appl. Math., 41 (1988), pp. 909-996].
1513|The Design and Use of Steerable Filters|Oriented filters are useful in many early vision and image processing tasks. One often needs to apply the same filter, rotated to different angles under adaptive control, or wishes to calculate the filter response at various orientations. We present an efficient architecture to synthesize filters of arbitrary orientations from linear combinations of basis filters, allowing one to adaptively &#034;steer&#034; a filter to any orientation, and to determine analytically the filter output as a function of orientation.
1514|Complete discrete 2-D Gabor transforms by neural networks for image analysis and compression|Abstract-A three-layered neural network is described for transforming two-dimensional discrete signals into generalized nonorthogonal 2-D “Gabor ” representations for image analysis, segmentation, and compression. These transforms are conjoint spatiahpectral representations [lo], [15], which provide a complete image description in terms of locally windowed 2-D spectral coordinates embedded within global 2-D spatial coordinates. Because intrinsic redundancies within images are extracted, the resulting image codes can be very compact. However, these conjoint transforms are inherently difficult to compute because t e elementary expansion functions are not orthogonal. One orthogonking approach developed for 1-D signals by Bastiaans [SI, based on biorthonormal expansions, is restricted by constraints on the conjoint sampling rates and invariance of the windowing function, as well as by the fact that the auxiliary orthogonalizing functions are nonlocal infinite series. In the present “neural network ” approach, based
1515|Multifrequency channel decompositions of images and wavelet models|Abstract-In this paper we review recent multichannel models de-veloped in psychophysiology, computer vision, and image processing. In psychophysiology, multichannel models have been particularly suc-cessful in explaining some low-level processing in the visual cortex. The expansion of a function into several frequency channels provides a rep-resentation which is intermediate between a spatial and a Fourier rep-resentation. We describe the mathematical properties of such decom-positions and introduce the wavelet transform. We review the classical multiresolution pyramidal transforms developed in computer vision and show how they relate to the decomposition of an image into a wavelet orthonormal basis. In the last section we discuss the properties of the zero crossings of multifrequency channels. Zero-crossings represen-tations are particularly well adapted for pattern recognition in com-puter vision. I.
1516|Deformable Kernels for Early Vision|Abstract-Early vision algorithms often have a first stage of linear-filtering that ‘extracts ’ from the image information at multiple scales of resolution and multiple orientations. A common difficulty in the design and implementation of such schemes is that one feels compelled to discretize coarsely the space of scales and orientations in order to reduce computation and storage costs. This discretization produces anisotropies due to a loss of translation-, rotation-, and scaling-invariance that makes early vision algorithms less precise and more difficult to design. This need not be so: one can compute and store efficiently the response of families of linear filters defined on a continuum of orientations and scales. A technique is presented that allows 1) computing the best approximation of a given family using linear combinations of a small number of ‘basis ’ functions; 2) describing all finite-dimensional families, i.e., the families of filters for which a finite dimensional representation is possible with no error. The tech-nique is based on singular value decomposition and may be ap-plied to generating filters in arbitrary dimensions and subject to arbitrary deformations; the relevant functional analysis results are reviewed and precise conditions for the decomposition to be feasible are stated. Experimental results are presented that dem-onstrate the applicability of the technique to generating multi-orientation multi-scale 2D edge-detection kernels. The implemen-tation issues are also discussed. Index TermsSteerable filters, wavelets, early vision, mul-tiresolution image analysis, multirate filtering, deformable filters, scale-space I.
1517|Pyramid Methods in Image Processing|: The data structure used to represent image information can be critical to the successful completion of an image processing task. One structure that has attracted considerable attention is the image pyramid This consists of a set of lowpass or bandpass copies of an image, each representing pattern information of a different scale. Here we describe a variety of pyramid methods that we have developed for image data compression, enhancement, analysis and graphics. 1984 RCA Corporation Final manuscript received November 12, 1984 Reprint Re-29-6-5  that can perform most of the routine visual tasks that humans do effortlessly. It is becoming increasingly clear that the format used to represent image data can be as critical in image processing as the algorithms applied to the data. A digital image is initially encoded as an array of pixel intensities, but this raw format i s not suited to most tasks. Alternatively, an image may be represented by its Fourier transform, with operations applied...
1518|Orthogonal pyramid transforms for image coding|We describe a set of pyramid transforms that decompose an image into a set of basis functions that are (a) spatial-frequency tuned, (b) orientation tuned, (c) spatially localized, and (d) self-similar. For computational reasons the set is also (e) orthogonal and lends itself to (f) rapid computation. The systems are derived from concepts in matrix algebra, but are closely connected to decompositions based on quadrature mirror filters. Our computations take place hierarchically, leading to a pyramid representation in which all of the basis functions have the same basic shape, and appear at many scales. By placing the high-pass and low-pass kernels on staggered grids, we can derive odd-tap QMF kernels that are quite compact. We have developed pyramids using separable, quincunx, and hexagonal kernels. Image data compression with the pyramids gives excellent results, both in terms of MSE and
1519|Detecting And Localizing Edges Composed Of Steps, Peaks And Roofs|It is well known that the projection of depth or orientation discontinuities in a physical scene results in im- age intensity edges which are not ideal step edges but are more typically a combination of steps, peak and roof profiles. However most edge detection schemes ignore the composite nature of these edges, resulting in systematic errors in detection and localization. We address the problem of detecting and localizing these edges, while at the same time also solving the problem of false responses in smoothly shaded regions with constant gradient of the image brightness. We show that a class of nonlinear filters, known as quadratic filters, are appropriate for this task, while linear filters are not. A series of performance criteria are derived for characterizing the $NR, localization and multiple responses of these filters in a manner analogous to Canny&#039;s criteria for linear filters. A two-dimensional version of the approach is developed which has the property of being able to represent multiple edges at the same location and determine the orientation of each to any desired precision. This permits junctions to be localized without rounding. Ezperimental results are presented.
1520|In Search of a General Picture Processing Operator|INTRODUCTION  Pictorial pattern recognition systems are often described as consisting of three parts: a preprocessing part, a feature extraction part, and a classification part. The preprocessing is used to enhance or sharpen the image to be processed. This is usually done using linear operations or operations on the gray scale such as thresholding [1-3] The classification part is fairly well understood [4,5]. The feature extractor, on the other hand, is very much dependent upon the actual problem and no general theory has emerged on how to deal with it. Feature extraction procedures so far have been ad hoc, often referred to as &#034;a bag of tricks.&#034;  The present work grew out of an interest in finding a single picture operator that could in parallel perform a number of useful operations and that could work on several levels in a hierarchy. One background to this interest is the feeling that the eyes and brains of humans and animals are likely to have such standard operators, as the micro
1521|Models and issues in data stream systems|In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues. 
1522|Randomized Algorithms|Randomized algorithms, once viewed as a tool in computational number theory, have by now found widespread application. Growth has been fueled by the two major benefits of randomization: simplicity and speed. For many applications a randomized algorithm is the fastest algorithm available, or the simplest, or both. A randomized algorithm is an algorithm that uses random numbers to influence the choices it makes in the course of its computation. Thus its behavior (typically quantified as running time or quality of output) varies from
1523|The space complexity of approximating the frequency moments|The frequency moments of a sequence containing mi elements of type i, for 1 = i = n, are the numbers Fk = ?n i=1 mki. We consider the space complexity of randomized algorithms that approximate the numbers Fk, when the elements of the sequence are given one by one and cannot be stored. Surprisingly, it turns out that the numbers F0, F1 and F2 can be approximated in logarithmic space, whereas the approximation of Fk for k = 6 requires n?(1) space. Applications to data bases are mentioned as well. 
1524|Multiparty Communication Complexity|A given Boolean function has its input distributed among many parties. The aim is to determine which parties to tMk to and what information to exchange with each of them in order to evaluate the function while minimizing the total communication. This paper shows that it is possible to obtain the Boolean answer deterministically with only a polynomial increase in communication with respect to the information lower bound given by the nondeterministic communication complexity of the function.
1525|NiagaraCQ: A Scalable Continuous Query System for Internet Databases|Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system’s performance and scalability. 1.
1526|Eddies: Continuously Adaptive Query Processing|In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.  In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry  during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates prom...
1527|Mining high-speed data streams|Categories and Subject ?????????? ? ?¨?????????????????????????¦???¦??????????¡¤?? ¡  ? ¡????????????????¦¡¤????§?£????
1528|Online Aggregation|Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in postgres. 1 Introduction  Aggregation is an incre...
1529|Efficient Filtering of XML Documents for Selective Dissemination of Information|Information Dissemination applications are gaining increasing  popularity due to dramatic improvements in  communications bandwidth and ubiquity. The sheer  volume of data available necessitates the use of selective  approaches to dissemination in order to avoid overwhelming  users with unnecessaryinformation. Existing  mechanisms for selective dissemination typically rely  on simple keyword matching or &#034;bag of words&#034; information  retrieval techniques. The advent of XML as a  standard for information exchangeand the development  of query languages for XML data enables the development  of more sophisticated filtering mechanisms that  take structure information into account. We have developed  several index organizations and search algorithms  for performing efficient filtering of XML documents for  large-scale information dissemination systems. In this  paper we describe these techniques and examine their  performance across a range of document, workload, and  scale scenarios.  1 
1530|External Memory Algorithms and Data Structures|Data sets in large applications are often too massive to fit completely inside the computer&#039;s internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this paper, we survey the state of the art in the design and analysis of external memory algorithms and data structures (which are sometimes referred to as &#034;EM&#034; or &#034;I/O&#034; or &#034;out-of-core&#034; algorithms and data structures). EM algorithms and data structures are often designed and analyzed using the parallel disk model (PDM). The three machine-independent measures of performance in PDM are the number of I/O operations, the CPU time, and the amount of disk space. PDM allows for multiple disks (or disk arrays) and parallel CPUs, and it can be generalized to handle tertiary storage and hierarchical memory. We discuss several important paradigms for how to solve batched and online problems efficiently in external memory. Programming tools and environments are available for simplifying the programming task. The TPIE system (Transparent Parallel I/O programming Environment) is both easy to use and efficient in terms of execution speed. We report on some experiments using TPIE in the domain of spatial databases. The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.  
1531|Random sampling with a reservoir|We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O(n(1 + log(N/n))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.
1532|Stable Distributions, Pseudorandom Generators, Embeddings and Data Stream Computation|In this paper we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular: ffl we show how to maintain (using only O(log n=ffl  2  ) words of storage) a sketch C(p) of a point p 2 l  n  1 under dynamic updates of its coordinates, such that given sketches C(p) and C(q) one can estimate jp \Gamma qj 1 up to a factor of (1 + ffl) with large probability. This solves the main open problem of [10].  ffl we obtain another sketch function C  0  which maps l  n  1 into a normed space l  m  1 (as opposed to C), such that m = m(n) is much smaller than n; to our knowledge this is the first dimensionality reduction lemma for l 1 norm  ffl we give an explicit embedding of l  n  2 into l  n  O(log n) 1  with distortion (1 + 1=n  \Theta(1)  ) and a nonconstructive embedding of l  n  2 into l  O(n)  1 with distortion  (1 + ffl) such that the embedding can be represented using only O(n log  2  n) bits (as opposed to at least...
1533|Mining time-changing data streams|Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a station-ary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying pro-cesses generating them changed during this time, sometimes radically. Although a number of algorithms have been pro-posed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes ques-tionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.
1534|Continuous Queries over Data Streams|In many recent applications, data may take the form of continuous data streams, rather than finite stored data sets. Several aspects of data management need to be re-considered in the presence of data streams, offering a new research direction for the database community. In this pa-per we focus primarily on the problem of query processing, specifically on how to define and evaluate continuous queries over data streams. We address semantic issues as well as efficiency concerns. Our main contributions are threefold. First, we specify a general and flexible architecture for query processing in the presence of data streams. Second, we use our basic architecture as a tool to clarify alternative semantics and processing techniques for continuous queries. The architecture also captures most previous work on continuous queries and data streams, as
1536|Fjording the Stream: An Architecture for Queries over Streaming Sensor Data|If industry visionaries are correct, our lives will soon be full of sensors, connected together in loose conglomerations via wireless networks, each monitoring and collecting data about the environment at large. These sensors behave very differently from traditional database sources: they have intermittent connectivity, are limited by severe power constraints, and typically sample periodically and push immediately, keeping no record of historical information. These limitations make traditional database systems inappropriate for queries over sensors. We present the Fjords architecture for managing multiple queries over many sensors, and show how it can be used to limit sensor resource demands while maintaining high query throughput. We evaluate our architecture using traces from a network of traffic sensors deployed on Interstate 80 near Berkeley and present performance results that show how query throughput, communication costs, and power consumption are necessarily coupled in sensor environments.
1537|Maintaining Stream Statistics over Sliding Windows (Extended Abstract)  (2002) |Mayur Datar  Aristides Gionis  y  Piotr Indyk  z  Rajeev Motwani  x  Abstract  We consider the problem of maintaining aggregates and statistics over data streams, with respect to the last N data elements seen so far. We refer to this model as the sliding window model. We consider the following basic problem: Given a stream of bits, maintain a count of the number of 1&#039;s in the last N elements seen from the stream. We show that using O(  1  ffl log  2  N) bits of memory, we can estimate the number of 1&#039;s to within a factor of 1 + ffl. We also give a matching lower bound of \Omega\Gamma  1  ffl log  2  N) memory bits for any deterministic or randomized algorithms. We extend our scheme to maintain the sum of the last N positive integers. We provide matching upper and lower bounds for this more general problem as well. We apply our techniques to obtain efficient algorithms for the Lp norms (for p 2 [1; 2]) of vectors under the sliding window model. Using the algorithm for the basic counting problem, one can adapt many other techniques to work for the sliding window model, with a multiplicative overhead of O(  1  ffl log N) in memory and a 1 + ffl factor loss in accuracy. These include maintaining approximate histograms, hash tables, and statistics or aggregates such as sum and averages.
1538|Continuously Adaptive Continuous Queries over Streams|We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive crossquery sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators -- both selections and join state -- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.
1539|Multiple-Query Optimization|Some recently proposed extensions to relational database systems, as well as to deductive database systems, require support for multiple-query processing. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions may expand to more than one actual query that has to be run over the database. It is an interesting problem then to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of the paper is a systematic look at the problem, along with the presentation and analysis of algorithms that can be used for multiple-query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably.
1540|Trajectory Sampling for Direct Traffic Observation|Traffic measurement is a critical component for the control and engineering of communication networks. We argue that traffic measurement should make it possible to obtain the spatial flow of traffic through the domain, i.e., the paths followed by packets between any ingress and egress point of the domain. Most resource allocation and capacity planning tasks can benefit from such information. Also, traffic measurements should be obtained without a routing model and without knowledge of network state. This allows the traffic measurement process to be resilient to network failures and state uncertainty.  We propose a method that allows the direct inference of traffic flows through a domain by observing the trajectories of a subset of all packets traversing the network. The key advantages of the method are that (i) it does not rely on routing state, (ii) its implementation cost is small, and (iii) the measurement reporting traffic is modest and can be controlled precisely. The key idea of the method is to sample packets based on a hash function computed over the packet content. Using the same hash function will yield the same sample set of packets in the entire domain, and enables us to reconstruct packet trajectories.  I. 
1541|  Wavelet-Based Histograms for Selectivity Estimation |Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations. In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data values give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using
1542|An Adaptive Query Execution System for Data Integration|Query processing in data integration occurs over networkbound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the  Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila  to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that the Tukwila architecture extends previous innovations in adaptive execution (such as...
1543|Surfing wavelets on streams: One-pass summaries for approximate aggregate queries|Abstract We present techniques for computing small spacerepresentations of massive data streams. These are inspired by traditional wavelet-based approx-imations that consist of specific linear projections of the underlying data. We present general&amp;quot;sketch &amp;quot; based methods for capturing various linear projections of the data and use them to pro-vide pointwise and rangesum estimation of data streams. These methods use small amounts ofspace and per-item time while streaming through the data, and provide accurate representation asour experiments with real data streams show.
1544|Approximate Query Processing Using Wavelets|Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today’s decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing
1545|Space-Efficient Online Computation of Quantile Summaries|An &amp;epsilon;-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of &amp;epsilon;N . We present a new online...
1546| Approximate Computation of Multidimensional Aggregates of Sparse Data Using Wavelets |Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries. In this paper, we present anovel method that provides approximate answers to high-dimensional OLAP aggregation queries in massive sparse data sets in a time-efficient and space-efficient manner. We construct a compact data cube, which is an approximate and space-efficient representation of the underlying multidimensional array, based upon a multiresolution wavelet decomposition. In the on-line phase, each aggregation query can generally be answered using the compact data cube in one I/O or a small number of I/Os, depending upon the desired accuracy. We present two I/O-efficient algorithms to construct the compact data cube for the important case of sparse high-dimensional arrays, which often arise in practice. The traditional histogram methods are infeasible for the massive high-dimensional data sets in OLAP applications. Previously developed wavelet techniques are efficient only for dense data. Our on-line query processing algorithm is very fast and capable of refining answers as the user demands more accuracy. Experiments on real data show that our method provides significantly more accurate results for typical OLAP aggregation queries than other efficient approximation techniques such as random sampling.
1547|Computing on Data Streams|In this paper we study the space requirement of algorithms that make  only one (or a small number of) pass(es) over the input data. We study such  algorithms under a model of data streams that we introduce here. We give  a number of upper and lower bounds for problems stemming from queryprocessing,  invoking in the process tools from the area of communication  complexity.
1548|Processing Complex Aggregate Queries over Data Streams|Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (i.e., data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed.
1549|Continual Queries for Internet Scale Event-Driven Information Delivery|In this paper we introduce the concept of continual queries, describe the design of a distributed  event-driven continual query system -- OpenCQ, and outline the initial implementation of OpenCQ  on top of the distributed interoperable information mediation system DIOM [21, 19]. Continual  queries are standing queries that monitor update of interest and return results whenever the update  reaches specified thresholds. In OpenCQ, users may specify to the system the information they would  like to monitor (such as the events or the update thresholds they are interested in). Whenever the  information of interest becomes available, the system immediately delivers it to the relevant users;  otherwise, the system continually monitors the arrival of the desired information and pushes it to the  relevant users as it meets the specified update thresholds. In contrast to conventional pull-based data  management systems such as DBMSs and Web search engines, OpenCQ exhibits two important featu...
1550|Join synopses for approximate query answering|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses (join samples) as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known. We also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations. One of our key contributions is a detailed analysis of the error bounds obtained for approximate answers that demonstrates the trade-offs in various methods, as well as the advantages in certain scenarios of a new subsampling method we propose. Our extensive set of experiments on the TPC-D benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper. 1
1551|An efficient cost-driven index selection tool for Microsoft SQL Server|In this paper we describe novel techniques that make it possible to build an industrial-strength tool for automating the choice of indexes in the physical design of a SQL database. The tool takes as input a workload of SQL queries, and suggests a set of suitable indexes. We ensure that the indexes chosen are effective in reducing the cost of the workload by keeping the index selection tool and the query optimizer &amp;quot;in step&amp;quot;. The number of index sets that must be evaluated to find the optimal configuration is very large. We reduce the complexity of this problem using three techniques. First, we remove a large number of spurious indexes from consideration by taking into account both query syntax and cost information. Second, we introduce optimizations that make it possible to cheaply evaluate the “goodness ” of an index set. Third, we describe an iterative approach to handle the complexity arising from multicolumn indexes. The tool has been implemented on Microsoft SQL Server 7.0. We performed extensive experiments over a range of workloads, including TPC-D. The results indicate that the tool is efficient and its choices are close to optimal. 1.
1552|Reductions in Streaming Algorithms, with an Application to Counting Triangles in Graphs |We introduce reductions in the streaming model as a tool in the design of streaming algorithms. We develop
1553|Computing iceberg queries efficiently|Many applications compute aggregate functions...
1554|Data-Streams and Histograms|Histograms have been used widely to capture data distribution, to represent the data by a small number of step functions. Dynamic programming algorithms which provide optimal construction of these histograms exist, albeit running in quadratic time and linear space. In this paper we provide linear time construction of 1 + epsilon approximation of optimal histograms, running in polylogarithmic space. Our results extend to the context of data-streams, and in fact generalize to give 1 + epsilon approximation of several problems in data-streams which require partitioning the index set into intervals. The only assumptions required are that the cost of an interval is monotonic under inclusion (larger interval has larger cost) and that the cost can be computed or approximated in small space. This exhibits a nice class of problems for which we can have near optimal data-stream algorithms.
1555|XJoin: A Reactively-Scheduled Pipelined Join Operator|Wide-area distribution raises significant performance problems for traditional query processing techniques as data access becomes less predictable due to link congestion, load imbalances, and temporary outages. Pipelined query execution is a promising approach to coping with unpredictability in such environments as it allows scheduling to adjust to the arrival properties of the data. We have developed a non-blocking join operator, called XJoin, which has a small memory footprint, allowing many such operators to be active in parallel. XJoin is optimized to produce initial results quickly and can hide intermittent delays in data arrival by reactively scheduling background processing. We show that XJoin is an effective solution for providing fast query responses to users even in the presence of slow and bursty remote sources.  
1556|Rate-Based Query Optimization for Streaming Information Sources|Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences. We present a preliminary validation of our rate-based optimization framework on a prototype XML query engine, though it is generic enough to be used in other database contexts. The results show that rate-based optimization is feasible and can indeed yield correct decisions.
1557|Making Views Self-Maintainable for Data Warehousing|A data warehouse stores materialized views over data from one or more sources in order to provide fast access to the integrated data, regardless of the availability of the data sources. Warehouse views need to be maintained inresponse to changes to the base data in the sources. Except for very simple views, maintaining a warehouse view requires access to data that is not available in the view itself. Hence, to maintain the view, one either has to query the data sources or store auxiliary data in the warehouse. We show that by using key and referential integrity constraints, we often can maintain a select-project-join view without going to the data sources or replicating the base relations in their entirety in the warehouse. We derive a set of auxiliary views such that the warehouse view and the auxiliary views together are self-maintainable|they can be maintained without going to the data sources or replicating all base data. In addition, our technique can be applied to simplify traditional materialized view maintenance by exploiting key and referential integrity constraints. 1
1558|Approximate Medians and other Quantiles in One Pass and with Limited Memory|We present new algorithms for computing approximate quantiles of large datasets in a single pass. The approximation guarantees are explicit, and apply without regard to the value distribution or the arrival distributions of the dataset. The main memory requirements are smaller than those reported earlier by an order of magnitude.  We also discuss methods that couple the approximation algorithms with random sampling to further reduce memory requirements. With sampling, the approximation guarantees are explicit but probabilistic, i.e., they apply with respect to a (user controlled) confidence parameter.  We present the algorithms, their theoretical analysis and simulation results.  1 Introduction  This article studies the problem of computing order statistics  of large sequences of online or disk-resident data using as little main memory as possible. We focus on computing  quantiles, which are elements at specific positions in the sorted order of the input. The OE-quantile, for OE 2 [0; ...
1559|Random Sampling for Histogram Construction: How much is enough?|Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining &#034;How much sampling is enough?&#034; We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a  conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose ...
1560|Sampling From a Moving Window Over Streaming Data|We introduce the problem of sampling from a moving window of recent items from a data stream and develop the &#034;chain-sample&#034; and &#034;priority-sample&#034; algorithms for this problem.
1561|Tracking join and self-join sizes in limited storage|This paper presents algorithms for tracking (approximate) join and self-join sizes in limited storage, in the presence of insertions and deletions to the data set(s). Such algorithms detect changes in join and self-join sizes without an expensive recomputation from the base data, and without the large space overhead required to maintain such sizes exactly. Query optimizers rely on fast, high-quality estimates of join sizes in order to select between various join plans, and estimates of self-join sizes are used to indicate the degree of skew in the data. For self-joins, we considertwo approaches proposed in [Alon, Matias, and Szegedy. The Space Complexity of Approximating the Frequency Moments. JCSS, vol. 58, 1999, p.137-147], which we denote tug-of-war and sample-count. Wepresent fast algorithms for implementing these approaches, and extensions to handle deletions as well as insertions. We also report on the rst experimental study of the two approaches, on a range of synthetic and real-world data sets. Our study shows that tug-of-war provides more accurate estimates for a given storage limit than sample-count, which in turn is far more accurate than a standard sampling-based approach. For example, tug-of-war needed only 4{256 memory words, depending on the data set, in order to estimate the self-join size
1562|Data Cube Approximation and Histograms via Wavelets (Extended Abstract)  (1998) |) Jeffrey Scott Vitter   Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  jsv@cs.duke.edu Min Wang  y Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  minw@cs.duke.edu Bala Iyer  Database Technology Institute IBM Santa Teresa Laboratory P.O. Box 49023 San Jose, CA 95161 USA  balaiyer@vnet.ibm.com Abstract  There has recently been an explosion of interest in the analysis of data in data warehouses in the field of On-Line Analytical Processing (OLAP). Data warehouses can be extremely large, yet obtaining quick answers to queries is important. In many situations, obtaining the exact answer to an OLAP query is prohibitively expensive in terms of time and/or storage space. It can be advantageous to have fast, approximate answers to queries. In this paper, we present an I/O-efficient technique based upon a multiresolution wavelet decomposition that yields an approximate a...
1563|The design and implementation of a sequence database system|This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. There are three distinct contributions made in this paper. (1) We describe the specification of sequence queries using the SEQUIN query language. (2) We quantitatively demonstrate the importance of various storage and optimization techniques by studying their effect on performance. (3) We present a novel nested design paradigm used in PREDATOR to combine sequence ‘and relational data.
1564|Random sampling techniques for space efficient online computation of order statistics of large datasets|In a recent paper [MRL98], we had described a general framework for single pass approximate quantile nding algorithms. This framework included several known algorithms as special cases. We had identi ed a new algorithm, within the framework, which had a signi cantly smaller requirement for main memory than other known algorithms. In this paper, we address two issues left open in our earlier paper. First, all known and space e cient algorithms for approximate quantile nding require advance knowledge of the length of the input sequence. Many important database applications employing quantiles cannot provide this information. In this paper, we present anovel non-uniform random sampling scheme and an extension of our framework. Together, they form the basis of a new algorithm which computes approximate quantiles without knowing the input sequence length. Second, if the desired quantile is an extreme value (e.g., within the top 1 % of the elements), the space requirements of currently known algorithms are overly pessimistic. We provide a simple algorithm which estimates extreme values using less space than required by the earlier more general technique for computing all quantiles. Our principal observation here is that random sampling is quanti ably better when estimating extreme values than is the case with the median.  
1565|Characterizing Memory Requirements for Queries over Continuous Data|This paper deals with continuous conjunctive queries with arithmetic comparisons and optional aggregation over multiple data streams. An algorithm is presented for determining whether or not any given query can be evaluated using a bounded amount of memory for all possible instances of the data streams. For queries that can be evaluated using bounded memory, an execution strategy based on constant-sized synopses of the data streams is proposed. For queries that cannot be evaluated using bounded memory, data stream scenarios are identified in which evaluating the queries requires memory linear in the size of the unbounded streams
1566|Congressional samples for approximate answering of group-by queries|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex decision support queries using precomputed summary statistics, such as samples. Decision support queries routinely segment the data into groups and then aggregate the information in each group (group-by queries). Depending on the data, there can be a wide disparity between the number of data items in each group. As a result, approximate answers based on uniform random samples of the data can result in poor accuracy for groups with very few data items, since such groups will be represented in the sample by very few (often zero) tuples. In this paper, we propose a general class of techniques for obtaining fast, highly-accurate answers for group-by queries. These techniques rely on precomputed non-uniform (biased) samples of the data. In particular, we proposecongressional samples, ahybrid union of uniform and biased samples. Given a xed amount of space, congressional samples seek to maximize the accuracy for all possible group-by queries on a set of columns. We present a one pass algorithm for constructing a congressional sample and use this technique to also incrementally maintain the sample up-to-date without accessing the base relation. We also evaluate query rewriting strategies for providing approximate answers from congressional samples. Finally, we conduct an extensive set of experiments on the TPC-D database, which demonstrates the e cacy of the techniques proposed. 1
1567|Histogram-Based Approximation of Set-Valued Query Answers|Answering queries approximately has recently  been proposed as a way to reduce query response  times in on-line decision support systems,  when the precise answer is not necessary  or early feedback is helpful. Most of the  work in this area uses sampling-based techniques  and handles aggregate queries, ignoring  queries that return relations as answers. In  this paper, we extend the scope of approximate  query answering to general queries. We propose  a novel and intuitive error measure for  quantifying the error in an approximate query  answer, which can be a multiset in general.
1568|Data Integration using Self-Maintainable Views|.  In this paper we de#ne the concept of self-maintainable views  # these are views that can be maintained using only the contents of  the view and the database modi#cations, without accessing any of the  underlying databases. We derive tight conditions under which several  types of select-project-join are self-maintainable upon insertions, deletions  and updates. Self-Maintainability is a desirable property for e#-  ciently maintaining large views in applications where fast response and  high availability are important. One example of suchanenvironment  is data warehousing wherein views are used for integrating data from  multiple databases.  1 Introduction  Most large organizations have related data in distinct databases. Many of these databases may be legacy systems, or systems separated for organizational reasons like funding and ownership. Integrating data from such distinct databases is a pressing business need. A common approach for integration is to de#ne an integrated view and...
1569|Monitoring XML Data on the Web|We consider the monitoring of a flow of incoming documents. More precisely, we present here the monitoring used in a very large warehouse built from XML documents found on the web. The flow of documents consists in XML pages (that are warehoused) and HTML pages (that are not). Our contributions are the following:  ffl a subscription language which specifies the monitoring of pages when fetched, the periodical evaluation of continuous queries and the production of XML reports. ffl the description of the architecture of the system we implemented that makes it possible to monitor a flow of millions of pages per day with millions of subscriptions on a single PC, and scales up by using more machines. ffl a new algorithm for processing alerts that can be used in a wider context. We support
1570|Expiring data in a warehouse|Data warehouses collect data into materi-alized views for analysis. After some time, some of the data may no longer be needed or may not be of interest. In this pa-per, we handle this by expiring or remov-ing unneeded materialized view tuples. A framework supporting such expiration is presented. Within it, a user or adminis-trator can declaratively request expirations and can specify what type of modifications are expected from external sources. The lat-ter can significantly increase the amount of data that can be expired. We present effi-cient algorithms for determining what data can be expired (data not needed for main-tenance of other views), taking into account the types of updates that may occur. 1
1572|Sequence Query Processing|Many applications require the ability to manipulate sequences of data. We motivate the importance of sequence query processing, and present a framework for the optimization of sequence queries based on several novel techniques. These include query transformations, optimizations that utilize meta--data, and caching of intermediate results. We present a bottom-up algorithm that generates an efficient query evaluation plan based on cost estimates. This work also identifies a number of directions in which future research can be directed.  1 Introduction  Many real life applications manipulate data that is inherently sequential. Such data is logically viewed and queried in terms of a sequence abstraction and is often physically stored as a sequence. Databases should (a) allow sequences to be queried in a declarative manner, utilizing the ordered semantics of the data, and (b) take advantage of the opportunities available for query optimization. Relational databases are inadequate in this re...
1573|SEQ: A Model for Sequence Databases|This paper presents the model which is the basis for a system to manage various kinds of sequence data. The model separates the data from the ordering information, and includes operators based on two distinct abstractions of a sequence. The main contributions of the model are: (a) it can deal with different types of sequence data, (b) it supports an expressive range of sequence queries, (c) it draws from many of the diverse existing approaches to modeling sequence data. 1
1574|Sampling Algorithms: Lower Bounds and Applications (Extended Abstract)  (2001) |]  Ziv Bar-Yossef  y  Computer Science Division  U. C. Berkeley  Berkeley, CA 94720  zivi@cs.berkeley.edu  Ravi Kumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  ravi@almaden.ibm.com  D. Sivakumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  siva@almaden.ibm.com  ABSTRACT  We develop a framework to study probabilistic sampling algorithms that approximate general functions of the form f : A  n  ! B, where A and B are arbitrary sets. Our goal is to obtain lower bounds on the query complexity of functions, namely the number of input variables x i that any sampling algorithm needs to query to approximate f(x1 ; : : : ; xn ).  We define two quantitative properties of functions --- the block sensitivity and the minimum Hellinger distance --- that give us techniques to prove lower bounds on the query complexity. These techniques are quite general, easy to use, yet powerful enough to yield tight results. Our applications include the mean and higher statistical moments, the median and other selection functions, and the frequency moments, where we obtain lower bounds that are close to the corresponding upper bounds.  We also point out some connections between sampling and streaming algorithms and lossy compression schemes.  1. 
1575|Approximating a Data Stream for Querying and Estimation: Algorithms and Performance Evaluation|Obtaining fast and good quality approximations to data distributions is a problem of central interest to database management. A variety of popular database applications including, approximate querying, similarity searching and data mining in most application domains, rely on such good quality approximations. Histogram based approximation is a very popular method in database theory and practice to succinctly represent a data distribution in a space efficient manner. In this paper, we place the problem of histogram construction into perspective and we generalize it by raising the requirement of a finite data set and/or known data set size. We consider the case of an infinite data set on which data arrive continuously forming an infinite data stream. In this context, we present the first single pass algorithms capable of constructing histograms of provable good quality. We present algorithms for the fixed window variant of the basic histogram construction problem, supporting incremental maintenance of the histograms. The proposed algorithms trade accuracy for speed and allow for a graceful tradeoff between the two, based on application requirements. In the case of approximate queries on infinite data streams, we present a detailed experimental evaluation comparing our algorithms with other applicable techniques using real data sets, demonstrating the superiority of our proposal. 1
1576|A robust, optimization-based approach for approximate answering of aggregate queries|The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are “similar ” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work. 1
1577|Online Dynamic Reordering for Interactive Data Processing|Abstract We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing.
1579|On Sampling and Relational Operators|A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. We highlight the primary difficulties, summarize the results of some recent work in this area, and indicate directions for future work.   
1580|The CN2 Induction Algorithm|Systems for inducing concept descriptions from examples are valuable tools for  assisting in the task of knowledge acquisition for expert systems. This paper presents  a description and empirical evaluation of a new induction system, cn2, designed for  the efficient induction of simple, comprehensible production rules in domains where  problems of poor description language and/or noise may be present. Implementations  of the cn2, id3 and aq algorithms are compared on three medical classification tasks.   
1581|Learning Decision Lists|This paper introduces a new representation for Boolean functions, called decision lists,  and shows that they are efficiently learnable from examples. More precisely, this result  is established for \k-DL&#034; { the set of decision lists with conjunctive clauses of size k at  each decision. Since k-DL properly includes other well-known techniques for representing  Boolean functions such as k-CNF (formulae in conjunctive normal form with at most k  literals per clause), k-DNF (formulae in disjunctive normal form with at most k literals  per term), and decision trees of depth k, our result strictly increases the set of functions  which are known to be polynomially learnable, in the sense of Valiant (1984). Our proof is  constructive: we present an algorithm which can efficiently construct an element of k-DL  consistent with a given set of examples, if one exists.
1582|DBpedia: A Nucleus for a Web of Open Data|Abstract DBpedia is a community effort to extract structured informa-tion from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data. 1
1583|Yago: A Core of Semantic Knowledge|We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains roughly 900,000 entities and 5,000,000 facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as hasWonPrize). The facts have been automatically extracted from the unification of Wikipedia and WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships – and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information
1584|Why and Where: A Characterization of Data Provenance|With the proliferation of database views and curated databases,  the issue of data provenance # where a piece of data came from and the  process by which it arrived in the database # is becoming increasingly  important, especially in scienti#c databases where understanding provenance  is crucial to the accuracy and currency of data. In this paper we  describe an approach to computing provenance when the data of interest  has been created by a database query.We adopt a syntactic approach  and present results for a general data model that applies to relational  databases as well as to hierarchical data such as XML. A novel aspect of  our work is a distinction between #why&#034; provenance #refers to the source  data that had some in#uence on the existence of the data# and #where&#034;  provenance #refers to the location#s# in the source databases from which  the data was extracted#.
1585|ULDBs: Databases with uncertainty and lineage|This paper introduces ULDBs, an extension of relational databases with simple yet expressive constructs for representing and manipulating both lineage and uncertainty. Uncertain data and data lineage are two important areas of data management that have been considered extensively in isolation, however many applications require the features in tandem. Fundamentally, lineage enables simple and consistent representation of uncertain data, it correlates uncertainty in query results with uncertainty in the input data, and query processing with lineage and uncertainty together presents computational benefits over treating them separately. We show that the ULDB representation is complete, and that it permits straightforward implementation of many relational operations. We define two notions of ULDB minimality—dataminimal and lineage-minimal—and study minimization of ULDB representations under both notions. With lineage, derived relations are no longer self-contained: their uncertainty depends on uncertainty in the base data. We provide an algorithm for the new operation of extracting a database subset in the presence of interconnected uncertainty. Finally, we show how ULDBs enable a new approach to query processing in probabilistic databases. ULDBs form the basis of the Trio system under development at Stanford.
1586|Semantic Wikipedia|Wikipedia is the world&#039;s largest collaboratively edited source of encyclopaedic knowledge. But in spite of its utility, its contents are barely machine-interpretable. Structural knowledge, e. g. about how concepts are interrelated, can neither be formally stated nor automatically processed. Also the wealth of numerical data is only available as plain text and thus can not be processed by its actual meaning. We provide
1587|Schema mediation in peer data management systems|permission of the IEEE. Such permission of the IEEE does not in any way imply IEEE endorsement of any of the University of Pennsylvania’s products or services. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to
1588|The Chatty Web: Emergent Semantics Through Gossiping|This paper describes a novel approach for obtaining semantic interoperability among data sources in a bottom-up, semi-automatic manner without relying on pre-existing, global semantic models. We assume that large amounts of data exist that have been organized and annotated according to local schemas. Seeing semantics as a form of agreement, our approach enables the participating data sources to incrementally develop global agreement in an evolutionary and completely decentralized process that solely relies on pair-wise, local interactions: Participants provide translations between schemas they are interested in and can learn about other translations by routing queries (gossiping). To support the participants in assessing the semantic quality of the achieved agreements we develop a formal framework that takes into account both syntactic and semantic criteria. The assessment process is incremental and the quality ratings are adjusted along with the operation of the system. Ultimately, this process results in global agreement, i.e., the semantics that all participants understand. We discuss strategies to efficiently find translations and provide results from a case study to justify our claims. Our approach applies to any system which provides a communication infrastructure (existing websites or databases, decentralized systems, P2P systems) and offers the opportunity to study semantic interoperability as a global phenomenon in a network of information sharing parties.
1589|What have Innsbruck and Leipzig in common? Extracting Semantics from Wiki Content|Abstract Wikis are established means for the collaborative authoring, versioning and publishing of textual articles. The Wikipedia project, for example, succeeded in creating the by far largest encyclopedia just on the basis of a wiki. Recently, several approaches have been proposed on how to extend wikis to allow the creation of structured and semantically enriched content. However, the means for creating semantically enriched structured content are already available and are, although unconsciously, even used by Wikipedia authors. In this article, we present a method for revealing this structured content by extracting information from template instances. We suggest ways to efficiently query the vast amount of extracted information (e.g. more than 8 million RDF statements for the English Wikipedia version alone), leading to astonishing query answering possibilities (such as for the title question). We analyze the quality of the extracted content, and propose strategies for quality improvements with just minor modifications of the wiki systems being currently used. 1
1590|Practical Lineage Tracing in Data Warehouses|We consider the view data lineage problem in a warehousing environment: For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formalize the problem and present a lineage tracing algorithm for relational views with aggregation. Based on our tracing algorithm, we propose a number of schemes for storing auxiliary views that enable consistent and efficient lineage tracing in a multisource data warehouse. We report on a performance study of the various schemes, identifying which schemes perform best in which settings. Based on our results, we have implemented a lineage tracing package in the WHIPS data warehousing system prototype at Stanford. With this package, users can select view tuples of interest, then efficiently &#034;drill down&#034; to examine the source data that produced them. 1 Introduction Data warehousing systems collect data from multiple distributed sources, integrate the information as materialized v...
1591|Wikipedia and the Semantic Web - The Missing Links|Wikipedia is the biggest collaboratively created source of encyclopaedic  knowledge. Growing beyond the borders of any traditional  encyclopaedia, it is facing new problems of knowledge management: The  current excessive usage of article lists and categories witnesses the fact  that 19th century content organization technologies like inter-article references  and indices are no longer su#cient for today&#039;s needs.
1592|Crossing the Structure Chasm|It has frequently been observed that most of the world&#039;s data lies outside  database systems. The reason is that database systems focus on structured data, leaving the unstructured realm to others. The world of unstructured data has several very appealing properties, such as ease of authoring, querying and data sharing. In contrast, authoring, querying and sharing structured data require significant effort, albeit with the benefit of rich query languages and exact answers. We argue
1593|Detecting faces in images: A survey| Images containing faces are essential to intelligent vision-based human computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation, and expression recognition. However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face regardless of its three-dimensional position, orientation, and the lighting conditions. Such a problem is challenging because faces are nonrigid and have a high degree of variability in size, shape, color, and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and evaluate these algorithms. We also discuss relevant issues such as data collection, evaluation metrics, and benchmarking. After analyzing these algorithms and identifying their limitations, we conclude with several promising directions for future research.  
1594|Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection|We develop a face recognition algorithm which is insensitive to gross variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as a coordinate in a high-dimensional space. We take advantage of the observation that the images of a particular face, under varying  illumination but fixed pose, lie in a 3-D linear subspace of the high dimensional image space -- if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing,  images will deviate from this linear subspace. Rather than explicitly modeling  this deviation, we linearly project the image into a subspace in a manner which  discounts those regions of the face with large deviation. Our projection method is  based on Fisher&#039;s Linear Discriminant and produces well separated classes in a low-dimensional  subspace even under severe variation in lighting and facial expressions. The Eigenface
1595|The cascade-correlation learning architecture|Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology, Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network.
1596|Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm|learning Boolean functions, linear-threshold algorithms Abstract. Valiant (1984) and others have studied the problem of learning various classes of Boolean functions from examples. Here we discuss incremental learning of these functions. We consider a setting in which the learner responds to each example according to a current hypothesis. Then the learner updates the hypothesis, if necessary, based on the correct classification of the example. One natural measure of the quality of learning in this setting is the number of mistakes the learner makes. For suitable classes of functions, learning algorithms are available that make a bounded number of mistakes, with the bound independent of the number of examples seen by the learner. We present one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions. The basic method can be expressed as a linear-threshold algorithm. A primary advantage of this algorithm is that the number of mistakes grows only logarithmically with the number of irrelevant attributes in the examples. At the same time, the algorithm is computationally efficient in both time and space. 1.
1598|PCA versus LDA|In the context of the appearance-based paradigm for object recognition, it is generally believed that algorithms based on LDA (Linear Discriminant Analysis) are superior to those based on PCA (Principal Components Analysis) . In this communication we show that this is not always the case. We present our case first by using intuitively plausible arguments and then by showing actual results on a face database. Our overall conclusion is that when the training dataset is small, PCA can outperform LDA, and also that PCA is less sensitive to different training datasets.  Keywords: face recognition, pattern recognition, principal components analysis, linear discriminant analysis, learning from undersampled distributions, small training datasets. 
1599|Classifying Facial Actions|AbstractÐThe Facial Action Coding System (FACS) [23] is an objective method for quantifying facial movement in terms of component actions. This system is widely used in behavioral investigations of emotion, cognitive processes, and social interaction. The coding is presently performed by highly trained human experts. This paper explores and compares techniques for automatically recognizing facial actions in sequences of images. These techniques include analysis of facial motion through estimation of optical flow; holistic spatial analysis, such as principal component analysis, independent component analysis, local feature analysis, and linear discriminant analysis; and methods based on the outputs of local filters, such as Gabor wavelet representations and local principal components. Performance of these systems is compared to naive and expert human subjects. Best performances were obtained using the Gabor wavelet representation and the independent component representation, both of which achieved 96 percent accuracy for classifying 12 facial actions of the upper and lower face. The results provide converging evidence for the importance of using local filters, high spatial frequencies, and statistical independence for classifying facial actions.
1600|The EM Algorithm for Mixtures of Factor Analyzers|Factor analysis, a statistical method for modeling the covariance structure of high dimensional data using a small number of latent variables, can be extended by allowing different local factor models in different regions of the input space. This results in a model which concurrently performs clustering and dimensionality reduction, and can be thought of as a reduced dimension mixture of Gaussians. We present an exact Expectation--Maximization algorithm for fitting the parameters of this mixture of factor analyzers. 1 Introduction  Clustering and dimensionality reduction have long been considered two of the fundamental problems in unsupervised learning (Duda &amp; Hart, 1973; Chapter 6). In clustering, the goal is to group data points by similarity between their features. Conversely, in dimensionality reduction, the goal is to group (or compress) features that are highly correlated. In this paper we present an EM learning algorithm for a method which combines one of the basic forms of dime...
1601|Shape manifolds, Procrustean metrics, and complex projective spaces|2. Shape-spaces and shape-manifolds 82 3. Procrustes analysis, and the invariant (quotient) metric on I j.... 87 4. Shape-measures and shape-densities 93 5. The manifold carrying the shapes of triangles 96
1602|Modeling the manifolds of images of handwritten digits|description length, density estimation.
1603|A Context-Dependent Attention System for a Social Robot|This paper presents part of an on-going project  to integrate perception, attention, drives, emotions,  behavior arbitration, and expressive acts  for a robot designed to interact socially with  humans. We present the design of a visual attention  system based on a model of human visual  search behavior from Wolfe (1994). The  attention system integrates perceptions (motion  detection, color saliency, and face popouts)  with habituation e#ects and influences  from the robot&#039;s motivational and behavioral  state to create a context-dependent attention  activation map. This activation map is used to  direct eye movements and to satiate the drives  of the motivational system.
1604|A Robust Visual Method for Assessing the Relative Performance of Edge-Detection Algorithms| A new method for evaluating edge detection algorithms is presented and applied to measure the relative performance of algorithms by Canny, Nalwa-Binford, Iverson-Zucker, Bergholm, and Rothwell. The basic measure of performance is a visual rating score which indicates the perceived quality of the edges for identifying an object. The process of evaluating edge detection algorithms with this performance measure requires the collection of a set of gray-scale images, optimizing the input parameters for each algorithm, conducting visual evaluation experiments and applying statistical analysis methods. The novel aspect of this work is the use of a visual task and real images of complex scenes in evaluating edge detectors. The method is appealing because, by definition, the results agree with visual evaluations of the edge images.  
1605|A Transform for Multiscale Image Segmentation by Integrated Edge and Region Detection| This paper describes a new transform to extract image regions at all geometric and photometric scales. It is argued that linear approaches such as convolution and matching have the fundamental shortcoming that they require a priori models of region shape. The proposed transform avoids this limitation by letting the structure emerge, bottom-up, from interactions among pixels, in analogy with statistical mechanics and particle physics. The transform involves global computations on pairs of pixels followed by vector integration of the results, rather than scalar and local linear processing. An attraction force field is computed over the image in which pixels belonging to the same region are mutually attracted and the region is characterized by a convergent flow. It is shown that the kansform possesses properties that allow multiscale segmentation, or extraction of original, unblurred structure at all different geometric and photometric scales present in the image. This is in contrast with much of the previous work wherein multiscale structure is viewed as the smoothed structure in a multiscale decimation of image signal. Scale is an integral parameter of the force (computation, and the number and values of scale parameters associated with the image can be estimated automatically. Regions are detected at all, a priori unknown, scales resulting in automatic construction of a segmentation tree, in which each pixel is annotated with descriptions of all the regions it belongs to. Although some of the analytical properties of the transform are presented for piecewise constant images, it is shown that the results hold for more general images, e.g., those containing noise and shading. Thus the proposed method is intended as a solution to the problem of multiscale, integraled edge and region detection, or low-level image segmentation. Experimental results with synthetic and real images are given to demonstrate the properties and segmentation performance of the transform.
1606|Finding Faces in Cluttered Scenes Using Random Labeled Graph Matching|An algorithm for locating quasi-frontal views of human faces in cluttered scenes is presented. The algorithm works by coupling a set of local feature detectors with a statistical model of the mutual distances between facial features; it is invariant with respect to translation, rotation (in the plane), and scale and can handle partial occlusions of the face. On a challenging database with complicated and varied backgrounds, the algorithm achieved a correct localization rate of 95% in images where the face appeared quasi-frontally. 1 Introduction The problem of face recognition has received considerable attention from the computer vision community, and a number of techniques have been proposed in the literature [3, 11, 12, 13, 14, 16, 17, 19]. However, in most of these studies the face was in a benign environment from which it could easily be extracted, or it was assumed to have been pre-segmented. For any of these recognition algorithms to work in a general setting, we need a system...
1607|Multi-Modal Tracking of Faces for Video Communications|This paper describes a system which uses multiple visual processes to detect and track faces for video compression and transmission. The system is based on an architecture in which a supervisor selects and activates visual processes in cyclic manner. Control of visual processes is made possible by a confidence factor which accompanies each observation. Fusion of results into a unified estimation for tracking is made possible by estimating a covariance matrix with each observation. Visual processes for face tracking are described using blink detection, normalised color histogram matching, and cross correlation (SSD and NCC). Ensembles of visual processes are organised into processing states so as to provide robust tracking. Transition between states is determined by events detected by processes. The result of face detection is fed into recursive estimator (Kalman filter). The output from the estimator drives a PD controller for a pan/tilt/zoom camera. The resulting system provides robust and precise tracking which operates continuously at approximately 20 images per second on a 150 megahertz computer work-station. 1.
1608|Face Localization via Shape Statistics|In this paper, a face localization system is proposed in which local detectors are coupled with a statistical model of the spatial arrangement of facial features to yield robust performance. The outputs from the local detectors are treated as candidate locations and constellations are formed from these. The effects of translation, rotation, and scale are eliminated by mapping to a set of shape variables. The constellations are then ranked according to the likelihood that the shape variables correspond to a face versus an alternative model. Incomplete constellations, which occur when some of the true features are missed, are handled in a principled way.  1 Introduction  The problem of face recognition has received considerable attention in the literature [11, 24, 21, 4, 19, 17, 22, 10]; however, in most of these studies, the faces were either embedded in a benign background or were assumed to have been pre-segmented. For any of these recognition algorithms to work in realworld applicati...
1609|Finding Face Features|We describe a computer program which understands a greyscale image of a face well  enough to locate individual face features such as eyes and mouth. The program has two  distinct components: modules designed to locate particular face features, usually in a  restricted area; and the overall control strategy which activates modules on the basis of  the current solution state, and assesses and integrates the results of each module.
1610|Joint Induction of Shape Features and Tree Classifiers|We introduce a very large family of binary features for two-dimensional shapes. The salient ones for separating particular shapes are determined by inductive learning during the construction of classi cation trees. There is a feature for every possible geometric arrangement of local topographic codes. The arrangements express coarse constraints on relative angles and distances among the code locations and are nearly invariant to substantial a ne and non-linear deformations. They are also partially ordered, which makes it possible to narrow the search for informative ones at each node of the tree. Di erent trees correspond to di erent aspects of shape. They are statistically weakly dependent due to randomization and are aggregated in a simple way. Adapting the algorithm to a shape family is then fully automatic once training samples are provided. As an illustration, we classify handwritten digits from the NIST database ? the error rate is:7%.
1611|Face Detection With Information-Based Maximum Discrimination|In this paper we present a visual learning technique that maximizes the discrimination between positive and negative examples in a training set. We demonstrate our technique in the context of face detection with complex background without color or motion information, which has proven to be a challenging problem. We use a family of discrete Markov processes to model the face and background patterns and estimate the probability models using the data statistics. Then, we convert the learning process into an optimization, selecting the Markov process that optimizes the information-based discrimination between the two classes. The detection process is carried out by computing the likelihood ratio using the probability model obtained from the learning procedure. We show that because of the discrete nature of these models, the detection process is, by almost two orders of magnitude, less computationally expensive than neural network approaches. However, no improvement in terms of correct-answ...
1612|Multi-Modal System for Locating Heads and Faces|We designed a modular system using a combination of shape analysis, color segmentation  and motion information for locating reliably heads  and faces of different sizes and orientations in  complex images. The first of the system&#039;s three channels does a shape analysis on gray-level images to determine the location of individual facial features as well as the outlines of heads. In the second channel the color space is analyzed with a clustering algorithm to find areas of skin colors. The color space  is first calibrated, using the results from the other channels. In the third channel motion information is extracted from frame differences. Head outlines are determined by analyzing the shapes of areas with large motion vectors. All three channels produce lists of shapes, each marking an area of the image where a facial feature or apart of the outline of a head may be present. Combinations of such shapes are  evaluated with n-gram searches to produce a list of likely head positions and the locations of facial features. We tested the system for tracking faces of people sitting in front of terminals and video phones and used it to track people entering through a doorway.
1613|Rule-Based Face Detection in Frontal Views|Face detection is a key problem in building automated systems that perform face recognition. A very attractive approach for face detection is based on multiresolution images (also known as mosaic images). Motivated by the simplicity of this approach, a rule-based face detection algorithm in frontal views is developed that extends the work of G. Yang and T.S. Huang. The proposed algorithm has been applied to frontal views extracted from the European ACTS M2VTS database that contains the videosequences of 37 different persons. It has been found that the algorithm provides a correct facial candidate in all cases. However, the success rate of the detected facial features (e.g. eyebrows/eyes, nostrils/nose, and mouth) that validate the choice of a facial candidate is found to be 86.5 % under the most strict evaluation conditions. 1. INTRODUCTION  Face recognition has been an active research topic in computer vision for more than two decades. A critical survey of the literature on human and ...
1614|Modelling Facial Colour and Identity with Gaussian Mixtures|An integrated system for the acquisition, normalisation and recognition of moving faces in dynamic scenes is introduced. Four face recognition tasks are defined and it is argued that modelling person-specific probability densities in a generic face space using mixture models provides a technique applicable to all four tasks. The use of Gaussian colour mixtures for face detection and tracking is also described. Results are presented using data from the integrated system.  Key words: Face recognition, Biometrics, Gaussian mixtures, Colour models.  1 Introduction  Face recognition in general and the recognition of moving people in natural scenes in particular, require a set of visual tasks to be performed robustly. These include (1) Acquisition: the detection and tracking of face-like image patches in a dynamic scene, (2) Normalisation: the segmentation, alignment and normalisation of the face images, and (3) Recognition: the representation and modelling of face images as identities, and ...
1615|Performance Assessment through Bootstrap|A new performance evaluation paradigm for computer vision systems is proposed. In real situation, the complexity of the  input data and/or of the computational procedure can make traditional error propagation methods infeasible. The new approach  exploits a resampling technique recently introduced in statistics, the bootstrap. Distributions for the output variables are obtained by  perturbing the nuisance properties of the input, i.e., properties with no relevance for the output under ideal conditions. From these  bootstrap distributions, the confidence in the adequacy of the assumptions embedded into the computational procedure for the given  input is derived. As an example, the new paradigm is applied to the task of edge detection. The performance of several edge  detection methods is compared both for synthetic data and real images. The confidence in the output can be used to obtain an  edgemap independent of the gradient magnitude.
1616|Fast Face Detection via Morphology-based Pre-processing|An efficient face detection algorithm which can detect multiple faces in cluttered  environment is proposed. The proposed system consists of three main steps. In the  first step, a morphology-based technique is devised to perform eye-analogue segmentation.  Morphological operations are applied to locate eye-analogue pixels in the original  image. Then, a labeling process is executed to generate the eye-analogue segments. In  the second step, the previously located eye-analogue segments are used as guides to  search for potential face regions. The last step of the proposed system is to perform  face verification. In this step, every face candidate obtained from the previous step is  normalized to a standard size. Then, each of these normalized potential face images  is fed into a trained backpropagation neural network for identification. After all the  true faces are identified, their corresponding poses are located based on the guidance of  optimizing a cost function. The proposed face...
1617|Probabilistic Affine Invariants for Recognition|Under a weak perspective camera model, the image plane coordinates in different views of a planar object are related by an affine transformation. Because of this property, researchers have attempted to use affine invariants for recognition. However, there are two problems with this approach: (1) objects or object classes with inherent variability cannot be adequately treated using invariants; and (2) in practice the calculated affine invariants can be quite sensitive to errors in the image plane measurements. In this paper we use probability distributions to address both of these difficulties. Under the assumption that the feature positions of a planar object can be modeled using a jointly Gaussian density, we have derived the joint density over the corresponding set of affine coordinates. Even when the assumptions of a planar object and a weak perspective camera model do not strictly hold, the results are useful because deviations from the ideal can be treated as deformability in the ...
1618|Mixtures of Eigenfeatures for Real-Time Structure from Texture|We describe a face modeling system which estimates complete facial structure and texture from a real-time video stream. The system begins with a face tracking algorithm which detects and stabilizes live facial images into a canonical 3D pose. The resulting canonical texture is then processed by a statistical model to filter imperfections and estimate unknown components such as missing pixels and underlying 3D structure. This statistical model is a soft mixture of eigenfeature selectors which span the 3D deformations and texture changes across a training set of laser scanned faces. An iterative algorithm is introduced for determining the dimensional partitioning of the eigenfeatures to maximize their generalization capability over a cross-validation set of data. The model&#039;s abilities to filter and estimate absent facial components are then demonstrated over incomplete 3D data. This ultimately allows the model to span known and regress unknown facial information from stabilized natural video sequences generated by a face tracking algorithm. The resulting continuous and dynamic estimation of the model&#039;s parameters over a video sequence generates a compact temporal description of the 3D deformations and texture changes of the face.  
1619|Scale Invariant Face Detection Method using Higher-Order Local Autocorrelation Features extracted from Log-Polar Image|This paper proposes a scale invariant face detection method which combines higher-order local autocorrelation (HLA C) features extracted from a log-polar transformed image with Linear Discriminant Analysis for &#034;face&#034; and &#034;not face&#034; classification. Since HLAC features of log-polar image are sensitive to shifts of a face, we utilize this property and develop a face detection method. HLA C features extracted from a log-polar image become scale and rotation invariant because scalings and rotations of a face are expressed as shifts in a log-polar image (coordinate). By combining these features with the Linear Discriminant Analysis which is extended to treat &#034;face&#034; and &#034;not face&#034; classes, a scale invariant face detection system can be realized.
1620|Detection of Human Faces Using Decision Trees|This paper proposes a novel algorithm for face detection using decision trees (DT) and shows its generality and feasibility using a data base consisting of 2,340 face images from the FERET data base (corresponding to 817 subjects and including 190 sets of duplicates) over a semi uniform background. The approach used for face detection involves three main stages, those of location, cropping, and post processing. The first stage finds a rough approximation for the possible location of the face box, the second stage will refine it, and the last stage decides whether a face is present in the image and if the answer is positive would normalize the face image. The algorithm does not require multiple (scale) templates and the accuracy achieved is 96%. Accuracy is based on the visual observation that the face box includes both eyes, nose, and mouth, and that the top side of the box is below the hairline. Experiments were also performed to assess the accuracy of the algorithm in rejecting image...
1621|Information Theoretic View-Based and Modular Face Detection|This paper describes information theoretic methods for the determination of the optimal subset of pixels for the problem of face detection in complex backgrounds. A view-based method is described, which has limitations due to misalignments. This motivates the modular feature based method which minimizes the misalignment problem. Empirical comparisons between the viewbased, modular, and sum of squared difference methods are made using four databases from three universities. 1. Introduction  The face detection problem may be described as follows: Given a test image (any scanned in photograph or frame from a video camera), find the locations and size of every human face within the image. The problem of face detection differs from the problem of face recognition in that face detection has exactly two classifications: face or nonface, whereas face recognition usually has a number of classifications equal to the number of individuals. Face detection is important to a wide variety of areas wh...
1622|Generalized likelihood ratio-based face detection and extraction of mouth features|isy.liu.se davoine,haibo,robert¥ Abstract. In this paper we describe a system to reliably localize the position of the speaker’s face and mouth in videophone sequences. A statistical scheme based on a subspace method is presented for detecting human faces under varying poses. We propose a new matching criterion based on the Generalized Likelihood Ratio. The criterion is optimized efficiently with respect to similarity, affine or perspective transform parameters using a coarse-to-fine search strategy combined with a simulated annealing algorithm. Moreover we propose to extract a vector of geometrical features (four points) on the outline of the mouth. The extraction consists in analyzing amplitude projections in the regions of the mouth. All the computations are performed on H263-coded frames, with a QCIF spatial resolution. To this end, we propose algorithms adapted to the poor quality of the images and suited to a further real-time application. 1
1623|Ensemble and Modular Approaches for Face Detection: A|A new learning model based on autoassociative neural networks is developped and applied to face detection. To extend the de-tection ability in orientation and to decrease the number of false alarms, different combinations of networks are tested: ensemble, conditional ensemble and conditional mixture of networks. The use of a conditional mixture of networks allows to obtain state of the art results on different benchmark face databases. 1 A constrained generative model Our purpose is to classify an extracted window x from an image as a face (x E V) or non-face (x EN). The set of all possible windows is E = V uN, with V n N = 0. Since collecting a representative set of non-face examples is impossible, face detection by a statistical model is a difficult task. An autoassociative network, using five layers of neurons, is able to perform a non-linear dimensionnality reduction [Kramer, 1991]. However, its use as an estimator, to classify an extracted window as face or non-face, raises two problems: 1. V&#039;, the obtained sub-manifold can contain non-face examples (V C V&#039;), 2. owing to local minima, the obtained solution can be close to the linear solution: the principal components analysis. Our approach is to use counter-examples in order to find a sub-manifold as close as possible to V and to constrain the algorithm to converge to a non-linear solution [Feraud, R. et al., 1997]. Each non-face example is constrained to be reconstructed as its projection on V. The projection P of a point x of the input space E on V, is defined by:
1624|Learning the Human Face Concept From Black and White Pictures|This study presents a learning approach for the face detection problem. Given an arbitrary black and white, still image, find the location and size of every human face it contains. Numerous applications of the automatic face detection have attracted considerable interest [1--7], but no present face detection system is completely satisfactory from the point of view of detection rate, false alarm rate and detection time. We describe an inductive learning detection method that produces a maximally specific hypothesis consistent with the training data. Three different sets of features were considered for defining the concept of a human face. The performance achieved is as follows: 85% detection rate, a false alarm rate of 0:04% of the number of windows analyzed and 1 minute detection time on a 320 \Theta 240 image on a Sun Ultrasparc 1.  I. Introduction This paper explores new ways of learning and retrieving the appearance of human faces in black and white pictures. The retrieval problem,...
1625|Feature selection based on mutual information: Criteria of max-depe ndency, max-relevance, and min-redundancy |Abstract—Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.
1626|A tutorial on support vector machines for pattern recognition|  The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.
1627|Survey on Independent Component Analysis|A common problem encountered in such disciplines as statistics, data analysis, signal processing, and neural network research, is nding a suitable representation of multivariate data. For computational and conceptual simplicity, such a representation is often sought as a linear transformation of the original data. Well-known linear transformation methods include, for example, principal component analysis, factor analysis, and projection pursuit. A recently developed linear transformation method is independent component analysis (ICA), in which the desired representation is the one that minimizes the statistical dependence of the components of the representation. Such a representation seems to capture the essential structure of the data in many applications. In this paper, we survey the existing theory and methods for ICA.  
1628|Statistical pattern recognition: A review|The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have bean receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field.
1629|A Comparison of Methods for Multiclass Support Vector Machines| Support vector machines (SVMs) were originally designed for binary classification. How to effectively extend it for multiclass classification is still an ongoing research issue. Several methods have been proposed where typically we construct a multiclass classifier by combining several binary classifiers. Some authors also proposed methods that consider all classes at once. As it is computationally more expensive to solve multiclass problems, comparisons of these methods using large-scale problems have not been seriously conducted. Especially for methods solving multiclass SVM in one step, a much larger optimization problem is required so up to now experiments are limited to small data sets. In this paper we give decomposition implementations for two such “all-together” methods. We then compare their performance with three methods based on binary classifications: “one-against-all,” “one-against-one,” and directed acyclic graph SVM (DAGSVM). Our experiments indicate that the “one-against-one” and DAG methods are more suitable for practical use than the other methods. Results also show that for large problems methods by considering all data at once in general need fewer support vectors. 
1630|Adaptive floating search methods in feature selection|A new suboptimal search strategy for feature selection is presented. It represents a more sophisticated version of &#034;classical&#034; floating search algorithms (Pudil et al., 1994), attempts to remove some of their potential deficiencies and facilitates finding a solution even closer to the optimal one.  
1631|Feature selection: Evaluation, application, and small sample performance|Abstract—A large number of algorithms have been proposed for feature subset selection. Our experimental results show that the sequential forward floating selection (SFFS) algorithm, proposed by Pudil et al., dominates the other algorithms tested. We study the problem of choosing an optimal feature set for land use classification based on SAR satellite images using four different texture models. Pooling features derived from different texture models, followed by a feature selection results in a substantial improvement in the classification accuracy. We also illustrate the dangers of using feature selection in small sample size situations. Index Terms—Feature selection, curse of dimensionality, genetic algorithm, node pruning, texture models, SAR image classification. 1
1632|Minimum redundancy feature selection from microarray gene expression data|Selecting a small subset of genes out of the thousands of genes in microarray data is important for accurate classification of phenotypes. Widely used methods typically rank genes according to their differential expressions among phenotypes and pick the top-ranked genes. We observe that feature sets so obtained have certain redundancy and study methods to minimize it. Feature sets obtained through the minimum redundancy – maximum relevance framework represent broader spectrum of characteristics of phenotypes than those obtained through standard ranking methods; they are more robust, generalize well to unseen data, and lead to significantly improved classifications in extensive experiments on 5 gene expressions data sets. 
1633|Feature selection for high-dimensional genomic microarray data|We report on the successful application of feature selection methods to a classification problem in molecular biology involving only 72 data points in a 7130 dimensional space. Our approach is a hybrid of filter and wrapper approaches to feature selection. We make use of a sequence of simple filters, culminating in Koller and Sahami’s (1996) Markov Blanket filter, to decide on particular feature subsets for each subset cardinality. We compare between the resulting subset cardinalities using cross validation. The paper also investigates regularization methods as an alternative to feature selection, showing that feature selection methods are preferable in this problem. 1.
1634|Input feature selection by mutual information based on parzen window|The corresponding author is Nojun Kwak and his e-mail address is underlined. This work is partly supported by the Brain Neuroinformatics Research Program from Korean government.
1635|Experiments with classifier combination rules |Abstract. A large experiment on combining classifiers is reported and discussed. It includes, both, the combination of different classifiers on the same feature set and the combination of classifiers on different feature sets. Various fixed and trained combining rules are used. It is shown that there is no overall winning combining rule and that bad classifiers as well as bad feature sets may contain valuable information for performance improvement by combining rules. Best performance is achieved by combining both, different feature sets and different classifiers. 1
1636|How Many Genes Are Needed for a Discriminant Microarray Data Analysis|The analysis of the leukemia data from Whitehead/MIT group is a discriminant analysis (also called a supervised learning). Among thousands of genes whose expression levels are measured, not all are needed for discriminant analysis: a gene may either not contribute to the separation of two types of tissues/cancers, or it may be redundant because it is highly correlated with other genes. There are two theoretical frameworks in which variable selection (or gene selection in our case) can be addressed. The first is model selection, and the second is model averaging. We have carried out model selection using Akaike information criterion and Bayesian information criterion with logistic regression (discrimination, prediction, or classification) to determine the number of genes that provide the best model. These model selection criteria set upper limits of 22-25 and 12-13 genes for this data set with 38 samples, and the best model consists of only one (no.4847, zyxin) or two genes. We have also carried out model averaging over the best single-gene logistic predictors using three different weights: maximized likelihood, prediction rate on training set, and equal weight. We have observed that the performance of most of these weighted predictors on the testing set is gradually reduced as more genes are included, but a clear cutoff that separates good and bad prediction performance is not found. 1 Li Yang 2
1637|A Bayesian Morphometry Algorithm |Most methods for structure-function analysis in medical images usually are based on voxel-wise statistical tests performed on registered Magnetic Resonance (MR) images across subjects. A major drawback of such methods is the inability to accurately locate regions that manifest nonlinear associations with clinical variables. In this paper we propose Bayesian Morphological Analysis (BMA) methods, based on a Bayesian-network representation, for the analysis of MR brain images. First, we describe how Bayesian networks can represent probabilistic associations among voxels and clinical (functional) variables. Second, we present a model-selection framework, which generates a Bayesian network that captures structure-function relationships from MR brain images and functional variables. We demonstrate our methods in the context of determining associations between regional brain atrophy (as demonstrated on MR images of the brain), and functional deficits. We employ two data sets for this evaluation: the first contains MR images of 11 subjects, where associations between regional atrophy and a functional deficit are almost linear; the second data set contains MR images of the ventricles of 84 subjects, where the structure-function association is nonlinear. Our methods successfully identify voxel-wise morphological changes that are associated with functional deficits in both data sets, whereas standard statistical analysis (i. e., t-test and paired t-test) finds only some of these changes in the linear-association case, and fails in the nonlinear-association case.
1638|Bayesian Clustering Methods For Morphological Analysis|Determining the relationship between structure (i.e. morphology) and function is a fundamental problem in brain research. In this paper we present a new framework based on Bayesian clustering methods for the voxel-wise statistical morphology-function analysis of registered MR images. We construct a Bayesian network to automatically identify the significant associations between voxel-wise morphological variables and functional variables, such as cognitive performance. A Bayesian latent variable induction method is applied to locate the homogeneous association regions on registered maps of morphological variables. Experimental results on images with simulated atrophy confirm that the new method outperforms conventional statistical method, based on linear statistics.
1639|Structure search and stability enhancement of Bayesian networks|Learning Bayesian network structure from large-scale data sets, without any expert-specified ordering of variables, remains a difficult problem. We propose systematic improvements to automatically learn Bayesian network structure from data. (1) We propose a linear parent search method to generate candidate graph. (2) We propose a comprehensive approach to eliminate cycles using minimal likelihood loss, a short cycle first heuristic, and a cut-edge repairing. (3) We propose structure perturbation to assess the stability of the network and a stability-improvement method to refine the network structure. The algorithms are easy to implement and efficient for large networks. Experimental results on two data sets show that our new approach outperforms existing methods. 1.
1640|Progressive Meshes |Highly detailed geometric models are rapidly becoming commonplace in computer graphics. These models, often represented as complex triangle meshes, challenge rendering performance, transmission bandwidth, and storage capacities. This paper introduces the progressive mesh (PM) representation, a new scheme for storing and transmitting arbitrary triangle meshes. This efficient, lossless, continuous-resolution representation addresses several practical problems in graphics: smooth geomorphing of level-of-detail approximations, progressive transmission, mesh compression, and selective refinement. In addition, we present a new mesh simplification procedure for constructing a PM representation from an arbitrary mesh. The goal of this optimization procedure is to preserve not just the geometry of the original mesh, but more importantly its overall appearance as defined by its discrete and scalar appearance attributes such as material identifiers, color values, normals, and texture coordinates. We demonstrate construction of the PM representation and its applications using several practical models.
1641|A Signal Processing Approach To Fair Surface Design|In this paper we describe a new tool for interactive free-form fair surface design. By generalizing classical discrete Fourier analysis to two-dimensional discrete surface signals -- functions defined on polyhedral surfaces of arbitrary topology --, we reduce the problem of surface smoothing, or fairing, to low-pass filtering. We describe a very simple surface signal low-pass filter algorithm that applies to surfaces of arbitrary topology. As opposed to other existing optimization-based fairing methods, which are computationally more expensive, this is a linear time and space complexity algorithm. With this algorithm, fairing very large surfaces, such as those obtained from volumetric medical data, becomes affordable. By combining this algorithm with surface subdivision methods we obtain a very effective fair surface design technique. We then extend the analysis, and modify the algorithm accordingly, to accommodate different types of constraints. Some constraints can be imposed without any modification of the algorithm, while others require the solution of a small associated linear system of equations. In particular, vertex location constraints, vertex normal constraints, and surface normal discontinuities across curves embedded in the surface, can be imposed with this technique.  CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/image generation - display algorithms; I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling - curve, surface, solid, and object representations;J.6[Com- puter Applications]: Computer-Aided Engineering - computeraided design  General Terms: Algorithms, Graphics.  1 
1642|The Plenoptic Function and the Elements of Early Vision|experiment. Electrophysiologists have described neurons in striate cortex that are selectively sensitive to certain visual properties; for reviews, see Hubel (1988) and DeValois and DeValois (1988). Psychophysicists have inferred the existence of channels that are tuned for certain visual properties; for reviews, see Graham (1989), Olzak and Thomas (1986), Pokorny and Smith (1986), and Watson (1986). Researchers in perception have found aspects of visual stimuli that are processed pre-attentively (Beck, 1966; Bergen &amp; Julesz, 1983; Julesz &amp; Bergen,  Motion Color Binocular disparity Retinal processing Early vision Memory Higher-level vision Etc... Retina More processing Still more processing Orientation Fig.1.1  A generic diagram for visual processing. In this approach, early vision consists of a set of parallel pathways, each analyzing some particular aspect of the visual stimulus.  1983; Treisman, 1986; Treisman &amp; Gelade, 1980). And in computational
1643|QuickTime VR - An Image-Based Approach to Virtual Environment Navigation|Traditionally, virtual reality systems use 3D computer graphics to model and render virtual environments in real-time. This approach usually requires laborious modeling and expensive special purpose rendering hardware. The rendering quality and scene complexity are often limited because of the real-time constraint. This paper presents a new approach which uses 360-degree cylindrical panoramic images to compose a virtual environment. The panoramic image is digitally warped on-the-fly to simulate camera panning and zooming. The panoramic images can be created with computer rendering, specialized panoramic cameras or by &#034;stitching&#034; together overlapping photographs taken with a regular camera. Walking in a space is currently accomplished by &#034;hopping&#034; to different panoramic points. The image-based approach has been used in the commercial product QuickTime VR, a virtual reality extension to Apple Computer&#039;s QuickTime digital multimedia framework. The paper describes the architecture, the fil...
1644|Epipolarplane image analysis: An approach to determining structure from motion|We present a technique for building a three-dimensional description of a static scene from a dense sequence of images. These images are taken in such rapid succession that they form a solid block of data in which the temporal continuity from image to image is approximately equal to the spatial continuity in an individual image. The technique utilizes knowledge of the camera motion to form and analyze slices of this solid. These slices directly encode not only the three-dimensional positions of objects, but also such spatiotemporal events as the occlusion of one object by another. For straight-line camera motions, these slices have a simple linear structure that makes them easier to analyze. The analysis computes the threedimensional positions of object features, marks occlusion boundaries on the objects, and builds a threedimensional map of &#034;free space.&#034; In our article, we first describe the application of this technique to a simple camera motion, and then show how projective duality is used to extend the analysis to a wider class of camera motions and object types that include curved and moving objects.  
1645|A physical Approach to Color Image Understanding|In this paper, we present an approach to color image understanding that can be used to segment and analyze sur-  faces with color variations due to highlights and shading. The work is based on a theory-the Dichromatic Reflec-  tion Model-which describes the color of the reflected light as a mixture of light from surface reflection (highlights) and body reflection (object color). In the past, we have shown how the dichromatic theory can be used to separate a color image into two intrinsic reflection images: an image of just the highlights, and the original image with the highlights removed. At that time, the algorithm could only be applied to hand-segmented images. This paper shows how the same reflection model can be used to include color image segmentation into the image analysis. The result is a color image understanding system, capable of generating physical descriptions of the reflection processes occurring in the scene. Such descriptions include the intrinsic reflection images, an image segmenta-  tion, and symbolic information about the object and highlight colors. This line of research can lead to based image understanding methods that are both more reliable and more useful than traditional methods.
1646|Modeling and Calibration of Automated Zoom Lenses|and should not be interpreted as necessarily representing o cial policies or endorsements,
1647|3-D Scene Representation as a Collection of Images and Fundamental Matrices|: In this report, we address the problem of the prediction of new views of a given scene from existing weakly or fully calibrated views called  reference views. Our method does not make use of a three-dimensional model of the scene, but of the existing relations between the images. The new views are represented in the reference views by a viewpoint and a retinal plane, i.e. by four points which can be chosen interactively. From this representation and from the constraints between the images, we derive an algorithm to predict the new views. We discuss the advantages of this method compared to the commonly used scheme : 3-D reconstruction-projection. We show some experimental results with synthetic and real data.  Key-words: 3-D scene representation, multi-view stereo, image synthesis (R&#039;esum&#039;e : tsvp) This work was partially supported by DRET contract No 91-815/DRET/EAR and by the EEC under Esprit project 6448, Viva Unite de recherche INRIA Sophia-Antipolis 2004 route des Lucioles, BP 9...
1648|Animating images with drawings|The work described here extends the power of 2D animation with a form of texture mapping conveniently controlled by line drawings. By tracing points, line segments, spline curves, or filled regions on an image, the animator defines features which can be used to animate the image. Animations of the control features deform the image smoothly. This development is in the tradition of &#034;skeleton&#034;-based animation, and &#034;feature&#034;-based image metamorphosis. By employing numerics developed in the computer vision community for rapid visual surface estimation, several important advantages are realized. Skeletons are generalized to include curved &#034;bones, &#034; the interpolating surface is better behaved, the expense of computing the animation is decoupled from the number of features in the drawing, and arbitrary holes or cuts in the interpolated surface can be accommodated. The same general scattered data interpolation technique is applied to the problem of mapping animation from one image and set of features to another, generalizing the prescriptive power of animated sequences and encouraging reuse of animated motion.
1649|Rendering Real-World Objects Using View Interpolation|This paper overviews the theoretical background along with the description of the preliminary experiments with the interpolated view synthesis, indicating that our approach is robust and feasible.
1650|Holographic Stereograms as Discrete Imaging Systems|Unlike holograms of real objects, holographic stereograms consist of information recorded from a relatively small number of discrete viewpoints. As discrete imaging systems, holographic stereograms are susceptible to aliasing artifacts caused by insufficient or improper sampling. A characterization of sampling-related image artifacts in holographic stereograms is presented. Constraints on image extent and resolution imposed by sampling are outlined. Methods of reducing or eliminating aliasing artifacts in both photographically-recorded and computer-generated holographic stereogram images are described. Results of this analysis can be generalized to describe other autostereoscopic displays. 1. INTRODUCTION Holographic stereography is the most widely used holographic technique for producing three-dimensional imagery from two-dimensional views. Holographic stereograms are the result of the merger of two approaches to three-dimensional imagery: display holography, with its roots in the wor...
1651|Generative Modeling: A Symbolic System for Geometric Modeling|This paper discusses a new, symbolic approach to geometric modeling called generative modeling. The approach allows specification, rendering, and analysis of a wide variety of shapes including 3D curves, surfaces, and solids, as well as higher-dimensional shapes such as surfaces deforming in time, and volumes with a spatially varying mass density. The system also supports powerful operations on shapes such as &#034;reparameterize this curve by arclength&#034;, &#034;compute the volume, center of mass, and moments of inertia of the solid bounded by these surfaces&#034;, or &#034;solve this constraint or ODE system&#034;. The system has been used for a wide variety of applications, including creating surfaces for computer graphics animations, modeling the fur and body shape of a teddy bear, constructing 3D solid models of elastic bodies, and extracting surfaces from magnetic resonance (MR) data. Shapes in the system are specified using a language which builds multidimensional parametric functions. The language is bas...
1653|Blind Signal Separation: Statistical Principles|Blind signal separation (BSS) and independent component analysis (ICA) are emerging techniques of array processing and data analysis, aiming at recovering unobserved signals or `sources&#039; from observed mixtures (typically, the output of an array of sensors), exploiting only the assumption of mutual independence between the signals. The weakness of the assumptions makes it a powerful approach but requires to venture beyond familiar second order statistics. The objective of this paper is to review some of the approaches that have been recently developed to address this exciting problem, to show how they stem from basic principles and how they relate to each other.
1654|GTM: The generative topographic mapping|Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis which is based on a linear transformations between the latent space and the data space. In this paper we introduce a form of non-linear latent variable model called the Generative Topographic Mapping for which the parameters of the model can be determined using the EM algorithm. GTM provides a principled alternative to the widely used Self-Organizing Map (SOM) of Kohonen (1982), and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multi-phase oil pipeline. Copyright c?MIT Press (1998). 1
1655|Keeping Neural Networks Simple by Minimizing the Description Length of the Weights |Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-off between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed efficiently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights.  
1656|Nonlinear source separation: the post-nonlinear mixtures|Abstract—In this paper, we address the problem of separation of mutually independent sources in nonlinear mixtures. First, we propose theoretical results and prove that in the general case, it is not possible to separate the sources without nonlinear distortion. Therefore, we focus our work on specific nonlinear mixtures known as post-nonlinear mixtures. These mixtures constituted by a linear instantaneous mixture (linear memoryless channel) followed by an unknown and invertible memoryless nonlinear distortion, are realistic models in many situations and emphasize interesting properties i.e., in such nonlinear mixtures, sources can be estimated with the same indeterminacies as in instantaneous linear mixtures. The separation structure of nonlinear mixtures is a two-stage system, namely, a nonlinear stage followed by a linear stage, the parameters of which are updated to minimize an output independence criterion expressed as a mutual information criterion. The minimization of this criterion requires knowledge or estimation of source densities or of their log-derivatives. A first algorithm based on a Gram–Charlier expansion of densities is proposed. Unfortunately, it fails for hard nonlinear mixtures. A second algorithm based on an adaptive estimation of the log-derivative of densities leads to very good performance, even with hard nonlinearities. Experiments are proposed to illustrate these results. Index Terms—Entropy, neural networks, nonlinear mixtures, source separation, unsupervised adaptive algorithms. I.
1657|Nonlinear Independent Component Analysis: Existence and Uniqueness Results|The question of existence and uniqueness of solutions for nonlinear independent component analysis is addressed. It is shown that if the space of mixing functions is not limited, there exists always an infinity of solutions. In particular, it is shown how to construct parameterized families of solutions. The indeterminacies involved are not trivial, as in the linear case. Next, it is shown how to utilize some results of complex analysis to obtain uniqueness of solutions. We show that for two dimensions, the solution is unique up to a rotation, if the mixing function is constrained to be a conformal mapping, together with some other assumptions. We also conjecture that the solution is strictly unique except in some degenerate cases, since the indeterminacy implied by the rotation is essentially similar to estimating the model of linear independent component analysis.
1658|Ensemble Learning|Introduction  When we say we are making a model of a system, we are setting up a tool which can be used to make inferences, predictions and decisions. Each model can be seen as a hypothesis, or explanation, which makes assertions about the quantities which are directly observable and which can only be inferred from their eect on observable quantities.  In the Bayesian framework, knowledge is contained in the conditional probability distributions of the models. We can use Bayes&#039; theorem to evaluate the conditional probability distributions for the unknown parameters, y, given the set of observed quantities, x, using  p (y jx ) =  p (x jy ) p (y)  p (x)  (1) The prior distribution p (y) contains our knowledge of the unknown variables before we make any observ
1659|Nonlinear Independent Component Analysis Using Ensemble Learning: Experiments And Discussion|In this paper, we present experimental results on a nonlinear independent component analysis approach based on Bayesian ensemble learning. The theory of the method is discussed in a companion paper. Simulations with artificial and natural data demonstrate the feasibility and good performance of the proposed approach. We also discuss the relationships of the method to other existing methods.  
1660|Information-Theoretic Approach to Blind Separation of Sources in Non-linear Mixture|The linear mixture model is assumed in most of the papers devoted to blind separation. A more realistic model for mixture should be non-linear. In this paper, a two-layer perceptron is used as a de-mixing system to separate sources in non-linear mixture. The learning algorithms for the de-mixing system are derived by two approaches: maximum entropy and minimum mutual information. The algorithms derived from the two approaches have a common structure. The new learning equations for the hidden layer are different from the learning equations for the output layer. The natural gradient descent method is applied in maximizing entropy and minimizing mutual information. The information (entropy or mutual information) back-propagation method is proposed to derive the learning equations for the hidden layer.
1661|Ensemble Learning For Independent Component Analysis|In this paper, a recently developed Bayesian method called ensemble learning is applied to independent component analysis (ICA). Ensemble learning is a computationally efficient approximation for exact Bayesian analysis. In general, the posterior probability density function (pdf) is a complex high dimensional function whose exact treatment is diffucult. In ensemble learning, the posterior pdf is approximated by a more simple function and Kullback-Leibler information is used as the criterion for minimising the misfit between the actual posterior pdf and its parametric approximation. In this paper, the posterior pdf is approximated by a diagonal Gaussian pdf. According to the ICA-model used in this paper, the measurements are generated by a linear mapping from mutually independent source signals whose distributions are mixtures of Gaussians. The measurements are also assumed to have additive Gaussian noise with diagonal covariance. The model structure and all parameters of the distribution...
1662|Statistical Independence and Novelty Detection with Information Preserving Nonlinear Maps|. According to Barlow (1989), feature extraction can be understood as finding a statistically independent representation of the probability distribution underlying the measured signals. The search for a statistically independent representation can be formulated by the criterion of minimal mutual information, which reduces to decorrelation in the case of Gaussian distributions. If non-Gaussian distributions are to be considered, minimal mutual information is the appropriate generalization of decorrelation as used in linear Principal Component Analyses (PCA). We also generalize to nonlinear transformations by only demanding perfect transmission of information. This leads to a general class of nonlinear transformations, namely symplectic maps. Conservation of information allows us to consider only the statistics of single coordinate. The resulting factorial representation of the joint probability distribution gives a density estimation. We apply this concept to the real world problem of e...
1663|Blind Source Separation Of Nonlinear Mixing Models|We present a new set of learning rules for the nonlinear blind source separation problem based on the information maximization criterion. The mixing model is divided into a linear mixing part and a nonlinear transfer channel. The proposed model focuses on a parametric sigmoidal nonlinearity and higher order polynomials. Our simulation results verify the convergence of the proposed algorithms. 1 INTRODUCTION  In blind source separation or independent component analysis (ICA) the problem is how to recover independent sources given the sensor outputs in which the sources have been mixed in an unknown channel. The problem has become increasingly important in the signal processing area due to their prospective application in speech recognition, telecommunications and medical signal processing. The linear blind source separation problem has been studied by researchers in the field of neural networks [1, 2, 5, 9] and statistical signal processing [4, 6]. Potential application in automatic spe...
1664|A Maximum Likelihood Approach to Nonlinear Blind Source Separation|In the basic signal model of blind source separation (BSS), an unknown linear mixing process is assumed. While this ensures under mild conditions a sufficiently unique solution, it is desirable to extend the problem to nonlinear mixtures. Unfortunately the nonlinear case is much more difficult to handle, and brings serious indeterminacies to the solutions in the general case. In this paper we propose a new maximum likelihood approach to the nonlinear BSS problem. It is assumed that the source densities are known and that the mixing mapping is regularized. By finding a regular separating mapping which maximizes the likelihood, we show experimentally that the sources can often be separated.
1665|Ensemble learning in Bayesian neural networks|Bayesian treatments of learning in neural networks are typically based either on a local Gaussian approximation to a mode of the posterior weight distribution, or on Markov chain Monte Carlo simulations. A third approach, called ensemble learning, was introduced by Hinton and van Camp (1993). It aims to approximate the posterior distribution by minimizing the Kullback-Leibler divergence between the true posterior and a parametric approximating distribution. The original derivation of a deterministic algorithm relied on the use of a Gaussian approximating distribution with a diagonal covariance matrix and hence was unable to capture the posterior correlations between parameters. In this chapter we show how the ensemble learning approach can be extended to full-covariance Gaussian distributions while remaining computationally tractable. We also extend the framework to deal with hyperparameters, leading to a simple re-estimation procedure. One of the benefits of our approach is that it yields a strict lower bound on the marginal likelihood, in contrast to other approximate procedures. 1
1666|Feature extraction through LOCOCODE|&#034;Low-complexity coding and decoding&#034; (Lococode) is a novel approach to sensory coding  and unsupervised learning. Unlike previous methods it explicitly takes into account the  information-theoretic complexity of the code generator: it computes lococodes that (1) convey  information about the input data and (2) can be computed and decoded by low-complexity  mappings. We implement Lococode by training autoassociators with Flat Minimum Search,  a recent, general method for discovering low-complexity neural nets. It turns out that this  approach can unmix an unknown number of independent data sources by extracting a minimal  number of low-complexity features necessary for representing the data. Experiments show:  unlike codes obtained with standard autoencoders, lococodes are based on feature detectors,  never unstructured, usually sparse, sometimes factorial or local (depending on statistical  properties of the data). Although Lococode is not explicitly designed to enforce sparse  or factorial codes, it extracts optimal codes for difficult versions of the &#034;bars&#034; benchmark  problem, whereas ICA and PCA do not. It also produces familiar, biologically plausible  feature detectors when applied to real world images. As a preprocessor for a vowel recognition  benchmark problem it sets the stage for excellent classification performance. Our results  reveil an interesting, previously ignored connection between two important fields: regularizer  research, and ICA-related research.
1667|Unsupervised Classification with Non-Gaussian Mixture Models using ICA|We present an unsupervised classification algorithm based on an ICA mixture model. The ICA mixture model assumes that the observed data can be categorized into several mutually exclusive data classes in which the components in each class are generated by a linear mixture of independent sources. The algorithm finds the independent sources, the mixing matrix for each class and also computes the class membership probability for each data point. This approach extends the Gaussian mixture model so that the classes can have non-Gaussian structure. We demonstrate that this method can learn efficient codes to represent images of natural scenes and text. The learned classes of basis functions yield a better approximation of the underlying distributions of the data, and thus can provide greater coding efficiency. We believe that this method is well suited to modeling structure in high-dimensional data and has many potential applications. 1 Introduction  Recently, Blind Source Separation (BSS) by...
1668|Entropy Manipulation of Arbitrary Nonlinear Mappings|We discuss an unsupervised learning method which is driven by an information theoretic based criterion. Information theoretic based learning has been examined by several authors Linsker [2, 3], Bell and Sejnowski [5], Deco and Obradovic [1], and Viola et al [6]. The method we discuss differs from previous work in that it is extensible to a feed-forward multi-layer perceptron with an arbitrary number of layers and makes no assumption about the underlying PDF of the input space. We show a simple unsupervised method by which multi-dimensional signals can be nonlinearly transformed onto a maximum entropy feature space resulting in statistically independent features. 1.0 
1669|Local Linear Independent Component Analysis Based on Clustering|In standard Independent Component Analysis (ICA), a linear data model is used for a global description of the data. Even though linear ICA yields meaningful results in many cases, it can provide a crude approximation only for general nonlinear data distributions. In this paper a new structure is proposed, where local ICA models are used in connection with a suitable grouping algorithm clustering the data. The clustering part is responsible for an overall coarse nonlinear representation of the data, while linear ICA models of each cluster are used for describing local features of the data. The goal is to represent the data better than in linear ICA while avoiding computational difficulties related with nonlinear ICA.
1670|Symplectic nonlinear component analysis|Statistically independent features can be extracted by nding a factorial representation of a signal distribution. Principal Component Analysis (PCA) accomplishes this for linear correlated and Gaussian distributed signals. Independent Component Analysis (ICA), formalized by Comon (1994), extracts features in the case of linear statistical dependent but not necessarily Gaussian distributed signals. Nonlinear Component Analysis nally should nd a factorial representation for nonlinear statistical dependent distributed signals. This paper proposes for this task a novel feed-forward, information conserving, nonlinear map- the explicit symplectic transformations. It also solves the problem of non-Gaussian output distributions by considering single coordinate higher order statistics.
1671|Local Independent Component Analysis Using Clustering|In standard ICA, a linear data model is used for a global description of the data. Even though linear ICA yields meaningful results in many cases, it can provide a crude approximation only for nonlinear data distributions. We propose a new structure, where local ICA models are used in connection with a suitable clustering algorithm grouping the data. The clustering part is responsible for an overall coarse nonlinear representation of the underlying data, while linear ICA models of each cluster are used for describing local features of the data. The goal is to represent the data better than in linear ICA while avoiding computational difficulties related with nonlinear ICA. We discuss connections to existing methods, and present experimental results for natural image data.
1672|Parameterized Complexity| the rapidly developing systematic connections between FPT and useful heuristic algorithms | a new and exciting bridge between the theory of computing and computing in practice.  The organizers of the seminar strongly believe that knowledge of parameterized complexity techniques and results belongs into the toolkit of every algorithm designer. The purpose of the seminar was to bring together leading experts from all over the world, and from the diverse areas of computer science that have been attracted to this new framework. The seminar was intended as the rst larger international meeting with a specic focus on parameterized complexity, and it hopefully serves as a driving force in the development of the eld.  1  We had 49 participants from Australia, Canada, India, Israel, New Zealand, USA, and various European countries. During the workshop 25 lectures were given. Moreover, one night session was devoted to open problems and Thursday was basically used for problem discussion
1673|Motion Planning in the Presence of Moving Obstacles|This paper investigates the computational complexity of planning the motion  of a body B in 2--D or 3--D space, so as to avoid collision with moving obstacles  of known, easily computed, trajectories. Dynamic movement problems  are of fundamental importance to robotics, but their computational complexity  has not previously been investigated.  We provide evidence that the 3-D dynamic movement problem is intractable  even if B has only a constant number of degrees of freedom of movement. In  particular, we prove the problem is PSPACE-hard if B is given a velocity  modulus bound on its movements and is NP hard even if B has no velocity  modulus bound, where in both cases B has 6 degrees of freedom. To prove  these results we use a unique method of simulation of a Turing machine which  uses time to encode configurations (whereas previous lower bound proofs in  robotic motion planning used the system position to encode configurations and  so required unbounded number of degrees of freedom)...
1674|Fixed-parameter tractability and completeness II: On completeness for W[1]|For many fixed-parameter problems that are trivially solvable in polynomial-time, such as k-DOMINATING SET, essentially no better algorithm is presently known than the one which tries all possible solutions. Other problems, such as FEEDBACK VERTEX SET, exhibit fixed-parameter tractability: for each fixed k the problem is solvable in time bounded by a polynomial of degree c, where c is a constant independent of k. In a previous paper, the W Hierarchy of parameterixed problems was defined, and complete problems were identified for the classes W[t] for t &gt;= 2. Our main result shows that INDEPENDENT SET is complete for W[1]. 
1675|Fixed-Parameter TRACTABILITY AND COMPLETENESS I: BASIC Results| For many fixed-parameter problems that are trivially soluable in polynomial time, such as (k-)DOMINATING SET, essentially no better algorithm is presently known than the one which tries all possible solutions. Other problems, such as (k-)FEEDBACK VERTEX SET, exhibit fixed-parameter tractability: for each fixed k the problem is soluable in time bounded by a polynomial of degree c, where c is a constant independent of k. We establish the main results of a completeness program which addresses the apparent fixed-parameter intractability of many parameterized problems. In particular, we define a hierarchy of classes of parameterized problems FPT W[I] c _ W[2] W[SAT] c _ W[P] and identify natural complete problems for W[t] for&gt; 2. (In other papers we have shown many problems complete for W [].) DOMINATING SET is shown to be complete for W[2], and thus is not fixed-parameter tractable unless INDEPENDENT SET, CLIQUE, IRREDUNDANT SET, and many other natural problems in W[2] are also fixed-parameter tractable. We also give a compendium of currently known hardness results as an appendix.
1676|Parameterized Computational Feasibility|Many natural computational problems have input consisting of  two or more parts. For example, the input might consist of a graph  and a positive integer. For many natural problems we may view one  of the inputs as a parameter and study how the complexity of the  problem varies if the parameter is held fixed. For many applications  of computational problems involving such a parameter, only a small  range of parameter values is of practical significance, so that fixedparameter  complexity is a natural concern. In studying the complexity  of such problems, it is therefore important to have a framework in  which we can make qualitative distinctions about the contribution  of the parameter to the complexity of the problem. In this paper  we survey one such framework for investigating parameterized computational  complexity and present a number of new results for this  theory.
1677|Scheduling Split Intervals|We consider the problem of scheduling jobs that are given as groups of non-intersecting segments on the real line. Each job Jj is associated with an interval, Ij,   which consists of up to t segments, for some t _) 1, a of their segments intersect. Such jobs show up in a I.I Problem Statement and Motivation. We wide range of applications, including the transmission consider the problem of scheduling jobs that are given of continuous-media data, allocation of linear resources as groups of non-intersecting segments on the real line. (e.g. bandwidth in linear processor arrays), and in Each job Jj is associated with a t-interval, Ij, which
1678|Subexponential Parameterized Algorithms Collapse the W-hierarchy (Extended Abstract)  (2001) |  It is shown that for essentially all MAX SNP-hard optimization problems finding exact  solutions in subexponential time is not possible unless W [1] = FPT . In particular, we show  that O(2  o(k)  p(n)) parameterized algorithms do not exist for Vertex Cover, Max Cut, Max  c-Sat, and a number of problems on bounded degree graphs such as Dominating Set and  Independent Set, unless W [1] = FPT . Our results are derived via an approach that uses  an extended parameterization of optimization problems and associated techniques to relate the  parameterized complexity of problems in FPT to the parameterized complexity of extended  versions that are W [1]-hard.   
1679|On the parameterized complexity of short computation and factorization|A completeness theory for parameterized computational complexity has been studied in a series of recent papers, and has been shown to have many applications in diverse problem domains including familiar graph-theoretic problems, VLSI layout, games, computational biology, cryptography, and computational learning [ADF,DEF,DF1-7,FH,FHW,FK]. We here study the parameterized complexity of two kinds of problems: (1) problems concerning parameterized computations of Turing machines, such as determining whether a nondeterministic machine can reach an accept state in k steps (the Short TM Computation Problem), and (2) problems concerning derivations and factorizations, such as determining whether a word x can be derived in a grammar G in k steps, or whether a permutation has a factorization of length k over a given set of generators. These include a natural parameterized version of the famous Post Correspondence Systems. We show hardness and completeness for these problems for various levels of the W hierarchy. In particular, we show that Short TM Computation is complete for W [1]. This gives a new and useful characterization of the most important of the apparently intractable parameterized complexity classes. The result could be viewed as one analogue of Cook’s theorem and we believe provides strong evidence for the parameterized intractability of W [1]. 1.
1680|Color coding|network motif counting and discovery
1681|Extremal values of the interval number of a graph|  The interval number i(G) of a simple graph G is the smallest number such that to each vertex in G there can be assigned a collection of at most finite closed intervals on the real line so that there is an edge between vertices v and w in G if and only if some interval for v intersects some interval for w. The well known interval graphs are precisely those graphs G with i(G)=&lt;I. We prove here that for any graph G with maximum degree d, i(G) &lt;- [1/2(d + 1)]. This bound is attained by every regular graph of degree d with no triangles, so is best possible. The degree bound is applied to show that i(G) &lt;- [1/2n] for graphs on n vertices and i(G)&lt;- [/J for graphs with e edges. 
1682|Approximation Algorithms for Hitting Objects with Straight Lines|In the hitting set problem one is given    subsets of a finite set  N and one has to find an X ae N of minimum cardinality that &#034;hits&#034; (intersects)  all of them. The problem is NP-hard. It is not known whether there exists a  polynomial-time approximation algorithm for the hitting set problem with a finite  performance ratio. Special cases of the hitting set problem are described for which  finite performance ratios are guaranteed. These problems arise in a geometric  setting. We consider special cases of the following problem: Given  n  compact  subsets of    , find a set of straight lines of minimum cardinality so that each  of the given subsets is hit by at least one line. The algorithms are based on  several techniques of representing objects bypoints, not necessarily points on the  objects, and solving (in some cases, only approximately) the problem of hitting the  representative points. Finite performance ratios are obtained when the dimension,  the number of types of sets to be hit and the number of directions of the hitting  lines are bounded.
1683|Nonoverlapping Local Alignments (Weighted Independent Sets of Axis Parallel Rectangles)  (1996) |We consider the following problem motivated by an application in computational molecular biology. We are given a set of weighted axis-parallel rectangles such that for any pair of rectangles and either axis, the projection of one rectangle does not enclose that of the other. Define a pair to be independent if their projections in both axes are disjoint. The problem is to find a maximum-weight independent subset of rectangles. We show that the problem is NP-hard even in the uniform case when all the weights are the same. We analyze the performance of a natural local-improvement heuristic for the general problem and prove a performance ratio of 3.25. We extend the heuristic to the problem of finding a maximum-weight independent set in (d+1)- claw free graphs, and show a tight performance ratio of d\Gamma1+  1  d . A performance ratio of  d  2  was known for the heuristic when applied to the uniform case. Our contributions are proving the hardness of the problem and providing a tight anal...
1684|Optimization problems in multipleinterval graphs|Multiple-interval graphs are a natural generalization of interval graphs where each vertex may have more then one interval associated with it. We initiate the study of optimization problems in multiple-interval graphs by considering three classical problems: Minimum Vertex Cover, Minimum Dominating Set, and Maximum Clique. We describe applications for each one of these problems, and then proceed to discuss approximation algorithms for them. Our results can be summarized as follows: Let t be the number of intervals associated with each vertex in a given multiple-interval graph. For Minimum Vertex Cover, we give a (2 - 1/t)-approximation algorithm which also works when a t-interval representation of our given graph is absent. Following this, we give a t 2-approximation algorithm for Minimum Dominating Set which adapts well to more general variants of the problem. We then proceed to prove that Maximum Clique is NP-hard already for 3-interval graphs, and provide a (t 2 -t+ 1)/2-approximation algorithm for general values of t = 2, using bounds proven for the so-called transversal number of t-interval families.
1685|Approximating the 2-interval pattern problem|  We address the problem of approximating the 2-Interval Pattern problem over its various models and restrictions. This problem, which is motivated by RNA secondary structure prediction, asks to find a maximum cardinality subset of a 2-interval set with respect to some prespecified model. For each such model, we give varying approximation quality depending on the different possible restrictions imposed on the input 2-interval set.  
1686|Rounding to an integral program|Abstract. We present a general framework for approximating several NP-hard problems that have two underlying properties in common. First, the problems we consider can be formulated as integer covering programs, possibly with additional side constraints. Second, the number of covering options is restricted in some sense, although this property may be well hidden. Our method is a natural extension of the threshold rounding technique. 1
1687|Parameterized Complexity Analysis in Robot Motion Planning|this paper, using the theory of parameter-
1688|Cyclical scheduling and multi-shift scheduling: complexity and approximation algorithms. Discrete Optimization|We consider the multiple shift scheduling problem modelled as a covering problem. Such problems are characterized by a constraint matrix that has in every column blocks of consecutive ones, each corresponding to a shift. We focus on three type of shift scheduling problems classified according to the column structure in the constraint matrix: consecutive ones columns, cyclical ones columns and k consecutive blocks columns. In particular the complexity of the cyclical scheduling problem, where the matrix satisfies the cyclical 1’s property in each column was noted recently by Hochbaum and Tucker to be open. They further showed that the unit demand case is polynomially solvable. Here we extend this result to the uniform requirements case, and provide a 2-approximation algorithm for the non-uniform case. We also establish that the cyclical scheduling problem’s complexity is equivalent to that of the exact matching problem – a problem the complexity status of which is known to be randomized polynomial, RP. We then investigate the three types of shift scheduling problems and show that while the consecutive ones version is polynomial and the k-block columns is NP-hard (for k = 2), For the k-blocks problem we give a simple k-approximation algorithm, which is the first approximation algorithm determined for the problem. 1
1689|Using fractional primal-dual to schedule split intervals with demands| Given a limited resource (such as bandwidth or memory) we consider the problem of schedul-ing requests from clients that are given as groups of non-intersecting time intervals. Each request j is associated with a demand (the amount of resource required), dj, a t-interval, which consistsof up to t segments, for some t&gt; = 1, and a weight, w(j). A schedule is a collection of requests. Itis feasible if for every time instance s, the total demand of scheduled requests whose t-intervalcontains s does not exceed 1, the amount of resource available. Our goal is to find a feasibleschedule that maximizes the total weight of scheduled requests. This problem generalizes many problems from the literature, and show up in a wide range of applications.We present a 6 t-approximation algorithm for this problem that uses a novel extension of theprimal-dual schema we call fractional primal-dual. A fractional primal-dual algorithm produces a primal solution x, and a dual solution y, whose value divided by r, the approximation ratio,bounds the weight of x. However, y is not a solution of the dual of an LP relaxation of theproblem. The algorithm induces a new LP that has the same objective function as the original problem, but contains inequalities that may not be valid with respect to the original problem. yis a feasible solution of the dual of the new LP. x is r-approximate, since some optimal solutionof an LP relaxation of the original problem is in the feasible set of this new LP. We present a fractional local ratio interpretation of our 6t-approximation algorithm. We alsodiscuss the connection between fractional primal-dual and the fractional local ratio technique. Specifically, we show that the former is the primal-dual manifestation of latter.
1690|  Dotted Interval Graphs and High Throughput Genotyping |We introduce a generalization of interval graphs, which we call dotted interval graphs (DIG). A dotted interval graph is an intersection graph of arithmetic progressions (=dotted intervals). Coloring of dotted intervals graphs naturally arises in the context of high throughput genotyping. We study the properties of dotted interval graphs, with a focus on coloring. We show that any graph is a DIG but that DIGd graphs, i.e. DIGs in which the arithmetic progressions have a jump of at most d, form a strict hierarchy. We show that coloring DIGd graphs is NP-complete even for d = 2. For any fixed d, we provide a 7 8d approximation for the coloring of DIGd graphs.  
1691|Rapid ab initio RNA folding including pseudoknots via graph tree decomposition|The prediction of RNA secondary structure including pseudoknots remains a challenge due to the in-tractable computation of the sequence conformation from nucleotide interactions. Optimal algorithms often assume a restricted class for the predicted RNA structures and yet still require a high-degree polynomial time complexity, which is too expensive to use. Heuristic methods may yield time-efficient algorithms but they do not guarantee optimality of the predicted structure. This paper introduces a new and efficient algorithm for the prediction of RNA structure with pseudoknots for which the structure is not restricted. Novel prediction techniques are developed based on graph tree decomposition. In particular, stem overlapping relationships are defined with a graph, in which a specialized maximum independent set corresponds to the desired optimal structure. Such a graph is tree decomposable; dynamic programming over a tree decomposition of the graph leads to an efficient optimal algorithm. The new algorithm is evaluated on a large number of RNA sequence sets taken from diverse resources. It demonstrates overall sensitivity and specificity that outperforms or is comparable with those of previous optimal and heuristic algorithms yet it requires significantly less time than other optimal algorithms.
1693|Software processes are software too |The major theme of this meeting is the exploration of the importance of.ul process as a vehicle for improving both the quality of software products and the the way in which we develop and evolve them. In beginning this exploration it seems important to spend at least a short time examining the nature of process and convincing ourselves that this is indeed a promising vehicle. We shall take as our elementary notion of a process that it is a systematic approach to the creation of a product or the accomplishment of some task. We observe that this characterization describes the notion of process commonly used in operating systems-- namely that a process is a computational task executing on a single computing device. Our characterization is much broader, however, describing any mechanism used to carry out work or achieve a goal in an orderly way.
1694|DistEdit: A Distributed Toolkit for Supporting Multiple Group Editors|The purpose of our project is to provide toolkits for building applications that support collaboration between people in distributed environments. In this paper, we describe one such toolkit, called DistEdit, that can be used to build interactive group editors for distributed environments. This toolkit has the ability to support different editors simultaneously and provides a high degree of fault-tolerance against machine crashes. To evaluate the toolkit, we modified two editors to make use of the toolkit. The resulting editors allow users to take turns at making changes while other users observe the changes as they occur. We give an evaluation of the toolkit based on the development and use of these editors.
1695|Computer Support for COOPERATIVE DESIGN|Computer support for design as cooperative work is the subject of our discussion in the context of our research program on Computer Support in Cooperative Design and Communication. We outline our theoretical perspective on design as cooperative work, and we exemplify our approach with reflections from a project on computer support for envisionment in design -- the APLEX and its use. We see envisionment facilities as support for both experiments with and communication about the future use situation. As a background we sketch the historical roots of our program -- the Scandinavian collective resource approach to design and use of computer artifacts, and make some critical reflections on the rationality of computer support for cooperative work.
1696|Prefix B-trees|Two modifications of B-trees are described, simple prefix B-trees and prefix B-trees. Both store only parts of keys, namely prefixes, in the index part of a B*-tree. In simple prefix B-trees those prefixes are selected carefully to minimize their length. In prefix B-trees the pre-fixes need not he fully stored, but are reconstructed as the tree is searched. Prefix B-trees are designed to combine some of the advantages of B-trees, digital search trees, and key compres-sion techniques while reducing the processing overhead of compression techniques.
1697|The C Programming Language|The C programming language was devised in the early 1970s as a system implementation language for the nascent Unix operating system. Derived from the typeless language BCPL, it evolved a type structure; created on a tiny machine as a tool to improve a meager programming environment, it has become one of the dominant languages of today. This paper studies its evolution.
1698|The Unix Time-Sharing System|Unix is a general-purpose, multi-user, interactive operating system for the larger Digital Equipment Corporation PDP-11 and the Interdata 8/32 computers. It offers a number of features seldom found even in larger operating systems, including i A hierarchical file system incorporating demountable volumes, ii Compatible file, device, and inter-process I/O, iii The ability to initiate asynchronous processes, iv System command language selectable on a per-user basis, v Over 100 subsystems including a dozen languages, vi High degree of portability. This paper discusses the nature and implementation of the file system and of the user command interface. I.
1699|Yacc: Yet Another Compiler-Compiler|Computer program input generally has some structure; in fact, every computer program
1700|Lint, a C Program Checker|Lint is a command which examines C source programs, detecting a number of bugs and obscurities. It enforces the type rules of C more strictly than the C compilers. It may also be used to enforce a number of portability restrictions involved in moving programs between different machines and/or operating systems. Another option detects a number of wasteful, or error prone, constructions which nevertheless are, strictly speaking, legal.  Lint accepts multiple input files and library specifications, and checks them for consistency. The separation of function between lint and the C compilers has both historical and practical rationale. The compilers turn C programs into executable files rapidly and efficiently. This is possible in part because the compilers do not do sophisticated type checking, especially between separately compiled programs. Lint takes a more global, leisurely view of the program, looking much more carefully at the compatibilities.  This document discusses the use of lint...
1701|Mentoring programs|ABSTRACT: The changing environment in the nonprofit sector has sub-jected microenterprise programs to a new paradigm that emphasizes rationality principles. These principles ask practitioners to increase their outcomes while minimizing costs and to demonstrate that they are doing so with outcome-assessment measurements. This paper pre-sents a case study of what happened to 11 microenterprise programs that adopted outcome assessment. Factors affecting the adoption of outcome assessment were changing norms in the nonprofit sector, demands from state legislators for information on program outcomes, and mandates from funders. A funding formula was implemented; pro-gram responses included going along, adopting practices to fit the for-mula, embracing outcome assessment as a way towards program improvement, and possibly eliminating ineffective programs. Unintended consequences and ways to avoid them are discussed. What is the impact of an increasingly rationalized, fiscally
1702|Portability of C Programs and the UNIX System|Computer programs are portable to the extent that they can be moved to new computing environments with much less effort than it would take to rewrite them. In the limit, a program is perfectly portable if it can be moved at will with no change whatsoever. Recent C language extensions have made it easier to write portable programs. Some tools have also been developed that aid in the detection of nonportable constructions. With these tools many programs have been moved from the PDP-11 on which they were developed to other machines. In particular, the UNIX operating system and most of its software have been transported to the Interdata 8/32. The source-language representation of most of the code involved is identical in all environments. 
1703|Managing Gigabytes: Compressing and Indexing Documents and Images - Errata|&gt; ! &#034;GZip&#034; page 64, Table 2.5, line &#034;progp&#034;: &#034;43,379&#034; ! &#034;49,379&#034; page 68, Table 2.6: &#034;Mbyte/sec&#034; ! &#034;Mbyte/min&#034; twice in the body of the table, and in the caption &#034;Mbyte/second&#034; ! &#034;Mbyte/minute&#034;  page 70, para 4, line 5: &#034;Santos&#034; ! &#034;Santis&#034; page 71, line 11: &#034;Fiala and Greene (1989)&#034; ! &#034;Fiala and Green (1989)&#034;  Chapter Three  page 89, para starting &#034;Using this method&#034;, line 2: &#034;hapax legomena &#034; !  &#034;hapax legomenon &#034; page 96, line 5: &#034;a such a&#034; ! &#034;such a&#034; page 98, line 6: &#034;shows that in fact none is an answer to this query&#034; !  &#034;shows that only document 2 is an answer to this query&#034; page 106, para 3, line 9: &#034;the bitstring in Figure 3.7b&#034; ! &#034;the bitstring in Figure 3.7c&#034; page 107, Figure 3.7: The coding shown in part (c) cannot be decoded ambiguously. For example, the sequence &#034;1010 0000 0001 0000
1704|Linked Data -- The story so far  |The term Linked Data refers to a set of best practices for publishing and connecting structured data on the Web. These best practices have been adopted by an increasing number of data providers over the last three years, leading to the creation of a global data space containing billions of assertions- the Web of Data. In this article we present the concept and technical principles of Linked Data, and situate these within the broader context of related technological developments. We describe progress to date in publishing Linked Data on the Web, review applications that have been developed to exploit the Web of Data, and map out a research agenda for the Linked Data community as it moves forward.
1705|Duplicate record detection: A survey|Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a dif cult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats or any combination of these factors. In this article, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar eld entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the ef ciency and scalability of approximate duplicate detection algorithms. We conclude with a coverage of existing tools and with a brief discussion of the big open problems in the area.  
1706|Sindice.com: A document-oriented lookup index for open linked data |Developers of Semantic Web applications face a challenge with respect to the decentralised publication model: how and where to find statements about encountered resources. The “linked data” approach mandates that resource URIs should be de-referenced to return resource metadata. But for data discovery linkage itself is not enough, and crawling and indexing of data is necessary. Existing Semantic Web search engines are focused on database-like functionality, compromising on index size, query performance and live updates. We present Sindice, a lookup index over resources crawled on the Semantic Web. Our index allows applications to automatically locate documents containing information about a given resource. In addition, we allow resource retrieval through uniquely identifying inverse-functional properties, offer a full-text search and index SPARQL endpoints. Finally we introduce an extension to the sitemap protocol which allows us to efficiently index large Semantic Web datasets with minimal impact on the data providers.
1707|Querying Distributed RDF Data Sources with SPARQL|Abstract. Integrated access to multiple distributed and autonomous RDF data sources is a key challenge for many semantic web applications. As a reaction to this challenge, SPARQL, the W3C Recommendation for an RDF query language, supports querying of multiple RDF graphs. However, the current standard does not provide transparent query federation, which makes query formulation hard and lengthy. Furthermore, current implementations of SPARQL load all RDF graphs mentioned in a query to the local machine. This usually incurs a large overhead in network traffic, and sometimes is simply impossible for technical or legal reasons. To overcome these problems we present DARQ, an engine for federated SPARQL queries. DARQ provides transparent query access to multiple SPARQL services, i.e., it gives the user the impression to query one single RDF graph despite the real data being distributed on the web. A service description language enables the query engine to decompose a query into sub-queries, each of which can be answered by an individual service. DARQ also uses query rewriting and cost-based query optimization to speed-up query execution. Experiments show that these optimizations significantly improve query performance even when only a very limited amount of statistical information is available. DARQ is available under GPL License at
1708|Principles of dataspace systems|The most acute information management challenges today stem from organizations relying on a large number of diverse, interrelated data sources, but having no means of managing them in a convenient, integrated, or principled fashion. These challenges arise in enterprise and government data management, digital libraries, “smart ” homes and personal information management. We have proposed dataspaces as a data management abstraction for these diverse applications and DataSpace Support Platforms (DSSPs) as systems that should be built to provide the required services over dataspaces. Unlike data integration systems, DSSPs do not require full semantic integration of the sources in order to provide useful services. This paper lays out specific technical challenges to realizing DSSPs and ties them to existing work in our field. We focus on query answering in DSSPs, the DSSP’s ability to introspect on its content, and the use of human attention to enhance the semantic relationships in a dataspace.  
1710|Triplify -- Light-Weight Linked Data Publication from Relational Databases|In this paper we present Triplify – a simplistic but effective approach to publish Linked Data from relational databases. Triplify is based on mapping HTTP-URI requests onto relational database queries. Triplify transforms the resulting relations into RDF statements and publishes the data on the Web in various RDF serializations, in particular as Linked Data. The rationale for developing Triplify is that the largest part of information on the Web is already stored in structured form, often as data contained in relational databases, but usually published by Web applications only as HTML mixing structure, layout and content. In order to reveal the pure structured information behind the current Web, we have implemented Triplify as a light-weight software component, which can be easily integrated into and deployed by the numerous, widely installed Web applications. Our approach includes a method for publishing update logs to enable incremental crawling of linked data sources. Triplify is complemented by a library of configurations for common relational schemata and a REST-enabled data source registry. Triplify configurations containing mappings are provided for many popular Web applications, including osCommerce, WordPress, Drupal, Gallery, and phpBB. We will show that despite its light-weight architecture Triplify is usable to publish very large datasets, such as 160GB of geo data from the OpenStreetMap project.
1711|Named Graphs|The Semantic Web consists of many RDF graphs nameable by URIs. This paper extends the syntax and semantics of RDF to cover such named graphs. This enables RDF statements that describe graphs, which is beneficial in many Semantic Web application areas. Named graphs are given an abstract syntax, a formal semantics, an XML syntax, and a syntax based on N3. SPARQL is a query language applicable to named graphs. A specific application area discussed in detail is that of describing provenance information. This paper provides a formally defined framework suited to being a foundation for the Semantic Web trust layer.
1712|Bootstrapping pay-as-you-go data integration systems|Data integration systems offer a uniform interface to a set of data sources. Despite recent progress, setting up and maintaining a data integration application still requires significant upfront effort of creating a mediated schema and semantic mappings from the data sources to the mediated schema. Many application contexts involving multiple data sources (e.g., the web, personal information management, enterprise intranets) do not require full integration in order to provide useful services, motivating a pay-as-you-go approach to integration. With that approach, a system starts with very few (or inaccurate) semantic mappings and these mappings are improved over time as deemed necessary. This paper describes the first completely self-configuring data integration system. The goal of our work is to investigate how advanced of a starting point we can provide a pay-as-you-go system. Our system is based on the new concept of a probabilistic mediated schema that is automatically created from the data sources. We automatically create probabilistic schema mappings between the sources and the mediated schema. We describe experiments in multiple domains, including 50-800 data sources, and show that our system is able to produce high-quality answers with no human intervention.
1713|Provenance Information in the Web of Data|The openness of the Web and the ease to combine linked data from different sources creates new challenges. Systems that consume linked data must evaluate quality and trustworthiness of the data. A common approach for data quality assessment is the analysis of provenance information. For this reason, this paper discusses provenance of data on the Web and proposes a suitable provenance model. While traditional provenance research usually addresses the creation of data, our provenance model also represents data access, a dimension of provenance that is particularly relevant in the context of Web data. Based on our model we identify options to obtain provenance information and we raise open questions concerning the publication of provenance-related metadata for linked data on the Web.
1714|Automatic Interlinking of Music Datasets on the Semantic Web |In this paper, we describe current efforts towards interlinking music-related datasets on the Web. We first explain some initial interlinking experiences, and the poor results obtained by taking a naïve approach. We then detail a particular interlinking algorithm, taking into account both the similarities of web resources and of their neighbours. We detail the application of this algorithm in two contexts: to link a Creative Commons music dataset to an editorial one, and to link a personal music collection to corresponding web identifiers. The latter provides a user with personally meaningful entry points for exploring the web of data, and we conclude by describing some concrete tools built to generate and use such links.
1715|The Open Provenance Model|The Open Provenance Model (OPM) is a community-driven data model for Provenance that is designed to support inter-operability of provenance technology. Underpinning OPM, is a notion of directed acyclic graph, used to represent data products and processes involved in past computations, and causal dependencies between these. The Open Provenance Model was derived following two “Provenance Challenges”, international, multidisciplinary activities trying to investigate how to exchange information between multiple systems supporting provenance and how to query it. The OPM design was mostly driven by practical and pragmatic considerations, and is being tested in a third Provenance Challenge, which has just started. The purpose of this paper is to investigate the theoretical foundations of this data model. The formalisation consists of a set-theoretic definition of the data model, a definition of the inferences by transitive closure that are permitted, a formal description of how the model can be used to express dependencies in past computations, and finally, a description of the kind of time-based inferences that are supported. A novel element that OPM introduces is the concept of an account, by which multiple descriptions of a same execution are allowed to co-exist in a same graph. Our formalisation gives a precise meaning to such accounts and associated notions of alternate and refinement. Warning It was decided that this paper should be released as early as possible since it brings useful clarifications on the Open Provenance Model, and therefore can benefit the Provenance Challenge 3 community. The reader should recognise that this paper is however an early draft, and several sections are incomplete. Additionally, figures rely on colours but these may be difficult to read when printed in a black and white. It is advisable to print the paper in colour. 1 1
1716|Which Semantic Web?|Through scenarios in the popular press and technical papers in the research literature, the promise of the Semantic Web has raised a number of different expectations. These expectations can be traced to three different perspectives on the Semantic Web. The Semantic Web is portrayed as: (1) a universal library, to be readily accessed and used by humans in a variety of information use contexts; (2) the backdrop for the work of computational agents completing sophisticated activities on behalf of their human counterparts; and (3) a method for federating particular knowledge bases and databases to perform anticipated tasks for humans and their agents. Each of these perspectives has both theoretical and pragmatic entailments, and a wealth of past experiences to guide and temper our expectations. In this paper, we examine all three perspectives from rhetorical, theoretical, and pragmatic viewpoints with an eye toward possible outcomes as Semantic Web efforts move forward.
1717|M.: Linked movie data base|The Linked Movie Database (LinkedMDB) project provides a demonstration of the first open linked dataset connecting several major existing (and highly popular) movie web resources. The database exposed by LinkedMDB contains millions of RDF triples with hundreds of thousands of RDF links to existing web data sources that are part of the growing Linking Open Data cloud, as well as to popular movierelated web pages such as IMDb. LinkedMDB uses a novel way of creating and maintaining large quantities of high quality links by employing state-of-the-art approximate join techniques for finding links, and providing additional RDF metadata about the quality of the links and the techniques used for deriving them.
1718|A Framework for Semantic Link Discovery over Relational Data |In this paper, we present a framework for online discovery of semantic links from relational data. Our framework is based on declarative specification of the linkage requirements by the user, that allows matching data items in many real-world scenarios. These requirements are translated to queries that can run over the relational data source, potentially using the semantic knowledge to enhance the accuracy of link discovery. Our framework lets data publishers to easily find and publish high-quality links to other data sources, and therefore could significantly enhance the value of the data in the next generation of web.
1719|How will we interact with the web of data|The Semantic Web is a global information space of linked data, designed not for human use but for consumption by machines. Right? Well, yes and no. It&#039;s true to say that machine-readable data,  given explicit semantics and published online,  coupled with the ability to link data in distributed data sets are the key selling points of the Semantic Web. Together, these features allow aggregation and integration of heterogeneous data on an unprecedented scale,  and machines will do the grunt work for us. However, without a human being somewhere in this process, to reap the rewards of these new capabilities, the endeavour is meaningless. Far from removing human beings from the equation, a Web of machine-readable data creates significant challenges and significant opportunities for human-computer interaction. To date,  the Semantic Web community has mostly been busy developing the technical infrastructure to make the Web of Data feasible in principle and on publishing linked data sets in order to make it a reality. If we are to fully exploit the challenges and opportunities of a Web of Data from a human perspective,  we need to move beyond the initial phase and work to understand how this changes the user interaction paradigm of the Web.
1720|What is the Size of the Semantic Web|Abstract: When attempting to build a scaleable Semantic Web application, one has to know about the size of the Semantic Web. In order to be able to understand the characteristics of the Semantic Web, we examined an interlinked dataset acting as a representative proxy for the Semantic Web at large. Our main finding was that regarding the size of the Semantic Web, there is more than the sheer number of triples; the number and type of links is an equally crucial measure.
1721|Tabulator Redux: Browsing and Writing Linked Data |second frame shows information within that source expanded, the third frame shows another source within that source expanded, and finally, the last frame shows that the label of that source has been edited from “Music and artist data interlinked ” to “Music and artist data linked on the Semantic Web” A first category of Semantic Web browsers was designed to present a given dataset (an RDF graph) for perusal in various forms. These include mSpace, Exhibit, and to a certain extent
1722|Integration of semantically annotated data by the knofuss architecture|Abstract. Most of the existing work on information integration in the Semantic Web concentrates on resolving schema-level problems. Specific issues of data-level integration (instance coreferencing, conflict resolu-tion, handling uncertainty) are usually tackled by applying the same techniques as for ontology schema matching or by reusing the solutions produced in the database domain. However, data structured according to OWL ontologies has its specific features: e.g., the classes are organized into a hierarchy, the properties are inherited, data constraints differ from those defined by database schema. This paper describes how these fea-tures are exploited in our architecture KnoFuss, designed to support data-level integration of semantic annotations. 1
1723|DBpedia Mobile - A Location-Aware Semantic Web Client|Abstract. DBpedia Mobile is a location-aware client for the Semantic Web that can be used on an iPhone and other mobile devices. Based on the current GPS position of a mobile device, DBpedia Mobile renders a map indicating nearby locations from the DBpedia dataset. Starting from this map, the user can explore background information about his surroundings by navigating along data links into other Web data sources. DBpedia Mobile has been designed for the use case of a tourist exploring a city. As the application is not restricted to a fixed set of data sources but can retrieve and display data from arbitrary Web data sources, DBpedia Mobile can also be employed within other use cases, including ones un-foreseen by its developers. Besides accessing Web data, DBpedia Mobile also enables users to publish their current location, pictures and reviews to the Semantic Web so that they can be used by other Semantic Web applications. Instead of simply being tagged with geographical coordi-nates, published content is interlinked with a nearby DBpedia resource and thus contributes to the overall richness of the Geospatial Semantic Web.
1724|Information-seeking on the Web with Trusted Social Networks – from Theory to Systems|This research investigates how synergies between the Web and social networks can enhance the process of obtaining relevant and trustworthy information. A review of literature on personalised search, social search, recommender systems, social networks and trust propagation reveals limitations of existing technology in areas such as relevance, collaboration, task-adaptivity and trust. In response to these limitations I present a Web-based approach to information-seeking using social networks. This approach takes a source-centric perspective on the information-seeking process, aiming to identify trustworthy sources of relevant information from within the user&#039;s social network. An empirical study of source-selection decisions in information- and recommendationseeking identified five factors that influence the choice of source, and its perceived trustworthiness. The priority given to each of these factors was found to vary according to the criticality and subjectivity of the task. A series of algorithms have been developed that operationalise three of these factors (expertise, experience, affinity) and generate from various data sources a number of trust metrics for use in social network-based information seeking. The most significant of these data sources is Revyu.com, a reviewing and rating Web site implemented as part of this research, that takes input from regular users and makes it available on the Semantic Web for easy re-use by the implemented algorithms. Output of the algorithms is used in Hoonoh.com, a Semantic Web-based system that has been developed to support users in identifying relevant and trustworthy information   sources within their social networks. Evaluation of this system&#039;s ability to predict source selections showed more promising results for the experience factor than for expertise or affinity. This may be attributed to the greater demands these two factors place in terms of input data. Limitations of the work and opportunities for future research are discussed.  
1725|Wrapper Induction for Information Extraction|The Internet presents numerous sources of useful information---telephone directories, product catalogs, stock quotes, weather forecasts, etc. Recently, many systems have been built that automatically gather and manipulate such information on a user&#039;s behalf. However, these resources are usually formatted for use by people (e.g., the relevant content is embedded in HTML pages), so extracting their content is difficult. Wrappers are often used for this purpose. A wrapper is a procedure for extracting a particular resource&#039;s content. Unfortunately, hand-coding wrappers is tedious. We introduce wrapper induction, a technique for automatically constructing wrappers. Our techniques can be described in terms of three main contributions. First, we pose the problem of wrapper construction as one of inductive learn...
1726|Learnability  and the Vapnik-Chervonenkis dimension| Valiant’s learnability model is extended to learning classes of concepts defined by regions in Euclidean space E”. The methods in this paper lead to a unified treatment of some of Valiant’s results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufftcient conditions are provided for feasible learnability. 
1727|The TSIMMIS Project: Integration of Heterogeneous Information Sources |The goal of the Tsimmis Project is to develop tools that facilitate the rapid integration of heterogeneous information sources that may include both structured and unstructured data. This paper gives an overview of the project, describing components that extract properties from unstructured objects, that translate information into a common object model, that combine information from several sources, that allow browsing of information, and that manage constraints across heterogeneous sites. Tsimmis is a joint project between Stanford and the IBM Almaden Research Center.
1728|An Introduction to Software Agents|ion and delegation: Agents can be made extensible and composable in ways that common iconic interface objects cannot. Because we can &#034;communicate&#034; with them, they can share our goals, rather than simply process our commands. They can show us how to do things and tell us what went wrong (Miller and Neches 1987). . Flexibility and opportunism: Because they can be instructed at the level of 16 BRADSHAW goals and strategies, agents can find ways to &#034;work around&#034; unforeseen problems and exploit new opportunities as they help solve problems. . Task orientation: Agents can be designed to take the context of the person&#039;s tasks and situation into account as they present information and take action. . Adaptivity: Agents can use learning algorithms to continually improve their behavior by noticing recurrent patterns of actions and events. Toward Agent-Enabled System Architectures In the future, assistant agents at the user interface and resource-managing agents behind the scenes will increas...
1729|Inference of reversible languages|  A famdy of efficient algorithms for referring certain subclasses of the regular languages from fmtte posttwe samples is presented These subclasses are the k-reversible languages, for k = 0, 1, 2,.... For each k there is an algorithm for finding the smallest k-reversible language containing any fimte posluve sample. It ts shown how to use this algorithm to do correct identification m the ILmlt of the k-reversible languages from posmve data A reversible language is one that Is k-reverstble for some k _ _ 0. An efficient algonthrn is presented for mfernng reversible languages from posmve and negative examples, and it is shown that it leads to correct identification m the hmlt of the class of reversible languages. Numerous examples are gtven to illustrate the algorithms and their behavior
1730|The Information Manifold|We describe the Information Manifold (IM), a system for browsing and querying of multiple networked information sources. As a first contribution, the system demonstrates the viability of knowledge representation technology for retrieval and organization of information from disparate (structured and unstructured) information sources. Such an organization allows the user to pose high-level queries that use data from multiple information sources. As a second contribution, we describe novel query processing algorithms used to combine information from multiple sources. In particular, our algorithms are guaranteed to find exactly the set of information sources relevant to a query, and to  completely exploit knowledge about local closed world information (Etzioni et al. 1994). Introduction  We are currently witnessing an explosion in the amount of information that is available online. For example, the rapid rise in popularity of the World Wide Web (WWW) has increased the amount of information...
1731|Semi-automatic Wrapper Generation for Internet Information Sources|To simplify the task of obtaining information from the vast number of information sources that are available on the World Wide Web (WWW), we are building tools to build information mediators for extracting and integrating data from multiple Web sources. In a mediator based approach, wrappers are built around individual information sources, that provide translation between the mediator query language and the individual source. We present an approach for semi-automatically generating wrappers for structured internet sources. The key idea is to exploit formatting information in Web pages from the source to hypothesize the underlying structure of a page. From this structure the system generates a wrapper that facilitates querying of a source and possibly integrating it with other sources. We demonstrate the ease with which we are able to build wrappers for a number of Web sources using our implemented wrapper generation toolkit.  1. Introduction  We are building information agents or media...
1732|Kqml: A language and protocol for knowledge and information exchange|Abstract. This paper describes the design of and experimentation with the Knowledge Query and Manipulation Language (KQML), a new language and protocol for exchanging information and knowledge. This work is part a larger effort, the ARPA Knowledge Sharing Effort which is aimed at developing techniques and methodology for building large-scale knowledge bases which are sharable and reusable. KQML is both a message format and a message-handling protocol to support run-time knowledge sharing among agents. KQML can be used as a language for an application program to interact with an intelligent system or for two or more intelligent systems to share knowledge in support of cooperative problem solving. KQML focuses on an extensible set of performatives, which defines the permissible operations that agents may attempt on each other’s knowledge and goal stores. The performatives comprise a substrate on which to develop higher-level models of inter-agent interaction such as contract nets and negotiation. In addition, KQML provides a basic architecture for knowledge sharing through a special class of agent called communication facilitators which coordinate the interactions of other agents The ideas which underlie the evolving design of KQML are currently being explored through experimental prototype systems which are being used to support several testbeds in such areas as concurrent engineering, intelligent design and intelligent planning and scheduling.
1733|Towards Heterogeneous Multimedia Information Systems: The Garlic Approach|Abstract: We provide an overview of the Garlic project, a new project at the IBM Almaden Research Center. The goal of this project is to develop a system and associated tools for the management of large quantities of heterogeneous multimedia information. Garlic permits traditional and multimedia data to be
1734|Intelligence without Robots (A Reply to Brooks)  (1993) |In his recent papers, entitled &#034;Intelligence without Representation and &#034;Intelligence without Reason,&#034; Brooks argues for studying complete agents in real-world environments and for mobile robots as the foundation for AI research. This article argues that, even if we seek to investigate complete agents in real-world environments, robotics is neither necessary nor sufficient as a basis for AI research. The article proposes real-world software environments, such as operating systems or databases, as a complementary substrate for intelligent-agents research, and considers the relative advantages of software environments as testbeds for AI. First, the cost, effort, and expertise necessary to develop and systematically experiment with software artifacts are relatively low. Second, software environments circumvent many thorny, but peripheral, research issues that are inescapable in physical environments. Brooks&#039;s mobile robots tug AI towards a bottom-up focus in which the mechanics of percept...
1735|Building Softbots for UNIX (Preliminary Report)  (1992) |AI is moving away from &#034;toy tasks&#034; such as block stacking towards real-world problems. This trend is positive, but the amount of preliminary groundwork required to tackle a real-world task can be staggering, particularly when developing an integrated agent architecture. To address this problem, we advicate real-world software environments, such as operating systems or databases, as domains for agent research. The cost, effort, and expertise required to develop and systematically experiment with software agents is relatively low. Furthermore, software environments circumvent many thorny, but peripheral, research issues that are inescapable in other environments. Thus, software environments enable us to test agents ina real world yet focus on core AI research issues. To support this claim, we describe our project to develop UNIX softbots (software robots) -- intelligent agnets that interact with UNIX. Existing softbots accept a diverse set of high-level goals, generate and execute plans to achieve these goals in real time, and recover from errors when necessary.
1736|Learning to Query the Web|The World Wide Web (WWW) is filled with &#034;resource directories&#034;---i.e., documents that collect together links to all known documents on a specific topic. Keeping resource directories up-to-date is difficult because of the rapid growth in online documents. We propose using machine learning methods to address this problem. In particular, we propose to treat a resource directory as a list of positive examples of an unknown concept, and then use machine learning methods to construct from these examples a definition of the unknown concept. If the learned definition is in the appropriate form, it can be translated into a query, or series of queries, for a WWW search engine. This query can be used at a later date to detect any new instances of the concept. We present experimental results with two implemented systems, and two learning methods. One system is interactive, and is implemented as an augmented WWW browser; the other is a batch system, which can collect and label documents without an...
1737|Using Natural Language Processing for Identifying and Interpreting Tables in Plain Text|Characteristics of Tables 2.1 Table Denotations A simple table in some abstract sense denotes a relation among sets of values. 2 (We can think of this as corresponding to relations or views in relational database terms. See, for example, [Ull88]) In these terms, the underlying representation of a table is a set of n-tuples, where n is the number of domains or value sets in the table. The n-tuples of the table is that subset of the cross-product D 1 \Theta D 2 \Theta : : : \Theta Dn for which the relation holds. This is the canonical form of the table. For our nlp application, we need a relatively detailed ontology or world model of the objects of the sublanguage of construction. The classes of this world model we call world domains; they include 1 We thank Stephen McCarron of BICC. We acknowledge the support of the Department of Trade and Industry, the Engineering and Physical Sciences Research Council, and the BICC Group on the CISAU project (IED4/1/5818), and the Human Communi...
1738|Towards Sophisticated Wrapping of Web-based Information Repositories|Access to on-line information via the Web is exploding. Index and retrieval engines already start to integrate a huge variety of heterogeneous repositories. However, the heterogeneity issue remains, both in terms of the search formats and the formats of the result pages. In this paper,
1739|On Learning from Noisy and Incomplete Examples|We investigate learnability in the PAC model when the data used for learning, attributes and labels, is either corrupted or incomplete. In order to prove our main results, we define a new complexity measure on statistical query (SQ) learning algorithms. The view of an SQ algorithm is the maximumover all queries in the algorithm, of the number of input bits on which the query depends. We show that a restricted view SQ algorithm for a class is a general sufficient condition for learnability in both the models of attribute noise and covered (or missing) attributes. We further show that since the algorithms in question are statistical, they can also simultaneously tolerate classification noise. Classes for which these results hold, and can therefore be learned with simultaneous attribute noise and classification noise, include  k-DNF, k-term-DNF by DNF representations, conjunctions with few relevant variables, and over the uniform distribution, decision lists. These noise models are the fi...
1740|The SWISS-MODEL Workspace: A web-based environment for protein structure homology modelling|Motivation: Homology models of proteins are of great interest for planning and analyzing biological experiments when no experimental three-dimensional structures are available. Building homology models requires specialized programs and up-to-date sequence and structural databases. Integrating all required tools, programs and databases into a single web-based workspace facilitates access to homology modelling from a computer with web connection without the need of downloading and installing large program packages and databases. Results: SWISS-MODEL Workspace is a web-based integrated service dedicated to protein structure homology modelling. It assists and guides the user in building protein homology models at different levels of complexity. A personal working environment is provided for each user where several modelling projects can be carried out in parallel. Protein sequence and structure databases necessary for modelling are accessible from the workspace and are updated in regular intervals. Tools for template selection, model building, and structure quality evaluation can be invoked from within the workspace. Workflow and usage of the workspace are illustrated by modelling human Cyclin A1 and human Transmembrane Protease
1741|Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features|structure
1743|Hidden Markov models for detecting remote protein homologies|A new hidden Markov model method (SAM-T98) for nding remote homologs of protein sequences is described and evaluated. The method begins with a single target sequence and iteratively builds a hidden Markov model (hmm) from the sequence and homologs found using the hmm for database search. SAM-T98 is also used to construct model libraries automatically from sequences in structural databases. We evaluate the SAM-T98 method with four datasets. Three of the test sets are fold-recognition tests, where the correct answers are determined by structural similarity. The fourth uses a curated database. The method is compared against wu-blastp and against double-blast, a two-step method similar to ISS, but using blast instead of fasta. Results SAM-T98 had the fewest errors in all tests| dramatically so for the fold-recognition tests. At the minimum-error point on the SCOP-domains test, SAM-T98 got 880 true positives and 68 false positives, double-blast got 533 true positives with 71 false positives, and wu-blastp got 353 true positives with 24 false positives. The method is optimized to recognize superfamilies, and would require parameter adjustment to be used to nd family or fold relationships. One key to the performance of the hmm method is a new score-normalization technique that compares the score to the score with a reversed model rather than to a uniform null model. Availability A World Wide Web server, as well as information on obtaining the Sequence Alignment and PREPRINT to appear in Bioinformatics, 1999
1744|SWISSMODEL: an automated protein homology-modeling server|SWISS-MODEL
1745|Twilight Zone of Protein Sequence Alignments|l findings are applicable to automatic database searches.  Keywords: alignment quality analysis/evolutionary conservation/ genome analysis/protein sequence alignment/sequence space hopping  Introduction  Protein sequence alignments in twilight zone  Protein sequences fold into unique three-dimensional (3D) structures. However, proteins with similar sequences adopt similar structures (Zuckerkandl and Pauling, 1965; Doolittle, 1981; Doolittle, 1986; Chothia and Lesk, 1986). Indeed, most protein pairs with more than 30 out of 100 identical residues were found to be structurally similar (Sander and Schneider, 1991). This high robustness of structures with respect to residue exchanges explains partly the robustness of organisms with respect to gene-replication errors, and it allows for the variety in evolution (Zuckerkandl and Pauling, 1965; Zuckerkandl, 1976; Doolittle, 1979, 1986). Structure alignments have uncovered homologous protein pairs with less than 10% pairwise sequence identity (
1746|SCOP database in 2004: refinements integrate structure and sequence family data|The Structural Classication of Proteins (SCOP) database is a comprehensive ordering of all proteins of known structure, according to their evolutionary and structural relationships. Protein domains in SCOP are hierarchically classied into families, superfamilies, folds and classes. The continual accumulation of sequence and structural data allows more rigorous analysis and provides important information for understanding the protein world and its evolutionary repertoire. SCOP participates in a project that aims to rationalize and integrate the data on proteins held in several sequence and structure databases. As part of this project, starting with release 1.63, we have initiated a renement of the SCOP classication, which introduces a number of changes mostly at the levels below superfamily. The pending SCOP reclassication will be carried out gradually through a number of future releases. In addition to the expanded set of static links to external resources, available at the level of domain entries, we have started modernization of the interface capabilities of SCOP allowing more dynamic links with other databases. SCOP can be accessed at http://scop.mrc-lmb.cam.ac.uk/scop.
1747|Hidden Markov models for sequence analysis: extension and analysis of the basic method|Hidden Markov models (HMMs) are a highly effective means of modeling a family of unaligned sequences or a common motif within a set of unaligned sequences. The trained HMM can then be used for discrimination or multiple alignment. The basic mathematical description of an HMM and its expectation-maximization training procedure is relatively straight-forward. In this paper, we review the mathematical extensions and heuristics that move the method from the theoretical to the practical. Then, we experimentally analyze the effectiveness of model regularization, dynamic model modification, and optimization strategies. Finally it is demonstrated on the SH2 domain how a domain can be found from unaligned sequences using a special model type. The experimental work was completed with the aid of the Sequence Alignment and Modeling software suite. 1 Introduction  Since their introduction to the computational biology community (Haussler et al., 1993; Krogh et al., 1994a), hidden Markov models (HMMs...
1748|PDBsum more: new summaries and analyses of the known 3D structures of proteins and nucleic acids|PDBsum is a database of mainly pictorial summaries of the 3D structures of proteins and nucleic acids in the Protein Data Bank. Its pages aim to provide an at-aglance view of the contents of every 3D structure, plus detailed structural analyses of each protein chain, DNA–RNA chain and any bound ligands and metals. In the past year, the database has been significantly improved, in terms of both appearance and new content. Moreover, it has moved to its new address at
1749|The Protein Data Bank and structural genomics|The Protein Data Bank (PDB;
1750|Intrinsic disorder in cell-signaling and cancer-associated proteins|The dominating concept that protein structure determines protein function is undergoing Abbreviations used: CDK, cyclin-dependent kinase; eIF4E, translation initiation factor (eIF) 4F; EU_SW,
1751|Protein distance constraints predicted by neural networks and probability density functions. Prot|3To whom correspondence should be addressed We predict interatomic Ca distances by two independent data driven methods. The first method uses statistically derived probability distributions of the pairwise distance between two amino acids, whilst the latter method consists of a neural network prediction approach equipped with windows taking the context of the two residues into account. These two methods are used to predict whether distances in independent test sets were above or below given thresholds. We investigate which distance thresholds produce the most information-rich constraints and, in turn, the optimal performance of the two methods. The predictions are based on a data set derived using a new threshold which defines when sequence similarity implies structural similarity. We show that distances in proteins are predicted more accurately by neural networks than by probability density functions. We show that the accuracy of the predictions can be further increased by using sequence profiles. A threading method based on the predicted distances is presented. A homepage with software, predictions and data related to this paper is available at
1752|EVA: evaluation of protein structure prediction servers|EVA
1755|Regularization and variable selection via the Elastic Net|Summary. We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.
1756|On Model Selection Consistency of Lasso|Sparsity or parsimony of statistical models is crucial for their proper interpretations, as  in sciences and social sciences. Model selection is a commonly used method to find such  models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani,  1996) is now being used as a computationally feasible alternative to model selection.
1757|Leave-One-Out Support Vector Machines|We present a new learning algorithm for pattern recognition inspired by a recent upper bound on leave--one--out error [ Jaakkola and Haussler, 1999 ] proved for Support Vector Machines (SVMs) [ Vapnik, 1995; 1998 ] . The new approach directly minimizes the expression given by the bound in an attempt to minimize leave--one--out error. This gives a convex optimization problem which constructs a sparse linear classifier in feature space using the kernel technique. As such the algorithm possesses many of the same properties as SVMs. The main novelty of the algorithm is that apart from the choice of kernel, it is parameterless -- the selection of the number of training errors is inherent in the algorithm and not chosen by an extra free parameter as in SVMs. First experiments using the method on benchmark datasets from the UCI repository show results similar to SVMs which have been tuned to have the best choice of parameter.  1 Introduction  Support Vector Machines (SVMs), motivated by minim...
1758|Sparse Principal Component Analysis|Principal component analysis (PCA) is widely used in data processing and dimensionality  reduction. However, PCA su#ers from the fact that each principal component is a linear combination  of all the original variables, thus it is often di#cult to interpret the results. We introduce  a new method called sparse principal component analysis (SPCA) using the lasso (elastic net)  to produce modified principal components with sparse loadings. We show that PCA can be  formulated as a regression-type optimization problem, then sparse loadings are obtained by imposing  the lasso (elastic net) constraint on the regression coe#cients. E#cient algorithms are  proposed to realize SPCA for both regular multivariate data and gene expression arrays. We  also give a new formula to compute the total variance of modified principal components. As  illustrations, SPCA is applied to real and simulated data, and the results are encouraging.
1759|Boosting with early stopping: convergence and consistency|Abstract Boosting is one of the most significant advances in machine learning for classification and regression. In its original and computationally flexible version, boosting seeks to minimize empirically a loss function in a greedy fashion. The resulted estimator takes an additive function form and is built iteratively by applying a base estimator (or learner) to updated samples depending on the previous iterations. An unusual regularization technique, early stopping, is employed based on CV or a test set. This paper studies numerical convergence, consistency, and statistical rates of convergence of boosting with early stopping, when it is carried out over the linear span of a family of basis functions. For general loss functions, we prove the convergence of boosting&#039;s greedy optimization to the infinimum of the loss function over the linear span. Using the numerical convergence result, we find early stopping strategies under which boosting is shown to be consistent based on iid samples, and we obtain bounds on the rates of convergence for boosting estimators. Simulation studies are also presented to illustrate the relevance of our theoretical results for providing insights to practical aspects of boosting. As a side product, these results also reveal the importance of restricting the greedy search step sizes, as known in practice through the works of Friedman and others. Moreover, our results lead to a rigorous proof that for a linearly separable problem, AdaBoost with ffl! 0 stepsize becomes an L1-margin maximizer when left to run to convergence. 1 Introduction In this paper we consider boosting algorithms for classification and regression. These algorithms present one of the major progresses in machine learning. In their original version, the computational aspect is explicitly specified as part of the estimator/algorithm. That is, the empirical minimization of an appropriate loss function is carried out in a greedy fashion, which means that at each step, a basis function that leads to the largest reduction of empirical risk is added into the estimator. This specification distinguishes boosting from other statistical procedures which are defined by an empirical minimization of a loss function without the numerical optimization details.
1760|Empty alternation|Abstract. We introduce the notion of empty alternation by investigating alternating automata which are restricted to empty their storage except for a logarithmically space-bounded tape before making an alternating transition. In particular, we consider the cases when the depth of alternation is bounded by a constant or a polylogarithmic function. In this way we get new characterizations of the classes AC k, SAC k and P using a push-down store and new characterizations of the class T P 2 using Turing tapes. 1
1761|Principled Disambiguation: Discriminating Adjective Senses with . . .|... In this paper we argue for a linguistically principled approach to disambiguation, in which relevant contextual clues are narrowly defined, in syntactic and semantic terms, and in which only highly reliable clues are exploited. Statistical methods play a definite role in this work, helping to organize and analyze data, but the disambiguation method itself does not employ statistical data or decision criteria. This approach results in improved understanding of the disambiguation problem both in general and on a word-specific basis and leads to broadly applicable and nearly errorless clues to word sense. The approach is illustrated by an experiment discriminating among the senses of adjectives, which have been relatively neglected in work on sense disambiguation. In particular, the paper assesses the potential of nouns for discriminating among the senses of adjectives that modify them. This assessment is based on an empirical study of five of the most frequent ambiguous adjectives in English: hard, light, old, right, and short. About three-quarters of all instances of these adjectives can be disambiguated almost errorlessly by the nouns they modify or by the syntactic constructions in which they occur. Such disambiguation requires only simple rules, which can be automated easily. Furthermore, a small number of semantic attributes supply a compact means of representing the noun clues in a very few rules. Clues other than nouns are required when modified nouns are not useable. The sense of an ambiguous modified noun may be needed to determine the relevant semantic attribute for disambiguation of a target adjective; and other adjectives, verbs, and grammatical constructions all show evidence of high reliability, and sometimes of high applicability, when they stand in specific, ...
1762|Projection Pursuit Regression|A new method for nonparametric multiple regression is presented. The procedure models the regression surface as a sum of general- smooth functions of linear combinations of the predictor variables in an iterative manner. It is more general than standard stepwise and stagewise regression procedures, does not require the definition of a metric in the predictor space, and lends itself to graphi-cal interpretation.
1763|A tree-structured approach to nonparametric multiple regression|In the nonparametric regression problem, one is given a set of vector valued variables 3 (termed carriers) and with each an associated scalar quantity Y (termed the response). This set of carriers and associated responses CYi,Xil (l&lt;i&lt;N) is termed the training sample. In addition (usually at some later time), one is given another set of vector
1764|Using information content to evaluate semantic similarity in a taxonomy|philip.resnikfleast.sun.com This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content. Experimental evaluation suggests that the measure performs encouragingly well (a correlation of r = 0.79 with a benchmark set of human similarity judgments, with an upper bound of r = 0.90 for human subjects performing the same task), and significantly better than the traditional edge counting approach (r = 0.66). 1
1765|SELECTION AND INFORMATION: A CLASS-BASED APPROACH TO LEXICAL RELATIONSHIPS|Selectional constraints are limitations on the applicability of predicates to arguments. For example, the statement “The number two is blue” may be syntactically well formed, but at some level it is anomalous — BLUE is not a predicate that can be applied to numbers. According to the influential theory of (Katz and Fodor, 1964), a predicate associates a set of defining features with each argument, expressed within a restricted semantic vocabulary. Despite the persistence of this theory, however, there is widespread agreement about its empirical shortcomings (McCawley, 1968; Fodor, 1977). As an alternative, some critics of the Katz-Fodor theory (e.g. (Johnson-Laird, 1983)) have abandoned the treatment of selectional constraints as semantic, instead treating them as indistinguishable from inferences made on the basis of factual knowledge. This provides a better match for the empirical phenomena, but it opens up a different problem: if selectional constraints are the same as inferences in general, then accounting for them will require a much more complete understanding of knowledge representation and inference than we have at present. The problem, then, is this: how can a theory of selectional constraints be elaborated without first having either an empirically adequate theory of defining features or a comprehensive theory of inference? In this dissertation, I suggest that an answer to this question lies in the representation of conceptual
1766|Disambiguating Noun Groupings with Respect to WordNet Senses|Word groupings useful for language processing tasks are increasingly available, as thesauri appear  on-line, and as distributional word clustering techniques improve. However, for many tasks, one is  interested in relationships among word senses, not words. This paper presents a method for automatic  sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in  on-line thesauri, or as the output of distributional clustering algorithms. Disambiguation is performed  with respect to WordNet senses, which are fairly fine-grained; however, the method also permits the  assiment of higher-level WordNet categories rather than sense labels. The method is illustrated  primarily by example, though results of a more rigorous evaluation are also presented.
1767|Semantic classes and syntactic ambiguity|resnik @ linc.cis.upenn.edu In this paper we propose to define selectional preference and semantic similarity as information-theoretic relationships involving conceptual classes, and we demonstrate the applicability of these definitions to the resolution of syntactic ambiguity. The space of classes is defined using WordNet [8], and conceptual relationships are determined by means of statistical analysis using parsed text in the Penn Treebank. 1.
1768|Learning probabilistic relational models|A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with &amp;quot;flat &amp;quot; data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning — the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases. 1
1769|Scale-free characteristics of random networks: The topology of the world-wide web|The world-wide web forms a large directed graph, whose vertices are documents and edges are links pointing from one document to another. Here we demonstrate that despite its apparent random character, the topology of this graph has a number of universal scale-free characteristics. We introduce a model that leads to a scale-free network, capturing in a minimal fashion the self-organization processes governing the world-wide web. 
1771|Data Mining for Direct Marketing: Problems and Solutions|Direct marketing is a process of identifying likely buyers of certain products and promoting the products accordingly. It is increasingly used by banks, insurance companies, and the retail industry. Data mining can provide an effective tool for direct marketing. During data mining, several specific problems arise. For example, the class distribution is extremely imbalanced (the response rate is about 1~), the predictive accuracy is no longer suitable for evaluating learning methods, and the number of examples can be too large. In this paper, we discuss methods of coping with these problems based on our experience on direct-marketing projects using data mining. 1
1772|ReferralWeb: Combining Social Networks and Collaborative Filtering|This paper appears in the Communications of the ACM,
1773|Cobot in LambdaMOO: A social statistics agent|We describe our development of Cobot, a software agent who lives in LambdaMOO, a popular virtual world frequented by hundreds of users. We present a detailed discussion of the functionality that has made him one of the objects most frequently interacted with in LambdaMOO, human or artificial.
1774|A decision theoretic approach to targeted advertising|A simple advertising strategy that can be used to help increase sales of a product is to mail out special o ers to selected potential customers. Because there is a cost associated with sending each o er, the optimal mailing strategy depends on both the bene t obtained from a purchase and how the o er a ects the buying behavior of the customers. In this paper, we describe two methods for partitioning the potential customers into groups, and show howto perform a simple cost-bene t analysis to decide which, if any, of the groups should be targeted. In particular, weconsidertwodecision-tree learning algorithms. The rst is an \o the shelf &#034; algorithm used to model the probability that groups of customers will buy the product. The second is a new algorithm that is similar to the rst, except that for each group, it explicitly models the probability of purchase under the two mailing scenarios: (1) the mail is sent tomembers of that group and (2) the mail is not sent to members of that group. Using data from a real-world advertising experiment, we compare the algorithms to each other and to a naive mail-to-all strategy. 1
1775|A theory of timed automata|Model checking is emerging as a practical tool for automated debugging of complex reactive systems such as embedded controllers and network protocols (see [23] for a survey). Traditional techniques for model checking do not admit an explicit modeling of time, and are thus, unsuitable for analysis of real-time systems whose correctness depends on relative magnitudes of different delays. Consequently, timed automata [7] were introduced as a formal notation to model the behavior of real-time systems. Its definition provides a simple way to annotate state-transition graphs with timing constraints using finitely many real-valued clock variables. Automated analysis of timed automata relies on the construction of a finite quotient of the infinite space of clock valuations. Over the years, the formalism has been extensively studied leading to many results establishing connections to circuits and logic, and much progress has been made in developing verification algorithms, heuristics, and tools. This paper provides a survey of the theory of timed automata, and their role in specification and verification of real-time systems.
1776|Design and Synthesis of Synchronization Skeletons Using Branching Time Temporal Logic|We propose a method of constructing concurrent programs in which the synchroni-zation skeleton of the program ~s automatically synthesized from a high-level (branching time) Temporal Logic specification. The synchronization skeleton is an abstraction of the actual program where detail irrelevant to synchronization is
1777|The algorithmic analysis of hybrid systems|We present a general framework for the formal specification and algorithmic analysis of hybrid systems. A hybrid system consists of a discrete program with an analog environment. We model hybrid systems as nite automata equipped with variables that evolve continuously with time according to dynamical laws. For verification purposes, we restrict ourselves to linear hybrid systems, where all variables follow piecewise-linear trajectories. We provide decidability and undecidability results for classes of linear hybrid systems, and we show that standard program-analysis techniques can be adapted to linear hybrid systems. In particular, we consider symbolic model-checking and minimization procedures that are based on the reachability analysis of an infinite state space. The procedures iteratively compute state sets that are definable as unions of convex polyhedra in multidimensional real space. We also present approximation techniques for dealing with systems for which the iterative procedures do not converge.  
1778|The Theory of Hybrid Automata|A hybrid automaton is a formal model for a mixed discrete-continuous system. We classify hybrid automata acoording to what questions about their behavior can be answered algorithmically. The classification reveals structure on mixed discrete-continuous state spaces that was previously studied on purely discrete state spaces only. In particular, various classes of hybrid automata induce finitary trace equivalence (or similarity, or bisimilarity) relations on an uncountable state space, thus permitting the application of various model-checking techniques that were originally developed for finite-state systems.
1779|UPPAAL in a Nutshell|. This paper presents the overall structure, the design criteria, and the main features of the tool box Uppaal. It gives a detailed user guide which describes how to use the various tools of Uppaal version 2.02 to construct  abstract models of a real-time system, to simulate  its dynamical behavior, to specify and verify its safety and bounded liveness properties in terms of its model. In addition, the paper also provides a short review on case-studies where Uppaal is applied, as well as references to its theoretical foundation. 1 Introduction Uppaal is a tool box for modeling, simulation and verification of real-time systems, based on constraint--solving and on-the-fly techniques, developed jointly by Uppsala University and Aalborg University. It is appropriate for systems that can be modeled as a collection of nondeterministic processes with finite control structure and real-valued clocks, communicating through channels and (or) shared variables [34, 26]. Typical application areas in...
1780|Symbolic Model Checking for Real-time Systems|We describe finite-state programs over real-numbered time in a guarded-command  language with real-valued clocks or, equivalently, as finite automata with  real-valued clocks. Model checking answers the question which states of a real-time  program satisfy a branching-time specification (given in an extension of CTL with clock  variables). We develop an algorithm that computes this set of states symbolically as a  fixpoint of a functional on state predicates, without constructing the state space.  For this purpose, we introduce a -calculus on computation trees over real-numbered  time. Unfortunately, many standard program properties, such as response for all  nonzeno execution sequences (during which time diverges), cannot be characterized  by fixpoints: we show that the expressiveness of the timed -calculus is incomparable  to the expressiveness of timed CTL. Fortunately, this result does not impair the  symbolic verification of &#034;implementable&#034; real-time programs---those whose safety...
1781|What&#039;s Decidable about Hybrid Automata?|. Hybrid automata model systems with both digital and analog components, such as embedded control programs. Many verification tasks for such programs can be expressed as reachability problems for hybrid automata. By improving on previous decidability and undecidability results, we identify a boundary between decidability and undecidability for the reachability problem of hybrid automata. On the positive side, we give an (optimal) PSPACE reachability algorithm for the case of initialized rectangular automata, where all analog variables follow independent trajectories within piecewise-linear envelopes and are reinitialized whenever the envelope changes. Our algorithm is based on the construction of a timed automaton that contains all reachability information about a given initialized rectangular automaton. The translation has practical significance for verification, because it guarantees the termination of symbolic procedures for the reachability analysis of initialized rectangular autom...
1782|Model-Checking in Dense Real-time|Model-checking is a method of verifying concurrent systems in which a state-transition graph model of the system behavior is compared with a temporal logic formula. This paper extends model-checking for the branching-time logic CTL to the analysis of real-time systems, whose correctness depends on the magnitudes of the timing delays. For specifications, we extend the syntax of CTL to allow quantitative temporal operators such as 93!5 , meaning &#034;possibly within 5 time units.&#034; The formulas of the resulting logic, Timed CTL  (TCTL), are interpreted over continuous computation trees, trees in which paths are maps from the set of nonnegative reals to system states. To model finitestate systems we introduce timed graphs --- state-transition graphs annotated with timing constraints. As our main result, we develop an algorithm for model-checking, for determining the truth of a TCTL-formula with respect to a timed graph. We argue that choosing a dense domain instead of a discrete domain to mo...
1783|Automatic Symbolic Verification of Embedded Systems|We present a model-checking procedure and its implementation for the automatic verification of embedded systems. The system components are described as Hybrid Automata -- communicating machines with finite control and real-valued variables that represent continuous environment parameters such as time, pressure, and temperature. The system requirements are specified in a temporal logic with stop watches, and verified by symbolic fixpoint computation. The verification procedure -- implemented in the Cornell Hybrid Technology Tool, HyTech -- applies to hybrid automata whose continuous dynamics is governed by linear constraints on the variables and their derivatives. We illustrate the method and the tool by checking safety, liveness, time-bounded, and duration requirements of digital controllers, schedulers, and distributed algorithms.
1784|The Tool KRONOS|KRONOS [6, 8] is a tool developed with the aim to assist the user to validate complex real-time systems. The tool checks whether a real-tinae system modeled by a timed automaton [4] satisfies a timing property specified by a formula of the temporal logic TCTL [3]. KRONOS implements the symbolic model-checking
1785|The Benefits of Relaxing Punctuality|The most natural, compositional, way of modeling real-time systems uses a dense domain for time. The satis ability of timing constraints that are capable of expressing punctuality in this model, however, is known to be undecidable. We introduce a temporal language that can constrain the time difference between events only with finite, yet arbitrary, precision and show the resulting logic to be EXPSPACE-complete. This result allows us to develop an algorithm for the verification of timing properties of real-time systems with a dense semantics.
1786|An Old-Fashioned Recipe for Real Time|this paper appeared in ACM Transactions on Programming Languages and Systems 16, 5 (September 1994) 1543-- 1571. The appendix was published electronically by the ACM.  Contents
1787|What Good Are Digital Clocks?|. Real-time systems operate in &#034;real,&#034; continuous time  and state changes may occur at any real-numbered time point. Yet  many verification methods are based on the assumption that states  are observed at integer time points only. What can we conclude if a  real-time system has been shown &#034;correct&#034; for integral observations?  Integer time verification techniques suffice if the problem of whether  all real-numbered behaviors of a system satisfy a property can be  reduced to the question of whether the integral observations satisfy a  (possibly modified) property. We show that this reduction is possible  for a large and important class of systems and properties: the class of  systems includes all systems that can be modeled as timed transition  systems; the class of properties includes time-bounded invariance  and time-bounded response.  1 Introduction  Over the past few years, we have seen a proliferation of formal methodologies for software and hardware design that emphasize the treatm...
1789|HYTECH: The next generation|Abstract. We describe a new implementation of HyTech 1,asymbolic model checker for hybrid systems. Given a parametric description of an embedded system as a collection of communicating automata, HyTech automatically computes the conditions on the parameters under which the system satis es its safety and timing requirements. While the original HyTech prototype was based on the symbolic algebra tool Mathematica, the new implementation is written in C ++ and builds on geometric algorithms instead of formula manipulation. The new HyTech o ers a cleaner and more expressive input language, greater portability, superior performance (typically two to three orders of magnitude), and new features such as diagnostic error-trace generation. We illustrate the e ectiveness of the new implementation by applying HyTech to the automatic parametric analysis of the generic railroad crossing benchmark problem [HJL93] and to an active structure control algorithm [ECB94]. 1
1790|Event-Clock Automata: A Determinizable Class of Timed Automata|We introduce event-recording automata. An event-recording automaton is a timed automaton that contains, for every event a, a clock that records the time of the last occurrence of a. The class of event-recording automata is, on one hand, expressive enough to model (finite) timed transition systems and, on the other hand, determinizable and closed under all boolean operations. As a result, the language inclusion problem is decidable for event-recording automata. We present a translation from timed transition systems to event-recording automata, which leads to an algorithm for checking if two timed transition systems have the same set of timed behaviors. We also consider event-predicting automata, which contain clocks that predict the time of the next occurrence of an event. The class of event-clock automata, which contain both event-recording and event-predicting clocks, is a suitable specification language for real-time properties. We provide an algorithm for checking if a timed automa...
1791|Computer-aided verification|How can a computer program developer ensure that a program actually implements its intended purpose? This article describes a method for checking the correctness of certain types of computer programs. The method is used commercially in the development of programs implemented as integrated circuits and is applicable to the development of “control-intensive ” software programs as well. “Divide-and-conquer ” techniques central to this method apply to a broad range of program verification methodologies. Classical methods for testing and quality control no longer are sufficient to protect us from communication network collapses, fatalities from medical machinery malfunction, rocket guidance failure, or a half-billion dollar commercial loss due to incorrect arithmetic in a popular integrated circuit. These sensational examples are only the headline cases. Behind them are multitudes of mundane programs whose failures merely infuriate their users and cause increased costs to their producers. A source of such problems is the growth in program complexity. The more a program controls, the more types of interactions it supports. For example, the telephone “call-forwarding ” service (forwarding incoming calls to a customer-designated number) interacts with the “billing ” program that must determine whether the forwarding number or the calling number gets charged for the additional connection to the customer-designated number. At the same time, call-forwarding interacts with the “connection ” program that deals with the issue of
1792|Model-checking for Probabilistic Real-time Systems|Model-checking is a method of verifying concurrent systems in which a state-graph model of the system behavior is compared with a temporal logic formula. This paper extends modelchecking to stochastic real-time systems, whose behavior depends on probabilistic choice and quantitative time. The specification language is TCTL, a branching-time temporal logic for expressing real-time properties. We interpret the formulas of the logic over generalized semiMarkov processes. Our model can express constraints like &#034;the delay between the request and the response is distributed uniformly between 2 to 4 seconds&#034;. We present an algorithm that combines model-checking for real-time non-probabilistic systems with model-checking for finite-state discrete-time Markov chains. The correctness of the algorithm is not obvious, because it analyzes the projection of a Markov process onto a finite state space. The projection process is not Markov, so our most significant result is that the model-checking algo...
1794|Liveness in Timed and Untimed Systems|When proving the correctness of algorithms in distributed systems, one generally considers safety conditions and liveness conditions. The Input/Output (I/O) automaton model and its timed version have been used successfully, but have focused on safety conditions and on a restricted form of liveness called fairness. In this paper we develop a new I/O automaton model, and a new timed I/O automaton model, that permit the verification of general liveness properties on the basis of existing verification techniques. Our models include a notion of environment-freedom which generalizes the idea of receptiveness of other existing formalisms, and enables the use of compositional verification techniques.
1795|Time-Constrained Automata|) Michael Merritt  AT&amp;T Bell Laboratories 600 Mountain Avenue Murray Hill, NJ 07974 merritt@research.att.com  Francesmary Modugno  School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 fmm@cs.cmu.edu  Mark R. Tuttle  DEC Cambridge Research Lab One Kendall Sq., Bldg. 700 Cambridge, MA 02139 tuttle@crl.dec.com Abstract  In this paper, we augment the input-output automaton model in order to reason about time in concurrent systems, and we prove simple properties of this augmentation. The input-output automata model is a useful model for reasoning about computation in concurrent and distributed systems because it allows fundamental properties such as fairness and compositionality to be expressed easily and naturally. A unique property of the model is that systems are modeled as the composition of  autonomous components. This paper describes a way to add a notion of time to the model in a way that preserves these properties. The result is a simple, compositional model fo...
1796|Modeling Urgency in Timed Systems|this paper and is the object of an ongoing work. Here, we restrict our attention to 1-safe TPNs.
1797|Modularity for Timed and Hybrid Systems| In a trace-based world, the modular specification, verification,  and control of live systems require each module to be receptive;  that is, each module must be able to meet its liveness assumptions no  matter how the other modules behave. In a real-time world, liveness is  automatically present in the form of diverging time. The receptiveness  condition, then, translates to the requirement that a module must be  able to let time diverge no matter how the environment behaves. We  study the receptiveness condition for real-time systems by extending the  model of reactive modules to timed and hybrid modules. We define the  receptiveness of such a module as the existence of a winning strategy in  a game of the module against its environment. By solving the game on  region graphs, we present an (optimal) Exptime algorithm for checking  the receptiveness of propositional timed modules. By giving a fixpoint  characterization of the game, we present a symbolic procedure for checking  the re...
1798|Efficient Timed Reachability Analysis using Clock Difference Diagrams|One of the major problems in applying automatic verification tools to industrial-size systems is the excessive amount of memory required during the state-space exploration of a model. In the setting of real-time, this problem of state-explosion requires extra attention as information must be kept not only on the discrete control structure but also on the values of continuous clock variables. In this
1799|A Kleene Theorem for Timed Automata|In this paper we define timed regular expressions, an extension of regular expressions for specifying sets of densetime discrete-valued signals. We show that this formalism is equivalent in expressive power to the timed automata of Alur and Dill by providing a translation procedure from expressions to automata and vice versa. The result is extended to !-regular expressions (B uchi&#039;s theorem). 1. Introduction  Timed automata, i.e. automata equipped with clocks [AD94], have been studied extensively in recent years as they provide a rigorous model for reasoning about the quantitative temporal aspects of systems. Together with realtime logics and process algebras they constitute the underlying theoretical basis for the specification and verification of real-time systems. Kleene&#039;s theorem [K56], stating that the regular (or rational) subsets of \Sigma    are exactly the recognizable ones (those accepted by finite automata), is one of the cornerstones of automata theory. No such theorem has ...
1800|Some progress in the symbolic verification of timed automata| In this paper we discuss the practical difficulty of analyzing the behavior of timed automata and report some results obtained using an experimental bdd-based extension of kronos. We have treated examples originating from timing analysis of asynchronous boolean networks and CMOS circuits with delay uncertainties and the results outperform those obtained by previous implementations of timed automata verification tools. 
1801|Timing Verification by Successive Approximation|We present an algorithm for verifying that a model M with timing constraints satisfies a given temporal property T . The model M is given as a parallel composition of !-automata P i , where each automaton P i is constrained by bounds on delays. The property T is given as an !-automaton as well, and the verification problem is posed as a language inclusion question L(M ) ` L(T ). In constructing the composition M of the constrained automata P i , one needs to rule out the behaviors that are inconsistent with the delay bounds, and this step is (provably) computationally expensive. We propose an iterative solution which involves generating successive approximations M j to M , with containment L(M ) ` L(M j ) and monotone convergence L(M j ) ! L(M ) within a bounded number of steps. As the succession progresses, the approximations M j become more complex. At any step of the iteration one may get a proof or a counterexample to the original language inclusion question. The described algori...
1802|It’s about Time: Real-time Logics Reviewed| We summarize and reorganize some of the last decade&#039;s research on real-time extensions of temporal logic. Our main focus is on tableau constructions for model checking linear temporal formulas with timing constraints. In particular, we find that a great deal of real-time verification can be performed in polynomial space, but also that considerable care must be exercised in order to keep the real-time verification problem in polynomial space, or even decidable.
1803|The Regular Real-Time Languages|. A specification formalism for reactive systems defines a class of !-languages. We call a specification formalism fully decidable if it is constructively closed under boolean operations and has a decidable satisfiability (nonemptiness) problem. There are two important, robust classes of !-languages that are definable by fully decidable formalisms. The !-regular languages are definable by finite automata, or equivalently, by the Sequential Calculus. The counter-free !-regular languages are definable by temporal logic, or equivalently, by the first-order fragment of the Sequential Calculus. The gap between both classes can be closed by finite counting (using automata connectives), or equivalently, by projection (existential second-order quantification over letters).  A specification formalism for real-time systems defines a class of timed !-languages, whose letters have real-numbered time stamps. Two popular ways of specifying timing constraints rely on the use of clocks, and on the use...
1804|Timing Analysis in COSPAN|. We describe how to model and verify real-time systems using the formal verification tool Cospan. The verifier supports automatatheoretic verification of coordinating processes with timing constraints. We discuss different heuristics, and our experiences with the tool for certain benchmark problems appearing in the verification literature. 1 Introduction  Model checking is a method of automatically verifying concurrent systems in which a finite-state model of a system is compared with a correctness requirement. This method has been shown to be very effective in detecting errors in high-level designs, and has been implemented in various tools. We consider the tool Cospan that is based on the theory of !-automata (!-automata are finite automata accepting infinite sequences, see [Tho90] for a survey, and [VW86, Kur94] for applications to verification). The system to be verified is modeled as a collection of coordinating processes described in the language S/R [Kur94]. The semantics of su...
1805|Time Abstracted Bisimulation: Implicit Specifications and Decidability|ed Bisimulation: Implicit Specifications and Decidability   Kim G. Larsen  y  and Yi Wang  z January 8, 1997 Abstract  In the last few years a number of real--time process calculi have emerged with the purpose of capturing important quantitative aspects of real--time systems. In addition, a number of process equivalences sensitive to time--quantities have been proposed, among these the notion of timed (bisimulation) equivalence in In this paper, we introduce a time--abstracting (bisimulation) equivalence, and investigate its properties with respect to the real--time process calculus of [Wan90]. Seemingly, such an equivalence would yield very little information (if any) about the timing properties of a process. However, time--abstracted reasoning about a composite process may yield important information about the relative timing--properties of the components of the system. In fact, we show as a main theorem that such implicit reasoning will reveal all timing aspects of a process. More p...
1806|Verifying Abstractions of Timed Systems|ions of Timed Systems  Serdar Ta¸siran  ?  Rajeev Alur  ??  Robert P. Kurshan  ??  Robert K. Brayton  ?  Abstract. Given two descriptions of a real-time system at different levels of abstraction, we consider the problem of proving that the refined representation is a correct implementation of the abstract one. To avoid the complexity of building a representation for the refined system in its entirety, we develop a compositional framework for the implementation check to be carried out in a module-by-module manner using assumeguarantee style proof rules. On the algorithmic side, we show that the problem of checking for timed simulation relations, a sufficient condition for correct implementation, is decidable. We study state homomorphisms as a way of specifying a correspondence between two modules. We present an algorithm for checking if a given mapping is a homomorphism preserving timed behaviors. We have implemented this check in the verifier  Cospan, and applied our method to the comp...
1807|The Observational Power of Clocks|We develop a theory of equivalences for timed systems. Two systems are equivalent iff external observers cannot observe differences in their behavior. The notion of equivalence depends, therefore, on the distinguishing power of the observers. The power of an observer to measure time results in untimed, clock, and timed equivalences: an untimed observer cannot measure the time difference between events; a clock observer uses a clock to measure time differences with finite precision; a timed observer is able to measure time differences with arbitrary precision. We show that the distinguishing power of clock observers grows with the number of observers, and approaches, in the limit, the distinguishing power of a timed observer. More precisely, given any equivalence for untimed systems, two timed systems are k-clock congruent, for a nonnegative integer k, iff their compositions with every environment that uses k clocks are untimed equivalent. Both k-clock bisimulation congruence and k-cloc...
1809|Analysis of Timed Systems Based on Time-Abstracting Bisimulations|. We adapt a generic minimal model generation algorithm to compute the coarsest finite model of the underlying infinite transition system of a timed automaton. This model is minimal modulo a timeabstracting bisimulation. Our algorithm uses a refinement method that avoids set complementation, and is considerably more efficient than previous ones. We use the constructed minimal model for verification purposes by defining abstraction criteria that allow to further reduce the model and to compare it to a specification. 1 Introduction  Behavioral equivalences based on bisimulation relations have proven useful for verifying the correctness of concurrent systems. They allow comparing an implementation to a usually more abstract specification both represented as labeled transition systems. This approach also allows reducing the size of the system by identifying equivalent states which is crucial to avoid the explosion of the state-space. Since the introduction of strong bisimulation in [Mil80]...
1810|Model Checking of Real-Time Systems: A Telecommunications Application|We describe the application of model checking tools to analyze a real-time software challenge in the design of Lucent Technologies&#039; 5ESS telephone switching system. We use two tools: COSPAN for checking real-time properties, and TPWB for checking probabilistic specifications. We report on the feedback given by the tools, and based on our experience, discuss the advantages and the limitations of the approach used.
1811|STARI: A Case Study in Compositional and Hierarchical Timing Verification|. In [TAKB96], we investigated techniques for checking if one real-time system correctly implements another and developed theory for hierarchical proofs and assume-guarantee style reasoning. In this study, using the techniques of [TAKB96], we verify the correctness of the timing of the communication chip STARI.  1 Introduction  We describe the application of the techniques and tools described in [TAKB96] to the verification of the high-bandwidth communication chip, STARI [G93]. STARI (by Greenstreet, [G93, G96]) is a self-timed FIFO that interfaces a transmitter and a receiver that operate at the same clock frequency but may have some skew between their clock signals (Figure 1). STARI can compensate for large, time varying skews and makes high bandwidth synchronous operation possible by eliminating the need for handshakes between the transmitter and the receiver. However, because there are no handshakes, certain timing properties need to be verified to show that the interface functions...
1812|State Minimization for Concurrent System Analysis Based on State Space Exploration|A fundamental issue in the automated analysis of concurrent systems is the efficient generation of the reachable state space. Since it is not possible to explore all the reachable states of a system if the number of states is very large or infinite, we need to develop techniques for minimizing the state space. This paper presents our approach to cluster subsets of states into equivalent classes. We assume that concurrent systems are specified as communicating state machines with arbitrary data space. We describe a procedure for constructing a minimal reachability state graph from communicating state machines. As an illustration of our approach, we analyze a producer-consumer program written in Ada. 1 Introduction  One of the most prohibitive barriers in automatic analysis based on state space exploration of a concurrent system is state explosion [4, 13]. Two major sources of state explosion are process composition and data space size. The state space of a system grows exponentially wit...
1813|The case for motivated reasoning|It is proposed that motivation may affect reasoning through reliance on a biased set of cognitive processes—that is, strategies for accessing, constructing, and evaluating beliefs. The motivation to be accurate enhances use of those beliefs and strategies that are considered most appropriate, whereas the motivation to arrive at particular conclusions enhances use of those that are considered most likely to yield the desired conclusion. There is considerable evidence that people are more likely to arrive at conclusions that they want to arrive at, but their ability to do so is constrained by their ability to construct seemingly reasonable justifications for these conclusions. These ideas can account for a wide variety of research concerned with motivated reasoning. The notion that goals or motives affect reasoning has a long and controversial history in social psychology. The propositions that motives may affect perceptions (Erdelyi, 1974), attitudes (Festinger, 1957), and attributions (Heider, 1958) have been put forth by some psychologists and challenged by others. Al-though early researchers and theorists took it for granted that motivation may cause people to make self-serving attributions
1814|Illusion and well-being: A social psychological perspective on mental health|Many prominent theorists have argued that accurate perceptions of the self, the world, and the future are essential for mental health. Yet considerable research evidence suggests that overly positive selfevaluations, exaggerated perceptions of control or mastery, and unrealistic optimism are characteristic of normal human thought. Moreover, these illusions appear to promote other criteria of mental health, including the ability to care about others, the ability to be happy or contented, and the ability to engage in productive and creative work. These strategies may succeed, in large part, because both the social world and cognitive-processing mechanisms impose filters on incoming information that distort it in a positive direction; negative information may be isolated and represented in as unthreatening a manner as possible. These positive illusions may be especially useful when an individual receives negative feedback or is otherwise threatened and may be especially adaptive under these circumstances. Decades of psychological wisdom have established contact with reality as a hallmark of mental health. In this view, the wcU-adjusted person is thought to engage in accurate reality testing, whereas the individual whose vision is clouded by illusion is regarded as vulnerable to, if not already a victim of, mental illness. Despite its plausibility, this viewpoint is increasingly difficult to maintain (cf. Lazarus, 1983). A substantial amount of research testifies to the prevalence of illusion in normal human
1815|Biased assimilation and attitude polarization: The effects of prior theories on subsequently considered evidence|People who hold strong opinions on complex social issues are likely to examine relevant empirical evidence in a biased manner. They are apt to accept &#034;confirming&#034; evidence at face value while subjecting &#034;discontinuing &#034; evidence to critical evaluation, and as a result to draw undue support for their initial positions from mixed or random empirical findings. Thus, the result of exposing contending factions in a social dispute to an identical body of relevant empirical evidence may be not a narrowing of disagreement but rather an increase in polarization. To test these assumptions and predictions, subjects supporting and opposing capital punishment were exposed to two purported studies, one seemingly confirming and one seemingly disconfirming their existing beliefs about the deterrent efficacy of the death penalty. As predicted, both proponents and opponents of capital punishment rated those results and procedures that confirmed their own beliefs to be the more convincing and probative ones, and they reported corresponding shifts in their beliefs as the various results and procedures were presented. The net effect of such evaluations
1816|Confirmation, Disconfirmation, and Information in Hypothesis Testing|Strategies for hypothesis testing in scientific investigation and everyday reasoning have interested both psychologists and philosophers. A number of these scholars stress the importance of disconnrmation in reasoning and suggest that people are instead prone to a general deleterious &#034;confirmation bias.&#034; In particular, it is suggested that people tend to test those cases that have the best chance of verifying current beliefs rather than those that have the best chance of falsifying them. We show, however; that many phenomena labeled &#034;confirmation bias&#034; are better understood in terms of a general positive test strategy. With this strategy, there is a tendency to test cases that are expected (or known) to have the property of interest rather than those expected (or known) to lack that property. This strategy is not equivalent to confirmation bias in the first sense; we show that the positive test strategy can be a very good heuristic for determining the truth or falsity of a hypothesis under realistic conditions. It can, however, lead to systematic errors or inefficiencies. The appropriateness of human hypothesis-testing strategies and prescriptions about optimal strategies must be understood in terms of the interaction between the strategy and the task at hand. 
1817|Cognitive consequences of forced compliance|WHAT happens to a person&#039;s private opinion if he is forced to do or say something contrary to that opinion? Only recently has there been, any experimental work related to this question. Two studies reported by Janis and King (1954; 1956) clearly showed that, at least under some conditions, the private opinion changes so as to bring it into closer correspondence with the overt behavior the person was forced to perform. Specifically, they showed that if a person is forced to improvise a speech supporting a point of view with which he disagrees, his private opinion moves toward the position advocated in the speech. The observed opinion change is greater than for persons who only hear the speech or for persons who read a prepared speech with emphasis solely on elocution and manner of delivery. The authors of these two studies explain their results mainly in terms of mental rehearsal and thinking up new arguments. Inthisway, they propose, theperson who is forced to improvise a speech convinces himself. They present some evidence, which is not altogether conclusive, in support of this explanation. We will have more to say concerning this explanation in discussing the results of our experiment. Kelrnan (1953) tried to pursue the matter further. He reasoned that if the person is induced to make an overt statement contrary to his private opinion by the offer of some reward, then the greater the reward offered, the greater should be the subsequent opinion change. His data, however, did not support this idea. He found, rather, that a large reward produced less subsequent opinion change than did a smaller reward. Actually, this finding by Kelman is consistent with the theory we will outline below but, for a number of reasons, is
1818|The Totalitarian Ego -- Fabrication and Revision of Personal History| This article argues that (a) ego, or self, is an organization of knowledge, (b) ego is characterized by cognitive biases strikingly analogous to totalitarian information-control strategies, and (c) these totalitarian-ego biases junction to preserve organization in cognitive structures. Ego&#039;s cognitive biases are egocentricity (self as the focus of knowledge), &#034;beneffectance&#034; (perception of responsibility for desired, but not undesired, outcomes), and cognitive conservatism (resistance to cognitive change). In addition to being pervasively evident in recent studies of normal human cognition, these three biases are found in actively functioning, higher level organizations of knowledge, perhaps best exemplified by theoretical paradigms in science. The thesis that egocentricity, beneffectance, and
1819|Reasons for Confidence|People are often overconfident in evaluating the correctness of their knowl-edge. The present studies investigated the possibility that assessment of con-fidence is biased by attempts to justify one&#039;s chosen answer. These attempts include selectively focusing on evidence supporting the chosen answer and disregarding evidence contradicting it. Experiment 1 presented subjects with two-alternative questions and required them to list reasons for and against each of the alternatives prior to choosing an answer and assessing the prob-ability of its being correct. This procedure produced a marked improvement in the appropriateness of confidence judgments. Experiment 2 simplified the manipulation by asking subjects first to choose an answer and then to list (a) one reason supporting that choice, (b) one reason contradicting it, or (c) one reason supporting and one reason contradicting. Only the listing of contradicting reasons improved the appropriateness of confidence. Correla-tional analyses of the data of Experiment 1 strongly suggested that the con-fidence depends on the amount and strength of the evidence supporting the answer chosen. e remarkable characteristic of human&gt;ry is its knowledge of its own content, nents of confidence in the correctness &#039;all and recognition performance are research was supported by the Advanced ch Projects Agency of the Department of e, and was monitored by Office of Naval
1820|Stability and malleability of the self-concept|The self-concept literature is characterized by a continuing controversy over whether the self-concept is stable or malleable. In this article we suggest that it is both but that the stability observed for general descriptions of the self may mask significant local variation. In this study the social environment was varied by creating a situation in which subjects found themselves to be either very unique or very similar to others. Following this manipulation, subjects responded to a series of self-concept mea-sures. Although the uniqueness and similarity subjects did not differ in the trait terms they used to describe themselves, they did differ systematically in their latency for these judgments, in positivity and negativity of their word associations, and in their judgments of similarity to reference groups. These findings imply that subjects made to feel unique recruited conceptions of themselves as similar to others, whereas subjects made to feel similar to others recruited conceptions of themselves as unique. The results suggest that very general self-descriptive measures are inadequate for revealing how the individual adjusts and calibrates the self-concept in response to challenges from the social environment. Two seemingly contradictory aspects of the self have been emphasized in the empirical self-concept literature. The self has been regarded as a stable and enduring structure that pro-tects itself against change (e.g., Greenwald, 1980; Markus,
1821|Effects of involvement on persuasion: A meta-analysis|Defines involvement as a motivational state induced by an association between an activated attitude and the self-concept. Integration ofthe available research su~ests hat he effects of involvement on attitude change depended on the aspect of message recipients &#039; elf-concept that was activated to create involvement: (a) their enduring values (value-relevant i volvement), (b) their ability to attain desirable outcomes (outcome-relevant involvement), or (e) the impression they make on others (im-pression-relevant i volvement). Findings howed that (a) with value-relevant i volvement, high-in-volvement subjects were less persuaded than low-involvement subjects; (b) with outcome-relevant involvement, high-involvement subjects were more persuaded than low-involvement subjects by strong arguments and (somewhat inconsistently) less persuaded by weak arguments; and (c) with impression-relevant involvement, high-involvement subjects were slightly less persuaded than low-involvement subjects. To understand the conditions under which people are per-suaded by others, researchers have often invoked the concept of involvement. Although this construct was popular prior to M. Sherifand Cantril&#039;s (1947) work (see A. G. Greenwald&#039;s, 1982,
1822|Accountability: A Social Magnifier of the Dilution Effect|This research demonstrated that accountability can not only reduce judgmental bias, but also exac-erbate it—in this case, the dilution effect. Ss made predictions from either diagnostic information alone or diagnostic information plus mixtures of additional data (nondiagnostic information, addi-tional diagnostic data pointing to either the same conclusion or the opposite conclusion). Relative to unaccountable Ss, accountable Ss (a) diluted their predictions in response to nondiagnostic infor-mation and (b) were more responsive to additional diagnostic information. The accountability ma-nipulation motivated subjects to use a wide range of information in making judgments, but did not make them more discriminating judges of the usefulness of that information. Cognitive social psychologists have painted numerous por-traits of the person as information processor. Early work em-phasized the rigorous rationality with which people analyzed and drew inferences from evidence: the correspondent infer-ence model, the causal schemata model, the covariation model, and the Bayesian model. Later work emphasized people&#039;s judg-mental shortcomings. People were depicted as cognitive misers whose preference for simple, easy-to-execute heuristics ren-dered them vulnerable to a variety of errors and biases (Abelson
1823|Outcome dependency: Attention, attribution, and attraction|Theoretical and empirical work on the processes by which we attribute dis-positional characteristics to others has focused almost exclusively on how such processes proceed once the perceiver has been motivated to initiate them. The problem of identifying the factors which prompt the perceiver to engage in an attributional analysis in the first place has been relatively ignored, even though the influence of such factors may extend beyond the initiation of the causal analysis to affect the manner in which it unfolds and, ultimately, the form and substance of its conclusion. From the assumption that the function of an attributional analysis is effective control of the social environment, it was hypothesized that high outcome dependency upon another, under conditions of high unfamiliarity, is associated with the initiation of an attributional anal-ysis as evidenced by increased attention to the other, better memory of the other&#039;s characteristics and behavior, more extreme and confidently given evalua-tions of the other on a variety of dispositional trait dimensions, and increased attraction to the other. These hypotheses were tested within the context of a
1824|Personal Involvement and Strategies for Making Contingency Judgements: A Stake in the Dating Game Makes a Difference|To examine the relation between degree of involvement in a task and the complexity of strategy a subject applies to the task, we randomly assigned 48 female university volunteers to either a dating condition (high-involvement) or one of two (low-involvement) control conditions. These subjects performed a covariation judgment task for which the likelihood of their using simple or complex strategies was calculated. High-involvement subjects used more complex strategies and tended to be more accurate. These data are discussed in terms of the functionality of human information processing, heuristic analyses of inference strategies, and the importance of considering level of personal involvement in analyses of task performance. There are often many ways to solve a problem. Whether the problem entails choos-ing a mate, buying a car, isolating x on one side of an equation, attributing a cause, de-ciding whether two variables covary, or skin-
1825|Social capital, intellectual capital, and the organizational advantage|Scholars of the theory of the firm have begun to emphasize the sources and conditions of what has been described a s &#034;the organizational advantage, &#034; rather than focus on the causes and consequences of market failure. Typically, researchers see such organizational advantage a s accruing from the particular capabilities organizations have for creating and sharing knowledge. In this article we seek to contribute to this body 01 work by developing the following arguments: (1) social capital facilitates the creation of new intellectual capital. (2) organizations. a s institutional settings. a re conducive to the development of high levels of social capital. and (3) it is because of their more dense social capital that firms. within certain limits. have a n advantage over markets in creating and sharing intellectual capital. We present a model that incorporates this overall argument in the form of a series of hypothesized relation-ships between different dimensions of social capital and the main mechanisms and processes necessary for the creation of intellectual capital. Kogut and Zander recently have proposed &#034;that a firm be understood a s a social commu-nity specializing in the speed and efficiency in the creation and transfer of knowledge &#034; (1996: 503). This is an important and relatively new perspective on the theory of the firm currently being formalized through the ongoing work of
1827|The Encyclopedia of Integer Sequences |This article gives a brief introduction to the On-Line Encyclopedia of Integer Sequences (or OEIS). The OEIS is a database of nearly 90,000 sequences of integers, arranged lexicographically. The entry for a sequence lists the initial terms (50 to 100, if available), a description, formulae, programs to generate the sequence, references, links to relevant web pages, and other
1828|GFUN: A Maple Package for the Manipulation of Generating and Holonomic Functions in One Variable|We describe the gfun package which contains functions for manipulating sequences, linear recurrences or di erential equations and generating functions of various types. This document isintended both as an elementary introduction to the subject and as a reference manual for the package.  
1829|Monstrous moonshine|A quick summary of the recent amazing discoveries about the Fischer-Griess &amp;quot;MONSTER &amp;quot; simple group.
1830|An elementary problem equivalent to the Riemann hypothesis |ABSTRACT. The problem is: Let Hn = n? n = 1, that with equality only for n = 1. j=1 1 j d = Hn + exp(Hn)log(Hn),
1831|On the existence of similar sublattices|Partial answers are given to two questions. When does a lattice ? contain a sublattice ? ' of index N that is geometrically similar to ?? When is the sublattice “clean”, in the sense A similarity s of norm c is a linear map from Rn to Rn such that su · sv = c u · v for u,v ? Rn. Let ? be an n-dimensional rational lattice, i.e. u · v ? Q for u,v ? ?. A sublattice ? '  ? ? is similar to ? if s(?)  = ? ' for some similarity s of norm c. We also call s a multiplier of norm c for ?. The index N = [?: ? ' ] is c n/2, so if n is odd c must be a square, say c = a 2,
1832|How to do MONTHLY problems with your computer|this article (PWZ) have just written a book [8] that describes the theoretical foundations of the solution of this problem, and also gives the software by means of which everyone can perform these sums
1833|Meanders: A Direct Enumeration Approach|We study the statistics of semi-meanders, i.e. configurations of a set of roads crossing a river through n bridges, and possibly winding around its source, as a toy model for compact folding of polymers. By analyzing the results of a direct enumeration up to n = 29, we perform on the one hand a large n extrapolation and on the other hand we reformulate the available data into a large q expansion, where q is a weight attached to each road. We predict a transition at q = 2 between a low-q regime with irrelevant winding, and a large-q regime with relevant winding.  
1834|Numerical Analogues of Aronson&#039;s Sequence|Aronson&#039;s sequence 1, 4, 11, 16, . . . is defined by the English sentence &#034;t is the first, fourth, eleventh, sixteenth, . . . letter of this sentence.&#034; This paper introduces some numerical analogues, such as: a(n) is taken to be the smallest positive integer greater than a(n-1) which is consistent with the condition &#034;n is a member of the sequence if and only if a(n) is odd.&#034; This sequence can also be characterized by its &#034;square&#034;, the sequence a    (n) = a(a(n)), which equals 2n + 3 for n    1. There are many generalizations of this sequence, some of which are new, while others throw new light on previously known sequences.
1835|The first column of an interspersion|In 1977, K B. Stolarsky [9] introduced an array of positive integers whose first row consists of the Fibonacci numbers {Fn:«&gt;2}:1 2 3 5 8 13.... The subsequent rows are &#034;generalized Fibonacci sequences. &#034; In fact, much more is true. The rows of the array are, in a sense, the set of all &#034;positive Fibonacci sequences &#034; of integers. This fact was proved by D. Morrison [7], who
1836|Grid Information Services for Distributed Resource Sharing|Grid technologies enable large-scale sharing of resources within formal or informal consortia of individuals and/or institutions: what are sometimes called virtual organizations. In these settings, the discovery, characterization, and monitoring of resources, services, and computations are challenging problems due to the considerable diversity, large numbers, dynamic behavior, and geographical distribution of the entities in which a user might be interested. Consequently, information services are a vital part of any Grid software infrastructure, providing fundamental mechanisms for discovery and monitoring, and hence for planning and adapting application behavior.  We present here an information services architecture that addresses performance, security, scalability, and robustness requirements. Our architecture defines simple low-level enquiry  and registration protocols that make it easy to incorporate individual entities into various information structures, such as aggregate directories that support a variety of different query languages and discovery strategies. These protocols can also be combined with other Grid protocols to construct additional higher-level services and capabilities such as brokering, monitoring, fault detection, and troubleshooting. Our architecture has been implemented as MDS-2, which forms part of the Globus Grid toolkit and has been widely deployed and applied.  
1837|Globus: A Metacomputing Infrastructure Toolkit|Emerging high-performance applications require the ability to exploit diverse, geographically distributed resources. These applications use high-speed networks to integrate supercomputers, large databases, archival storage devices, advanced visualization devices, and/or scientific instruments to form networked virtual supercomputers or metacomputers. While the physical infrastructure to build such systems is becoming widespread, the heterogeneous and dynamic nature of the metacomputing environment poses new challenges for developers of system software, parallel tools, and applications. In this article, we introduce Globus, a system that we are developing to address these challenges. The Globus system is intended to achieve a vertically integrated treatment of application, middleware, and network. A low-level toolkit provides basic mechanisms such as communication, authentication, network information, and data access. These mechanisms are used to construct various higher-level metacomp...
1838|Unreliable Failure Detectors for Reliable Distributed Systems|We introduce the concept of unreliable failure detectors and study how they can be used to solve Consensus in asynchronous systems with crash failures. We characterise unreliable failure detectors in terms of two properties — completeness and accuracy. We show that Consensus can be solved even with unreliable failure detectors that make an infinite number of mistakes, and determine which ones can be used to solve Consensus despite any number of crashes, and which ones require a majority of correct processes. We prove that Consensus and Atomic Broadcast are reducible to each other in asynchronous systems with crash failures; thus the above results also apply to Atomic Broadcast. A companion paper shows that one of the failure detectors introduced here is the weakest failure detector for solving Consensus [Chandra et al. 1992].
1840|An Architecture for a Secure Service Discovery Service|The widespread deployment of inexpensive communications technology, computational resources in the networking infrastructure, and network-enabled end devices poses an interesting problem for end users: how to locate a particular network service or device out of hundreds of thousands of accessible services and devices. This paper presents the architecture and implementation of a secure Service Discovery Service (SDS). Service providers use the SDS to advertise complex descriptions of available or already running services, while clients use the SDS to compose complex queries for locating these services. Service descriptions and queries use the eXtensible Markup Language (XML) to encode such factors as cost, performance, location, and device- or service-specific capabilities. The SDS provides a highlyavailable, fault-tolerant, incrementally scalable service for locating services in the wide-area. Security is a core component of the SDS and, where necessary, communications are both encrypt...
1842|A Directory Service for Configuring High-Performance Distributed Computations|High-performance execution in distributed computing environments often requires careful selection and configuration not only of computers, networks, and other resources but also of the protocols and algorithms used by applications. Selection and configuration in turn require access to accurate, up-to-date information on the structure and state of available resources. Unfortunately, no standard mechanism exists for organizing or accessing such information. Consequently, different tools and applications adopt ad hoc mechanisms, or they compromise their portability and performance by using default configurations. We propose a solution to this problem: a Metacomputing Directory Service that provides efficient and scalable access to diverse, dynamic, and distributed information about resource structure and state. We define an extensible data model to represent the information required for distributed computing, and we present a scalable, high-performance, distributed implementation. The dat...
1843|Development of the Domain Name System|(Originally published in the Proceedings of SIGCOMM ‘88,
1844|Forecasting Network Performance to Support Dynamic Scheduling Using the Network Weather Service|The Network Weather Service is a generalizable and extensible facility designed to provide dynamic resource performance forecasts in metacomputing environments. In this paper, we outline its design and detail the predictive performance of the forecasts it generates. While the forecasting methods are general, we focus on their ability to predict the TCP/IP end-to-end throughput and latency that is attainable by an application using systems located at different sites. Such network forecasts are needed both to support scheduling [5], and by the metacomputing software infrastructure to develop quality-of-service guarantees [10, 17]. Keywords: scheduling, metacomputing, quality-ofservice, statistical forecasting, network performance monitoring  1. Introduction  As network technology advances, the resulting improvements in interprocess communication speeds make it possible to use interconnected but separate computer systems as a high-performance computational platform or metacomputer. Effect...
1845|Characterizing End-to-End Packet Delay and Loss in the Internet|We use the measured round trip delays of small UDP probe packets sent at regular time intervals to characterize the end-to-end packet delay and loss behavior in the Internet. By varying the interval between probe packets, it is possible to study the structure of the Internet load over different time scales. In this paper, the time scales of interest range from a few milliseconds to a few minutes. Our observations agree with results obtained by others using simulation and experimental approaches. For example, our estimates of Internet workload are consistent with the hypothesis of a mix of bulk traffic with larger packet size, and interactive traffic with smaller packet size. The interarrival time distribution for Internet packets is consistent with an exponential distribution. We also observe a phenomenon of compression (or clustering) of the probe packets similar to the acknowledgement compression phenomenon recently observed in TCP. Our results also show interesting and less expected...
1846|Autopilot: Adaptive control of distributed applications|With increasing development of applications for heterogeneous, distributed computing grids, the focus of performance analysis has shifted from a posteriori optimization on homogeneous parallel systems to application tuning for heterogeneous resources with time varying availability. This shift has profound implications for performance instrumentation and analysis techniques. Autopilot is a new infrastructure for dynamic performance tuning of heterogeneous computational grids based on closed loop control. This paper describes the Autopilot model of distributed sensors, actuators, and decision procedures, reports preliminary performance benchmarks, and presents a case study in which the Autopilot library is utilized in the development of an adaptive parallel input/output system.  
1847|Proxy-Based Authorization and Accounting for Distributed Systems|Despite recent widespread interest in the secure authentication of principals across computer networks there has been considerably less discussion of distributed mechanisms to support authorization and accounting. By generalizing the authentication model to support restricted proxies, both authorization and accounting can be easily supported. This paper presents the proxy model for authorization and shows how the model can be used to support a wide range of authorization and accounting mechanisms. The proxy model strikes a balance between access-control-list and capability-based mechanisms allowing each to be used where appropriate and allowing their use in combination. The paper describes how restricted proxies can be supported using existing authentication methods.   
1848|The NetLogger Methodology for High Performance Distributed Systems Performance Analysis|We describe a methodology that enables the real-time diagnosis of performance problems in complex high-performance distributed systems. The methodology includes tools for generating precision event logs that can be used to provide detailed end-to-end application and system level monitoring; a Java agent-based system for managing the large amount of logging data; and tools for visualizing the log data and real-time state of the distributed system. We developed these tools for analyzing a high-performance distributed system centered around the transfer of large amounts of data at high speeds from a distributed storage server to a remote visualization client. However, this methodology should be generally applicable to any distributed system.
1849|Locating Objects in Wide-Area Systems|Locating mobile objects in a worldwide system requires a scalable location service. An object can be a telephone or a notebook computer, but also a software or data object, such as a file or an electronic document. Our service strictly separates an object&#039;s name from the addresses where it can be contacted. This is done by introducing a location-independent object handle. An object&#039;s name is bound to its unique object handle, which, in turn, is mapped to the addresses where the object can be contacted. To locate an object, we need only its object handle. We present a scalable location service based on a worldwide distributed search tree that adapts dynamically to an object&#039;s migration pattern to optimize lookups and updates. 
1850|A Fault Detection Service for Wide Area Distributed Computations|The potential for faults in distributed computing systems is a significant complicating factor for application developers. While a variety of techniques exist for detecting and correcting faults, the implementation of these techniques in a particular context can be difficult. Hence, we propose a fault detection service designed to be incorporated, in a modular fashion, into distributed computing systems, tools, or applications. This service uses well-known techniques based on unreliable fault detectors to detect and report component failure, while allowing the user to tradeoff timeliness of reporting against false positive rates. We describe the architecture of this service, report on experimental results that quantify its cost and accuracy, and describe its use in two applications, monitoring the status of system components of the GUSTO computational grid testbed and as part of the NetSolve network-enabled numerical solver.
1851|Online Prediction of the Running Time of Tasks|We describe and evaluate the Running Time Advisor (RTA), a system that can predict the running time of a compute-bound task on a typical shared, unreserved commodity host. The prediction is computed from linear time series predictions of host load and takes the form of a confidence interval that neatly expresses the error associated with the measurement and prediction processes--- error that must be captured to make statistically valid decisions based on the predictions. Adaptive applications make such decisions in pursuit of consistent high performance, choosing, for example, the host where a task is most likely to meet its deadline. We begin by describing the system and summarizing the results of our previously published work on host load prediction. We then describe our algorithm for computing predictions of running time from host load predictions. Finally, we evaluate the system using over 100,000 randomized testcases run on 39 different hosts.
1852| 	 The Architecture of the Remos System       |Remos provides resource information to distributed applications. Its design goals of scalability, flexibility, and portability are achieved through an architecture that allows components to be positioned across the network, each collecting informationabout its local network. To collect information from different types of networks and from hosts on those networks, Remos provides several collectors that use different technologies, such as SNMP or benchmarking. By matching the appropriate collector to each particular network environment and by providing an architecture for distributing the output of these collectors across all querying environments, Remos collects appropriately detailed information at each site and distributes this information where needed in a scalable manner. Prediction services are integrated at the user-level, allowing history-based data collected across the network to be used to generate the predictions needed by a particular user. Remos has been implemented and tested in a variety of networks and is in use in a number of different environments. 
1853|A scalable, deployable directory service framework for the internet|This paper describes a directory service framework for the Internet that fits within the approach outlined in the IETF’s RFC 1588. This framework consists of a global directory service that enables virtually any local directory service to operate under it. We also include an optimized local directory service, thereby providing a complete solution for Internet directory service. Our approach uses proven Internet technology (e.g., the Domain Name System and Uniform Resource Locators) and successful or promising pieces of other services (e.g., X.500 and WHOIS++). Previous attempts to create a unified Internet directory service, such as X.500, LDAP, WHOIS++, and SOLO, have not been fully accepted because of difficulties in implementation and deployment. Therefore, we designed our approach with ease of implementation and deployment in mind. To that end, our approach attempts to co-opt the installed base making a switch to the new service as seamless as possible.
1854|White Paper: A Grid Monitoring Service Architecture (DRAFT)  (2001) |Large distributed systems such as Computational and Data Grids require a substantial amount of monitoring  data be collected for a variety of tasks such as fault detection, performance analysis, performance  tuning, performance prediction, and scheduling. Some tools are currently available and others  are being developed for collecting and forwarding this data. The goal of this paper is to describe a  common architecture with all the major components and their essential interactions in just enough  detail that Grid Monitoring systems that follow the architecture described can easily devise common  APIs and wire protocols. To aid implementation, we also discuss the performance characteristics of a  Grid Monitoring system and identify areas that are critical to proper functioning of the system.
1855|Model selection and accounting for model uncertainty in graphical models using Occam&#039;s window|We consider the problem of model selection and accounting for model uncertainty in high-dimensional contingency tables, motivated by expert system applications. The approach most used currently is a stepwise strategy guided by tests based on approximate asymptotic P-values leading to the selection of a single model; inference is then conditional on the selected model. The sampling properties of such a strategy are complex, and the failure to take account of model uncertainty leads to underestimation of uncertainty about quantities of interest. In principle, a panacea is provided by the standard Bayesian formalism which averages the posterior distributions of the quantity of interest under each of the models, weighted by their posterior model probabilities. Furthermore, this approach is optimal in the sense of maximising predictive ability. However, this has not been used in practice because computing the posterior model probabilities is hard and the number of models is very large (often greater than 1011). We argue that the standard Bayesian formalism is unsatisfactory and we propose an alternative Bayesian approach that, we contend, takes full account of the true model uncertainty byaveraging overamuch smaller set of models. An efficient search algorithm is developed for nding these models. We consider two classes of graphical models that arise in expert systems: the recursive causal models and the decomposable
1857|The hot hand in basketball: On the misperception of random sequences|We investigate the origin and the validity of common beliefs regarding “the hot hand ” and “streak shooting ” in the game of basketball. Basketball players and fans alike tend to believe that a player’s chance of hitting a shot are greater following a hit than following a miss on the previous shot. However, detailed analyses of the shooting records of the Philadelphia 76ers provided no evidence for a positive correlation between the outcomes of successive shots. The same conclusions emerged from free-throw records of the Boston Celtics, and from a controlled shooting experiment with the men and women of Cornell’s varsity teams. The outcomes of previous shots influenced Cornell players ’ predictions but not their performance. The belief in the hot hand and the “detection ” of streaks in random sequences is attributed to a general misconception of chance according to which even short random sequences are thought to be highly rep-resentative of their generating process. G 1985 Academic Press. Inc. In describing an outstanding performance by a basketball player, re-porters and spectators commonly use expressions such as “Larry Bird has the hot hand ” or “Andrew Toney is a streak shooter. ” These phrases express a belief that the performance of a player during a particular period
1858|Simulating ratios of normalizing constants via a simple identity: A theoretical exploration|Abstract: Let pi(w),i =1, 2, be two densities with common support where each density is known up to a normalizing constant: pi(w) =qi(w)/ci. We have draws from each density (e.g., via Markov chain Monte Carlo), and we want to use these draws to simulate the ratio of the normalizing constants, c1/c2. Such a computational problem is often encountered in likelihood and Bayesian inference, and arises in fields such as physics and genetics. Many methods proposed in statistical and other literature (e.g., computational physics) for dealing with this problem are based on various special cases of the following simple identity: c1 c2 = E2[q1(w)a(w)] E1[q2(w)a(w)]. Here Ei denotes the expectation with respect to pi (i =1, 2), and a is an arbitrary function such that the denominator is non-zero. A main purpose of this paper is to provide a theoretical study of the usefulness of this identity, with focus on (asymptotically) optimal and practical choices of a. Using a simple but informative example, we demonstrate that with sensible (not necessarily optimal) choices of a, we can reduce the simulation error by orders of magnitude when compared to the conventional importance sampling method, which corresponds to a =1/q2. We also introduce several generalizations of this identity for handling more complicated settings (e.g., estimating several ratios simultaneously) and pose several open problems that appear to have practical as well as theoretical value. Furthermore, we discuss related theoretical and empirical work.
1859|Model determination using predictive distributions with implementation via sampling-based-methods (with Discussion  (1992) |Reproduction in whole or in part is permitted
1860|Inference for nonconjugate bayesian models using the gibbs sampler. Canadian Journal of statistics|JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Statistical Society of Canada is collaborating with JSTOR to digitize, preserve and extend access to The
1861|Subregion-Adaptive Integration of Functions Having a Dominant Peak|Many statistical multiple integration problems involve integrands that have a dominant peak. In applying numerical methods to solve these problems, statisticians have paid relatively little attention to existing quadrature methods and available software developed in the numerical analysis literature. One reason these methods have been largely overlooked, even though they are known to be more efficient than Monte Carlo for well-behaved problems of low dimensionality, may be that when applied naively they are poorly suited for peaked-integrand problems. In this paper we use transformations based on &#034;split-t&#034; distributions to allow the integrals to be efficiently computed using a subregion-adaptive numerical integration algorithm. Our split-t distributions are modifications of those suggested by Geweke (1989) and may also be used to define Monte Carlo importance functions. We then compare our approach to Monte Carlo. In the several examples we examine here, we find subregion-adaptive inte...
1863|Auction Theory: A Guide to the Literature|  This paper provides an elementary, non-technical, survey of auction theory, by introducing and describing some of the critical papers in the subject. (The most important of these are reproduced in a companion book, The Economic Theory of Auctions, Paul Klemperer (ed.), Edward Elgar (pub.), forthcoming.) We begin with the most fundamental concepts, and then introduce the basic analysis of optimal auctions, the revenue equivalence theorem, and marginal revenues. Subsequent sections address risk-aversion, affiliation, asymmetries, entry, collusion, multi-unit auctions, double auctions, royalties, incentive contracts, and other topics. Appendices contain technical details, some simple worked examples, and a bibliography for each section.
1864|Multimarket Oligopoly: Strategic Substitutes and complements |A firm’s actions in one market can change competitors’ strategies in a second market by affecting its own marginal costs in that other mar-ket. Whether the action provides costs or benefits in the second market depends on (a) whether it increases or decreases marginal costs in the second market and (b) whether competitors’ products are strategic substitutes or strategic complements. The latter distinction is determined by whether more “aggressive” play (e.g., lower price or higher quantity) by one firm in a market lowers or raises compet-ing firms’ marginal profitabilities in that market. Many recent results in oligopoly theory can be most easily understood in terms of strategic substitutes and complements. 
1866|An efficient ascending-bid auction for multiple objects|In multiple-object environments where individual bidders may demand more than one object, standard methods of auction generally result in allocative inefficiency. This paper proposes a new ascending-bid method for auctioning homogeneous goods, such as Treasury bills or communications spectrum. The auctioneer announces a current price, bidders report back the quantity demanded at that price, and the auctioneer raises the price. Objects are awarded to bidders at the current price whenever they are “clinched,” and the process continues until the market clears. With pure private values, the proposed (dynamic) auction yields the same outcome as the (sealed-bid) Vickrey auction, but may be simpler for bidders to understand and has the advantage of assuring the privacy of the upper portions of bidders ’ demand curves. With interdependent values, the proposed auction may still yield efficiency, whereas the Vickrey auction fails due to a problem which could be described as the “Generalized Winner’s Curse.” 
1867|The Dynamics of Incentive Contracts|you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at
1869|Demand Reduction and Inefficiency in Multi-unit Auctions|Auctions typically involve the sale of many related goods. Treasury, spectrum and electricity auctions are examples. In auctions where bidders pay the market-clearing price for items won, large bidders have an incentive to reduce demand in order to pay less for their winnings. This incentive creates an inefficiency in multiple-item auctions. Large bidders reduce demand for additional items and so sometimes lose to smaller bidders with lower values. We demonstrate this inefficiency in an auction model which allows interdependent values. We also establish that the ranking of the uniform-price and pay-as-bid auctions is ambiguous in both revenue and efficiency terms. Bidding behavior in spectrum auctions, electricity auctions, and experiments highlights the empirical importance of demand reduction.
1870|Toeholds and Takeovers|Part ownership of a takeover target can help a bidder win a takeover auction, often at a low price. A bidder with a &#034;toehold&#034; bids aggressively in a standard ascending auction because its offers are both bids for the remaining shares and asks for its own holdings. While the direct effect of a toehold on a bidder&#039;s strategy may be small, the indirect effect is large in a common value auction. When a firm bids more aggressively, its competitors face an increased winner&#039;s curse and must bid more conservatively. This allows the toeholder to bid more aggressively still, and so on. One implication is that a controlling minority shareholder may be immune to outside offers. The board of a target may increase the expected sale price by allowing a second bidder to buy a toehold on favorable terms, or by running a sealed bid auction.
1871|Efficient Design with Interdependent Valuations|We study efficient, Bayes-Nash incentive compatible mechanisms in a social choice setting that allows for informational and allocative externalities. We show that such mechanisms exist only if a congruence condition relating private and social rates of information substitution is satisfied. If signals are multidimensional, the congruence condition is determined by an integrability constraint, and it can hold only in non-generic cases such as the private value case or the symmetric case. If signals are one-dimensional, the congruence condition reduces to a monotonicity constraint and it can be generically satisfied. We apply the results to the study of multi-object auctions, and we discuss why such auctions cannot be reduced to one-dimensional models without loss of generality.  
1872|Dissolving A Partnership Efficiently|this paper: What partnerships can be dissolved efficiently? At first glance, one might think that the set of dissolvable partnerships is empty; that is, the incomplete information about valuations necessarily leads to some inefficiency in trade. This is not the case. The following theorem gives a necessary and sufficient condition for the existence of an efficient trading mechanism. THEOREM 1: A partnership with ownership rights r and valuations independently drawn from F can be dissolved efficiently if and only if (D)
1873|An analysis of the war of attrition and the all-pay auction|We study the war of attrition and the all-pay auction when players ’ signals are affiliated and symmetrically distributed. We (a) find sufficient conditions for the existence of symmetric monotonicequilibrium bidding strategies; and (b) examine the performance of these auction forms in terms of the expected revenue accruing to the seller. Under our conditions the war of attrition raises greater expected revenue than all other other known sealed bid auction forms. 1
1874|Dragon—Slaying and Ballroom Dancing: The Private Supply of a Public Good|Many public goods typically are supplied by the efforts of a single individual. A purely self-interested agent could provide a public good if his own participation in the benefits justifies his cost. In this paper we model the decision of how a private individual decides when to take the initiative and pay for the provision of a public good. As an application of the optimal auctions literature to the public goods problem, the emphasis is placed on the effect of additional agents on potential supply. The free rider problem is shown to be less important as the population size of potential volunteers increases; we demonstrate conditions in which the first best is attained in the limit as the population size approaches infinity. 1.
1876|Strategic Jump Bidding in English Auctions|This paper solves for equilibria of sequential bid (or English) auctions with affiliated values when jump bidding strategies may be employed to intim-idate one’s opponents. In these equilibria, jump bids serve as correlating devices which select asymmetric bidding functions to be played subse-quently. Each possibility of jump bidding provides a Pareto improvement for the bidders from the symmetric equilibrium of a sealed bid, second-price auction. The expanded set of equilibria can approximate either first or second-price outcomes and produce exactly the set of expected prices between those two bounds. These results contrast with standard conclu-sions that equate English and second-price auctions. 1
1877|Differential Payments within a Bidder Coalition and the Shapley Value|Bidder coalitions at English auctions frequently distribute collusive gains among members via a secondary auction or &#034;knockout. &#034; When coalition members are sufficiently heterogeneous, nested coalition structures are observed in which a knockout is conducted at each level of nesting. The nested knockout&#039;s characteris-tics are investigated. Within many settings we find that the expected payments to coalition members via the nested knockout equal the Shapley value. Incentive compatibility problems of the nested knockout are also analyzed. Our understanding of auction schemes has progressed significantly since William Vick-rey&#039;s (1961) seminal work. Recent research has focused on the optimal design of auc-tions as well as the strategic behavior of bidders and auctioneers within specific envi-ronments. A rich set of results has emerged from these investigations. However, one fre-
1878|Imperfect competition in auction design|JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship.
1879|The Optimality of Being Efficient|In an optimal auction, a revenue-optimizing seller often awards goods inefficiently, either by placing them in the wrong hands or by withholding goods from the market. This conclusion rests on two assumptions: (1) the seller can prevent resale among bidders after the auction; and (2) the seller can commit to not sell the withheld goods after the auction. We examine how the optimal auction problem changes when these assumptions are relaxed. In sharp contrast to the no resale assumption, we assume perfect resale: all gains from trade are exhausted in resale. In a multiple object model with independent signals, we characterize optimal auctions with resale. We prove generally that with perfect resale, the seller&#039;s incentive to misassign goods is destroyed. Moreover, with discrete types, any misassignment of goods strictly lowers the seller&#039;s revenue from the optimum. In auction markets followed by perfect resale, it is optimal to assign goods to those with the highest values.  
1880|Coordination in split award auctions|We analyze split award procurement auctions in which a buyer divides full production between two suppliers or awards all production to a single supplier, and suppliers have private cost information. An intriguing feature of split awards is that the equilibrium bids are implicitly coordinated Because a split award price is the sum of offered split prices, each supplier can unilaterally veto a split award by bidding very high for the split. The need to coordinate is reflected in a split pri;e that does not vary with private information. We also explore conditions under which split award auctions may be preferred to winner-take-all auctions. I.
1881|The Tobacco Deal| We analyse the major economic issues raised by the 1997 Tobacco Resolution and the ensuing proposed legislation that were intended to settle tobacco litigation in the United States. By settling litigation largely in return for tax increases, the Resolution was a superb example of a &#034;win-win&#034; deal. The taxes would cost the companies about $1 billion per year, but yield the government about $13 billion per year, and allow the lawyers to claim fees based on hundreds of billions in “damages”. Only consumers, in whose name many of the lawsuits were filed, lost out. Though the strategy seems brilliant for the parties involved, the execution was less intelligent. We show that alternative taxes would be considerably superior to those proposed, and explain problems with the damage payments required from the firms, and the legal protections offered to them. We argue that the legislation was not particularly focused on youth smoking, despite the rhetoric. However, contrary to conventional wisdom, youth smokers are not especially valuable to the companies, so marketing restrictions are a sensible part of any deal. The individual state settlements set very dangerous examples which could open up unprecedented opportunities for collusion throughout the economy, and the multistate settlement of November 1998 is equally flawed. The fees proposed for the lawyers (around $15 billion) and the equally remarkable proposed payoff for Liggett (perhaps $400 million annually, for a company with a prior market value of about $100 million) also set terrible examples. We conclude with some views about how public policy might do better. 
1882|Ontologies:  Silver Bullet for Knowledge Management and Electronic Commerce|  Currently computers are changing from single isolated devices to entry points into a world wide network of information exchange and business transactions called the World Wide Web (WWW). Therefore support in the exchange of data, information, and knowledge exchange is becoming the key issue in current computer technology. Ontologies provide a shared and common understanding of a domain that can be communicated between people and application systems. Therefore, they may play a major role in supporting information exchange processes in various areas. This book discusses the role ontologies will play in knowledge management and in electronic commerce. In addition, I show how arising web standards such as RDF and XML can be used as
1883|CLASSIC: A Structural Data Model for Objects|CLASSIC is a data model that encourages the description ofobjects not only in terms of their relations to other known objects, but in terms of a level of intensional structure as well. The CLASSIC language of structured descriptions permits i) partial descriptions of individuals, under an `open world&#039; assumption, ii) answers to queries either as extensional lists of valuesorasdescriptions that necessarily hold of all possible answers, and iii) an easily extensible schema, which can be accessed uniformly with the data. One of the strengths of the approach is that the same language plays multiple roles in the processes of defining and populating the DB, as well as querying and answering. classic (for which we have a prototype main-memory implementation) can actively discover new information about objects from several sources: it can recognize new classes under which an object falls based on a description of the object, it can propagate some deductive consequences of DB upda...
1884|Knowledge Representation with Logic Programs| In this tutorial-overview, which resulted from a lecture course given by the authors at
1885|Constraint Networks| Constraint-based reasoning is a paradigm for formulating knowledge as a set of constraints without specifying the method by which these constraints are to be satisfied. A variety of techniques have been developed for finding partial or complete solutions for different kinds of constraint expressions. These have been successfully applied to diverse tasks such as design, diagnosis, truth maintenance, scheduling, spatiotemporal reasoning, logic programming and user interface. Constraint networks are graphical representations used to guide strategies for solving constraint satisfaction problems (CSPs). 
1886|A sufficient condition for backtrack-free search| A constraint satisfaction problem revolves finding values for a set of variables subject of a set of constraints (relations) on those variables Backtrack search is often used to solve such problems. A relationship involving the structure of the constraints i described which characterizes tosome degree the extreme case of mimmum backtracking (none) The relationship involves a concept called &#034;width,&#034; which may provide some guidance in the representation f constraint satisfaction problems and the order m which they are searched The width concept is studied and applied, in particular, to constraints which form tree structures.
1887|On the desirability of acyclic database schemes| A class of database schemes, called acychc, was recently introduced. It is shown that this class has a number of desirable properties. In particular, several desirable properties that have been studied by other researchers m very different terms are all shown to be eqmvalent to acydicity. In addition, several equivalent charactenzauons of the class m terms of graphs and hypergraphs are given, and a smaple algorithm for determining acychclty is presented. Also given are several eqmvalent characterizations of those sets M of multivalued dependencies such that M is the set of muRlvalued dependencies that are the consequences of a given join dependency. Several characterizations for a conflict-free (in the sense of Lien) set of muluvalued dependencies are provided.
1889|Experimental evaluation of preprocessing techniques in constraint satisfaction problems|This paper presents an evaluation of two orthogonal schemes for improving the efficiency of solving constraint satisfaction problems (CSPs). The first scheme involves a class of pre-processing techniques designed to make the representation of the CSP more explicit, including directional-arcconsistency, directional-path-consistency and adaptive-consistency. The second scheme aims at improving the order in which variables are chosen for evaluation during the search. In the first part of the experiment we tested the performance of backtracking (and its common enhancement-backjumping) with and without each of the preprocessings techniques above. The results show that directional arc-consistency, a scheme which embodies the simplest form of constraint recording, outperforms all other preprocessing techniques. The results of the second part of the experiment suggest that the best variable ordering is achieved by the fixed max-cardinality search order. 1.
1890|Directed constraint networks: A relational framework for causal modeling|U.S.A. Normally, constraint networks are undirected, since constraints merely tell us which sets of values are compatible, and compatibility is a symmetrical relationship. In contrast, causal models use directed links, conveying cause-effect asymmetries. In this paper we give a relational semantics to this directionality, thus explaining why prediction is easy while diagnosis and planning are hard. We use this semantics to show that certain relations possess intrinsic directionalities, similar to those characterizing causal influences. We also use this semantics to decide when and how an unstructured set of symmetrical constraints can be configured so as to form a directed causal theory. 1.
1891|Iterative decoding of binary block and convolutional codes|Abstract- Iterative decoding of two-dimensional systematic convolutional codes has been termed “turbo ” (de)coding. Using log-likelihood algebra, we show that any decoder can he used which accepts soft inputs-including a priori values-and delivers soft outputs that can he split into three terms: the soft channel and a priori inputs, and the extrinsic value. The extrinsic value is used as an a priori value for the next iteration. Decoding algorithms in the log-likelihood domain are given not only for convolutional codes hut also for any linear binary systematic block code. The iteration is controlled by a stop criterion derived from cross entropy, which results in a minimal number of iterations. Optimal and suboptimal decoders with reduced complexity are presented. Simulation results show that very simple component codes are sufficient, block codes are appropriate for high rates and convolutional codes for lower rates less than 213. Any combination of block and convolutional component codes is possible. Several interleaving techniques are described. At a bit error rate (BER) of lo- * the performance is slightly above or around the bounds given by the cutoff rate for reasonably simple block/convolutional component codes, interleaver sizes less than 1000 and for three to six iterations. Index Terms- Concatenated codes, product codes, iterative decoding, “soft-inlsoft-out ” decoder, “turbo ” (de)coding.
1893|Distributed hierarchical processing in the primate cerebral cortex|In recent years, many new cortical areas have been identified in the macaque monkey. The number of identified connections between areas has increased even more dramatically. We report here on (1) a summary of the layout of cortical areas associated with vision and with other modalities, (2) a computerized database for storing and representing large amounts of information on connectivity patterns, and (3) the application of these data to the analysis of hierarchical organization of the cerebral cortex. Our analysis concentrates on the visual system, which includes 25 neocortical areas that are predominantly or exclusively visual in function, plus an additional 7 areas that we regard as visual-association areas on the basis of their extensive visual inputs. A total of 305 connections among these 32 visual and
1894|Learning and development in neural networks: The importance of starting small|It is a striking fact that in humans the greatest learnmg occurs precisely at that point in time- childhood- when the most dramatic maturational changes also occur. This report describes possible synergistic interactions between maturational change and the ability to learn a complex domain (language), as investigated in con-nectionist networks. The networks are trained to process complex sentences involving relative clauses, number agreement, and several types of verb argument structure. Training fails in the case of networks which are fully formed and ‘adultlike ’ in their capacity. Training succeeds only when networks begin with limited working memory and gradually ‘mature ’ to the adult state. This result suggests that rather than being a limitation, developmental restrictions on resources may constitute a necessary prerequisite for mastering certain complex domains. Specifically, successful learning may depend on starting small.
1895|Evolving Dynamical Neural Networks for Adaptive Behavior|We would like the behavior of the artificial agents that we construct to be as well-adapted to their environments as natural animals are to theirs. Unfortunately, designing controllers with these properties is a very difficult task. In this article, we demonstrate that continuous-time recurrent neural networks are a viable mechanism for adaptive agent control and that the genetic algorithm can be used to evolve effective neural controllers. A significant advantage of this approach is that one need specify only a measure of an agent’s overall performance rather than the precise motor output trajectories by which it is achieved. By manipulating the performance evaluation, one can place selective pressure on the development of controllers with desired properties. Several novel controllers have been evolved, including a chemotaxis controller that switches between different strategies depending on environmental conditions, and a locomotion controller that takes advantage of sensory feedback if available but that can operate in its absence if necessary.
1896|From local actions to global tasks: Stigmergy and collective robotics|This paper presents a series of experiments where a group of mobile robots gather 81 randomly distributed objects and cluster them into one pile. Coordination of the agents ’ movements is achieved through stigmergy. This principle, originally developed for the description of termite building behaviour, allows indirect communication between agents through sensing and modification of the local environment which determines the agents’ behaviour. The efficiency of the work was measured for groups of one to five robots working together. Group size is a critical factor. The mean time to accomplish the task decreases for one, two, and three robots respectively, then increases again for groups of four and five agents, due to an exponential increase in the number of interactions between robots which are time consuming and may eventually result in the destruction of existing clusters. We compare our results with those reported by Deneubourg et al. (1990) where similar clusters are observed in ant colonies, generated by the probabilistic behaviour of workers. 
1897|What Are Plans for?|What plans are like depends on how they&#039;re used. We contrast two views of plan use. On the plan-as-program view, plan use is the execution of an effective procedure. On the plan-as-communication view, plan use is like following natural language instructions. We have begun work on computational models of plans-as-communications, building on our previous work on improvised activity and on ideas from sociology.
1898|Building Brains for Bodies|We describe a project to capitalize on newly available levels of computational resources in order to understand human cognition. We are building an integrated physical system including vision, sound input and output, and dextrous manipulation, all controlled by a continuously operating large scale parallel MIMD computer. The resulting system will learn to &#034;think&#034; by building on its bodily experiences to accomplish progressively more abstract tasks. Past experience suggests that in attempting to build such an integrated system we will have to fundamentally change the way artificial intelligence, cognitive science, linguistics, and philosophy think about the organization of intelligence. We expect to be able to better reconcile the theories that will be developed with current work in neuroscience.
1899|The evolution of emergent computation|A simple evolutionary process can discover sophisticated methods for emergent information-processing in decentralized spatially-extended systems. The mechanisms underlying the resulting emergent computation are explicated by a novel technique for analyzing particle-based logic embedded in pattern-forming systems. Understanding how globally-coordinated computation can emerge in evolution is relevant both for the scientific understanding of natural information processing and for engineering new forms of parallel computing systems. * Correspondence author. Many systems in nature exhibit sophisticated collective information-processing abilities that emerge from the individual actions of simple components interacting via restricted communica-tion pathways. Some often-cited examples include efficient foraging and intricate nest-building in insect societies (1), the spontaneous aggregation of a reproductive multicellular organism from individual amoeba in the life cycle of the Dictyostelium slime mold (2), the parallel and distributed processing of sensory information by assemblies of neurons in the brain (3), and the optimal pricing of goods in an economy arising from agents obeying local rules of commerce (4). Allowing global coordination to emerge from a decentralized collection of simple components
1900|Language as a Dynamical System|Introduction Despite considerable diversity among theories about how humans process language, there are a number of fundamental assumptions which are shared by most such theories. This consensus extends to the very basic question about what counts as a cognitive process. So although many cognitive scientists are fond of referring to the brain as a `mental organ&#039; (e.g., Chomsky, 1975)---implying a similarity to other organs such as the liver or kidneys---it is also assumed that the brain is an organ with special properties which set it apart. Brains `carry out computation&#039; (it is argued)
1901|Tuning of MST neurons to spiral motions|Cells in the dorsal division of the medial superior temporal area (MSTd) have large receptive fields and respond to ex-pansion/contraction, rotation, and translation motions. These same motions are generated as we move through the en-vironment, leading investigators to suggest that area MSTd analyzes the optical flow. One influential idea suggests that navigation is achieved by decomposing the optical flow into the separate and discrete channels mentioned above, that is, expansion/contraction, rotation, and translation. We di-rectly tested whether MSTd neurons perform such a decom-position by examining whether there are cells that are pref-erentially tuned to intermediate spiral motions, which combine both expansion/contraction and rotation components. The finding that many cells in MSTd are preferentially selective for spiral motions indicates that this simple three-channel decomposition hypothesis for MSTd does not appear to be correct. Instead, there is a continuum of patterns to which MSTd cells are selective. In addition, we find that MSTd cells maintain their selec-tivity when stimuli are moved to different locations in their large receptive fields. This position invariance indicates that MSTd cells selective for expansion cannot give precise in-formation about the retinal location of the focus of expan-sion. Thus, individual MSTd neurons cannot code, in a pre-cise fashion, the direction of heading by using the location of the focus of expansion. The only way this navigational information could be accurately derived from MSTd is through the use of a coarse, population encoding. Positional invari-ance and selectivity for a wide array of stimuli suggest that MSTd neurons encode patterns of motion per se, regardless of whether these motions are generated by moving objects or by motion induced by observer locomotion. [Key words: visual system, motion perception, visoalpath-ways, extrastriate cortex, area MST, parietal cortex] The medial superior temporal area in the macaque visual system contains at least two major subdivisions: a ventral-lateral one
1902|Survey of decision field theory|This article summarizes the cumulative progress of a cognitive-dynamical approach to decision making and preferential choice called decision field theory. This review includes applications to (a) binary decisions among risky and uncertain actions, (b) multi-attribute preferential choice, (c) multi-alternative preferential choice, and (d) certainty equivalents such as prices. The theory provides natural explanations for violations of choice principles including strong stochastic transitivity, independence of irrelevant alternatives, and regularity. The theory also accounts for the relation between choice and decision time, preference reversals between choice and certainty equivalents, and preference reversals under time pressure. Comparisons with other dynamic models of decision-making and other random utility models of preference are discussed.
1903|The art of artificial intelligence -- 1. Themes and case studies of knowledge engineering|The knowledge engineer practices the art of bringing the principles and tools of AI research to bear on difficult applications problems requiring experts&#039; knowledge for their solution. The technical issues of acquiring this knowledge, representing it, and using it appropriately to construct and explain lines-of-reasoning, are important problems in the design of knowledge-based systems. Various systems that have achieved expert level performance in scientific and medical inference illuminate the art of knowledge
1904|Semantic similarity based on corpus statistics and lexical taxonomy|This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task. 1.
1905|Experiments on Using Semantic Distances Between Words in Image Caption Retrieval|Traditional approaches to information retrieval are based upon representing a user&#039;s query as a bag of query terms  and a document as a bag of index terms and computing a degree of similarity between the two based on the overlap or number of query terms in common between them. Our long-term approach to IR applications is based upon precomputing semantically-based word-word similarities, work which is described elsewhere, and using these as part of the document-query similarity measure. A basic premise of our word-to-word similarity measure is that the input to this computation is the correct or intended word sense but in information retrieval applications, automatic and accurate word sense disambiguation remains an unsolved problem. In this paper we describe our first successful application of these ideas to an information retrieval application, specifically the indexing and retrieval of captions describing the content of images. We have hand-captioned 2714 images and to circumvent, fo...
1906|Using wordnet in a knowledge-based approach to information retrieval|Abstract: The application of natural language processing tools and techniques to information retrieval tasks has long since been identified as potentially useful for the quality of information retrieval. Traditionally, IR has been based on matching words or terms in a query with words or terms in a document. In this paper we introduce an approach to IR based on computing a semantic distance measurement between concepts or words and using this word distance to compute a similarity between a query and a document. Two such semantic distance measures are presented in this paper and both are benchmarked on queries and documents from the TREC collection. Although our results in terms of precision and recall are disappointing, we rationalise this in terms of our experimental setup and our results show promise for future work in this area. 1
1907|A Proposal for Word Sense Disambiguation using Conceptual Distance|This paper presents a method for the resolution of lexical ambiguity and its automatic evaluation over the Brown Corpus. The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose. This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process. The results of the experiment have been automatically evaluated against SemCor, the sense-tagged version of the Brown Corpus.
1908|Similarity between words computed by spreading activation on an English dictionary|This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis. The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary, LDOCE (Longman Dictionary of Contemporary English). Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary, and indirectly the similarity of all the other words in LDOCE. The similarity represents the strength of lexical cohesion or semantic relation, and also provides valuable information about similarity and coherence of texts. 1
1909|WordNet and distributional analysis: A class-based approach to lexical discovery|It has become common in statistical studies of natural language data to use measures of lexical association, such as the information-theoretic measure of mutual information, to extract useful relationships
1910|Co-Occurrence Vectors From Corpora Vs. Distance Vectors From Dictionaries|A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the interword distances in dictionary definitions. The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal (20M total words) was higher than that by using distance vectors from the Collins English Dictionary (60K head words + 1.6M definition words). However, other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors.
1911|Information Retrieval Using Robust Natural Language Processing|We developed a prototype information retrieval system  which uses advanced natural language processing  techniques to enhance the effectiveness of traditional  key-word based document retrieval. The backbone  of our system is a statistical retrieval engine  which performs automated indexing of documents,  then search and ranking in response to user queries.
1913|Toward a Conceptual Framework for Mixed-Method Evaluation Designs. Educational Evaluation and Policy Analysis|In recent years evaluators of educational and social programs have expanded their method-ological repertoire with designs that include the use of both qualitative and quantitative methods. Such practice, however, needs to be grounded in a theory that can meaningfully guide the design and implementation of mixed-method evaluations. In this study, a mixed-method conceptual framework was developed from the theoretical literature and then refined through an analysis of 57 empirical mixed-method evaluations. Five purposes for mixed-method evaluations are identified in this conceptual framework: triangulation, complemen-tarity, development, initiation, and expansion. For each of the five purposes, a recommended design is also presented in terms of seven relevant design characteristics. These design elements encompass issues about methods, the phenomena under investigation, paradigmatic framework, and criteria for implementation. In the empirical review, common misuse of the term triangulation was apparent in evaluations that stated such a purpose but did not employ an appropriate design. In addition, relatively few evaluations in this review integrated the different method types at the level of data analysis. Strategies for integrated data analysis are among the issues identified as priorities for further mixed-method work. The inevitable organizational, political, and interpersonal challenges of program evaluation mandate the use of multiple tools from evaluators &#039; full methodological reper-toire (Cook, 1985; Mathison, 1988). In re-cent years, this repertoire has been consid-erably expanded with the acceptance of qualitative methods as appropriate, legiti-mate, and even preferred for a wide range of evaluation settings and problems. Concom-itantly, evaluators have expressed renewed interest in mixed-method evaluation designs An earlier version of this paper was presented as a panel at the 1988 Annual Meeting of the American Evaluation Association in New Orle-ans. The authors are indebted to Melvin Mark for his insightful and constructive comments on the work presented herein, both at the conference and in subsequent personal communications
1914|Against the quantitative–qualitative incompatibility thesis or dogmas die hard|Over approximately the last 20 years, the use of qualitative methods in educational research as evolved from being scoffed at to being viewed as useful for provisional exploration, to being accepted as a valuable alternative approach in its own right, to being embraced as capable of thoroughgoing integration with quantitative methods. Progress has been halting, and it is not surprising that certain thinkers are now balking at the latest stage of development. The chief worry is that the capitulation to &#034;what works &#034; ignores the incompatibility of the competing positivistic and interpretivist epistemological p radigms that purportedly undergird quantitative and qualitative methods, respectively. Appealing to a pragmatic philosophical perspective, this paper argues that no incompatibili-ty between quantitative and qualitative methods exists at either the level of practice or that of epistemology and that there are thus no good reasons for educational researchers tofear forging ahead
1915|Quantitative versus qualitative research: An attempt to clarify the Issue|This paper will describe points of disagreement between quant i ta-tive research and qualitative, or in te rpre t ive, research. After a brief historical overview, the dis-cussion will focus on how each per-spective responds to three major and closely r e l a t e d ques t ions: (1) What is the relationship of the investigator to what is investigat-ed? (2) What is the relationship between facts and values in the process of i nves t iga t ion? and (3) What is the goal of investiga-tion? Educational researchers have r ecen t ly devoted i n c r e a s i n g amounts of time and energy to the issue of one method versus the other. Unfortunately, much of the discussion has tended to obfuscate rather than clarify. There has been a tendency to engage in polemics and, at times, name calling. We have all heard, if not seen in print as frequently or as bluntly, one side refer to the other as &#034;bank-rup t,  &#034; &#034;number-c runcher s,  &#034; or &#034;storytellers. &#034; There has also been a tendency to see t he two ap-proaches, if not as interchange-able, certainly as complementary. The implication is that research-ers may variously mix the two ap-proaches for any par t icular re-
1916|The integration of field work and survey methods|you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at.
1917|Causal and Functional Explanations|Abstract. Functional explanation, for long the mainstay of psychology’s autonomy, has recently come under attack. It is sometimes argued that higher-level generalizations are causally impotent, and do not really explain anything. Presumably only the reduction of higher-level patterns to underlying causal physical properties, and the specifying of lower-level, local causal mechanisms, provides genuine explanations. Two lines of argument are critically discussed: causal exclusion and multiple realization. These bear upon the credibility of functional explanation, and upon the presumed explanatory superiority of causal mechanisms over functional higher-level generalizations. It is argued that the causal exclusion argument conflates metaphysics with explanation, and that, rather than pointing towards reductionism, multiple realization indicates the indispensability of higher (functional) generalizations, alongside lower (causal) explanations; the choice for higher or lower level depends on context and explanatory interest. The notion of screening-off suggests a criterion for the legitimacy of higher-level characterizations. A brief example from the history of genetics is discussed to illustrate these ideas. This leads to a plea for pluralism in explanation.
1919|Pfam protein families database |Pfam is a comprehensive collection of protein domains and families, represented as multiple sequence alignments and as profile hidden Markov models. The current release of Pfam (22.0) contains 9318 protein families. Pfam is now based not only on the UniProtKB sequence database, but also on NCBI GenPept and on sequences from selected metage-nomics projects. Pfam is available on the web from the consortium members using a new, consistent and improved website design in the UK
1920|What is a hidden Markov model?|Often, problems in biological sequence analysis are just a matter of putting the right label on each residue. In gene identification, we want to label nucleotides as exons, introns, or intergenic sequence. In sequence alignment, we want to associate residues in a query sequence with ho-mologous residues in a target database sequence. We can always write an ad hoc program for any given problem, but the same potentially frustrating issues will always recur. One issue is that we often want to incorporate multiple heterogenous sources of information. A genefinder, for in-stance, ought to combine splice site consenses, codon bias, exon/intron length preferences, and open reading frame analysis all in one scoring system. How should all those parameters be set? How should different kinds of information be weighted? A second issue is being able to interpret results probabilistically. Finding a best scoring answer is one thing, but what does the score mean, and how confident are we that the best answer, or any given part of it, is correct? A third issue is extensibility. The moment we perfect our ad hoc genefinder, we wish we had also modeled translational initiation consensus, alternative splicing, and a polyadenylation signal. All too often, piling more reality onto a fragile ad hoc program makes it collapse under its own weight. Hidden Markov models (HMMs) are a formal foundation for making probabilistic models of
1921|Hidden Markov models in computational biology: applications to protein modeling|Hidden.Markov Models (HMMs) are applied t.0 the problems of statistical modeling, database searching and multiple sequence alignment of protein families and protein domains. These methods are demonstrated the on globin family, the protein kinase catalytic domain, and the EF-hand calcium binding motif. In each case the parameters of an HMM are estimated from a training set of unaligned sequences. After the HMM is built, it is used to obtain a multiple alignment of all the training sequences. It is also used to search the. SWISS-PROT 22 database for other sequences. that are members of the given protein family, or contain the given domain. The Hi &#034; produces multiple alignments of good quality that agree closely with the alignments produced by programs that incorporate threedimensional structural information. When employed in discrimination tests (by examining how closely the sequences in a database fit the globin, kinase and EF-hand HMMs), the &#039;\ HMM is able to distinguish members of these families from non-members with a high degree of accuracy. Both the HMM and PROFILESEARCH (a technique used to search for relationships between a protein sequence and multiply aligned sequences) perform better in these tests than PROSITE (a dictionary of sites and patterns in proteins). The HMM appecvs to have a slight advantage over PROFILESEARCH in terms of lower rates of false
1922|Protein homology detection by HMM-HMM comparison|Motivation: Protein homology detection and sequence alignment are at the basis of protein structure prediction, function prediction, and evolution. Results: We have generalized the alignment of protein se-quences with a profile hidden Markov model (HMM) to the case of pairwise alignment of profile HMMs. We present a method for detecting distant homologous relationships between proteins based on this approach. The method (HHsearch) is benchmarked together with BLAST, PSI-BLAST, HMMER, and the profile-profile comparison tools PROF_SIM and COMPASS, in an all-against-all compari-son of a database of 3691 protein domains from SCOP 1.63 with pairwise sequence identities below 20%. Sensitivity: When predicted secondary structure is included in the HMMs, HHsearch is able to detect between 2.7 and 4.2 times more homologs than PSI-BLAST or HMMER and between 1.44 and 1.9 times more than COMPASS or PROF_SIM for a rate of false positives of 10%. Approxi-mately half of the improvement over the profile–profile com-parison methods is attributable to the use of profile HMMs in place of simple profiles. Alignment quality: Higher sensitivity is mirrored by an in-creased alignment quality. HHsearch produced 1.2, 1.7, and 3.3 times more good alignments (“balanced ” score&gt; 0.3) than the next best method (COMPASS), and 1.6, 2.9, and 9.4 times more than PSI-BLAST, at the family, super-family, and fold level. Speed: HHsearch scans a query of 200 residues against 3691 domains in 33s on an AMD64 3GHz PC. This is 10 times faster than PROF_SIM and 17 times faster than
1923|A combined transmembrane topology and signal peptide prediction method|Hidden Markov models (HMMs) have been successfully applied to the tasks of transmembrane protein topology prediction and signal peptide prediction. In this paper we expand upon this work by making use of the more powerful class of dynamic Bayesian networks (DBNs). Our model, Philius, is inspired by a previously published HMM, Phobius, and combines a signal peptide submodel with a transmembrane submodel. We introduce a two-stage DBN decoder that combines the power of posterior decoding with the grammar constraints of Viterbi-style decoding. Philius also provides protein type, segment, and topology confidence metrics to aid in the interpretation of the predictions. We report a relative improvement of 13 % over Phobius in full-topology prediction accuracy on transmembrane proteins, and a sensitivity and specificity of 0.96 in detecting signal peptides. We also show that our confidence metrics correlate well with the observed precision. In addition, we have made predictions on all 6.3 million proteins in the Yeast Resource Center (YRC) database. This large-scale study provides an overall picture of the relative numbers of proteins that include a signal-peptide and/or one or more transmembrane segments as well as a valuable resource for the scientific community. All DBNs are implemented using the Graphical Models Toolkit. Source code for the models described here is available at
1924|The Sorcerer II Global Ocean Sampling expedition: Expanding the universe of protein families. PLoS Biol 5: e16|Metagenomics projects based on shotgun sequencing of populations of micro-organisms yield insight into protein families. We used sequence similarity clustering to explore proteins with a comprehensive dataset consisting of sequences from available databases together with 6.12 million proteins predicted from an assembly of 7.7 million Global Ocean Sampling (GOS) sequences. The GOS dataset covers nearly all known prokaryotic protein families. A total of 3,995 medium- and large-sized clusters consisting of only GOS sequences are identified, out of which 1,700 have no detectable homology to known families. The GOS-only clusters contain a higher than expected proportion of sequences of viral origin, thus reflecting a poor sampling of viral diversity until now. Protein domain distributions in
1925|J.C.: The sorcerer ii global ocean sampling expedition: Northwest atlantic through eastern tropical pacific. PLoS Biol|The world’s oceans contain a complex mixture of micro-organisms that are for the most part, uncharacterized both genetically and biochemically. We report here a metagenomic study of the marine planktonic microbiota in which surface (mostly marine) water samples were analyzed as part of the Sorcerer II Global Ocean Sampling expedition.
1926|The pairwise energy content estimated from amino acid composition discriminates between folded and intrinsically unstructured proteins|Intrinsically unstructured/disordered proteins/ domains (IUPs), such as p21, 1 the N-terminal domain of p53 2 or the transactivator domain of CREB, 3 exist in a largely disordered structural state,
1927|A NEW GENERATION OF HOMOLOGY SEARCH TOOLS BASED ON PROBABILISTIC INFERENCE|Many theoretical advances have been made in applying probabilistic inference methods to improve the power of sequence homology searches, yet the BLAST suite of programs is still the workhorse for most of the field. The main reason for this is practical: BLAST’s programs are about 100-fold faster than the fastest competing implementations of probabilistic inference methods. I describe recent work on the HMMER software suite for protein sequence analysis, which implements probabilistic inference using profile hidden Markov models. Our aim in HMMER3 is to achieve BLAST’s speed while further improving the power of probabilistic inference based methods. HMMER3 implements a new probabilistic model of local sequence alignment and a new heuristic acceleration algorithm. Combined with efficient vector-parallel implementations on modern processors, these improvements synergize. HMMER3 uses more powerful log-odds likelihood scores (scores summed over alignment uncertainty, rather than scoring a single optimal alignment); it calculates accurate expectation values (E-values) for those scores without simulation using a generalization of Karlin/Altschul theory; it computes posterior distributions over the ensemble of possible alignments and returns posterior probabilities (confidences) in each aligned residue; and it does all this at an overall speed comparable to BLAST. The HMMER project aims to usher in a new generation of more powerful homology search tools based on probabilistic inference methods.
1928|Accelerated Profile HMM Searches|Profile hidden Markov models (profile HMMs) and probabilistic inference methods have made important contributions to the theory of sequence database homology search. However, practical use of profile HMM methods has been hindered by the computational expense of existing software implementations. Here I describe an acceleration heuristic for profile HMMs, the ‘‘multiple segment Viterbi’ ’ (MSV) algorithm. The MSV algorithm computes an optimal sum of multiple ungapped local alignment segments using a striped vector-parallel approach previously described for fast Smith/Waterman alignment. MSV scores follow the same statistical distribution as gapped optimal local alignment scores, allowing rapid evaluation of significance of an MSV score and thus facilitating its use as a heuristic filter. I also describe a 20-fold acceleration of the standard profile HMM Forward/Backward algorithms using a method I call ‘‘sparse rescaling’’. These methods are assembled in a pipeline in which high-scoring MSV hits are passed on for reanalysis with the full HMM Forward/Backward algorithm. This accelerated pipeline is implemented in the freely available HMMER3 software package. Performance benchmarks show that the use of the heuristic MSV filter sacrifices negligible sensitivity compared to unaccelerated profile HMM searches. HMMER3 is substantially more sensitive and 100- to 1000-fold faster than HMMER2. HMMER3 is now about as fast as BLAST for protein searches.
1929|FastTree 2 -- Approximately Maximum-Likelihood Trees for Large Alignments|Background: We recently described FastTree, a tool for inferring phylogenies for alignments with up to hundreds of thousands of sequences. Here, we describe improvements to FastTree that improve its accuracy without sacrificing scalability. Methodology/Principal Findings: Where FastTree 1 used nearest-neighbor interchanges (NNIs) and the minimum-evolution criterion to improve the tree, FastTree 2 adds minimum-evolution subtree-pruning-regrafting (SPRs) and maximumlikelihood NNIs. FastTree 2 uses heuristics to restrict the search for better trees and estimates a rate of evolution for each site (the ‘‘CAT’ ’ approximation). Nevertheless, for both simulated and genuine alignments, FastTree 2 is slightly more accurate than a standard implementation of maximum-likelihood NNIs (PhyML 3 with default settings). Although FastTree 2 is not quite as accurate as methods that use maximum-likelihood SPRs, most of the splits that disagree are poorly supported, and for large alignments, FastTree 2 is 100–1,000 times faster. FastTree 2 inferred a topology and likelihood-based local support values for 237,882 distinct 16S ribosomal RNAs on a desktop computer in 22 hours and 5.8 gigabytes of memory. Conclusions/Significance: FastTree 2 allows the inference of maximum-likelihood phylogenies for huge alignments.
1931|Metagenomics for studying unculturable microorganisms: cutting the Gordian knot|electronic version of this article is the complete one and can be
1932|A: Pfam 10 years on: 10,000 families and still growing |Classifications of proteins into groups of related sequences are in some respects like a periodic table for biology, allowing us to understand the underlying molecular biology of any organism. Pfam is a large collection of protein domains and families. Its scientific goal is to provide a complete and accurate classification of protein families and domains. The next release of the database will contain over 10 000 entries, which leads us to reflect on how far we are from completing this work. Currently Pfam matches 72 % of known protein sequences, but for proteins with known structure Pfam matches 95%, which we believe represents the likely upper bound. Based on our analysis a further 28 000 families would be required to achieve this level of coverage for the current sequence database.We also show that as more sequences are added to the sequence databases the fraction of sequences that Pfam matches is reduced, suggesting that continued addition of new families is essential to maintain its relevance.
1933|Representative proteomes: a stable, scalable and unbiased proteome set for sequence analysis and functional annotation. PLoS One 6: e18910|The accelerating growth in the number of protein sequences taxes both the computational and manual resources needed to analyze them. One approach to dealing with this problem is to minimize the number of proteins subjected to such analysis in a way that minimizes loss of information. To this end we have developed a set of Representative Proteomes (RPs), each selected from a Representative Proteome Group (RPG) containing similar proteomes calculated based on co-membership in UniRef50 clusters. A Representative Proteome is the proteome that can best represent all the proteomes in its group in terms of the majority of the sequence space and information. RPs at 75%, 55%, 35 % and 15 % co-membership threshold (CMT) are provided to allow users to decrease or increase the granularity of the sequence space based on their requirements. We find that a CMT of 55 % (RP55) most closely follows standard taxonomic classifications. Further analysis of this set reveals that sequence space is reduced by more than 80 % relative to UniProtKB, while retaining both sequence diversity (over 95 % of InterPro domains) and annotation information (93 % of experimentally characterized proteins). All sets can be browsed and are available for sequence similarity searches and download at
1934|Functional evaluation of domain–domain interactions and human protein interaction networks|Abstract: Large amounts of protein and domain interaction data are being produced by experimental high-throughput techniques and computational approaches. To gain insight into the value of the provided data, we used our new similarity measure based on the Gene Ontology to evaluate the molecular functions and biological processes of interacting proteins or domains. The applied measure particularly addresses the frequent annotation of proteins or domains with multiple Gene Ontology terms. Using our similarity measure, we compare predicted domain-domain and human protein-protein interactions with experimentally derived interactions. The results show that our similarity measure is of significant benefit in quality assessment and confidence ranking of domain and protein networks. We also derive useful confidence score thresholds for dividing domain interaction predictions into subsets of low and high confidence. 1
1935|Control of protein functional dynamics by peptide linkers |Abstract: Control of structural flexibility is essential for the proper functioning of a large number of proteins and multiprotein complexes. At the residue level, such flexibility occurs due to local relaxation of peptide bond angles whose cumulative effect may result in large changes in the secon-dary, tertiary or quaternary structures of protein molecules. Such flexibility, and its absence, most often depends on the nature of interdomain linkages formed by oligopeptides. Both flexible and rela-tively rigid peptide linkers are found in many multidomain proteins. Linkers are thought to control favorable and unfavorable interactions between adjacent domains by means of variable softness furnished by their primary sequence. Large-scale structural heterogeneity of multidomain proteins and their complexes, facilitated by soft peptide linkers, is now seen as the norm rather than the exception. Biophysical discoveries as well as computational algorithms and databases have reshaped our understanding of the often spectacular biomolecular dynamics enabled by soft linkers. Absence of such motion, as in so-called molecular rulers, also has desirable functional effects in protein architecture. We review here the historic discovery and current understanding of the nature of domains and their linkers from a structural, computational, and biophysical point of view. A number of emerging applications, based on the current understanding of the structural properties
1936|Unfoldomics of Human Genetic Diseases: Illustrative Examples of Ordered and Intrinsically Disordered Members of the Human Diseasome |Abstract: Intrinsically disordered proteins (IDPs) constitute a recently recognized realm of atypical biologically active proteins that lack stable structure under physiological conditions, but are commonly involved in such crucial cellular processes as regulation, recognition, signaling and control. IDPs are very common among proteins associated with various diseases. Recently, we performed a systematic bioinformatics analysis of the human diseasome, a network that linked the human disease phenome (which includes all the human genetic diseases) with the human disease genome (which contains
1937|Some informational aspects of visual perception|The ideas of information theory are at present stimulating many different areas of psychological inquiry. In providing techniques for quantifying situations which have hitherto been difficult or impossible to quantify, they suggest new and more precise ways of conceptualizing these situations (see Miller [12] for a general discussion and bibliography). Events ordered in time are particularly amenable to informational analysis; thus language sequences are being extensively studied, and other sequences, such as those of music, plainly invite research. In this paper I shall indicate some of the ways in which the concepts and techniques of information theory may clarify our understanding of visual perception. When we begin to consider perception as an information-handling process, it quickly becomes clear that much of the information received by any higher organism is redundant. Sensory events are highly interdependent in both space and time: if we know at a given moment the states of a limited number of receptors (i.e., whether they are firing or not firing), we can make better-than-chance inferences with respect to the prior and subsequent states of these receptors, and also with respect to the present, prior, and subsequent states of other receptors. The preceding statement, taken in its broadest im-
1938|Child psychology|Completion of this dissertation, and indeed my entire postgraduate education, would not have been possible without the help of many people. I thank Dr. Stephen Cook for his endless patience, boundless support, and careful attention to my work. I am grateful to Dr. Jim Clopton for his open door, wise counsel, and generous assistance with every concern I brought to him. I thank Dr. Florence Phillips for instilling within me a love for my profession and an understanding of what it means to be a recipient of &#039;Hmconditional positive regard. &#034; I would also like to thank the staff and feculty of the Texas Tech University Psychology Department for the consistently high quaUty of their instruction. My gratitude extends to Kami Leonard and Veronika Polisenska for the long hours and late nights they spent assisting me in collecting data for this project. I thank my parents, Gary and Nanetta Thompson, for their support, encouragement, and love. I know it has been a long &#034;row to hoe, &#034; but I have reached the end. Finally, I extend my endless love and gratitude to my husband, Jude Stanley, for his patience and love. My accomplishment is also yours.
1940|PAC-Learnability of Determinate Logic Programs|The field of Inductive Logic Programming (ILP) is concerned with inducing logic programs from examples in the presence of background knowledge. This paper defines the ILP problem, and describes the various syntactic restrictions that are commonly used for learning first-order representations. We then derive some positive results concerning the learnability of these restricted classes of logic programs, by reducing the ILP problem to a standard propositional learning problem. More specifically, k-clause predicate definitions consisting of determinate, function-free, non-recursive Horn clauses with variables of bounded depth are polynomially learnable under a broad class of probability distributions, called simple distributions. Similarly, recursive k-clause definitions are polynomially learnable under simple distributions if we allow existential and membership queries about the target concept. 
