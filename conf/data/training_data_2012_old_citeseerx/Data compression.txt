ID|Title|Summary
1|A universal algorithm for sequential data compression|A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source.
3|Compression of Individual Sequences via Variable-Rate Coding|this paper contains two parts: descrip five part (Section II) where all the results are stated 532 XEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 1T-24, NO. 5, SEPTEMF..R 1978  and discussed and a formal part (Section III) where all proofs except that of Theorem 2 are given. The proof of Theorem 2, which is constructive and thus informative, is presented in the mainstream of Section II
4|Suffix arrays: A new method for on-line string searches|A new and conceptually simple data structure, called a suffix array, for on-line string searches is intro-duced in this paper. Constructing and querying suffix arrays is reduced to a sort and search paradigm that employs novel algorithms. The main advantage of suffix arrays over suffix trees is that, in practice, they use three to five times less space. From a complexity standpoint, suffix arrays permit on-line string searches of the type, &#034;Is W a substring of A?&#034; to be answered in time O(P + log N), where P is the length of W and N is the length of A, which is competitive with (and in some cases slightly better than) suffix trees. The only drawback is that in those instances where the underlying alphabet is finite and small, suffix trees can be constructed in O(N) time in the worst case, versus O(N log N) time for suffix arrays. However, we give an augmented algorithm that, regardless of the alphabet size, constructs suffix arrays in O(N) expected time, albeit with lesser space efficiency. We believe that suffix arrays will prove to be better in practice than suffix trees for many applications.  
5|Data Compression|This paper surveys a variety of data compression methods spanning almost forty years of research, from the work of Shannon, Fano and Huffman in the late 40&#039;s to a technique developed in 1986. The aim of data compression is to reduce redundancy in stored or communicated data, thus increasing effective data density. Data compression has important application in the areas of file storage and distributed systems. Concepts from information theory, as they relate to the goals and evaluation of data compression methods, are discussed briefly. A framework for evaluation and comparison of methods is constructed and applied to the algorithms presented. Comparisons of both theoretical and empirical natures are reported and possibilities for future research are suggested. INTRODUCTION Data compression is often referred to as coding, where coding is a very general term encompassing any special representation of data which satisfies a given need. Information theory is defined to be the study of eff...
6|The quadtree and related hierarchical data structures|A tutorial survey is presented of the quadtree and related hierarchical data structures. They are based on the principle of recursive decomposition. The emphasis is on the representation of data used in applications in image processing, computer graphics, geographic information systems, and robotics. There is a greater emphasis on region data (i.e., two-dimensional shapes) and to a lesser extent on point, curvilinear, and threedimensional data. A number of operations in which such data structures find use are examined in greater detail.
7|A Locally Adaptive Data Compression Scheme|  A data compression scheme that exploits locality of reference, such as occurs when words are used frequently over short intervals and then fall into long periods of disuse, is described. The scheme is based on a simple heuristic for self-organizing sequential search and on variable-length encodings of integers. We prove that it never performs much worse than Huffman coding and can perform substantially better; experiments on real files show that its performance is usually quite close to that of Huffman coding. Our scheme has many implementation advantages: it is simple, allows fast encoding and decod-ing, and requires only one pass over the data to be compressed (static Huffman coding takes two passes).
8|Fast Algorithms for Manipulating Formal Power Series  |  The classical algorithms require order n ~ operations to compute the first n terms in the reversion of a power series or the composition of two series, and order nelog n operations if the fast Founer transform is used for power series multiplication In this paper we show that the composition and reversion problems are equivalent (up to constant factors), and we give algorithms which require only order (n log n) ~/2 operations In many cases of practical importance only order n log n operations are required, these include certain special functions of power series and power series solution of certain differential equations Applications to root-finding methods which use inverse interpolation and to queueing theory are described, some results on multivariate power series are stated, and several open questions are mentioned.
9|Design and analysis of dynamic Huffman codes|Abstract. A new one-pass algorithm for constructing dynamic Huffman codes is introduced and analyzed. We also analyze the one-pass algorithm due to Failer, Gallager, and Knuth. In each algorithm, both the sender and the receiver maintain equivalent dynamically varying Huffman trees, and the coding is done in real time. We show that the number of bits used by the new algorithm to encode a message containing t letters is &lt; t bits more than that used by the conventional two-pass Huffman scheme, independent of the alphabet size. This is best possible in the worst case, for any one-pass Huffman method. Tight upper and lower bounds are derived. Empirical tests show that the encodings produced by the new algorithm are shorter than those of the other one-pass algorithm and, except for long messages, are shorter than those of the two-pass method. The new algorithm is well suited for on-line encoding/decoding in data networks and for file compression.
10|Interval and Recency Rank Source Coding: Two On-Line Adaptive Variable-Length Schemes|In the schemes presented the encoder maps each message into a codeword in a prefix-free codeword set. In interval encoding the codeword is indexed by the interval since the last previous occurrence of that message, and the codeword set must be countably infinite. In recency rank encoding the codeword is indexed by the number of distinct messages in that interval, and there must be no fewer codewords than messages. The decoder decodes each codeword on receipt. Users need not know message probabilities, but must agree on indexings, of the codeword set in an order of increasing length and of the message set in some arbitrary order. The  average codeword length over a communications bout is never much larger  than the value for an off-line scheme which maps the jth most frequent  message in the bout into the jth shortest codeword in the given set, and is never too much larger than the value for off-line Huffman encoding of messages into the best codeword set for the bout message frequencies.
11|Data Compression Using Adaptive Coding and Partial String Matching|The recently developed technique of arithmetic coding, in conjunction with a Markov model of the source, is a powerful method of data compression in situations where a linear treatment is inappropriate. Adaptive coding allows the model to be constructed dynamically by both encoder and decoder during the course of the transmission, and has been shown to incur a smaller coding overhead than explicit transmission of the model&#039;s statistics. But there is a basic conflict between the desire to use high-order Markov models and the need to have them formed quickly as the initial part of the message is sent. This paper describes how the conflict can be resolved with partial string matching, and reports experimental results which show that mixed-case English text can be coded in as little as 2.2 bits/ character with no prior knowledge of the source.
12|Data compression and harmonic analysis|  In this paper we review some recent interactions between harmonic analysis and data compression. The story goes back of course to Shannon’s R(D) theory...
13|A theory for multiresolution signal decomposition : the wavelet representation|Abstract-Multiresolution representations are very effective for analyzing the information content of images. We study the properties of the operator which approximates a signal at a given resolution. We show that the difference of information between the approximation of a signal at the resolutions 2 ’ + ’ and 2jcan be extracted by decomposing this signal on a wavelet orthonormal basis of L*(R”). In LL(R), a wavelet orthonormal basis is a family of functions (  @ w (2’ ~-n)),,,“jEZt, which is built by dilating and translating a unique function t+r (xl. This decomposition defines an orthogonal multiresolution representation called a wavelet representation. It is computed with a pyramidal algorithm based on convolutions with quadrature mirror lilters. For images, the wavelet representation differentiates several spatial orientations. We study the application of this representation to data compression in image coding, texture discrimination and fractal analysis. Index Terms-Coding, fractals, multiresolution pyramids, quadrature mirror filters, texture discrimination, wavelet transform. I I.
14|A new fast and efficient image codec based on set partitioning in hierarchical trees|Albstruct- Embedded zerotree wavelet (EZW) coding, intro-duced by J. M. Shapiro, is a very effective and computationally simple technique for image compression. Here we offer an alter-native explanation of the principles of its operation, so that the reasons for its excellent performance can be better understood. Theise principle9 are partial ordering by magnitude with a set parlitioning sorting algorithm, ordered bit plane transmission, and exploitation of self-similarity across different scales of an image wavelet transform. Moreover, we present a new and different implementation based on set partitioning in hierarchical trees (SPIHT), which provides even better performance than our previously reported extension of EZW that surpassed the performance of the original EZW. The image coding results, calculated from actual file sizes and images reconstructed by the decoding algorithm, are either comparable to or surpass previous results obtained through much more sophisticated and computationally complex methods. In addition, the new coding and decoding procedures are extremely fast, and they can be made even faster, with only small loss in performance, by omitting entropy coding of the bit stream by arithmetic code. I.
16|Quantization|  The history of the theory and practice of quantization dates to 1948, although similar ideas had appeared in the literature as long ago as 1898. The fundamental role of quantization in modulation and analog-to-digital conversion was first recognized during the early development of pulsecode modulation systems, especially in the 1948 paper of Oliver, Pierce, and Shannon. Also in 1948, Bennett published the first high-resolution analysis of quantization and an exact analysis of quantization noise for Gaussian processes, and Shannon published the beginnings of rate distortion theory, which would provide a theory for quantization as analog-to-digital conversion and as data compression. Beginning with these three papers of fifty years ago, we trace the history of quantization from its origins through this decade, and we survey the fundamentals of the theory and many of the popular and promising techniques for quantization. 
17|Image Coding based on Mixture Modeling of Wavelet Coefficients and a Fast Estimation-Quantization Framework|We introduce a new image compression paradigm that combines compression efficiency with speed, and is based on an independent &#034;infinite&#034; mixture model which accurately captures the space-frequency characterization of the wavelet image representation. Specifically, we model image wavelet coefficients as being drawn from an independent Generalized Gaussian distribution field, of fixed unknown shape for each subband, having zero mean and unknown slowly spatiallyvarying variances. Based on this model, we develop a powerful &#034;on the fly&#034;  Estimation-Quantization (EQ) framework that consists of: (i) first finding the Maximum-Likelihood estimate of the individual spatially-varying coefficient field variances based on causal and quantized spatial neighborhood contexts; and (ii) then applying an off-line Rate-Distortion (R-D) optimized quantization /entropy coding strategy, implemented as a fast lookup table, that is optimally matched to the derived variance estimates. A distinctive feature of o...
18|Origins of Scaling in Natural Images|One of the most robust qualities of our visual world is the scaleinvariance of natural images. Not only has scaling been found in different visual environments, but the phenomenon also appears to be calibration independent. This paper proposes a simple property of natural images which explains this robustness: They are collages of regions corresponding to statistically independent &#034;objects&#034;. Evidence is provided for these objects having a power-law distribution of sizes within images, from which follows scaling in natural images. It is commonly suggested that scaling instead results from edges, each with power spectrum 1/k². This hypothesis is refuted by example. 
19|CART and Best-Ortho-Basis: A Connection|We study what we call \Dyadic CART &#034;  { a method of nonparametric regression which constructs a recursive partition by optimizing a complexity-penalized sum of squares, where the optimization is over all recursive partitions arising from midpoint splits. We show that the method is adaptive to unknown degree of anisotropic smoothness. Speci cally, consider the \mixed smoothness &#034; classes consisting of bivariate functions f(x1;x2) whose nite di erence of distance h in direction i is bounded in L p norm by Ch i, i =1;2. We show that Dyadic CART, with an appropriate complexity penalty 2 parameter Const log(n), is within logarithmic terms of minimax over every mixed smoothness class 0 &lt;C&lt;1,0&lt; 1; 2 1. The proof shows that Dyadic CART is identical to a certain adaptive Best-Ortho-Basis Algorithm based on the library of all anisotropic Haar bases. Then it applies empirical basis selection ideas of Donoho and Johnstone (1994). The basis empirically selected by Dyadic CART isshown to be nearly as good as a basis ideally adapted to the underlying f. The risk of estimation in an ideally adapted anisotropic Haar basis is shown to be comparable to the minimax risk over mixed smoothness classes. Underlying the success of this argument is harmonic analysis of mixed smoothness classes. We show that for each mixed smoothness class there is an anisotropic Haar basis which is a best orthogonal basis for representing that smoothness class; the basis is optimal not just within the library of anisotropic Haar bases, but among all orthogonal bases of L 2 [0; 1] 2.
21|Unconditional bases and bit-level compression|A previous article [2] gave results showing that an orthogonal basis which is an unconditional basis for a functional class F furnishes an optimal representation of elements of F for certain de-noising and compression tasks. Since publication of that article, the author received several queries which pointed out that the definition of compression in that article was
22|Optimal Prefetching via Data Compression|Caching and prefetching are important mechanisms for speeding up access time to data on secondary storage. Recent work in competitive online algorithms has uncovered several promising new algorithms for caching. In this paper we apply a form of the competitive philosophy for the first time to the problem of prefetching to develop an optimal universal prefetcher in terms of fault ratio, with particular applications to large-scale databases and hypertext systems. Our prediction algorithms for prefetching are novel in that they are based on data compression techniques that are both theoretically optimal and good in practice. Intuitively, in order to compress data effectively, you have to be able to predict future data well, and thus good data compressors should be able to predict well for purposes of prefetching. We show for powerful models such as Markov sources and nth order Markov sources that the page fault rates incurred by our prefetching algorithms are optimal in the limit for almost all sequences of page requests.
23|Learnability  and the Vapnik-Chervonenkis dimension| Valiant’s learnability model is extended to learning classes of concepts defined by regions in Euclidean space E”. The methods in this paper lead to a unified treatment of some of Valiant’s results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufftcient conditions are provided for feasible learnability. 
24|Design and Evaluation of a Compiler Algorithm for Prefetching|Software-controlled data prefetching is a promising technique for improving the performance of the memory subsystem to match today&#039;s high-performance processors. While prefetching is useful in hiding the latency, issuing prefetches incurs an instruction overhead and can increase the load on the memory subsystem. As a result, care must be taken to ensure that such overheads do not exceed the benefits.
25|The OO7 benchmark|The OO7 Benchmark represents a comprehensive test of OODBMS performance. In this report we describe the benchmark and present performance results from its implementation in four OODB systems. It is our hope that the OO7 Benchmark will provide useful insight for end-users evaluating the performance of OODB systems; we also hope that the research community will nd that OO7 provides a database schema, instance, and workload that is useful for evaluating new techniques and algorithms for OODBMS implementation.
26|Arithmetic coding|Arithmetic coding is a data compression technique that encodes data (the data string) by creating a code string which represents a fractional value on the number line between 0 and 1. The coding algorithm is symbolwise recursive; i.e., it operates upon and encodes (decodes) one data symbol per iteration or recursion. On each recursion, the algorithm successively partitions an interval
27|Efficient Distribution-free Learning of Probabilistic Concepts|In this paper we investigate a new formal model of machine learning in which the concept (boolean function) to be learned may exhibit uncertain or probabilistic behavior---thus, the same input may sometimes be classified as a positive example and sometimes as a negative example. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. We adopt from the Valiant model of learning [27] the demands that learning algorithms be efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. In addition to giving many efficient algorithms for learning natural classes of p-concepts, we study and develop in detail an underlying theory of learning p-concepts. 1 Introduction  Consider the following scenarios: A meteorologist is attempting to predict tomorrow&#039;s weather as accurately as pos...
28|Universal prediction of individual sequences|Abstruct-The problem of predicting the next outcome of an individual binary sequence using finite memory, is considered. The finite-state predictability of an infinite sequence is defined as the minimum fraction of prediction errors that can be made by any finite-state (FS) predictor. It is proved that this FS pre-dictability can be attained by universal sequential prediction schemes. Specifically, an efficient prediction procedure based on the incremental parsing procedure of the Lempel-Ziv data com-pression algorithm is shown to achieve asymptotically the FS predictability. Finally, some relations between compressibility and predictability are pointed out, and the predictability is proposed as an additional measure of the complexity of a sequence. Index Terms-Predictability, compressibility, complexity, fi-nite-state machines, Lempel- Ziv algorithm.
29|Competitive Paging Algorithms|The paging problem is that of deciding which pages to keep in a memory of k pages in order to minimize the number of page faults. We develop the marking algorithm, a randomized on-line algorithm for the paging problem. We prove that its expected cost on any sequence of requests is within a factor of 2Hk of optimum. (Where Hk is the kth harmonic number, which is roughly In k.) The best such factor that can be achieved is Hk. This is in contrast to deterministic algorithms, which cannot be guaranteed to be within a factor smaller than k of optimum. An alternative to comparing an on-line algorithm with the optimum off-line algorithm is the idea of comparing it to several other on-line algorithms. We have obtained results along these lines for the paging problem. Given a set of on-line algorithms and a set
30|Reducing Memory Latency via Non-blocking and Prefetching Caches|Non-blocking caches and prefetching caches are two techniques for hiding memory latency by exploiting the overlap of processor computations with data accesses. A non-blocking cache allows execution to proceed concurrently with cache misses as long as dependency constraints are observed, thus exploiting post-miss operations. A prefetching cache generates prefetch requests to bring data in the cache before it is actually needed, thus allowing overlap with pre-miss computations. In this paper, we evaluate the effectiveness of these two hardware-based schemes. We propose a hybrid design based on the combination of these approaches. We also consider compiler-based optimizations to enhance the effectiveness of non-blocking caches. Results from instruction level simulations on the SPEC benchmarks show that the hardware prefetching caches generally outperform non-blocking caches. Also, the relative effectiveness of non-blocking caches is more adversely affected by an increase in memory latency...
32|Competitive Paging With Locality of Reference|Abstract The Sleator-Tarjan competitive analysis of paging [Comm. of the ACM; 28:202- 208, 1985] gives us the ability to make strong theoretical statements about the performance of paging algorithms without making probabilistic assumptions on the input. Nevertheless practitioners voice reservations about the model, citing its inability to discern between LRU and FIFO (algorithms whose performances differ markedly in practice), and the fact that the theoretical competitiveness of LRU is much larger than observed in practice. In addition, we would like to address the following important question: given some knowledge of a program&#039;s reference pattern, can we use it to improve paging performance on that program?
33|Fido: A cache that learns to fetch|This paper describes Fido, a predictive cache [Palmer 19901 that prefetches by employing an associative memory to recognize access patterns within a context over time. Repeated training adapts the associative memory contents to data and access pattern changes, allowing on-line access predictions for prefetching. We discuss two salient elements of Fido- MLP, a replace-ment policy for managing prefetched objects, and Estimating Prophet, the component that recognizes patterns and predicts access. We then present some early simulation results which suggest that predictive caching works well and conclude that it is a promising method. 1
34|On the Computational Complexity of Approximating Distributions by Probabilistic Automata|We introduce a rigorous performance criterion for training algorithms for probabilistic automata (PAs) and hidden Markov models (HMMs), used extensively for speech recognition, and analyze the complexity of the training problem as a computational problem. The PA training problem is the problem of approximating an arbitrary, unknown source distribution by distributions generated by a PA. We investigate the following question about this important, well-studied problem: Does there exist an efficient training algorithm such that the trained PAs provably converge to a model close to an optimum one with high confidence, after only a feasibly small set of training data? We model this problem in the framework of computational learning theory and analyze the sample as well as computational complexity. We show that the number of examples required for training PAs is moderate -- essentially linear in the number of transition probabilities to be trained and a low-degree polynomial in the example l...
35|Strongly Competitive Algorithms for Paging with Locality of Reference| What is the best paging algorithm if one has partial information about the possible sequences of page requests? We give a partial answer to this question, by presenting the analysis of strongly competitive paging algorithms in the access graph model. This model restricts page requests so that they conform to a notion of locality of reference, given by an arbitrary access graph. We first consider optimal algorithms for undirected access graphs. Borodin et al. [2] define an algorithm, called FAR, and prove that it is within a logarithmic factor of the optimal on-line algorithm. We prove that FAR is in fact strongly competitive, i.e. within a constant factor of the optimum. For directed access graphs, we present an algorithm that is strongly competitive on structured program graphs-- graphs which model a subset of the request sequences of structured programs.
36|On the Performance of Object Clustering Techniques|We investigate the performance of some of the best-known object clustering algorithms on four different workloads based upon the Tektronix benchmark. For all four workloads, stochastic clustering gave the best performance for a variety of performance metrics. Since stochastic clustering is computationally expensive, it is interesting that for every workload there was at least one cheaper clustering algorithm that matched or almost matched stochastic clustering. Unfortunately, for each workload, the algorithm that approximated stochastic clustering was different. Our experiments also demonstrated that even when the workload and object graph are fixed, the choice of the clustering algorithm depends upon the goals of the system. For example, if the goal is to perform well on traversals of small portions of the database starting with a cold cache, the important metric is the per-traversal expansion factor, and a well-chosen placement tree will be nearly optimal; if the goal is to achieve a...
37|MARKOV PAGING|This paper considers the problemof paging under the assumption that the sequence of pages accessed is generated by a Markov chain. We use this model to study the fault-rate of paging algorithms. We first draw on the theory of Markov decision processes to characterize the paging algorithmthat achieves optimal fault-rate on any Markov chain. Next, we address the problemof devising a paging strategy with low fault-rate for a given Markov chain. We show that a number of intuitive approaches fail. Our main result is a polynomial-time procedure that, on any Markov chain, will give a paging algorithm with fault-rate at most a constant times optimal. Our techniques show also that some algorithms that do poorly in practice fail in the Markov setting, despite known (good) performance guarantees when the requests are generated independently from a probability distribution.
38|Software Support for Speculative Loads|This paper describes a simple hardware mechanism and related compiler support for software-controlled speculative  loads. The compiler issues speculative load instructions based on anticipated data references and the ability of the memory system to hide memory latency in high-performance processors. The architectural support for such a mechanism is simple and minimal, yet handles faults gracefully. We have simulated the speculative load mechanism based on a MIPS processor and a detailed memory system. The results of scientific kernel loops indicate that our speculative load technique is an effective approaches to hiding memory latency. 1 Introduction  The performance gap between processors and memory has widened in the last few years. In the last decade, microprocessor speeds have increased at a rate of 50% to 100% each year whereas DRAM speeds have increased at a rate of 10% or less each year [13]. As the performance gap becomes wider, high-performance processors become more sensitive...
39|A Status Report on Research in Transparent Informed Prefetching|This paper focuses on extending the power of caching and prefetching to reduce file read latencies by exploiting application level hints about future I/O accesses. We argue that systems that disclose high-level knowledge can transfer optimization information across module boundaries in a manner consistent with sound software engineering principles. Such Transparent Informed Prefetching (TIP) systems provide a technique for converting the high throughput of new technologies such as disk arrays and log-structured file systems into low latency for applications. Our preliminary experiments show that even without a highthroughput I/O subsystem TIP yields reduced execution time of up to 30% for applications obtaining data from a remote file server and up to 13% for applications obtaining data from a single local disk. These experiments indicate that greater performance benefits will be available when TIP is integrated with low level resource management policies and highly parallel I/O subsys...
40|Analysis of Arithmetic Coding for Data Compression|Arithmetic coding, in conjunction with a suitable probabilistic model, can provide nearly optimal data compression. In this article we analyze the effect that the model and the particular implementation of arithmetic coding have on the code length obtained. Periodic scaling is often used in arithmetic coding implementations to reduce time and storage requirements; it also introduces a recency effect which can further affect compression. Our main contribution is introducing the concept of weighted entropy and using it to characterize in an elegant way the effect that periodic scaling has on the code length. We explain why and by how much scaling increases the code length for files with a homogeneous distribution of symbols, and we characterize the reduction in code length due to scaling for files exhibiting locality of reference. We also give a rigorous proof that the coding effects of rounding scaled weights, using integer arithmetic, and encoding end-of-file are negligible.  
41|Discrete sequence prediction and its applications|Learning from experience to predict sequences of discrete symbols is a fundamental problem in machine learning with many applications. We present a simple and practi-ca] algorithm (TDAG) for discrete sequence prediction. Based on a text-compression method, the TDAG algorithm limits the growth of storage by retaining the most likely prediction contexts and discarding (forgetting) less likely ones. The storage/speed tradeoffs are parameterized so that the algorithm can be used in a variety of applications. Our experiments verify its performance on data compression tasks and show how it applies to two problems: dynamica]ly optimizing Prolog programs for good average-case behavior and maintaining a cache for a database on mass storage.
42|Optimal Prediction for Prefetching in the Worst Case|Response time delays caused by I/O are a major problem in many systems and database applications. Prefetching and cache replacement methods are attracting renewed attention because of their success in avoiding costly I/Os. Prefetching can be looked upon as a type of online sequential prediction, where the predictions must be accurate as well as made in a computationally efficient way. Unlike other online problems, prefetching cannot admit a competitive analysis, since the optimal offline prefetcher incurs no cost when it knows the future page requests. Previous analytical work on prefetching [J. Assoc. Comput. Mach., 143 (1996), pp. 771–793] consisted of modeling the user as a probabilistic Markov source. In this paper, we look at the much stronger form of worst-case analysis and derive a randomized algorithm for pure prefetching. We compare our algorithm for every page request sequence with the important class of finite state prefetchers, making no assumptions as to how the sequence of page requests is generated. We prove analytically that the fault rate of our online prefetching algorithm converges almost surely for every page request sequence to the fault rate of the optimal finite state prefetcher for the sequence. This analysis model can be looked upon as a generalization of the competitive framework, in that it compares an online algorithm in a worst-case manner over all sequences with a powerful yet nonclairvoyant opponent. We simultaneously achieve the computational goal of implementing our prefetcher in optimal constant expected time per prefetched page using the optimal dynamic discrete random variate generator of Matias, Vitter, and Ni [Proc. 4th Annual SIAM/ACM
43|Compressive sampling|  Conventional wisdom and common practice in acquisition and reconstruction of images from frequency data follow the basic principle of the Nyquist density sampling theory. This principle states that to reconstruct an image, the number of Fourier samples we need to acquire must match the desired resolution of the image, i.e. the number of pixels in the image. This paper surveys an emerging theory which goes by the name of “compressive sampling” or “compressed sensing,” and which says that this conventional wisdom is inaccurate. Perhaps surprisingly, it is possible to reconstruct images or signals of scientific interest accurately and sometimes even exactly from a number of samples which is far smaller than the desired resolution of the image/signal, e.g. the number of pixels in the image. It is believed that compressive sampling has far reaching implications. For example, it suggests the possibility of new data acquisition protocols that translate analog information into digital form with fewer sensors than what was considered necessary. This new sampling theory may come to underlie procedures for sampling and compressing data simultaneously. In this short survey, we provide some of the key mathematical insights underlying this new theory, and explain some of the interactions between compressive sampling and other fields such as statistics, information theory, coding theory, and theoretical computer science.
45|Compressed sensing|We study the notion of Compressed Sensing (CS) as put forward in [14] and related work [20, 3, 4]. The basic idea behind CS is that a signal or image, unknown but supposed to be compressible by a known transform, (eg. wavelet or Fourier), can be subjected to fewer measurements than the nominal number of pixels, and yet be accurately reconstructed. The samples are nonadaptive and measure ‘random ’ linear combinations of the transform coefficients. Approximate reconstruction is obtained by solving for the transform coefficients consistent with measured data and having the smallest possible `1 norm. We perform a series of numerical experiments which validate in general terms the basic idea proposed in [14, 3, 5], in the favorable case where the transform coefficients are sparse in the strong sense that the vast majority are zero. We then consider a range of less-favorable cases, in which the object has all coefficients nonzero, but the coefficients obey an `p bound, for some p ? (0, 1]. These experiments show that the basic inequalities behind the CS method seem to involve reasonable constants. We next consider synthetic examples modelling problems in spectroscopy and image pro-
46|ATOMIC DECOMPOSITION BY BASIS PURSUIT|The Time-Frequency and Time-Scale communities have recently developed a large number of overcomplete waveform dictionaries -- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the Method of Frames (MOF), Matching Pursuit (MP), and, for special dictionaries, the Best Orthogonal Basis (BOB). Basis Pursuit (BP) is a principle for decomposing a signal into an &#034;optimal&#034; superposition of dictionary elements, where optimal means having the smallest l 1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP and BOB, including better sparsity, and super-resolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation de-noising, and multi-scale edge denoising. Basis Pursuit in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.
47|Robust Uncertainty Principles: Exact Signal Reconstruction From Highly Incomplete Frequency Information|This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal and a randomly chosen set of frequencies. Is it possible to reconstruct from the partial knowledge of its Fourier coefficients on the set? A typical result of this paper is as follows. Suppose that is a superposition of spikes @ Aa @ A @ A obeying @?? ? A I for some constant H. We do not know the locations of the spikes nor their amplitudes. Then with probability at least I @ A, can be reconstructed exactly as the solution to the I minimization problem I aH @ A s.t. ” @ Aa ”  @ A for all
48|Near Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?|Suppose we are given a vector f in RN. How many linear measurements do we need to make about f to be able to recover f to within precision ? in the Euclidean (l2) metric? Or more exactly, suppose we are interested in a class F of such objects— discrete digital signals, images, etc; how many linear measurements do we need to recover objects from this class to within accuracy ?? This paper shows that if the objects of interest are sparse or compressible in the sense that the reordered entries of a signal f ? F decay like a power-law (or if the coefficient sequence of f in a fixed basis decays like a power-law), then it is possible to reconstruct f to within very high accuracy from a small number of random measurements. typical result is as follows: we rearrange the entries of f (or its coefficients in a fixed basis) in decreasing order of magnitude |f | (1)  = |f | (2)  =... = |f | (N), and define the weak-lp ball as the class F of those elements whose entries obey the power decay law |f | (n)  = C · n -1/p. We take measurements <f, Xk>, k = 1,..., K, where the Xk are N-dimensional Gaussian
49|Decoding by Linear Programming|This paper considers the classical error correcting problem which is frequently discussed in coding theory. We wish to recover an input vector f ? Rn from corrupted measurements y = Af + e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the l1-minimization problem (?x?l1:= i |xi|) min g?R n ?y - Ag?l1 provided that the support of the vector of errors is not too large, ?e?l0: = |{i: ei ?= 0} |  = ? · m for some ?&gt; 0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work [5]. Finally, underlying the success of l1 is a crucial property we call the uniform uncertainty principle that we shall describe in detail.
50|The Dantzig Selector: Statistical Estimation When p Is Much Larger Than n|In many important statistical applications, the number of variables or parameters p is much larger than the number of observations n. Suppose then that we have observations y = Xß + z, where ß ? Rp is a parameter vector of interest, X is a data matrix with possibly far fewer rows than columns, n « p, and the zi’s are i.i.d. N(0,s2). Is it possible to estimate ß reliably based on the noisy data y? To estimate ß, we introduce a new estimator—we call it the Dantzig selector—which is a solution to the l1-regularization problem min ˜ß?R p ? ˜ß?l1 subject to ?X * r?l 8  = (1 + t-1 v) 2logp · s, where r is the residual vector y - X ˜ß and t is a positive scalar. We show that if X obeys a uniform uncertainty principle (with unit-normed columns) and if the true parameter vector ß is sufficiently sparse (which here roughly guarantees that the model is identifiable), then with very large probability,
51|The space complexity of approximating the frequency moments|The frequency moments of a sequence containing mi elements of type i, for 1 = i = n, are the numbers Fk = ?n i=1 mki. We consider the space complexity of randomized algorithms that approximate the numbers Fk, when the elements of the sequence are given one by one and cannot be stored. Surprisingly, it turns out that the numbers F0, F1 and F2 can be approximated in logarithmic space, whereas the approximation of Fk for k = 6 requires n?(1) space. Applications to data bases are mentioned as well. 
52| Optimally sparse representation in general (non-orthogonal) dictionaries via l¹ minimization  (2002) |Given a ‘dictionary’ D = {dk} of vectors dk, we seek to represent a signal S as a linear combination S = ? k ?(k)dk, with scalar coefficients ?(k). In particular, we aim for the sparsest representation possible. In general, this requires a combinatorial optimization process. Previous work considered the special case where D is an overcomplete system consisting of exactly two orthobases, and has shown that, under a condition of mutual incoherence of the two bases, and assuming that S has a sufficiently sparse representation, this representation is unique and can be found by solving a convex optimization problem: specifically, minimizing the l¹ norm of the coefficients ?. In this paper, we obtain parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems. We introduce the Spark, ameasure of linear dependence in such a system; it is the size of the smallest linearly dependent subset (dk). We show that, when the signal S has a representation using less than Spark(D)/2 nonzeros, this representation is necessarily unique. We
53|Uncertainty principles and ideal atomic decomposition|Suppose a discrete-time signal S(t), 0 t&lt;N, is a superposition of atoms taken from a combined time/frequency dictionary made of spike sequences 1ft = g and sinusoids expf2 iwt=N) = p N. Can one recover, from knowledge of S alone, the precise collection of atoms going to make up S? Because every discrete-time signal can be represented as a superposition of spikes alone, or as a superposition of sinusoids alone, there is no unique way of writing S as a sum of spikes and sinusoids in general. We prove that if S is representable as a highly sparse superposition of atoms from this time/frequency dictionary, then there is only one such highly sparse representation of S, and it can be obtained by solving the convex optimization problem of minimizing the `1 norm of the coe cients among all decompositions. Here \highly sparse &#034; means that Nt + Nw &lt; p N=2 where Nt is the number of time atoms, Nw is the number of frequency atoms, and N is the length of the discrete-time signal.
54|For Most Large Underdetermined Systems of Linear Equations the Minimal l1-norm Solution is also the Sparsest Solution|We consider linear equations y = Fa where y is a given vector in R n, F is a given n by m matrix with n &lt; m = An, and we wish to solve for a ? R m. We suppose that the columns of F are normalized to unit l 2 norm 1 and we place uniform measure on such F. We prove the existence of ? = ?(A) so that for large n, and for all F’s except a negligible fraction, the following property holds: For every y having a representation y = Fa0 by a coefficient vector a0 ? R m with fewer than ? · n nonzeros, the solution a1 of the l 1 minimization problem min ?x?1 subject to Fa = y is unique and equal to a0. In contrast, heuristic attempts to sparsely solve such systems – greedy algorithms and thresholding – perform poorly in this challenging setting. The techniques include the use of random proportional embeddings and almost-spherical sections in Banach space theory, and deviation bounds for the eigenvalues of random Wishart matrices.
55|Just Relax: Convex Programming Methods for Identifying Sparse Signals in Noise|This paper studies a difficult and fundamental problem that arises throughout electrical engineering, applied mathematics, and statistics. Suppose that one forms a short linear combination of elementary signals drawn from a large, fixed collection. Given an observation of the linear combination that has been contaminated with additive noise, the goal is to identify which elementary signals participated and to approximate their coefficients. Although many algorithms have been proposed, there is little theory which guarantees that these algorithms can accurately and efficiently solve the problem. This paper studies a method called convex relaxation, which attempts to recover the ideal sparse signal by solving a convex program. This approach is powerful because the optimization can be completed in polynomial time with standard scientific software. The paper provides general conditions which ensure that convex relaxation succeeds. As evidence of the broad impact of these results, the paper describes how convex relaxation can be used for several concrete signal recovery problems. It also describes applications to channel coding, linear regression, and numerical analysis.
56|New tight frames of curvelets and optimal representations of objects with piecewise C² singularities|This paper introduces new tight frames of curvelets to address the problem of finding optimally sparse representations of objects with discontinuities along C2 edges. Conceptually, the curvelet transform is a multiscale pyramid with many directions and positions at each length scale, and needle-shaped elements at fine scales. These elements have many useful geometric multiscale features that set them apart from classical multiscale representations such as wavelets. For instance, curvelets obey a parabolic scaling relation which says that at scale 2-j, each element has an envelope which is aligned along a ‘ridge ’ of length 2-j/2 and width 2-j. We prove that curvelets provide an essentially optimal representation of typical objects f which are C2 except for discontinuities along C2 curves. Such representations are nearly as sparse as if f were not singular and turn out to be far more sparse than the wavelet decomposition of the object. For instance, the n-term partial reconstruction f C n obtained by selecting the n largest terms in the curvelet series obeys ?f - f C n ? 2 L2 = C · n-2 · (log n) 3, n ? 8. This rate of convergence holds uniformly over a class of functions which are C 2 except for discontinuities along C 2 curves and is essentially optimal. In comparison, the squared error of n-term wavelet approximations only converges as n -1 as n ? 8, which is considerably worst than the optimal behavior.
57|Sampling signals with finite rate of innovation|Abstract—Consider classes of signals that have a finite number of degrees of freedom per unit of time and call this number the rate of innovation. Examples of signals with a finite rate of innovation include streams of Diracs (e.g., the Poisson process), nonuniform splines, and piecewise polynomials. Even though these signals are not bandlimited, we show that they can be sampled uniformly at (or above) the rate of innovation using an appropriate kernel and then be perfectly reconstructed. Thus, we prove sampling theorems for classes of signals and kernels that generalize the classic “bandlimited and sinc kernel ” case. In particular, we show how to sample and reconstruct periodic and finite-length streams of Diracs, nonuniform splines, and piecewise polynomials using sinc and Gaussian kernels. For infinite-length signals with finite local rate of innovation, we show local sampling and reconstruction based on spline kernels. The key in all constructions is to identify the innovative part of a signal (e.g., time instants and weights of Diracs) using an annihilating or locator filter: a device well known in spectral analysis and error-correction coding. This leads to standard computational procedures for solving the sampling problem, which we show through experimental results. Applications of these new sampling results can be found in signal processing, communications systems, and biological systems. Index Terms—Analog-to-digital conversion, annihilating filters, generalized sampling, nonbandlimited signals, nonuniform splines, piecewise polynomials, poisson processes, sampling. I.
58|On sparse representations in arbitrary redundant bases|Abstract—The purpose of this contribution is to generalize some recent results on sparse representations of signals in redundant bases. The question that is considered is the following: given a matrix of dimension ( ) with and a vector = , find a sufficient condition for to have a unique sparsest representation as a linear combination of columns of. Answers to this question are known when is the concatenation of two unitary matrices and either an extensive combinatorial search is performed or a linear program is solved. We consider arbitrary matrices and give a sufficient condition for the unique sparsest solution to be the unique solution to both a linear program or a parametrized quadratic program. The proof is elementary and the possibility of using a quadratic program opens perspectives to the case where = + with a vector of noise or modeling errors. Index Terms—Basis pursuit, global matched filter, linear program, quadratic program, redundant dictionaries, sparse representations. I.
59|A generalized uncertainty principle and sparse representation in pairs of bases|An elementary proof of a basic uncertainty principle concerning pairs of representations of R N vectors in different orthonormal bases is provided. The result, slightly stronger than stated before, has a direct impact on the uniqueness property of the sparse representation of such vectors using pairs of orthonormal bases as overcomplete dictionaries. The main contribution in this paper is the improvement of an important result due to Donoho and Huo concerning the replacement of the l0 optimization problem by a linear programming minimization when searching for the unique sparse representation. 1
60|Signal reconstruction from noisy random projections|Recent results show that a relatively small number of random projections of a signal can contain most of its salient information. It follows that if a signal is compressible in some orthonormal basis, then a very accurate reconstruction can be obtained from random projections. We extend this type of result to show that compressible signals can be accurately recovered from random projections contaminated with noise. We also propose a practical iterative algorithm for signal reconstruction, and briefly discuss potential applications to coding, A/D conversion, and remote wireless sensing. Index Terms sampling, signal reconstruction, random projections, denoising, wireless sensor networks
61|Signal recovery from partial information via Orthogonal Matching Pursuit| This article demonstrates theoretically and empirically that a greedy algorithm called Orthogonal Matching Pursuit (OMP) can reliably recover a signal with m nonzero entries in dimension d given O(m ln d) random linear measurements of that signal. This is a massive improvement over previous results for OMP, which require O(m 2) measurements. The new results for OMP are comparable with recent results for another algorithm called Basis Pursuit (BP). The OMP algorithm is much faster and much easier to implement, which makes it an attractive alternative to BP for signal recovery problems.  
62|Weierstrass and Approximation Theory |We discuss and examine Weierstrass&#039; main contributions to approximation theory.
63|Quantitative Robust Uncertainty Principles and Optimally Sparse Decompositions|In this paper, we develop a robust uncertainty principle for finite signals in C N which states that for nearly all choices T, ? ? {0,..., N - 1} such that |T | + |? |  ? (log N) -1/2 · N, there is no signal f supported on T whose discrete Fourier transform ˆ f is supported on ?. In fact, we can make the above uncertainty principle quantitative in the sense that if f is supported on T, then only a small percentage of the energy (less than half, say) of ˆ f is concentrated on ?. As an application of this robust uncertainty principle (QRUP), we consider the problem of decomposing a signal into a sparse superposition of spikes and complex sinusoids f(s)  = ? a1(t)d(s - t) + ? a2(?)e i2p?s/N /  v N. t?T We show that if a generic signal f has a decomposition (a1, a2) using spike and frequency locations in T and ? respectively, and obeying ??? |T | + |? |  = Const · (log N) -1/2 · N, then (a1, a2) is the unique sparsest possible decomposition (all other decompositions have more non-zero terms). In addition, if |T | + |? |  = Const · (log N) -1 · N, then the sparsest (a1, a2) can be found by solving a convex optimization problem. Underlying our results is a new probabilistic approach which insists on finding the correct uncertainty relation or the optimally sparse solution for nearly all subsets but not necessarily all of them, and allows to considerably sharpen previously known results [9, 10]. In fact, we show that the fraction of sets (T, ?) for which the above properties do not hold can be upper bounded by quantities like N -a for large values of a. The QRUP (and the application to finding sparse representations) can be extended to general pairs of orthogonal bases F1, F2 of C N. For nearly all choices G1, G2 ? {0,..., N - 1} obeying |G1 | + |G2 |  ? µ(F1, F2) -2 · (log N) -m, where m = 6, there is no signal f such that F1f is supported on G1 and F2f is supported on G2 where µ(F1, F2) is the mutual coherence between F1 and F2.
64|Neighborly Polytopes and Sparse Solutions of Underdetermined Linear Equations|Consider a d × n matrix A, with d &lt; n. The problem of solving for x in y = Ax is underdetermined, and has many possible solutions (if there are any). In several fields it is of interest to find the sparsest solution – the one with fewest nonzeros – but in general this involves combinatorial optimization. Let ai denote the i-th column of A, 1 = i = n. Associate to A the quotient polytope P formed by taking the convex hull of the 2n points (±ai) in R d. P is centrosymmetric and is called (centrally) k-neighborly if every subset of k + 1 elements (±ilail)k+1 l=1 are the vertices of a face of P. We show that if P is k-neighborly, then if a system y = Ax has a solution with at most k nonzeros, that solution is also the unique solution of the convex optimization problem min ?x?1 subject to y = Ax; the converse holds as well. This complete equivalence between the study of sparse solution by l 1 minimization and neighborliness of convex polytopes immediately gives new results in each field. On the one
66|For most large underdetermined systems of equations, the minimal l1-norm near-solution approximates the sparsest near-solution|We consider inexact linear equations y ˜ Fa where y is a given vector in R n, F is a given n by m matrix, and we wish to find an a0,? which is sparse and gives an approximate solution, obeying ?y - Fa0,??2 = ?. In general this requires combinatorial optimization and so is considered intractable. On the other hand, the l 1 minimization problem min ?a?1 subject to ?y - Fa?2 = ?, is convex, and is considered tractable. We show that for most F the solution ˆa1,? = ˆa1,?(y, F) of this problem is quite generally a good approximation for ˆa0,?. We suppose that the columns of F are normalized to unit l 2 norm 1 and we place uniform measure on such F. We study the underdetermined case where m ~ An, A&gt; 1 and prove the existence of ? = ?(A) and C&gt; 0 so that for large n, and for all F’s except a negligible fraction, the following approximate sparse solution property of F holds: For every y having an approximation ?y - Fa0?2 = ? by a coefficient vector a0 ? R m with fewer than ? · n nonzeros, we have ?ˆa1,? - a0?2 = C · ?. This has two implications. First: for most F, whenever the combinatorial optimization result a0,? would be very sparse, ˆa1,? is a good approximation to a0,?. Second: suppose we are given noisy data obeying y = Fa0 + z where the unknown a0 is known to be sparse and the noise ?z?2 = ?. For most F, noise-tolerant l 1-minimization will stably recover a0 from y in the presence of noise z. We study also the barely-determined case m = n and reach parallel conclusions by slightly different arguments. The techniques include the use of almost-spherical sections in Banach space theory and concentration of measure for eigenvalues of random matrices.
67|Sparse reconstruction by convex relaxation: Fourier and Gaussian measurements|Abstract — This paper proves best known guarantees for exact reconstruction of a sparse signal f from few non-adaptive universal linear measurements. We consider Fourier measurements (random sample of frequencies of f) and random Gaussian measurements. The method for reconstruction that has recently gained momentum in the Sparse Approximation Theory is to relax this highly non-convex problem to a convex problem, and then solve it as a linear program. What are best guarantees for the reconstruction problem to be equivalent to its convex relaxation is an open question. Recent work shows that the number of measurements k(r, n) needed to exactly reconstruct any r-sparse signal f of length n from its linear measurements with convex relaxation is usually O(r polylog(n)). However, known guarantees involve huge constants, in spite of very good performance of the algorithms in practice. In attempt to reconcile theory with practice, we prove the first guarantees for universal measurements (i.e. which work for all sparse functions) with reasonable constants. For Gaussian measurements, k(r, n) ? 11.7 r ˆ 1.5 + log(n/r)  ˜ , which is optimal up to constants. For Fourier measurements, we prove the best known bound k(r, n) = O(r log(n)  · log 2 (r) log(r log n)), which is optimal within the log log n and log 3 r factors. Our arguments are based on the
68|A new compressive imaging camera architecture using optical-domain compression|Compressive Sensing is an emerging field based on the revelation that a small number of linear projections of a compressible signal contain enough information for reconstruction and processing. It has many promising implications and enables the design of new kinds of Compressive Imaging systems and cameras. In this paper, we develop a new camera architecture that employs a digital micromirror array to perform optical calculations of linear projections of an image onto pseudorandom binary patterns. Its hallmarks include the ability to obtain an image with a single detection element while sampling the image fewer times than the number of pixels. Other attractive properties include its universality, robustness, scalability, progressivity, and computational asymmetry. The most intriguing feature of the system is that, since it relies on a single photon detector, it can be adapted to image at wavelengths that are currently impossible with conventional CCD and CMOS imagers.
69|Error Correction via Linear Programming|Suppose we wish to transmit a vector f ? Rn reliably. A frequently discussed approach consists in encoding f with an m by n coding matrix A. Assume now that a fraction of the entries of Af are corrupted in a completely arbitrary fashion. We do not know which entries are affected nor do we know how they are affected. Is it possible to recover f exactly from the corrupted m-dimensional vector y? This paper proves that under suitable conditions on the coding matrix A, the input f is the unique solution to the l1-minimization problem (?x?l1: = i |xi|) min ?y - Ag?l1 g?Rn provided that the fraction of corrupted entries is not too large, i.e. does not exceed some strictly positive constant ? * (numerical values for ? * are given). In other words, f can be recovered exactly by solving a simple convex optimization problem; in fact, a linear program. We report on numerical experiments suggesting that l1-minimization is amazingly effective; f is recovered exactly even in situations where a very significant fraction of the output is corrupted.
70|Smallest singular value of random matrices and geometry of random polytopes|geometry of random polytopes
71|Compressive imaging for video representation and coding|Abstract. Compressive Sensing is an emerging field based on the revelation that a small group of nonadaptive linear projections of a compressible signal contains enough information for reconstruction and processing. In this paper, we propose algorithms and hardware to support a new theory of Compressive Imaging. Our approach is based on a new digital image/video camera that directly acquires random projections of the light field without first collecting the pixels/voxels. Our camera architecture employs a digital micromirror array to perform optical calculations of linear projections of an image onto pseudorandom binary patterns. Its hallmarks include the ability to obtain an image with a single detection element while measuring the image/video fewer times than the number of pixels/voxels; this can significantly reduce the computation required for video acquisition/encoding. Since our system relies on a single photon detector, it can also be adapted to image at wavelengths that are currently impossible with conventional CCD and CMOS imagers. We are currently testing a prototype design for the camera and include experimental results. Index Terms: camera, compressive sensing, imaging, incoherent projections, linear programming, random
72|On sparse representations in pairs of bases|Abstract—In previous work, Elad and Bruckstein (EB) have provided a sufficient condition for replacing an optimization by linear program-ming minimization when searching for the unique sparse representation. We establish here that the EB condition is both sufficient and necessary. Index Terms—Dictionary, sparse representation, tight frame. I.
73|Random sampling of sparse trigonometric polynomials|We investigate the problem of reconstructing sparse multivariate trigonometric polynomials from few randomly taken samples by Basis Pursuit and greedy algorithms such as Orthogonal Matching Pursuit (OMP) and Thresholding. While recovery by Basis Pursuit has recently been studied by several authors, we provide theoretical results on the success probability of reconstruction via Thresholding and OMP for both a continuous and a discrete probability model for the sampling points. We present numerical experiments, which indicate that usually Basis Pursuit is significantly slower than greedy algorithms, while the recovery rates are very similar.
74|Improved Time Bounds for Near-Optimal Sparse Fourier Representations|We study the problem of finding a Fourier representation R of B terms for a given discrete signal A of length N . The Fast Fourier Transform (FFT) can find the optimal N-term representation in O(N log N)  time, but our goal is to get sublinear algorithms for B ! N , typically, B  N . Suppose kAk2  M kRoptk 2 , where Ropt is the optimal output. The previously best known algorithms output R such that kA \Gamma Rk    poly(B; log(1=ffi); log N; log M; 1=ffl): Even though this is sublinear in the input size, the dominating term is the polynomial factor in B which is B   . In our experience, this is a limitation in practice. Our main result is a significantly improved algorithm for this problem. Our algorithms output R  such that kA \Gamma Rk    B \Delta poly(log(1=ffi); log N; log M; 1=ffl): We also obtain improvements for higher dimensional Fourier transforms. We need two crucial ideas to achieve this bound: bulk sampling and estimation for multipoint polynomial evaluation using an unevenly-spaced Fourier tranform, and construction and use of arithmeticprogression independent random variables. Our improved algorithms are likely to find many applications. 1 
75|Randomized Interpolation and Approximation of Sparse Polynomials|We present a randomized algorithm that interpolates a sparse polynomial in polynomial time in the bit complexity model. The algorithm can be also applied to approximate polynomials that can be approximated by sparse polynomials (the approximation is in the L_2 norm).
76|Optimal computation|Abstract. A large portion of computation is concerned with approximating a function u. Typically, there are many ways to proceed with such an approximation leading to a variety of algorithms. We address the question of how we should evaluate such algorithms and compare them. In particular, when can we say that a particular algorithm is optimal or near optimal? We shall base our analysis on the approximation error that is achieved with a given (computational or information) budget n. We shall see that the formulation of optimal algorithms depends to a large extent on the context of the problem. For example, numerically approximating the solution to a PDE is different from approximating a signal or image (for the purposes of compression).
77|Greed is Good: Algorithmic Results for Sparse Approximation|This article presents new results on using a greedy algorithm, orthogonal matching pursuit (OMP), to solve the sparse approximation problem over redundant dictionaries. It provides a sufficient condition under which both OMP and Donoho’s basis pursuit (BP) paradigm can recover the optimal representation of an exactly sparse signal. It leverages this theory to show that both OMP and BP succeed for every sparse input signal from a wide class of dictionaries. These quasi-incoherent dictionaries offer a natural generalization of incoherent dictionaries, and the cumulative coherence function is introduced to quantify the level of incoherence. This analysis unifies all the recent results on BP and extends them to OMP. Furthermore, the paper develops a sufficient condition under which OMP can identify atoms from an optimal approximation of a nonsparse signal. From there, it argues that OMP is an approximation algorithm for the sparse problem over a quasi-incoherent dictionary. That is, for every input signal, OMP calculates a sparse approximant whose error is only a small factor worse than the minimal error that can be attained with the same number of terms. 
78|Translation-invariant de-noising|De-Noising with the traditional (orthogonal, maximally-decimated) wavelet transform sometimes exhibits visual artifacts; we attribute some of these – for example, Gibbs phenomena in the neighborhood of discontinuities – to the lack of translation invariance of the wavelet basis. One method to suppress such artifacts, termed “cycle spinning ” by Coifman, is to “average out ” the translation dependence. For a range of shifts, one shifts the data (right or left as the case may be), De-Noises the shifted data, and then unshifts the de-noised data. Doing this for each of a range of shifts, and averaging the several results so obtained, produces a reconstruction subject to far weaker Gibbs phenomena than thresholding based De-Noising using the traditional orthogonal wavelet transform. Cycle-Spinning over the range of all circulant shifts can be accomplished in order nlog 2(n) time; it is equivalent to de-noising using the undecimated or stationary wavelet transform. Cycle-spinning exhibits benefits outside of wavelet de-noising, for example in cosine packet denoising, where it helps suppress ‘clicks’. It also has a counterpart in frequency domain de-noising, where the goal of translation-invariance is replaced by modulation invariance, and the central shift-De-Noise-unshift operation is replaced by modulate-De-Noise-demodulate. We illustrate these concepts with extensive computational examples; all figures presented here are reproducible using the WaveLab software package. 1
79|WaveLab and Reproducible Research|WaveLab is a library of  Matlab  routines for wavelet analysis, wavelet-packet  analysis, cosine-packet analysis and matching pursuit. The library is available free of  charge over the Internet. Versions are provided for Macintosh, UNIX and Windows  machines. WaveLab makes
80|Near-optimal sparse Fourier representations via sampling|We give an algorithm for nding a Fourier representation R ofBterms for a given discrete signal A of lengthN, such thatkA,Rk 2 2 is within the factor (1 +) of best possible kA,Roptk 2 2. Our algorithm can access A by reading its values on a sample setT [0;N), chosen randomly from a (non-product) distribution of our choice, independent of A. That is, we sample non-adaptively. The total time cost of the algorithm is polynomial inB log(N) log(M) = (where M is the ratio of largest to smallest numerical quantity encountered), which implies a similar bound for the number of samples. 1.
81|Just relax: Convex programming methods for subset selection and sparse approximation|Subset selection and sparse approximation problems request a good approximation of an input signal using a linear combination of elementary signals, yet they stipulate that the approximation may only involve a few of the elementary signals. This class of problems arises throughout electrical engineering, applied mathematics and statistics, but small theoretical progress has been made over the last fifty years. Subset selection and sparse approximation both admit natural convex relaxations, but the literature contains few results on the behavior of these relaxations for general input signals. This report demonstrates that the solution of the convex program frequently coincides with the solution of the original approximation problem. The proofs depend essentially on geometric properties of the ensemble of elementary signals. The results are powerful because sparse approximation problems are combinatorial, while convex programs can be solved in polynomial time with standard software. Comparable new results for a greedy algorithm, Orthogonal Matching Pursuit, are also stated. This report should have a major practical impact because the theory applies immediately to many real-world signal processing problems.  
83|Implementing the PPM Data Compression Scheme|The “Prediction by Partial Matching” (PPM) data com-pression algorithm developed by Cleary and Witten is capable of very high compression rates, encoding English text in as little as 2.2 bits/character. Here it is shown that the estimates made by Cleary and Witten of the resources required to implement the scheme can be revised to allow for a tractable and useful implementation. In particular, a variant is described that encodes and decodes at over 4 kbytes/s on a small workstation, and operates within a few hundred kilobytes of data space, but still obtains compression of about 2.4 bits/character on English text. 
84|Data Compression Using Dynamic Markov Modelling|A method to dynamically construct Markov models that describe the characteristics of binary messages is developed. Such models can be used to predict future message characters and can therefore be used as a basis for data compression. To this end, the Markov modelling technique is combined with Guazzo coding to produce a powerful method of data compression. The method has the advantage of being adaptive: messages may be encoded or decoded with just a single pass through the data. Experimental results reported here indicate that the Markov modelling approach generally achieves much better data compression than that observed with competing methods on typical computer data. Categories and Subject Descriptors: E.4 [Coding and Information Theory]: data compaction and compression; C.2.0 [Computer-Communication Networks]: data communications   General Terms: Experimentation, Algorithms Additional Key Words and Phrases: Data compression, text compression, adaptive coding, Guazzo coding January...
85|Data Compression and Database Performance|Data compression is widely used in data management to save storage space and network bandwidth. In this report, we outline the performance improvements that can be achieved by exploiting data compression in query processing. The novel idea is to leave data in compressed state as long as possible, and to only uncompress data when absolutely necessary. We will show that many query processing algorithms can manipulate compressed data just as well as decompressed data, and that processing compressed data can speed query processing by a factor much larger than the compression factor.
86|A Performance Evaluation of Four Parallel Join Algorithms in a Shared-Nothing Multiprocessor Environment|The join operator has been a cornerstone of relational database systems since their inception. As such, much time and effort has gone into making joins efficient. With the obvious trend towards multiprocessors, attention has focused on efficiently parallelizing the join operation. In this paper we analyze and compare four parallel join algorithms. Grace and Hybrid hash represent the class of hash-based join methods, Simple hash represents a looping algorithm with hashing, and our last algorithm is the more traditional sort-merge. The Gamma database machine serves as the host for the performance comparison. Gamma’s shared-nothing architecture with commercially available components is becoming increasingly common, both in research and in industry. 1.
87|Relating data compression and learnability|We explore the learnability of two-valued functions from samples using the paradigm of Data Compression. A first algorithm (compression) choses a small subset of the sample which is called the kernel. A second algorithm predicts future values of the function from the kernel, i.e. the algorithm acts as an hypothesis for the function to be learned. The second algorithm must be able to reconstruct the correct function values when given a point of the original sample. We demonstrate that the existence of a suitable data compression scheme is sufficient to ensure learnability. We express the probability that the hypothesis predicts the function correctly on a random sample point as a function of the sample and kernel sizes. No assumptions are made on the probability distributions according to which the sample points are generated. This approach provides an alternative to that of [BEHW86], which uses the Vapnik-Chervonenkis dimension to classify learnable geometric concepts. Our bounds are derived directly from the kernel size of the algorithms rather than from the Vapnik-Chervonenkis dimension of the hypothesis class. The proofs are simpler and the introduced compression scheme provides a rigorous model for studying data compression in connection with machine learning. 1
88|A Theory of the Learnable|  Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems.
89|The JPEG still picture compression standard|This paper is a revised version of an article by the same title and author which appeared in the April 1991 issue of Communications of the ACM. For the past few years, a joint ISO/CCITT committee known as JPEG (Joint Photographic Experts Group) has been working to establish the first international compression standard for continuous-tone still images, both grayscale and color. JPEG’s proposed standard aims to be generic, to support a wide variety of applications for continuous-tone images. To meet the differing needs of many applications, the JPEG standard includes two basic compression methods, each with various modes of operation. A DCT-based method is specified for “lossy’ ’ compression, and a predictive method for “lossless’ ’ compression. JPEG features a simple lossy technique known as the Baseline method, a subset of the other DCT-based modes of operation. The Baseline method has been by far the most widely implemented JPEG method to date, and is sufficient in its own right for a large number of applications. This article provides an overview of the JPEG standard, and focuses in detail on the Baseline method. 1
90|Data Compression Support in Databases|Computers running database management applications often manage large amounts of data. Typically, the price of the I/O sub-system is a considerable portion of the com-puting hardware. Fierce price competition demands every possible savings. Lossless data compression methods, when appropri-ately integrated with the dbms, yield sig-niflcant savings. Roughly speaking, a slight increase in cpu cycles is more than offset by savings in I/O subsystem. Various de-sign issues arise in the use of data compres-sion in the dbms- from the choice of algo-rithm, statistics collection, hardware ver-sus software based compression, location of the compression function in the overall computer system architecture, unit of com-pression, update in place, and the applica-tion of log ’ to compressed data. These are methodic &amp; y examined and trade-offs dis-cussed in the context of choices made for IBM’s DB2 dbms product. 1
91|Data Compression Techniques |Abstract: Data compression has important application in the field of file storage and distributed systems. It helps in reducing redundancy in stored or communicated data. This paper studies various compression techniques and analyzes the approaches used in data compression. Furthermore, information theory concepts that relates to aims and evaluation of data compression methods are briefly discussed. A framework for the evaluation and comparison of various compression algorithms is constructed and applied to the algorithms presented here. This paper reports the theoretical and practical nature of compression algorithms. Moreover, it also discusses the future possibilities of research work in the field of data compression.
92|Potential benefits of delta encoding and data compression for HTTP|Caching in the World Wide Web currently follows a naive model, which assumes that resources are referenced many times between changes. The model also provides no way to update a cache entry if a resource does change, except by transferring the resource&#039;s entire new value. Several previous papers have proposed updating cache entries by transferring only the differences, or &#034;delta,&#034; between the cached entry and the current value. In this paper,
93|Web Server Workload Characterization: The Search for Invariants (Extended Version)  (1996) |The phenomenal growth in popularity of the World Wide Web (WWW, or the Web) has made WWW traffic the largest contributor to packet and byte traffic on the NSFNET backbone. This growth has triggered recent research aimed at reducing the volume of network traffic produced by Web clients and servers, by using caching, and reducing the latency for WWW users, by using improved protocols for Web interaction. Fundamental to the goal of improving WWW performance is an understanding of WWW workloads. This paper presents a workload characterization study for Internet Web servers. Six different data sets are used in this study: three from academic environments, two from scientific research organizations, and one from a commercial Internet provider. These data sets represent three different orders of magnitude in server activity, and two different orders of magnitude in time duration, ranging from one week of activity to one year of activity. Throughout the study, emphasis is placed on finding wor...
94|RCS --  a system for version control|An important problem in program development and maintenance is version control, i.e., the task of keeping a software system consisting of many versions and configurations well organized. The Revision Control System (RCS) is a software tool that assists with that task. RCS manages revisions of text documents, in particular source programs, documentation, and test data. It automates the storing, retrieval, logging and identification of revisions, and it provides selection mechanisms for composing configurations. This paper introduces basic version control concepts and discusses the practice of version control using RCS. For conserving space, RCS stores deltas, i.e., differences between successive revisions. Several delta storage methods are discussed. Usage statistics show that RCS’s delta storage method is space and time efficient. The paper concludes with a detailed survey of version control tools.
95|Limits of instruction-level parallelism|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There two other research laboratories located in Palo Alto, the Network Systems
96|Eliminating receive livelock in an interrupt-driven kernel|Most operating systems use interface interrupts to schedule network tasks. Interrupt-driven systems can provide low overhead and good latency at low of-fered load, but degrade significantly at higher arrival rates unless care is taken to prevent several pathologies. These are various forms of receive livelock, in which the system spends all its time processing interrupts, to the exclusion of other neces-sary tasks. Under extreme conditions, no packets are delivered to the user application or the output of the system. To avoid livelock and related problems, an operat-ing system must schedule network interrupt handling as carefully as it schedules process execution. We modified an interrupt-driven networking implemen-tation to do so; this eliminates receive livelock without degrading other aspects of system performance. We present measurements demonstrating the success of our approach. 1.
97|An enhanced access and cycle time model for on-chip caches|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
98|Compressing TCP/IP headers for low-speed serial links|This RFC is a proposed elective protocol for the Internet community and requests discussion and suggestions for improvement. It describes a method for compressing the headers of TCP/IP datagrams to improve performance over low speed serial links. The motivation, implementation and performance of the method are described. C code for a sample implementation is given for reference. Distribution of this memo is unlimited. NOTE: Both ASCII and Postscript versions of this document are available. The ASCII version, obviously, lacks all the figures and all the information encoded in typographic variation (italics, boldface, etc.). Since this information was, in the author’s opinion, an essential part of the document, the ASCII version is at best incomplete and at worst misleading. Anyone who plans to work with this protocol is strongly encouraged obtain the Postscript version of this RFC.
99|Rate of Change and other Metrics: a Live Study of the World Wide Web|Caching in the World Wide Web is based on two critical assumptions: that a significant fraction of requests reaccess resources that have already been retrieved; and that those resources do not change between accesses.  We tested the validity of these assumptions, and their dependence on characteristics of Web resources, including access rate, age at time of reference, content type, resource size, and Internet top-level domain. We also measured the rate at which resources change, and the prevalence of duplicate copies in the Web.  We quantified the potential benefit of a shared proxycaching server in a large environment by using traces that were collected at the Internet connection points for two large corporations, representing significant numbers of references. Only 22% of the resources referenced in the traces we analyzed were accessed more than once, but about half of the references were to those multiplyreferenced resources. Of this half, 13% were to a resource that had been modifi...
100|Available Instruction-Level Parallelism for Superscalar and Superpipelined Machines|Superscalar machines can issue several instructions per cycle. Superpipelined machines can issue only one instruction per cycle, but they have cycle times shorter than the latency of any functional unit. In this paper these two techniques are shown to be roughly equivalent ways of exploiting instruction-level parallelism. A parameterizable code reorganization and simulation system was developed and used to measure instruction-level parallelism for a series of benchmarks. Results of these simulations in the presence of various compiler optimizations are presented. The average degree of superpipelining metric is introduced. Our simulations suggest that this metric is already high for many machines. These machines already exploit all of the instruction-level parallelism available in many non-numeric applications, even without parallel instruction issue or higher degrees of pipelining.  
101|Cache Write Policies and Performance|This paper investigates issues involving writes and caches. First, tradeoffs between write-through and write-back caching when writes hit in a cache are considered. A mixture of these two alternatives, called write caching is proposed. Write caching places a small fully-associative cache behind a write-through cache. A write cache can eliminate almost as much write traffic as a write-back cache. Second, tradeoffs on writes that miss in the cache are investigated. In particular, whether the missed cache block is fetched on a write miss, whether the missed cache block is allocated in the cache, and whether the cache line accessed is invalidated are considered. Depending on the combination of these polices chosen, the entire cache miss rate can vary by a factor of two on some applications. Furthermore, the combination of no-fetch-on-write and write-allocate can provide better performance than cache line allocation instructions. Finally, the traffic at the back side of write-through and wr...
102|Compacting Garbage Collection with Ambiguous Roots|This paper introduces a copying garbage collection algorithm which is able to compact most of the accessible storage in the heap without having an explicitly defined set of pointers that contain the roots of all accessible storage. Using &#034;hints &#034; found in the processor’s registers and stack, the algorithm is able to divide heap allocated objects into two groups: those that might be referenced by a pointer in the stack or registers, and those that are not. The objects which might be referenced are left in place, and the other objects are copied into a more compact representation. A Lisp compiler and runtime system which uses such a collector need not have complete control of the processor in order to force a certain discipline on the stack and registers. A Scheme implementation has been done for the Digital WRL Titan processor which uses a garbage collector based on this &#034;mostly copying &#034; algorithm. Like other languages for the Titan, it uses the Mahler intermediate language as its target. This simplifies the compiler and allows it to take advantage of the significant machine dependent optimizations provided by Mahler. The common intermediate language also simplifies call-outs from Scheme programs to functions written in other languages and call-backs from functions in other languages. Measurements of the Scheme implementation show that the algorithm is efficient, as little unneeded storage is retained and only a very small fraction of the heap is left in place. Simple pointer manipulation protocols also mean that compiler support is not needed in order to correctly handle pointers. Thus it is reasonable to provide garbage collected storage in languages such as C. A collector written in C which uses this algorithm is included in the Appendix. i A further problem is the occasional difficulty of determining exactly what Lists are not garbage at any given stage; if the programmer has been using any nonstandard techniques or keeping any pointer values in unusual places, chances are good that the garbage collector will go awry. Knuth, Volume I 1.
103|Tradeoffs in Two-Level On-Chip Caching|The performance of two-level on-chip caching is investigated for a range of technology and architecture assumptions. The area and access time of each level of cache is modeled in detail. The results indicate that for most workloads, twolevel cache configurations (with a set-associative second level) perform marginally better than single-level cache configurations that require the same chip area once the first-level cache sizes are 64KB or larger. Two-level configurations become even more important in systems with no off-chip cache and in systems in which the memory cells in the first-level caches are multiported and hence larger than those in the second-level cache. Finally, a new replacement policy called  two-level exclusive caching is introduced. Two-level exclusive caching improves the performance of two-level caching organizations by increasing the effective associativity and capacity. d i g i t a l  Western Research Laboratory 250 University Avenue Palo Alto, California 94301 USA...
105|Systems for Late Code Modification|Modifying code after the compiler has generated it can be useful for both optimization and instrumentation. This paper compares the code modification systems of Mahler and pixie, and describes two new systems we have built that are hybrids of the two. This paper covers material presented at the CODE &#039;91 International Workshop on Code Generation, Schloss Dagstuhl, Germany, May 20-24, 1991. i  1. Introduction  Late code modification is the process of modifying the output of a compiler after the compiler has generated it. The reasons one might want to do this fall into two categories, optimization and instrumentation. Some forms of optimization must be performed on assembly-level or machinelevel code. The oldest is peephole optimization [11], which acts to tidy up code that a compiler has generated; it has since been generalized to include transformations on more machine-independent code [2,3]. Reordering of code to avoid pipeline stalls [4,7,18] is most often done after the code is gene...
106|Complexity/Performance Tradeoffs with Non-Blocking Loads|Non-blocking loads are a very effective technique for tolerating the cache-miss latency on data cache references. We describe several methods for implementing non-blocking loads. A range of resulting hardware complexity/performance tradeoffs are investigated using an object-code translation and instrumentation system. We have investigated the SPEC92 benchmarks and have found that for the integer benchmarks, a simple hit-under-miss implementation achieves almost all of the available performance improvement for relatively little cost. However, for most of the numeric benchmarks, more expensive implementations are worthwhile. The results also point out the importance of using a compiler capable of scheduling load instructions for cache misses rather than cache hits in nonblocking systems.  This Research Report is a preprint of a paper to appear at the 21st Annual International Symposium on Computer Architecture.  d i g i t a l  Western Research Laboratory 250 University Avenue Palo Alto, ...
107|Efficient Procedure Mapping using Cache Line Coloring|As the gap between memory and processor performance continues to widen, it becomes increasingly important to exploit cache memory effectively. Both hardware and software approaches can be explored to optimize cache performance. Hardware designers focus on cache organization issues, including replacement policy, associativity, line size and the resulting cache access time. Software writers use various optimization techniques, including software prefetching, data scheduling and code reordering. Our focus is on improving memory usage through code reordering compiler techniques. In this
108|Optimistic Deltas for WWW Latency Reduction|When a machine is connected to the Internet via a slow network, such as a 28.8 Kbps modem, the cumulative latency to communicate over the Internet to World Wide Web servers and then transfer documents over the slow network can be significant. We have built a system that optimistically transfers data that may be out of date, then sends either a subsequent confirmation that the data is current or a delta to change the older version to the current one. In addition, if both sides of the slow link already store the same older version, just the delta need be transferred to update it. Our mechanism is optimistic because it assumes that much of the time there will be sufficient idle time to transfer most or all of the older version before the newer version is available, and because it assumes that the changes between the two versions will be small relative to the actual document. Timings of retrievals of random URLs in the Internet support the former assumption, while experiments using a versi...
109|Long address traces from RISC machines: Generation and analysis|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
110|Memory-system Design Considerations for Dynamically-scheduled Processors|We identify performance trends and design relationships between the following components of the data memory hierarchy in a dynamically-scheduled processor: the register file, the lockup-free data cache, the stream buffers, and the interface between these components and the lower levels of the memory hierarchy. Similar performance was obtained from all systems having support for fewer than four in-flight misses, irrespective of the register-file size, the issue width of the processor, and the memory bandwidth. While providing support for more than four inflight misses did increase system performance, the improvement was less than that obtained by increasing the number of registers. The addition of stream buffers to the investigated systems led to a significant performance increase, with the larger increases for systems having less in-flight-miss support, greater memory bandwidth, or more instruction issue capability. The performance of these systems was not significantly affected by the...
111|SCHEME-&gt;C: a Portable Scheme-to-C Compiler|One way to make compilers portable is to have them compile to an intermediate language which is implemented on multiple architectures. By using C as the intermediate language and compiling the LISP dialect Scheme to it, it might be possible to achieve the following benefits. First, since C is the lingua franca of workstations the resulting system should be very portable. Second, it should allow Scheme programs to interact with those written in other languages. Finally, it should simplify the compiler as it need not repeat the optimization capability available in the C compiler. However, there might be some unacceptable costs associated with this. First, there might not be a clean translation from Scheme to C, so that the implementation is not quite Scheme. Second, the two-stage translation might result in inefficient code. Finally, the generated code might be so stylized that it is neither portable nor compatible with other programming languages. To investigate these issues, such a compiler and run-time system were constructed at Digital Equipment Corporation&#039;s Western Reaearch Laboratory. Experience with the system shows, that there is a translation of Scheme to C, that has good performance, and is portable.
112|A Simulation Based Study of TLB Performance|caches. The instruction TLB should be made  smaller than the data TLB, as instruction referThis  paper presents the results of a simulation-based study ence streams exhibit greater locality than those  of various translation lookaside buffer (TLB) architectures, for data. The appropriate size tradeoff is difin  the context of a modern VLSI RISC processor. The ficult to determine and once made is fixed. If  simulators used address traces, generated by instrumented the instruction TLB is too small, performance  versions of the SPECmarks and several other programs run- will suffer. If it is too large, the space available  for the data TLB is compromised and again per- ning on a DECstation 5000. The performance of two-level formance suffers. TLBs and fully-associative TLBs were investigated. The amount of memory mapped was found to be the dominant . Two-level TLB architectures. A small instrucfactor  in TLB performance. Small first-level FIFO instruc- tion TLB (i.e., micro-TLB), can be ref...
113|Experience with a Software-Defined Machine Architecture|We built a system in which the compiler back end and the linker work  together to present an abstract machine at a considerably higher level than  the actual machine. The intermediate language translated by the back end is  the target language of all high-level compilers and is also the only assembly  language generally available. This lets us do intermodule register allocation,  which would be harder if some of the code in the program had come from a  traditional assembler, out of sight of the optimizer. We do intermodule  register allocation and pipeline instruction scheduling at link time, using information  gathered by the compiler back end. The mechanism for analyzing  and modifying the program at link time was also useful in a wide array of  instrumentation tools.  i  1. Introduction  When our lab built its experimental RISC workstation, the Titan, we defined a high-level assembly language as the official interface to the machine. This high-level assembly language, called Mahler,...
114|The Mahler experience: using an intermediate language as the machine description|Division of a compiler into a front end and a back end that communicate via an intermediate language is a well-known technique. We go farther and use the intermediate language as the official description of a family of machines with simple instruction sets and addressing capabilities, hiding some of the inconvenient details of the real machine from the users and the front end compilers. Then we can implement each machine in this fam-ily with whatever technology is appropriate, without having to make the details match those of other machines in the family. Each machine can therefore be faster than it would be if we required the machines to be object-code compatible, but the front end compilers need not change to accommodate that flexibility. To do this credibly, we have had to hide not only the existence of the details but also the performance conse-quences of hiding them. The back end that compiles and links the intermediate language tries to produce code that does not suffer a performance penalty because of the details that were hidden from the front end compiler. To accomplish this, we have used a number of link-time optimizations, including instruction scheduling and interprocedural register allocation, to hide the existence of such idiosyncracies as delayed branches and non-infinite register sets. For the most part we have been successful. 1.
115|How Useful Are Non-blocking Loads, Stream Buffers, and Speculative Execution in Multiple Issue Processors?|We investigate the relative performance impact of non-blocking loads, stream buffers, and speculative execution both used individually and in conjunction with each other. We have simulated the SPEC92 benchmarks on a statically scheduled quad-issue processor model, running code from the Multiflow compiler. Non-blocking loads and stream buffers both provide a significant performance advantage, and their combination performs significantly better than either alone. For example, with a 64-byte, 2-way set associative cache with 32 cycle fetch latency, non-blocking loads reduce the run-time by 21% while stream-buffers reduce it by 26%, and the combined use of the two yields a 47% reduction. The addition of speculative execution further improves the performance of the systems that we have simulated, with or without non-blocking loads and stream buffers, by an additional 20% to 40%. We expect that the use of all three of these techniques will be important in future generations of microprocessor...
116|Fine-Grain Software Distributed Shared Memory on SMP Clusters|Commercial SMP nodes are an attractive building block for software distributed  shared memory systems. The advantages of using SMP nodes include  fast communication among processors within the same SMP node and  potential gains from clustering where remote data fetched by one processor  is used by other processors on the same node. The widespread availability of  SMP servers with small numbers of processors has led several researchers to  consider their use as building blocks for Shared Virtual Memory (SVM) systems.  These systems exploit the SMP cache-coherence hardware to support  fine-grain communication within a node, and use software to support communication  across nodes at a coarser page-size granularity. Our goal is to  explore the use of SMP nodes in the context of the Shasta system. A unique  aspect of Shasta compared to most other software distributed shared  memory systems is that shared data can be kept coherent at a fine  granularity. Shasta implements this coherence by i...
117|Software Methods for System Address Tracing: Implementation and Validation|Systems for recording address traces of operating system activity have frequently relied on special-purpose hardware and microcode modifications for data collection [1, 2, 11, 10, 32, 30]. In the last decade, changes in computer systems design have made the implementation of such hardware and microcode-based tracing systems impractical. This paper documents the evolution of a group of software methods to collect system traces. The tools require no special-purpose hardware and no hardware modifications. We have applied these tools to three substantially different operating systems and two processor architectures. This paper describes the instrumentation techniques, the means used to assure the quality of the collected data, and our evaluation of correctness and accuracy of traces. Our experience shows that software methods can yield trace of very good quality, and can be used to measure complex software systems.  
118|An Empirical Study of Delta Algorithms|. Delta algorithms compress data by encoding one file in terms of another. This type of compression is useful in a number of situations: storing multiple versions of data, distributing updates, storing backups, transmitting video sequences, and others. This paper studies the performance parameters of several delta algorithms, using a benchmark of over 1300 pairs of files taken from two successive releases of GNU software. Results indicate that modern delta compression algorithms based on Ziv-Lempel techniques significantly outperform diff, a popular but older delta compressor, in terms of compression ratio. The modern compressors also correlate better with the actual difference between files; one of them is even faster than diff in both compression and decompression speed. 1 Introduction Delta algorithms, i.e., algorithms that compute differences between two files or strings, have a number of uses when multiple versions of data objects must be stored, transmitted, or proce...
119|Interleaved Fin Thermal Connectors for Multichip Modules|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
120|Fluoroelastomer Pressure Pad Design for Microelectronic Applications|The elastic properties of gum rubber and fluoroelastomers were studied by a variety of numerical and experimental methods. Results were applied to the design of flat pressure pads for microelectronic applications. The goal was to develop an understanding sufficient that designers could quickly develop acceptable fluoroelastomer pressure pads without further detailed studies. The effort centered on optimizing the performance of a 14 mm square by 0.8 mm thick pad under a fixed normal force. The primary optimization criterion was minimization of the maximum normal contact stresses applied by the pad to a rigid surface. Judicious perforation of flat pads greatly reduced adverse contact stress gradients. The preferred design used four 1.2 mm holes symmetrically arrayed in a 4 mm square grid centered on the pad. Compared to an unperforated pad, this arrangement yielded a 28% reduction in maximum contact stresses. i  ii  Fluoroelastomer Pressure Pad Design for Microelectronic Applications  ...
121|Cache Coherence in Distributed Systems|v Abstract  Caching has long been recognized as a powerful performance enhancement technique in many areas of computer design. Most modern computer systems include a hardware cache between the processor and main memory, and many operating systems include a software cache between the file system routines and the disk hardware.  In a distributed file system, where the file systems of several client machines are separated from the server backing store by a communications network, it is desirable to have a cache of recently used file blocks at the client, to avoid some of the communications overhead. In this configuration, special care must be taken to maintain consistency between the client caches, as some disk blocks may be in use by more than one client. For this reason, most current distributed file systems do not provide a cache at the client machine. Those systems that do place restrictions on the types of file blocks that may be shared, or require extra communication to confirm that...
122|The USENET Cookbook: an experiment in electronic publishing|Much of the research taking place in the field called &#034;electronic publishing&#034; would perhaps be better called &#034;electronic printing&#034; or &#034;electronic typography&#034; or &#034;electronic drawing&#034; or &#034;electronic file cabinets&#034;. The word &#034;publishing&#034; has traditionally meant &#034;to make generally known&#034; or &#034;to disseminate&#034;. In December 1985 I began a venture in true electronic publishing, &#034;true&#034; in the sense that its primary goals were to explore electronic dissemination rather than electronic typesetting or formatting. I wanted to start a periodical that could be distributed electronically, that would use computers for every aspect of its production and distribution process, that would be on a topic of wide enough interest that I could get subscribers in as many countries as possible, and that was on a topic that I would find sufficiently interesting to be able to maintain interest in it long enough to get substantial experience. The obvious topic was cookery. I began a weekly magazine whose contents are...
123|Virtual Memory vs. The File System|This paper examines the behavior of mechanisms for providing variablesize file data caches. It presents the results of running virtual-memoryand file-intensive benchmarks on the Sprite operating system [OCD88]; the benchmarks are designed to simulate real-life applications that represent the worst case for variable-size cache mechanisms. The results indicate that variable-size cache mechanisms work well when virtualmemory - and file-intensive programs are run in sequence; the cache is able to change in size in order to provide overall performance no worse than that provided by a small fixed-size cache. However, when interactive programs are run concurrently with file-intensive programs, variable-size cache mechanisms perform very poorly if file pages and virtual-memory pages are treated equally. In order to guarantee good interactive response, virtual memory pages must be given preference over file pages. i  VIRTUAL MEMORY VS. THE FILE SYSTEM ii  VIRTUAL MEMORY VS. THE FILE SYSTEM Ta...
124|Recursive Layout Generation|We present a recursive method for generating layout for VLSI chips which combines the flexibility of gate array and standard cell layout with the control and density of custom layout. The method allows seamless integration of hand-drawn and synthesized layout, so that hand layout need only be used where the increase in density is justified. Layout is generated automatically with predictable results; small changes in the source result in small changes of the overall layout. The system is versatile enough to build dense VLSI microprocessor chips automatically. d i g i t a l  Western Research Laboratory 250 University Avenue Palo Alto, California 94301 USA  ii  Table of Contents  1. Introduction 1 2. The Annotated Hierarchical Netlist 1 2.1. Cell Generators 2 2.2. Netlist Traversal 5 3. Layout Generation 5 3.1. Hand-Drawn Cells 6 3.2. Leaf Cells 7 3.3. Composite Cells 8 3.4. Routing 9 3.5. Netlist Hierarchy Equals Layout Hierarchy? 10 4. Results 11 References 13 iii  iv  List of Figures  ...
125|Precise Robotic Paste Dot Dispensing|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
126|Drip: A Schematic Drawing Interpreter|This paper presents a design capture system in which schematics are translated into a procedural netlist specification language. The circuit designer draws schematics with a standard structured graphics editor that knows nothing about netlists or schematics. The translator program analyzes the structured graphics output file and translates it into a procedural netlist specification. d i g i t a l  Western Research Laboratory 250 University Avenue Palo Alto, California 94301 USA  ii  Table of Contents  1. Introduction 1 2. Basics 2 2.1. Simple Example 2 2.2. Structured Graphics 3 3. Generating Procedures 4 3.1. Frames and Evaluation 4 3.2. 2D Ordering 5 4. Drawing Interpretation 7 4.1. Icons 8 5. Analysis of Non-Evaluation Objects 9 5.1. Binding Text to Objects 9 5.2. Wires 10 5.3. Wire Subscripting 11 6. Error Reporting 11 7. Experiences 12 Acknowledgements 12 References 12 iii  iv  List of Figures  Figure 1: Code Generated for &#034;CELL: orN&#034; 2 Figure 2: 2D ordering of objects 5 Figur...
127|A New Data Structure for Representing Sorted Lists|  In this paper we explore the use of weak B-trees to represent sorted lists. In weak B-trees each node has at least a and at most b sons where 2a&lt;b. We analyse the worst case cost of sequences of insertions and deletions in weak B-trees. This leads to a new data structure (level-linked weak B-trees) for representing sorted lists when the access pattern exhibits a (time-varying) locality of reference. Our structure is substantially simpler than the one proposed in [7], yet it has many of its properties. Our structure is as simple as the one proposed in [5], but our structure can treat arbitrary sequences of insertions and deletions whilst theirs can only treat non-interacting insertions and deletions. We also show that weak B-trees support concurrent operations in an efficient way. 
129|i r t h , N . , Systematic Programming: An|This final report describes achievements and activities of a 3-year federally supported project by the University of North Carolina at Charlotte to develop and deliver semester-long courses for professionals charged with providing transition services to students with disabilities. The project developed four courses and delivered them using distance education technology at the home campus and three partner sites across the state. Each of the project&#039;s objectives is addressed. Evaluation results support the project&#039;s effectiveness. One-hundred-fifty-six professionals received training through the courses and 45 sets of course materials were disseminated to North Carolina universities and state agencies. This package also contains the course materials for the four courses: (1) &#034;Transition and Life Skills, &#034; which explores individualized transition planning, career education and community-based instruction, adult services, and the roles of business and industry; (2) &#034;Community Based Methods, &#034; which considers curricula and instructional methods associated with transition and supported employment services, supports, and outcomes for individuals with disabilities; (3) &#034;Interagency Collaboration&#034;; and (4) &#034;Internship. &#034; Each course packet includes a sample course syllabus/course assignments, sample lecture notes and PowerPoint slides, and a sample test bank. (DB) Reproductions supplied by EDRS are the best that can be made from the original document.
130|EEG Data Compression Techniques|In this paper EEG and Holter EEG data compression techniques which allow perfect reconstruction of the recorded waveform from the compressed one are presented and discussed.  Data compression permits one to achieve significant reduction in the space required to store signals and in transmission time.  The Huffman coding technique in conjunction with derivative computation reaches high compression ratios (on average 49 % on Holter and 58 % on EEG signals) with low computational complexity. By exploiting this result a simple and fast encoder/decoder scheme capable of real time performance on a PC was implemented.  This simple technique is compared with other predictive transformations, vector quantization, discrete cosine transform and repetition count compression methods.  Finally, it is shown that the adoption of a collapsed Huffman tree for the encoding/decoding operations allows one to choose the maximum codeword length without significantly affecting the compression ratio. Therefore, low cost commercial microcontrollers and storage devices can be effectively used to store long Holter EEGs in a compressed format.  Keywords--- Data Compression, Huffman Code, EEG Signal. I. 
131|Splay Trees for Data Compression|We present applications of splay trees to two topics in data compression. First is a variant of the move-to-front (mtf) data compression (of Bentley,Sleator Tarjan and Wei) algorithm, where we introduce secondary list(s). This seems to capture higher-order correlations. An implementation of this algorithm with Sleator-Tarjan splay trees runs in time (provably) proportional to the entropy of the input sequence. When tested on some telephony data, compression ratio and run time showed significant improvements over original mtf-algorithm, making it competitive or better than popular programs. For stationary ergodic sources, we analyse the compression and output distribution of the original mtf-algorithm, which suggests why the secondary list is appropriate to introduce. We also derive analytical upper bounds on the average codeword length in terms of stochastic parameters of the source. Secondly, we consider the compression (or coding) of source sequences where the codewords are required ...
132|Self-adjusting binary search trees|  The splay tree, a self-adjusting form of binary search tree, is developed and analyzed. The binary search tree is a data structure for representing tables and lists so that accessing, inserting, and deleting items is easy. On an n-node splay tree, all the standard search tree operations have an amortized time bound of O(log n) per operation, where by “amortized time ” is meant the time per operation averaged over a worst-case sequence of operations. Thus splay trees are as efficient as balanced trees when total running time is the measure of interest. In addition, for sufficiently long access sequences, splay trees are as efficient, to within a constant factor, as static optimum search trees. The efftciency of splay trees comes not from an explicit structural constraint, as with balanced trees, but from applying a simple restructuring heuristic, called splaying, whenever the tree is accessed. Extensions of splaying give simplified forms of two other data structures: lexicographic or multidimensional search trees and link/ cut trees.
133|Data Compression Standards|This paper first explores the history of the Digital Video MPEG Standards  as created by the MPEG Committee of the International Standards Organization  (ISO) for data compression. Included are existing standards, i.e.
135|Visual data compression for multimedia applications |In this paper, the compression of visual information in the framework of multimedia applications is discussed. To this end, major approaches to compress still as well as moving pictures are reviewed. The most important objective in any compression algorithm is that of compression efficiency. High-compression coding of still pictures can be split into three categories: waveform, second-generation, and fractal coding techniques. Each coding approach introduces a different artifact at the target bit rates. The primary objective of most ongoing research in this field is to mask these artifacts as much as possible to the human visual system. Video-compression techniques have to deal with data enriched by one more component, namely, the temporal coordinate. Either compression techniques developed for still images can be generalized for three-dimensional signals (space and time) or a hybrid approach can be defined based on motion compensation. The video-compression techniques can then be classified into the following four classes: waveform, object-based, model-based, and fractal coding techniques. The aim of this paper is to provide the reader with a tutorial on major visual data-compression techniques and a list of references for further information on the details of each method. Keywords — H.261, H.263, image compression, interactivity, JPEG, MPEG, video compression.
137|Practical Implementations of Arithmetic Coding|We provide a tutorial on arithmetic coding, showing how it provides nearly optimal data compression and how it can be matched with almost any probabilistic model. We indicate the main disadvantage of arithmetic coding, its slowness, and give the basis of a fast, space-efficient, approximate arithmetic coder with only minimal loss of compression efficiency. Our coder is based on the replacement of arithmetic by table lookups coupled with a new deterministic probability estimation scheme. 
138|Cryptography in Data Compression |This paper describes cryptographic methods for concealing information during data compression processes. These include novel approaches of adding pseudo random shuffles into the processes of dictionary coding (Lampel-Ziv compression), arithmetic coding, and Huffman coding. An immediate application of using these methods to provide multimedia security is proposed. Keywords. cryptography, data compression, multimedia security. 1. 
139|Arithmetic coding revisited|Over the last decade, arithmetic coding has emerged as an important compression tool. It is now the method of choice for adaptive coding on multisymbol alphabets because of its speed, low storage requirements, and effectiveness of compression. This article describes a new implementation of arithmetic coding that incorporates several improvements over a widely used earlier version by Witten, Neal, and Cleary, which has become a de facto standard. These improvements include fewer multiplicative operations, greatly extended range of alphabet sizes and symbol probabilities, and the use of low-precision arithmetic, permitting implementation by fast shift/add operations. We also describe a modular structure that separates the coding, modeling, and probability estimation components of a compression system. To motivate the improved coder, we consider the needs of a word-based text compression program. We report a range of experimental results using this and other models. Complete source code is available.
140|A Chosen Plaintext Attack On An Adaptive Arithmetic Coding Compression Algorithm|The data security provided by an adaptive arithmetic coding compression algorithm is investigated. An analysis is presented of the dependence of the model upon the input text. It is shown that the number of possible states of the model may be greatly reduced by a series of suitably chosen input strings. These form the basis of a successful chosen plaintext attack in which the model is reduced, and a similar model at an interception point adjusted until decryption is possible. The security is found to have been greatly enhanced by the (fortuitous) effect of some minor implementation details. Security may be improved by regular re-initialisation and adjustment of one of the model parameters (the total cumulative frequency). The algorithm provides significant data security, but is vulnerable to a concerted attack.
141|Optimization Methods for Data Compression|Optimization Methods for Data Compression  A dissertation presented to the Faculty of the  Graduate School of Arts and Sciences of Brandeis  University, Waltham, Massachusetts  by Giovanni Motta  Many data compression algorithms use ad--hoc techniques to compress data efficiently. Only in very few cases, can data compressors be proved to achieve optimality on a specific information source, and even in these cases, algorithms often use sub--optimal procedures in their execution.
142|H.263+: Video coding at low bit rates| In this tutorial paper, we discuss the ITU-T H.263+ (or H.263 Version 2) low-bit-rate video coding standard. We first describe, briefly, the H.263 standard including its optional modes. We then address the 12 new negotiable modes of H.263+. Next, we present experimental results for these modes, based on our public-domain implementation (see our Web site at
143|A Corpus for the Evaluation of Lossless Compression Algorithms|This paper investigates how the reliability of these evaluations can be ensured, particularly the repeatability of experiments, in line with scientific method. The evaluation of compression methods can be analytical or empirical. Analytical results are generally expressed in terms of the compression of a system relative to the entropy of the source, which is assumed to belong to a specified class. Such results tend to have only asymptotic significance; for example, the LZ78 method [ZL78] converges to the entropy for very large inputs, but in practical situations files are far too short for this convergence to have any significance. For this reason empirical results are needed to establish the practical worth of a method. The main factor measured in typical empirical experiments is the amount of compression achieved on some set of files. Researchers also often report the speed of compression, and the amount of primary memory required to perform the compression. The speed and memory requirements can be different for the encoding and decoding processes, and may depend on the file being compressed. This can result in a daunting number of factors that need to be presented. A number of authors have used the &#034;Calgary corpus&#034; of texts to provide empirical results for lossless compression algorithms. This corpus was collected in 1987, although it was not published until 1990 [BCW90]. Recent advances with compression algorithms have been achieving relatively small improvements in compression, measured using the Calgary corpus. There is a concern that algorithms are being fine-tuned to this corpus, and that small improvements measured in this way may not apply to other files. Furthermore, the corpus is almost ten years old, and over
144|A new model of LPC excitation for producing natural-sounding speech at low bit-rates|The excitation for LPC speech synthesis usually consists of two separate signals- a delta-function pulse once every pitch penod for voiced speech and white noise for unvoiced speech. This manner of representing excitation requires that speech segments be classified accurately into voiced and unvoiced categories and the pitch period of voiced segments be known. it is now well recognised that such a rigid idealization of the vocal excitation Li often responsible for the unna-tural qualizy a.ssock#ed with synthesized speech. This paper describes a new approach to the excitation problem that does not require a priori knowledge of either the voiced-unvoiced decision or the pitch period. All classes of sounds are generated by exciting the LPC filter with a sequence of pulses; the amplitudes and locations of the pulses are determined using a non-iterative analysis-by-synthesis procedure. This procedure minimizes a perceptual-distance metric representing subjectively-important differences between the waveforms of the origi-nal and the synthetic speech signals. The disiance metric takes account of the finite-frequency resolution as well as the ditTerential sensitivity of the human ear to errors in the formant and inter-formant regions of the speech spectrum.
145|Multiple Description Decoding of Overcomplete Expansions Using Projections onto Convex Sets|This paper presents a POCS-based algorithm for consistent reconstruction  of a signal x 2 R  K  from any subset of quantized coefficients y 2 R  N  in an  N \Theta K overcomplete frame expansion y = Fx, N = 2K. By choosing the  frame operator F to be the concatenation of two K \Theta K invertible transforms,  the projections may be computed in R  K  using only the transforms and their  inverses, rather than in the larger space R  N  using the pseudo-inverse as proposed  in earlier work. This enables practical reconstructions from overcomplete  frame expansions based on wavelet, subband, or lapped transforms of an entire  image, which has heretofore not been possible.  1 Introduction  Multiple description (MD) source coding is the problem of encoding a single source fX i g into N separate binary descriptions at rates R 1 ; : : : ; RN bits per symbol such that any subset S of the descriptions may be received and together decoded to an expected distortion D S commensurate with the total b...
147|Construction of Low Complexity Regular Quantizers for Overcomplete Expansions in R^N|In this paper, we study the construction of structured regular quantizers for  overcomplete expansions in 7 N. Our goal is to design structured quantizers al-  lowing simple reconstruction algorithms with low (memory and computational)  complexity and having good performance in terms of accuracy. Most related  work to date in quantized redundant expansions has assumed that uniform  scalar quantization with the same stepsize was used on the redundant expansion  and then has dealt with more complex methods to improve the reconstruction.
148|Ziv-Lempel Compressors with Deferred-Innovation|this paper is to explain the expansion caused by deferred innovation. II. Compression with Deferred Innovation A. Novel Pairs Suppose a deferred-innovation LZ algorithm operates on a string of b-bit input characters producing B-bit output words,* and assume that, as in most implementations, the dictionary of citations is initiallized with all individual symbols of the input alphabet. For an input string &#034;x y x z . . .&#034; such an algorithm will output B bits for the first &#034;x&#034;, and store &#034;xy&#034;; then output B
149|1993]. “A Mean–Removed Variation of Weighted Universal Vector Quantization for Image Coding |Abstract- Approaching a multi-codebook system as a codebook of codebooks, weighted universal vector quantiza-tion (WUVQ) uses traditional codeword design techniques to design locally optimal multi-codebook systems. Application of this technique to a sequence of medical images produces a 10.3 dB improvement over standard full search vector quan-tization followed by entropy coding at the cost of increased complexity. In this paper we propose a mean-removed vari-ation of WUVQ. Each codebook in the system is given a “mean ” or “prediction ” value which is subtracted from all supervectors that map to the given codebook. The chosen codebook’s codewords are then used to encode the result-ing residuals. Application of the mean-removed system to the medical data set achieves up to.5 dB improvement over WUVQ at no rate expense.
150|Hardware Data Compression for Wireless |The purpose of this work is to realise a breakthrough in the area of lossless data compression hardware by researching a low design complexity, high-compression, real-time statistical compression chip. We will use our expertise designing high-performance data compression technologies to solve the algorithmic and architectural issues that have prevented practical implementations in the past. The statistical compression chip will combine a context-based variable-order model with a high-speed multiplication-free arithmetic coder. The combination of these two techniques has been shown in software to be able to outperform the compression ratios offered by any dictionary-based technique.
151|The Laplacian Pyramid as a Compact Image Code| We describe a technique for image encoding in which local operators of many scales but identical shape serve as the basis functions. The representation differs from established techniques in that the code elements are localized in spatial frequency as well as in space. Pixel-to-pixel correlations are first removed by subtracting a lowpass filtered copy of the image from the image itself. The result is a net data compression since the difference, or error, image has low variance and entropy, and the low-pass filtered image may represented at reduced sample density. Further data compression is achieved by quantizing the difference image. These steps are then repeated to compress the low-pass image. Iteration of the process at appropriately expanded scales generates a pyramid data structure. The encoding process is equivalent to sampling the image with Laplacian operators of many scales. Thus, the code tends to enhance salient image features. A further advantage of the present code is that it is well suited for many image analysis tasks as well as for image compression. Fast algorithms are described for coding and decoding. A
152|Overview of Radar Data Compression |Radar data is routinely transmitted in real-time from the coterminous United States (CONUS) radar sites and placed on the Internet for incorporation into nowcasting, hydrology and modeling applications in near-realtime. Radar data are also archived for on-demand retrieval by the National Climate Data Center (NCDC). Data compression is used operationally to reduce bandwidth and storage requirements. There are several data compression techniques used operationally on radar data. Custom compression techniques have been devised for radar data that outperform generic techniques but the radar operations groups ultimately use off-the-shelf solutions. The underlying ideas behind compressibity are useful beyond just reducing the amount of data for transmission and archival. The compressibility of radar data has been found useful for devising quality control algorithms, especially for the detection and removal of test patterns.
153|An automated technique to quality control radar reflectivity data|Echoes in radar reflectivity data do not always correspond to precipitating particles. Echoes on radar may be due to biological targets such as insects, birds or wind-borne particles, due to anomalous propagation (AP) or ground clutter (GC) or due to test and interference patterns that inadvertently seep into the final products. Although weather forecasters can usually identify, and account for, the presence of such contamination, automated weather radar algorithms are drastically affected. Several horizontal and vertical features have been proposed to discriminate between precipitation echoes and echoes that do not correspond to precipitation. None of these features by themselves can discriminate between precipitating and non-precipitating areas. In this paper, we use a neural network to combine the individual features, some of which have already been proposed in the literature and some of which we introduce in this paper, into a single discriminator that can distinguish between “good ” and “bad ” echoes (i.e., precipitation
154|Coauthors, 2002: Project CRAFT: A test bed for demonstrating the real time acquisition and archival of WSR-88D level II data |by
155|Data Compression Using Antidictionaries|| We give a new text compression scheme based on Forbidden Words (&#034;antidictionary&#034;). We prove that our algorithms attain the entropy for balanced binary sources. They run in linear time. Moreover, one of the main advantages of this approach is that it produces very fast decompressors. A second advantage is a synchronization property that is helpful to search compressed data and allows parallel compression. The techniques used in this paper are from Information Theory and Finite Automata.  Keywords| Data Compression, Lossless compression, Information Theory, Finite Automaton, Forbidden Word, Pattern Matching.  I. 
157|Automata and Forbidden Words|Let L(M) be the (factorial) language avoiding a given anti-factorial language M . We design an automaton accepting L(M) and built from the language M . The construction is effective if M is finite. If M is the set of minimal forbidden words of a single word v, the automaton turns out to be the factor automaton of v (the minimal automaton accepting the set of factors of v).  We also give an algorithm that builds the trie of M from the factor automaton of a single word. It yields a non-trivial upper bound on the number of minimal forbidden words of a word. 
158|Text Compression Using Antidictionaries|We give a new text compression scheme based on Forbidden Words (&#034;antidictionary&#034;). We prove that our algorithms attain the entropy for equilibrated binary sources. One of the main advantage of this approach is that it produces very fast decompressors. A second advantage is a synchronization property that is helpful to search compressed data and to parallelize the compressor. Our algorithms can also be presented as &#034;compilers&#034; that create compressors dedicated to any previously fixed source. The techniques used in this paper are from Information Theory and Finite Automata; as a consequence, this paper shows that Formal Language Theory (in particular Finite Automata Theory) can be useful in Data Compression.  Keywords: data compression, information theory, finite automaton, forbidden word, pattern matching. 1 Introduction  We present a simple text compression method called DCA (Data Compression with Antidictionaries) that uses some &#034;negative&#034; information about the text, which is describe...
159|Forbidden Words in Symbolic Dynamics|We introduce an equivalence relation &#039; between functions from N  to N. By describing a symbolic dynamical system in terms of forbidden  words, we prove that the &#039;-equivalence class of the function that  counts the minimal forbidden words of a system is a topological invariant  of the system. We show that the new invariant is independent  from previous ones, but it is not characteristic. In the case of soc  systems we prove that the &#039;-equivalence of the corresponding functions  is a decidable question. As a more special application, we show,  by using the new invariant, that two systems associated to Sturmian  words having \dierent slope&#034; are not conjugate.  Classication: Symbolic Dynamics, Combinatoric on words, Automata and Formal Languages.  1 Introduction  In this paper we present a new topological invariant for Symbolic Dynamics. The techniques we use and some complementary results are from Combinatorics on words and from the theory of Automata and Formal Languages. Indeed there...
160|On Compact Directed Acyclic Word Graphs|The Directed Acyclic Word Graph (DAWG) is a space-efficient  data structure to treat and analyze repetitions in a text, especially  in DNA genomic sequences. Here, we consider the Compact Directed  Acyclic Word Graph of a word. We give the first direct algorithm to  construct it. It runs in time linear in the length of the string on a fixed  alphabet. Our implementation requires half the memory space used by  DAWGs.
161|Pattern Matching in Text Compressed by Using Antidictionaries|In this paper we focus on the problem of compressed pattern matching for  the text compression using antidictionaries, which is a new compression scheme  proposed recently by Crochemore et al. (1998). We show an algorithm which  preprocesses a pattern of length m and an antidictionary M in O(m  2  + kMk)  time, and then scans a compressed text of length n in O(n + r) time to find all  pattern occurrences, where kMk is the total length of strings in M and r is the  number of the pattern occurrences.   
162|Minimal Forbidden Words and Factor Automata|. Let L(M) be the (factorial) language avoiding a given antifactorial language M . We design an automaton accepting L(M) and built from the language M . The construction is effective if M is finite. If M is the set of minimal forbidden words of a single word v, the automaton turns out to be the factor automaton of v (the minimal automaton accepting the set of factors of v).  We also give an algorithm that builds the trie of M from the factor automaton of a single word. It yields a non-trivial upper bound on the number of minimal forbidden words of a word.  Keywords: factorial language, anti-factorial language, factor code, factor automaton, forbidden word, avoiding a word, failure function. 1 Introduction  Let L ` A    be a factorial language, i.e., a language containing all factors of its words. A word w 2 A    is called a minimal forbidden word for L if w = 2 L and all proper factors of w belong to L. We denote by MF (L) the language of minimal forbidden words for L.  The study of co...
164|Wavelets and audio data compression |Abstract- In this work we present an audio data compression software package which uses a Discrete Wavelet Transform based compression procedure. The used wavelet function can be chosen from the well known Daubechies class of compactly supported wavelets. Our implementation uses an adaptive manner for the wavelet domain threshold value computation. The package has two major components: a compression software which reads a standard audio data file and the reconstruction software which reads a compressed file and generates a standard audio file. Experimental results are also presented. I. DATA COMPRESSION WITH WAVELETS Data compression is a very largely used procedure for data storage purposes. There exist a large variety of compression algorithms, many of them standardised and each of them having its advantages and its backdraws. They offers speed, high compression ratio, portability etc., but there is not a single algorithm which best fit to all kind of applications. They were classified in two major categories, the first includes the ones for compression without loss and the second one includes those for compression with loss. All the algorithms have the same purpose, to reduce the information redundancy from the considered data set, but those from the first class allow a perfect reconstruction while those from the second class does not. Those procedures which allow small information losses can achieve higher compression ratio, but they can not be utilised in applications where the data set integrity is primordial (for file compression for example). This second class includes various voice and image compression algorithms. In this work we present a complete audio compression/decompression software package, which implements an algorithm from this second class, which exploits a very popular orthogonal transform named the Discrete Wavelet Transform (DWT), [1], [2]. The topics of the wavelet transforms and their properties had been studied very extensively by the authors in the context of various data compression and signal to noise (SNR) enhancement
165|Orthonormal bases of compactly supported wavelets|  Several variations are given on the construction of orthonormal bases of wavelets with compact support. They have, respectively, more symmetry, more regularity, or more vanishing moments for the scaling function than the examples constructed in Daubechies [Comm. Pure Appl. Math., 41 (1988), pp. 909-996].
166|Nonlinear Wavelet Methods for Recovery of Signals, Densities, and Spectra from Indirect and Noisy Data|. We describe wavelet methods for recovery of objects from noisy and incomplete data. The common themes: (a) the new methods utilize nonlinear operations in the wavelet domain; (b) they accomplish tasks which are not possible by traditional linear/Fourier approaches to such problems. We attempt to indicate the heuristic principles, theoretical foundations, and possible application areas for these methods. Areas covered: (1) Wavelet De-Noising. (2) Wavelet Approaches to Linear Inverse Problems. (4) Wavelet Packet De-Noising. (5) Segmented MultiResolutions. (6) Nonlinear Multi-resolutions. 1. Introduction.  With the rapid development of computerized scientific instruments comes a wide variety of interesting problems for data analysis and signal processing. In fields ranging from Extragalactic Astronomy to Molecular Spectroscopy to Medical Imaging to Computer Vision, one must recover a signal, curve, image, spectrum, or density from incomplete, indirect, and noisy data. What can wavelets ...
167|Techniques For Variable Rate Speech Coding Using Wavelet Representations |This paper presents two techniques for variable rate speech coding using wavelet representations. In the first method, the Daubechies wavelet is used to produce a wavelet representation of the speech signal. This representation is encoded using some of the properties of the speech signal to allocate bits to the various subbands. A technique to lower the bit rate using vector quantisation is discussed and perceptual results are reported. The second technique caters to the encoding of speech in the presence of road noise, as in the cellular environment. It has been observed that in the presence of noise, the wavelet extrema representation provides a &#034;denoising effect&#034;, and the temporal correlation of the speech signal can be exploited. A modification of Vetterli &#039;s projection algorithm using spline interpolation has been found to give perceptually better results for speech encoded in the presence of noise. Examples of signals encoded by this method are shown. Preliminary results indicate...
168|Combining Models in Data Compression |We propose Beta Weighting as a simple linear weighting scheme for combining different models in data compression. Suppose we are given a finite number of models. Under the assumption that with a given a priori probability distribution one of the models is the best – but we do not know which one and we do not have further knowledge about the models – Beta Weighting is optimal in the sense that it yields minimum redundancy. Every single update of each weight requires only a constant number of arithmetic operations. 1
169|The Context-Tree Weighting Method: Basic Properties|We describe a sequential universal data compression procedure for binary tree sources that performs the &#034;double mixture.&#034; Using a context tree, this method weights in an efficient recursive way the coding distributions corresponding to all bounded memory tree sources, and achieves a desirable coding distribution for tree sources with an unknown model and unknown parameters. Computational and storage complexity of the proposed procedure are both linear in the source sequence length. We derive a natural upper bound on the cumulative redundancy of our method for individual sequences. The three terms in this bound can be identified as coding, parameter, and model redundancy. The bound holds for all source sequence lengths, not only for asymptotically large lengths. The analysis that leads to this bound is based on standard techniques and turns out to be extremely simple. Our upper bound on the redundancy shows that the proposed context-tree weighting procedure is optimal in the sense that it achieves the Rissanen (1984) lower bound.
170|Switching between two universal source coding algorithms|This paper discusses a switching method which can be used to combine two sequential universal source coding algorithms. The switching method treats these two algorithms as black-boxes and can only use their estimates of the probability distributions for the consecutive symbols of the source sequence. Three weighting algorithms based on this switching method are presented. Empirical results show that all three weighting algorithms give a performance better than the performance of the source coding algorithms they combine. 1
171|Implementing the Context Tree Weighting Method for Text Compression|Context tree weighting method is a universal compression algorithm for FSMX sources. Though we expect that it will have good compression ratio in practice, it is difficult to implement it and in many cases the implementation is only for estimating compression ratio. Though Willems and Tjalkens showed practical implementation using not block probabilities but conditional probabilities, it is used for only binary alphabet sequences. We extend the method for multi-alphabet sequences and show a simple implementation using PPM techniques. We also propose a method to optimize a parameter of the context tree weighting for binary alphabet case. Experimental results on texts and DNA sequences show that the performance of PPM can be improved by combining the context tree weighting and that DNA sequences can be compressed in less than 2.0 bpc.
172|The switching method: elaborations|The switching method [4] is a scheme which combines two universal source coding algorithms. The two universal source coding algorithms both estimate the probability distribution of the source symbols, and the switching method allows an encoder to choose which of the two probability distributions it uses for every source symbol. The switching algorithm is an efficient weighting algorithm that uses this switching method. This paper focuses on the companion algorithm, the algorithm running in parallel to the main CTW-algorithm. 1 The switching method: A short introduction The switching method [4] defines a way in which two modeling algorithms can be combined. Consider a source sequence x1,..., xN. Suppose that two sequential modeling algorithms, A and B, run both along the entire source sequence, and give for every symbol an estimate of its probability distribution. These modeling algorithms could be memoryless estimators, estimators for fixed tree models, or entire universal source coding algorithms on their own. At each moment the encoder in the switching method uses
173|Context-Tree Weighting and Maximizing: Processing Betas. Inaugural Workshop ITA (Information Theory and its  (2006) |Abstract — The context-tree weighting method (Willems, Shtarkov, and Tjalkens [1995]) is a sequential universal source coding method that achieves the Rissanen lower bound [1984] for tree sources. The same authors also proposed context-tree maximizing, a two-pass version of the context-tree weighting method [1993]. Later Willems and Tjalkens [1998] described a method based on ratios (betas) of sequence probabilities that can be used to reduce the storage complexity of the contexttree weighting method. These betas can be applied to express a posteriori model probabilities in a recursive way (Willems, Nowbahkt-Irani, Volf [2001]). In the present paper we present new results related to betas. These results provide a new view on the relation between context-tree weighting and maximizing. ?10 = 0.3 ?00 = 0.5
174|Language Acquisition and Data Compression|Statistical data compression requires a stochastic language model which must rapidly adapt to new data as it is encountered. A grammatical inference engine is introduced which satisfies this requirement; it is able to discover structure in arbitrary data using nothing more than the predictions of a simple trigram model. We show that compression may be used as an alternative to perplexity for language model evaluation, and that the information processing techniques employed by our system may reflect what happens in the human brain. 1 Introduction Grammatical inference is the process of programming a computer to automatically infer a grammar for a language [8]. We consider a grammar to be nothing more than a model for some data. Applications such as speech recognition and data compression require a stochastic language model, and well-defined performance measures exist for such models. It is easy to get caught in the trap of building complicated models which utilise various ad hoc techni...
175|Bayesian Methods for Adaptive Models|The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non-linear models. This framework quantitatively embodies `Occam&#039;s razor&#039;. Over-complex and under-regularised models are automatically inferred to be less probable, even though their flexibility allows them to fit the data better. When applied to `neural networks&#039;, the Bayesian framework makes possible (1) objective comparison of solutions using alternative network architectures; (2) objective stopping rules for network pruning or growing procedures; (3) objective choice of type of weight decay terms (or regularisers); (4) on--line techniques for optimising weight decay (or regularisation constant) magnitude; (5) a measure of the effective number of well-determined parameters in a model; (6) quantified estimates of the error bars on network parameters and on network output. In the case of classification models, it is sho...
176|Natural Language Grammatical Inference|This project is concerned with programming a computer to make predictions about which words are most likely to follow a small segment of English text. At first this may seem a strange problem, but I intend to show that there exist a wide range of applications that would benefit from such a program. Indeed, my motivation for approaching this problem was to provide a way of improving the accuracy of speech recognition systems. Additionally, I am interested with the problem of Grammatical Inference. In fact, the word prediction problem and the Grammatical Inference problem are intertwined, and it seems that approaching either one will lead to the other. Grammatical Inference entails inferring a grammar for an arbitrary language from a finite set of sample sentences in the language. It is quite easy to measure the performance of a word prediction system, providing that its prediction is given as a probability distribution. This allows us to compare our predictor with others, such as the tr...
177|The Guessing Game: A Paradigm for Artificial Grammar Learning|In a guessing game, Ss reconstruct a sequence by guessing each successive element of the sequence from a finite set of alternatives, receiving feedback after each guess. An upper bound on Ss knowledge of the sequence is given by  H, the estimated entropy of the numbers of guesses. The method provides a measure of learning independent of material type and distractors, and the resulting data set is very rich. Here, the method is applied to artificial grammar learning; Ss were exposed to strings from a finite state grammar and subsequently distinguished between strings that followed or violated the grammar reliably better than Ss who had not seen the learning strings (but who themselves performed at above chance levels). Ss knowledge of the strings,  H, reflected both grammaticality and exposure to learning strings, and was correlated with overall judgement performance. For non-grammatical strings, the strings that Ss knew most about were those they found most difficult to classify correc...
178|ECG Data Compression |Electrocardiogram (ECG) signal play important role in diagnosis and analysis of heart diseases. For effective detection and diagnosis of cardiac diseases, the ECG data are continuously recorded, stored and transmitted. But long-term continuous monitoring generates large amount of data which becomes difficult for storage as well as for transmission. Therefore a efficient ECG data compression algorithm is needed that gives better compression ratio and less loss of data in reconstructed signal. This paper presents a wavelet transform and turning point algorithm based ECG data compression. The data is taken from physionet and implemented using discrete wavelet i.e daubechies (db7) wavelet. Then energy packing efficiency of wavelet coefficients is found and then turning point algorithm is applied to the wavelet coefficients to get compressed form of data and reconstruction is done using inverse wavelet transform. These steps are implemented in MATLAB.
179|Simultaneous Data Compression and Encryption |ABSTRACT-Data compression is known for reducing storage and communication costs. It involves transforming data of a given format, called source message to data of a smaller sized format called codeword. Data encryption is known for protecting formation from eavesdropping. It transforms data of a given format, called plaintext, to another format, called cipher text, using an encryption key. The major problem existing with the current compression and encryption methods is the speed, i.e. the processing time required by a computer. To lessen the problem, combine the two processes into one. The basic idea of the combining the two processes is to add a pseudo random shuffle into a data compression. The aim behind the shuffling process is to get the different Huffman table for the original Huffman tree by shuffling nodes in the tree. Using the numbers generated by pseudo random generator, nodes in the Huffman tree are shuffled. After the encryption we will get one mapping Huffman table, which is not identical to the original. Finally this table only will send across the network. Once the Huffman table is encrypted no one having the decompression module can decrypt it. So in this new algorithm compression and encryption is done simultaneously. KEYWORDS:Encryption,Decryption,Compression,Datacompre ssion,Decompression,Huffman compression. 1.
180|Lossless Data Compression |This document has been approved for publication by the Management Council of the Consultative Committee for Space Data Systems (CCSDS) and represents the consensus technical agreement of the participating CCSDS Member Agencies. The procedure for review and authorization of CCSDS Recommendations is detailed in the Procedures Manual for the Consultative Committee for Space Data Systems (reference [B1]), and the record of Agency participation in the authorization of this document can be obtained from the CCSDS Secretariat at the address below. This Recommendation is published and maintained by: CCSDS Secretariat Program Integration Division (Code MG) National Aeronautics and Space Administration Washington, DC 20546, USA  CCSDS RECOMMENDATION FOR LOSSLESS DATA COMPRESSION CCSDS 121.0-B-1 Page ii May 1997
181|Vector Map Data Compression with |Wavelets and wavelet transforms can be used for vector-map data compression. The choice of wavelet, the level of decomposition, the method of thresholding, the height of the threshold, relative CPU times and file sizes, and reconstructed map appearance were investigated using the Wavelet Toolbox of MATLAB. Quantitative error measures were obtained. For two test vector-map data sets consisting of longitude and latitude points, compressions of 35 to 50 percent (1–5:1 to 2:1) were obtained with root-mean-square errors less than 0–003 to 0–01 ° longitude}latitude for wavelet packet decompositions using selected wavelets.
182|Wavelet Multiscale Edge Detection for Extraction of Geographic Features from High Resolution Satellite Images to Improve Vector-map Databases|Although numerous at smaller geographic scales, vector databases often do not exist at the more detailed, larger scales. A possible solution is the use of image processing techniques to detect edges in high-resolution satellite imagery. Features such as roads and airports are formed from the edges and matched up with similar features in existing low-resolution vector map databases. By replacing the old features with the new more accurate features, the resolution of the existing map database is improved. To accomplish this, a robust edge detection algorithm is needed that will perform well in noisy conditions. This paper studies and tests one such method, the Wavelet Multi-scale Edge Detector. The wavelet transform breaks down a signal into frequency bands at different levels. Noise present at lower scales smoothes out at higher levels. It is demonstrated that this property can be used to detect edges in noisy satellite imagery. Once edges are located, a new method will be proposed for storing these edges geographically so that features can be formed and paired with existing features in a vector map database.
183|Entropy-Based Algorithms For Best Basis Selection|pretations (position, frequency, and scale), and we have experimented with feature-extraction methods that use best-basis compression for front-end complexity reduction. The method relies heavily on the remarkable orthogonality properties of the new libraries. It is obviously a nonlinear transformation to represent a signal in its own best basis, but since the transformation is orthogonal once the basis is chosen, compression via the best-basis method is not drastically affected by noise: the noise energy in the transform values cannot exceed the noise energy in the original signal. Furthermore, we can use information cost functionals defined for signals with normalized energy, since all expansions in a given library will conserve energy. Since two expansions will have the same energy globally, it is not necessary to normalize expansions to compare their costs. This feature greatly enlarges the class of functionals usable by the method, speeds the best-basis search, and provides a geom
184|Computation of channel capacity and rate-distortion functions|A&amp;r&amp;-By defining mutual information as a maximum over an appropriate space, channel capacities can be defined as double maxima and rate-distortion functions as double minima. This approach yields valuable new insights regarding the computation of channel capacities and rate-distortion functions. In particular, it suggests a simple algo-rithm for computing channel capacity that consists of a mapping from the set of channel input probability vectors into itself such that the sequence of probability vectors generated by successive applications of the mapping converges to the vector that achieves the capacity of the given channel. Analogous algorithms then are provided for computing ram-distortion functions and constrained channel capacities. The algo-rithms apply both to discrete and to continuous alphabet channels or sources. In addition, a formalization of the theory of channel capacity in the presence of constraints is included. Among the examples is the calculation of close upper and lower bounds to the rate-distortion function of a binary symmetric Markov source. C
185|The Rate-Distortion Region for Multiple Descriptions without Excess Rate|During recent years there has been strong interest in a certain source coding problem, which some authors call the &#034;problem of multiple descriptions&#034;. Old and new wringing techniques enable us to establish a single--letter characterization of the rate--distrotion region in the case of no excess rate for the joint description. 1 The Result  Since the origin of the problem of multiple descriptiona and motivations for its study have already been described in an extensive literature [1]--[9], we present our result immediately. It goes considerably beyond those of [17], where the reader also will find a detailed discussion of previously known results. We are given the following. 1) A sequence (X t )  1  t=1 of independent and identically distributed random variables with values in a finite set X , that is, a discrete memoryless source (DMS). 2) Three finite reconstruction spaces   X 0 ,   X 1 , and   X 2 , together with associated per-- letter distortion measures  d i : X \Theta    X i ! R ...
186|Vector Quantization with Complexity Costs|Vector quantization is a data compression method where a set of data points is encoded by a reduced set of reference vectors, the codebook. We discuss a vector quantization strategy which jointly optimizes distortion errors and the codebook complexity, thereby, determining the size of the codebook. A maximum entropy estimation of the cost function yields an optimal number of reference vectors, their positions and their assignment probabilities. The dependence of the codebook density on the data density for different complexity functions is investigated in the limit of asymptotic quantization levels. How different complexity measures influence the efficiency of vector quantizers is studied for the task of image compression, i.e., we quantize the wavelet coefficients of gray level images and measure the reconstruction error. Our approach establishes a unifying framework for different quantization methods like K-means clustering and its fuzzy version, entropy constrained vector quantizati...
187|Vector Quantization of Image Subbands: A Survey|Subband and wavelet decompositions are powerful tools in image coding, because of their decorrelating effects on image pixels, the concentration of energy in a few coefficients, their multirate/multiresolution framework, and their frequency splitting which allows for efficient coding matched to the statistics of each frequency band and to the characteristics of the human visual system. Vector quantization provides a means of converting the decomposed signal into bits in a manner that takes advantage of remaining inter- and intra-band correlation as well as of the more flexible partitions of higher dimensional vector spaces. Since 1988 a growing body of research has examined the use of vector quantization for subband/wavelet transform coefficients. We present a survey of these methods. 1 Introduction Image compression maps an original image into a bit stream suitable for communication over or storage in a digital medium. The number of bits required to represent the coded image should b...
188|A Vector Quantization Approach to Universal Noiseless Coding and Quantization|Abstract-A two-stage code is a block code in which each block of data is coded in two stages: the first stage codes the identity of a block code among a collection of codes, and the second stage codes the data using the identified code. The collection of codes may he noiseless codes, fixed-rate quantizers, or variable-rate quantizers. We take a vector quantization approach to two-stage coding, in which the first stage code can be regarded as a vector quantizer that “quantizes ” the input data of length n to one of a fixed collection of block codes. We apply the generalized Lloyd algorithm to the first-stage quantizer, using induced measures of rate and distortion, to design locally opti-mal two-stage, codes. On a source of medical images, two-stage variahle-rate vector quantizers designed in this way outperform standard (one-stage) fixed-rate vector quantizers by over 9 dB. The tail of the operational distortion-rate function of the first-stage quantizer determines the optimal rate of convergence of the redundancy of a universal sequence of two-stage codes. We show that there exist two-stage universal noiseless codes, fixed-rate quantizers, and variable-rate quantizers whose per-letter rate and distortion redundancies converge to zero as (k/2)n- ’ logn, when the universe of sources has finite dimension k. This extends the achievability part of Rissanen’s theorem from universal noiseless codes to universal quantizers. Further, we show that the redundancies converge as O(n-’) when the universe of sources is countable, and as O(r~-l+‘) when the universe of sources is infinite-dimensional, under appropriate conditions. Index Terms-Two-stage, adaptive, compression, minimum de-scription length, clustering. I.
189|The Minimax Distortion Redundancy in Empirical Quantizer Design|We obtain minimax lower and upper bounds for the expected distortion redundancy of empirically designed vector quantizers. We show that the mean squared distortion of a vector quantizer designed from n i.i.d. data points using any design algorithm is at least\Omega  i  n  \Gamma1=2  j  away from the optimal distortion for some distribution on a bounded subset of R  d  . Together with existing upper bounds this result shows that the minimax distortion redundancy for empirical quantizer design, as a function of the size of the training data, is asymptotically on the order of n  1=2  . We also derive a new upper bound for the performance of the empirically optimal quantizer.   
190|The Optimal Lattice Quantizer in Three Dimensions|The body-centered cubic lattice is shown to have the smallest mean squared error of any lattice quantizer in three dimensions, assuming that the input to the quantizer has a uniform distribution.  
191|Asymptotic Performance of Multiple Description Lattice Quantizers|The high-rate squared-error distortions of a balanced multiple description lattice vector quantizer are analyzed for a memoryless source with probability density function p, differential entropy h(p) !  1, and lattice codebook . For any a 2 (0; 1) and rate pair (R; R), it is shown that the two-channel distortion   d0 and the channel 1 (or channel 2) distortion  ds  satisfy  lim  R!1   d02  2R(1+a)  = G()2  2h(p)  =4  and  lim  R!1   ds2  2R(1\Gammaa)  = G(SL)2  2h(p)  ;  where G() is the normalized second moment of a Voronoi cell of the lattice  and G(SL) is the normalized second moment of a sphere in L dimensions.  I. Introduction  We consider a two-channel multiple description quantization system for a discrete-memoryless source with differential entropy h(p). The quantizer transmits information on each channel at rate R bits/sample. The mean-squared error when both channels work is denoted by   d0 and when either channel works is denoted by   ds . It has been shown [1] that for a un...
192|Optimization of Lattices for Quantization|A training algorithm for the design of lattices for vector quantization is presented. The algorithm uses a steepest descent method to adjust a generator matrix, in the search for a lattice whose Voronoi regions have minimal normalized second moment. The numerical elements of the found generator matrices are interpreted and translated into exact values. Experiments show that the algorithm is stable, in the sense that several independent runs reach equivalent lattices. The obtained lattices reach as low second moments as the best previously reported lattices, or even lower. Specifically, we report lattices in 9 and 10 dimensions with normalized second moments of 0.0716 and 0.0708, respectively, and nonlattice tessellations in 7 and 9 dimensions with 0.0727 and 0.0711, which improves on previously known values. The new 9and 10-dimensional lattices suggest that Conway and Sloane&#039;s conjecture on the duality between the optimal lattices for packing and quantization might be false. A discussi...
193|Theory and Practice of Vector Quantizers Trained on Small Training Sets|We examine how the performance of a memoryless vector quantizer changes as a function of its training set size. Specifically, we study how well the training set distortion predicts test distortion when the training set is a randomly drawn subset of blocks from the test or training image(s). Using the Vapnik-Chervonenkis dimension, we derive formal bounds for the difference of test and training distortion of vector quantizer codebooks. We then describe extensive empirical simulations that test these bounds for a variety of bit rates and vector dimensions, and give practical suggestions for determining the training set size necessary to achieve good generalization from a codebook. We conclude that, by using training sets comprised of only a small fraction of the available data, one can produce results that are close to the results obtainable when all available data are used. 1 Introduction  Vector quantization (VQ) [7, 8] is a data compression technique that can be used to reduce the sto...
194|Block-Constrained Methods of Fixed-Rate, Entropy-Coded, Scalar Quantization|Motivated by the recentwork of Laroia and Farvardin, this paper presents new reducedcomplexity methods for avoiding the bu#ering problems associated with entropy-coded, scalar quantization. Basically, given a #xed-size source block, these methods use dynamic programming and other techniques to search the sequences produceable byanentropycoded, scalar quantizer for one with minimum distortion subject to a constraint on the number bits produced by some binary encoding of these sequences. The result is that although some encoding methods mighthaveavariable rate on the sample level, the overall quantizer has a #xed rate on the block level. A general class of such methods, called block-constrained quantization, is introduced. Away to reduce the encoding complexity and several ways to to simplify the search complexity are found. A node-varying method with improved performance is given. New insightinto the performance of block-constrained quantizers is presented. Compared to the original Laroia-Farvardin method, the results presented here show small improvements in performance and large reductions in complexity. Key Words : Entropy Coding, Scalar Quantization, Structured Vector Quantization, Block-Constrained Quantization, Fixed-Rate Quantization.
195|In Search of the Optimal Searching Sequence for VQ Encoding|The codeword searching sequence is sometimes very vital to the efficiency of a VQ encoding algorithm. In this paper, we evaluate some necessary criteria for the derivation of an optimal searching sequence and derive the optimal searching sequence based on such criteria.  Keywords---  I. Introduction  T  WO common strategies have been used to reduce the complexity inherent in Vector Quantization (VQ) encoding algorithms. One resorts to simpler but suboptimal variants and sacrifices quality such as the tree searched VQ[1]. The other remains with the original VQ and devises fast algorithms such as the partial distance search (PDS). This second category of algorithms is more flexible since they are codebook-independent[2-8], but the searching sequence of the codewords is very vital to the efficiency of the algorithms.  Consider the case that one has to represent a given Ddimensional input vector ~x = (x 1 ; x 2 :::x D ) with a particular codeword selected from a codebook containing N codew...
196|Critical Data Compression|A new approach to data compression is developed and applied to mul-timedia content. This method separates messages into components suit-able for both lossless coding and ’lossy ’ or statistical coding techniques, compressing complex objects by separately encoding signals and noise. This is demonstrated by compressing the most significant bits of data exactly, since they are typically redundant and compressible, and either fitting a maximally likely noise function to the residual bits or compress-ing them using lossy methods. Upon decompression, the significant bits are decoded and added to a noise function, whether sampled from a noise model or decompressed from a lossy code. This results in compressed data similar to the original. Signals may be separated from noisy bits by considering derivatives of complexity in a manner akin to Kolmogorov’s approach or by empirical testing. The critical point separating the two represents the level beyond which compression using exact methods be-
197|Algorithmic information theory|This paper reviews algorithmic information theory, which is an attempt to apply information-theoretic and probabilistic ideas to recursive function theory. Typical concerns in this approach are, for example, the number of bits of information required to specify an algorithm, or the probability that a program whose bits are chosen by coin flipping produces a given output. During the past few years the definitions of algorithmic information theory have been reformulated. The basic features of the new formalism are presented here and certain results of R. M. Solovay are reported.
198|On Macroscopic Complexity and Perceptual Coding|ar
199|Bigtable: A distributed storage system for structured data|Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.  
200|MapReduce: Simplified Data Processing on Large Clusters|MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.  
201|Space/Time Trade-offs in Hash Coding with Allowable Errors|this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash- coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency
202|The Google File System |We have designed and implemented the Google File Sys-tem, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous dis-tributed file systems, our design has been driven by obser-vations of our application workloads and technological envi-ronment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore rad-ically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our ser-vice as well as research and development efforts that require large data sets. The largest cluster to date provides hun-dreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use. Categories and Subject Descriptors D [4]: 3—Distributed file systems
203|The ubiquitous B-tree|B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and nnplemented using B-trees This paper reviews B-trees and shows why they have been so successful It discusses the major variations of the B-tree, especially the B+-tree,
204|Parallel database systems: the future of high performance database systems|Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1.
205|Resource Containers: A New Facility for Resource Management in Server Systems|General-purpose operating systems provide inadequate support for resource management in large-scale servers. Applications lack sufficient control over scheduling and management of machine resources, which makes it difficult to enforce priority policies, and to provide robust and controlled service. There is a fundamental mismatch between the original design assumptions underlying the resource management mechanisms of current general-purpose operating systems, and the behavior of modern server applications. In particular, the operating system’s notions of protection domain and resource principal coincide in the process abstraction. This coincidence prevents a process that manages large numbers of network connections, for example, from properly allocating system resources among those connections. We propose and evaluate a new operating system abstraction called a resource container, which separates the notion of a protection domain from that of a resource principal. Resource containers enable fine-grained resource management in server systems and allow the development of robust servers, with simple and firm control over priority policies. 1
206|Operating System Support for Planetary-Scale Network Services|PlanetLab is a geographically distributed overlay network designed to support the deployment and evaluation of planetary-scale network services. Two high-level goals shape its design. First, to enable a large research community to share the infrastructure, PlanetLab provides distributed virtualization, whereby each service runs in an isolated slice of PlanetLab’s global resources. Second, to support competition among multiple network services, PlanetLab decouples the operating system running on each node from the networkwide services that define PlanetLab, a principle referred to as unbundled management. This paper describes how Planet-Lab realizes the goals of distributed virtualization and unbundled management, with a focus on the OS running on each node. 1
207|Paxos made live: an engineering perspective|We describe our experience building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected algorithmic and engineering problems encountered, and the solutions we found for them. Our measurements indicate that we have built a competitive system. 1
208|Integrating compression and execution in column-oriented database systems|Column-oriented database system architectures invite a reevaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads. In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in roworiented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.
209|Weaving Relations for Cache Performance|Relational database systems have traditionally optimzed for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM) (a.k.a., slotted pages). Recent research, however, indicates that cache utilization and performance is becoming increasingly important on modern platforms. In this paper, we first demonstrate that in-page data placement is the key to high cache performance and that NSM exhibits low cache utilization on modern platforms. Next, we propose a new data organization model called PAX (Partition Attributes Across), that significantly improves cache performance by grouping together all values of each attribute within each page. Because PAX only affects layout inside the pages, it incurs no storage penalty and does not affect I/O behavior. According to our experimental results, when compared to NSM (a) PAX exhibits superior cache and memory bandwidth utilization, saving at least 75% of NSM&#039;s stall time due to data cache accesses, (b) range selection queries and updates on memoryresident relations execute 17-25% faster, and (c) TPC-H queries involving I/O execute 11-48% faster.
210|Data Placement in Bubba|Thus paper examines the problem of data placement in Bubba, a highly-parallel system for data-intensive applications bemg developed at MCC “Highly-parallel” lmplres that load balancmng IS a cntlcal performance issue “Data-mtenave” means data IS so large that operatrons should be executed where the data resides As a result, data placement becomes a cntlcal performance issue In general, determmmg the optimal placement of data across processmg nodes for performance IS a difficult problem We describe our heuristic approach to solvmg the data placement problem w Bubba We then present expenmental results using a specific workload to provide msrght into the problem Several researchers have argued the benefits of deelustering (1 e, spreading each base relation over many nodes) We show that as declustermg IS increased. load balancing contmues to improve However, for transactions mvolvmg complex Joins, further declusterrng reduces throughput because of communications, startup and termmatron overhead We argue that data placement, especially declustermg, m a highly-parallel system must be considered early in the design, so that mechanrsms can be included for supportmg variable declustermg, for mmtmlzmg the most significant overheads associated with large-scale declustenng, and for gathering the required statistics.
211|DB2 Parallel Edition|The rate of increase in database size and response time requirements has outpaced  advancements in processor and mass storage technology. One way to satisfy the increasing  demand for processing power and I/O bandwidth in database applications is to have a  number of processors, loosely or tightly coupled, serving database requests concurrently.  Technologies developed during the last decade have made commercial parallel database  systems a reality and these systems have made an inroad into the stronghold of traditionally  mainframe based large database applications. This paper describes the parallel database  project initiated at IBM Research at Hawthorne and the DB2/AIX-PE product based on  it.  1 Introduction  Large scale parallel processing technology has made giant strides in the past decade and there is no doubt that it has established a place for itself. However, almost all of the applications harnessing this technology are scientific or engineering applications. The lack of com...
212|Data Compression with Restricted Parsings |Abstract- We consider a class of algorithms related to Lempel-Ziv that incorporate restrictions on the manner in which the data can be parsed with the goal of introducing new tradeoffs between implementation complexity and data compression ratios. Our main motivation lies within the field of compressed memory computer systems. Here requirements include extremely fast decompression and compression speeds, adequate compression performance on small data block lengths, and minimal hardware area and energy requirements. We describe the approach and provide experimental data concerning its compression performance with respect to known alternatives. We show that for a variety of data sets stored in a typical main memory, this direction yields results close to those of earlier techniques, but with significantly lower energy consumption at comparable or better area requirements. The technique thus may be of eventual interest for a number of applications requiring high compression bandwidths and efficient hardware implementation. 1 I.
213|Banked Multiported Register Files for High-Frequency Superscalar Microprocessors|Multiported register files are a critical component of high-performance superscalar microprocessors. Conventional multiported structures can consume significant power and die area. We examine the designs of banked multiported register files that employ multiple interleaved banks of fewer ported register cells to reduce power and area. Banked register files designs have been shown to provide sufficient bandwidth for a superscalar machine, but previous designs had complex control structures that would likely limit cycle time and add to design complexity. We develop a banked register file with much simpler and faster control logic while only slightly increasing the number of ports per bank. We present area, delay, and energy numbers extracted from layouts of the banked register file. For a four-issue superscalar processor, we show that we can reduce area by a factor of three, access time by 20%, and energy by 40%, while decreasing IPC by less than 5%.
214|Algorithms and Data Structures for Compressed-memory Machines|This paper gives an overview of a set of algorithms and data structures developed for systems with main-memory compression. In such systems, essentially all data are maintained in compressed form, and decompressed on access. The advantage is a potentially substantial improvement in price/performance, since the memory is typically the most expensive component in the central electronic complex (excluding disk storage) in server-class machines. The approaches described here are incorporated in IBM Memory Expansion Technology (MXT) [1], which for typical systems yields a factor of 2 expansion in the effective memory size, with generally minimal effect on performance. It is well known that the contents of memory are generally compressible. This can occur because of repeated patterns in data or programs, or alternatively because pages may not be entirely filled; that is, they may have long strings of zeros. Such compressibility traditionally is exploited in a number of ways, including compression of files to be sent to disks or transmitted over networks, and also maintaining substantial portions of main-memory contents in compressed form. The latter approach can be seen in the IBM System/390* (for example see [2]), for which a primary use is in
215|Pattern Recognition by Data Compression|Combinatorial pattern recognition is used for discrimination of handwritten  signatures. The method partitions an input image into overlapping  pieces. A piece is as large as possible under the constraints first that it  must have an identical copy at a smaller row number, second that the  piece must have a fixed width. The partitioning into pieces is performed  according to a generalisation of the Lempel--Ziv procedure for lossless  data compression[2]. A set of programs has been implemented. Applications  are shown for recognition of graphical patterns in printed text  and for discrimination of handwritten signatures.  Introduction  The design is based on the belief that maximal compression of data must reveal regular patterns. Kolmogorov complexity [3] is defined using partial recursive functions for compression, but the Kolmogorov complexity of a string is not computable. Lempel and Ziv [2] have defined a weaker concept for the complexity of a string. They base their definition on ...
216|Scale-Space Theory in Computer Vision|A basic problem when deriving information from measured data, such as images, originates from the fact that objects in the world, and hence image structures, exist as meaningful entities only over certain ranges of scale. &#034;Scale-Space Theory in Computer Vision&#034; describes a formal theory for representing the notion of scale in image data, and shows how this theory applies to essential problems in computer vision such as computation of image features and cues to surface shape. The subjects range from the mathematical foundation to practical computational techniques. The power of the methodology is illustrated by a rich set of examples.

This book is the first monograph on scale-space theory. It is intended as an introduction, reference, and inspiration for researchers, students, and system designers in computer vision as well as related fields such as image processing, photogrammetry, medical image analysis, and signal processing in general.

For more information, please see http://www.nada.kth.se/~tony/book.html 
217|Adaptive Online Data Compression|Quickly transmitting large datasets in the context of distributed computing on wide area networks can be achieved by compressing data before transmission. However, such an approach is not efficient when dealing with higher speed networks. Indeed, the time to compress a large file and to send it is greater than the time to send the uncompressed file. In this paper, we explore and enhance an algorithm that allows us to overlap communications with compression and to automatically adapt the compression effort to currently available network and processor resources. 1
218|NetSolve: A Network Server for Solving Computational Science Problems|This paper presents a new system, called NetSolve, that allows users to access computational resources, such as hardware and software, distributed across the network. This project has been motivated by the need for an easy-to-use, efficient mechanism for using computational resources remotely. Ease of use is obtained as a result of different interfaces, some of which do not require any programming effort from the user. Good performance is ensured by a load-balancing policy that enables NetSolve to use the computational resource available as efficiently as possible. NetSolve is designed to run on any heterogeneous network and is implemented as a fault-tolerant client-server application. Keywords  Distributed System, Heterogeneity, Load Balancing, Client-Server, Fault Tolerance, Linear Algebra, Virtual Library. University of Tennessee - Technical report No cs-95-313   Department of Computer Science, University of Tennessee, TN 37996  y  Mathematical Science Section, Oak Ridge National La...
219|The Compression Cache: Using On-line Compression to Extend Physical Memory|This paper describes a method for trading off computation for disk or network I/O by using less expensive on-line compression. By using some memory to store data in compressed format, it may be possible to fit the working set of one or more large applications in relatively small memory. For working sets that are too large to fit in memory even when compressed, compression still provides a benefit by reducing bandwidth and space requirements. Overall, the effectiveness of this compression cache depends on application behavior and the relative costs of compression and I/O. Measurements using Sprite on a DECstation  1  5000/200 workstation with a local disk indicate that some memory-intensive applications running with a compression cache can run two to three times faster than on an unmodified system. Better speedups would be expected in a system with a greater disparity between the speed of its processor and the bandwidth to its backing store.  1 Introduction  Over the past decade, the pr...
220|Ninf: A network based information library for global world-wide computing infrastructure|Abstract. Ninf is an ongoing global network-wide computing infrastructure project which allows users to access computational resources including hardware, software and scientific data distributed across a wide area network. Ninf is intended not only to exploit high performance in network parallel computing, but also to provide high quality numerical computation services and accesses to scientific database published by other researchers. Computational resources are shared as Ninf remote libraries executable at a remote Ninf server. Users can build an application by calling the libraries with the Ninf Remote Procedure Call, which is designed to provide a programming interface similar to conventional function calls in existing languages, and is tailored for scientific computation. In order to facilitate location transparency and network-wide parallelism, Ninf metaserver maintains global resource information regarding computational server and databases, allocating and scheduling coarse-grained computation for global load balancing. Ninf also interfaces with the WWW browsers for easy accessibility. 1
221|Operating System Support for Protocol Boosters|&#034;Protocol Boosters&#034; are modules inserted into protocol graphs. They allow the protocol&#039;s behavior to adapt to its environment. Boosters can mask undesirable properties of links or subnets in an internetwork. The method permits use of proprietary protocols and supports end-to-end optimizations. We have implemented Protocol Boosters support in the FreeBSD version of UNIX for Intel architecture machines. Our prototype embeds boosters in the 4.4 BSD-Lite Internet Protocol (IP) stack. We have measured the performance of two prototype boosters: an encryption booster (for passage across insecure subnets) and a compression booster (for passage across bandwidth-impaired subnets). Our measurement data suggests that OS support for this method can be constructed with low performance overhead; execution of the protocol elements dominates any overhead introduced by our implementation. We discuss some lessons learned from the implementation. 1 Introduction  Network protocols are designed to meet appl...
222|Optimal Grain Size Computation for Pipelined Algorithms|. In this paper, we present a method for overlapping communications on parallel computers for pipelined algorithms. We first introduce a general theoretical model which leads to a generic computation scheme for the optimal packet size. Then, we use the OPIUM  3  library, which provides an easy-to-use and efficient way to compute, in the general case, this optimal packet size, on the column LU factorization; the implementation and performance measures are made on an Intel Paragon.  Keywords : Communications overlap, pipelined algorithms, optimal packet size computation. 1 Introduction  Parallel distributed memory machines improve performances and memory capacity but their use adds an overhead due to the communications. To obtain programs that perform and scale well, this overhead must be hidden. Several solutions exist. The choice of a good data distribution is the first step that can be done to lower the number and the size of communications. Depending of the dependences within the cod...
223|Improving application performance through swap compression|Rights to individual papers remain with the author or the author&#039;s employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein. For more information about the USENIX Association:
224|Architectures for Application Transparent Proxies: A Study of Network Enhancing Software|Proxies, software deployed inside the network, play a fundamental role in the Internet by providing enhanced functionality to the network. Deployment of proxies is a flexible way of extending the Internet architecture with new services and to cope with problems that were not foreseen at the time the original Internet protocols were defined. The creation of the Internet is an enormous investment in time, effort and money, and proxies allow us to build on the existing infrastructure to enhance its functionality, rather than replace it. As the use of proxies increase, so does the problem of proxy configuration and deployment, especially with respect to interference between different proxies. With a limited number of different proxies, this problem can be dealt with manually, or by encoding knowledge of interfering proxies into each proxy. As the number of proxies grow, methods to automatically detect and cope with conflicts must be devised. Therefore, proxies need to coordinate with each other. Towards this end, a signalling protocol that can be used to establish and configure a sequence of one or more proxies along an end-to-end flow is proposed. The protocol is realized as an extension of IP, using an IP option, which simplifies its deployment in the Internet. In order to facilitate reasoning about coordination, conflicts and deployment of proxies, a model has been developed. The model is based on the concept of regions, also developed in the thesis. Regions are interconnected parts of the network that share a common property, e.g. administrative control or error characteristic. Along with the model, a classification of proxy architectures with respect to how they gather information and deploy proxies is presented. A method based on this model is also proposed. We also propose an algorithm for controlling compression to maximize perceived throughput in situations where available bandwidth and CPU power varies. Along with the algorithm, experimental results that show that the algorithm approximates the best non-adaptive choice in a number of situations are presented. This algorithm has been implemented as an end-to-end enhancement.
225|Data Compression By Unsupervised Classification|: This paper deals with a general class of classification methods which are related both to vector quantization in the sense of Pollard, [12], as well as to competitive learning in the sense of Kohonen, [10]. The basic duality of minimum variance partitioning and vector quantization known from statistical cluster analysis is shown to be true for this whole class of classification problems. The paper contains theoretical results like existence of optima, consistency of approximate optima and characterization of local optima as fixpoints of a fix point algorithm. A fix point algorithm is proposed and its termination after finite time is proved for empirical distributions. The construction of a particular classification method is based on a statistical information measure specified by a convex function. Modifying this convex function gives room for suggesting a large variety of new classification procedures, e.g. of robust quantifiers. 1 Introduction  1.1 Some Background  Data compression...
228|Regularization and variable selection via the Elastic Net|Summary. We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.
229|On Model Selection Consistency of Lasso|Sparsity or parsimony of statistical models is crucial for their proper interpretations, as  in sciences and social sciences. Model selection is a commonly used method to find such  models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani,  1996) is now being used as a computationally feasible alternative to model selection.
230|Leave-One-Out Support Vector Machines|We present a new learning algorithm for pattern recognition inspired by a recent upper bound on leave--one--out error [ Jaakkola and Haussler, 1999 ] proved for Support Vector Machines (SVMs) [ Vapnik, 1995; 1998 ] . The new approach directly minimizes the expression given by the bound in an attempt to minimize leave--one--out error. This gives a convex optimization problem which constructs a sparse linear classifier in feature space using the kernel technique. As such the algorithm possesses many of the same properties as SVMs. The main novelty of the algorithm is that apart from the choice of kernel, it is parameterless -- the selection of the number of training errors is inherent in the algorithm and not chosen by an extra free parameter as in SVMs. First experiments using the method on benchmark datasets from the UCI repository show results similar to SVMs which have been tuned to have the best choice of parameter.  1 Introduction  Support Vector Machines (SVMs), motivated by minim...
231|Sparse Principal Component Analysis|Principal component analysis (PCA) is widely used in data processing and dimensionality  reduction. However, PCA su#ers from the fact that each principal component is a linear combination  of all the original variables, thus it is often di#cult to interpret the results. We introduce  a new method called sparse principal component analysis (SPCA) using the lasso (elastic net)  to produce modified principal components with sparse loadings. We show that PCA can be  formulated as a regression-type optimization problem, then sparse loadings are obtained by imposing  the lasso (elastic net) constraint on the regression coe#cients. E#cient algorithms are  proposed to realize SPCA for both regular multivariate data and gene expression arrays. We  also give a new formula to compute the total variance of modified principal components. As  illustrations, SPCA is applied to real and simulated data, and the results are encouraging.
232|Boosting with early stopping: convergence and consistency|Abstract Boosting is one of the most significant advances in machine learning for classification and regression. In its original and computationally flexible version, boosting seeks to minimize empirically a loss function in a greedy fashion. The resulted estimator takes an additive function form and is built iteratively by applying a base estimator (or learner) to updated samples depending on the previous iterations. An unusual regularization technique, early stopping, is employed based on CV or a test set. This paper studies numerical convergence, consistency, and statistical rates of convergence of boosting with early stopping, when it is carried out over the linear span of a family of basis functions. For general loss functions, we prove the convergence of boosting&#039;s greedy optimization to the infinimum of the loss function over the linear span. Using the numerical convergence result, we find early stopping strategies under which boosting is shown to be consistent based on iid samples, and we obtain bounds on the rates of convergence for boosting estimators. Simulation studies are also presented to illustrate the relevance of our theoretical results for providing insights to practical aspects of boosting. As a side product, these results also reveal the importance of restricting the greedy search step sizes, as known in practice through the works of Friedman and others. Moreover, our results lead to a rigorous proof that for a linearly separable problem, AdaBoost with ffl! 0 stepsize becomes an L1-margin maximizer when left to run to convergence. 1 Introduction In this paper we consider boosting algorithms for classification and regression. These algorithms present one of the major progresses in machine learning. In their original version, the computational aspect is explicitly specified as part of the estimator/algorithm. That is, the empirical minimization of an appropriate loss function is carried out in a greedy fashion, which means that at each step, a basis function that leads to the largest reduction of empirical risk is added into the estimator. This specification distinguishes boosting from other statistical procedures which are defined by an empirical minimization of a loss function without the numerical optimization details.
233|Empty alternation|Abstract. We introduce the notion of empty alternation by investigating alternating automata which are restricted to empty their storage except for a logarithmically space-bounded tape before making an alternating transition. In particular, we consider the cases when the depth of alternation is bounded by a constant or a polylogarithmic function. In this way we get new characterizations of the classes AC k, SAC k and P using a push-down store and new characterizations of the class T P 2 using Turing tapes. 1
234|Orthogonal pyramid transforms for image coding|We describe a set of pyramid transforms that decompose an image into a set of basis functions that are (a) spatial-frequency tuned, (b) orientation tuned, (c) spatially localized, and (d) self-similar. For computational reasons the set is also (e) orthogonal and lends itself to (f) rapid computation. The systems are derived from concepts in matrix algebra, but are closely connected to decompositions based on quadrature mirror filters. Our computations take place hierarchically, leading to a pyramid representation in which all of the basis functions have the same basic shape, and appear at many scales. By placing the high-pass and low-pass kernels on staggered grids, we can derive odd-tap QMF kernels that are quite compact. We have developed pyramids using separable, quincunx, and hexagonal kernels. Image data compression with the pyramids gives excellent results, both in terms of MSE and
235|Data compression on the sphere|Received —; accepted — Large data-sets defined on the sphere arise in many fields. In particular, recent and forthcoming observations of the anisotropies of the cosmic microwave background (CMB) made on the celestial sphere contain approximately three and fifty mega-pixels respectively. The compression of such data is therefore becoming increasingly important. We develop algorithms to compress data defined on the sphere. A Haar wavelet transform on the sphere is used as an energy compression stage to reduce the entropy of the data, followed by Huffman and run-length encoding stages. Lossless and lossy compression algorithms are developed. We evaluate compression performance on simulated CMB data, Earth topography data and environmental illumination maps used in computer graphics. The CMB data can be compressed to approximately 40 % of its original size for essentially no loss to the cosmological information content of the data, and to approximately 20 % if a small cosmological information loss is tolerated. For the topographic and illumination data compression ratios of approximately 40:1 can be achieved when a small degradation in quality is allowed. We make our SZIP program that implements these compression algorithms available publicly. Key words. methods: numerical – cosmology: cosmic background radiation
236|Lossless and Lossy Data Compression |Data compression (or source coding) is the process of creating binary representations of data which require less storage space than the original data  [7; 14; 15]. Lossless compression is used where perfect reproduction is required while lossy compression is used where perfect reproduction is not possible or requires too many bits. Achieving optimal compression with respect to resource constraints is a difficult problem. For instance, in lossless compression, it has been shown to be NP-complete [13]. In this paper, we present genetic algorithms for performing lossless and lossy compressions respectively on text data and Gaussian-Markov sources. 1 Introduction  Finding the optimal way to compress data with respect to resource constraints remains one of the most challenging problems in the field of source coding. As genetic algorithms [5, 8] are becoming a widely-used and accepted method for very difficult problems [2], we present a variety of genetic algorithms for performing both lossl...
237|Least squares quantization in pcm|Abstract-It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quantization schemes for 26 quanta, b = 1,2, t,7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes. T I.
238|Using Genetic Algorithms to Solve NP-Complete Problems|A strategy for using Genetic Algorithms (GAs) to solve NP-complete problems is presented. The key aspect of the approach taken is to exploit the observation that, although all NP-complete problems are equally difficult in a general computational sense, some have much better GA representations than others, leading to much more successful use of GAs on some NP-complete problems than on others. Since any NP-complete problem can be mapped into any other one in polynomial time, the strategy described here consists of identifying a canonical NP-complete problem on which GAs work well, and solving other NP-complete problems indirectly by mapping them onto the canonical problem. Initial empirical results are presented which support the claim that the Boolean Satisfiability Problem (SAT) is a GAeffective canonical problem, and that other NPcomplete problems with poor GA representations can be solved efficiently by mapping them first onto SAT problems.  
239|An Evolutionary Approach to Vector Quantizer Design|Vector quantization is a lossy coding technique for encoding a set of vectors from different sources such as image and speech. The design of vector quantizers that yields the lowest distortion is one of the most challenging problems in the field of source coding. However, this problem is known to be difficult [3]. The conventional solution technique works through a process of iterative refinements which yield only locally optimal results. In this paper, we design and evaluate three versions of genetic algorithms for computing vector quantizers. Our preliminary study with Gaussian-Markov sources showed that the genetic approach outperforms the conventional technique in most cases. 1. Introduction  Vector Quantization (VQ) is a lossy source coding technique that maps a sequence of continuous or discrete k-dimensional vectors into a digital sequence suitable for communication over or storage in a digital channel [2, 5, 11]. The goal is data compression: to reduce the bit rate so as to min...
240|Physics and Image Data Compression|We show how several basic image compression methods (predictive coding, transform coding, and pyramid coding) are based on self-similarity, and a 1/f  2  power law. Phase transitions often show self-similarity which is characterized by a spectral power law. Natural images often show a self-similarity which is also characterized by a power law spectrum which is near 1/f  2  . Exploring physical analogs leads to greater unity among current methods of compression and perhaps lead to improved techniques.  1. INTRODUCTION  Because of limited capacity in transmission and storage a great deal of work has been done on image data compression methods in the past. These methods were first developed by examining pixel statistics, specifically entropy and correlation. Later methods made use of studies of the Human Visual System (HVS) to determine how more lossy coding could be achieved. An important function of animal vision is to represent information as concisely as possible. The goal of image co...
241|Relations between the statistics of natural images and the response properties of cortical cells|The relative efficiency of any particular image-coding scheme should be defined only in relation to the class of images that the code is likely to encounter. To understand the representation of images by the mammalian visual system, it might therefore be useful to consider the statistics of images from the natural environment (i.e., images with trees, rocks, bushes, etc). In this study, various coding schemes are compared in relation to how they represent the information in such natural images. The coefficients of such codes are represented by arrays of mechanisms that respond to local regions of space, spatial frequency, and orientation (Gabor-like transforms). For many classes of image, such codes will not be an efficient means of representing information. However, the results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy (e.g., correlation between the intensities of neighboring pixels) into first-order redundancy (i.e., the response distribution of the coefficients). Such coding produces a relatively high signal-to-noise ratio and permits information to be transmitted with only a subset of the total number of cells. These results support Barlow&#039;s theory that the goal of natural vision is to represent the information in the natural environment with minimal redundancy.
242|Human discrimination of fractal images|Knill et al.
243|Self-Organizing Linear Search|this article. Two examples of simple permutation algorithms are move-to-front, which moves the accessed record to the front of the list, shifting all records previously ahead of it back one position; and transpose, which merely exchanges the accessed record with the one immediately ahead of it in the list. These will be described in more detail later. Knuth [1973] describes several search methods that are usually more efficient than linear search. Bentley and McGeoch [1985] justify the use of self-organizing linear search in the following three contexts:
244|The eyes have it: A task by data type taxonomy for information visualizations| A useful starting point for designing advanced graphical user interjaces is the Visual lnformation-Seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. This paper offers a task by data type taxonomy with seven data types (one-, two-, three-dimensional datu, temporal and multi-dimensional data, and tree and network data) and seven tasks (overview, Zoom, filter, details-on-demand, relate, history, and extracts). 
245|Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Displays|This paper offers new principles for visual information seeking (VIS). A key concept is to support browsing, which is distinguished from familiar query composition and information retrieval because of its emphasis on rapid filtering to reduce result sets, progressive refinement of search parameters, continuous reformulation of goals, and visual scanning to identify results. VIS principles developed include: dynamic query filters (query parameters are rapidly adjusted with sliders, buttons, maps, etc.), starfield displays (two-dimensional scatterplots to structure result sets and zooming to reduce clutter), and tight coupling (interrelating query components to preserve display invariants and support progressive refinement combined with an emphasis on using search output to foster search input). A FilmFinder prototype using a movie database demonstrates these principles in a VIS environment.
246|Tree visualization with Tree-maps: A 2-d space-filling approach|this paper deals with a two-dimensional (2-d) space-filling approach in which each node is a rectangle whose area is proportional to some attribute such as node size. Research on relationships between 2-d images and their representation in tree structures has focussed on node and link representations of 2-d images. This work includes quad-trees (Samet, 1989) and their variants which are important in image processing. The goal of quad trees is to provide a tree representation for storage compression and efficient operations on bit-mapped images. XY-trees (Nagy &amp; Seth, 1984) are a traditional tree representation of two-dimensional layouts found in newspaper, magazine, or book pages. Related concepts include k-d trees (Bentley and Freidman, 1979), which are often explained with the help of a
247|Treemaps: a space-filling approach to the visualization of hierarchical information structures|This paper describes a novel methodfor the visualization of hierarchically structured information. The Tree-Map visualization technique makes 100 % use of the available display space, mapping the full hierarchy onto a rectangular region in a space-filling manner. This efficient use of space allows very large hierarchies to be displayed in their entirety and facilitates the presentation of semantic information. 1
248|The Table Lens: Merging Graphical and Symbolic Representations in an Interactive Focus+Context Visualization for Tabular Information|We present a new visualization, called the Table Lens, for visualizing and making sense of large tables. The visualization uses a focus context (fisheye) technique that works effectively on tabular information because it allows display of crucial label information and multiple distal focal areas. In addition, a graphical mapping scheme for depicting table contents has been developed for the most widespread kind of tables, the cases-by-variables table. The Table Lens fuses symbolic and graphical representations into a single coherent view that can be fluidly adjusted by the user. This fusion and interactivity enables an extremely rich and natural style of direct manipulation exploratory data analysis.
249|Dynamic Queries for Information Exploration: An Implementation and Evaluation|We designed, implemented and evaluated a new concept for direct manipulation of databases, called dynamic queries, that allows users to formulate queries with graphical widgets, such as sliders. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. Eighteen undergraduate chemistry students performed statistically significantly faster usingadynamicqueries interface compared to two interfaces both providing form fillin as input method, one with graphical visualization output and one with all-textual output. The interfaces were used to expore the periodic table of elements and search on their properties. 1. INTRODUCTION Mostdatabasesystems require the user to create andformulate a complex query, whichpresumes that the user is familiar with the logical structure of the database [4]. The queries on a database are usually expressed in high level query languages (such as SQL,QUEL). This works well for many applications, but it ...
250|Visualizing the non-visual: spatial analysis and interaction with information from test documents|This paper describes an approach to IV that involves spatializing text content for enhanced visual browsing and analysis. The application arena is large text document corpora such as digital libraries, regulations andprocedures, archived reports, etc. The basic idea is that text content from these sources may be transformed to a spatial representation that preserves informational characteristics from the documents. The spatial representation may then be visually browsed and analyzed in ways that avoid language processing and that reduce the analysts’ mental workload. The result is an interaction with text that more nearly resembles perception and action with the natural world than with the abstractions of written language. 1
251|Dynamic Queries for Visual Information Seeking|Dynamic queries are a novel approach to information seeking that may enable users to cope with information overload. They allow users to see an overview of the database, rapidly (100 msec updates) explore and conveniently filter out unwanted information. Users fly through information spaces by incrementally adjusting a query (with sliders, buttons, and other filters) while continuously viewing the changing results. Dynamic queries on the chemical table of elements, computer directories, and a real estate database were built and tested in three separate exploratory experiments. These results show statistically significant performance improvements and user enthusiasm more commonly seen with video games. Widespread application seems possible but research issues remain in database and display algorithms, and user interface design. Challenges include methods for rapidly displaying and changing many points, colors, and areas; multidimensional pointing; incorporation of sound and visual displ...
252|LifeLines: Visualizing Personal Histories|LifeLines provide a general visualization environment for personal histories that can be applied to medical and court records, professional histories and other types of biographical data. A one screen overview shows multiple facets of the records. Aspects, for example medical conditions or legal cases, are displayed as individual time lines, while icons indicate discrete events, such as physician consultations or legal reviews. Line color and thickness illustrate relationships or significance, rescaling tools and filters allow users to focus on part of the information. LifeLines reduce the chances of missing information, facilitate spotting anomalies and trends, streamline access to details, while remaining tailorable and easily transferable between applications. The paper describes the use of LifeLines for youth records of the Maryland Department of Juvenile Justice and also for medical records. User&#039;s feedback was collected using a Visual Basic prototype for the youth record. Techniq...
253|Graphical Fisheye Views|A fisheye camera lens is a very wide angle lens that magnifies nearby objects while shrinking distant objects. It is a valuable tool for seeing both &#034;local detail&#034; and &#034;global context&#034; simultaneously. This paper describes a system for viewing and browsing graphs using a software analog of a fisheye lens. We first show how to implement such a view using solely geometric transformations. We then describe a more general transformation that allows global information about the graph to affect the view. Our general transformation is a fundamental extension to previous research in fisheye views.  
254|The dynamic HomeFinder: evaluating dynamic queries in a real-estate information exploration system|We designed, implemented, and evaluated a new concept for visualizing and searching databases utilizing direct manipulation called dynarruc queries. Dynamic queries allow users to formulate queries by adjusting graphical widgets, such as sliders, and see the results immediately. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. User testing was done with eighteen undergraduate students who performed significantly faster using a dynamic queries interface compared to both a natural language system and paper printouts. The interfaces were used to explore a real-estate database and find homes meeting specific search criteria. 1
255|InfoCrystal: a visual tool for information retrieval|This paper introduces a novel representation, called the I n f o C r y s t a l T M,  that can be used as a v isual i za t ion tool as well as a visual query lan-g u a g e to help users search for information. The lnfoCrysta1 visualizes all the possible relation-ships among N concepts. Users can assign relevance weights to the concepts and use thresholding to select relationships of interest. The lnfocrystal allows users to specify Boolean as well as v e c t o r-s p a c e queries graphically. Arbitrarily complex queries can be created by using the 1nfoCrystals as building blocks and organizing them in a hierarchical structure. The 1nfoCrystal enables users to explore and filter information in a flexible, dynamic and interactive way.
256|IVEE: An Information Visualization &amp; Exploration Environment|The Information Visualization and Exploration Environment (IVEE) is a system for automatic creation of dynamic queries applications. IVEE imports database relations and automatically creates environments holding visualizations and query devices. IVEE offers multiple visualizations such as maps and starfields, and multiple query devices, such as sliders, alphasliders, and toggles. Arbitrary graphical objects can be attached to database objects in visualizations. Multiple visualizations may be active simultaneously. Users can interactively lay out and change between types of query devices. Users may retrieve details-on-demand by clicking on visualization objects. An HTML file may be provided along with the database, specifying how details-ondemand information should be presented, allowing for presentation of multimedia information in database objects. Finally, multiple IVEE clients running on separate workstations on a network can communicate by letting one users actions affect the visua...
257|The Information Mural: A Technique for Displaying and Navigating Large Information Spaces|Abstract—Information visualizations must allow users to browse information spaces and focus quickly on items of interest. Being able to see some representation of the entire information space provides an initial gestalt overview and gives context to support browsing and search tasks. However, the limited number of pixels on the screen constrain the information bandwidth and make it difficult to completely display large information spaces. The Information Mural is a two-dimensional, reduced representation of an entire information space that fits entirely within a display window or screen. The Mural creates a miniature version of the information space using visual attributes, such as gray-scale shading, intensity, color, and pixel size, along with antialiased compression techniques. Information Murals can be used as stand-alone visualizations or in global navigational views. We have built several prototypes to demonstrate the use of Information Murals in visualization applications; subject matter for these views includes computer software, scientific data, text documents, and geographic information. Index Terms—Information visualization, software visualization, data visualization, focus+context, navigation, browsers. 1 INFORMATION MURALS
258|Visdb: Database exploration using multidimensional visualization|In this paper we describe the VisDB system, which allows an exploration of large databases using visualization techniques. The goal of the system is to support the query specification process by using each pixel of the display to represent one data item of the database. By arranging and coloring the pixels according to the relevance of the data items with respect to the query, the user gets a visual impression of the resulting data set. Using sliders for each condition of the query, the user may change the query dynamically and receives immediate feedback from the visual representation of the resulting data set. Different visualization techniques are available for different stages of exploration. The first technique uses multiple windows for the different query parts, providing visual feedback for each part of the query and helping the user to understand the overall result. The second technique is an extension of the first one, providing additional information by assigning two dimensions to the axes. The third technique uses a grouping of dimensions and is designed to support a focused search on smaller data sets.
259|The Alphaslider: A Compact and Rapid Selector|Research has suggested that rapid, serial, visual presentation of text (RSVP) may be an effective way to scan and search through lists of text strings in search of words, names, etc. The Alphaslider widget employs RSVP as a method for rapidly scanning and searching lists or menus in a graphical user interface environment. The Alphaslider only uses an area less than 7 cm x 2.5 cm. The tiny size of the Alphaslider allows it to be placed on a credit card, on a control panel for a VCR, or as a widget in a direct manipulation based database interface. An experiment was conducted with four Alphaslider designs which showed that novice Alphaslider users could locate one item in a list of 10,000 film titles in 24 seconds on average, an expert user in about 13 seconds.  KEYWORDS: Alphaslider, widget, selection technology, menus, dynamic queries  INTRODUCTION  Selecting items from lists is a common task in today&#039;s society. New and exciting applications for selection technology are credit card siz...
260|The Continuous Zoom: A Constrained Fisheye Technique for Viewing and Navigating Large Information Spaces|Navigating and viewing large information spaces, such as hierarchically-organized networks from complex realtime systems, suffer the problems of viewing a large space on a small screen. Distorted-view approaches, such as fisheye techniques, have great potential to reduce these problems by representing detail within its larger context but introduce new issues of focus, transition between views and user disorientation from excessive distortion. We present a fisheyebased method which supports multiple focus points, enhances continuity through smooth transitions between views, and maintains location constraints to reduce the user’s sense of spatial disorientation. These are important requirements for the representation and navigation of networked systems in supervisory control applications. The method consists of two steps: a global allocation of space to rectangular sections of the display, based on scale factors, followed by degree-of-interest adjustments. Previous versions of the algorithm relied solely on relative scale factors to assign size; we present a new version which allocates space more efficiently using a dynamically calculated degree of interest. In addition to the automatic system sizing, manual user control over the amount of space assigned each area is supported. The amount of detail shown in various parts of the network is controlled by pruning the hierarchy and presenting those sections in summary form. KEYWORDS: graphical user interface, supervisory control systems, information space, hierarchical network, information visualization, fisheye view, navigation.
261|Enhanced Dynamic Queries via Movable Filters|Traditional database query systems allow users to construct complicated database queries from specialized database language primitives. While powerful and expressive, such systems are not easy to use, especially for browsing or exploring the data. Information visualization systems address this problem by providing graphical presentations of the data and direct manipulation tools for exploring the data. Recent work in this area has reported the value of dynamic queries coupled with two-dimensional data representations for progressive refinement of user queries. However, the queries generated by these systems are limited to conjunctions of global ranges of parameter values. In this paper, we extend dynamic queries by encoding each operand of the query as a Magic Lens filter. Compound queries can be constructed by overlapping the lenses. Each lens includes a slider and a set of buttons to control the value of the filter function and to define the compostion operation generated by overlapp...
262|Navigating Large Networks with Hierarchies|This paper is aimed at the exploratory visualization of networks where there is a strength or weight associated with each link, and makes use of any hierarchy present on the nodes to aid the investigation of large networks. It describes a method of placing nodes on the plane that gives meaning to their relative positions. The paper discusses how linking and interaction principles aid the user in the exploration. Two examples are given; one of electronic mail communication over eight months within a department, another concerned with changes to a large section of a computer program. I. THE PROBLEM It has almost become a clichŽ to start a paper with the observation that the amount of data in the world is growing rapidly, and that current efforts to extract useful information from data lag far behind the ability to create data. However the clichŽ is true, and no less so in the field of network analysis and visualization than in any other. In many areas, scientists are realizing that the tools they have been using are limited in utility when applied to large, information-rich networks. Not only are networks of interest large in terms of size (as measured by number of nodes or links between nodes), but also in terms of the data collected for each node or link. The ability to examine statistics on the nodes and relate them to the network is of crucial importance. Examples of areas in which the analysis of large networks is important include: i. Trade flows. The concern in this area is monitoring imports and exports of various products at several levels; international, interstate and local. Besides examining many types of trade goods, there is also strong interest in spotting temporal patterns. ii. Communication networks. This is an important and wide category, covering not only telecommunication networks, but also electronic mail (email), financial transaction, ATM/bank data transferal and other data distribution networks.
263|Using Aggregation and Dynamic Queries for Exploring Large Data Sets|When working with large data sets, users perform three primary types of activities: data manipulation, data analysis, and data visualization. The data manipulation process involves the selection and transformation of data prior to viewing. This paper addresses user goals for this process and the interactive interface mechanisms that support them. We consider three classes of data manipulation goals: controlling the scope (selecting the desired portion of the data), selecting the focus of attention (concentrating on the attributes of data that are relevant to current analysis), and choosing the level of detail (creating and decomposing aggregates of data). We use this classification to evaluate the functionality of existing data exploration interface techniques. Based on these results, we have expanded an interface mechanism called the Aggregate Manipulator (AM) and combined it with Dynamic Query (DQ) to provide complete coverage of the data manipulation goals. We use real estate sales data to demonstrate how the AM and DQ synergistically function in our interface.
264|Interacting with Huge Hierarchies: Beyond Cone Trees|This paper describes an implementation of a tool for visualizing and interacting with huge information hierarchies. Existing systems for visualizing huge hierarchies using cone trees &#034;break down&#034; once the hierarchy to be displayed exceeds roughly 1000 nodes, due to increasing visual clutter. This paper describes a system called fsviz which visualizes arbitrarily large hierarchies while retaining user control. This is accomplished by augmenting cone trees with several graphical and interaction techniques: usage-based filtering, animated zooming, handcoupled rotation, fish-eye zooming, coalescing of nodes, texturing, effective use of colour for depth cueing, and the applications of dynamic queries. The fsviz system also improves upon earlier cone tree visualization systems through a more elaborate node layout algorithm. This algorithm enhances the usefulness of cone tree visualization for large hierarchies by all but eliminating clutter.  Keywords: Information Visualization, Information ...
265|Using treemaps to visualize the Analytic Hierarchy Process|this article. References
266|Exploratory Access to Geographic Data Based on the Map-Overlay Metaphor|Many geographic information systems (GISs) attempt to imitate the manual process of laying transparent map layers over one another on a light table and analyzing the resulting configurations. While this map-overlay metaphor, familiar to many geo-scientists, has been used as a design principle for the underlying architecture of GISs, it has not yet been visually manifested at the user interface. To overcome this shortage, a new direct manipulation user interface for overlay-based GISs has been designed and prototyped. It is characterized by the separation of map layers into data cubes and map templates such that different thematic data can be combined and the same kind of data can be displayed in different formats. This paper introduces the conceptual objects that the user manipulates at the screen surface and discusses ways to visualize effectively the objects and operations upon them.
267|Visualizing Network Data|Networks are critical to modern society, and a thorough understanding of how they behave is crucial to their efficient operation. Fortunately, data on networks is plentiful; by visualizing this data, it is possible to greatly improve our understanding. Our focus is on visualizing the data associated with a network and not on simply visualizing the structure of the network itself. We begin with three static network displays; two of these use geographical relationships, while the third is a matrix arrangement that gives equal emphasis to all network links. Static displays can be swamped with large amounts of data; hence we introduce directmanipulation techniques that permit the graphs to continue to reveal relationships in the context of much more data. In effect, the static displays are parameterized so that interesting views may easily be discovered interactively. The software to carry out this network visualization is called SeeNet.  1. INTRODUCTION  We are currently in the midst of a...
268|Reconciling Data Compression and . . . |  While data compression and Kolmogorov complexity are both about effective coding of words, the two settings differ in the following respect. A compression algorithm or compressor, for short, has to map a word to a unique code for this word in one shot, whereas with the standard notions of Kolmogorov complexity a word has many different codes and the minimum code for a given word cannot be found effectively. This gap is bridged by introducing decidable Turing machines and a corresponding notion of Kolmogorov complexity, where compressors and suitably normalized decidable machines are essentially the same concept. Kolmogorov complexity defined via decidable machines yields characterizations in terms of the intial segment complexity of sequences of the concepts of Martin-Löf randomness, Schnorr randomness, Kurtz randomness, and computable dimension. These results can also be reformulated in terms of time-bounded Kolmogorov complexity. Other applications of decidable machines are presented, such as a simplified proof of the Miller-Yu theorem (characterizing Martin-Löf randomness by the plain complexity of the initial segments) and a new characterization of computably traceable sequences via a natural lowness notion for decidable machines.
269|Lowness Properties and Randomness |The set A is low for Martin-Lof random if each random set is  already random relative to A. A is K-trivial if the prefix complexity K of each  initial segment of A is minimal, namely      K(n)+O(1). We show  that these classes coincide. This implies answers to questions of Ambos-Spies  and Kucera [2], showing that each low for Martin-Lof random set is #  2 . Our  class induces a natural intermediate #  3 ideal in the r.e. Turing degrees (which  generates the whole class under downward closure). Answering
270|Calibrating randomness |2. Sets, measure, and martingales 4 2.1. Sets and measure 4 2.2. Martingales 5
271|On initial segment complexity and degrees of randomness |Abstract. One approach to understanding the fine structure of initial segment complexity was introduced by Downey, Hirschfeldt and LaForte. They define X =K Y to mean that (?n) K(X ? n)  = K(Y ? n) +O(1). The equivalence classes under this relation are the K-degrees. We prove that if X ? Y is 1-random, then X and Y have no upper bound in the K-degrees (hence, no join). We also prove that n-randomness is closed upward in the K-degrees. Our main tool is another structure intended to measure the degree of randomness of real numbers: the vL-degrees. Unlike the K-degrees, many basic properties of the vL-degrees are easy to prove. We show that X =K Y implies X =vL Y, so some results can be transferred. The reverse implication is proved to fail. The same analysis is also done for =C, the analogue of =K for plain Kolmogorov complexity. Two other interesting results are included. First, we prove that for any Z ? 2?, a 1-random real computable from a 1-Z-random real is automatically 1-Z-random. Second, we give a plain Kolmogorov complexity characterization of 1-randomness. This characterization is related to our proof that X =C Y implies X =vL Y. 1.
272|Computational Randomness and Lowness|. We prove that there are uncountably many sets that are low for the class of Schnorr random reals. We give a purely recursion theoretic characterization of these sets and show that they all have Turing degree incomparable to 0  0  . This contrasts with a result of Kucera and Terwijn [5] on sets that are low for the class of Martin-Lof random reals. The Cantor space 2  !  is the set of infinite binary sequences; these are called  reals and are identified with subsets of !. If oe 2 2  !!  , that is, oe is a finite binary sequence, we denote by [oe] the set of reals that extend oe. These form a basis of clopen sets for the usual discrete topology on 2  !  . Write joej for the length of oe 2 2  !!  . The Lebesgue measure  on 2  !  is defined by stipulating that [oe] = 2  \Gammajoej  . With every set U ` 2  !!  we associate the open set  S  oe2U [oe]. When it is convenient, we confuse U with the open set associated to it, in particular we write U for the measure of the open set correspondi...
274|Recursive computational depth|In the 1980&#039;s, Bennett introduced computational depth as a formal measure of the amount of computational history that is evident in an object&#039;s structure. In particular, Bennett identi ed the classes of weakly deep and strongly deep sequences, and showed that the halting problem is strongly deep. Juedes, Lathrop, and Lutz subsequently extended this result by de ning the class of weakly useful sequences, and proving that every weakly useful sequence is strongly deep. The present paper investigates re nements of Bennett&#039;s notions of weak and strong depth, called recursively weak depth (introduced by Fenner, Lutz and Mayordomo) and recursively strong depth (introduced here). It is argued that these re nements naturally capture Bennett&#039;s idea that deep objects are those which \contain internal evidence of a nontrivial causal history. &amp;quot; The fundamental properties of recursive computational depth are developed, and it is shown that the recursively weakly (respectively, strongly) deep sequences form a proper subclass of the class of weakly (respectively, strongly) deep sequences. The above-mentioned theorem of Juedes, Lathrop, and Lutz is then strengthened by proving that every weakly useful sequence is recursively strongly deep. It follows from these results that not every strongly deep sequence is weakly useful, thereby answering a question posed by Juedes.
275|Lowness for the class of Schnorr random reals|We answer a question of Ambos-Spies and Kucera in the affirmative. They asked whether, when a real is low for Schnorr randomness, it is already low for Schnorr tests.
276|Lowness for weakly 1-generic and Kurtz-random|Abstract. We prove that a set is low for weakly 1-generic iff it has neither dnr nor hyperimmune Turing degree. As this notion is more general than being recursively traceable, we refute a recent conjecture on the characterization of these sets. Furthermore, we show that every set which is low for weakly 1-generic is also low for Kurtz-random. 1
277|Why Computational Complexity Requires Stricter Martingales | The word &amp;quot;martingale &amp;quot; has related, but different, meanings in probability theory and theoretical computer science. In computational complexity and algorithmic information theory, a martingale is typically a function d on strings such that E(d(wb)|w)  = d(w) for all strings w, where the conditional expectation is computed over all possible values of the next symbol b. In modern probability theory a martingale is typically a sequence,0,,1,,2,... of random variables such that E(,n+1|,0,...,,n)  =,n for all n.
278|Schnorr dimension| Following Lutz’s approach to effective (constructive) dimension, we define a notion of dimension for individual sequences based on Schnorr’s concept(s) of randomness. In contrast to computable randomness and Schnorr randomness, the dimension concepts defined via computable martingales and Schnorr tests coincide, i.e. the Schnorr Hausdorff dimension of a sequence always equals its computable Hausdorff dimension. Furthermore, we give a machine characterization of Schnorr dimension, based on prefixfree machines whose domain has computable measure. Finally, we show that there exist computably enumerable sets which are Schnorr (computably) irregular: while every c.e. set has Schnorr Hausdorff dimension 0 there are c.e. sets of computable packing dimension 1, a property impossible in the case of effective (constructive) dimension, due to Barzdin?s’ Theorem. In fact, we prove that every hyperimmune Turing degree contains a set of computable packing dimension 1.
279|Complete discrete 2-D Gabor transforms by neural networks for image analysis and compression|Abstract-A three-layered neural network is described for transforming two-dimensional discrete signals into generalized nonorthogonal 2-D “Gabor ” representations for image analysis, segmentation, and compression. These transforms are conjoint spatiahpectral representations [lo], [15], which provide a complete image description in terms of locally windowed 2-D spectral coordinates embedded within global 2-D spatial coordinates. Because intrinsic redundancies within images are extracted, the resulting image codes can be very compact. However, these conjoint transforms are inherently difficult to compute because t e elementary expansion functions are not orthogonal. One orthogonking approach developed for 1-D signals by Bastiaans [SI, based on biorthonormal expansions, is restricted by constraints on the conjoint sampling rates and invariance of the windowing function, as well as by the fact that the auxiliary orthogonalizing functions are nonlocal infinite series. In the present “neural network ” approach, based
280|Optical Information Processing Based on an Associative-Memory Model of Neural Nets with Thresholding and Feedback|The remarkable collective computational properties of the Hopfield model for neural networks [Proc. Nat. Acad. Sci. USA 79, 2554 (1982)] are reviewed. These include recognition from partial input, robustness, and error-correction capability. Features of the model that make its optical implementation attractive are discussed, and specific optical implementation schemes are given. Optical information-processing systems can have high processing power because of the large degree of parallelism as well as the interconnection capability that is achievable. Typically, more than 106 parallel processing channels are available in the optical system, and furthermore each of these channels can be optically interconnected (broadcasted) to 106 other channels. The majority of optical processors are analog systems, designed to perform linear operations. The accuracy of an analog processor is limited by the linear dynamic range of the devices used (detectors, light modulators).
281|Energy Aware Lossless Data Compression|Wireless transmission of a bit can require over 1000 times more energy than a single 32-bit computation. It would therefore seem desirable to perform significant computation to reduce the number of bits transmitted. If the energy required to compress data is less than the energy required to send it, there is a net energy savings and consequently, a longer battery life for portable computers. This paper reports on the energy of lossless data compressors as measured on a StrongARM SA-110 system. We show that with several typical compression tools, there is a net energy increase when compression is applied before transmission. Reasons for this increase are explained, and hardwareaware programming optimizations are demonstrated. When applied to Unix compress, these optimizations improve energy efficiency by 51%. We also explore the fact that, for many usage models, compression and decompression need not be performed by the same algorithm. By choosing the lowest-energy compressor and decompressor on the test platform, rather than using default levels of compression, overall energy to send compressible web data can be reduced 31%. Energy to send harder-to-compress English text can be reduced 57%. Compared with a system using a single optimized application for both compression and decompression, the asymmetric scheme saves 11% or 12% of the total energy depending on the dataset.
282|A Low-bandwidth Network File System|This paper presents LBFS, a network file system designed for low bandwidth networks. LBFS exploits similarities between files or versions of the same file to save bandwidth. It avoids sending data over the network when the same data can already be found in the server&#039;s file system or the client&#039;s cache. Using this technique, LBFS achieves up to two orders of magnitude reduction in bandwidth utilization on common workloads, compared to traditional network file systems
283|Efficient Algorithms for Sorting and Synchronization|This thesis presents efficient algorithms for internal and external parallel sorting and remote data update. The sorting algorithms approach the problem by concentrating first on highly efficient but incorrect algorithms followed by a cleanup phase that completes the sort. The remote data update algorithm, rsync, operates by exchanging block signature information followed by a simple hash search algorithm for block matching at arbitrary byte boundaries. The last chapter of the thesis examines a number of related algorithms for text compression, differencing and incremental backup.
284|Energy-Efficient Design of Battery-Powered Embedded Systems|Energy-efficient design of battery-powered systems demands optimizations in both hardware and software. We present a modular approach for enhancing instruction level simulators with cycle-accurate simulation of energy dissipation in embedded systems. Our methodology has tightly coupled component models thus making our approach more accurate. Performance and energy computed by our simulator are within a 5% tolerance of hardware measurements on the SmartBadge [2]. We show how the simulation methodology can be used for hardware design exploration aimed at enhancing the SmartBadge with realtime MPEG video feature.  In addition, we present a profiler that relates energy consumption to the source code. Using the profiler we can quickly and easily redesign the MP3 audio decoder software to run in real time on the SmartBadge with low energy consumption. Performance increase of 92% and energy consumption decrease of 77% over the original executable specification have been achieved.  Keywords--- low-power-design, system-level, performancetradeoffs, power-consumption-model  I. 
285|Algorithmic Transforms for Efficient Energy Scalable Computation|We introduce the notion of energy scalable computation on general purpose processors. The principle idea is to maximize computational quality for a given energy constraint. The desirable energy-quality behavior of algorithms is discussed. Subsequently the energy-quality scalability of three distinct categories of commonly used signal processing algorithms (viz. filtering, frequency domain transforms and classification) are analyzed on the StrongARM SA-1100 processor and transformations are described which obtain significant improvements in the energy-quality scalability of the algorithm. I. INTRODUCTION  In embedded systems, energy is a precious resource and must be used efficiently. Therefore, it is highly desirable that we structure our algorithms and systems in such a fashion that computational accuracy can be traded off with energy requirement. At the heart of such transformations lies the concept of incremental refinement [1]. Consider the scenario where an individual is using his...
286|A Unified Header Compression Framework for Low-Bandwidth Links|Compressing protocol headers has traditionally been an attractive way of conserving bandwidth over low-speed links, including those in wireless systems. However, despite the growth in recent years in the number of end-to-end protocols beyond TCP/IP, header compression deployment for these protocols has not kept pace. This is in large part due to complexities in implementation, which often requires a detailed knowledge of kernel internals, and a lack of a common way of pursuing the general problem across a variety of end-to-end protocols. To address this, rather than defining several new protocol-specific standards, we present a unified framework for header compression. This framework includes a simple, platform-independent header description language that protocol implementors can use to describe high-level header properties, and a platform-specific code generation tool that produces kernel source code automatically from this header specification. Together, the high-level description l...
287|Increasing Effective Link Bandwidth by Suppressing Replicated Data|In the Internet today, transfer rates are often limited by the bandwidth of a bottleneck link rather than the computing power available at the ends of the links. To address this problem, we have utilized inexpensive commodity hardware to design a novel link layer caching and compression scheme that reduces bandwidth consumption. Our scheme is motivated by the prevalence of repeated transfers of the same information, as may occur due to HTTP, FTP, and DNS traffic. Unlike existing link compression schemes, it is able to detect and use the long-range correlation of repeated transfers. It also complements application level systems that reduce bandwidth usage, e.g., Web caches, by providing additional protection at a lower level, as well as an alternative in situations where application-level cache deployment is not practical or economic. We make three contributions in this paper. First, to motivate our scheme we show by packet trace analysis that there is significant replication of data at the pack...
288|Source Code Optimization and Profiling of Energy Consumption in Embedded Systems|This paper presents a source code optimization methodology and a profiling tool that have been developed to help designers in optimizing software performance and energy in embedded systems. Code optimizations are applied at three levels of abstraction: algorithmic, data and instruction-level. The profiler exploits a cycle-accurate energy consumption simulator [3] to relate the embedded system energy consumption and performance to the source code. Thus, it can be used for analysis (i.e., to find energy-critical sections of the code), and for validation (i.e., to assess the impact of each code optimization) .  Code optimizations and profiling tool are used to optimize and tune the implementation of an MPEG Layer III (MP3) audio decoder for the SmartBadge [2] portable embedded system. We show that using our methodology and tool we can quickly and easily redesign the MP3 audio decoder software to run in real time with low energy consumption. Performance increase of 92% and energy consumpti...
289|Low Power Embedded Software Optimization using Symbolic Algebra|The market demand for portable multimedia applications has exploded in the recent years. Unfortunately, for such applications current compilers and software optimization methods often require designers to do part of the optimization manually. Specifically, the high-level arithmetic optimizations and the use of complex  instructions are left to the designers&#039; ingenuity. In this  paper, we present a tool flow, SymSoft, that automates the optimization of power-intensive algorithmic constructs using symbolic algebra techniques combined with energy  profiling. SymSoft is used to optimize and tune the algorithmic level description of an MPEG Layer III (MP3) audio decoder for the SmartBadge [2] portable embedded system. We show that our tool lowers the number of instructions and memory accesses and thus lowers the system power consumption. The optimized MP3 audio  decoder software meets real-time constraints on the SmartBadge system with low energy consumption. Furthermore, the performance improves by a factor of 7.27 and the energy consumption decreases by a factor of 4.45 over the original executable specification.
290|Efficient Web Browsing for Mobile Clients using HTTP Compression|Efficient web browsing on mobile computers presents a unique challenge. These machines are different from other classes of client computers since they have relatively lowbandwidth connections and they are battery-powered and therefore limited by their energy consumption. However, they tend to interact with the same servers for the delivery of web content. This project investigates optimizing the final critical link between a mobile client and a stationary base station by compressing HTTP request and response messages. Using a split proxy design, compression of individual request messages reduces bandwidth by 26% to 34% across a variety of benchmark traces, and applying compression to response messages yields savings of 59% to 82% of the compressible data. Higher compression rates are achieved by using streaming compression algorithms to compress the streams of request and response messages. In this case, the bandwidth for requests sees an order of magnitude improvement, and the respons...
291|Trace-Based Analysis of Duplicate Suppression in HTTP|Many HTTP resources (pages, graphics, etc.) are exact duplicates of other resources with different URLs. If an HTTP cache contains a duplicate of a requested resource, and could detect this, it could avoid substantial network costs by returning the cached duplicate in place of the requested URL. Previous studies have shown that there is substantial duplication of content in both HTTP and FTP, and several protocols have been proposed to support efficient and safe duplicate suppression in HTTP. We use traces covering millions of HTTP requests to quantify the potential benefit of an HTTP duplicate-suppression extension. In particular, we show that the benefits vary depending on content-type, and that a small fraction of Web servers account for most of the duplicated resources.
292|Implementation of a Power-Saving Protocol for Ad Hoc Wireless Networks|We describe the design and implementation of a power-saving protocol for ad hoc wireless networks. We present the Span power-saving protocol and discuss its implementation in the context of the Linux operating system. We address the issues of ad hoc routing, link layer design, and integration with the Linux networking stack using the 802.11b wireless link technology. From this thesis, we conclude that Span can be implemented on an 802.11b network with reasonable performance for most networking applications. Furthermore, our implementation of Span yields a lifetime improvement of between 12% and 29% at each node in an ad hoc network. We argue that with additional hardware, Span can outperform conventional 802.11 ad hoc networks in terms of capacity, latency, and power savings.
293|Adaptive Image Compression for Wireless Multimedia Communication|To enable ubiquitous wireless multimedia communication, the bottlenecks to communicating multimedia data over wireless channels must be addressed. Two significant bottlenecks which need to be overcome are the bandwidth and energy consumption requirements for mobile multimedia communication. In this paper, we address the bandwidth and energy dissipation bottlenecks by adapting image compression parameters to current communication conditions and constraints. We focus on the JPEG image compression algorithm, and present the results of varying some image compression parameters on energy dissipation, bandwidth required, and quality of image received. We present a methodology for selecting the JPEG image compression parameters in order to minimize energy consumption while meeting latency, bandwidth, and quality of image constraints.  I. 
294|Power Evaluation of Itsy Version 2.4|in 1982. We focus on information technology that is relevant to the technical strategy of the Corporation, and that has the potential to open new business opportunities. Research at WRL includes Internet protocol design and implementation, tools to optimize compiled binary code files, hardware and software mechanisms to support scalable shared memory, graphics VLSI ICs, handheld computing, and more. As part of WRL tradition, we test our ideas by extensive software or hardware prototyping. We publish the results of our work in a variety of journals, conferences, research reports, and technical notes. This document is a technical note. We use technical notes for rapid distribution of technical material; usually this represents research in progress. Research reports are normally accounts of completed research and may include material from earlier technical notes, conference papers, or magazine articles. You can retrieve research reports and technical notes via the World Wide Web at:
295|Eprof: An energy profiler for the iPaq|Introduction: As the use of portable electronic devices has become increasingly widespread, efforts to extend battery life have become more crucial. Much work has gone toward improving the efficiency of hardware components and recently more emphasis has been placed on software techniques that help conserve energy. Energy profilers help programmers determine what parts of their code are most energy-intensive by correlating energy use with functions or instructions within a program. Being able to see where energy is consumed allows programmers to concentrate on the
296|DATA COMPRESSION FOR CMB EXPERIMENTS|We discuss data compression for CMB experiments. Although “radical compression” to Cl bands, via quadratic estimators or local bandpowers, potentially offers a great savings in computation time, they do a considerably worse job at recovering the full likelihood than the the signal-to-noise eigenmode method of compression. We model a CMB observation at a pixel p = 1...Np, as ?p = sp + np, where s and n represent the contribution of the CMB signal and the noise, respectively, to the observation. The signal is given by sp = Fplmalm; alm is the spherical harmonic decomposition of the temperature and F encodes the beam and any chopping strategy of the experiment. We also assume that both the signal and noise contributions are described by independent, zero-mean, gaussian probability distributions, with correlation matrices given by <spsp' >  = CTpp ' and <npnp' >  = Cnpp ', so <?p?p' >  = CTpp ' + Cnpp '; here, CT = CT(?) is calculated as a function of ?, the parameters of the theory being tested in the likelihood function, and the noise matrix can include the effect of constraints due to, e.g., average or gradient removal. For Gaussian theories of adiabatic fluctuations, ? is typically the cosmological parameters; alternately they could be some set of phenomological parameters such as the value of the temperature power spectrum Cl in some bands. With this notation, the likelihood function is L?(?)  = P(?|?) = exp -1 2?p (CT(?) + Cn) -1
297|Universal data compression and linear prediction|The relationship between prediction and data compression can be extended to universal prediction schemes and universal data compression. Recentwork shows that minimizing the sequential squared prediction error for individual sequences can be achieved using the same strategies which minimize the sequential codelength for data compression of individual sequences. De ning a \probability &#034; as an exponential function of sequential loss, results from universal data compression can be used to develop universal linear prediction algorithms. Speci cally, we present an algorithm for linear prediction of individual sequences which istwice-universal, over parameters and model orders. 1
298|Universal schemes for sequential decision from individual data sequences|Sequential decision algorithms are investigated, under a family of additive performance criteria, for individual data sequences, with various application areas in information theory and signal processing. Simple universal sequential schemes are known, under certain conditions, to approach optimality uniformly as fast as n-l log n, where n is the sample size. For the case of finite-alphabet observations, the class of schemes that can be implemented by bite-state machines (FSM’s), is studied. It is shown that Markovian machines with daently long memory exist that are asympboticaily nerrly as good as any given FSM (deterministic or WomhI) for the purpose of sequential decision. For the continuous-valued observation case, a useful class of parametric schemes is discussed with special attention to the recursive least squares W) algorithm.  
299|The Transform and Data Compression Handbook|The goal of image compression is to store an image in a more compact form, i.e., a representation that requires fewer bits for encoding than the original image. This is possible for images because, in their “raw ” form, they contain a high degree of redun-dant data. Most images are not haphazard collections of arbitrary intensity transitions.
300|Optimal Unsupervised Learning in a Single-Layer Linear Feedforward Neural Network|A new approach to unsupervised learning in a single-layer linear feedforward neural network is discussed. An optimality principle is proposed which is based upon preserving maximal information in the output units. An algorithm for unsupervised learning based upon a Hebbian learning rule, which achieves the desired optimality is presented, The algorithm finds the eigenvectors of the input correlation matrix, and it is proven to converge with probability one. An implementation which can train neural networks using only local &#034;synaptic&#034; modification rules is described. It is shown that the algorithm is closely related to algorithms in statistics (Factor Analysis and Principal Components Analysis) and neural networks (Self-supervised Backpropagation, or the &#034;encoder&#034; problem). It thus provides an explanation of certain neural network behavior in terms of classical statistical techniques. Examples of the use of a linear network for solving image coding and texture segmentation problems are presented. Also, it is shown that the algorithm can be used to find &#034;visual receptive fields&#034; which are qualitatively similar to those found in primate retina and visual cortex.
301|A basic theory |ar
302|Global convergence of oja’s subspace algorithm for principal component extraction|Abstract—Oja’s principal subspace algorithm is a well-known and powerful technique for learning and tracking principal information in time series. A thorough investigation of the convergence property of Oja’s algorithm is undertaken in this paper. The asymptotical convergence rates of the algorithm is discovered. The dependence of the algorithm on its initial weight matrix and the singularity of the data covariance matrix is comprehensively addressed. Index Terms—Convergence rate, global convergence, principal components extraction. I.
303|APPLICATIOIU OF SPLAY TREES TO DATA COMPRESSION |The splay-prefix algorithm is one of the simplest and fastest adaptive data compression algorithms based on the use of a prefix code. The data structures used in the splay-prefix algorithm can also be applied to arithmetic data compression. Applications of these algorithms to encryption and image processing are suggested.
304|Multiple Description Coding: Compression Meets the Network|This article focuses on the compressed representations of the pictures
305|Receiver-driven Layered Multicast|State of the art, real-time, rate-adaptive, multimedia applications adjust their transmission rate to match the available network capacity. Unfortunately, this source-based rate-adaptation performs poorly in a heterogeneous multicast environment because there is no single target rate — the conflicting bandwidth requirements of all receivers cannot be simultaneously satisfied with one transmission rate. If the burden of rate-adaption is moved from the source to the receivers, heterogeneity is accommodated. One approach to receiver-driven adaptation is to combine a layered source coding algorithm with a layered transmission system. By selectively forwarding subsets of layers at constrained network links, each user receives the best quality signal that the network can deliver. We and others have proposed that selective-forwarding be carried out using multiple IP-Multicast groups where each receiver specifies its level of subscription by joining a subset of the groups. In this paper, we extend the multiple group framework with a rate-adaptation protocol called Receiver-driven Layered Multicast, or RLM. Under RLM, multicast receivers adapt to both the static heterogeneity of link bandwidths as well as dynamic variations in network capacity (i.e., congestion). We describe the RLM protocol and evaluate its performance with a preliminary simulation study that characterizes user-perceived quality by assessing loss rates over multiple time scales. For the configurations we simulated, RLM results in good throughput with transient short-term loss rates on the order of a few percent and long-term loss rates on the order of one percent. Finally, we discuss our implementation of a software-based Internet video codec and its integration with RLM.
307|Successive refinement of information|Abstrocr-The successive refinement of information consists of first approximating data using a few bits of information, then iteratively improving the approximation as more and more information is supplied. The god is to achieve an optimal description at each stage. In general an ongoing description is sought which is rate-distortion optimal whenever it is interrupted. It is shown that a rate distortion problem is successively refinable if and only if the individual solutions of the rate distortion problems can be written as a Markov chain. This implies in particular that tree structured descriptions are optimal if and only if the rate distortion problem is successively rethable. Successive refinement is shown to be possible for all fmite alphabet signals with Hamming distortion, for Gaussian signals with squared-error distortion, and for Laplacian signals with absolute-error distortion. However, a simple counterexample witb absolute error distortion and a symmetric source distribution shows that successive refinement is not always achievable. lnder Term-Rate distortion, refinement, progressive transmission, multiuser information theory, squared-error distortion, tree structure. I.
308|Unequal Loss Protection: Graceful Degradation of Image Quality over Packet Erasure Channels throught Forward Error Correction| We present the unequal loss protection (ULP) framework in which unequal amounts of forward error correction are applied to progressive data to provide graceful degradation of image quality as packet losses increase. We develop a simple algorithm that can find a good assignment within the ULP framework. We use the Set Partitioning in Hierarchical Trees coder in this work, but our algorithm can protect any progressive compression scheme. In addition, we promote the use of a PMF of expected channel conditions so that our system can work with almost any model or estimate of packet losses. We find that when optimizing for an exponential packet loss model with a mean loss rate of 20 % and using a total rate of 0.2 bits per pixel on the Lenna image, good image quality can be obtained even when 40% of transmitted packets are lost.  
309|Fast quantizing and decoding and algorithms for lattice quantizers and codes|and their duals a very fast algorithm is given for finding the closest lattice point to an arbitrary point. If these lattices are used for vector quantizing of uniformly distributed data, the algorithm finds the minimum distortion lattice point. If the lattices are used as codes for a Gaussian channel, the algorithm performs maximum likelihood decoding. T I.
310|Theoretical Foundations of Transform Coding|This article explains the fundamental principles of transform coding; these principles apply equally well to images, audio, video, and various other types of data, so abstract formulations are given. Much of the material presented here is adapted from [14, Chap. 2, 4]. The details on wavelet transform-based image compression and the JPEG2000 image compression standard are given in the following two articles of this special issue [38], [37]
311|Multiple Description Coding via Polyphase Transform and Selective Quantization|In this paper, we present an ecient Multiple Description Coding (MDC) technique to achieve robust communication over unreliable channels such as a lossy packet network. We first model such unreliable channels as erasure channels and then we present a MDC system using polyphase transform and selective quantization to recover channel erasures. Different from previous MDC work, our system explicitly separates description generation and redundancy addition which greatly reduces the implementation complexity specially for systems with more than two descriptions. Our system also realizes a Balanced Multiple Description Coding (BMDC) framework which can generate descriptions of statistically equal rate and importance. This property is well matched to communication systems with no priority mechanisms for data delivery, such as today&#039;s Internet.
312|Optimal multiple description transform coding of Gaussian vectors|Includes minor corrections. Multiple description coding (MDC) is source coding for multiple channels such that a decoder which receives an arbitrary subset of the channels may pro-duce a useful reconstruction. Orchard et al. [1] proposed a transform coding method for MDC of pairs of independent Gaussian random variables. This pa-per provides a general framework which extends multiple description transform coding (MDTC) to any number of variables and expands the set of transforms which are considered. Analysis of the general case is provided, which can be used to numerically design optimal MDTC systems. The case of two variables sent over two channels is analytically optimized in the most general setting where channel failures need not have equal probability or be independent. It is shown that when channel failures are equally probable and independent, the transforms used in [1] are in the optimal set, but many other choices are pos-sible. A cascade structure is presented which facilitates low-complexity design, coding, and decoding for a system with a large number of variables. 1
313|Quantized Frame Expansions as Source-Channel Codes for Erasure Channels|Quantized frame expansions are proposed as a method for generalized multiple description coding, where each quantized coe cient is a description. Whereas previous investigations have revealed the robustness of frame expansions to additive noise and quantization, this represents a new application of frame expansions. The performance of a system based on quantized frame expansions is compared to that of a system with a conventional block channel code. The new system performs well when the number of lost descriptions (erasures on an erasure channel) is hard to predict. 1
314|An Integrated Source Coding and Congestion Control Framework for Video Streaming in the Internet|We describe a framework for video transmission over the Internet that features the coordinated operation of an application-layer video source coding algorithm and a transport-layer rate control mechanism. The proposed video coding scheme operates on a progressively encoded video stream and provides graceful resilience to network packet drops. The robustness is enabled through a generalized Multiple Description (MD) coding strategy, architected as an adaptive array of packet-erasure correction codes. The video coding algorithm is matched to an efficient and reactive rate control mechanism that minimizes the fluctuation of rate and uses the profile of past losses to adjust the rate in a TCP-friendly manner.  While the two constituent algorithms identified above are interesting in their own right, a key feature of this work is the integration of these algorithms in a simple framework that seeks to maximize the expected delivered video quality at the receiver through coordinated adaptation...
315|Asymmetric multiple description lattice vector quantizers|Abstract—We consider the design of asymmetric multiple description lattice quantizers that cover the entire spectrum of the distortion profile, ranging from symmetric or balanced to successively refinable. We present a solution to a labeling problem, which is an important part of the construction, along with a general design procedure. The high-rate asymptotic performance of the quantizer is also studied. We evaluate the rate-distortion performance of the quantizer and compare it to known information-theoretic bounds. The high-rate asymptotic analysis is compared to the performance of the quantizer. Index Terms—Cubic lattice, high-rate quantization, lattice quantization, multiple descriptions, quantization, source coding, successive refinement, vector quantization. I.
316|SPIHT for Generalized Multiple Description Coding   |We present a simple and efficient scheme for using the Set Partitioning in Hierarchical Trees SPIHT) image compression algorithm [1] in a generalized multiple description framework. To combatpacketloss,controlledamountsofredundancyare addedtotheoriginaldataduringthecompressionprocess. Unequalloss protectionisimplementedbyvaryingtheamount of redundancywiththeimportanceofdata. Thealgorithmachievesgracefuldegradationofimagequalityin thepresenceofincreasingdescriptionloss; high image quality is obtained even when over half of the descriptions are lost.  
317|Optimal Filter Banks for Multiple Description Coding: Analysis and Synthesis|Multiple Description (MD) coding is a source coding technique for information transmission over unreliable networks. In MD coding, the coder generates several different descriptions of the same signal and the decoder can produce a useful reconstruction of the source with any received subset of these descriptions. In this paper we study the problem of MD coding of stationary Gaussian sources with memory. First, we compute an approximate MD rate distortion region for these sources, which we prove to be asymptotically tight at high rates. This region generalizes the MD rate distortion region of El Gamal, Cover and Ozarow for memoryless Gaussian sources. Then, we develop an algorithm for the design of optimal two-channel biorthogonal filter banks for MD coding of Gaussian sources. We show that optimal filters are obtained by allocating the redundancy over frequency with a reverse &#034;water-filling&#034; strategy. Finally, we present experimental results which show the effectiveness of our filter banks in the low complexity, low rate regime.
318|Generalized Multiple Description Coding through Unequal Loss Protection    |In this paper we present an approach to the generalized Multiple Description problem that is fundamentally different from previously published algorithms. Our approach uses explicitchannelcodingintheformof Unequal LossProtectiontoobtainasolutionthatincorporates
many important properties: it can be used with any progressive source coder;
itgeneratesabalancedencoding withinformationequally
dispersedamong thedescriptions;
itaddsaquantiableamountofredundancy;
itadaptsthatamountofredundancytoexpected channel conditions;
and itcanoptimizefordifferentdistortionmeasures.
These properties allow the system to gradually improve image quality as the number of received descriptions increases. We compare our system to previously published results and show that forward error correction in Multiple Description coding can surpass them by a significant margin.  
319|The analysis and design of windowed Fourier frame based multiple description source coding schemes |In this paper the windowed Fourier encoding-decoding scheme applied to the multiple description compression problem is analyzed. In the general case, four window functions are needed to dene the encoder and decoder, although this number can be reduced to three or two by using time-shift or frequency-shift division schemes. The encoding coecients are next divided into two groups according to the eveness of either modulation or translation index. The distortion on each channel is analyzed using the Zak transform. For the optimal windows, explicit representation formulas are obtained and non-localization results are proved. Asymptotic formulas of the total distortion and transmission rate are established and the redundancy is shown to trade-o  between these two. Key words: multiple description coding, windowed Fourier transform, redundant sets Acknowledgments The authors wish to thank the anonymous referees for their comments and suggestions regarding the original manuscript.
320|Multiple Description Perceptual Audio Coding with Correlating Transforms|In audio communication over a lossy packet network, concealment techniques are used to mitigate the effects of lost packets. This concealment is markedly improved if the compressed representation retains redundancy to aid in the estimation of lost information. A perceptual audio coder employing multiple description correlating transforms demonstrates this phenomenon. Permission to publish this abstract separately is granted. EDICS Category: SA 2.5 Corresponding author: Jelena Kovacevi&#039;c e-mail: jelena@bell-labs.com Room 2C-176, Bell Labs voice: +1 908-582-6504 600 Mountain Avenue fax: +1 908-582-3340 Murray Hill, NJ 07974 This work was completed while the first author was a consultant at Bell Labs. Submitted as a correspondence item (revised from an August 1998 submission). May 6, 1999 Submitted to IEEE Trans. Speech &amp; Audio Proc.  2 AREAN, KOVA CEVI &#039; C &amp; GOYAL: MULTIPLE DESCRIPTION PERCEPTUAL AUDIO CODING I. Introduction  Most state-of-the-art audio coders combine source coding prin...
321|A Robust Codec for Transmission of Very Low Bit-rate Video over Channels with Bursty Errors|We describe a robust codec for the transmission of very low bit-rate video over channels with a variety of errors including random and bursty bit errors and packet loss. The codec exploits adaptivity to give good performance with a low overhead. By only protecting macroblocks which would otherwise be poorly concealed by the decoder, the codec allows adaptive selection of the parts of video to protect. For protection, it uses multiple description codes which indirectly provide frequency based adaptivity by protecting the more significant DCT coefficients. Simulations show significant improvements in the performance of the codec when compared to codecs which use intra macroblock updating (raster scan and random) at the same overhead. The codec is efficient in its use of bits and has good error resilience properties both objectively and subjectively over a wide range of conditions. Further, transcoding of the received bit-stream to the standard H.263 syntax is relatively easy.  I. Introdu...
322|Multiple description trellis-coded quantization|Abstract — We present a construction of multiple description trellis-coded quantizers. We use the tensor product of trellises to build a trellis which is applicable to multiple description coding. The problems of index assignment and set partitioning for the resulting trellis are considered. The Viterbi algorithm provides the best path for encoding and the design procedure utilizes a generalized Lloyd algorithm. The encoding process simultaneously generates all the transmitted sequences. Furthermore, the complexity of the scheme is almost independent of the rate. The quantizer provides remarkable performance with little encoding complexity. Index Terms — Diversity systems, multiple description, source coding, trellis-coded quantization.
323|Multiple description lattice vector quantization: Variations and extensions|Multiple description lattice vector quantization (MDLVQ) is a technique for two-channel multiple description coding. We observe that MDLVQ, in the form introduced by Servetto, Vaishampayan and Sloane in 1999, is inherently optimized for the central decoder; i.e., for a zero probability of a lost description. With a nonzero probability of description loss, performance is improved by modifying the encoding rule (using nearest neighbors with respect to “multiple description distance”) and by perturbing the lattice codebook. The perturbation maintains many symmetries and hence does not significantly effect encoding or decoding complexity. An extension to more than two descriptions with attractive decoding properties is outlined. 1
324|Quantized Oversampled Filter Banks with Erasures|Oversampled filter banks can be used to enhance resilience to erasures in communication systems in much the same way that finite-dimensional frames have previously been applied. This paper extends previous finite dimensional treatments to frames and signals in  l  2 (Z) with frame expansions that can be implemented efficiently with filter banks. It is shown that tight frames attain best performance. In particular, if encoding with a uniform frame, the quantization error is minimized if and only if the frame is tight. In case of one erasure and if encoding with a strongly uniform frame, tight frames are still optimal. In case of more erasures, an expression for the mean square error is given and some general considerations are presented. 1 
325|The Multiple Description Rate Region at High Resolution|Consider encoding a source X into two descriptions, such that the first, the second and both descriptions allow decoding of X with distortion levels d 1 , d 2 and d 0 , respectively, relative to a distortion measure ae(x;  x). Ozarow have found an explicit characterization for the region  R    (oe  2  ; d 1 ; d 2 ; d 0 ) of admissible rate pairs of the two descriptions, for a Gaussian source X    ¸ N (0; oe  2  ), relative to the squared-error distortion measure ae(x;  x) = (x \Gamma  x)  2  . In fact, this is the only case for which the multiple description rate-distortion region is completely known. We show that for a general real valued source, a locally quadratic distortion measure of the form  ae(x;  x) = w(x)  2  (x \Gamma  x)  2  + o((x \Gamma  x)  2  ), and small distortion levels, the region of admissible rate pairs equals approximately  R    i  P x 2  2Eflog w(X)g  ; d 1 ; d 2 ; d 0  j where P x is the entropy-power of the source. Applications to companding quantization are a...
326|On Optimal Frame Expansions for Multiple Description Quantization|We study the problem of finding the optimal overcomplete (frame) expansion and bit allocation for multiple description quantization of a Gaussian signal at high rates over a lossy channel. We provide a general analysis for the problem and solve it for the case of a 3 \Theta 2 frame expansion. 1 Introduction  Recently, the use of multiple description quantization for use over lossy (erasure) channels has been studied extensively. Among the linear transform based approaches to multiple description coding have been the critically sampled correlating transforms approach [1, 2, 3, 4, 5] and the overcomplete frame expansion approach [6, 4, 7, 8]. In [3], Goyal et al. present an analysis for optimizing the correlating transform in the critically sampled case. In this paper, we perform a similar analysis on an overcomplete expansion. In multiple description quantization using overcomplete (frame) expansions, an input signal  x 2 R  K  is represented by a vector y = Fx 2 R  N  , N ? K, as shown...
327|On Multiple Descriptions and Team Guessing|Witsenhausen&#039;s hyperbola bound for the multiple description problem without excess rate in case of a binary source is not right for exact joint reproductions. However, this bound is tight for almost--exact joint reproductions (Theorem 1, conjectured by Witsenhausen). The proof is based on an approximative form of the team guessing lemma for sequences of random variables. (This result may be of interest also for team guessing.) The hyperbola bound is also tight for exact joint reproductions and arbitrarily small, but possible, excess rate (Theorem 2). The proof of this result uses our covering lemma. 1 The Problem of Multiple Descriptions  During the last years a strong interest has developed in a certain source--coding problem called the &#034;problem of multiple descriptions&#034;. Since the origin of this problem and the motivations for its study have already been extensively described (see [1]--[9]), we begin immediately with the formal setup. Let (X t )  1  t=1 be a sequence of independent a...
328|Data compression of multiresolution surfaces|In this paper we introduce a new compressed representation for multiresolution models (MRM) of triangulated surfaces of 3D-objects. Associated with the representation we present compression and decompression algorithms. Our representation allows us to extract the surface at variable resolution in time linear in the output size. It applies to MRMs generated by different simplification algorithms like local vertex deletion or edge and triangle collapse. The time required to transmit models over communication lines and the space needed to store the MRMs is significantly reduced. 
329|Progressive Meshes |Highly detailed geometric models are rapidly becoming commonplace in computer graphics. These models, often represented as complex triangle meshes, challenge rendering performance, transmission bandwidth, and storage capacities. This paper introduces the progressive mesh (PM) representation, a new scheme for storing and transmitting arbitrary triangle meshes. This efficient, lossless, continuous-resolution representation addresses several practical problems in graphics: smooth geomorphing of level-of-detail approximations, progressive transmission, mesh compression, and selective refinement. In addition, we present a new mesh simplification procedure for constructing a PM representation from an arbitrary mesh. The goal of this optimization procedure is to preserve not just the geometry of the original mesh, but more importantly its overall appearance as defined by its discrete and scalar appearance attributes such as material identifiers, color values, normals, and texture coordinates. We demonstrate construction of the PM representation and its applications using several practical models.
330|Decimation of triangle meshes|The polygon remains a popular graphics primitive for computer graphics application. Besides having a simple representation, computer rendering of polygons is widely supported by commercial graphics hardware and software.
331|Multiresolution Analysis of Arbitrary Meshes|In computer graphics and geometric modeling, shapes are often represented by triangular meshes. With the advent of laser scanning systems, meshes of extreme complexity are rapidly becoming commonplace. Such meshes are notoriously expensive to store, transmit, render, and are awkward to edit. Multiresolution analysis offers a simple, unified, and theoretically sound approach to dealing with these problems. Lounsbery et al. have recently developed a technique for creating multiresolution representations for a restricted class of meshes with subdivision connectivity. Unfortunately, meshes encountered in practice typically do not meet this requirement. In this paper we present a method for overcoming the subdivision connectivity restriction, meaning that completely arbitrary meshes can now be converted to multiresolution form. The method is based on the approximation of an arbitrary initial mesh M by a mesh M    that has subdivision connectivity and is guaranteed to be within a specified tolerance. The key
332|View-Dependent Refinement of Progressive Meshes |Level-of-detail (LOD) representations are an important tool for realtime rendering of complex geometric environments. The previously introduced progressive mesh representation defines for an arbitrary triangle mesh a sequence of approximating meshes optimized for view-independent LOD. In this paper, we introduce a framework for selectively refining an arbitrary progressive mesh according to changing view parameters. We define efficient refinement criteria based on the view frustum, surface orientation, and screen-space geometric error, and develop a real-time algorithm for incrementally refining and coarsening the mesh according to these criteria. The algorithm exploits view coherence, supports frame rate regulation, and is found to require less than 15 % of total frame time on a graphics workstation. Moreover, for continuous motions this work can be amortized over consecutive frames. In addition, smooth visual transitions (geomorphs) can be constructed between any two selectively refined meshes. A number of previous schemes create view-dependent LOD meshes for height fields (e.g. terrains) and parametric surfaces (e.g. NURBS). Our framework also performs well for these special cases. Notably, the absence of a rigid subdivision structure allows more accurate approximations than with existing schemes. We include results for these cases as well as for general meshes.
333|Re-Tiling Polygonal Surfaces|This paper presents an automatic method of creating surface models at several levels of detail from an original polygonal description of a given object. Representing models at various levels of detail is important for achieving high frame rates in interactive graphics applications and also for speeding-up the off-line rendering of complex scenes. Unfortunately, generating these levels of detail is a time-consuming task usually left to a human modeler. This paper shows how a new set of vertices can be distributed over the surface of a model and connected to one another to create a re-tiling of a surface that is faithful to both the geometry and the topology of the original surface. Themain contributions of this paper are: 1) a robust method of connecting together new vertices over a surface, 2) a way of using an estimate of surface curvature to distribute more new vertices at regions of higher curvature and 3) a method of smoothly interpolating between models that represent the same object at different levels of detail. The key notion in the re-tiling procedure is the creation of an intermediate model called the mutual tessellation of a surface that contains both the vertices from the original model and the new points that are to become vertices in the re-tiled surface. The new model is then created by removing each original vertex and locally re-triangulating the surface in a way that matches the local connectedness of the initial surface. This technique for surface retessellation has been successfully applied to iso-surface models derived from volume data, Connolly surface molecular models and a tessellation of a minimal surface of interest to mathematicians. CRCategoriesandSubjectDescriptors: I.3.3 [ComputerGraph- ics]: Picture/Image Generation -- Display algorithms
334|Mesh Optimization|We present a method for solving the following problem: Given a set of data points scattered in three dimensions and an initial triangular mesh wH, produce a mesh w, of the same topological type as wH, that fits the data well and has a small number of vertices. Our approach is to minimize an energy function that explicitly models the competing desires of conciseness of representation and fidelity to the data. We show that mesh optimization can be effectively used in at least two applications: surface reconstruction from unorganized points, and mesh simplification (the reduction of the number of vertices in an initially dense mesh of triangles).
335|Mesh Reduction with Error Control|In many cases the surfaces of geometric models consist of a large number of triangles. Several algorithms were developed to reduce the number of triangles required to approximate such objects. Algorithms that measure the deviation between the approximated object and the original object are only available for special cases. In this paper we use the Hausdorff distance between the original and the simplified mesh as a geometrically meaningful error value which can be applied to arbitrary triangle meshes. We present a new algorithm to reduce the number of triangles of a mesh without exceeding a user-defined Hausdorff distance between the original and simplified mesh. As this distance is parameterization-independent, its use as error measure is superior to the use of the L  1  -Norm between parameterized surfaces. Furthermore the Hausdorff distance is always less than the distance induced by the L  1  -Norm. This results in higher reduction rates. Excellent results were achieved by the new ...
336|Multiresolution Decimation based on Global Error|Due to the surface meshes produced at increasing complexity in many applications, interest in efficient simplification algorithms and multiresolution representation is very high. An enhanced simplification approach together with a general multiresolution data scheme are presented here. JADE, a new simplification solution based on the Mesh Decimation approach has been designed to provide both increased approximation precision, based on global error management, and multiresolution output. Moreover, we show that with a small increase in memory, which is needed to store the multiresolution data representation, we are able to extract any level of detail representation from the simplification results in an extremely efficient way. Results are reported on empirical time complexity, approximation quality, and simplification power. Keywords: surface modeling, mesh simplification, bounded approximation error, multiresolution. Address to which proofs should be sent:  R. SCOPIGNO, CNUCE -- Consigl...
337|Hierarchical Triangulation for Multiresolution Surface Description|A new hierarchical triangle-based model for representing surfaces over sampled data is proposed, which is based on the subdivision of the surface domain into nested triangulations, called a Hierarchical Triangulation (HT). The model allows compression of spatial data and representation of a surface at successively finer degrees of resolution. An HT is a collection of triangulations organized in a tree, where each node, except for the root, is a triangulation refining a face belonging to its parent in the hierarchy. We present a topological model for representing an HT, and algorithms for its construction and for the extraction of a triangulation at a given degree of resolution. The surface model, called a Hierarchical Triangulated Surface (HTS), is obtained by associating data values with the vertices of triangles, and defining suitable functions that describe the surface over each triangular patch. We consider an application of a piecewise-linear version of the HTS to interpolate topo...
338|Discretized Marching Cubes|Since the introduction of standard techniques for isosurface extraction from volumetric datasets, one of the hardest problems has been to reduce the number of triangles (or polygons) generated. This paper presents an algorithm that considerably reduces the number of polygons generated by a Marching Cubes-like scheme without excessively increasing the overall computational complexity. The algorithm assumes discretization of the dataset space and replaces cell edge interpolation by midpoint selection. Under these assumptions, the extracted surfaces are composed of polygons lying within a finite number of incidences, thus allowing simple merging of the output facets into large coplanar polygons. An experimental evaluation of the proposed approach on datasets related to biomedical imaging and chemical modelling is reported. 1 Introduction The use of the Marching Cubes (MC) technique, originally proposed by W. Lorensen and H. Cline [7], is considered to be a standard approach to the proble...
339|On Levels of Detail in Terrains|In many applications it is important that one can view a scene at different levels of detail. A prime example is flight simulation: a high level of detail is needed when flying low, whereas a low level of detail suffices when flying high. More precisely, one would like to visualize the part of the scene that is close at a high level of detail, and the part that is far away at a low level of detail. We propose a hierarchy of detail levels for a polyhedral terrain (or, triangulated irregular network) that allows this: given a view point, it is possible to select the appropriate level of detail for each part of the terrain in such a way that the parts still fit together continuously. The main advantage of our structure is that it uses the Delaunay triangulation at each level, so that triangles with very small angles are avoided. This is the first method that uses the Delaunay triangulation and still allows to combine different levels into a single representation.  
340|Representation and Visualization of Terrain Surfaces at Variable Resolution|We present a new approach for managing the multiresolution representation of discrete topographic surfaces. A Triangulated Irregular Network (TIN) representing the surface is built from sampled data by iteratively refining an initial triangulation that covers the whole domain. The refinement process generates triangulations of the domain corresponding to increasingly finer approximations of the surface. Such triangulations are embedded into a structure in a three dimensional space. The resulting representation scheme encodes all intermediate representations that were generated during refinement. We propose a data structure and traversal algorithms that are oriented to the efficient extraction of approximated terrain models with an arbitrary precision, either constant or variable over the domain.  1. Introduction  The search for multiresolution representation schemes has recently become very popular. Major applications involve generic surfaces embedded in 3D space  16;8;27  , terrains i...
341|Multiresolution Representations for Surfaces Meshes|We describe a method for constructing multi resolution models (MRM) of complex triangle meshes  and show how these representations can be used to create view dependent adaptive approximations  of triangle meshes on the fly with guaranteed approximation error.  We use a modified one-sided Hausdorff distance as a geometrically meaningful error value between  the original and the simplified mesh. Since this distance is parameterization-independent, its use as  error measure is superior to the use of the L  1  -Norm between parameterized surfaces.  The proposed multi-resolution model supports progressive transmission, smooth geomorphing  of different levels of detail approximations, mesh compression and selective error tolerance editing.  Various examples demonstrate the excellent results that were achieved by this new method with  triangle meshes from different application areas such as volume rendering, terrain modeling and the  approximations of parameterized surfaces.  CR Descriptors: ...
342|Generation of Multiresolution Models from CAD - Data for Real Time Rendering|. A mesh refinement and a mesh simplification algorithm are presented. Both algorithms guarantee a user-defined error tolerance and deliver a multiresolution model. After the computation of the multiresolution model triangulation of the surface patches at variable resolutions can be incrementally generated on-the-fly at rendering time. The resulting triangulations form hierarchical Delaunay triangulations in parameter space. 1 Introduction and Previous work  The visualization of large CAD-models, like cars, trains, aero planes, etc. becomes a major challenge in the context of virtual reality. In most experiments a number of such models have to be visualized and animated simultaneously. Examples are the optimization of the cabin of a train or the optimization of a driver&#039;s position in a car. To examine the panorama in such a place, not only the train or car themselves have to be visualized but also other cars, pedestrians, buildings, etc. Despite the performance of modern graphics hardw...
343|Incremental View-dependent Multiresolution Triangulation of Terrain|A view-dependent multiresolution triangulation algorithm is presented for a real-time flythrough. The triangulation of the terrain is generated incrementally onthe -fly during the rendering time. We show that since the view changes smoothly only a few incremental modifications are required to update the triangulation to a new view. The resulting triangles form a multiresolution Delaunay triangulation which satisfies a predetermined view-dependent error tolerance. The presented method provides a guaranteed-quality mesh since it has control over the global geometric approximation error of the multiresolution view-dependent triangulation. 1 Introduction  Triangular meshes are currently the most widely-used representation of terrain models in computer graphics. Planar polygons and triangles in particular are standard rendering primitives of common graphics workstations that can rapidly render polygons. Terrain data is usually reconstructed by photogrammetric modeling techniques and other a...
345|Design an Algorithm for Data Compression using |Compression is the science and art of representing the information in a compact form rather than its original or uncompressed form in data storage or transmission. Data compression is storing data in a format that requires less space than usual. Nowadays, data compression is a common requirement in telecommunication industry. Since last few decades, the research is continuing in the field of compression and various data compression techniques are already developed to compress different data formats. But, a data compression program using strange number system (specially using pentaoctagesimal SNS) tries to compact the data in some special format, such that the data occupies less disk space or that it can be transmitted in less time. A decompression program restores the information again. This paper mainly describes a simple compression and decompression process which is free from time complexity.
346|Lossless Data Compression Substitution Algorithms Based on Tables|Lossless data	compression	algorithms	based	on substitution tables
347|Identifying hierarchical structure in sequences: A linear-time algorithm|SEQUITUR is an algorithm that infers a hierarchical structure from a sequence of discrete symbols by replacing repeated phrases with a grammatical rule that generates the phrase, and continuing this process recursively. The result is a hierarchical representation of the original sequence, which offers insights into its lexical structure. The algorithm is driven by two constraints that reduce the size of the grammar, and produce structure as a by-product. SEQUITUR breaks new ground by operating incrementally. Moreover, the method’s simple structure permits a proof that it operates in space and time that is linear in the size of the input. Our implementation can process 50,000 symbols per second and has been applied to an extensive range of real world sequences. 1.
348|Data Compression in Wireless Sensor Network |Wireless sensor networks are resource constrain. They are limited power supply, bandwidth for communication, processing speed and memory space. The possible way of achieving maximum utilization of those resources is applying data compression on sensor data. Since processing data consumes less power than transmitting data in wireless medium. So it is effective to apply data compression before transmitting data for reducing power consumption by a sensor node. In this we propose a improved LZW data compression algorithm particularly suited to be used on available commercial nodes of a WSN, where energy, memory and computational resources are very limited. Since processing data consumes less power than transmitting data in wireless medium. So it is effective to apply data compression before transmitting data for reducing power consumption by a sensor node.
349|A Survey on Sensor Networks|Recent advancement in wireless communica- tions and electronics has enabled the develop- ment of low-cost sensor networks. The sensor networks can be used for various application areas (e.g., health, military, home). For different application areas, there are different technical issues that researchers are currently resolving. The current state of the art of sensor networks is captured in this article, where solutions are discussed under their related protocol stack layer sections. This article also points out the open research issues and intends to spark new interests and developments in this field.
350|Qualitative Researching|ltaic (PV) electricity production from an intermittent Since 1978, compressed air energy storage (CAES) compressed air can then be released on demand to the CAES plant’s turbo-generator set to generate premium value electricity. The first CAES plant was built in broadened in the ittency of wind g with Cavallo,2 nomic feasibility as turbine (GT) oduction.3–8 The ate underground o the wind farms and shape the
351|Alphalinolenic acid supplementation and conversion to n-3 long-chain polyunsaturated fatty acids in humans|A statement on PUFA nutrition developed and edited based on input from ISSFAL members and accepted by vote of the ISSFAL Board of Directors. Summary. Blood levels of polyunsaturated fatty acids (PUFA) are considered bio-markers of status. Alpha-linolenic acid, ALA, the plant omega-3, is the dietary precursor for the long chain omega-3 PUFA eicosapentaenoic acid (EPA), docosapentaenoic acid (DPA), and docosahexaenoic acid (DHA). Studies in normal healthy adults consuming western diets which are rich in linoleic acid (LA) show that supplemental ALA raises EPA and DPA status in the blood and in breast milk. However, ALA or EPA dietary sup-plements have little effect on blood or breast milk DHA levels, whereas consumption of preformed DHA is effective in raising blood DHA levels. Addition of ALA to the diets of formula-fed infants does raise DHA, but no level of ALA tested raises DHA to levels achievable with preformed DHA at intakes similar to typical human milk DHA supply. The DHA status of infants and adults consuming preformed DHA in their diets is, on av-erage, greater than that of people who do not consume DHA. With no other changes in diet, improvement of blood DHA status can be achieved with dietary supplements of preformed DHA, but not with supplementation of ALA, EPA, or other precursors. Final Page 2
352|N-3 PolyUnsaturated Fatty Acids Shift Estrogen Signaling to Inhibit Human Breast Cancer Cell Growth |Although evidence has shown the regulating effect of n-3 poly-unsaturated fatty acid (n-3 PUFA) on cell signaling transduction, it remains unknown whether n-3 PUFA treatment modulates estrogen signaling. The current study showed that docosahexaenoic acid (DHA, C22:6), eicosapentaenoic acid (EPA, C20:5) shifted the pro-survival and proliferative effect of estrogen to a pro-apoptotic effect in human breast cancer (BCa) MCF-7 and T47D cells. 17 b-estradiol (E2) enhanced the inhibitory effect of n-3 PUFAs on BCa cell growth. The IC50 of DHA or EPA in MCF-7 cells decreased when combined with E2 (10 nM) treatment (from 173 mM for DHA only to 113 mM for DHA+E2, and from 187 mm for EPA only to 130 mm for EPA+E2). E2 also augmented apoptosis in n-3 PUFA-treated BCa cells. In contrast, in cells treated with stearic acid (SA, C18:0) as well as cells not treated with fatty acid, E2 promoted breast cancer cell growth. Classical (nuclear) estrogen receptors may not be involved in the pro-apoptotic effects of E2 on the n-3 PUFA-treated BCa cells because ERa agonist failed to elicit, and ERa knockdown failed to block E2 pro-apoptotic effects. Subsequent studies reveal that G protein coupled estrogen receptor 1 (GPER1) may mediate the pro-apoptotic effect of estrogen. N-3 PUFA treatment initiated the pro-apoptotic signaling of estrogen by increasing GPER1-cAMP-PKA signaling response, and blunting EGFR, Erk 1/2, and AKT activity. These findings may not only provide the evidence to link n-3 PUFAs biologic effects and the pro-apoptotic signaling of
353|Mixtures of Probabilistic Principal Component Analysers|Principal component analysis (PCA) is one of the most popular techniques for processing, compressing and visualising data, although its effectiveness is limited by its global linearity. While nonlinear variants of PCA have been proposed, an alternative paradigm is to capture data complexity by a combination of local linear PCA projections. However, conventional PCA does not correspond to a probability density, and so there is no unique way to combine PCA models. Previous attempts to formulate mixture models for PCA have therefore to some extent been ad hoc. In this paper, PCA is formulated within a maximum-likelihood framework, based on a specific form of Gaussian latent variable model. This leads to a well-defined mixture model for probabilistic principal component analysers, whose parameters can be determined using an EM algorithm. We discuss the advantages of this model in the context of clustering, density modelling and local dimensionality reduction, and we demonstrate its applicat...
354|Hierarchical mixtures of experts and the EM algorithm|We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM’s). Learning is treated as a max-imum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parame-ters of the architecture. We also develop an on-line learning algorithm in which the pa-rameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain. 
355|Modeling the manifolds of images of handwritten digits|description length, density estimation.
356|Dimension Reduction by Local Principal Component Analysis|Reducing or eliminating statistical redundancy between the components of high-dimensional vector data enables a lower-dimensional representation without significant loss of information. Recognizing the limitations of principal component analysis (PCA), researchers in the statistics and neural network communities have developed nonlinear extensions of PCA. This article develops a local linear approach to dimension reduction that provides accurate representations and is fast to compute. We exercise the algorithms on speech and image data, and compare performance with PCA and with neural network implementations of nonlinear PCA. We find that both nonlinear techniques can provide more accurate representations than PCA and show that the local linear techniques outperform neural network implementations. 
357|A Hierarchical Latent Variable Model for Data Visualization|Visualization has proven to be a powerful and widely-applicable tool for the analysis and interpretation of multi-variate data. Most visualization algorithms aim to find a projection from the data space down to a two-dimensional visualization space. However, for complex data sets living in a high-dimensional space it is unlikely that a single two-dimensional projection can reveal all of the interesting structure. We therefore introduce a hierarchical visualization algorithm which allows the complete data set to be visualized at the top level, with clusters and sub-clusters of data points visualized at deeper levels. The algorithm is based on a hierarchical mixture of latent variable models, whose parameters are estimated using the expectation-maximization algorithm. We demonstrate the principle of the approach on a toy data set, and we then apply the algorithm to the visualization of a synthetic data set in 12 dimensions obtained from a simulation of multi-phase flows in oil pipelines,...
358|A Novelty Detection Approach to Classification|Novelty Detection techniques are conceptlearning methods that proceed by recognizing positive instances of a concept rather than differentiating between its positive and negative instances. Novelty Detection approaches consequently require very few, if any, negative training instances. This paper presents a particular Novelty Detection approach to classification that uses a Redundancy Compression and Non-Redundancy Differentiation technique based on the [Gluck &amp; Myers, 1993] model of the hippocampus, a part of the brain critically involved in learning and memory. In particular, this approach consists of training an autoencoder to reconstruct positive input instances at the output layer and then using this autoencoder to recognize novel instances. Classification is possible, after training, because positive instances are expected to be reconstructed accurately while negative instances are not. The purpose of this paper is to compare HIPPO, the system that implements this technique, to C4.5 and feedforward neural network classification on several applications. 1
359|Principal Curves Revisited|A principal curve (Hastie and Stuetzle, 1989) is a smooth curve passing through the &#034;middle&#034; of a distribution or data cloud, and is a generalization of linear principal components. We give an alternative definition of a principal curve, based on a mixture model. Estimation is carried out through an EM algorithm. Some comparisons are made to the Hastie- Stuetzle definition.
360|Recognizing handwritten digits using mixtures of linear models|We construct a mixture of locally linear generative models of a collection of pixel-based images of digits, and use them for recognition. Different models of a given digit are used to capture different styles of writing, and new images are classified by evaluating their log-likelihoods under each model. We use an EM-based algorithm in which the M-step is computationally straightforward principal components analysis (PCA). Incorporating tangent-plane information [12] about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the PCA, and it demonstrably improves performance. 1
361|Nonlinear image interpolation using manifold learning|The problem of interpolating between specified images in an image sequence is a simple, but important task in model-based vision. We describe an approach based on the abstract task of &#034;manifold learning &#034; and present results on both synthetic and real image se quences. This problem arose in the development of a combined lip-reading and speech recognition system. 1
