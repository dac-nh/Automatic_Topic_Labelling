ID|Title|Summary
1|Chaining Layered Integrity Checks|Chaining Layered Integrity Checks  William A. Arbaugh  Jonathan M. Smith  &#034;Dovery, no Provery&#034;  &#034;Trust, but Verify&#034;  In a system, the integrity of lower layers is typically treated as axiomatic by higher layers. Under the presumption that the hardware comprising the system (the lowest layer) is valid, the integrity of a layer can be guaranteed if and only if: (1) the integrity of the lower layers is checked, and (2) transitions to higher layers occur only after integrity checks on them are complete. The resulting integrity &#034;chain&#034; inductively guarantees system integrity.
2|New Directions in Cryptography|Two kinds of contemporary developments in cryptography are examined. Widening applications of teleprocessing have given rise to a need for new types of cryptographic systems, which minimize the need for secure key distribution channels and supply the equivalent of a written signature. This paper suggests ways to solve these currently open problems. It also discusses how the theories of communication and computation are beginning to provide the tools to solve cryptographic problems of long standing.
3|Proof-Carrying Code|This paper describes proof-carrying code (PCC), a mechanism by which a host system can determine with certainty that it is safe to execute a program supplied (possibly in binary form) by an untrusted source. For this to be possible, the untrusted code producer must supply with the code a safety proof that attests to the code&#039;s adherence to a previously defined safety policy. The host can then easily and quickly validate the proof without using cryptography and without consulting any external agents. In order to gain preliminary experience with PCC, we have performed several case studies. We show in this paper how proof-carrying code might be used to develop safe assembly-language extensions of ML programs. In the context of this case study, we present and prove the adequacy of concrete representations for the safety policy, the safety proofs, and the proof validation. Finally, we briefly discuss how we use proof-carrying code to develop network packet filters that are faster than similar filters developed using other techniques and are formally guaranteed to be safe with respect to a given operating system safety policy. 
5|HMAC: Keyed-Hashing for Message Authentication|This memo provides information for the Internet community. This memo does not specify an Internet standard of any kind. Distribution of this memo is unlimited. This document describes HMAC, a mechanism for message authentication using cryptographic hash functions. HMAC can be used with any iterative cryptographic hash function, e.g., MD5, SHA-1, in combination with a secret shared key. The cryptographic strength of HMAC depends on the properties of the underlying hash function. 1.
6|Safe Kernel Extensions Without Run-Time Checking |Abstract This paper describes a mechanism by which an operating system kernel can determine with certainty that it is safe to execute a binary supplied by an untrusted source. The kernel first defines a safety policy and makes it public. Then, using this policy, an application can provide binaries in a special form called proof-carrying code, or simply PCC. Each PCC binary contains, in addition to the native code, a formal proof that the code obeys the safety policy. The kernel can easily validate the proof without using cryptography and without consulting any external trusted entities. If the validation succeeds, the code is guaranteed to respect the safety policy without relying on run-time checks. The main practical difficulty of PCC is in generating the safety proofs. In order to gain some preliminary experience with this, we have written several network packet filters in hand-tuned DEC Alpha assembly language, and then generated PCC binaries for them using a special prototype assembler. The PCC binaries can be executed with no run-time overhead, beyond a one-time cost of 1 to 3 milliseconds for validating the enclosed proofs. The net result is that our packet filters are formally guaranteed to be safe and are faster than packet filters created using Berkeley Packet Filters, Software Fault Isolation, or safe languages such as Modula-3.
7|Ants: A toolkit for building and dynamically deploying network protocols|We present a novel approach to building and deploying network protocols. The approach is based on mobile code, demand loading, and caching techniques. The architecture of our system allows new protocols to be dynamically deployed at both routers and end systems, without the need forcoordination and without unwanted interaction between co-existing protocols. In this paper, we describe our architecture and its realization in a prototype implementation. To demonstrate how to exploit our architecture, we present two simple protocols that operate within our prototype to introduce multicast and mobility services into a network that initially lacks them. 1
8|Designing Programs That Check Their Work|A program correctness checker is an algorithm for checking the output of a computation. That is, given a program and an instance on which the program is run, the checker certifies whether the output of the program on that instance is correct. This paper defines the concept of a program checker. It designs program checkers for a few specific and carefully chosen problems in the class FP of functions computable in polynomial time. Problems in FP for which checkers are presented in this paper include Sorting, Matrix Rank and GCD. It also applies methods of modern cryptography, especially the idea of a probabilistic interactive proof, to the design of program checkers for group theoretic computations. Two strucural theorems are proven here. One is a characterization of problems that can be checked. The other theorem establishes equivalence classes of problems such that whenever one problem in a class is checkable, all problems in the class are checkable.
9|A Secure and Reliable Bootstrap Architecture|In a computer system, the integrity of lower layers is typically treated as axiomatic by higher layers. Under the presumption that the hardware comprising the machine (the lowest layer) is valid, integrity of a layer can be guaranteed if and only if.&#039; (1) the integrity of the lower layers is checked, and (2) transitions to higher layers occur only ter integrity checks on them are complete. The resulting integrity &#034;chain&#034; inductively guarantees system integrity. When these
10|Authentication and Authenticated Key Exchanges|We discuss two-party mutual authentication protocols providing authenticated key exchange, focusing on those using asymmetric techniques. A simple, efficient protocol referred to as the station-to-station (STS) protocol is introduced, examined in detail, and considered in relation to existing protocols. The definition of a secure protocol is considered, and desirable characteristics of secure protocols are discussed. 
11|The design and implementation of Tripwire: A file system integrity checker|At the heart of most computer systems is a file system. The file system contains user data, executable programs, configuration and authorization information, and (usually) the base executable version of the operating system itself. The ability to monitor file systems for unauthorized or unexpected changes gives system administrators valuable data for protecting and maintaining their systems. However, in environments of many networked heterogeneous platforms with different policies and software, the task of monitoring changes becomes quite daunting. Tripwire is tool that aids UNIX system administrators and users in monitoring a designated set of files and directories for any changes. Used with system files on a regular (e.g., daily) basis, Tripwire can notify system administrators of corrupted or altered files, so corrective actions may be taken in a timely manner. Tripwire may also be used on user or group files or databases to signal changes. This paper describes the design and implementation of the Tripwire tool. It uses interchangeable &#034;signature&#034; routines to identify changes in files, and is highly configurable. Tripwire is no-cost software, available on the Internet, and is currently in use on thousands of machines around the world. 
12|Using Secure Coprocessors|The views and conclusions in this document are those of the authors and do not necessarily represent the official policies or endorsements of any of the research sponsors. How do we build distributed systems that are secure? Cryptographic techniques can be used to secure the communications between physically separated systems, but this is not enough: we must be able to guarantee the privacy of the cryptographic keys and the integrity of the cryptographic functions, in addition to the integrity of the security kernel and access control databases we have on the machines. Physical security is a central assumption upon which secure distributed systems are built; without this foundation even the best cryptosystem or the most secure kernel will crumble. In this thesis, I address the distributed security problem by proposing the addition of a small, physically secure hardware module, a secure coprocessor, to standard workstations and PCs. My central axiom is that secure coprocessors are able to maintain the privacy of the data they process. This thesis attacks the distributed security problem from multiple sides. First, I analyze the security properties of existing system components, both at the hardware and
13|The SwitchWare Active Network Architecture|Active networks must balance the flexibility of a programmable network infrastructure against the safety and security requirements inherent in sharing that infrastructure. Furthermore, this balance must be achieved while maintaining the usability of the network. The SwitchWare active network architecture is a novel approach to achieving this balance using three layers: active packets, which contain mobile programs that replace traditional packets; active extensions, which provide services on the network elements, and which can be dynamically loaded, and; a secure active router infrastructure, which forms a high integrity base upon which the security of the other layers depends. In addition to integrity-checking and cryptography-based authentication, security in our architecture depends heavily on verification techniques from programming languages, such as strong type checking.
14|A Taxonomy of Replay Attacks|This paper presents a taxonomy of replay attacks on cryptographic protocols in terms of message origin and destination. The taxonomy is independent of any method used to analyze or prevent such attacks. It is also complete in the sense that any replay attack is composed entirely of elements classified by the taxonomy. The classification of attacks is illustrated using both new and previously known attacks on protocols. The taxonomy is also used to discuss the appropriateness of particular countermeasures and protocol analysis methods to particular kinds of replays. Introduction Cryptographic protocols employ cryptography to achieve some security function. But, for many of these protocols the structure, hence the security, of the employed cryptographic algorithms is not considered to be part of the protocol itself. These algorithms are generically represented by notation capturing only gross features, e.g., whether the algorithm is for public or shared keys, whether it produces a hash...
15|Dyad: A System for Using Physically Secure Coprocessors|The Dyad project at Carnegie Mellon University is using physically secure coprocessors to achieve new protocols and systems addressing a number of perplexing security problems. These coprocessors can be produced as boards or integrated circuit chips and can be directly inserted in standard workstations or PC-style computers. This paper presents a set of security problems and easily implementable solutions that exploit the power of physically secure coprocessors: (1) protecting the integrity of publicly accessible workstations, (2) tamper-proof accounting/audit trails, (3) copy protection, and (4) electronic currency without centralized servers. We outline the architectural requirements for the use of secure coprocessors.  1 Introduction and Motivation  The Dyad project at Carnegie Mellon University is using physically secure coprocessors to achieve new protocols and systems addressing a number of perplexing security problems. These coprocessors can be produced as boards or integrated ...
16|Fail-Stop Protocols: An Approach to Designing Secure Protocols|This paper presents a methodology to facilitate the design and analysis of secure cryptographic protocols. This work is based on a novel notion of a fail-stop protocol, which automatically halts in response to any active attack. This paper suggests types of protocols that are fail-stop, outlines some proof techniques for them, and uses examples to illustrate how the notion of a failstop protocol can make protocol design easier and can provide a more solid basis for some proposed protocol analysis methods.
17|Experiences with Tripwire: Using Integrity Checkers for Intrusion Detection|Tripwire is an integrity checking program written for the UNIX environment. It gives system administrators the ability to monitor file systems for added, deleted, and modified files. Intended to aid intrusion detection, Tripwire was officially released on November 2, 1992. It is being actively used at thousands of sites around the world. Published in volume 26 ofcomp.sources.unix on the USENET and archived at numerous FTP sites around the world, Tripwire is widely available and widely distributed. It is recommended by various computer security response teams, including the CERT and CIAC. This paper begins by motivating the need for an integrity checker by presenting a hypothetical situation any system administrator could face. An overview of Tripwire is then described, emphasizing the salient aspects of Tripwire configuration that supports its use at sites employing modern variants of the UNIX operating system. Experiences with how Tripwire has been used in “in the field ” are then presented, along with some conjectures on the prevalence and extent of system breakins. Novel uses of Tripwire and notable configurations of Tripwire are also presented.
18|  A Secure Active Network Environment Architecture -- Realization in SwitchWare |Active Networks is a network infrastructure which is programmable on a per-user or even per-packet basis. Increasing the flexibility of such network infrastructures invites new security risks. Coping with these security risks represents the most fundamental contribution of Active Network research. The security concerns can be divided into those which affect the network as a whole and those which affect individual elements. It is clear that the element problems must be solved first, as the integrity of networklevel solutions will be based on trust of the network elements. In this
19|A Security Risk of Depending on Synchronized Clocks|. Many algorithms or protocols, in particular cryptographic protocols such as authentication protocols, use synchronized clocks and depend on them for correctness. This note describes a scenario where a clock synchronization failure renders a protocol vulnerable to an attack even after the faulty clock has been resynchronized. The attack exploits a postdated message by first suppressing it and replaying it later. 1 Introduction  Synchronized clocks have become a reality in distributed systems. Many algorithms or protocols use them to improve performance; some depend on them for correctness [7]. This note is particularly concerned with cryptographic protocols, such as some authentication protocols, which depend on synchronized clocks to timestamp messages so that the recipients can verify the timeliness of the messages and recognize and reject replays of messages communicated in the past [2, 4, 8]. Clocks can become unsynchronized due to sabotage on or faults in the clocks or the synchr...
20|Authentication for DHCP Messages|This document specifies an Internet standards track protocol for the Internet community, and requests discussion and suggestions for improvements. Please refer to the current edition of the &#034;Internet Official Protocol Standards &#034; (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (2001). All Rights Reserved. This document defines a new Dynamic Host Configuration Protocol (DHCP) option through which authorization tickets can be easily generated and newly attached hosts with proper authorization can be automatically configured from an authenticated DHCP server. DHCP provides a framework for passing configuration information to hosts on a TCP/IP network. In some situations, network administrators may wish to constrain the allocation of addresses to authorized hosts. Additionally, some network administrators may wish to provide for authentication of the source and contents of DHCP messages. 1.
21|The Operating System Kernel as a Secure Programmable Machine|To provide modularity and performance, operating system kernels should have only minimal embedded functionality. Today&#039;s operating systems are large, inefficient and, most importantly, inflexible. In our view, most operating system performance and flexibility problems can be eliminated simply by pushing the operating system interface lower. Our goal is to put abstractions traditionally implemented by the kernel out into user-space, where user-level libraries and servers abstract the exposed hardware resources. To achieve this goal, we have defined a new operating system structure, exokernel, that safely exports the resources defined by the underlying hardware. To enable applications to benefit from full hardware functionality and performance, they are allowed to download additions to the supervisor-mode execution environment. To guarantee that these extensions are safe, techniques such as code inspection, inlined cross-domain procedure calls, and secure languages are used. To test and ...
22|A Generalized Computing Model of Active Networks|Introduction  As network connectivity grows throughout the world, the uses of the network also grow. The current network infrastructure cannot and does not keep up with this increase in protocols. We believe that the &#034;Active Networks&#034; approach will help to deal with this problem. Active Networks are those networks in which a node of the network can undergo state changes and can provide information about its current state through execution of programs. Thus, it becomes possible to deploy a new protocol by loading it into a switch or for a packet to attempt to find an optimal route by discovering the state of the switch through which it is currently traveling. In the SwitchWare architecture for active networks, each node is capable of executing programs written in Caml, a dialect of ML.  1.1 Problem Statement  We see several problems with existing networking technologies, specifically the Internet technology. First, as the Internet has moved from an experimental n
23|The Birlix security architecture|Asecurity architecture isacomplete framework that allows to enforce coexisting application and environment speci c security policies by applying security mechanisms in a consistent manner. It is argued that such a security architecture can be derived smoothly from an object-oriented architecture such as BirliX. BirliX applications run on an abstract machine that provides types and instances. Applications are clusters of instances that invoke methods on each other. The BirliX Security Architecture provides means to control the invocations (access control) and to control the enforcement of the abstract machine (infrastructure). Both, access control and infrastructure, are controlled by users respectively systems administrators to support application dependent security policies. 1
24|Automated Recovery in a Secure Bootstrap Process|Integrity is rarely a valid presupposition in many systems architectures, yet it is necessary to make any security guarantees. To address this problem, we have designed a secure bootstrap process, AEGIS, which presumes a minimal amount of integrity, and which we have prototyped on the Intel x86 architecture. The basic principle is sequencing the bootstrap process as a chain of progressively higher levels of abstraction, and requiring each layer to check a digital signature of the next layer before control is passed to it. A major design decision is the consequence of a failed integrity check. A simplistic strategy is to simply halt the bootstrap process. However, as we show in this paper, the AEGIS bootstrap process can be augmented with automated recovery procedures which preserve the security properties of AEGIS under the additional assumption of the availability of a trusted repository. We describe two means by which such a repository can be implemented, and focus our attention on a network-accessible repository.
25|Attack Class: Address Spoofing|We present an analysis of a class of attacks we call address spoofing. Fundamentals of internetwork routing and communication are presented, followed by a discussion of the address spoofing class. The attack class is made concrete with a discussion of a well known incident. We conclude by dispelling several myths of purported security solutions including the security provided by one-time passwords. 1
26|Engineering a Security Kernel for MULTICS|This paper describes a research project to engineer a security kernel for Multics, a general-purpose, remotely accessed, multiuser computer system. The goals are to identify the minimum mechanism that must be correct to guarantee computer enforcement of desired constraints on information access, to simplify the structure of that minimum mechanism to make verification of correctness by auditing possible, and to demonstrate by test&#039;implementation that the security kernel so developed is capable of supporting the functionality of Multics completely and efficiently. The paper presents the overall viewpoint and plan for the project and discusses initial strategies being employed to define and structure the security kernel.
27|The influence of software structure on reliability|This paper assumes software structure to be characterized by the interfaces between subsystems or modules. Reliability is considered to be a measure of the extent to which the system can be expected to deliver usa-ble services when those services are demanded. It is argued that reliability and correctness (in the sense used in current computer literature) are not synonyms. The differences suggest techniques by which the re-liability of software can be improved even while the production of correct software remains beyond our reach. In general, the techniques involve considering certain unpleasant facts of life at an early stage in the de-sign process, the stage where the structure is determined, rather than later. An appendix gives some speci-fic examples of questions which, if they are thoughtfully considered early in the design, can lead to more reliable systems.
29|Transaction management with integrity checking |Abstract. Database integrity constraints, understood as logical conditions that must hold for any database state, are not fully supported by current database technology. It is typically up to the database designer and application programmer to enforce integrity via triggers or tests at the application level, which are difficult to maintain and error prone. Two important aspects must be taken care of. 1. It is too time consuming to check integrity constraints from scratch after each update, so simplified checks before each update should be used relying on the assumption that the current state is consistent. 2. In concurrent database systems, besides the traditional correctness criterion, the execution schedule must ensure that the different transactions can overlap in time without destroying the consistency requirements tested by other, concurrent transactions. We show in this paper how to apply a method for incremental integrity checking to automatically extend update transactions with locks and simplified consistency tests on the locked elements. All schedules produced in this way are conflict serializable and preserve consistency in an optimized way. 1
30|Simplification of database integrity constraints revisited: A transformational approach|Abstract. Complete checks of database integrity constraints may be prohibitively time consuming, and several methods have been suggested for producing simplified checks for each update. The present approach introduces a set of transformation operators that apply to database in-tegrity constraints with each operator representing a concise, semantics-preserving operation. These operators are applied in a procedure produc-ing simplified constraints for parametric transaction patterns, which then can be instantiated and checked for consistency at run-time but before any transaction is executed. The operators provide a flexibility for other database enhancements and the work may also be seen as more system-atic and general when compared with other approaches. The framework is formulated with first-order clause logic but with the perspective of being applied with present-day database technology. 1
32|Optimal database locks for efficient integrity checking|Abstract. In concurrent database systems, correctness of update transactions refers to the equivalent effects of the execution schedule and some serial schedule over the same set of transactions. Integrity constraints add further semantic requirements to the correctness of the database states reached upon the execution of update transactions. Several methods for efficient integrity checking and enforcing exist. We show in this paper how to apply one such method to automatically extend update transactions with locks and simplified consistency tests on the locked entities. All schedules produced in this way are conflict serializable and preserve consistency. For certain classes of databases we also guarantee that the amount of locked database entities is minimal. 1
33|Integrity Checking in Deductive Databases|We describe the theory and implementation of a general theorem-proving technique for checking integrity of deductive databases recently proposed by Sadri and Kowalski. The method uses an extension of the SLDNF proof procedure and achieves the effect of the simplification algorithms of Nicolas, Lloyd, Topor et al, and Decker by reasoning forwards from the update and thus focusing on the relevant parts of the database and the relevant constraints.
34|A Proof Procedure Using Connection Graphs|  Various deficiencies of resolution systems are investigated and a new theorem-proving system designed to remedy those deficiencms is presented The system is notable for eliminating redundancies present in SL-resolutlon, for incorporating preprocessing procedures, for liberahzing the order in which subgoals can be activated, for incorporating multidirectmnal searches, and for giving immediate access to pairs of clauses which resolve Examples of how the new system copes with the deficiencies of other theorem-proving systems are chosen from the areas of predicate logic programming and language parsing. The paper emphasizes the historical development of the new system, beginning as a supplement to SL-resolution in the form of classification trees and incorporating an analogue of the Waltz algorithm for picture Interpretation The paper ends with a discussion of the opportunities for using look-ahead to guide the search for proofs
35|Integrity Checking For Process Hardening|Computer intrusions can occur in various ways. Many of them occur by exploiting program flaws and system configuration errors. Existing solutions that detects specific kinds of flaws are substantially different from each other, so aggregate use of them may be incompatible and require substantial changes in the current system and computing practice. Intrusion detection systems may not be the answer either, because they are inherently inaccurate and susceptible to false positives/negatives. This dissertation presents a taxonomy of security flaws that classifies program vulnerabilities into finite number of error categories, and presents a security mechanism that can produce accurate solutions for many of these error categories in a modular fashion. To be accurate, a solution should closely match the characteristic of the target error category. To ensure this, we focus only on error categories whose characteristics can be defined in terms of a violation of process integrity. The thesis of this work is that the proposed approach produces accurate solutions for many error categories. To prove the accuracy of produced solutions, we define the process integrity checking approach and analyze its properties. To prove that this approach can cover many error categories, we develop a classification of program security flaws and find error characteristics (in terms of a process integrity) from many of these categories. We
36|Perspectives on Program Analysis|eing analysed. On the negative side, the semantic correctness of the analysis is seldom established and therefore there is often no formal justification for the program transformations for which the information is used.  The semantics based approach [1; 5] is often based on domain theory in the form of abstract domains modelling sets of values, projections, or partial equivalence relations. The approach tends to focus more directly on discovering the extensional properties of interest: for constant propagation it might operate on sets of values with constancy corresponding to singletons, and for neededness analysis it might perform a strictness analysis and use the strictness information for neededness (or make use of the &#034;absence&#034; notion from projection analysis and attempt to discover the di#erence). On the positive side, this usually gives rise to provably correct analyses, although there are sometimes complications (due to deciding what information to stick onto the
37|Timing Attacks on Implementations of Diffie-Hellman, RSA, DSS, and Other Systems|By carefully measuring the amount of time required to perform private key operations, attackers may be able to find fixed Diffie-Hellman exponents, factor RSA keys, and break other cryptosystems. Against a vulnerable system, the attack is computationally inexpensive and often requires only known ciphertext. Actual systems are potentially at risk, including cryptographic tokens, network-based cryptosystems, and other applications where attackers can make reasonably accurate timing measurements. Techniques for preventing the attack for RSA and Diffie-Hellman are presented. Some cryptosystems will need to be revised to protect against the attack, and new protocols and algorithms may need to incorporate measures to prevent timing attacks.
38|StackGuard: Automatic adaptive detection and prevention of buffer-overflow attacks|1
39|Points-to Analysis in Almost Linear Time|We present an interprocedural flow-insensitive points-to analysis based on type inference methods with an almost linear time cost complexity. To our knowledge, this is the asymptotically fastest non-trivial interprocedural points-to analysis algorithm yet described. The algorithm is based on a non-standard type system. The type inferred for any variable represents a set of locations and includes a type which in turn represents a set of locations possibly pointed to by the variable. The type inferred for a function variable represents a set of functions it may point to and includes a type signature for these functions. The results are equivalent to those of a flow-insensitive alias analysis (and control flow analysis) that assumes alias relations are reflexive and transitive. This work makes
40|Cyclone: A safe dialect of C | Cyclone is a safe dialect of C. It has been designed from the ground up to prevent the buffer overflows, format string attacks, and memory management errors that are common in C programs, while retaining C&#039;s syntax and semantics. This paper examines safety violations enabled by C&#039;s design, and shows how Cyclone avoids them, without giving up C&#039;s hallmark control over low-level details such as data representation and memory management.
41|A First Step towards Automated Detection of Buffer Overrun Vulnerabilities|We describe a new technique for finding potential buffer overrun vulnerabilities in security-critical C code. The key to success is to use static analysis: we formulate detection of buffer overruns as an integer range analysis problem. One major advantage of static analysis is that security bugs can be eliminated before code is deployed. We have implemented our design and used our prototype to find new remotely-exploitable vulnerabilities in a large, widely deployed software package. An earlier hand audit missed these bugs.  
42|CCured: Type-Safe Retrofitting of Legacy Code|In this paper we propose a scheme that combines type inference and run-time checking to make existing C programs type safe. We describe the CCured type system, which extends that of C by separating pointer types according to their usage. This type system allows both pointers whose usage can be verified statically to be type safe, and pointers whose safety must be checked at run time. We prove a type soundness result and then we present a surprisingly simple type inference algorithm that is able to infer the appropriate pointer kinds for existing C programs. Our experience with the CCured system shows that the inference is very effective for many C programs, as it is able to infer that most or all of the pointers are statically verifiable to be type safe. The remaining pointers are instrumented with efficient run-time checks to ensure that they are used safely. The resulting performance loss due to run-time checks is 0–150%, which is several times better than comparable approaches that use only dynamic checking. Using CCured we have discovered programming bugs in established C programs such as several SPECINT95 benchmarks.
43|State Transition Analysis: A Rule-Based Intrusion Detection Approach|This paper presents a new approach to representing and detecting computer penetrations in real-time. The approach, called state transition analysis, models penetrations as a series of state changes that lead from an initial secure state to a target compromised state. State transition diagrams, the graphical representation of penetrations, identify precisely the requirements for and the compromise of a penetration and present only the critical events that must occur for the successful completion of the penetration. State transition diagrams are written to correspond to the states of an actual computer system, and these diagrams form the basis of a rule-based expert system for detecting penetrations, called the State Transition Analysis Tool (STAT). The design and implementation of a UNIX-specific prototype of this expert system, called USTAT, is also presented. This prototype provides a further illustration of the overall design and functionality of this intrusion detection approach. Lastly, STAT is compared to the functionality of comparable intrusion detection tools.
44|Efficient Detection of All Pointer and Array Access Errors|We present a pointer and array access checking technique that provides complete error coverage through a simple set of program transformations. Our technique, based on an extended safe pointer representation, has a number of novel aspects. Foremost, it is the first technique that detects all spatial and temporal access errors. Its use is not limited by the expressiveness of the language; that is, it can be applied successfully to compiled or interpreted languages with subscripted and mutable pointers, local references, and explicit and typeless dynamic storage management, e.g., C. Because it is a source level transformation, it is amenable to both compile- and run-time optimization. Finally, its performance, even without compile-time optimization, is quite good. We implemented a prototype translator for the C language and analyzed the checking overheads of six non-trivial, pointer intensive programs. Execution overheads range from 130 % to 540%; with text and data size overheads typically below 100%. 
45|Remote Timing Attacks are Practical|Timing attacks are usually used to attack weak computing devices such as smartcards. We  show that timing attacks apply to general software systems. Specifically, we devise a timing  attack against OpenSSL. Our experiments show that we can extract private keys from an  OpenSSL-based web server running on a machine in the local network. Our results demonstrate  that timing attacks against network servers are practical and therefore all security systems  should defend against them.
46|Backwards-compatible bounds checking for arrays and pointers in C programs|function-typed variables, virtual functions, and 7/7 call-backs. 8/8 Maintain shadow bitmap:  Maintain a map indicating which storage regions are valid. Update it when stack allocations,  malloc  and  free  occur. Augment each memory access instruction with code to check whether the address is valid [Hastings and Joyce, 1992].   Advantages:  Fairly ecient Doesn&#039;t require access to source code, so can (must) be applied to all constituents of application    False negatives - fails to ag accesses to a valid region using an 9/9 improperly-derived pointer 10/10 Summarise requirements:  Track intended referent for each pointer  It is not good enough just to check that accesses are to valid locations  No change to pointer representation  In order to inter-operate with unchecked code without restriction, no information can be bundled with the pointer. 11/11 How to do it . . . 3: the central idea  Invariant: Assume all stored pointers are properly-derived pointers to their intended referent Im
48|A PATTERN MATCHING MODEL FOR MISUSE INTRUSION DETECTION  |This paper describes a generic model of matching that can be usefully applied to misuse intrusion detection. The model is based on Colored Petri Nets. Guards define the context in which signatures are matched. The notion of start and final states, and paths between them define the set of event sequences matched by the net. Partial order matching can also be specified in this model. The main benefits of the model are its generality, portability and flexibility. 
49|A Taxonomy of Computer Program Security Flaws, with Examples|This paper provides a taxonomy for computer program security flaws together with an appendix that carefully documents 50 actual security flaws. These flaws have all been described previously in the open literature, but in widely separated places. For those new to the field of computer security, they provide a good introduction to the characteristics of security flaws and how they can arise. Because these flaws were not randomly selected from a valid statistical sample of such flaws, we make no strong claims concerning the likely distribution of actual security flaws within the taxonomy. However, this method of organizing security flaw data can help those who have custody of more representative samples to organize them and to focus their efforts to remove and, eventually, to prevent the introduction of security flaws. Categories and Subject Descriptors: D.4.6[Operating Systems]:Security and Protection---access
50|FormatGuard: Automatic Protection From printf Format String Vulnerabilities|Symposium
51|Checking for Race Conditions in File Accesses|Flaws due to race conditions in which the binding of a name to an object changes between repeated references occur in many programs. We examine one type of this flaw in the UNIX operating system, and describe a semantic method for detecting possible instances of this problem. We present the results of one such analysis in which a previously undiscovered race condition flaw was found. 
52|Automated Detection of Vulnerabilities in Privileged Programs by Execution Monitoring|We present a method for detecting exploitations of vulnerabilities in privileged programs by monitoring their execution using audit trials, where the monitoring is with respect to specifications of the security-relevant behavior of the programs. Our work is motivated by the intrusion detection paradigm, but is an attempt to avoid ad hoc approaches to codifying misuse behavior. Our approach is based on the observation that although privileged programs can be exploited (due to errors) to cause security compromise in systems because of the privileges accorded to them, the intended behavior of privileged programs is, of course, limited and benign. The key, then is to specify the intended behavior (i.e., the program policy) and to detect any action by privileged program that is outside the intended behavior and that imperils security. We describe a program policy specification language, which is based on simple predicate logic and regular expressions. In addition, we present specifications ...
53|LCLint: A Tool for Using Specifications to Check Code|This paper describes LCLint, an efficient and flexible tool that accepts as input programs (written in ANSI C) and various levels of formal specification. Using this information, LCLint reports inconsistencies between a program and its specification. We also describe our experience using LCLint to help understand, document, and re-engineer legacy code. Keywords: C, Larch, LCLint, lint, specifications, static checking 1 Introduction Software engineers have long understood that static analysis of program texts can both reduce the number of residual errors andimprove the maintainability of programs. Traditional static checkers [10, 20] detect type errors and simple anomalies such as obviously uninitialized variables. These checkers demand little effort from the user, and are frequently used. However, their utility is limited by their lack of information about the intent of the programmer. At the other extreme are program verification systems [2]. They are used to demonstrate that a prog...
54|What are race conditions? some issues and formalizations|In shared-memory parallel programs that use explicit synchronization, race conditions result when accesses to shared memory are not properly synchronized. Race conditions are often considered to be manifestations of bugs, since their presence can cause the program to behave unexpectedly, Unfortunately, there has been little agreement in the literature as to precisely what constitutes a race condition. Two different notions have been implicitly considered: one pertaining to programs intended to be deterministic (which we call general races) and the other to nondeterministic programs containing critical sections (which we call data races). However, the differences between general races and data races have not yet been recognized. This paper examines these differences by characterizing races using a formal model and exploring their properties. We show that two variations of each type of race exist: feasible general races and data races capture the intuitive notions desired for debugging and apparent races capture less accurate notions implicitly assumed by most dynamic race detection methods. We also show that locating feasible races is an NP-hard problem, implying that only the apparent races, which are approximations to feasible races, can be detected in practice. The complexity of dynamically locating apparent races depends on the type of synchronization used by the program, Apparent
55|Security for Mobile Agents: Authentication and State Appraisal|. Mobile agents are processes which can autonomously migrate  to new hosts. Despite its many practical benets, mobile agent technology  results in signicant new security threats from malicious agents and  hosts. The primary added complication is that, as an agent traverses  multiple hosts that are trusted to dierent degrees, its state can change  in ways that adversely impact its functionality. In this paper, we discuss  achievable security goals for mobile agents, and we propose an architecture  to achieve these goals. The architecture models the trust relations  between the principals of mobile agent systems. A unique aspect of the  architecture is a \state appraisal&#034; mechanism that protects users and  hosts from attacks via state modications and that provides users with  exible control over the authority of their agents.  1 Introduction  Currently, distributed systems employ models in which processes are statically attached to hosts and communicate by asynchronous messages or s...
56|A Static Vulnerability Scanner for C and C++ Code|We describe ITS4, a tool for statically scanning security-critical C source code for vulnerabilities. Compared to other approaches, our scanning technique stakes out a new middle ground between accuracy and efficiency. This method is efficient enough to offer real-time feedback to developers during coding while producing few false negatives. Unlike other techniques, our method is also simple enough to scan C++ code despite the complexities inherent in the language. Using ITS4 we found new remotelyexploitable vulnerabilities in a widely distributed software package as well as in a major piece of e-commerce software. The ITS4 source distribution is available at http:  //www.rstcorp.com/its4.  1. 
57|Artificial Intelligence and Intrusion Detection: Current and Future Directions|Intrusion Detection systems (IDSs) have previously been built by hand. These systems have difficulty successfully classifying intruders, and require a significant amount of computational overhead making it difficult to create robust real-time IDS systems. Artificial Intelligence techniques can reduce the human effort required to build these systems and can improve their performance. Learning and induction are used to improve the performance of search problems, while clustering has been used for data analysis and reduction. AI has recently been used in Intrusion Detection (ID) for anomaly detection, data reduction and induction, or discovery, of rules explaining audit data. We survey uses of artificial intelligence methods in ID, and present an example using feature selection to improve the classification of network connections. The network connection classification problem is related to ID since intruders can create &#034;private&#034; communications services undetectable by normal means. We als...
58|Use of A Taxonomy of Security Faults|Security in computer systems is important so as to ensure reliable operation and to protect the integrity of stored information. Faults in the implementation of critical components can be exploited to breach security and penetrate a system. These faults must be identified, detected, and corrected to ensure reliability and safeguard against denial of service, unauthorized modification of data, or disclosure of information. We define a classification of security faults in the Unix operating system. We state the criteria used to categorize the faults and present examples of the different fault types. We present the design and implementation details of a prototype database to store vulnerability information collected from different sources. The data is organized according to our fault categories. The information in the database can be applied in static audit analysis of systems, intrusion detection, and fault detection. We also identify and describe software testing methods that should be effective in detecting different faults in our classification scheme.
59|Improving Computer Security using Extended Static Checking|We describe a method for finding security flaws in source code by way of static analysis. The method is notable because it allows a user to specify a wide range of security properties while also leveraging a set of predefined common flaws. It works by using an automated theorem prover to analyze verification conditions generated from C source code and a set of specifications that define security properties. We demonstrate that the method can be used to identify real vulnerabilities in real programs.
60|Janus: An Approach for Confinement of Untrusted Applications|Security is a serious concern on today&#039;s computer networks. Many applications are not very good at resisting attack, and our operating systems are not very good at preventing the spread of any intrusions that may result. In this thesis, we propose to manage the risk of a security breach by confining these untrusted (and untrustworthy) applications in a carefully sanitized space. We design a secure environment for confinement of untrusted applications by restricting the program&#039;s access to the operating system. In our prototype implementation, we intercept and filter dangerous system calls via the Solaris process tracing facility. This enables us to build a simple, clean, user-mode mechanism for con ning untrusted applications. Our implementation has negligible performance impact, and can protect pre-existing legacy code.
61|Type-Assisted Dynamic Buffer Overflow Detection|Programs written in C are inherently vulnerable to buffer overflow attacks. Functions are frequently passed pointers as parameters without any hint of their sizes. Since their sizes are unknown, most run time buffer overflow detection techniques instead rely on signatures of known attacks or loosely estimate the range of the referenced buffers. Although they are effective in detecting most attacks, they are not infallible. In this paper we present a buffer overflow detection technique that range checks the referenced buffers at run time. Our solution is a small extension to a generic C compiler that augments executable files with type information of automatic buffers (local variables and parameters of functions) and static buffers (global variables in data / bss section) in order to detect the actual occurrence of buffer overflow. It also maintains the sizes of allocated heap buffers. A simple implementation is described, with which we currently protect vulnerable copy functions in the C library.
62|Precise Flow-Insensitive May-Alias Analysis is NP-Hard|this paper we show that precise flow-insensitive may-alias analysis is NP-hard given arbitrary levels of pointers and arbitrary pointer dereferencing.
63|On Preventing Intrusions by Process Behavior Monitoring|Society&#039;s increasing reliance on networked information systems to support critical infrastructures has prompted interest in making the information systems survivable, so that they continue to perform critical functions even in the presence of vulnerabilities susceptible to malicious attacks. To enable vulnerable systems to survive attacks, it is necessary to detect attacks and isolate failures resulting from attacks before they damage the system by impacting functionality, performance or security. The key research problems in this context include: . detecting in-progress attacks before they cause  damage, as opposed to detecting attacks after  they have succeeded,  . localizing and/or minimizing damage by isolating  attacked components in real-time, and  . tracing the origin of attacks.
64|A Taxonomy of UNIX System and Network Vulnerabilities|Ambrose Bierce defined ``history&#039;&#039; as ``a record of mistakes made in the past, so we shall know when we make them again.&#039;&#039; Although sardonic, his definition describes the state of affairs of computer system vulnerabilities.
65|RaceGuard: Kernel Protection from Temporary File Race Vulnerabilities|Temporary file race vulnerabilities occur when privileged programs attempt to create temporary files in an unsafe manner. &#034;Unsafe&#034; means &#034;non-atomic with respect to an attacker&#039;s activities.&#034; There is no portable standard for safely (atomically) creating temporary files, and many operating systems have no safe temporary file creation at all. As a result, many programs continue to use unsafe means to create temporary files, resulting in widespread vulnerabilities. This paper presents RaceGuard: a kernel enhancement that detects attempts to exploit temporary file race vulnerabilities, and does so with sufficient speed and precision that the attack can be halted before it takes effect. RaceGuard has been implemented, tested, and measured. We show that RaceGuard is effective at stopping temporary file race attacks, preserves compatibility (no legitimate software is broken), and preserves performance (overhead is minimal).
66|Dynamic detection and prevention of race conditions in file accesses|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
67|Buffer Overflow and Format String Overflow Vulnerabilities|This article surveys representative techniques of exploiting bu#er overflow and format string overflow vulnerabilities and their currently available defensive measures. We also describe our bu#er overflow detection technique that range checks the referenced bu#ers at run time. We augment executable files with type information of automatic bu#ers (local variables and parameters of functions) and static bu#ers (global variables in the data/bss section), and maintain the sizes of allocated heap bu#ers in order to detect an actual occurrence of bu#er overflow. We describe a simple implementation with which we currently protect vulnerable copy functions in the C library
68|Categorization of software errors that led to security breaches|A set of errors known to have led to security breaches in computer systems was  analyzed. The analysis led to a categorization of these errors. After examining several  proposed schemes for the categorization of software errors a new scheme was developed  and used. This scheme classifies errors by their cause, the nature of their impact, and  the type of change, or fix, made to remove the error. The errors considered in this work  are found in a database maintained by the COAST laboratory. The categorization is the  first step in the investigation of the effectiveness of various measures of code coverage  in revealing software errors that might lead to security breaches.
69|Model Checking Programs|The majority of work carried out in the formal methods community throughout the last three decades has (for good reasons) been devoted to special languages designed to make it easier to experiment with mechanized formal methods such as theorem provers, proof checkers and model checkers. In this paper we will attempt to give convincing arguments for why we believe it is time for the formal methods community to shift some of its attention towards the analysis of programs written in modern programming languages. In keeping with this philosophy we have developed a verification and testing environment for Java, called Java PathFinder (JPF), which integrates model checking, program analysis and testing. Part of this work has consisted of building a new Java Virtual Machine that interprets Java bytecode. JPF uses state compression to handle big states, and partial order and symmetry reduction, slicing, abstraction, and runtime analysis techniques to reduce the state space. JPF has been applied to a real-time avionics operating system developed at Honeywell, illustrating an intricate error, and to a model of a spacecraft controller, illustrating the combination of abstraction, runtime analysis, and slicing with model checking.
70|Statecharts: A Visual Formalism For Complex Systems|We present a broad extension of the conventional formalism of state machines and state diagrams, that is relevant to the specification and design of complex discrete-event systems, such as multi-computer real-time systems, communication protocols and digital control units. Our diagrams, which we call statecharts, extend conventional state-transition diagrams with essentially three olements, dealing, respectively, with the notions of hierarchy, concurrency and communication. These transform the language of state diagrams into a highly structured&#039; and economical description language. Statecharts are thus compact and expressive--small diagrams can express complex behavior--as well as compositional and modular. When coupled with the capabilities of computerized graphics, statecharts enable viewing the description at different levels of detail, and make even very large specifications manageable and comprehensible. In fact, we intend to demonstrate here that statecharts counter many of the objections raised against conventional state diagrams, and thus appear to render specification by diagrams an attractive and plausible approach. Statecharts can be used either as a stand-alone behavioral description or as part of a more general design methodology that deals also with the system&#039;s other aspects, such as functional decomposition and data-flow specification. We also discuss some practical experience that was gained over the last three years in applying the statechart formalism to the specification of a particularly complex system.
71|An axiomatic basis for computer programming|In this paper an attempt is made to explore the logical founda-tions of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This in-volves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantages, both theoretical and prac-tical, may follow from a pursuance of these topics.
72|The model checker SPIN|Abstract—SPIN is an efficient verification system for models of distributed software systems. It has been used to detect design errors in applications ranging from high-level descriptions of distributed algorithms to detailed code for controlling telephone exchanges. This paper gives an overview of the design and structure of the verifier, reviews its theoretical foundation, and gives an overview of significant practical applications. Index Terms—Formal methods, program verification, design verification, model checking, distributed systems, concurrency.
73|Counterexample-guided Abstraction Refinement|We present an automatic iterative abstraction-refinement methodology  in which the initial abstract model is generated by an automatic analysis of  the control structures in the program to be verified. Abstract models may admit  erroneous (or &#034;spurious&#034;) counterexamples. We devise new symbolic techniques  which analyze such counterexamples and refine the abstract model correspondingly.
74|A Survey of Program Slicing Techniques|A program slice consists of the parts of a program that (potentially) affect the  values computed at some point of interest, referred to as a slicing criterion. The task  of computing program slices is called program slicing. The original definition of a  program slice was presented by Weiser in 1979. Since then, various slightly different  notions of program slices have been proposed, as well as a number of methods to  compute them. An important distinction is that between a static and a dynamic slice.  The former notion is computed without making assumptions regarding a program&#039;s  input, whereas the latter relies on some specific test case.  Procedures, arbitrary control flow, composite datatypes and pointers, and interprocess  communication each require a specific solution. We classify static and dynamic  slicing methods for each of these features, and compare their accuracy and efficiency.  Moreover, the possibilities for combining solutions for different features are investigated....
75|UPPAAL in a Nutshell|. This paper presents the overall structure, the design criteria, and the main features of the tool box Uppaal. It gives a detailed user guide which describes how to use the various tools of Uppaal version 2.02 to construct  abstract models of a real-time system, to simulate  its dynamical behavior, to specify and verify its safety and bounded liveness properties in terms of its model. In addition, the paper also provides a short review on case-studies where Uppaal is applied, as well as references to its theoretical foundation. 1 Introduction Uppaal is a tool box for modeling, simulation and verification of real-time systems, based on constraint--solving and on-the-fly techniques, developed jointly by Uppsala University and Aalborg University. It is appropriate for systems that can be modeled as a collection of nondeterministic processes with finite control structure and real-valued clocks, communicating through channels and (or) shared variables [34, 26]. Typical application areas in...
76|Bandera: Extracting Finite-state Models from Java Source Code|Finite-state verification techniques, such as model checking, have shown promise as a cost-effective means for finding defects in hardware designs. To date, the application of these techniques to software has been hindered by several obstacles. Chief among these is the problem of constructing a finite-state model that approximates the executable behavior of the software system of interest. Current best-practice involves handconstruction of models which is expensive (prohibitive for all but the smallest systems), prone to errors (which can result in misleading verification results), and difficult to optimize (which is necessary to combat the exponential complexity of verification algorithms).  In this paper, we describe an integrated collection of program analysis and transformation components, called Bandera, that enables the automatic extraction of safe, compact finite-state models from program source code. Bandera takes as input Java source code and generates a program model in the input language of one of several existing verification tools; Bandera also maps verifier outputs back to the original source code. We discuss the major components of Bandera and give an overview of how it can be used to model check correctness properties of Java programs.  
77|Model Checking for Programming Languages using VeriSoft|Verification by state-space exploration, also often referred to as &#034;model checking&#034;, is an effective method for analyzing the correctness of concurrent reactive systems (e.g., communication protocols). Unfortunately, existing model-checking techniques are restricted to the verification of properties of models, i.e., abstractions, of concurrent systems.  In this paper, we discuss how model checking can be extended to deal directly with &#034;actual&#034; descriptions of concurrent systems, e.g., implementations of communication protocols written in programming languages such as C or C++. We then introduce a new search technique that is suitable for exploring the state spaces of such systems. This algorithm has been implemented in VeriSoft, a tool for systematically exploring the state spaces of systems composed of several concurrent processes executing arbitrary C code. As an example of application, we describe how VeriSoft successfully discovered an error in a 2500-line C program controlling rob...
78|Model Checking Java Programs Using Java PathFinder|. This paper describes a translator called Java PathFinder (Jpf), from Java to Promela, the modeling language of the Spin model checker. Jpf translates a given Java program into a Promela model, which then can be model checked using Spin. The Java program may contain assertions, which are translated to similar assertions in the Promela model. The Spin model checker will then look for deadlocks and violations of any stated assertions. Jpf generates a Promela model with the same state space characteristics as the Java program. Hence, the Java program must have a finite and tractable state space. This work should be seen in a broader attempt to make formal methods applicable within NASA&#039;s areas such as space, aviation, and robotics. The work is a continuation of an effort to formally analyze, using Spin, a multi--threaded operating system for the Deep-Space 1 space craft, and of previous work in applying existing model checkers and theorem provers to real applications. Key words: Program...
79|Abstract interpretation frameworks|We introduce abstract interpretation frameworks which are variations on the archetypal framework using Galois connections between concrete and abstract semantics, widenings and narrowings and are obtained by relaxation of the original hypotheses. We consider various ways of establishing the correctness of an abstract interpretation depending on how the relation between the concrete and abstract semantics is defined. We insist upon those correspondences allowing for the inducing of the approximate abstract semantics from the concrete one. Furthermore we study various notions interpretation.
80|Bebop: A Symbolic Model Checker for Boolean Programs|We present the design, implementation and empirical evaluation of Bebop -- a symbolic model checker for boolean programs. Bebop represents control flow explicitly, and sets of states implicitly using BDDs. By harnessing the inherent modularity in procedural abstraction and exploiting the locality of variable scoping, Bebop is able to model check boolean programs with several thousand lines of code, hundreds of procedures, and several thousand variables in a few minutes.
81|PVS: Combining Specification, Proof Checking, and Model Checking|rem Proving and Typechecking  The PVS specification language is based on classical, simply typed higher-order logic, but the type system has been augmented with subtypes and dependent types. Though typechecking is undecidable for the PVS type system, the PVS typechecker automatically checks for simple type correctness and generates proof obligations corresponding to predicate subtypes. These proof obligations can be discharged through the use of the PVS proof checker. PVS also has parametric theories so that it is possible to capture, say, the notion of sorting with respect to arbitrary sizes, types, and ordering relations. By exploiting subtyping, dependent typing, and parametric theories, researchers at NASA Langley Research Center and SRI have developed a very general bit-vector library. Paul Miner at NASA ?  The development of PVS was funded by SRI International through IR&amp;D funds. Various applications and customizations have been funded by NSF Grant CCR9300
82|Boolean and Cartesian Abstraction for Model Checking C Programs|The problem of model checking a specification in form of a C program with recursive procedures and many thousands of lines of code has not been addressed before. In this paper, we show how we attack this problem using an abstraction that is formalized with the Cartesian abstraction. It is implemented through a source-to-source transformation into a `Boolean&#039; C program; we give an algorithm to compute the transformation with a cost that is exponential in its theoretical worst-case complexity but feasible in practice.
83|Validity Checking for Combinations of Theories with Equality|. An essential component in many verification methods is a fast decision procedure for validating logical expressions. This paper presents the algorithm used in the Stanford Validity Checker (SVC) which has been used to aid several realistic hardware verification efforts. The logic for this decision procedure includes Boolean and uninterpreted functions and linear arithmetic. We have also successfully incorporated other interpreted functions, such as array operations and linear inequalities. The primary techniques which allow a complete and efficient implementation are expression sharing, heuristic rewriting, and congruence closure with interpreted functions. We discuss these techniques and present the results of initial experiments in which SVC is used as a decision procedure in PVS, resulting in dramatic speed-ups. 1 Introduction Decision procedures are emerging as a central component of formal verification systems. Such a procedure can be included as a component of a general-purpos...
84|Experience with Predicate Abstraction|This reports some experiences with a recently-implemented  prototype system for verification using predicate abstraction, based on  the method of Graf and Saidi [9]. Systems are described using a language  of iterated guarded commands, called MurOE  \Gamma\Gamma  (since it is a simplified  version of our MurOE protocol description language). The system makes  use of two libraries: SVC [1] (an efficient decision procedure for quantifierfree  first-order logic) and the CMU BDD library. The use of these libraries  increases the scope of problems that can be handled by predicate  abstraction through increased efficiency, especially in SVC, which is typically  called thousands of times. The verification system also provides  limited support for quantifiers in formulas. The system ...
85|Formal Analysis of a Space Craft Controller using SPIN|Abstract. This report documents an application of the nite state model checker Spin to formally verify a multi{threaded plan execution programming language. The plan execution language is one componentof NASA&#039;s New Millennium Remote Agent, an arti cial intelligence based spacecraft control system architecture that is scheduled to launch inDecember of 1998 as part of the Deep Space 1 mission to Mars. The language is concretely named Esl (Executive Support Language) and is basically a language designed to support the construction of reactive control mechanisms for autonomous robots and space crafts. It o ers advanced control constructs for managing interacting parallel goal-andevent driven processes, and is currently implemented as an extension to amulti-threaded Common Lisp. A total of 5 errors were in fact identi ed, 4 of which were important. This is regarded as a very successful result. According to the Remote Agent programming team the e ort has had a major impact, locating errors that would probably not have been
86|Generating finite-state abstractions of reactive systems using decision procedures|Abstract. We present an algorithm that uses decision procedures to generate finite-state abstractions of possibly infinite-state systems. The algorithm compositionally abstracts the transitions of the system, relative to a given, fixed set of assertions. Thus, the number of validity checks is proportional to the size of the system description, rather than the size of the abstract state-space. The generated abstractions are weakly preserving for ?CTL * temporal properties. We describe several applications of the algorithm, implemented using the decision procedures of the Stanford Temporal Prover (STeP). 1
87|Formal Analysis of the Remote Agent Before and After Flight|This paper describes two separate efforts that used the SPIN model checker to verify deep space autonomy flight software. The first effort occurred at the beginning of a spiral development process and found five concurrency errors early in the design cycle that the developers acknowledge would not have been found through testing. This effort required a substantial manual modeling effort involving both abstraction and translation from the prototype LISP code to the PROMELA language used by SPIN. This experience and others led to research to address the gap between formal method tools and the development cycle used by software developers. The Java PathFinder tool which directly translates from Java to PROMELAwas developed as part of this research, as well as automatic abstraction tools. In 1999 the flight software flew on a space mission, and a deadlock occurred in a sibling subsystem to the one which was the focus of the first verification effort. A second quick-response &#034;cleanroom &#034; ve...
88|Precise Executable Interprocedural Slices|this paper produces executable slices that are more precise than those produces by Weiser. Here the term &#034;precise&#034; means that the algorithm correctly handles calling context and that it may selectively choose parameters from call statements. Precise does not mean it produces the smallest possible slice, which would require solving unsolvable data-flow problems. As a consequence, the algorithm must make safe approximations: although it may include unnecessary statements, it does include all necessary statements.
89|Program Slicing of Hardware Description Languages|Hardware description languages (HDLs) are used today to describe circuits at all levels. In large HDL programs, there is a need for source code reduction techniques to address a myriad of problems in design, simulation, testing, and formal verification. Program slicing is a static program analysis technique that allows an analyst to automatically extract portions of programs relevant to the aspects being analyzed. We extend program slicing to HDLs, thus allowing for automatic program reduction to let the user focus on relevant code portions. We have implemented a VHDL slicing tool composed of a general inter-procedural slicer and a front-end that captures VHDL execution semantics. This report provides an introduction to the theory of inter-procedural program slicing, a discussion of how to slice VHDL programs, a description of the resulting tool, and a discussion of some applications and experimental results.  1 Introduction  Hardware description languages (HDLs) are used today to desc...
90|Parameterized Verification of Multithreaded Software Libraries|The growing popularity of multi-threading has led to a great number of software libraries that support access by multiple threads. We present Local/Global Finite State Machines (LGFSMs) as a model for a certain class of multithreaded libraries. We have developed a tool called Beacon that does parameterized model checking of LGFSMs. We demonstrate the expressiveness of LGFSMs as models, and the effectiveness of Beacon as a model checking tool by (1) modeling a multithreaded memory manager Rockall developed at Microsoft Research as an LGFSM, and (2) using Beacon to check a critical safety property of Rockall.
91|Dependence Analysis of Parallel and Distributed Programs and Its Applications|This paper surveys the program dependence analysis technique for parallel and/or distributed programs and its applications from the viewpoint of software engineering. We present primary program dependences which may exist in a parallel and/or distributed program, a general approach to define, analyze, and represent these program dependences formally, and applications of an explicit program dependence based representation for parallel and/or distributed programs in various software engineering activities. We also suggest some research problems on this direction.  
92|Using Predicate Abstraction to Reduce Object-Oriented Programs for Model Checking|While it is becoming more common to see model checking applied to software requirements specifications, it is seldom applied to software implementations. The Automated Software Engineering group at NASA Ames is currently investigating the use of model checking for actual source code, with the eventual goal of allowing software developers to augment traditional testing with model checking. Because model checking suffers from the state-explosion problem, one of the main hurdles for program model checking is reducing the size of the program. In this paper we investigate the use of abstraction techniques to reduce the state-space of a real-time operating system kernel written in C++. We show how informal abstraction arguments could be formalized and improved upon within the framework of predicate abstraction, a technique based on abstract interpretation. We introduce some extensions to predicate abstraction that all allow it to be used within the class-instance framework of object-oriented...
93|Mechanical Verification of a Garbage Collector|Abstract. We describe how the PVS verification system has been used to verify a safety property of a garbage collection algorithm, originally suggested by Ben-Ari. The safety property basically says that “nothing but garbage is ever collected”. Although the algorithm is relatively simple, its parallel composition with a “user ” program that (nearly) arbitrarily modifies the memory makes the verification quite challenging. The garbage collection algorithm and its composition with the user program is regarded as a concurrent system with two processes working on a shared memory. Such concurrent systems can be encoded in PVS as state transition systems, very similar to the models of, for example, UNITY and TLA. The algorithm is an excellent test-case for formal methods, be they based on theorem proving or model checking. Various hand-written proofs of the algorithm have been developed, some of which are wrong. David Russinoff has verified the algorithm in the Boyer-Moore prover, and our proof is an adaption of this proof to PVS. We also model check a finite state version of the algorithm in the Stanford model checker Murphi, and we compare the result with the PVS verification. 1
94|Checking Temporal Properties of Software with Boolean Programs|A fundamental issue in model checking of software is the choice of a model for software. We present a model called boolean programs that is expressive enough to capture interesting properties of programs and is amenable to model checking. We present a model checking algorithm for boolean programs using context-free-language reachability. The model checking algorithm allows procedure calls with unbounded recursion, exploits locality of variable scopes, and gives short error traces. Furthermore, we give a process for incrementally re ning an initial skeletal boolean program B (representing a source program P ) with respect to a particular reachability query in P . The presence of infeasible paths in P may lead to the model checker reporting false positive errors in B. We show how to re ne B by introducing boolean variables to rule out the infeasible paths. The process uses ideas from model checking and symbolic execution to automatically perform predicate abstraction.
95|Modular and Incremental Analysis of Concurrent Software Systems|Modularization and abstraction are the keys to practical verification and analysis of large and complex systems. We present in an incremental methodology for the automatic analysis and verification of concurrent software systems. Our methodology is based on the theory of abstract interpretation. We first propose a compositional data flow analysis algorithm that computes invariants of concurrent systems by composing invariants generated separately for each component. We present a novel compositional rule allowing us to obtain invariants of the whole system as conjunctions of local invariants of each component. We also show how the generated invariants are used to construct, almost for free, finite state abstractions of the original system that preserve safety properties. This reduces dramatically the cost of computing such abstractions as reported in previous work. We finally give a novel refinement algorithm that refines the constructed abstraction until the property of interest is pro...
96|Offline Integrity Checking of Untrusted Storage|We extend the offline memory correctness checking scheme presented by Blum et. al [BEG 91] to develop an offline checker that can detect attacks by active adversaries. We introduce the concept of incremental multiset hashes, and detail one example: MSet-XOR MAC, which uses a secret key, and is efficient as updating the hash costs a few hash and XOR operations. Using multiset hashes as our underlying cryptographic tool, we introduce a primitive, bag integrity checking, to explain offline integrity checking; we demonstrate how this primitive can be used to build cryptographically secure integrity checking schemes for random access memories and disks.
97|Practical Byzantine Fault Tolerance |This paper describes a new replication algorithm that is able to tolerate Byzantine faults. We believe that Byzantinefault-tolerant algorithms will be increasingly important in the future because malicious attacks and software errors are increasingly common and can cause faulty nodes to exhibit arbitrary behavior. Whereas previous algorithms assumed a synchronous system or were too slow to be used in practice, the algorithm described in this paper is practical: it works in asynchronous environments like the Internet and incorporates several important optimizations that improve the response time of previous algorithms by more than an order of magnitude. We implemented a Byzantine-fault-tolerant NFS service using our algorithm and measured its performance. The results show that our service is only 3 % slower than a standard unreplicated NFS.  
98|Architectural Support for Copy and Tamper Resistant Software|Implementing copy protection on software is a difficult problem that has resisted a satisfactory solution for many years. This paper proposes a set of features that allows a machine to execute XOM code: code where neither the instructions or the data are visible to entities outside the running process. To support XOM code we use a machine that supports internal compartments, where a process in one compartment cannot read data from another compartment. All data that leaves the machine is encrypted, since we assume secure compartments cannot be guaranteed by anything outside the machine. The design of this machine poses some interesting trade-offs between security, efficiency and flexibility. We explore some of the potential security issues as one pushes the machine to become more efficient and flexible. Our analysis indicates, while not cheap, it is possible to create a normal multi-tasking machine where nearly all applications can be run in XOM mode. While a virtual XOM machine is possible, the underlying hardware needs to support a unique private key, asymmetric decryption, private memory, fast symmetric ciphers, and traps on cache misses for efficient operation.
99|XOR MACS: New Methods for Message Authentication using Finite Pseudorandom Functions|We describe a new approach for authenticating messages. Our “XOR MACs ” have several nice features, including parallelizability, incrementality, and provable security. Our method uses any finite pseudorandom function (PRF). The finite PRF can be “instantiated” via DES (yielding an alternative to the CBC MAC), via the compression function of MD5 (yielding an alternative to various “keyed MD5 ” constructions), or in a variety of other ways. The proven security is quantitative, expressing the adversary’s inability to forge in terms of her (presumed) inability to break the underlying finite PRF. This is backed by attacks showing the analysis is tight. Our proofs exploit linear algebraic techniques, and relate the security of a given XOR scheme to the probability that a certain associated matrix is of full rank. Our analysis shows that XOR schemes are actually more secure than the CBC MAC, in a
100|Proactive Recovery in a Byzantine-Fault-Tolerant System|This paper describes an asynchronous state-machine replication system that tolerates Byzantine faults, which can be caused by malicious attacks or software errors. Our system is the first to recover Byzantine-faulty replicas proactively and it performs well because it uses symmetric rather than public-key cryptography for authentication. The recovery mechanism allows us to tolerate any number of faults over the lifetime of the system provided fewer than 1/3 of the replicas become faulty within a window of vulnerability that is small under normal conditions. The window may increase under a denial-of-service attack but we can detect and respond to such attacks. The paper presents results of experiments showing that overall performance is good and that even a small window of vulnerability has little impact on service latency.
101|A new paradigm for collision-free hashing: incrementality at reduced cost|We present a simple, new paradigm for the design of collision-free hash functions. Any function emanating from this paradigm is incremental. (This means that if a message x which Ihave previously hashed is modi ed to x 0 then rather than having to re-compute the hash of x 0 from scratch, I can quickly \update &amp;quot; the old hash value to the new one, in time proportional to the amount of modi cation made in x to get x 0.) Also any function emanating from this paradigm is parallelizable, useful for hardware implementation. We derive several speci c functions from our paradigm. All use a standard hash function, assumed ideal, and some algebraic operations. The rst function, MuHASH, uses one modular multiplication per block of the message, making it reasonably e cient, and signi cantly faster than previous incremental hash functions. Its security is proven, based on the hardness of the discrete logarithm problem. A second function, AdHASH, is even faster, using additions instead of multiplications, with security proven given either that approximation of the length of shortest lattice vectors is hard or that the weighted subset sum problem is hard. A third function, LtHASH, is a practical variant of recent lattice based functions, with security proven
102|Unifying File System Protection|This paper describes an efficient and elegant architecture for unifying the meta-data protection of journaling file systems with the data integrity protection of collision -resistant cryptographic hashes. Traditional file system journaling protects the ordering of meta-data operations to maintain consistency in the presence of crashes. However, journaling does not protect important system meta-data and application data from modification or misrepresentation by faulty or malicious storage devices. With the introduction of both storage-area networking and increasingly complex storage systems into server architectures, these threats become an important concern.  This paper presents the protected file system (PFS), a file system that unifies the meta-data update protection of journaling with strong data integrity. PFS computes hashes from file system blocks and uses these hashes to later verify the correctness of their contents. Hashes are stored within a system log, apart from the blocks they describe, but potentially on the same storage system. The write-ahead logging (WAL) protocol and the file system buffer cache are used to aggregate hash writes and allow hash computations and writes to proceed in the background.  PFS does not require the sharing of secrets between the operating system and the storage system nor the deployment of any special cryptographic firmware or hardware. PFS is an end-to-end solution and will work with any block-oriented device, from a disk drive to a monolithic RAID system, without modification.  1 
103|Don&#039;t Trust Your File Server|All too often, decisions about whom to trust in computer systems are driven by the needs of system management rather than data security. In particular, data storage is often entrusted to people who have no role in creating or using the data---through outsourcing of data management, hiring of outside consultants to administer servers, or even collocation servers in physically insecure machine rooms to gain better network connectivity. This paper
104|Small-Bias Probability Spaces: Efficient Constructions and Applications|We show how to efficiently construct a small probability space on n binary random variables such that for every subset, its parity is either zero or one with &#034;almost&#034; equal probability. They are called ffl-biased random variables. The number of random bits needed to generate the random variables is O(log n + log 1 ffl ). Thus, if ffl is polynomially small, then the size of the sample space is also polynomial. Random variables that are ffl-biased can be used to construct &#034;almost&#034; k-wise independent random variables where ffl is a function of k. These probability spaces have various applications: 1. Derandomization of algorithms: many randomized algorithms that require only k- wise independence of their random bits (where k is bounded by O(log n)), can be derandomized by using ffl-biased random variables. 2. Reducing the number of random bits required by certain randomized algorithms, e.g., verification of matrix multiplication. 3. Exhaustive testing of combinatorial circui...
105|Cryptographic Support for Secure Logs on Untrusted Machines|In many real-world applications, sensitive information must be kept in log files on an untrusted machine. In the event that an attacker captures this machine, we would like to guarantee that he will gain little or no information from the log files and to limit his ability to corrupt the log files. We describe a computationally cheap method for making all log entries generated prior to the logging machine&#039;s compromise impossible for the attacker to read, and also impossible to undetectably modify or destroy.  
106|The Design and Implementation of an Operating System to Support Distributed Multimedia Applications|Support for multimedia applications by general purpose computing platforms has been the subject of considerable research. Much of this work is based on an evolutionary strategy in which small changes to existing systems are made. The approach adopted here is to start ab initio with no backward compatibility constraints. This leads to a novel structure for an operating system. The structure aims to decouple applications from one another and to provide multiplexing of all resources, not just the CPU, at a low level. The motivation for this structure, a design based on the structure, and its implementation on a number of hardware platforms is described. I. Introduction G ENERAL purpose multimedia computing platforms should endow text, images, audio and video with equal status: interpreting an audio or video stream should not be a privileged task of special functions provided by the operating system, but one of ordinary user programs. Support for such processing on a platform on which ot...
107|Scout: A Communications-Oriented Operating System|This white paper describes Scout, a new operating system being designed for systems connected  to the National Information Infrastructure (NII). Scout provides a communication-oriented  software architecture for building operating system code that is specialized for the different  systems that we expect to be available on the NII. It includes an explicit path abstraction that  both facilitates effective resource management and permits optimizations of the critical path  that I/O data follows. These path-enabled optimizations, along with the application of advanced  compiler techniques, result in a system that has both predictable and scalable performance. 
108|PLAN: A Programming Language for Active Networks|PLAN (Programming Language for Active Networks) is a new language for programs that are carried in the packets of a programmable network. PLAN programs replace the packet headers (which can be viewed as `dumb&#039; programs) used in current networks. As a header replacement, PLAN programs must be lightweight and of limited functionality. These limitations are mitigated by allowing PLAN code to call service routines written in other, more powerful languages. These service routines may also be loaded into the routers dynamically. This two-level architecture, in which PLAN serves as a scripting or `glue&#039; language for more general services, is the primary contribution of the paper. PLAN is a strict functional language providing a limited set of primitives and datatypes. PLAN defines primitives for remotely executing PLAN programs on other nodes, and these primitives are used to provide basic data transport in the network. Because remote execution makes debugging difficult, PLAN provides strong ...
109|IDS: File Integrity Checking |This paper is from the SANS Institute Reading Room site. Reposting is not permitted without express written permission.
110|Integrity Checking for Nested Transactions|In this paper, we present a mechanism to specify and to validate consistency constraints in object oriented databases. Constraints are specified using pre and post-conditions associated with an exception handling mechanism. During transaction run-time, we treat exceptions corresponding to errors (in this case, we use immediate exceptions which are processed immediately) or presumption of errors (in this case, we use deferred exceptions which are processed at the end of the transaction), in order to insure validation. To refine our mechanism, we enlarge it to nested transactions. Deferred exceptions can be processed at each node of the transaction tree. Using a predefined exception, we propagate an abort of a sub-transaction to its parent transaction. Thus, the parent transaction can choose among different policies for processing the sub-transaction abort. 1 Introduction  For the programming point of view, writing applications in an object-oriented databases framework is still a delicat...
111|Thémis: A Database Programming Language Handling Integrity Constraints|This article presents a database programming language, Thémis, which supports subtyping and class hierarchies, and allows for the definition of integrity constraints in a global and declarative way. We first describe the salient features of the language: types, names, classes, integrity constraints (including methods), and transactions. The inclusion of methods into integrity constraints allows an increase of the declarative power of these constraints. Indeed, the information needed to define a constraint is not always stored in the database through attributes, but is sometimes computed or derived data. Then, we address the problem of efficiently checking constraints. More specifically, we consider two different problems: (1) statically reducing the number of constraints to be checked, and (2) generating an efficient run-time checker. Using simple strategies, one can significantly improve the efficiency of the verification. We show how to reduce the number of constraints to be checked by characterizing the portions of the database that are involved in both the constraints and in a transaction. We also show how to generate efficient algorithms for checking a large class of constraints. We show how all the techniques presented exploit the underlying type system, which provides significant help in solving (1) and (2). Last, the current status of the Thémis prototype is presented.
112|Avenues to Flexible Data Integrity Checking |Traditional methods for integrity checking in relational or deductive databases heavily rely on the assumption that data have integrity before the execution of updates. In this way, as has always been claimed, one can automatically derive strategies to check, in an incremental way, whether data preserve their integrity after the update. On the other hand, this consistency assumption greatly reduces applicability of such methods, since it is most often the case that small parts of a database do not comply with the integrity constraints, especially when the data are distributed or have been integrated from different sources. In this paper we revisit integrity checking from an inconsistency-tolerant viewpoint. We show that most methods for integrity checking (though not all) are still applicable in the presence of inconsistencies and may be used to guarantee that the satisfied instances of the integrity constraints will continue to be satisfied after the update. 1
113|Constraint Checking with Partial Information (Extended Abstract)  (1994) |)  Ashish Gupta Yehoshua Sagivy Jeffrey D. Ullman Jennifer Widom  Dept. of Computer Science Stanford Univ., Stanford CA 94305  email: --agupta,sagiv,ullman,widom@cs.stanford.edu  contact phone: (415) 723--9745 FAX: (415) 725--7411  1. Introduction  The general problem we address is that of telling whether constraints remain satisfied as a database changes. Because checking a constraint on a database can be as complex as answering a query, it is desirable to minimize the work done in checking. As an extreme example, we would like to know when any update is guaranteed not to create a violation of a given constraint. In less extreme cases, we look for checks that can be done efficiently and that guarantee no violation occurs, even though sometimes a further, more expensive computation may be needed to check the constraint if the first test fails.  Constraints  A constraint is a query whose result is a 0-ary predicate that we call panic. If the query produces ; on a given database D, then ...
114|Specifying and querying database repairs using logic programs with exceptions|Abstract Databases may be inconsistent with respect to a given set of integrity constraints. Nevertheless, most of the data may be consistent. In this paper we show how to specify consistent data and how to query a relational database in such a way that only consistent data is retrieved. The specification and queries are based on disjunctive extended logic programs with positive and negative exceptions that generalize those previously introduced by Kowalski and Sadri.
116|Incremental Maintenance of Recursive Views Using Relational Calculus/SQL|Views are a central component of both traditional database systems and new applications such as data warehouses. Very often the desired views (e.g. the transitive closure) cannot be defined in the standard language of the underlying database system. Fortunately, it is often possible to incrementally maintain these views using the standard language. For example, transitive closure of acyclic graphs, and of undirected graphs, can be maintained in relational calculus after both single edge insertions and deletions. Many such results have been published in the theoretical database community. The purpose of this survey is to make these useful results known to the wider database research and development community. There are many interesting issues involved in the maintenance of recursive views. A maintenance algorithm may be applicable to just one view, or to a class of views specified by a view definition language such as Datalog. The maintenance algorithm can be specified in a maintenance language of different expressiveness, such as the conjunctive queries, the relational calculus or SQL. Ideally, this maintenance language should be less expensive than the view def-inition language. The maintenance algorithm may allow updates of different kinds, such as just single tuple insertions, just single tuple deletions, special set-based insertions and/or deletions, or combinations thereof. The view maintenance algorithms may also need to maintain auxiliary relations to help maintain the views of interest. It is of interest to know the minimal arity necessary for these auxiliary relations
117|Compiling Integrity Checking into Update Procedures |Integrity checking has been investigated extensively in the field of deductive databases. Methods have been developed to optimise the checking of an update by specialising the constraints for the information that could have been affected by it. The optimisation has been applied to sets of updates resulting from the execution of unspecified update procedures. This paper investigates the compilation of integrity checking into the procedures themselves. The paper introduces a (procedural) update language, and describes how constraints are compiled into procedures expressed in this language. The compilation yields conditions on the original database state that guarantee safety of the update. The paper also shows why compilation into procedures offers important possibilities for optimisation not available in the earlier framework. 1
118|A uniform approach to constraint satisfaction and constraint satisfiability in deductive databases|ABSTRACT Integrity maintenance methods have been defined for preventing updates from violating integrity constraints. Depending on the update, the full check for constraint satisfaction is reduced to checking certain instances of some relevant constraints only. In the first part of the paper new ideas are proposed for enhancing the efficiency of such a method. The second part is devoted to checking constraint satisfiability, i.e., whether a database exists in which all constraints are simultaneously satisfied. A satisfiability checking method is presented that employs integrity maintenance techniques. Simple Prolog programs are given that serve both as specifications as well as a basis for an efficient implementation. 1
119|Integrity verification in knowledge bases |ABSTlZACT In order to faithfully describe real-life applications, knowledge bases have to manage general integrity constraints. In this article, we analyse methods for an efficient verification of integrity constraints in updated knowledge bases. These methods rely on the satisfaction of the integrity constraints before the update for simplifying their evaluation in the updated knowledge base. During the last few years, an increasing amount of publications has been devoted to various aspects of this problem. Since they use distinct formalisms and different terminologies, they are di~cult to compare. Moreover, it is often complex to recognize commonalities and to find out whether techniques described in different articles are in principle different. A first part of this report aims at giving a comprehensive state-of-the-art in integrity verification. It describes integrity constraint verification techniques in a common formalism. A second part of this report is devoted to comparing several proposals. The differences and similarities between various methods are investigated. 1
120|ABSTRACT Integrity Checking for Uncertain Data |The uncertainty associated to stored information can be put in direct correspondence to the extent to which these data violate conditions expressed as semantic integrity constraints. Thus, imposing and checking such constraints provides a better control over uncertain data. We present and discuss a condition which ensures the violation tolerance of methods for integrity checking. Usually, such methods are supposed to work correctly only if all constraints are satisfied before each update. Applied to express and check conditions about uncertain data, violation tolerance means that stored data the uncertainty of which violates integrity can be tolerated while updates can be safely checked for introducing violations of constraints about uncertainty. We also discuss the soundness and completeness of violation-tolerant integrity checking and assert it for several methods. 1.
121|Abduction in Logic Programming |Abduction in Logic Programming started in the late 80s,  early 90s, in an attempt to extend logic programming into a framework  suitable for a variety of problems in Artificial Intelligence and other areas  of Computer Science. This paper aims to chart out the main developments  of the field over the last ten years and to take a critical view of  these developments from several perspectives: logical, epistemological,  computational and suitability to application. The paper attempts to expose  some of the challenges and prospects for the further development  of the field.
122|Query Answering in Inconsistent Databases|In this chapter, we summarize the research on querying inconsistent databases we have been conducting over the last five years. The formal framework we have used is based on two concepts: repair and consistent query answer. We describe different approaches to the issue of computing consistent query answers: query transformation, logic programming, inference in annotated logics, and specialized algorithms. We also characterize the computational complexity of this problem. Finally, we discuss related research in artificial intelligence, databases, and logic programming.
123|Integrity= validity+ completeness|Database integrity has two complementary components: validity, which guarantees that all false information is excluded from the database, and completeness, which guarantees that all true infor-mation is included in the database. This article describes a uniform model of integrity for relational databases, that considers both validity and completeness. To a large degree, this model subsumes the prevailing model of integrity (i.e., integrity constraints). One of the features of the new model is the determination of the integrity of answers issued by the database system in response to user queries. To users, answers that are accompanied with such detailed certifications of their integrity are more meaningful. First, the model is defined and discussed. Then, a specific mechanism is described that implements this model. With this mechanism, the determination of the integrity of an answer is a process analogous to the determination of the answer itself.
124|Further improvement on integrity constraint checking for stratifiable deductive databases|Integrity constraint checking for stratifiable deductive databases has been studied by many authors. However, most of these methods may perform unnecessary checking if the update is irrelevant to the constraints. [Lee941 proposed a set called relevant set which can be incorporated in these works to reduce unnecessary checking. [Lee941 adopts a top-down approach and makes use of constants and evaluable functions in the constraints and deductive rules to reduce the search space. In this paper, we further extend this idea to make use of relational predicates, instead of only constants and evaluable functions in [Lee94]. We first show that this extension is not a trivial one as extra database retrieval cost is incurred. We then present a new method to construct a pre-test which can be incorporated in most existing methods to reduce the average checking costs in terms of database accesses by a significant factor. Our method also differs from other partial checking methods as we can handle multiple updates. 1
125|Symbolic Model Checking for Real-time Systems|We describe finite-state programs over real-numbered time in a guarded-command  language with real-valued clocks or, equivalently, as finite automata with  real-valued clocks. Model checking answers the question which states of a real-time  program satisfy a branching-time specification (given in an extension of CTL with clock  variables). We develop an algorithm that computes this set of states symbolically as a  fixpoint of a functional on state predicates, without constructing the state space.  For this purpose, we introduce a -calculus on computation trees over real-numbered  time. Unfortunately, many standard program properties, such as response for all  nonzeno execution sequences (during which time diverges), cannot be characterized  by fixpoints: we show that the expressiveness of the timed -calculus is incomparable  to the expressiveness of timed CTL. Fortunately, this result does not impair the  symbolic verification of &#034;implementable&#034; real-time programs---those whose safety...
126|Hybrid Automata: An Algorithmic Approach to the Specification and Verification of Hybrid Systems|We introduce the framework of hybrid automata as a model and specification language for hybrid systems. Hybrid automata can be viewed as a generalization of timed automata, in which the behavior of variables is governed in each state by a set of differential equations. We show that many of the examples considered in the workshop can be defined by hybrid automata. While the reachability problem is undecidable even for very restricted classes of hybrid automata, we present two semidecision procedures for verifying safety properties of piecewise-linear hybrid automata, in which all variables change at constant rates. The two procedures are based, respectively, on minimizing and computing fixpoints on generally infinite state spaces. We show that if the procedures terminate, then they give correct answers. We then demonstrate that for many of the typical workshop examples, the procedures do terminate and thus provide an automatic way for verifying their properties. 1 Introduction  More and...
128|Hardware Mechanisms for Memory Integrity Checking|Memory integrity verification is a useful primitive when implementing secure processors that are resistant to attacks on hardware components. This paper proposes new hardware schemes to verify the integrity of untrusted external memory using a very small amount of trusted on-chip storage. Our schemes maintain incremental multiset hashes of all memory reads and writes at run-time, and can verify a sequence of memory operations at a later time. We study the advantages and disadvantages of the two new schemes and two existing integrity checking schemes, MACs and hash trees, when implemented in hardware in a microprocessor. Simulations show that the new schemes outperform existing schemes of equivalent functionality when integrity verification is infrequent.
129|Building a high-performance, programmable secure coprocessor|Abstract. Unsecure computational environments threaten many nancial cryptography implementations, and other sensitive computation. High-performance secure coprocessors can address these threats. However, using this technology for practical security solutions requires overcoming numerous technical and business obstacles. These obstacles motivate building a high-performance secure coprocessor that balances security with easy third-party programmability|but these obstacles also provide many design challenges. This paper discusses some of issues we faced when attempting to build such a device. 1
130|Checking the Correctness of Memories|We extend the notion of program checking to include programs which alter their  environment. In particular, we consider programs which store and retrieve data from  memory. The model we consider allows the checker a small amount of reliable memory.  The checker is presented with a sequence of requests (on-line) to a data structure which  must reside in a large but unreliable memory. We view the data structure as being  controlled by an adversary. We want the checker to perform each operation in the input  sequence using its reliable memory and the unreliable data structure so that any error  in the operation of the structure will be detected by the checker with high probability.  We present checkers for various data structures. We prove lower bounds of log n  on the amount of reliable memory needed by these checkers where n is the size of  the structure. The lower bounds are information theoretic and apply under various  assumptions. We also show time-space tradeoffs for checking random access memories  as a generalization of those for coherent functions.  1 
131|Stack and Queue Integrity on Hostile Platforms|When computationally intensive tasks have to be carried out on trusted, but limited, platforms such as smart cards, it becomes necessary to compensate for the limited resources #memory, CPU speed# by o#- loading implementations of data structures on to an available #but insecure, untrusted# fast co-processor. However, data structures such as stacks, queues, RAMS, and hash tables can be corrupted #and made to behave incorrectly# by a potentially hostile implementation platform or by an adversary knowing or choosing data structure operations. This paper examines approaches that can detect violations of datastructure invariants, while placing limited demands on the resources of the secure computing platform.  1 Introduction  Smart cards, set-top boxes, consumer electronics and other forms of trusted hardware #2, 3, 16# have been available #or are being proposed #1## for applications such as electronic commerce. We shall refer to these devices as T . These devices are typically composed of...
132|Caches and Merkle Trees for Efficient Memory Authentication|We describe a hardware scheme to authenticate all or a part of untrusted external memory using trusted on-chip storage. Our scheme uses Merkle trees and caches to efficiently authenticate memory. Proper placement of Merkle tree checking and generation is critical to ensure good performance. Na ve schemes where the Merkle tree machinery is placed between caches can result in a large increase in memory bandwidth usage. Weintegrate the Merkle tree machinery with one of the cache levels to significantly reduce memory bandwidth requirements. We present anevaluation of the area and performance costs of various schemes using simulation. For most benchmarks, the performance overhead of authentication using our integrated Merkle tree/caching scheme is less than 25%, whereas the overhead of authentication for a na ve scheme can be as large as 10.We explore tradeoffs between external memory overhead and processor performance.
133|How to Manage Persistent State in DRM Systems|Digital Rights Managements (DRM) systems often must manage persistent state, which includes protected content, an audit trail, content usage counts, certificates and decryption keys. Ideally, persistent state that has monetary value should be stored in a physically secure server. However, frequently the persistent state may need to be stored in a hostile environment. For example, for good performance and to support disconnected operation, recent audit records may be stored on a consumer device. The device&#039;s user may have an incentive to alter the audit trail and thus obtain content for free. In this paper we explain the need for persistent state in DRM systems, describe several methods for maintaining persistent state depending on the system requirements, and then focus on the the special case of protecting persistent state in hostile environments.  1 
134|Incremental integrity checking: Limitations and possibilities |Abstract. Integrity checking is an essential means for the preservation of the intended semantics of a deductive database. Incrementality is the only feasible approach to checking and can be obtained with respect to given update patterns by exploiting query optimization techniques. By reducing the problem to query containment, we show that no procedure exists that always returns the best incremental test (aka simplification of integrity constraints), and this according to any reasonable criterion measuring the checking effort. In spite of this theoretical limitation, we develop an effective procedure allowing general parametric updates that, for given database classes, returns ideal simplifications and also applies to recursive databases. Finally, we point out the improvements with respect to previous methods based on an experimental evaluation. 1
135|A New Method For Integrity Constraint Checking In Deductive Databases|In the literature, several integrity checking methods for updates in deductive  databases are described. All these methods try to instantiate the specified integrity  constraints with the update in order to constrain the full check to only a relevant  part of the database. Globally, they can be divided in two major classes of methods;  methods based on &#034;induced updates&#034; and methods based on &#034;potential updates&#034;. In  this article a new method will be presented. This method represents also a new class  of methods. While in the first two classes one has to generate induced updates and  potential updates respectivily, even if they are not relevant to any of the constraints,  the new method, which will be called the &#034;method based on inconsistency rules&#034;,  does not have this drawback. Therefore, the proposed method is potentially far more  efficient than any other method based on induced updates or potential updates  .  1 INTRODUCTION  The relational database model is in several cases not suf...
136|Efficient integrity checking for databases with recursive views |Abstract. Efficient and incremental maintenance of integrity constraints involving recursive views is a difficult issue that has received some attention in the past years, but for which no widely accepted solution exists yet. In this paper a technique is proposed for compiling such integrity constraints into incremental and optimized tests specialized for given update patterns. These tests may involve the introduction of new views, but for relevant cases of recursion, simplified integrity constraints are obtained that can be checked more efficiently than the original ones and without auxiliary views. Notably, these simplified tests are derived at design time and can be executed before the particular database update is made and without simulating the updated state. In this way all overhead due to optimization or execution of compensative actions at run time is avoided. It is argued that, in the recursive case, earlier approaches have not achieved comparable optimization with the same level of generality. 1
137|Radmind: The Integration of Filesystem Integrity Checking With Filesystem Management|We review two surveys of large-scale system management, observing common problems across tools and over many years. We also note that filesystem management tools interfere with filesystem integrity checking tools. Radmind, an integrated filesystem management and integrity checking tool, solves many of these problems. Radmind also provides a useful platform upon which to build further large-scale, automatic system management tools.
138|Global impact analysis of dynamic library dependencies| Sowhat is an administrative tool that performs global impact analysis of dynamic library dependencies for Solaris systems. Sowhat runs in two phases. It first builds a database of dependencies offline in the background, and then answers user queries and generates reports in real time based upon stored knowledge. Using sowhat, one can find problems with library bindings in large program repositories before these problems annoy potential users.  
139|Tor: The Second-Generation Onion Router|We present Tor, a circuit-based low-latency anonymous communication service. This second-generation Onion Routing system addresses limitations in the original design. Tor adds perfect forward secrecy, congestion control, directory servers, integrity checking, configurable exit policies, and a practical design for rendezvous points. Tor works on the real-world Internet, requires no special privileges or kernel modifications, requires little synchronization or coordination between nodes, and provides a reasonable tradeoff between anonymity, usability, and efficiency. We briefly describe our experiences with an international network of more than a dozen hosts. We close with a list of open problems in anonymous communication. 
141|Wide-area cooperative storage with CFS|The Cooperative File System (CFS) is a new peer-to-peer readonly storage system that provides provable guarantees for the efficiency, robustness, and load-balance of file storage and retrieval. CFS does this with a completely decentralized architecture that can scale to large systems. CFS servers provide a distributed hash table (DHash) for block storage. CFS clients interpret DHash blocks as a file system. DHash distributes and caches blocks at a fine granularity to achieve load balance, uses replication for robustness, and decreases latency with server selection. DHash finds blocks using the Chord location protocol, which operates in time logarithmic in the number of servers. CFS is implemented using the SFS file system toolkit and runs on Linux, OpenBSD, and FreeBSD. Experience on a globally deployed prototype shows that CFS delivers data to clients as fast as FTP. Controlled tests show that CFS is scalable: with 4,096 servers, looking up a block of data involves contacting only seven servers. The tests also demonstrate nearly perfect robustness and unimpaired performance even when as many as half the servers fail.
142|Introducing Tarzan, a Peer-to-Peer Anonymizing Network Layer|We introduce Tarzan, a peer-to-peer anonymous network layer that provides generic IP forwarding. Unlike prior anonymizing layers, Tarzan is flexible, transparent, decentralized, and highly scalable. Tarzan achieves these properties by building anonymous IP tunnels between an open-ended set of peers. Tarzan can provide anonymity to existing applications, such as web browsing and file sharing, without change to those applications. Performance tests show that Tarzan imposes minimal overhead over a corresponding non-anonymous overlay route.
143|Anonymous connections and onion routing|Abstract—Onion routing is an infrastructure for private communication over a public network. It provides anonymous connections that are strongly resistant to both eavesdropping and traffic analysis. Onion routing’s anonymous connections are bidirectional, near real-time, and can be used anywhere a socket connection can be used. Any identifying information must be in the data stream carried over an anonymous connection. An onion is a data structure that is treated as the destination address by onion routers; thus, it is used to establish an anonymous connection. Onions themselves appear different to each onion router as well as to network observers. The same goes for data carried over the connections they establish. Proxy-aware applications, such as web browsers and e-mail clients, require no modification to use onion routing, and do so through a series of proxies. A prototype onion routing network is running between our lab and other sites. This paper describes anonymous connections and their implementation using onion routing. This paper also describes several application proxies for onion routing, as well as configurations of onion routing networks. Index Terms—Anonymity, communications, Internet, privacy, security, traffic analysis.
144|Mixminion: Design of a Type III Anonymous Remailer Protocol|Abstract. We present Mixminion, a message-based anonymous remailer protocol that supports secure single-use reply blocks. MIX nodes cannot distinguish Mixminion forward messages from reply messages, so forward and reply messages share the same anonymity set. We add directory servers that allow users to learn public keys and performance statistics of participating remailers, and we describe nymservers that allow users to maintain long-term pseudonyms using single-use reply blocks as a primitive. Our design integrates link encryption between remailers to provide forward anonymity. Mixminion brings together the best solutions from previous work to create a conservative design that protects against most known attacks. Keywords: anonymity, MIX-net, peer-to-peer, remailer, nymserver, reply block 1
145|The NRL Protocol Analyzer: An Overview|this paper we give an overview of how the Analyzer works and describe its achievements so far. We also show how our use of the Prolog language benefited us in the design and implementation of the Analyzer. / 1. INTRODUCTION
146|The Free Haven Project: Distributed Anonymous Storage Service|We present a design for a system of anonymous storage which resists the attempts of powerful adversaries to find or destroy any stored data. We enumerate distinct notions of anonymity for each party in the system, and suggest a way to classify anonymous systems based on the kinds of anonymity provided. Our design ensures the availability of each document for a publisher-specified lifetime. A reputation system provides server accountability by limiting the damage caused from misbehaving servers. We identify attacks and defenses against anonymous storage services, and close with a list of problems which are currently unsolved.
147|Publius: A robust, tamper-evident, censorship-resistant, web publishing system|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
148|Web MIXes: A system for anonymous and unobservable Internet access|We present the architecture, design issues and functions of a MIX-based system for anonymous and unobservable real-time Internet access. This system prevents trac analysis as well as ooding attacks. The core technologies include an adaptive, anonymous, time/volumesliced channel mechanism and a ticket-based authentication mechanism. The system also provides an interface to inform anonymous users about their level of anonymity and unobservability.
149|Hiding Routing Information|. This paper describes an architecture, Onion Routing, that limits a network&#039;s vulnerability to traffic analysis. The architecture provides anonymous socket connections by means of proxy servers. It provides real-time, bi-directional, anonymous communication for any protocol that can be adapted to use a proxy service. Specifically, the architecture provides for bi-directional communication even though no-one but the initiator&#039;s proxy server knows anything but previous and next hops in the communication chain. This implies that neither the respondent nor his proxy server nor any external observer need know the identity of the initiator or his proxy server. A prototype of Onion Routing has been implemented. This prototype works with HTTP (World Wide Web) proxies. In addition, an analogous proxy for TELNET has been implemented. Proxies for FTP and SMTP are under development. 1 Introduction  This paper presents an architecture that limits a network&#039;s vulnerability to traffic analysis. We c...
150|The Eternity Service|The Internet was designed to provide a communications channel that is as resistant to denial of service attacks as human ingenuity can make it. In this note, we propose the construction of a storage medium with similar properties. The basic idea is to use redundancy and scattering techniques to replicate data across a large set of machines (such as the Internet), and add anonymity mechanisms to drive up the cost of selective service denial attacks. The detailed design of this service is an interesting scientific problem, and is not merely academic: the service may be vital in safeguarding individual rights against new threats posed by the spread of electronic publishing.
151|Towards an Analysis of Onion Routing Security|This paper presents a security of Onion Routing, an application independent infrastructure for traffic-analysis-resistant and anonymous Internet connections. It also includes an overview of the current system design, definitions of security goals and new adversary models.
152|ISDN-MIXes: Untraceable Communication with Very Small Bandwidth Overhead|Untraceable communication for services like telephony is often considered infeasible in the near future because of bandwidth limitations. We present a technique, called ISDN-MIXes, which shows that this is not the case. As little changes as possible are made to the narrowband-ISDN planned by the PTTs. In particular, we assume the same subscriber lines with the same bit rate, and the same long-distance network between local exchanges, and we offer the same services. ISDN-MIXes are a combination of a new variant of CHAUM&#039;s MIXes, dummy traffic on the subscriber lines (where this needs no additional bandwidth), and broadcast of incoming-call messages in the subscriber-area. 1 Introduction  The need to keep communication untraceable, i.e. to keep secret who communicates with whom, has been discussed, e.g., in [Chau_81, Cha8_85, PfWa_86, PfPW_88]. Untraceable communication for services like telephony is often considered infeasible in the near future because of bandwidth limitations. We pres...
153|Fingerprinting websites using traffic analysis|  I present a traffic analysis based vulnerability in SafeWeb, an encrypting web proxy. This vulnerability allows someone monitoring the trac of a SafeWeb user to determine if the user is visiting certain websites. I also describe a successful implementation of the attack. Finally, I discuss methods for improving the attack and for defending against the attack. 
154|On the Economics of Anonymity|Decentralized anonymity infrastructures are still not in wide use today. While there are technical barriers to a secure robust design, our lack of understanding of the incentives to participate in such systems remains a major roadblock. Here we explore some reasons why anonymity systems are particularly hard to deploy, enumerate the incentives to participate either as senders or also as nodes, and build a general model to describe the effects of these incentives. We then describe and justify some simplifying assumptions to make the model manageable, and compare optimal strategies for participants based on a variety of scenarios.
155|P5: A Protocol for Scalable Anonymous Communication|We present a protocol for anonymous communication over the Internet. Our protocol, called P 5 (Peer-to-Peer Personal Privacy Protocol) provides sender-, receiver-, and sender-receiver anonymity. P 5 is designed to be implemented over the current Internet protocols, and does not require any special infrastructure support. A novel feature of P 5 is that it allows individual participants to trade-off degree of anonymity for communication efficiency, and hence can be used to scalably implement large anonymous groups. We present a description of P 5, an analysis of its anonymity and communication efficiency, and evaluate its performance using detailed packet-level simulations. 
156|Tangler: A Censorship-Resistant Publishing System Based On Document Entanglements|We describe the design of a censorship-resistant system that employs a unique document storage mechanism. Newly published documents are dependent on the blocks of previously published documents. We call this dependency an entanglement. Entanglement makes replication of previously published content an intrinsic part of the publication process. Groups of files, called collections, can be published together and named in a host-independent manner. Individual documents within a collection can be securely updated in such a way that future readers of the collection see and tampercheck the updates. The system employs a self-policing network of servers designed to eject non-compliant servers and prevent them from doing more harm than good. 1.
157|Passive Attack Analysis for Connection-Based Anonymity Systems|In this paper we consider low latency connection-based anonymity system which  can be used for applications like web browsing or SSH. Although several such systems  have been designed and built, their anonymity has so far not been adequately  evaluated.
158|Reliable MIX Cascade Networks through Reputation|We describe a MIX cascade protocol and a reputation system that together increase the reliability of a network of MIX cascades. In our protocol, MIX nodes periodically generate a communally random seed that, along with their reputations, determines cascade configuration.
159|A reputation system to increase MIX-net reliability|Abstract. We describe a design for a reputation system that increases the reliability and thus efficiency of remailer services. Our reputation system uses a MIX-net in which MIXes give receipts for intermediate messages. Together with a set of witnesses, these receipts allow senders to verify the correctness of each MIX and prove misbehavior to the witnesses. We suggest a simple model and metric for evaluating the reliability of a MIX-net, and show that our reputation system improves over randomly picking MIX paths while maintaining anonymity. 1
160|Mix-networks with Restricted Routes|We present a mix network topology that is based on sparse expander graphs, with each mix only communicating with a few neighbouring others. We analyse the anonymity such networks provide, and compare it with fully connected mix networks and mix cascades. We prove that such a topology is efficient since it only requires the route length of messages to be relatively small in comparison with the number of mixes to achieve maximal anonymity. Additionally mixes can resist intersection attacks while their batch size, that is directly linked to the latency of the network, remains constant. A worked example of a network is also presented to illustrate how these results can be applied to create secure mix networks in practise.
161|Data Integration: A Theoretical Perspective|Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. The problem of designing data integration systems is important in current real world applications, and is characterized by a number of issues that are interesting from a theoretical point of view. This document presents on overview of the material to be presented in a tutorial on data integration. The tutorial is focused on some of the theoretical issues that are relevant for data integration. Special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and reasoning on queries.
162|A Survey of Approaches to Automatic Schema Matching|Schema matching is a basic problem in many database application domains, such as data integration, E-business, data warehousing, and semantic query processing. In current implementations, schema matching is typically performed manually, which has significant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match operation for specific application domains. We present a taxonomy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distinguish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classification we review some previous match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component.
163|The Lorel Query Language for Semistructured Data|We present the Lorel language, designed for querying semistructured data. Semistructured data is becoming more and more prevalent, e.g., in structured documents such as HTML and when performing simple integration of data from multiple sources. Traditional data models and query languages are inappropriate, since semistructured data often is irregular, some data is missing, similar concepts are represented using different types, heterogeneous sets are present, or object structure is not fully known. Lorel is a user-friendly language in the SQL/OQL style for querying such data effectively. For wide applicability, the simple object model underlying Lorel can be viewed as an extension of ODMG and the language as an extension of OQL. The main novelties of the Lorel language are: (i) extensive use of coercion to relieve the user from the strict typing of OQL, which is inappropriate for semistructured data
164|Querying Heterogeneous Information Sources Using Source Descriptions|We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can ask). (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an imp...
165|Answering Queries Using Views: A Survey|The problem of answering queries using views is to find efficient methods of answering a query using a set of previously defined materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems. In query optimization, finding a rewriting of a query using a set of materialized views can yield a more efficient query execution plan. To support the separation of the logical and physical views of data, a storage schema can be described using views over the logical schema. As a result, finding a query execution plan that accesses the storage amounts to solving the problem of answering queries using views. Finally, the problem arises in data integration systems, where data sources can be described as precomputed views over a mediated schema. This article surveys the state of the art on the problem of answering queries using views, and synthesizes the disparate works into a coherent framework. We describe the different applications of the problem, the algorithms proposed to solve it and the relevant theoretical results.
166|Information integration using logical views|A number of ideas concerning information-integration tools can be thought of as constructing answers to queries using views that represent the capabilities of information sources. We review the formal basis of these techniques, which are closely related to containment algorithms for conjunctive queries and/or Datalog programs. Then we compare the approaches taken by AT&amp;T Labs&#039; &#034;Information Manifold&#034; and the Stanford &#034;Tsimmis&#034; project in these terms.
167|Answering queries using views|We consider the problem of computing answers to queries by using materialized views. Aside from its potential in optimizing query evaluation, the problem also arises in applications such as Global Information Systems, Mobile Computing and maintaining physical data independence. We consider the problem of nding a rewriting of a query that uses the materialized views, the problem of nding minimal rewritings, and nding complete rewritings (i.e., rewritings that use only the views). We show that all the possible rewritings can be obtained by considering containment mappings from the views to the query, and that the problems we consider are NP-complete when both the query and the views are conjunctive and don&#039;t involve built-in comparison predicates. We show that the problem has two independent sources of complexity (the number of possible containment mappings, and the complexity of deciding which literals from the original query can be deleted). We describe a polynomial time algorithm for nding rewritings, and show that under certain conditions, it will nd the minimal rewriting. Finally, we analyze the complexity of the problems when the queries and views may be disjunctive and involve built-in comparison predicates. 1
168|The TSIMMIS Approach to Mediation: Data Models and Languages|TSIMMIS -- The Stanford-IBM Manager of Multiple Information Sources -- is a system for integrating information. It o ers a data model and a common query language that are designed to support the combining of information from many different sources. It also o ers tools for generating automatically the components that are needed to build systems for integrating information. In this paper we shall discuss the principal architectural features and their rationale. 
169|A Query Language and Optimization Techniques for Unstructured Data|A new kind of data model has recently emerged in which the database is not constrained by a conventional schema. Systems like ACeDB, which has become very popular with biologists, and the recent Tsimmis proposal for data integration organize data in tree-like structures whose components can be used equally well to represent sets and tuples. Such structures allow great flexibility in data representation  What query language is appropriate for such structures? Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or &#034;vertical&#034; dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the &#034;horizontal&#034; dimension of UnQL.  
170|Index Structures for Path Expressions|In recent years there has been an increased interest in managing data which does not conform to traditional data models, like the relational or object oriented model. The reasons for this non-conformance are diverse. One one hand, data may not conform to such models at the physical level: it may be stored in data exchange formats, fetched from the Internet, or stored as structured les. One the other hand, it may not conform at the logical level: data may have missing attributes, some attributes may be of di erent types in di erent data items, there may be heterogeneous collections, or the data may be simply specified by a schema which is too complex or changes too often to be described easily as a traditional schema. The term semistructured data has been used to refer to such data. The data model proposed for this kind of data consists of an edge-labeled graph, in which nodes correspond to objects and edges to attributes or values. Figure 1 illustrates a semistructured database providing information about a city. Relational databases are traditionally queried with associative queries, retrieving tuples based on the value of some attributes. To answer such queries efciently, database management systems support indexes for translating attribute values into tuple ids (e.g. B-trees or hash tables). In object-oriented databases, path queries replace the simpler associative queries. Several data structures have been proposed for answering path queries e ciently: e.g., access support relations 14] and path indexes 4]. In the case of semistructured data, queries are even more complex, because they may contain generalized path expressions 1, 7, 8, 16]. The additional exibility is needed in order to traverse data whose structure is irregular, or partially unknown to the user.
171|Complexity of answering queries using materialized views|WC study the complexity of the problem of answering queries using materinlized views, This problem has attracted a lot of attention re-cently because of its relevance in data integration. Previous work considered only conjunctive view definitions. We examine the con-sequences of allowing more expressive view definition languages. Tl~olanguagcsweconsiderforviewdefinitionsanduserqueriesare: conjunctive qucrics with inequality, positive queries, datalog, and first-order logic. We show that the complexity of the problem de-pcnds on whether views are assumed to store all the tuples that sat-isfy the view definition, or only a subset of it. Finally, we apply the results to the view consistency and view self-maintainability prob-lems which nrise in data warehousing. 1
172|Semistructured data|In semistructured data, the information that is normally as-sociated with a schema is contained within the data, which is sometimes called “self-describing”. In some forms of semi-structured data there is no separate schema, in others it exists but only places loose constraints on the data. Semi-structured data has recently emerged as an important topic of study for a variety of reasons. First, there are data sources such as the Web, which we would like to treat as databases but which cannot be constrained by a schema. Second, it may be desirable to have an extremely flexible format for data exchange between disparate databases. Third, even when dealing with structured data, it may be helpful to view it. as semistructured for the purposes of browsing. This tu-torial will cover a number of issues surrounding such data: finding a concise formulation, building a sufficiently expres-sive language for querying and transformation, and opti-mizat,ion problems. 1 The motivation The topic of semistructured data (also called unstructured data) is relatively recent, and a tutorial on the topic may well be premature. It represents, if anything, the conver-gence of a number of lines of thinking about new ways to represent and query data that do not completely fit with conventional data models. The purpose of this tutorial is to to describe this motivation and to suggest areas in which further research may be fruitful. For a similar exposition, the reader is referred to Serge Abiteboul’s recent survey pa-per PI. The slides for this tutorial will be made available from a section of the Penn database home page
173| On the Decidability of Query Containment under Constraints |Query containment under constraints is the problem of checking whether for every database satisfying a given set of constraints, the result of one query is a subset of the result of another query. Recent research points out that this is a central problem in several database applications, and we address it within a setting where constraints are specified in the form of special inclusion dependencies over complex expressions, built by using intersection and difference of relations, special forms of quantification, regular expressions over binary relations, and cardinality constraints. These types of constraints capture a great variety of data models, including the relational, the entity-relational, and the object-oriented model. We study the problem of checking whether q is contained in q ' with respect to the constraints specified in a schema S, where q and q ' are nonrecursive Datalog programs whose atoms are complex expressions. We present the following results on query containment. For the case where q does not contain regular expressions, we provide a method for deciding query containment, and analyze its computational complexity. We do the same for the case where neither S nor q, q ' contain number restrictions. To the best of our knowledge, this yields the first decidability result on containment of conjunctive queries with regular expressions. Finally, we prove that the problem is undecidable for the case where we admit inequalities in q'.  
174|Optimizing Queries with Materialized Views|While much work has addressed the problem of maintaining materialized views, the important question of optimizing queries in the presence of materialized views has not been resolved. In this paper, we analyze the optimization question and provide a comprehensive and efficient solution. Our solution has the desirable property that it is a simple generalization of the traditional query optimization algorithm. 1 Introduction  The idea of using materialized views for the benefit of improved query processing has been proposed in the literature more than a decade ago. In this context, problems such as definition of views, composition of views, maintenance of views [BC79, KP81, SI84, BLT86, CW91, Rou91, GMS93] have been researched but one topic has been conspicuous by its absence. This concerns the problem of the judicious use of materialized views in answering a query. It may seem that materialized views should be used to evaluate a query whenever they are applicable. In fact, blind applicat...
175|Integration of Heterogeneous Databases Without Common Domains Using Queries Based on Textual Similarity|Most databases contain &#034;name constants&#034; like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. However, in many cases, this assumption does not hold; determining if two name constants should be considered identical can require detailed knowledge of the world, the purpose of the user&#039;s query, or both. In this paper, we reject the assumption that global domains can be easily constructed, and assume instead that the names are given in natural language text. We then propose a logic called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. We describe an efficient implementation of WHIRL and evaluate it experimentally on data extracted from the World Wide Web. We show that WHIR...
176|Context Interchange: New Features and Formalisms for the Intelligent Integration of Information|The Context Interchange strategy presents a novel perspective for mediated data access in which semantic conflicts among heterogeneous systems are not identified a priori, but are detected and reconciled by a context mediator through comparison of contexts axioms corresponding to the systems engaged in data exchange. In this article, we show that queries formulated on shared views, export schema, and shared “ontologies ” can be mediated in the same way using the Context Interchange framework. The proposed framework provides a logic-based object-oriented formalism for representing and reasoning about data semantics in disparate systems, and has been validated in a prototype implementation providing mediated data access to both traditional and web-based information sources. Categories and Subject Descriptors: H.2.4 [Database Management]: Systems—Query processing; H.2.5 [Database Management]: Heterogeneous Databases—Data translation
177|Answering queries using templates with binding patterns|When integrating heterogeneous information re-sources, it is often the case that the source is rather limited in the kinds of queries it can answer. If a query is asked of the entire system, we have a new kind of optimization problem, in which we must try to express the given query in terms of the lim-ited query templates that this source can answer. For the case of conjunctive queries, we show how to decide with a nondeterministic polynomial-time al-gorithm whether the given query can be answered. We then extend our results to allow arithmetic comparisons in the given query and in the tem-plates.
178|Catching the Boat with Strudel: Experiences with a Web-Site Management System|The Strudel system applies concepts from database management systems to the process of building Web sites. Strudel&#039;s key idea is separating the management of the site&#039;s data, the creation and management of the site&#039;s structure, and the visual presentation of the site&#039;s pages. First, the site builder creates a uniform model of all data available at the site. Second, the builder uses this model to declaratively define the Web site&#039;s structure by applying a &#034;site-definition query&#034; to the underlying data. The result of evaluating this query is a &#034;site graph&#034;, which represents both the site&#039;s content and structure. Third, the builder specifies the visual presentation of pages in Strudel&#039;s HTML-template language. The data model underlying Strudel is a semi-structured model of labeled directed graphs. We describe Strudel&#039;s key characteristics, report on our experiences using Strudel, and present the technical problems that arose from our experience. We describe our experience constructing sev...
179|Answering Recursive Queries Using Views|We consider the problem of answering datalog queries using materialized views. The ability to answer queries using views is crucial in the context of information integration. Previous work on answering queries using views restricted queries to being conjunctive. We extend this work to general recursive queries: Given a datalog program P and a set of views, is it possible to find a datalog program that is equivalent to P and only uses views as EDB predicates? In this paper, we show that the problem of whether a datalog program can be rewritten into an equivalent program that only uses views is undecidable. On the other hand, we prove that a datalog program P can be effectively rewritten into a program that only uses views, that is contained in P,  and that contains all programs that only use views and are contained in P. As a consequence, if there exists a program equivalent to P that only uses views, then our construction is guaranteed to yield a program equivalent to P.  1 Introductio...
180|Query Caching and Optimization in Distributed Mediator Systems|Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source&#039;s performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a costbased optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants  mechanism, which s...
181|Managing Semantic Heterogeneity in Databases : A Theoretical Perspective, Tutorial at PODS|A full version of this tutorial appears at
182|Description Logics in Data Management|Description logics and reasoners, which are descendants of the kl-one language, have been studied in depth in Artificial Intelligence. After a brief introduction, we survey in this paper their application to the problems of information management, using the framework of an abstract information server equipped with several operations -- each involving one or more languages. Specifically, we indicate how one can achieve enhanced access to data and knowledge by using descriptions in languages for schema design and integration, queries, answers, updates, rules, and constraints.
183|The Information Manifold|We describe the Information Manifold (IM), a system for browsing and querying of multiple networked information sources. As a first contribution, the system demonstrates the viability of knowledge representation technology for retrieval and organization of information from disparate (structured and unstructured) information sources. Such an organization allows the user to pose high-level queries that use data from multiple information sources. As a second contribution, we describe novel query processing algorithms used to combine information from multiple sources. In particular, our algorithms are guaranteed to find exactly the set of information sources relevant to a query, and to  completely exploit knowledge about local closed world information (Etzioni et al. 1994). Introduction  We are currently witnessing an explosion in the amount of information that is available online. For example, the rapid rise in popularity of the World Wide Web (WWW) has increased the amount of information...
184|Optimizing Regular Path Expressions Using Graph Schemas|Query languages for data with irregular structure use regular path expressions for navigation. This feature is useful for querying data where parts of the structure is either unknown, unavailable to the user, or changes frequently. Naive execution of regular path expressions is inefficient, however, because it ignores any structure in the data. We describe two optimization techniques for queries with regular path expressions. Both rely on graph schemas for specifying partial knowledge about the data&#039;s structure. Query pruning uses this structure to restrict navigation to only a fragment of the data; we give an efficient algorithm for rewriting any regular path expression query into a pruned one. Query rewriting using state extents  can eliminate or reduce navigation altogether; it is reminiscent of optimizing relational queries using indices. There may be several ways to optimize a query using state extents; we give a polynomial-space algorithm that finds all such optimizations. For re...
185|Description logic framework for information integration|Information Integration is one of the core problems in distributed databases, cooperative information systems, and data warehousing, which are key areas in the software development industry. Two critical factors for the design and maintenance of applications requiring Information Integration are conceptual modeling of the domain, and reasoning support over the conceptual representation. We demonstrate that Knowledge Representation and Reasoning techniques can play an important role for both of these factors, by proposing a Description Logic based framework for Information Integration. We show that the development of successful Information Integration solutions requires not only to resort to very expressive Description Logics, but also to significantly extend them. We present a novel approach to conceptual modeling for Information Integration, which allows for suitably modeling the global concepts of the application, the individual information sources, and the constraints among different sources. Moreover, we devise inference procedures for the fundamental reasoning services, namely relation and concept subsumption, and query containment. Finally, we present a methodological framework for Information Integration, which can be applied in several contexts, and highlights the role of reasoning services within the design process. 1
186|Equivalences among relational expressions with the union and difference operators|ABSTRACT Queries in relational databases can be formulated in terms of relational expressions using the relational operations elect, project, join, union, and difference The equivalence problem for these queries is studied with query optimization m mind It ts shown that testmg eqmvalence of relational expressions with the operators elect, project, join, and union is complete m the class FIt of the polynomial-time hierarchy A nonprocedural representation for queries formulated by these expressions i proposed This method of query representation can be viewed as a generahzatlon f tableaux or conjunctive queries (which are used to represent expressions with only select, project, and join) Furthermore, this method is extended to queries formulated by relatmnal expressions that also contain the difference operator, provided that the project operator isnot applied to subexpresstons with the difference operator A procedure for testing eqmvalence of these queries is given It ts shown that testmg containment of tableaux is a necessary step in testing equivalence of queries with union and difference Three important cases m which containment of tableaux can be tested m polynomial time are described, although the containment problem is shown to be NP-complete ven for tableaux that correspond to expressions with only one project and several join operators KEY WORDS AND PHRASES relatmnal database, relational algebra, query optimization, equivalence of queries, conjunctive query, tableau, NP-complete, polynomial-time hierarchy, H 2P-complete CR CATEGORIES 4 33, 5 25
187|Representing and Using Interschema Knowledge in Cooperative Information Systems|Managing interschema knowledge is an essential task when dealing with cooperative information systems. We propose a logical approach to the problem of both expressing interschema knowledge, and reasoning about it. In particular, we set up a structured representation language for expressing semantic interdependencies between classes belonging to different database schemas, and present a method for reasoning over such interdependencies. The language and the associated reasoning technique makes it possible to build a logic-based module that can draw useful inferences whenever the need arises of both comparing and combining the knowledge represented in the various schemas. Notable examples of such inferences include checking the coherence of interschema knowledge, and providing integrated access to a cooperative information system.
188|Navigational Plans For Data Integration|We consider the problem of building data integration systems when the data sources are webs of data, rather than sets of relations. Previous approaches to modeling data sources are inappropriate in this context because they do not capture the relationships between linked data and the need to navigate through paths in the data source in order to obtain the data. We describe a language for modeling data sources in this new context. We show that our language has the required expressive power, and that minor extensions to it would make query answering intractable. We provide a sound and complete algorithm for reformulating a user query into a query over the data sources, and we show how to create query execution plans that both query and navigate the data sources.  Introduction  The purpose of data integration is to provide a uniform  interface to a multitude of data sources. Data integration applications arise frequently as corporations attempt to provide their customers and employees wit...
189|Tableau Techniques For Querying Information Sources Through Global Schemas|. The foundational homomorphism techniques introduced by Chandra and Merlin for testing containment of conjunctive queries have recently attracted renewed interest due to their central role in information integration applications. We show that generalizations of the classical tableau representation of conjunctive queries are useful for computing query answers in information integration systems where information sources are modeled as views defined on a virtual global schema. We consider a general situation where sources may or may not be known to be correct and complete. We characterize the set of answers to a global query and give algorithms to compute a finite representation of this possibly infinite set, as well as its certain and possible approximations. We show how to rewrite a global query in terms of the sources in two special cases, and show that one of these is equivalent to the Information Manifold rewriting of Levy et al. 1 Introduction Information Integration systems [Ull...
190|What Can Databases Do for Peer-to-Peer?|The Internet community has recently been focused on peer-to-peer systems like Napster, Gnutella, and Freenet. The  grand vision --- a decentralized community of machines pooling their resources to benefit everyone --- is compelling for  many reasons: scalability, robustness, lack of need for administration, and even anonymity and resistance to censorship.
191|Data Integration under Integrity Constraints|Data integratio n systemspro vide accessto a seto fhetero - geneo us, auto no mo us data so urces thro ugh a so -called glo bal schema. There are basically two appro aches fo r designing a data integratio n system. In the glo bal-centric appro ach,o ne defines the elementso f the glo bal schema as viewso ver the so urces, whereas in the lo cal-centric appro ach, o e characterizes the so rces as viewso ver theglo al schema. It is well kno wn that pro cessing queries in the latter appro ach is similar to query answering with inc o plete infoC atio , and, therefo9 is a c o plex task. On theo ther hand, it is a co mmo no pinio n that query pro cessing is much easier in the fo rmer appro ach. In this paper we sho w the surprising result that, when theglo al schema is expressed in the relatio al mo del with integrity c o straints, eveno f simple types, the pr o lemo f inco6 plete info rmatio n implicitly arises, making querypro cessing di#cult in the glo al-centric approC h as well. We thenfo cuso n glo al schemas with key andfo eign key co straints, which represents a situat io which is veryco#=W in practice, and we illustrate techniques fo e#ectively answering queries po sed to the data integratio n system in this case. 1 
192|Towards Heterogeneous Multimedia Information Systems: The Garlic Approach|Abstract: We provide an overview of the Garlic project, a new project at the IBM Almaden Research Center. The goal of this project is to develop a system and associated tools for the management of large quantities of heterogeneous multimedia information. Garlic permits traditional and multimedia data to be
193|Rewriting of Regular Expressions and Regular Path Queries|Recent work on semi-structured data has revitalized the interest in path queries, i.e., queries that ask for all pairs of objects in the database that are connected by a path conforming to a certain specification, in particular to a regular expression. Also, in semi-structured data, as well as in data integration, data warehousing, and query optimization, the problem of view-based query rewriting is receiving much attention: Given a query and a collection of views, generate a new query which uses the views and provides the answer to the original one. In this paper we address the problem of view-based query rewriting in the context of semi-structured data. We present a method for computing the rewriting of a regular expression E in terms of other regular expressions. The method computes the exact rewriting (the one that defines the same regular language as E) if it exists, or the rewriting that defines the maximal language contained in the one defined by E, otherwise. We present a complexity analysis of both the problem and the method, showing that the latter is essentially optimal. Finally, we illustrate how to exploit the method for view-based rewriting of regular path queries in semi-structured data. The complexity results established for the rewriting of regular expressions apply also to the case of regular path queries. 
194|CARIN: A Representation Language Combining Horn Rules and Description Logics|.  We describe CARIN, a novel family of representation languages, which integrate the expressive power of Horn rules and of description logics. We address the key issue in designing such a language, namely, providing a sound and complete inference procedure. We identify existential entailment as a core problem in reasoning in  CARIN, and describe an existential entailment algorithm for CARIN  languages whose description logic component is ALCNR. This algorithm entails several important results for reasoning in CARIN, most notably: (1) a sound and complete inference procedure for non recursive  CARIN-ALCNR, and (2) an algorithm for determining rule subsumption over ALCNR. 1 Introduction  Horn rule languages have formed the basis for many Artificial Intelligence application languages because their expressive power is sufficient for many applications, and they have good computational properties. One of the significant limitations of Horn rules is that they are not expressive enough to mod...
195|Query optimization in the presence of limited access patterns|We consider the problem of query optimization in the presence of limitations on access patterns to the data (i.e., when one must provide values for one of the attributes of a relation in order to obtain tuples). We show that in the presence of limited access patterns we must search a space of annotated query plans, where the annotations describe the inputs that must be given to the plan. We describe a theoretical and experimental analysis of the resulting search space and a novel query optimization algorithm that is designed to perform well under the di erent conditions that may arise. The algorithm searches the set of annotated query plans, pruning invalid and non-viable plans as early as possible in the search space, and it also uses a best- rst search strategy in order to produce a rst complete plan early in the search. We describe experiments to illustrate the performance of our algorithm. 1
196|Quality-driven Integration of Heterogeneous Information Systems|Integrated access to information that is spread over multiple, distributed, and heterogeneous sources is an important problem in many scientific and commercial domains. While much work has been done on query processing and choosing plans under cost criteria, very little is known about the important problem of incorporating the information quality aspect into query planning. In this paper we describe a framework for multidatabase query processing that fully includes the quality of information in many facets, such as completeness, timeliness, accuracy, etc. We seamlessly include information quality into a multidatabase query processor based on a view-rewriting mechanism. We model information quality at different levels to ultimately find a set of high-quality query answering plans.
197|Query Containment for Conjunctive Queries With Regular Expressions|The management of semistructured data has recently received significant attention because of the need of several applications to model and query large volumes of irregular data. This paper considers the problem of query containment for a query language over semistructured data, StruQL0 , that contains the essential feature common to all such languages, namely the ability to specify regular path expressions over the data. We show here that containment of StruQL0 queries is decidable. First, we give a semantic criterion for StruQL0 query containment: we show that it suffices to check containment on only finitely many canonical databases. Second, we give a syntactic criteria for query containment, based on a notion of query mappings, which extends containment mappings for conjunctive queries. Third, we consider a certain fragment of StruQL0 , obtained by imposing restrictions on the regular path expressions, and show that query containment for this fragment of  StruQL0 is NP complete.  1 ...
198|Recursive Plans for Information Gathering|Generating query-answering plans for information gathering agents requires to translate a user query, formulated in terms of a set of virtual relations, to a query that uses relations that are actually stored in information sources. Previous solutions to the translation problem produced sets of conjunctive plans, and were therefore limited in their ability to handle information sources with binding-pattern limitations, and to exploit functional dependencies in the domain model. As a result, these plans were incomplete w.r.t. sources encountered in practice (i.e., produced only a subset of the possible answers). We describe the novel class of recursive  information gathering plans, which enables us to settle two open problems. First, we describe an algorithm for finding a query plan that produces the maximal set of answers from the sources in the presence of functional dependencies. Second, we describe an analogous algorithm in the presence of binding-pattern restrictions in the sources...
199|EQUIVALENCES AMONG RELATIONAL EXPRESSIONS|Many database queries can be formulated in terms of expressions whose operands represent tables of information (relations) and whose operators are the relational operations select, project, and join. This paper studies the equivalence problem for these relational expressions, with expression optimization in mind. A matrix, called a tableau, is proposed as a natural representative for the value of an expression. It is shown how tableaux can be made to reflect functional dependencies among attributes. A polynomial time algorithm is presented for the equivalence of tableaux that correspond to an important subset of expressions, although the equivalence problem is shown to be NP-complete under slightly more general circumstances.
200|On the equivalence of recursive and nonrecursive Datalog programs|vardi Abstract: We study the problem of determining whether a given recursive Datalog program is equivalent to a given nonrecursive Datalog program. Since nonrecursive Dat-alog programs are equivalent to unions of conjunctive queries, we study also the problem of determining whether a given recursive Datalog program is contained in a union of con-junctive queries. For this problem, we prove doubly exponential upper and lower time bounds. For the equivalence problem, we prove triply exponential upper and lower time bounds. 1
201|Containment of conjunctive regular path queries with inverse |Reasoning on queries is a basic problem both in knowledge representation and databases. A fundamental form of reasoning on queries is checking containment, i.e., verifying whether one query yields necessarily a subset of the result of another query. Query containment is crucial in several contexts, such as query optimization, knowledge base verification, information integration, database integrity checking, and cooperative answering. In this paper we address the problem of query containment in the context of semistructured knowledge bases, where the basic querying mechanism, namely regular path queries,
202|Rewriting Aggregate Queries Using Views|We investigate the problem of rewriting queries with aggregate operators using views that mayormay not contain aggregate operators. A rewriting of a query is a second query that uses view predicates such that evaluating first the views and then the rewriting yields the same result as evaluating the original query. In this sense, the original query and the rewriting are equivalent modulo the view definitions. The queries and views we consider correspond to unnested SQL queries, possibly with union, that employ the operators min, max, count, and sum. Our approach is based on syntactic characterizations of the equivalence of aggregate queries. One contribution of this paper are characterizations of the equivalence of disjunctive aggregate queries, which generalize our previous results for the conjunctive case. For each operator &amp;alpha;, we introduce several types of queries using views as candidates for rewritings. We unfold such a candidate by replacing each occurrence of a view predicate with ...
203|An Extensible Framework for Data Cleaning|Data integration solutions dealing with large amounts of data have been strongly required in the last few years.  Besides the traditional data integration problems (e.g. schema integration, local to global schema mappings),  three additional data problems have to be dealt with: (1) the absence of universal keys across dierent databases  that is known as the object identity problem, (2) the existence of keyboard errors in the data, and (3) the presence  of inconsistencies in data coming from multiple sources. Dealing with these problems is globally called the data  cleaning process. In this work, we propose a framework which oers the fundamental services required by this  process: data transformation, duplicate elimination and multi-table matching. These services are implemented  using a set of purposely designed macro-operators. Moreover, we propose an SQL extension for specifying  each of the macro-operators. One important feature of the framework is the ability of explicitly includ...
204|Answering regular path queries using views|Query answering using views amounts to computing the answer to a query having information only on the extension of a set of views. This problem is relevant in several fields, such as information integration, data warehousing, query optimization, mobile computing, and maintaining physical data independence. We address query answering using views in a context where queries and views are regular path queries, i.e., regular expressions that denote the pairs of objects in the database connected by a matching path. Regular path queries are the basic query mechanism when the database is conceived as a graph, such as in semistructured data and data on the web. We study algorithms for answering regular path queries using views under different assumptions, namely, closed and open domain, and sound, complete, and exact information on view extensions. We characterize data, expression, and combined complexity of the problem, showing that the proposed algorithms are essentially optimal. Our results are the first to exhibit decidability in cases where the language for expressing the query and the views allows for recursion. 
205|Query Planning and Optimization in Information Integration|Information integration systems, also knows as mediators, information brokers, or information gathering agents, provide uniform user interfaces to varieties of different information sources. With corporate databases getting connected by intranets, and vast amounts of information becoming available over the Internet, the need for information integration systems is increasing steadily. Our work focusses on query planning in such systems...
206|Answering queries using views over description logics knowledge bases|Answering queries using views amounts to computing the answer to a query having information only on the extension of a set of precomputed queries (views). This problem is relevant in several fields, such as information integration, query optimization, and data warehousing, and has been studied recently in different settings. In this paper we address answering queries using views in a setting where intensional knowledge about the domain is represented using a very expressive Description Logic equipped with n-ary relations, and queries are nonrecursive datalog queries whose predicates are the concepts and relations that appear in the Description Logic knowledge base. We study the problem under different assumptions, namely, closed and open domain, and sound, complete, and exact information on view extensions. We show that under the closed domain assumption, in which the set of all objects in the knowledge base coincides with the set of objects stored in the views, answering queries using views is already intractable. We show also that under the open domain assumption the problem is decidable in double exponential time.
207|A Formal Perspective on the View Selection Problem|The view selection problem is to choose a set of views to materialize over a database schema, such that the cost of evaluating a set of workload queries is minimized and such that the views t into a prespeci ed storage constraint. The two main applications of the view selection problem are materializing views in a database to speed up query processing, and selecting views to materialize in a data warehouse to answer decision support queries. We describe several fundamental results concerning the view selection problem. We consider the problem for views and workloads that consist of equalityselection, project and join queries, and show that the complexity of the problem depends crucially on the quality of the estimates that a query optimizer has on the size of the views it is considering to materialize. When a query optimizer has good estimates of the sizes of the views, we show that an optimal choice of views may involve a number of views that is exponential in the size of the database schema. On the other hand, when an optimizer uses standard estimation heuristics, we show that the number of necessary views and the expression size of each view are polynomially bounded. 1
208|Query containment for data integration systems|The problem of query containment is fundamental to many aspects of database systems,including query optimization,determining independence of queries from updates,and rewriting queries using views. In the data-integration framework,however,the standard notion of query containment does not suffice. We define relative containment,which formalizes the notion of query containment relative to the sources available to the data-integration system. First,we provide optimal bounds for relative containment for several important classes of datalog queries,including the common case of conjunctive queries. Next,we provide bounds for the case when sources enforce access restrictions in the form of binding pattern constraints. Surprisingly,we show that relative containment for conjunctive queries is still decidable in this case,even though it is known that finding all answers to such queries may require a recursive datalog program over the sources. Finally,we provide tight bounds for variants of relative containment when the queries and source descriptions may contain comparison predicates.
209|View-based query processing and constraint satisfaction|View-based query processing requires to answer a query posed to a database only on the basis of the information on a set of views, which are again queries over the same database. This problem is relevant in many aspects of database management, and has been addressed by means of two basic approaches, namely, query rewriting and query answering. In the former approach, one tries to compute a rewriting of the query in terms of the views, whereas in the latter, one aims at directly answering the query based on the view extensions. We study view-based query processing for the case of regular-path queries, which are the basic querying mechanisms for the emergent field of semistructured data. Based on recent results, we first show that a rewriting is in general a co-NP function wrt to the size of view extensions. Hence, the problem arises of characterizing which instances of the problem admit a rewriting that is PTIME. A second contribution of the work is to establish a tight connection between view-based query answering and constraint-satisfaction problems, which allows us to show that the above characterization is going to be difficult. As a third contribution of our work, we present two methods for computing PTIME rewritings of specific forms. The first method, which is based on the established connection with constraint-satisfaction problems, gives us rewritings expressed in Datalog with a fixed number of variables. The second method, based on automata-theoretic techniques, gives us rewritings that are formulated as unions of conjunctive regular-path queries with a fixed number of variables.  
210|Optimization Properties for Classes of Conjunctive Regular Path Queries |Abstract. We are interested in the theoretical foundations of the optimization of conjunctive regular path queries (CRPQs). The basic problem here is deciding query containment both in the absence and presence of constraints. Containment without constraints for CRPQs is EXPSPACE-complete, as opposed to only NP-complete for relational conjunctive queries. Our past experience with implementing similar algorithms suggests that staying in PSPACE might still be useful. Therefore we investigate the complexity of containment for a hierarchy of fragments of the CRPQ language. The classifying principle of the fragments is the expressivity of the regular path expressions allowed in the query atoms. For most of these fragments, we give matching lower and upper bounds for containment in the absence of constraints. We also introduce for every fragment a naturally corresponding class of constraints in whose presence we show both decidability and undecidability results for containment in various fragments. Finally, we apply our results to give a complete algorithm for rewriting with views in the presence of constraints for a fragment that contains Kleene-star and disjunction. 1
211|Deciding Containment for Queries with Complex Objects and Aggregations|We address the problem of query containment and query equivalence for complex objects. We show that for a certain conjunctive query language for complex objects, query containment and weak query equivalence are decidable. Our results have two consequences. First, when the answers of the two queries are guaranteed not to contain empty sets, then weak equivalence coincides with equivalence, and our result answers partially an open problem about the equivalence of nest; unnest queries for complex objects [GPG90]. Second, we derive an NP-complete algorithm for checking the equivalence of certain conjunctive queries with grouping and aggregates. Our results rely on a translation of the containment and equivalence conditions for complex objects into novel conditions on conjunctive queries, which we call simulation and strong simulation. These conditions are more complex than containment of conjunctive queries, because they involve arbitrary numbers of quantifier alternations. We prove that c...
212|Capability Based Mediation in TSIMMIS|Introduction  The TSIMMIS system [1] integrates data from multiple heterogeneous sources and provides users with seamless integrated views of the data. It translates a user query on the integrated views into a set of source queries and postprocessing steps that compute the answer to the user query from the results of the source queries. TSIMMIS uses a  mediation architecture [11] to accomplish this (Figure 1). User Source 1 Source 2 Source N  Mediator Figure 1: The TSIMMIS Architecture Many other data integration systems like Garlic [2, 9] and Information Manifold [4] employ a similar architecture. One of the distinguishing features of TSIMMIS is its use of a semi-structured data model (called the Object Exchange Model or OEM [7]) for dealing with the heterogeneity of the data sources. In particular, it employs source wrappers [3] that provide a uniform OEM interface to the mediator. In SIGMO
213|Query Answering in Information Systems with Integrity Constraints|The specifications of most of the nowadays ubiquitous informations systems include integrity constraints, i.e. conditions rejecting so-called &#034;invalid&#034; or &#034;inconsistent &#034; data. Information system consistency and query answering have been formalized referring to classical logic implicitly assuming that query answering only makes sense with consistent information systems. In practice, however, inconsistent as well as consistent information systems need to be queried. In this paper, it is first argued that classical logic is inappropriate for a formalization of information systems because of its global notion of inconsistency. It is claimed that information systems inconsistency should be understood as a  local notion. Then, it is shown that minimal logic, a constructivistic weakening of classical logic which precludes refutation proofs, provides for local inconsistencies that conveniently reflect a practitioner&#039;s intuition. Further, minimal logic is shown to be a convenient foundation fo...
214|Query Folding with Inclusion Dependencies|Query folding is a technique for determining how a query may be answered using a given set of resources, which may include materialized views, cached results of previous queries, or queries answerable by other databases. The power of query folding can be considerably enhanced by taking into account integrity constraints that are known to hold on base relations. This paper describes an extension of query folding that utilizes inclusion dependencies to find foldings of queries that would otherwise be overlooked. We describe a complete strategy for finding foldings in the presence of inclusion dependencies and present a basic algorithm that implements that strategy. We also describe extensions to this algorithm when both inclusion and functional dependencies are considered.
215|Information Integration: the MOMIS Project Demonstration|ranted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 26th VLDB Conference,  Cairo, Egypt, 2000.  2  MOMIS is a joint project among the Universit`a di Modena e Reggio Emilia, the Universit`a di Milano, and the Universit`a di Brescia within the national research project INTERDATA, theme n.3 &#034;Integration of Information over the Web&#034;, coordinated by V. De Antonellis, Universit`a di Brescia.  1. a common data model, ODM I 3  , which is defined according to the ODL I 3 language, to describe source schemas for integration purposes. ODM I 3 and ODL I 3&lt;F12.24
216|Query Planning with Limited Source Capabilities|In information-integration systems, sources may have diverse and limited query capabilities. In this paper we show that because sources have restrictions on retrieving their information, sources not mentioned in a query can contribute to the query result by providing useful bindings. In some cases we can access sources repeatedly to retrieve bindings to answer a query, and query planning thus becomes considerably more challenging. We find all the obtainable answers to a query by translating the query and source descriptions to a simple recursive Datalog program, and evaluating the program on the source relations. This program often accesses sources that are not in the query. Some of these accesses are essential, as they provide bindings that let us query sources, which we could not do otherwise. However, some of these accesses can be proven not to add anything to the query&#039;s answer. We show in which cases these off-query accesses are useless, and prove that in these cases we can comput...
217|Querying Aggregate Data|We introduce a first-order language with real polynomial arithmetic and aggregation operators (count, iterated sum and multiply), which is well suited for the definition of aggregate queries involving complex statistical functions. It offers a good trade-off between expressive power and complexity, with a tractable data complexity. Interestingly, some fundamental properties of first-order with real arithmetic are preserved in the presence of aggregates. In particular, there is an effective quantifier elimination for formulae with aggregation. We consider the problem of querying data that has already been aggregated in aggregate views, and focus on queries with an aggregation over a conjunctive query. Our main conceptual contribution is the introduction of a new equivalence relation among conjunctive queries, the isomorphism modulo a product. We prove that the equivalence of aggregate queries such as for instance averages reduces to it. Deciding if two queries are isomorphic modulo a p...
218|Accessing data integration systems through conceptual schemas|Abstract. Data integration systems provide access to a set of heterogeneous, autonomous data sources through a so-called global, or mediated view. There is a general consensus that the best way to describe the global view is through a conceptual data model, and that there are basically two approaches for designing a data integration system. In the global-as-view approach, one defines the concepts in the global schema as views over the sources, whereas in the local-as-view approach, one characterizes the sources as views over the global schema. It is well known that processing queries in the latter approach is similar to query answering with incomplete information, and, therefore, is a complex task. On the other hand, it is a common opinion that query processing is much easier in the former approach. In this paper we show the surprising result that, when the global schema is expressed in terms of a conceptual data model, even a very simple one, query processing becomes difficult in the global-as-view approach also. We demonstrate that the problem of incomplete information arises in this case too, and we illustrate some basic techniques for effectively answering queries posed to the global schema of the data integration system. 1
219|Answering Queries Using Materialized Views With Disjunctions|We consider the problem of answering datalog queries using  materialized views. More  specifi.
220|On Answering Queries in the Presence of Limited Access Patterns|. In information-integration systems, source relations often  have limitations on access patterns to their data; i.e., when one must  provide values for certain attributes of a relation in order to retrieve its  tuples. In this paper we consider the following fundamental problem: can  we compute the complete answer to a query by accessing the relations  with legal patterns? The complete answer to a query is the answer that  we could compute if we could retrieve all the tuples from the relations.  We give algorithms for solving the problem for various classes of queries,  including conjunctive queries, unions of conjunctive queries, and conjunctive  queries with arithmetic comparisons. We prove the problem is  undecidable for datalog queries. If the complete answer to a query cannot  be computed, we often need to compute its maximal answer. The  second problem we study is, given two conjunctive queries on relations  with limited access patterns, how to test whether the maximal answer to...
221|Answering Queries Using Limited External Query Processors|When answering queries using external information sources, their contents can be described by views. To answer a query, we must rewrite it using the set of views presented by the sources. When the external information sources also have the ability to answer some (perhaps limited) sets of queries that require performing operations on their data, the set of views presented by the source may be infinite (albeit encoded in some finite fashion). Previous work on answering queries using views has only considered the case where the set of views is finite. In order to exploit the ability of information sources to answer more complex queries, we consider the problem of answering conjunctive queries using infinite sets of conjunctive views. Our first result is that an infinite set of conjunctive views can be partitioned into a finite number of equivalence classes, such that picking one view from every nonempty class is sufficient to determine whether the query can be answered using the views. Se...
222|On the Content of Materialized Aggregate Views|We consider the problem of answering queries using only materialized views. We first show that if the views subsume the query from the point of view of the information content, then the query can be answered using only the views, but the resulting query might be extremely inefficient. We then focus on aggregate views and queries over a single relation, which are fundamental in many applications such as data warehousing. We show that in this case, it is possible to guarantee that as soon as the views subsume the query, it can be completely rewritten in terms of the views in a simple query language. Our main contribution is the conception of various rewriting algorithms which run in polynomial time, and the proof of their completeness which relies on combinatorial arguments. Finally, we discuss the choice of materializing or not ratio views such as average and percentage, important for the design of materialized views. We show that it has an impact on the information content, which can b...
223|Lossless Regular Views|If the only information we have on a certain database is through a set of views, the question arises of whether this is sucient to answer completely a given query. We say that the set of views is lossless with respect to the query, if, no matter what the database is, we can answer the query by solely relying on the content of the views. The question of losslessness has various applications, for example in query optimization, mobile computing, data warehousing, and data integration. We study this problem in a context where the database is semistructured, and both the query and the views are expressed as regular path queries. The form of recursion present in this class prevents us from applying known results to our case.
224|View-based query answering and query containment over semistructured data|Abstract. The basic querying mechanism over semistructured data, namely regular path queries, asks for all pairs of objects that are connected by a path conforming to a regular expression. We consider conjunctive two-way regular path queries (C2RPQc’s), which extend regular path queries with two features. First, they add the inverse operator, which allows for expressing navigations in the database that traverse the edges both backward and forward. Second, they allow for using conjunctions of atoms, where each atom specifies that a regular path query with inverse holds between two terms, where each term is either a variable or a constant. For such queries we address the problem of view-based query answering, which amounts to computing the result of a query only on the basis of a set of views. More specifically, we present the following results: (1) We exhibit a mutual reduction between query containment and the recognition problem for view-based query answering for C2RPQc’s, i.e., checking whether a given tuple is in the certain answer to a query. Based on such a result, we can show that the problem of view-based query answering for C2RPQc’s is EXPSPACE-complete. (2) By exploiting techniques based on alternating two-way automata we show that for the restricted class of tree two-way regular path queries (in which the links between variables form a tree), query containment and view-based query answering are, rather surprisingly, in PSPACE (and hence, PSPACE-complete). (3) We present a technique to obtain view-based query answering algorithms that compute the whole set of tuples in the certain answer, instead of requiring to check each tuple separately. The technique is parametric wrt the query language, and can be applied both to C2RPQc’s and to tree-queries. 1
225|Models for Information Integration: Turning Local-as-View Into Global-as-View|There are basically two approaches for designing a data integration system. In the global-as-view approach, one defines the concepts in the global schema as views over the sources, whereas in the local-as-view approach, one characterizes the sources as views over the global schema. The goal of this paper is to verify whether we can transform a data integration system built with the local-as-view approach into a system following the global-as-view approach. We study the problem in a setting where the global schema is expressed in the relational model with inclusion dependencies, and the queries used in the integration systems (both the queries on the global schema, and the views in the mapping) are expressed in the language of conjunctive queries. The result we present is that such a transformation exists: we can always transform a local-as-view system into a global-as-view system such that, for each query, the set of answers to the query wrt the former is the same as the set of answers wrt the latter.
226|Query Rewriting using Semistructured Views|We address the problem of query rewriting for MSL, a semistructured language developed at Stanford in the TSIMMIS project for information integration. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting  queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing containment mappings, the chase, and unification -- techniques which were developed for structured, relational data. At the same time we develop an algorithm for equivalence checking of MSL queries. We show that the rewriting algorithm is sound and complete, i.e., it always finds every conjunctive MSL rewriting query of q, and we discuss its complexity. We currently incorporate the algorithm in the TSIMMIS system. 1 Introduction  Recently, many semistructured data models, query and view definition languages have been proposed [GM  +  97, MAG  +  97, BDHS96, AV97a, MM97, KS95, PGMU96,...
227|Intensional Query Answering by Partial Evaluation|. Intensional query answering aims at providing a response to a query addressed to a knowledge base by making use of the intensional knowledge as opposed to extensional. Such a response is an abstract description of the conventional answer that can be of interest in many situations, for example it may increase the cooperativeness of the system, or it may replace the conventional answer in case access to the extensional part of the knowledge base is costly as for Mobile Systems. In this paper we present a general framework to generate intensional answers in knowledge bases adhering to the logic programming paradigm. Such a framework is based on a program transformation technique, namely Partial Evaluation, and allows for generating complete and procedurally complete (wrt SLDNF-resolution) sets of intensional answers, treating both recursion and negation conveniently. Keywords: Knowledge bases, intensional query answering, logic programs, partial evaluation 1. Introduction Intensional an...
228|Mapping kernel objects to enable systematic integrity checking|Dynamic kernel data have become an attractive target for kernelmode malware. However, previous solutions for checking kernel integrity either limit themselves to code and static data or can only inspect a fraction of dynamic data, resulting in limited protection. Our study shows that previous solutions may reach only 28 % of the dynamic kernel data and thus may fail to identify function pointers manipulated by many kernel-mode malware. To enable systematic kernel integrity checking, in this paper we present KOP, a system that can map dynamic kernel data with nearly complete coverage and nearly perfect accuracy. Unlike previous approaches, which ignore generic pointers, unions and dynamic arrays when locating dynamic kernel objects, KOP (1) applies interprocedural points-to analysis to compute all possible types for generic pointers (e.g., void*), (2) uses a pattern matching algorithm to resolve
229|Program Analysis and Specialization for the C Programming Language|Software engineers are faced with a dilemma. They want to write general and wellstructured programs that are flexible and easy to maintain. On the other hand, generality has a price: efficiency. A specialized program solving a particular problem is often significantly faster than a general program. However, the development of specialized software is time-consuming, and is likely to exceed the production of today’s programmers. New techniques are required to solve this so-called software crisis. Partial evaluation is a program specialization technique that reconciles the benefits of generality with efficiency. This thesis presents an automatic partial evaluator for the Ansi C programming language. The content of this thesis is analysis and transformation of C programs. We develop several analyses that support the transformation of a program into its generating extension. A generating extension is a program that produces specialized programs when executed on parts of the input. The thesis contains the following main results.
230|Efficient Context-Sensitive Pointer Analysis for C Programs|This paper proposes an efficient technique for contextsensitive pointer analysis that is applicable to real C programs. For efficiency, we summarize the effects of procedures using partial transfer functions. A partial transfer function (PTF) describes the behavior of a procedure assuming that certain alias relationships hold when it is called. We can reuse a PTF in many calling contexts as long as the aliases among the inputs to the procedure are the same. Our empirical results demonstrate that this technique is successful---a single PTF per procedure is usually sufficient to obtain completely context-sensitive results. Because many C programs use features such as type casts and pointer arithmetic to circumvent the high-level type system, our algorithm is based on a low-level representation of memory locations that safely handles all the features of C. We have implemented our algorithm in the SUIF compiler system and we show that it runs efficiently for a set of C benchmarks. 1 Introd...
231|A Virtual Machine Introspection Based Architecture for Intrusion Detection|Today&#039;s architectures for intrusion detection force the IDS designer to make a difficult choice. If the IDS resides on the host, it has an excellent view of what is happening in that host&#039;s software, but is highly susceptible to attack. On the other hand, if the IDS resides in the network, it is more resistant to attack, but has a poor view of what is happening inside the host, making it more susceptible to evasion. In this paper we present an architecture that retains the visibility of a host-based IDS, but pulls the IDS outside of the host for greater attack resistance. We achieve this through the use of a virtual machine monitor. Using this approach allows us to isolate the IDS from the monitored host but still retain excellent visibility into the host&#039;s state. The VMM also offers us the unique ability to completely mediate interactions between the host software and the underlying hardware. We present a detailed study of our architecture, including Livewire, a prototype implementation. We demonstrate Livewire by implementing a suite of simple intrusion detection policies and using them to detect real attacks.
232|Unification-based Pointer Analysis with Directional Assignments|This paper describes a new algorithm for flow and context insensitive pointer analysis of C programs. Our studies show that the most common use of pointers in C programs is in passing the addresses of composite objects or updateable values as arguments to procedures. Therefore, we have designed a low-cost algorithm that handles this common case accurately. In terms of both precision and running time, this algorithm lies between Steensgaard&#039;s algorithm, which treats assignments bi-directionally using unification, and Andersen&#039;s algorithm, which treats assignments directionally using subtyping. Our &#034;one level flow&#034; algorithm uses a restricted form of subtyping to avoid unification of symbols at the top levels of pointer chains in the points-to graph, while using unification elsewhere in the graph. The method scales easily to large programs. For instance, we are able to analyze a 1.4 MLOC (million lines of code) program in two minutes, using less than 200MB of memory. At the same time, the pr...
233|SecVisor: A Tiny Hypervisor to Provide Lifetime Kernel Code Integrity for Commodity OSes|We propose SecVisor, a tiny hypervisor that ensures code integrity for commodity OS kernels. In particular, SecVisor ensures that only user-approved code can execute in kernel mode over the entire system lifetime. This protects the kernel against code injection attacks, such as kernel rootkits. SecVisor can achieve this property even against an attacker who controls everything but the CPU, the memory controller, and system memory chips. Further, SecVisor can even defend against attackers with knowledge of zero-day kernel exploits. Our goal is to make SecVisor amenable to formal verification and manual audit, thereby making it possible to rule out known classes of vulnerabilities. To this end, SecVisor offers small code size and small external interface. We rely on memory virtualization to build SecVisor and implement two versions, one using software memory virtualization and the other using CPU-supported memory virtualization. The code sizes of the runtime portions of these versions are 1739 and 1112 lines, respectively. The size of the external interface for both versions of SecVisor is 2 hypercalls. It is easy to port OS kernels to SecVisor. We port the Linux kernel version 2.6.20 by adding 12 lines and deleting 81 lines, out of a total of approximately 4.3 million lines of code in the kernel.
234|Ultra-fast aliasing analysis using CLA: a million lines of C code in a second|We describe the design and implementation of a system for very fast points-to analysis. On code bases of about a million lines of unpreprocessed C code, our system performs eldbased Andersen-style points-to analysis in less than a second and uses less than 10MB of memory. Our tw o main contributions are a database-centric analysis architecture called compile-link-analyze (CLA), and a new algorithm for implementing dynamic transitive closure. Our points-to analysis system is built into a forward data-dependence analysis tool that is deployed within Lucent to help with consistent type modi cations to large legacy C code bases. 1.
235|Automated Detection of Persistent Kernel Control-Flow Attacks|This paper presents a new approach to dynamically monitoring operating system kernel integrity, based on a property called state-based control-flow integrity (SBCFI). Violations of SBCFI signal a persistent, unexpected modification of the kernel’s control-flow graph. We performed a thorough analysis of 25 Linux rootkits and found that 24 (96%) employ persistent control-flow modifications; an informal study of Windows rootkits yielded similar results. We have implemented SBCFI enforcement as part of the Xen and VMware virtual machine monitors. Our implementation detected all the control-flow modifying rootkits we could install, while imposing negligible overhead for both a typical web server workload and CPU-intensive workloads when operating at 1 second intervals on a multi-core machine.
236|Guest-Transparent Prevention of Kernel Rootkits with VMM-Based Memory Shadowing |Abstract. Kernel rootkits pose a significant threat to computer systems as they run at the highest privilege level and have unrestricted access to the resources of their victims. Many current efforts in kernel rootkit defense focus on the detection of kernel rootkits – after a rootkit attack has taken place, while the smaller number of efforts in kernel rootkit prevention exhibit limitations in their capability or deployability. In this paper we present a kernel rootkit prevention system called NICKLE which addresses a common, fundamental characteristic of most kernel rootkits: the need for executing their own kernel code. NICKLE is a lightweight, virtual machine monitor (VMM) based system that transparently prevents unauthorized kernel code execution for unmodified commodity (guest) OSes. NICKLE is based on a new scheme called memory shadowing, wherein the trusted VMM maintains a shadow physical memory for a running VM and performs real-time kernel code authentication so that only authenticated kernel code will be stored in the shadow memory. Further, NICKLE transparently routes guest kernel instruction fetches to the shadow memory at runtime. By doing so, NICKLE guarantees that only the authenticated kernel code will be executed, foiling the kernel rootkit’s attempt to strike in the first place. We have implemented NICKLE in three VMM platforms: QEMU+KQEMU, VirtualBox, and VMware Workstation. Our experiments with 23 real-world kernel rootkits targeting the Linux or Windows OSes demonstrate NICKLE’s effectiveness. Furthermore, our performance evaluation shows that NICKLE introduces small overhead to the VMM platform. 1
237|Hypervisor Support for Identifying Covertly Executing Binaries|Hypervisors have been proposed as a security tool to defend against malware that subverts the OS kernel. However, hypervisors must deal with the semantic gap between the low-level information available to them and the high-level OS abstractions they need for analysis. To bridge this gap, systems have proposed making assumptions derived from the kernel source code or symbol information. Unfortunately, this information is nonbinding – rootkits are not bound to uphold these assumptions and can escape detection by breaking them. In this paper, we introduce Patagonix, a hypervisorbased system that detects and identifies covertly executing binaries without making assumptions about the OS kernel. Instead, Patagonix depends only on the processor hardware to detect code execution and on the binary format specifications of executables to identify code and verify code modifications. With this, Patagonix can provide trustworthy information about the binaries running on a system, as well as detect when a rootkit is hiding or tampering with executing code. We have implemented a Patagonix prototype on the Xen 3.0.3 hypervisor. Because Patagonix makes no assumptions about the OS kernel, it can identify code from application and kernel binaries on both Linux and Windows XP. Patagonix introduces less than 3 % overhead on most applications. 1
238|An architecture for specification-based detection of semantic integrity violations in kernel dynamic data|The ability of intruders to hide their presence in compromised systems has surpassed the ability of the current generation of integrity monitors to detect them. Once in control of a system, intruders modify the state of constantly-changing dynamic kernel data structures to hide their processes and elevate their privileges. Current monitoring tools are limited to detecting changes in nominally static kernel data and text and cannot distinguish a valid state change from tampering in these dynamic data structures. We introduce a novel general architecture for defining and monitoring semantic integrity constraints using a specification language-based approach. This approach will enable a new generation of integrity monitors to distinguish valid states from tampering.
239|The Ant and the Grasshopper: Fast and Accurate Pointer Analysis for Millions of Lines of Code|Pointer information is a prerequisite for most program analyses, and the quality of this information can greatly affect their precision and performance. Inclusion-based (i.e. Andersen-style) pointer analysis is an important point in the space of pointer analyses, offering a potential sweet-spot in the trade-off between precision and performance. However, current techniques for inclusion-based pointer analysis can have difficulties delivering on this potential. We introduce and evaluate two novel techniques for inclusionbased pointer analysis—one lazy, one eager 1 —that significantly improve upon the current state-of-the-art without impacting precision. These techniques focus on the problem of online cycle detection, a critical optimization for scaling such analyses. Using a suite of six open-source C programs, which range in size from 169K to 2.17M LOC, we compare our techniques against the best three current inclusion-based analyses—described by Heintze and Tardieu [11], by Pearce et al. [22], and by Berndl et al. [4]. The combination of our two techniques results in an algorithm which is on average 3.2 × faster than Heintze and Tardieu’s algorithm, 6.4 × faster than Pearce et al.’s algorithm, and 20.6 × faster than Berndl et al.’s algorithm. We also investigate the use of different data structures to represent points-to sets, examining the impact on both performance and memory consumption. We compare a sparse-bitmap implementation used in the GCC compiler with a BDD-based implementation, and we find that the BDD implementation is on average 2 × slower than using sparse bitmaps but uses 5.5 × less memory.  
240|Automatic Inference and Enforcement of Kernel Data Structure Invariants |Kernel-level rootkits affect system security by modifying key kernel data structures to achieve a variety of malicious goals. While early rootkits modified control data structures, such as the system call table and values of function pointers, recent work has demonstrated rootkits that maliciously modify non-control data. Prior techniques for rootkit detection fail to identify such rootkits either because they focus solely on detecting control data modifications or because they require elaborate, manually-supplied specifications to detect modifications of non-control data. This paper presents a novel rootkit detection technique that automatically detects rootkits that modify both control and non-control data. The key idea is to externally observe the execution of the kernel during a training period and hypothesize invariants on kernel data structures. These invariants are used as specifications of data structure integrity during an enforcement phase; violation of these invariants indicates the presence of a rootkit. We present the design and implementation of Gibraltar, a tool that uses the above approach to infer and enforce invariants. In our experiments, we found that Gibraltar can detect rootkits that modify both control and non-control data structures, and that its false positive rate and monitoring overheads are negligible. 1.
241|Robust signatures for kernel data structures|Kernel-mode rootkits hide objects such as processes and threads using a technique known as Direct Kernel Object Manipulation (DKOM). Many forensic analysis tools attempt to detect these hidden objects by scanning kernel memory with handmade signatures; however, such signatures are brittle and rely on non-essential features of these data structures, making them easy to evade. In this paper, we present an automated mechanism for generating signatures for kernel data structures and show that these signatures are robust: attempts to evade the signature by modifying the structure contents will cause the OS to consider the object invalid. Using dynamic analysis, we profile the target data structure to determine commonly used fields, and we then fuzz those fields to determine which are essential to the correct operation of the OS. These fields form the basis of a signature for the data structure. In our experiments, our new signature matched the accuracy of existing scanners for traditional malware and found processes hidden with our prototype rootkit that all current signatures missed. Our techniques significantly increase the difficulty of hiding objects from signature scanning.
242|Digging for data structures|Because writing computer programs is hard, computer programmers are taught to use encapsulation and mod-ularity to hide complexity and reduce the potential for errors. Their programs will have a high-level, hierar-chical structure that reflects their choice of internal ab-stractions. We designed and forged a system, Laika, that detects this structure in memory using Bayesian unsu-pervised learning. Because almost all programs use data structures, their memory images consist of many copies of a relatively small number of templates. Given a mem-ory image, Laika can find both the data structures and their instantiations. We then used Laika to detect three common polymor-phic botnets by comparing their data structures. Because it avoids their code polymorphism entirely, Laika is ex-tremely accurate. Finally, we argue that writing a data structure polymorphic virus is likely to be considerably harder than writing a code polymorphic virus. 1
243|Efficient field-sensitive pointer analysis for C |The subject of this paper is flow- and context-insensitive pointer analysis. We present a novel approach for precisely modelling struct variables and indirect function calls. Our method emphasises efficiency and simplicity and is based on a simple language of set constraints. We obtain an O(v 4) bound on the time needed to solve a set of constraints from this language, where v is the number of constraint variables. This gives, for the first time, some insight into the hardness of performing field-sensitive pointer analysis of C. Furthermore, we experimentally evaluate the time versus precision trade-off for our method by comparing against the field-insensitive equivalent. Our benchmark suite consists of 11 common C programs ranging in size from 15,000 to 200,000 lines of code. Our results indicate the field-sensitive analysis is more expensive to compute, but yields significantly better precision. In addition, our technique has been integrated into the latest release (version 4.1) of the GNU Compiler GCC. Finally, we identify several previously unknown issues with an alternative and less precise approach to modelling struct variables, known as
244|A Comparative Analysis of Methodologies for Database Schema Integration| One of the fundamental principles of the database approach is that a database allows a nonredundant, unified representation of all data managed in an organization. This is achieved only when methodologies are available to support integration across organizational and application boundaries. 

Methodologies for database design usually perform the design activity by separately producing several schemas, representing parts of the application, which are subsequently merged. Database schema integration is the activity of integrating the schemas of existing or proposed databases into a global, unified schema. The aim of the paper is to provide first a unifying framework for the problem of schema integration, then a comparative review of the work done thus far in this area. Such a framework, with the associated analysis of the existing approaches, provides a basis for identifying strengths and weaknesses of individual methodologies, as well as general guidelines for future improvements and extensions.
245|The TSIMMIS Project: Integration of Heterogeneous Information Sources |The goal of the Tsimmis Project is to develop tools that facilitate the rapid integration of heterogeneous information sources that may include both structured and unstructured data. This paper gives an overview of the project, describing components that extract properties from unstructured objects, that translate information into a common object model, that combine information from several sources, that allow browsing of information, and that manage constraints across heterogeneous sites. Tsimmis is a joint project between Stanford and the IBM Almaden Research Center.
246|Mediators in the architecture of future information systems|The installation of high-speed networks using optical fiber and high bandwidth messsage forwarding gateways is changing the physical capabilities of information systems. These capabilities must be complemented with corresponding software systems advances to obtain a real benefit. Without smart software we will gain access to more data, but not improve access to the type and quality of information needed for decision making. To develop the concepts needed for future information systems we model information processing as an interaction of data and knowledge. This model provides criteria for a high-level functional partitioning. These partitions are mapped into information process-ing modules. The modules are assigned to nodes of the distributed information systems. A central role is assigned to modules that mediate between the users &#039; workstations and data re-sources. Mediators contain the administrative and technical knowledge to create information needed for decision-making. Software which mediates is common today, but the structure, the interfaces, and implementations vary greatly, so that automation of integration is awkward. By formalizing and implementing mediation we establish a partitioned information sys-
247|Object exchange across heterogeneous information sources|We address the problem of providing integrated access to diverse and dynamic information sources. We explain how this problem differs from the traditional database integration problem and we focus on one aspect of the information integration problem, namely information exchange. We define an object-based information exchange model and a corresponding query language that we believe are well suited for integration of diverse information sources. We describe how, the model and language have been used to integrate heterogeneous bibliographic information sources. We also describe two general-purpose libraries we have implemented for object exchange between clients and servers.
248|Querying object-oriented databases|We present a novel language for querying object-oriented databases. The language is built around the idea of extended path expressions that substantially generalize [ZAN83], and on an adaptation of the first-order formalization of object-oriented languages from [KW89, KLW90, KW92]. The language incorporates features not found in earlier proposals; it is easier to use and has greater expressive power. Some of the salient features of our language are: ffl Precise model-theoretic semantics. ffl A very expressive form of path expressions that not only can do joins, selections and unnesting, but can also be used to explore the database schema. ffl Views can be defined and manipulated in a much more uniform way than in other proposals. ffl Database schema can be explored in the very same language that is used to retrieve data. Unlike in relational languages, the user needs not know anything about the system tables that store schema information. ffl The notions of a type and type-correctness have precise meaning. It accommodates a wide variety of queries that might be deemed well- or ill-typed under different circumstances. In particular, we show that there is more than one way of settling the issue of type correctness. For expository purposes and due to space limitation, we chose to make a number of simplifying assumptions and left some features out. A more complete account can be found in [KSK92].
249|Temporal databases|A temporal database (see Temporal Database) contains time-varying data. Time is an important aspect of all real-world phenomena. Events occur at specific points in time; objects and the relationships among objects exist over time. The ability to model this temporal dimension of the real world is essential to many computer applications, such as accounting, banking, econometrics, geographical information systems, inventory control, law, medical records, multi-media, process control, reservation systems, and scientific data analysis. Conventional databases represent the state of an enterprise at a single moment of time. Although the contents of the database continue to change as new information is added, these changes are viewed as modifications to the state, with the old, out-of-date data being deleted from the database. The current contents of the database may be viewed as a snapshot of the enterprise. When a conventional database is used, the attributes involving time are manipulated solely by the application programs, with little help
250|Managing semantic heterogeneity with production rules and persistent queues|Abstract. We show that production rules and persis-tent queues together provide a convenient mechanism for maintaining consistency in semantically heterogeneous multi-database environments. We describe a specification language and methods for automatically deriving production rules that maintain (1) existence dependencies, in which the presence of data in one database implies the presence of related data in another, and (2) value dependencies, in which the value of data in one database is baaed on the value of related data in another. The production rules derived from dependency specifications use persistent queues to monitor and maintain the dependencies automatically, asynchronously, incremen-tally, and correctly. 1
251|Combining Theory and Practice in Integrity Control: A Declarative Approach to the Specification of a Transaction Modification Subsystem|Integrity control is generally considered an important topic in the field of database system research. In the database literature, many proposals for integrity control mechanisms can be found. A large group of proposals has a formal character, and does not cover complete algorithms that can be used in a real-world database system with multi-update transactions. Another group of proposals is system-oriented and often lacks a complete formal background on transactions and integrity control; algorithms are usually described in system terms. This paper combines the essentials of both groups: it presents a declarative specification of a transaction-based integrity control technique that has a solid formal basis and can easily be applied in real-world database systems. The technique, called transaction modification, features simple semantics, full transaction support, and extensibility to parallel data processing. These claims are supported by a prototype implementation of a transaction modi...
252|Constraint Management in Loosely Coupled Distributed Databases|We provide a framework for managing integrity constraints that span multiple databases in loosely coupled, heterogeneous environments. Our framework enables the formal description of (1) interfaces provided by a database for the data items involved in multi-database constraints; (2) strategies for monitoring and maintaining multi-database constraints; (3) guarantees regarding the consistency of multi-database constraints. With our approach one can define &#034;relaxed&#034; constraints that only hold at certain times or under certain conditions. Such constraints appear often in practice and cannot be handled effectively by conventional, transaction-based approaches. We also describe a toolkit, based on our framework, for enforcing constraints over heterogeneous systems. The toolkit includes a general-purpose, distributed constraint manager that can be easily configured to a given environment and constraints. A first version of the toolkit has been implemented and is under evaluation. 1 Introduct...
253|SIMPLIcity: Semantics-Sensitive Integrated Matching for Picture LIbraries|The need for efficient content-based image retrieval has increased tremendously in many application areas such as biomedicine, military, commerce, education, and Web image classification and searching. We present here SIMPLIcity (Semanticssensitive Integrated Matching for Picture LIbraries), an image retrieval system, which uses semantics classification methods, a wavelet-based approach for feature extraction, and integrated region matching based upon image segmentation. As in other regionbased retrieval systems, an image is represented by a set of regions, roughly corresponding to objects, which are characterized by color, texture, shape, and location. The system classifies images into semantic categories, such as textured-nontextured, graphphotograph. Potentially, the categorization enhances retrieval by permitting semantically-adaptive searching methods and narrowing down the searching range in a database. A measure for the overall similarity between images is developed using a region-matching scheme that integrates properties of all the regions in the images. Compared with retrieval based on individual regions, the overall similarity approach 1) reduces the adverse effect of inaccurate segmentation, 2) helps to clarify the semantics of a particular region, and 3) enables a simple querying interface for region-based image retrieval systems. The application of SIMPLIcity to several databases, including a database of about 200,000 general-purpose images, has demonstrated that our system performs significantly better and faster than existing ones. The system is fairly robust to image alterations.
254|Photobook: Content-Based Manipulation of Image Databases|We describe the Photobook system, which is a set of interactive tools for browsing and searching images and image sequences. These query tools differ from those used in standard image databases in that they make direct use of the image content rather than relying on text annotations. Direct search on image content is made possible by use of semantics-preserving image compression, which reduces images to a small set of perceptually-significant coefficients. We describe three types of Photobook descriptions in detail: one that allows search based on appearance, one that uses 2-D shape, and a third that allows search based on textural properties. These image content descriptions can be combined with each other and with textbased descriptions to provide a sophisticated browsing and search capability. In this paper we demonstrate Photobook on databases containing images of people, video keyframes, hand tools, fish, texture swatches, and 3-D medical data.  
255|NeTra: A toolbox for navigating large image databases|. We present here an implementation of NeTra, a prototype image retrieval system that uses color, texture, shape and spatial location information in segmented image regions to search and retrieve similar regions from the database. A distinguishing aspect of this system is its incorporation of a robust automated image segmentation algorithm that allows object- or region-based search. Image segmentation significantly improves the quality of image retrieval when images contain multiple complex objects. Images are segmented into homogeneous regions at the time of ingest into the database, and image attributes that represent each of these regions are computed. In addition to image segmentation, other important components of the system include an efficient color representation, and indexing of color, texture, and shape features for fast search and retrieval. This representation allows the user to compose interesting queries such as &#034;retrieve all images that contain regions that have the colo...
256|Blobworld: A System for Region-Based Image Indexing and Retrieval|. Blobworld is a system for image retrieval based on finding coherent image regions which roughly correspond to objects. Each image is automatically segmented into regions (&#034;blobs&#034;) with associated color and texture descriptors. Querying is based on the attributes of one or two regions of interest, rather than a description of the entire image. In order to make large-scale retrieval feasible, we index the blob descriptions using a tree. Because indexing in the high-dimensional feature space is computationally prohibitive, we use a lower-rank approximation to the high-dimensional distance. Experiments show encouraging results for both querying and indexing. 1 Introduction  From a user&#039;s point of view, the performance of an information retrieval system can be measured by the quality and speed with which it answers the user&#039;s information need. Several factors contribute to overall performance:  -- the time required to run each individual query,  -- the quality (precision/recall) of each i...
257|Visual Information Retrieval|ND BUSINESSMAN CALVIN MOORES COINED the term information retrieval [10] to describe the process through which a prospective user of information can convert a request for information into a useful collection of references. &#034;Information retrieval,&#034; he wrote, &#034;embraces the intellectual aspects of the description of information and its specification for search, and also whatever systems, techniques, or machines that are employed Amarnath Gupta and Ramesh Jain 72 May 1997/Vol. 40, No. 5 COMMUNICATIONS OF THE ACM lar expressions to describe a clip. There is also a deeper reason: The information sought is inherently in the form of imagery that a textual language, however powerful, is unable to express adequately, making query processing inefficient. HE ROLE OF THE EMERGING FIELD OF visual information retrieval (VIR) systems is to go beyond text-based descri
258|A probabilistic approach to object recognition using local photometry and global geometry|Abstract. Many object classes, including human faces, can be modeled as a set of characteristic parts arranged in a variable spatial con guration. We introduce a simpli ed model of a deformable object class and derive the optimal detector for this model. However, the optimal detector is not realizable except under special circumstances (independent part positions). A cousin of the optimal detector is developed which uses \soft &#034; part detectors with a probabilistic description of the spatial arrangement of the parts. Spatial arrangements are modeled probabilistically using shape statistics to achieve invariance to translation, rotation, and scaling. Improved recognition performance over methods based on \hard &#034; part detectors is demonstrated for the problem of face detection in cluttered scenes. 1
259|WALRUS: A Similarity Retrieval Algorithm for Image Databases|Traditional approaches for content-based image querying typically compute a single signature for each image based on color histograms, texture, wavelet transforms etc., and return as the query result, images whose signatures are closest to the signature of the query image. Therefore, most traditional methods break down when images contain similar objects that are scaled differently or at different locations, or only certain regions of the image match.  In this paper, we propose WALRUS (WAveLet-based Retrieval of User-specified Scenes), a novel similarity retrieval algorithm that is robust to scaling and translation of objects within an image. WALRUS employs a novel similarity model in which each image is first decomposed into its regions, and the similarity measure between a pair of images is then defined to be the fraction of the area of the two images covered by matching regions from the images. In order to extract regions for an image, WALRUS considers sliding windows of varying siz...
260|Content-based image indexing and searching using Daubechies&#039; wavelets|This paper describes WBIIS (Wavelet-Based Image Indexing and Searching), a new image indexing and retrieval algorithm with partial sketch image searching capability for large image databases. The algorithm characterizes the color variations over the spatial extent of the image in a manner that provides semantically meaningful image comparisons. The indexing algorithm applies a Daubechies&#039; wavelet transform for each of the three opponent color components. The wavelet coefficients in the lowest few frequency bands, and their variances, are stored as feature vectors. To speed up retrieval, a two-step procedure is used that first does a crude selection based on the variances, and then renes the search by performing a feature vector match between the selected images and the query. For better accuracy in searching, two-level multiresolution matching may also be used. Masks are used for partial-sketch queries. This technique performs much better in capturing coherence of image, object granular...
261|The earth mover’s distance, multi-dimensional scaling, and color-based image retrieval|In this paper we present a novel approach tothe problem of navigating through a database of color images. We consider the images as points in a metric space in which we wish to move around so as to locate image neighborhoods of interest, based on color information. The data base images are mapped to distributions in color space, these distributions are appropriately compressed, and then the distances between all pairs I;J of images are computed based on the work needed to rearrange the mass in the compressed distribution representing I to that of J. We also propose the use of multi-dimensional scaling (MDS) techniques to embed a group of images as points in a two- or three-dimensional Euclidean space so that their distances are preserved as much as possible. Such geometric embeddings allow the user to perceive the dominant axes of variation in the displayed image group. In particular, displays of 2-d MDS embeddings can be used to organize and re ne the results of a nearest-neighbor query in a perceptually intuitive way. By iterating this process, the user is able to quickly navigate to the portion of the image space of interest. 1
262|Image Classification and Querying using Composite Region Templates|The tremendous growth in digital imagery is driving the need for more sophisticated  methods for automatic image analysis, cataloging, and searching.  We present a method for classifying and querying images based on the spatial  orderings of regions or objects using composite region templates (CRTs). The  CRTs capture the spatial information statistically and provide a robust way to  measure similarity in the presence of region insertions, deletions, substitutions, replications and relocations. The CRTs can be used for classifying and annotating  images by assigning symbols to the regions or objects and by extracting  symbol strings from spatial scans of the images. The symbol strings can be  decoded using a library of annotated CRTs to automatically label and classify  the images. The CRTs can also be used for searching bysketch or example by  measuring image similarity based on relative counts of the CRTs.  
263|Finding Similar Patterns in Large Image Databases|We address a new and rapidly growing application, automated searching through large sets of images to find a pattern &#034;similar to this one.&#034; Classical matched filtering fails at this problem since patterns, particularly textures, can differ in every pixel and still be perceptually similar. Most potential recognition methods have not been tested on large sets of imagery. This paper evaluates a key recognition method on a library of almost 1000 images, based on the entire Brodatz texture album. The features used for searching rely on a significant improvement to the traditional Karhunen-Lo&#039;eve (KL) transform which makes it shift-invariant. Results are shown for a variety of false alarm rates and for different subsets of KL features.  1 Introduction  As vastly increasing amounts of image and video are stored in computers it becomes harder for humans to locate a particular scene or video clip. It is currently impossible, in the general case, to semantically describe an image to the computer...
264|Unsupervised Multiresolution Segmentation for Images with Low Depth of Field|This paper describes a novel multiresolution image  segmentation algorithm for low DOF images. The algorithm is designed to  separate a sharply focused object-of-interest from other foreground or background objects. The algorithm is fully automatic in that all parameters are image  independent. A multiscale approach based on high frequency wavelet coefficients and their statistics is used to perform context-dependent classification of individual blocks of the image. Unlike other edge-based approaches, our algorithm does not rely on the process of connecting object boundaries. The algorithm has achieved high accuracy when tested on more than 100 low DOF images, many with  inhomogeneous foreground or background distractions. Compared with the state of the art algorithms, this new algorithm provides better accuracy at higher speed. Index TermsContent-based image retrieval, image region segmentation, low  depth-of-field, wavelet, multiresolution image analysis
265|Semantic Clustering and Querying on Heterogeneous Features for Visual data|  The effectiveness of the content-based image retrieval can be enhanced using the heterogeneous features embedded in the images. However, since the features in texture, color, and shape are generated using different computation methods and thus may require different similarity measurements, the integration of the...
266|System for Screening Objectionable Images|As computers and Internet become more and more available to families, access of objectionable graphics by children is increasingly a problem that many parents are concerned about. This paper describes WIPE TM (Wavelet Image Pornography Elimination), a system capable of classifying an image as objectionable or benign. The algorithm uses a combination of an icon filter, a graph-photo detector, a color histogram filter, a texture filter, and a wavelet-based shape matching algorithm to provide robust screening of on-line objectionable images. Semantically-meaningful feature vector matching is carried out so that comparisons between a given on-line image and images in a pre-marked training data set can be performed efficiently and effectively. The system is practical for real-world applications, processing queries at the speed of less than 2 seconds each, including the time to compute the feature vector for the query, on a Pentium Pro PC. Besides its exceptional speed, it has demonstrated 9...
267|Visual Similarity, Judgmental Certainty and Stereo Correspondence|Normal human vision is nearly infallible in modeling the visually sensed physical environment in which it evolved. In contrast, most currently available computer vision systems fall far short of human performance in this task, and further, they are generally not capable of being able to assert the correctness of their judgments. In computerized stereo matching systems, correctness of the similarity/identity-matching is almost never guaranteed. In this paper, we explore the question of the extent to which judgments of similarity/identity can be made essentially error-free in support of obtaining a relatively dense depth model of a natural outdoor scene. We argue for the necessity of simultaneously producing a crude scene-specific semantic &#034;overlay&#034;. For our experiments, we designed awavelet-based stereo matching algorithm and use &#034;classification-trees&#034; to create a primitive semantic overlay of the scene. A series of mutually independent filters has been designed and implemented based on the study of different error sources. Photometric appearance, camera imaging geometry and scene constraints are utilized in these filters. When tested on different sets of stereo images, our system has demonstrated above 97% correctness on asserted matches. Finally,we provide a principled basis for relatively dense depth recovery.
268|Three Types of Redundancy in Integrity Checking; an optimal solution|Known methods for checking integrity constraints in deductive  databases do not eliminate all aspects of redundancy in integrity  checking. By making the redundancy aspects of integrity constraint  checking explicit, independently from any chosen method, it is possible  to develop a new method that is optimal with respect to the classified  redundancy aspects. We distinguish three types of redundancy and  propose an integrity checking method based on revised inconsistency  rules.
269|Supporting Real-Time Applications in an Integrated Services Packet Network: Architecture and Mechanism|This paper considers the support of real-time applications in an
270|Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment|The problem of multiprogram scheduling on a single processor is studied from the viewpoint...
271|A generalized processor sharing approach to flow control in integrated services networks: The single-node case|Abstruet-The problem of allocating network resources to the users of an integrated services network is investigated in the context of rate-based flow control. The network is assumed to be a virtual circuiq comection-based packet network. We show that the use of Generalized processor Sharing (GPS), when combined with Leaky Bucket admission control, allows the network to make a wide range of worst-case performance guarantees on throughput and delay. The scheme is flexible in that d~erent users may be given widely different performance guarantees, and is efilcient in that each of the servers is work conserving. We present a practicat packet-by-packet service discipline, PGPS (first proposed by Deme5 Shenker, and Keshav [7] under the name of Weighted Fair Queueing), that closely approximates GPS. This altows us to relate ressdta for GPS to the packet-bypacket scheme in a precise manner. In this paper, the performance of a single-server GPS system is analyzed exactty from the standpoint of worst-case packet delay and burstiness when the sources are constrained by leaky buckets. The worst-case sewdon backlogs are also determined. In the sequel to this paper, these results are extended to arbitrary topology networks with multiple nodes. I.
272|A Scheme for Real-Time Channel Establishment in Wide-Area Networks|Multimedia communication involving digital audio and/or digital video has rather strict delay requirements. A real-time channel is defined in this paper as a simplex connection between a source and a destination characterized by parameters representing the performance requirements of the client. A real-time service is capable of creating realtime channels on demand and guaranteeing their performance. These guarantees often take the form of lower bounds on the bandwidth allocated to a channel and upper bounds on the delays to be experienced by a packet on the channel. In this paper
273|Rate Controlled Servers for Very High-Speed Networks|Future high-speed networks are expected to carry traffic with a wide range of performance requirements. We describe two queue service disciplines, rate-based scheduling and hierarchical round robin scheduling, that allow some connections to receive guaranteed rate and jitter performance, while others receive best effort service. Rate-based scheduling is designed for fast packet networks, while hierarchical round robin is an extension of round robin scheduling suitable for use in networks based on the Asynchronous Transfer Mode (ATM) being defined in CCITT. Both schemes are feasible at rates of one Gigabit/sec. The schemes allow strict bounds on the buffer space required for rate controlled connections and can provide efficient utilization of transmission bandwidth.  Introduction  Future high-speed networks are expected to carry traffic with a wide range of performance requirements. A classic tradeoff in network design is between providing quality of service guarantees on one hand, and ...
274|Distributed Scheduling Based On Due Dates And Buffer Priorities|We are motivated by the problem of scheduling a large semiconductor manufacturing  facility, where jobs of wafers, each with a desired due date, follow essentially  the same route through the manufacturing system, returning several times to many  of the service centers for the processing of successive layers. Neglecting the randomness  introduced by yield, such a system can be modeled as a non-acyclic ow line.
275|Real-Time Scheduling with Quality of Service Constraints|Can the introduction of traffic classes improve upon the performance of ATM networks? We investigate this issue within the framework provided by a class of networks that guarantees quality of service. To provide a meaningful comparison we define the concept of schedulable region, a region in the space of loads for which the quality of service is guaranteed. We show the dependence of the schedulable region on the scheduling algorithm employed, the quality of service parameters and the traffic statistics. An efficient real-time scheduling algorithm is introduced that substantially increases the schedulable region without incurring prohibitive complexity costs. The schedulable region associated with this algorithm is compared with the ones generated by the static priority scheduling algorithm and a variant of the minimum laxity threshold algorithm. The size and shape of the schedulable region is explored by means of simulations. 
276|JFlow: Practical Mostly-Static Information Flow Control|A promising technique for protecting privacy and integrity of sensitive data is to statically check information flow within programs that manipulate the data. While previous work has proposed programming language extensions to allow this static checking, the resulting languages are too restrictive for practical use and have not been implemented. In this paper, we describe the new language JFlow, an extension to the Java language that adds statically-checked information flow annotations. JFlow provides several new features that make information flow checking more flexible and convenient than in previous models: a decentralized label model, label polymorphism, run-time label checking, and automatic label inference. JFlow also supports many language features that have never been integrated successfully with static information flow control, including objects, subclassing, dynamic type tests, access control, and exceptions. This paper defines the JFlow language and presents formal rules tha...
277|A Lattice Model of Secure Information Flow|This paper investigates mechanisms that guarantee secure information flow in a computer system. These mechanisms are examined within a mathematical framework suitable for formulating the requirements of secure information flow among security classes. The central component of the model is a lattice structure derived from the security classes and justified by the semantics of information flow. The lattice properties permit concise formulations of the security requirements of different existing systems and facilitate the construction of mechanisms that enforce security. The model provides a unifying view of all systems that restrict information flow, enables a classification of them according to security objectives, and suggests some new approaches. It also leads to the construction of automatic program certification mechanisms for verifying the secure flow of information through a program.
278|Data Security|The rising abuse of computers and increasing threat to personal privacy through data banks have stimulated much interest m the techmcal safeguards for data. There are four kinds of safeguards, each related to but distract from the others. Access controls regulate which users may enter the system and subsequently whmh data sets an active user may read or wrote. Flow controls regulate the dissemination of values among the data sets accessible to a user. Inference controls protect statistical databases by preventing questioners from deducing confidential information by posing carefully designed sequences of statistical queries and correlating the responses. Statlstmal data banks are much less secure than most people beheve. Data encryption attempts to prevent unauthorized disclosure of confidential information in transit or m storage. This paper describes the general nature of controls of each type, the kinds of problems they can and cannot solve, and their inherent limitations and weaknesses. The paper is intended for a general audience with little background in the area.
279|A SOUND TYPE SYSTEM FOR SECURE FLOW ANALYSIS|Ensuring secure information ow within programs in the context of multiple sensitivity levels has been widely studied. Especially noteworthy is Denning&#039;s work in secure ow analysis and the lattice model [6][7]. Until now, however, the soundness of Denning&#039;s analysis has not been established satisfactorily. Weformulate Denning&#039;s approach as a type system and present a notion of soundness for the system that can be viewed as a form of noninterference. Soundness is established by proving, with respect to a standard programming language semantics, that all well-typed programs have this noninterference property.
280|A Note on the Confinement Problem|This not explores the problem of confining a program during its execution so that it cannot transmit information to any other program except its caller. A set of examples attempts to stake out the boundaries of the problem. Necessary conditions for a solution are stated and informally justified.
281|Certification of Programs for Secure Information Flow|This paper presents a certification mechanism for verifying the secure flow of information through a program. Because it exploits the properties of a lattice structure among security classes, the procedure is sufficiently simple that it can easily be included in the analysis phase of most existing compilers. Appropriate semantics are presented and proved correct. An important application is the confinement problem: The mechanism can prove that a program cannot cause supposedly nonconfidential results to depend on confidential input data.
282|Secrecy by Typing in Security Protocols|We develop principles and rules for achieving secrecy properties in security protocols. Our approach is based on traditional classification techniques, and extends those techniques to handle concurrent processes that use shared-key cryptography. The rules have the form of typing rules for a basic concurrent language with cryptographic primitives, the spi calculus. They guarantee that, if a protocol typechecks, then it does not leak its secret inputs.
283|The slam calculus: programming with secrecy and integrity|The SLam calculus is a typed ?-calculus that maintains security information as well as type information. The type system propagates security information for each object in four forms: the object’s creators and readers, and the object’s indirect creators and readers (i.e., those agents who, through flow-of-control or the actions of other agents, can influence or be influenced by the content of the object). We prove that the type system prevents security violations and give some examples of its power. 1
284|Secure information flow in a multi-threaded imperative language|Previously, we developed a type system to ensure secure information flow in a sequential, imperative programming language [VSI96]. Program variables are classified as either high or low security; intuitively, we wish to prevent information from flowing from high variables to low variables. Here, we extend the analysis to deal with a multithreaded language. We show that the previous type system is insufficient to ensure a desirable security property called noninterference. Noninterference basically means that the final values of low variables are independent of the initial values of high variables. By modifying the sequential type system, we are able to guarantee noninterference for concurrent programs. Crucial to this result, however, is the use of purely nondeterministic thread scheduling. Since implementing such scheduling is problematic, we also show how a more restrictive type system can guarantee noninterference, given a more deterministic (and easily implementable) scheduling policy, such as round-robin time slicing. Finally, we consider the consequences of adding a clock to the language.  
286|Dynamic Typing in a Statically Typed Language |Statically typed programming languages allow earlier error checking, better enforcement of disciplined programming styles, and generation of more e cient object code than languages where all type consistency checks are performed at run time. However, even in statically typed languages, there is often the need to deal with data whose type cannot be determined at compile time. To handle such situations safely, we propose to add a type Dynamic whose values are pairs of a value v andatype tag T where v has the type denoted by T. Instances of Dynamic are built with an explicit tagging construct and inspected with a type safe typecase construct. This paper explores the syntax, operational semantics, and denotational semantics of a simple language including the type Dynamic. Wegive examples of how dynamically typed values can be used in programming. Then we discuss an operational semantics for our language and obtain a soundness theorem. We present two formulations of the denotational semantics of this language and relate them to the operational semantics. Finally,we consider the implications of polymorphism and some implementation issues.
287|Typeful programming|There exists an identifiable programming style based on the widespread use of type information handled through mechanical typechecking techniques. This typeful programming style is in a sense independent of the language it is embedded in; it adapts equally well to functional, imperative, object-oriented, and algebraic programming, and it is not incompatible with relational and concurrent programming. The main purpose of this paper is to show how typeful programming is best supported by sophisticated type systems, and how these systems can help in clarifying programming issues and in adding power and regularity to languages. We start with an introduction to the notions of types, subtypes and polymorphism. Then we introduce a general framework, derived in part from constructive logic, into which most of the known type systems can be accommodated and extended. The main part of the paper shows how this framework can be adapted systematically to cope with actual programming constructs. For concreteness we describe a particular programming language with advanced features; the emphasis here is on the combination of subtyping and polymorphism. We then discuss how typing concepts apply to large programs, made of collections of modules, and very large programs, made of collections of large programs. We also sketch how typing applies to system programming; an area which by nature escapes rigid typing. In summary, we compare the most common programming styles, suggesting that many of them are compatible with, and benefit from, a typeful discipline.
288|Complete, Safe Information Flow with Decentralized Labels|The growing use of mobile code in downloaded applications and servlets has increased interest in robust mechanisms for ensuring privacy and secrecy. Information flow control is intended to directly address privacy and secrecy concerns, but most information flow models are too restrictive to be widely used. The decentralized label model is a new information flow model that extends traditional models with per-principal information flow policies and also permits a safe form of declassification. This paper extends this new model further, making it more flexible and expressive. We define a new formal semantics for decentralized labels and a corresponding new rule for relabeling data that is both sound and complete. We also show that these extensions preserve the ability to statically check information flow.  1 Introduction  The growing use of mobile code in downloaded applications and servlets has increased interest in robust mechanisms for ensuring privacy and secrecy. A key problem is tha...
289|Mostly-Static Decentralized  Information Flow Control|The growing use of mobile code in downloaded programs such as applets and servlets has increased interest in robust mechanisms for ensuring privacy and secrecy. Common security mechanisms such as sandboxing and access control are either too restrictive or too weak---they prevent applications from sharing data usefully, or allow private information to leak. For example, security mechanisms in Java prevent many useful applications while still permitting Trojan horse applets to leak private information. This thesis describes the decentralized label model, a new model of information flow control that protects private data while allowing applications to share data. Unlike previous approaches to privacy protection based on information flow, this label model is decentralized: it allows cooperative computation by mutually distrusting principals, without mediation by highly trusted agents. Cooperative computation is possible because individual principals can declassify their own data without infringing on other principals&#039; privacy. The decentralized label model permits programs using it to be checked statically, which is important for the precise detection of information leaks. This thesis also
290|Tractable Constraints in Finite Semilattices|. We introduce the notion of definite inequality constraints involving  monotone functions in a finite meet-semilattice, generalizing the  logical notion of Horn-clauses, and we give a linear time algorithm for  deciding satisfiability. We characterize the expressiveness of the framework  of definite constraints and show that the algorithm uniformly solves  exactly the set of all meet-closed relational constraint problems, running  with small linear time constant factors for any fixed problem. We give  an alternative technique which reduces inequalities to satisfiability of  Horn-clauses (hornsat) and study its efficiency. Finally, we show that  the algorithm is complete for a maximal class of tractable constraints,  by proving that any strict extension will lead to NP-hard problems in  any meet-semilattice.  Keywords: Finite semilattices, constraint satisfiability, program analysis, tractability, algorithms.  1 Introduction  Many program analysis problems can be solved by generating a...
291|Remote Integrity Check with Dishonest Storage Server ? |Abstract. We are interested in this problem: a verifier, with a small and reliable storage, wants to periodically check whether a remote server is keeping a large file x. A dishonest server, by adapting the challenges and responses, tries to discard partial information of x and yet evades detection. Besides the security requirements, there are considerations on communication, storage size and computation time. Juels et al. [10] gave a security model for Proof of Retrievability (POR) system. The model imposes a requirement that the original x can be recovered from multiple challenges-responses. Such requirement is not necessary in our problem. Hence, we propose an alternative security model for Remote Integrity Check (RIC). We study a few schemes and analyze their efficiency and security. In particular, we prove the security of a proposed scheme HENC. This scheme can be deployed as a POR system and it also serves as an example of an effective POR system whose “extraction ” is not verifiable. We also propose a combination of the RSA-based scheme by Filho et al. [7] and the ECC-based authenticator by Naor et al. [12], which achieves good asymptotic performance. This scheme is not a POR system and seems to be a secure RIC. In-so-far, all schemes that have been proven secure can also be adopted as POR systems. This brings out the question of whether there are fundamental differences between the two models. To highlight the differences, we introduce a notion, trap-door compression, that captures a property on compressibility. 1
292|Public-key cryptosystems based on composite degree residuosity classes| This paper investigates a novel computational problem, namely the Composite Residuosity Class Problem, and its applications to public-key cryptography. We propose a new trapdoor mechanism and derive from this technique three encryption schemes: a trapdoor permutation and two homomorphic probabilistic encryption schemes computationally comparable to RSA. Our cryptosystems, based on usual modular arithmetics, are provably secure under appropriate assumptions in the standard model.  
293|Provable Data Possession at Untrusted Stores|We introduce a model for provable data possession (PDP) that allows a client that has stored data at an untrusted server to verify that the server possesses the original data without retrieving it. The model generates probabilistic proofs of possession by sampling random sets of blocks from the server, which drastically reduces I/O costs. The client maintains a constant amount of metadata to verify the proof. The challenge/response protocol transmits a small, constant amount of data, which minimizes network communication. Thus, the PDP model for remote data checking supports large data sets in widely-distributed storage systems. We present two provably-secure PDP schemes that are more efficient than previous solutions, even when compared with schemes that achieve weaker guarantees. In particular, the overhead at the server is low (or even constant), as opposed to linear in the size of the data. Experiments using our implementation verify the practicality of PDP and reveal that the performance of PDP is bounded by disk I/O and not by cryptographic computation.
294|Pors: proofs of retrievability for large files|Abstract. In this paper, we define and explore proofs of retrievability (PORs). A POR scheme enables an archive or back-up service (prover) to produce a concise proof that a user (verifier) can retrieve a target file F, that is, that the archive retains and reliably transmits file data sufficient for the user to recover F in its entirety. A POR may be viewed as a kind of cryptographic proof of knowledge (POK), but one specially designed to handle a large file (or bitstring) F. We explore POR protocols here in which the communication costs, number of memory accesses for the prover, and storage requirements of the user (verifier) are small parameters essentially independent of the length of F. In addition to proposing new, practical POR constructions, we explore implementation considerations and optimizations that bear on previously explored, related schemes. In a POR, unlike a POK, neither the prover nor the verifier need actually have knowledge of F. PORs give rise to a new and unusual security definition whose formulation is another contribution of our work. We view PORs as an important tool for semi-trusted online archives. Existing cryptographic techniques help users ensure the privacy and integrity of files they retrieve. It is also natural, however, for users to want to verify that archives do not delete or modify files prior to retrieval. The goal of a POR is to accomplish these checks without users having to download the files themselves. A POR can also provide quality-of-service guarantees, i.e., show that a file is retrievable within a certain time bound. Key words: storage systems, storage security, proofs of retrievability, proofs of knowledge 1
295|Efficient Memory Integrity Verification and Encryption for Secure Processors|Secure processors enable new sets of applications such as commercial grid computing, software copy-protection, and secure mobile agents by providing security from both physical and software attacks. This paper proposes new hardware mechanisms for memory integrity verification and encryption, which are two key primitives required in singlechip secure processors. The integrity verification mechanism offers significant performance advantages over existing ones when the checks are infrequent as in grid computing applications. The encryption mechanism improves the performance in all cases. 1.
296|pStore: A Secure Peer-to-Peer Backup System|In an effort to combine research in peer-to-peer systems with techniques for incremental backup systems, we propose pStore: a secure distributed backup system based on an adaptive peer-to-peer network. pStore exploits unused personal hard drive space attached to the Internet to provide the distributed redundancy needed for reliable and effective data backup. Experiments on a 30 node network show that 95% of the files in a 13 MB dataset can be retrieved even when 7 of the nodes have failed. On top of this reliability, pStore includes support for file encryption, versioning, and secure sharing. Its custom versioning system permits arbitrary version retrieval similar to CVS. pStore provides this functionality at less than 10% of the network bandwidth and requires 85% less storage capacity than simpler local tape backup schemes for a representative workload.
297|The complexity of online memory checking|We consider the problem of storing a large file on a remote and unreliable server. To verify that the file has not been corrupted, a user could store a small private (randomized) “fingerprint” on his own computer. This is the setting for the well-studied authentication problem in cryptography, and the required fingerprint size is well understood. We study the problem of sub-linear authentication: suppose the user would like to encode and store the file in a way that allows him to verify that it has not been corrupted, but without reading the entire file. If the user only wants to read q bits of the file, how large does the size s of the private fingerprint need to be? We define this problem formally, and show a tight lower bound on the relationship between s and q when the adversary is not computationally bounded, namely: s × q = ?(n), where n is the file size. This is an easier case of the online memory checking problem, introduced by Blum et al. in 1991, and hence the same (tight) lower bound applies also to that problem. It was previously shown that when the adversary is computationally bounded, under the assumption that one-way functions exist, it is possible to construct much better online memory checkers. T he same is also true for sub-linear authentication schemes. We show that the existence of one-way functions is also a necessary condition: even slightly breaking the s × q = ?(n) lower bound in a computational setting implies the existence of one-way functions. 1
298|On the compressibility of NP instances and cryptographic applications|We initiate the study of compression that preserves the solution to an instance of a problem rather than preserving the instance itself. Our focus is on the compressibility of NP decision problems. We consider NP problems that have long instances but relatively short witnesses. The question is, can one efficiently compress an instance and store a shorter representation that maintains the information of whether the original input is in the language or not. We want the length of the compressed instance to be polynomial in the length of the witness rather than the length of original input. Such compression enables to succinctly store instances until a future setting will allow solving them, either via a technological or algorithmic breakthrough or simply until enough time has elapsed. We give a new classification of NP with respect to compression. This classification forms a strati-fication of NP that we call the VC hierarchy. The hierarchy is based on a new type of reduction called W-reduction and there are compression-complete problems for each class. Our motivation for studying this issue stems from the vast cryptographic implications compressibility has. For example, we say that SAT is compressible if there exists a polynomial p(·, ·) so that given a formula consisting of m clauses over n variables it is possible to come up with an equivalent (w.r.t satisfiability) formula of size at most p(n, logm). Then given a compression algorithm for SAT we provide a construction of collision resistant hash functions from any one-way function. This task was shown to be impossible via black-box reductions [57], and indeed the construction presented is inherently non-black-box. Another application of SAT compressibility is a cryptanalytic result concerning the limitation of everlasting security in the bounded storage model when mixed with (time) complexity based cryptography. In addition, we study an approach to constructing an Oblivious Transfer Protocol from any one-way function. This approach is based on compression for SAT that also has a property that we call witness retrievability. However, we mange to prove severe limitations on the ability to achieve witness retrievable compression of SAT. 1
299|Flexible Integrity Checking of Hard and Soft Constraints |Hard constraints must always hold. Violations of soft constraints may be tolerable. Inconsistency-tolerant in-tegrity checking serves to flexibly check both hard and soft constraints in a uniform manner. With an extended exam-ple for risk management, we illustrate that inconsistency-tolerant integrity checking methods are more efficient and more reliable for checking hard and soft constraints than traditional approaches. 1
300|Semiring-Based Constraint Satisfaction and Optimization|We introduce a general framework for constraint satisfaction and optimization where classical CSPs, fuzzy CSPs, weighted CSPs, partial constraint satisfaction, and others can be easily cast. The framework is based on a semiring structure, where the set of the semiring specifies the values to be associated with each tuple of values of the variable domain, and the two semiring operations (1 and 3) model constraint projection and combination respectively. Local consistency algorithms, as usually used for classical CSPs, can be exploited in this general framework as well, provided that certain conditions on the semiring operations are satisfied. We then show how this framework can be used to model both old and new constraint solving and optimization schemes, thus allowing one to both formally justify many informally taken choices in existing schemes, and to prove that local consistency techniques can be used also in newly defined schemes.
301|The role of deontic logic in the specification of information systems|In this paper we discuss the role that deontic logic plays in the specification of information systems, either because constraints on the systems directly concern norms or, and even more importantly, system constraints are considered ideal but violable (so-called ‘soft ’ constraints). To overcome the traditional problems with deontic logic (the so-called paradoxes), we first state the importance of distinguishing between ought-to-be and ought-to-do constraints and next focus on the most severe paradox, the so-called Chisholm paradox, involving contrary-to-duty norms. We present a multi-modal extension of standard deontic logic (SDL) to represent the ought-to-be version of the Chisholm set properly. For the ought-to-do variant we employ a reduction to dynamic logic, and show how the Chisholm set can be treated adequately in this setting. Finally we discuss a way of integrating both ought-to-be and ought-to-do reasoning, enabling one to draw conclusions from ought-to-be constraints to ought-to-do ones, and show by an example the use(fulness) of this. 1. Introduction: Soft Constraints
302|Classifying integrity checking methods with regard to inconsistency|fi
303|Computer Support For Protocol-Based Treatment Of Cancer|This paper focuses on the current functionality of OaSiS and discusses the use of safety related knowledge identified from an extensive study of oncology protocols and from discussions with clinicians, pharmacists and medical informaticians. The safety critical nature of the domain imposes requirements on software designers and implementers to ensure that the translation from paper to computerised protocol is completed thoroughly and correctly [9]. OaSiS has been implemented within RED, a project funded by the UK DTI and SERC &#034;Safety Critical Systems&#034; research programme. A major influence on OaSiS is the work at Stanford University on the ONCOCIN [32], EON [26] and OPAL [25] family of computer systems. In the OaSiS prototype, this is particularly evident in the user interface and its combined use of graphical, form-based 1 An earlier version of this paper was presented at the 2nd International Conference on the Practical Application of PROLOG, London, 1994. 2
304|A Relaxed Approach to Integrity and Inconsistency in Databases |Abstract. We demonstrate that many, though not all integrity checking methods are able to tolerate inconsistency, without having been aware of it. We show that it is possible to use them to beneficial effect and without further ado, not only for preserving integrity in consistent databases, but also in databases that violate their constraints. This apparently relaxed attitude toward integrity and inconsistency stands in contrast to approaches that are much more cautious wrt the prevention, identification, removal, repair and tolerance of inconsistent data that violate integrity. We assess several well-known methods in terms of inconsistency tolerance and give examples and counter-examples thereof. 1
305|Interprocedural Slicing Using Dependence Graphs|... This paper concerns the problem of interprocedural slicing---generating a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system dependence  graph, which extends previous dependence representations to incorporate collections of procedures (with procedure calls) rather than just monolithic programs. Our main result is an algorithm for interprocedural slicing that uses the new representation. (It should be noted that our work concerns a somewhat restricted kind of slice: Rather than permitting a program to be sliced with respect to program point p and an arbitrary variable, a slice must be taken with respect to a variable that is defined or used at p.) The chief
306|PVS: A Prototype Verification System|PVS is a prototype system for writing specifications and constructing proofs. Its development has been shaped by our experiences studying or using several other systems and performing a number of rather substantial formal verifications (e.g., [5,6,8]). PVS is fully implemented and freely available. It has been used to construct proofs of nontrivial difficulty with relatively modest amounts of human effort. Here, we describe some of the motivation behind PVS and provide some details of the system. Automated reasoning systems typically fall in one of two classes: those that provide powerful automation for an impoverished logic, and others that feature expressive logics but only limited automation. PVS attempts to tread the middle ground between these two classes by providing mechanical assistance to support clear and abstract specifications, and readable yet sound proofs for difficult theorems. Our goal is to provide mechanically-checked specificati
307|Patterns in Property Specifications for Finite-state Verification|Model checkers and other finite-state verification tools allow developers to detect certain kinds of errors automatically. Nevertheless, the transition of this technology from research to practice has been slow. While there are a number of potential causes for reluctance to adopt such formal methods, we believe that a primary cause is that practitioners are unfamiliar with specification processes, notations, and strategies. In a recent paper, we proposed a pattern-based approach to the presentation, codification and reuse of property specifications for finite-state verification. Since then, we have carried out a survey of available specifications, collecting over 500 examples of property specifications. We found that most are instances of our proposed patterns. Furthermore, we have updated our pattern system to accommodate new patterns and variations of existing patterns encountered in this survey. This paper reports the results of the survey and the current status of our pattern system.
308|Protocol Verification as a Hardware Design Aid|The role of automatic formal protocol verification  in hardware design is considered. Principles are identified that maximize the benefits of protocol verification while minimizing the labor and computation required. A new protocol description language and verifier (both called Mur&#039;) are described, along with experiences in applying them to two industrial protocols that were developed as  part of hardware designs. 
309|Evaluating Deadlock Detection Methods for Concurrent Software|Static analysis of concurrent programs has been hindered by the well known state explosion problem. Although many different techniques have been proposed to combat this state explosion, there is little empirical data comparing the performance of the methods. This information is essential for assessing the practical value of a technique and for choosing the best method for a particular problem. In this paper, we carry out an evaluation of three techniques for combating the state explosion problem in deadlock detection: reachability search with a partial order state space reduction, symbolic model checking, and inequality necessary conditions. We justify the method used for the comparison, and carefully analyze several sources of potential bias. The results of our evaluation provide valuable data on the kinds of programs to which each technique might best be applied. Furthermore, we believe that the methodological issues we discuss are of general significance in comparison of analysis te...
310|Slicing Software for Model Construction|Applying finite-state verification techniques (e.g., model checking) to software requires that program  source code be translated to a finite-state transition system that safely models program behavior.  Automatically checking such a transition system for a correctness property is typically very costly,  thus it is necessary to reduce the size of the transition system as much as possible. In fact, it is often  the case that much of a program&#039;s source code is irrelevant for verifying a given correctness property.  In this paper, we apply program slicing techniques to remove automatically such irrelevant code  and thus reduce the size of the corresponding transition system models. We give a simple extension of  the classical slicing definition, and prove its safety with respect to model checking of linear temporal  logic (LTL) formulae. We discuss how this slicing strategy fits into a general methodology for deriving  effective software models using abstraction-based program specializati...
311|Software Model Checking -- Extracting Verification Models  from source code|To formally verify a large software application, the standard method is to  invest a considerable amount of time and expertise into the manual  construction of an abstract model, which is then analyzed for its properties by  either a mechanized or by a human prover. There are two main problems with  this approach. The first problem is that this verification method can be no  more reliable than the humans that perform the manual steps. If rate of error  for human work is a function of problem size, this holds not only for the  construction of the original application, but also for the construction of the  model. This means that the verification process tends to become unreliable for  larger applications. The second problem is one of timing and relevance. Software
312|A Formal Study of Slicing for Multi-threaded Programs with JVM Concurrency Primitives|. Previous work has shown that program slicing can be a useful  step in model-checking software systems. We are interested in applying  these techniques to construct models of multi-threaded Java programs.  Past work does not address the concurrency primitives found in Java,  nor does it provide the rigorous notions of slice correctness that are necessary  for reasoning about programs with non-deterministic behaviour  and potentially infinite computation traces.  In this paper, we define the semantics of a simple multi-threaded language  with concurrency primitives matching those found in the Java  Virtual Machine, we propose a bisimulation-based notion of correctness  for slicing in this setting, we identify notions of dependency that are  relevant for slicing multi-threaded Java programs, and we use these dependencies  to specify a program slicer for the language presented in the  paper. Finally, we discuss how these dependencies can be refined to take  into account common programmin...
313|Constructing Compact Models of Concurrent Java Programs|Finite-state verification technology (e.g., model checking) provides a powerful means to detect concurrency errors, which are often subtle and difficult to reproduce. Nevertheless, widespread use of this technology by developers is unlikely until tools provide automated support for extracting the required finite-state models directly from program source. In this paper, we explore the extraction of compact concurrency models from Java code. In particular, we show how static pointer analysis, which has traditionally been used for computing alias information in optimizers, can be used to greatly reduce the size of finite-state models of concurrent Java programs.
314|Verification of Erlang Programs using Abstract Interpretation and Model Checking|We present an approach for the verification of Erlang programs using abstract interpretation and model checking. In general model checking for temporal logics like LTL and Erlang programs is undecidable. Therefore we define a framework for abstract interpretations for a core fragment of Erlang. operational semantics preserves all paths of the standard operational semantics. We consider properties that have to hold on all paths of a system, like properties in LTL. If these properties can be proved for the abstract operational semantics, they also hold for the Erlang program. They can be proved with model checking if the abstract operational semantics is a finite transition system. Therefore we introduce a example abstract interpretation, which has this property. We have implemented this approach as a prototype and were able to prove properties like mutual exclusion or the absence of deadlocks and lifelocks for some Erlang programs.
315|An Optimizing Compiler for Efficient Model Checking|Different model checking tools offer a variety of specification languages to encode systems. These specifications are compiled into an intermediate form from which the global automata are derived at verification time. Some tools, such as SPIN, provide the user with constructs that can be used to affect the size of the global automata. In other tools, such as Mur&#039;, the user specifies a system directly in terms of its global automata using a guarded command language, and hence has complete control over the automata sizes. Our experience shows that using low-level specifications we can significantly reduce verification times. The question then is, whether we can derive the low-level representations directly from a high-level specification without user intervention or dependence on user annotations. We address this problem in this paper. We develop an optimizing compilation technique that transforms high-level specifications based on value-passing CCS into rules  from which transitions of ...
316|Specializing Configurable Systems for Finite-state Verification|As finite-state verification techniques and tools, such as model checkers, continue to mature, researchers and practitioners attempt to apply them in increasingly realistic software development settings. Concurrent applications, and components of those applications, are often implemented as configurable systems (i.e., where size, structure or selected behavior aspects are taken as system inputs). These systems are typically implemented using dynamically allocated data and threads of control. This use of dynamism makes it very difficult to render behavioral models of configurable systems that would be suitable as input to finite-state verification tools. Currently, configurable systems can only be verified by performing hand-transformations of the source code that are often time-consuming, tedious, and error-prone. In this paper, we apply partial evaluation techniques to transform source code automatically into a form from which finite-state systems can be extracted. We illustrate these...
317|Integration of the cognitive and the psychodynamic unconscious|Cognitive-experiential self-theory integrates the cognitive and the psychodynamic unconscious by assuming the ex-istence of two parallel, interacting modes of information processing: a rational system and an emotionally driven experiential system. Support for the theory is provided by the convergence of a wide variety of theoretical positions on two similar processing modes; by real-life phenom-ena—such as conflicts between the heart and the head; the appeal of concrete, imagistic, and narrative represen-tations; superstitious thinking; and the ubiquity of religion throughout recorded history—and by laboratory research, including the prediction of new phenomena in heuristic reasoning. N early 100 years ago, Freud introduced a dualtheory of information processing that placeddeviant behavior squarely in the realm of the natural sciences and, more particularly, in psychology.
318|Transfer of Cognitive Skill|A framework for skill acquisition is proposed that includes two major stages in the development of a cognitive skill: a declarative stage in which facts about the skill domain are interpreted and a procedural stage in which the domain knowledge is directly embodied in procedures for performing the skill. This general framework has been instantiated in the ACT system in which facts are encoded in a propositional network and procedures are encoded as productions. Knowledge compilation is the process by which the skill transits from the declarative stage to the procedural stage. It consists of the subprocesses of composition, which collapses sequences of productions into single productions, and proceduralization, which embeds factual knowledge into productions. Once proceduralized, further learning processes operate on the skill to make the productions more selective in their range of applications. These processes include generalization, discrimination, and strengthening of productions. Comparisons are made to similar concepts from past learning theories. How these learning mechanisms apply to produce the power law speedup in processing time with practice is discussed. It requires at least 100 hours of learning and practice to acquire any significant cognitive skill to a reasonable degree of proficiency. For instance, after 100 hours a student learning to program a computer has achieved only a very modest facility in the skill. Learning one&#039;s primary language takes tens of thousands of hours. The psychology of human learning has been very thin in ideas about what happens to skills under the impact of this amount of learning—and for obvious reasons. This article presents a theory about the changes in the nature of a skill over such large time scales and about the basic learning processes that are responsible.
319|Implicit learning and tacit knowledge|I examine the phenomenon of implicit learning, the process by which knowledge about the rale-governed complexities of the stimulus environment is acquired independently of conscious attempts to do so. Our research with the two, seemingly disparate experimental paradigms of synthetic grammar learning and probability learning is reviewed and integrated with other approaches to the general problem of unconscious cognition. The conclusions reached are as follows: (a) Implicit learning produces a tacit knowledge base that is abstract and representative of the structure of the environment; (b) such knowledge is optimally acquired independently of conscious efforts to learn; and (c) it can be used implicitly to solve problems and make accurate decisions about novel stimulus circumstances. Various epistemological issues and related prob-1 lems such as intuition, neuroclinical disorders of learning and memory, and the relationship of evolutionary processes to cognitive science are also discussed. Some two decades ago the term implicit learning was first used to characterize how one develops intuitive knowledge about the underlying structure of a complex stimulus envi-
320|How mental systems believe|Is there a difference between believing and merely understanding an idea?Descartes thought so. He considered the acceptance and rejection of an idea to be alternative outcomes of an effortful assessment process that occurs subsequent to the automatic comprehension of that idea. This article examined Spinoza&#039;s alternative suggestion that (a) the acceptance of an idea is part of the automatic comprehension of that idea and (b) the rejection of an idea occurs subsequent to, and more effortfully than, its acceptance. ideas is quite similar to the mental representation of physical objects: People believe in the ideas they comprehend, as quickly and automatically as they believe in the objects they see. Research in social and cognitive psychology suggests that Spinoza&#039;s model may be a more accurate account of human belief than is that of Descartes. Though Truth and Falsehood bee Neare twins, yet Truth a little elder is.--John Donne, 1635/1930, p. 129 Everyone knows that understanding is one thing and believing is another, that people can consider ideas without considering them so, and that one must have an idea before one can determine its merit. &#034;Everyone knows the difference.., between supposing a proposition and acquiescing in its truth &#034; (James, 1890, p. 283). Nonetheless, this article suggests that what everyone knows may be, at least in part, wrong. It will be argued that the comprehension and acceptance of ideas are not clearly separable psychological acts, but rather that comprehension includes acceptance of that which is comprehended.
321|The self-concept revisited: Or a theory of a theory|Is there a need for a self-concept in psychology? Almost from the beginning, the field has been divided on this question. From a behavioristic viewpoint, the self-concept has an aura of mysticism about it, appearing not far removed from the concept of a soul. One can neither see a self-concept, nor touch it, and no one has succeeded as yet in adequately defining it as a hypothetical construct. Definitions that are offered tend to lack meaningful referents or to be circular. Thus, the self has been defined in terms of the &#034;I&#034; or the &#034;me, &#034; or both, or as the individual&#039;s reactions to himself. Some authors, apparently having despaired of providing an adequate definition, dispense with the matter by an appeal to common sense and by asserting that everyone knows he has a self as
322|Cognitive-experiential selftheory and subjective probability: Further evidence for two conceptual systems|Three experiments (N = 1,331) demonstrated that research findings on suspiciousness about coinci-dences (Miller, Turnbull,  &amp; McFarland, 1989) can be accounted for in terms of subjective probabil-ity, as predicted by cognitive-experiential self-theory (CEST) but in contrast with the norm theory (NT) account offered by Miller et al. (1989). Ss participated in a hypothetical (Experiments 1 and 2) or real (Experiment 3) lottery game in which they chose between 2 bowls offering equivalent probabilities of winning or losing but differing with respect to absolute numbers (e.g., 1 in 10 vs. 10 in 100). Responses across 4 conditions (2 probability levels x 2 outcome types) and across the 3 experiments supported predictions derived from CEST but not those derived from NT. Results are discussed in terms of 2 conceptual systems, rational and experiential, that operate by different rules of inference. Cognitive psychologists over the past several decades have adopted and investigated an explanation for irrational decision making based on a model of humans as limited-capacity infor-mation processors who use cognitive shortcuts, or heuristics, to solve problems that arise in everyday living (for reviews, see
323|Dynamic taint analysis for automatic detection, analysis, and signature generation of exploits on commodity software|Software vulnerabilities have had a devastating effect on the Internet. Worms such as CodeRed and Slammer can compromise hundreds of thousands of hosts within hours or even minutes, and cause millions of dollars of damage [32, 51]. To successfully combat these fast automatic Internet attacks, we need fast automatic attack detection and filtering mechanisms. In this paper we propose dynamic taint analysis for automatic detection and analysis of overwrite attacks, which include most types of exploits. This approach does not need source code or special compilation for the monitored program, and hence works on commodity software. To demonstrate this idea, we have implemented TaintCheck, a mechanism that can perform dynamic taint analysis by performing binary rewriting at run time. We show that TaintCheck reliably detects most types of exploits. We found that TaintCheck produced no false positives for any of the many different programs that we tested. Further, we show how we can use a two-tiered approach to build a hybrid exploit detector that enjoys the same accuracy as TaintCheck but have extremely low performance overhead. Finally, we propose a new type of automatic signature generation—semanticanalysis based signature generation. We show that by backtracing the chain of tainted data structure rooted at the detection point, TaintCheck can automatically identify which original flow and which part of the original flow have caused the attack and identify important invariants of the payload that can be used as signatures. Semantic-analysis based signature generation can be more accurate, resilient against polymorphic worms, and robust to attacks exploiting polymorphism than the pattern-extraction based signature generation methods.
324|Bro: A System for Detecting Network Intruders in Real-Time|We describe Bro, a stand-alone system for detecting network intruders in real-time by passively monitoring a network link over which the intruder&#039;s traffic transits. We give an overview of the system&#039;s design, which emphasizes highspeed (FDDI-rate) monitoring, real-time notification, clear separation between mechanism and policy, and extensibility. To achieve these ends, Bro is divided into an “event engine” that reduces a kernel-filtered network traffic stream into a series of higher-level events, and a “policy script interpreter” that interprets event handlers written in a specialized language used to express a site&#039;s security policy. Event handlers can update state information, synthesize new events, record information to disk, and generate real-time notifications via syslog. We also discuss a number of attacks that attempt to subvert passive monitoring systems and defenses against these, and give particulars of how Bro analyzes the six applications integrated into it so far: Finger, FTP, Portmapper, Ident, Telnet and Rlogin. The system is publicly available in source code form.  
326|A Secure Environment for Untrusted Helper Applications -- Confining the Wily Hacker |Many popular programs, such as Netscape, use untrusted helper applications to process data from the network. Unfortunately, the unauthenticated network data they interpret could well have been created by an adversary, and the helper applications are usually too complex to be bug-free. This raises significant security concerns. Therefore, it is desirable to create a secure environment to contain untrusted helper applications. We propose to reduce therisk  of a security breachby restricting the program&#039;s access to the operating system. In particular, we intercept and filter dangerous system calls via the Solaris process tracing facility. This enabled us to build a simple, clean, user-mode implementation of as ecure  environment for untrusted helper applications. Our implementation has negligible performance impact, and can protect pre-existing applications.
327|Flow-Sensitive Type Qualifiers|We present a system for extending standard type systems with flow-sensitive type qualifiers. Users annotate their programs with type qualifiers, and inference checks that the annotations are correct. In our system only the type qualifiers are modeled flow-sensitively - the underlying standard types are unchanged, which allows us to obtain an efficient constraint-based inference algorithm that integrates flow-insensitive alias analysis, effect inference, and ideas from linear type systems to support strong updates. We demonstrate the usefulness of flow-sensitive type qualifiers by finding a number of new locking bugs in the Linux kernel.
328|Dynamic Program Slicing|The conventional notion of a program slice---the set of all statements that might affect the value of a variable occurrence---is totally independent of the program input values. Program debugging, however, involves analyzing the program behavior under the specific inputs that revealed the bug. In this paper we address the dynamic counterpart of the static slicing problem---finding all statements that really affected the value of a variable occurrence for the given program inputs. Several approaches for computing dynamic slices are examined. The notion of a Dynamic Dependence Graph and its use in computing dynamic slices is discussed. We introduce the concept of a Reduced Dynamic Dependence Graph whose size does not depend on the length of execution history, which is unbounded in general, but whose size is bounded and is proportional to the number of dynamic slices arising during the program execution.
329|Autograph: Toward automated, distributed worm signature detection|Today’s Internet intrusion detection systems (IDSes) monitor edge networks ’ DMZs to identify and/or filter malicious flows. While an IDS helps protect the hosts on its local edge network from compromise and denial of service, it cannot alone effectively intervene to halt and reverse the spreading of novel Internet worms. Generation of the worm signatures required by an IDS—the byte patterns sought in monitored traffic to identify worms—today entails non-trivial human labor, and thus significant delay: as network operators detect anomalous behavior, they communicate with one another and manually study packet traces to produce a worm signature. Yet intervention must occur early in an epidemic to halt a worm’s spread. In this paper, we describe Autograph, a system that automatically generates signatures for novel Internet worms that propagate using TCP transport. Autograph generates signatures by analyzing the prevalence of portions of flow payloads, and thus uses no knowledge of protocol semantics above the TCP level. It is designed to produce signatures that exhibit high sensitivity (high true positives) and high specificity (low false positives); our evaluation of the system on real DMZ traces validates that it achieves these goals. We extend Autograph to share port scan reports among distributed monitor instances, and using trace-driven simulation, demonstrate the value of this technique in speeding the generation of signatures for novel worms. Our results elucidate the fundamental trade-off between early generation of signatures for novel worms and the specificity of these generated signatures. 1
330|Improving Host Security with System Call Policies|We introduce a system that eliminates the need to run programs in privileged process contexts. Using our system, programs run unprivileged but may execute certain operations with elevated privileges as determined by a configurable policy eliminating the need for suid or sgid binaries. We present the design and analysis of the &#034;Systrace&#034; facility which supports fine grained process confinement, intrusion detection, auditing and privilege elevation. It also facilitates the often difficult process of policy generation. With Systrace, it is possible to generate policies automatically in a training session or generate them interactively during program execution. The policies describe the desired behavior of services or user applications on a system call level and are enforced to prevent operations that are not explicitly permitted. We show that Systrace is efficient and does not impose significant performance penalties.
331|Automated worm fingerprinting|Network worms are a clear and growing threat to the security of today’s Internet-connected hosts and networks. The combination of the Internet’s unrestricted connectivity and widespread software homogeneity allows network pathogens to exploit tremendous parallelism in their propagation. In fact, modern worms can spread so quickly, and so widely, that no human-mediated reaction can hope to contain an outbreak. In this paper, we propose an automated approach for quickly detecting previously unknown worms and viruses based on two key behavioral characteristics – a common exploit sequence together with a range of unique sources generating infections and destinations being targeted. More importantly, our approach – called “content sifting ”  – automatically generates precise signatures that can then be used to filter or moderate the spread of the worm elsewhere in the network. Using a combination of existing and novel algorithms we have developed a scalable content sifting implementation with low memory and CPU requirements. Over months of active use at UCSD, our Earlybird prototype system has automatically detected and generated signatures for all pathogens known to be active on our network as well as for several new worms and viruses which were unknown at the time our system identified them. Our initial experience suggests that, for a wide range of network pathogens, it may be practical to construct fully automated defenses – even against so-called “zero-day” epidemics. 1
332|Secure Execution Via Program Shepherding|We introduce program shepherding, a method for monitoring control flow transfers during program execution to enforce a security policy. Program shepherding provides three techniques as building blocks for security policies. First, shepherding can restrict execution privileges on the basis of code origins. This distinction can ensure that malicious code masquerading as data is never executed, thwarting a large class of security attacks. Second, shepherding can restrict control transfers based on instruction class, source, and target. For example, shepherding can forbid execution of shared library code except through declared entry points, and can ensure that a return instruction only targets the instruction after a call. Finally, shepherding guarantees that sandboxing checks placed around any type of program operation will never be bypassed. We have implemented these capabilities efficiently in a runtime system with minimal or no performance penalties. This system operates on unmodified native binaries, requires no special hardware or operating system support, and runs on existing IA-32 machines under both Linux and Windows.
333|Address obfuscation: an efficient approach to combat a broad range of memory error exploits|Rights to individual papers remain with the author or the author&#039;s employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein.
334|Anomalous payload-based network intrusion detection|We present a payload-based anomaly detector, we call PAYL, for intrusion detection. PAYL models the normal application payload of network traffic in a fully automatic, unsupervised fashion. The method we choose is very efficient; our goal is to deploy the detector in high bandwidth environments either on a firewall, a network appliance, a proxy server or on the target hosts. We first compute during a training phase a profile byte frequency distribution and their standard deviation of the application payload flowing to a single host and port. We then use Mahalanobis distance during the detection phase to calculate the similarity of new data against the pre-computed profile. The detector compares this measure against a threshold and generates an alert when the distance of the new input exceeds this threshold. The model is host- and port-specific and is conditioned on the payload length. Thus, a set of profiles are computed for every possible length payload. A second phase clusters the profiles to increase accuracy and decrease resource consumption. The method has the advantage of being very fast to compute, is state-less and does not parse the input stream, generates a small model, and can be easily modified to an incremental online learning algorithm so that the model is continuously updated to maintain an accurate normal model in real-time. The modeling method is completely unsupervised, and is tolerant to noise in the training data. Furthermore, the method is also resistant to mimicry-attack; attackers would need to model the site’s normal payload distributions in order to pad their poisoned payload to go unnoticed by the
335|Valgrind: A program supervision framework|a;1
336|Countering Code-Injection Attacks With Instruction-Set Randomization|We describe a new, general approach for safeguarding systems against any type of code-injection attack. We apply Kerckhoff’s principle, by creating process-specific randomized instruction sets (e.g., machine instructions) of the system executing potentially vulnerable software. An attacker who does not know the key to the randomization algorithm will inject code that is invalid for that randomized processor, causing a runtime exception. To determine the difficulty of integrating support for the proposed mechanism in the operating system, we modified the Linux kernel, the GNU binutils tools, and the bochs-x86 emulator. Although the performance penalty is significant, our prototype demonstrates the feasibility of the approach, and should be directly usable on a suitable-modified processor (e.g., the Transmeta Crusoe). Our approach is equally applicable against code-injecting attacks in scripting and interpreted languages, e.g., web-based SQL injection. We demonstrate this by modifying the Perl interpreter to permit randomized script execution. The performance penalty in this case is minimal. Where our proposed approach is feasible (i.e., in an emulated environment, in the presence of programmable or specialized hardware, or in interpreted languages), it can serve as a low-overhead protection mechanism, and can easily complement other mechanisms.
337|Honeycomb - Creating Intrusion Detection Signatures Using Honeypots|Abstract — This paper describes a system for automated generation of attack signatures for network intrusion detection systems. Our system applies pattern-matching techniques and protocol conformance checks on multiple levels in the protocol hierarchy to network traffic captured a honeypot system. We present results of running the system on an unprotected cable modem connection for 24 hours. The system successfully created precise traffic signatures that otherwise would have required the skills and time of a security officer to inspect the traffic manually. Index Terms — network intrusion detection, traffic signatures, honeypots, pattern detection, protocol analysis, longest-commonsubstring algorithms, suffix trees. I.
338|Understanding data lifetime via whole system simulation|Rights to individual papers remain with the author or the author&#039;s employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein.
339|A Practical Dynamic Buffer Overflow Detector|Despite previous efforts in auditing software manually and automatically, buffer overruns are still being discovered in programs in use. A dynamic bounds checker detects buffer overruns in erroneous software before it occurs and thereby prevents attacks from corrupting the integrity of the system. Dynamic buffer overrun detectors have not been adopted widely because they either (1) cannot guard against all buffer overrun attacks, (2) break existing code, or (3) incur too high an overhead. This paper presents a practical detector called CRED (C Range Error Detector) that avoids each of these deficiencies. CRED finds all buffer overrun attacks as it directly checks for the bounds of memory accesses. Unlike the original referent-object based bounds-checking technique, CRED does not break existing code because it uses a novel solution to support program manipulation of out-of-bounds addresses. Finally, by restricting the bounds checks to strings in a program, CRED’s overhead is greatly reduced without sacrificing protection in the experiments we performed. CRED is implemented as an extension of the GNU C compiler version 3.3.1. The simplicity of our design makes possible a robust implementation that has been tested on over 20 open-source programs, comprising over 1.2 million lines of C code. CRED proved effective in detecting buffer overrun attacks on programs with known vulnerabilities, and is the only tool found to guard against a testbed of 20 different buffer overflow attacks[34]. Finding overruns only on strings impose an overhead of less
340|Shield: Vulnerability-Driven Network Filters for Preventing Known Vulnerability Exploits|Software patching has not been an effective first-line defense preventing large-scale worm attacks, even when patches had long been available for their corresponding vulnerabilities. Generally, people have been reluctant to patch their systems immediately, because patches are perceived to be unreliable and disruptive to apply. To address this problem, we propose a first-line worm defense in the network stack, using shields -- vulnerability-specific, exploit-generic network filters installed in end systems once a vulnerability is discovered and before the patch is applied. These filters examine the incoming or outgoing traffic of vulnerable applications, and drop traffic that exploits vulnerabilities. Shields are less disruptive to install and uninstall, easier to test for bad side effects, and hence more reliable than traditional software patches. In this paper, we show...
341|Hardening COTS Software with Generic Software Wrappers|Numerous techniques exist to augment the security functionality of Commercial Off-The-Shelf (COTS) applications and operating systems, making them more suitable for use in mission-critical systems. Although individually useful, as a group these techniques present difficulties to system developers because they are not based onacommon framework which might simplify integration and promote portability and reuse. This paper presents techniques for developing Generic Software Wrappers -- protected, non-bypassable kernel-resident software extensions for augmenting security without modi cation of COTS source. We describe the key elements of our work: our high-level Wrapper Definition Language (WDL), and our framework for configuring, activating, and managing wrappers. We also discuss code reuse, automatic management of extensions, a framework for system-building through composition, platform-independence, and our experiences with our Solaris and FreeBSD prototypes.  
343|Hunting for metamorphic|As virus writers developed numerous polymorphic engines, virus scanners became stronger in their defense against them. A virus scanner which used a code emulator to detect viruses looked like it was on steroids compared to those without an emulator-based scanning engine. Nowadays, most polymorphic viruses are considered boring. Even though they can be extremely hard to detect, most of today’s products are able to deal with them relatively easily. These are the scanners that survived the DOS polymorphic days. For some of the scanners DOS polymorphic viruses meant the ‘end of days’. Other scanners died with the macro virus problem. For most products the next challenge to take is 32-bit metamorphosis. Metamorphic viruses are nothing new. We have seen them in DOS days, though some of them, like ACG, already used 32-bit instructions. The next step is 32-bit metamorphosis under Windows environments. Virus writers already took the first step in that direction. In this paper the authors will examine metamorphic engines to provide a better general understanding of the problem that we are facing. The authors also provide detection examples of some of the metamorphic viruses. VIRUS BULLETIN CONFERENCE ©2001 Virus Bulletin Ltd, The Pentagon, Abingdon, Oxfordshire, OX14 3YP, England. Tel +44 1235 555139. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form, without the prior written permission of the publishers. 124 • SZÖR &amp; FERRIE, HUNTING FOR METAMORPHIC 1
344|MAPbox: Using Parameterized Behavior Classes to Confine Applications|Designing a suitable mechanism to confine commonly used applications is challenging as such a mechanism needs to satisfy conflicting requirements. The trade-off is between configurability and ease of use. In this paper, we present the design, implementation and evaluation of MAPbox,  a general-purpose confinement mechanism that retains the ease of use of specialized sandboxes such as Janus and SBOX while providing significantly more configurability. The key idea is to group application behaviors into classes based on the expected functionality and the resources required to achieve that functionality. Classification of behaviors provides a set of behavior labels (class names) that can be used to concisely communicate the expected functionality of programs between the provider and the users. This is similar to the MIME-types used to concisely describe the expected format of data files. Classification of application behaviors also allows class-specific sandboxes to be built and instantiat...
345|Using CQUAL for static analysis of authorization hook placement|The Linux Security Modules (LSM) framework is a set of authorization hooks for implementing flexible access control in the Linux kernel. While much effort has been devoted to defining the module interfaces, little attention has been paid to verifying the correctness of hook placement. This paper presents a novel approach to the verification of LSM authorization hook placement using CQUAL, a type-based static analysis tool. With a simple CQUAL lattice configuration and some GCC-based analyses, we are able to verify complete mediation of operations on key kernel data structures. Our results reveal some potential security vulnerabilities of the current LSM framework, one of which we demonstrate to be exploitable. Our experiences demonstrate that combinations of conceptually simple tools can be used to perform fairly complex analyses. 1
346|Mitigating buffer overflows by operating system randomization|Abstract We propose three methods for mitigating buffer overflows by using operating system randomization: randomization of system call mappings, randomization of global library entry points, and randomization of stack placement. These mechanisms are light weight. They increase the heterogeneity of systems and make it more difficult for attackers to exploit buffer overflow vunerabilities. We also present a mechanism, timed capabilities, for more secure resource management.
347|On gray-box program tracking for anomaly detection|Rights to individual papers remain with the author or the author&#039;s employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein.
348|TRON: Process-Specific File Protection for the UNIX Operating System|The file protection mechanism provided in UNIX is insufficient for current computing environments. While the UNIX file protection system attempts to protect users from attacks by other users, it does not directly address the agents of destruction--- executing processes. As computing environments become more interconnected and interdependent, there is increasing pressure and opportunity for users to acquire and test non--secure, and possibly malicious, software. We introduce TRON, a process--level discretionary access control system for UNIX. TRON allows users to specify capabilities for a process&#039; access to individual files, directories, and directory trees. These capabilities are enforced by system call wrappers compiled into the operating system kernel. No privileged system calls, special files, system administrator intervention, or changes to the file system are required. Existing UNIX programs can be run without recompilation under TRON--enhanced UNIX. Thus, TRON improves UNIX secu...
349|A Network Worm Vaccine Architecture|The ability of worms to spread at rates that effectively preclude human-directed reaction has elevated them to a first-class security threat to distributed systems. We present the first reaction mechanism that seeks to automatically patch vulnerable software. Our system employs a collection of sensors that detect and capture potential worm infection vectors. We automatically test the effects of these vectors on appropriately-instrumented sandboxed instances of the targeted application, trying to identify the exploited software weakness. Our heuristics allow us to automatically generate patches that can protect against certain classes of attack, and test the resistance of the patched application against the infection vector. We describe our system architecture, discuss the various components, and propose directions for future research.
350|we contain Internet worms|Worm containment must be automatic because worms can spread too fast for humans to respond. Recent work has proposed a network centric approach to automate worm containment: network traffic is analyzed to derive a packet classifier that blocks (or rate-limits) worm propagation. This approach has fundamental limitations because the analysis has no information about the application vulnerabilities exploited by worms. This paper proposes Vigilante, a new host centric approach for automatic worm containment that addresses these limitations. Vigilante relies on collaborative worm detection at end hosts in the Internet but does not require hosts to trust each other. Hosts detect worms by analysing attempts to infect applications and broadcast self-certifying alerts (SCAs) when they detect a worm. SCAs are automatically generated machine-verifiable proofs of vulnerability; they can be independently and inexpensively verified by any host. Hosts can use SCAs to generate filters or patches that prevent infection. We present preliminary results showing that Vigilante can effectively contain fast spreading worms that exploit unknown vulnerabilities. 1.
352|Buttercup: On network-based detection of polymorphic buffer overflow vulnerabilities|Attack polymorphism is a powerful tool for the attackers in the Internet to evade signature-based intrusion detection/prevention systems. In addition, new and faster Internet worms can be coded and launched easily by even high school students anytime against our critical infrastructures, such as DNS or update servers. We believe that polymorphic Internet worms will be developed in the future such that many of our current solutions might have a very small chance to survive. In this paper, we propose a simple solution called “Buttercup ” to counter against attacks based on buffer-overflow exploits (such as CodeRed, Nimda, Slammer, and Blaster). We have implemented our idea in SNORT, and included 19 return address ranges of buffer-overflow exploits. With a suite of tests against 55 TCPdump traces, the false positive rate for our best algorithm is as low as 0.01%. This indicates that, potentially, Buttercup can drop 100 % worm attack packets on the wire while only 0.01 % of the good packets will be sacrificed.
353|Run-time type checking for binary programs|Abstract. Many important software systems are written in the C programming language. Unfortunately, the C language does not provide strong safety guarantees, and many common programming mistakes introduce type errors that are not caught by the compiler. These errors only manifest themselves at run time through unexpected program behavior, and it is often hard to isolate and identify their causes. This paper presents the Hobbes run-time type checker for compiled C programs. Our tool interprets compiled binaries, tracks type information for all memory and register locations, and reports warnings when a variety of type errors occur. Because the Hobbes type checker does not rely on source code, it is effective in many situations where similar tools are not, such as when full source code is not available or when C source is linked with program fragments written in assembly or other languages. 1
354|Efficient Integrity Checking over XML Documents |Abstract. The need for incremental constraint maintenance within collections of semi-structured documents has been ever increasing in the last years due to the widespread diffusion of XML. This problem is addressed here by adapting to the XML data model some constraint verification techniques known in the context of deductive databases. Our approach allows the declarative specification of constraints as well as their optimization w.r.t. given update patterns. Such optimized constraints are automatically translated into equivalent XQuery expressions in order to avoid illegal updates. This automatic process guarantees an efficient integrity checking that combines the advantages of declarativity with incrementality and early detection of inconsistencies. 1
355|Relational Databases for Querying XML Documents:  Limitations and Opportunities|XML is fast emerging as the dominant standard for representing data in the World Wide Web. Sophisticated query engines that allow users to effectively tap the data stored in XML documents will be crucial to exploiting the full power of XML. While there has been a great deal of activity recently proposing new semistructured data models and query languages for this purpose, this paper explores the more conservative approach of using traditional relational database engines for processing XML documents conforming to Document Type Descriptors (DTDs). To this end, we have developed algorithms and implemented a prototype system that converts XML documents to relational tuples, translates semi-structured queries over XML documents to SQL queries over tables, and converts the results to XML. We have qualitatively evaluated this approach using several real DTDs drawn from diverse domains. It turns out that the relational approach can handle most (but not all) of the semantics of semi-structured queries over XML data, but is likely to be effective only in some cases. We identify the causes for these limitations and propose certain extensions to the relational 
356|Storing semistructured data with STORED |Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistrcutured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.
357|Updating XML|As XML has developed over the past few years, its role has expanded beyond its original domain as a semantics-preserving markup language for online documents, and it is now also the de facto format for interchanging data between heterogeneous systems. Data sources export XML &#034;views&#034; over their data, and other systems can directly import or query these views. As a result, there has been great interest in languages and systems for expressing queries over XML data, whether the XML is stored in a repository or generated as a view over some other data storage format.
358|Storing and Querying XML Data using an RDMBS|XML is rapidly becoming a popular data format. It can be expected that soon large volumes of XML data will exist. XML data is either produced manually (like html documents today), or it is generated by a new generation of software tools for the WWW and/or electronic data interchange (EDI). The purpose of this paper is to present the results of an initial study about storing and querying XML data. As
359|From XML schema to relations: A cost-based approach to XML storage|bohannon,juliana,prasan,simeon¡ XML has become an important medium for data representation, particularly when that data is exchanged over or browsed on the Internet. As the volume of XML data increases, there is a growing interest in storing XML in relational databases so that the well-developed features of these systems (e.g., concurrency control, crash recovery, query processors) can be re-used. However, given the wide variety of XML applications and the mismatch between XML’s nested-tree structure and the flat tuples of the relational model, storing XML documents in relational databases presents interesting challenges. LegoDB is a cost-based XML-to-relational mapping engine that addresses this problem. It explores a space of possible mappings and selects the best mapping for a given application (defined by an XML Schema, XML data statistics, and an XML query workload). LegoDB leverages existing XML and relational technologies: it represents the target application using XML standards and constructs the space of configurations using XML-specific operations, and it uses a traditional relational optimizer to obtain accurate cost estimates of the derived configurations. In this paper, we describe the LegoDB mapping engine and provide experimental results that demonstrate the effectiveness of this approach. 1
360|Validating Streaming XML Documents|This paper investigates the on-line validation of streaming XML documents with respect to a DTD, under memory constraints. We first consider validation using constant memory, formalized by a finite-state automaton  (fsa). We examine two flavors of the problem, depending on whether or not the XML document is assumed to be well-formed. The main results of the paper provide conditions on the DTDs under which validation of either flavor can be done using an fsa. For DTDs that cannot be validated by an fsa, we investigate two alternatives. The first relaxes the constant memory requirement by allowing a stack bounded in the depth of the XML document, while maintaining the deterministic, one-pass requirement. The second approach consists in refining the DTD to provide additional information that allows validation by an fsa.
361|XPath query containment |Consider an XML publish-subscribe scenario with hundreds of subscribers and tens of thousands of XML documents to be delivered per day. Subscribers specify the documents in which they are interested in
362|Constraints and Redundancy in Datalog|Two types of redundancies in datalog programs are considered. Redundancy based on reachability  eliminates rules and predicates that do not participate in any derivation tree of a fact for the query predicate. Redundancy based on irrelevance  is similar, but considers only minimal derivation trees, that is, derivation trees having no pair of identical atoms, such that one is an ancestor of the other. Algorithms for detecting these redundancies are given, including the case of programs with constraint literals. These algorithms not only detect redundancies in the presence of constraints, but also push constraints from the given query and rules to the EDB predicates. Under certain assumptions discussed in the paper, the constraints are pushed to the EDB as tightly as possible.  
363|A Survey of Current Methods for Integrity Constraint Maintenance and View Updating|During the process of updating a database, two interrelated problems could  arise. On one hand, when an update is applied to the database, integrity  constraints could become violated, thus falsifying database consistency. In  this case, the integrity constraint maintenance approach tries to obtain  additional updates to be applied to re-establish database consistency. On the  other hand, when an update request consist on updating some derived  predicate, a view updating mechanism must be applied to translate the  update request into correct updates on the underlying base facts.  In this paper, we propose a general framework to compare and classify  current methods in the field of view updating and integrity constraint  maintenance. In this sense, we classify them considering how they tackle  with both problems and, we also state the main drawbacks these methods  have.  1. Introduction  Most databases, like relational or deductive ones, allow the definition of intentional information l...
364|  Incremental Validation of  XML Documents |We investigate the incremental validation of XML documents with respect to DTDs and XML Schemas, under updates consisting of element tag renamings, insertions and deletions. DTDs are modeled as extended context-free grammars and XML Schemas are abstracted as &#034;specialized DTDs&#034;, allowing to decouple element types from element tags. For DTDs, we exhibit an O(m log n) incremental validation algorithm using an auxiliary structure of size O(n), where n is the size of the document and m the number of updates. For specialized DTDs, we provide an O(m log² n) incremental algorithm, again using an auxiliary structure of size O(n). This is a significant improvement over brute-force re-validation from scratch. 
365|Query Decomposition and View Maintenance for Query Languages for Unstructured Data|Recently, several query languages have been proposed for querying information sources whose data is not constrained by a schema, or whose schema is unknown. Examples include: LOREL (for querying data combined from several heterogeneous sources), W3QS (for querying the World Wide Web); and UnQL (for querying unstructured data). The natural data model for such languages is that of a rooted, labeled graph. Their main novelty is the ability to express queries which traverse arbitrarily long paths in the graph, typically described by a regular expression. Such queries however may prove difficult to evaluate in the case when the data is distributed on several sites, with many edges going between sites. A typical case is that of a collection of WWW sites, with links pointing freely from one site to another (even forming cycles). A naive query shipping strategy may force the query to migrate back and forth between the various sites, leading to poor performance (or even non-termination). We pre...
366|A Unified Constraint Model for XML|Integrity constraints are an essential part of modern schema definition languages. They are useful for semantic specification, update consistency control, query optimization, etc. In this paper, we propose UCM, a model of integrity constraints for XML that is both simple and expressive. Because it relies on a single notion of keys and foreign keys, the UCM model is easy to use and makes formal reasoning possible. Because it relies on a powerful type system, the UCM model is expressive, capturing in a single framework the constraints found in relational databases, object-oriented schemas and XML DTDs. We study the problem of consistency of UCM constraints, the interaction between constraints and subtyping, and algorithms for implementing these constraints.
367|Candan. Incremental maintenance of path-expression views|Caching data by maintaining materialized views typically requires updating the cache appropriately to reflect dynamic source updates. Extensive research has addressed the problem of incremental view maintenance for relational data but only few works have addressed it for semi-structured data. In this paper we address the problem of incremental maintenance of views defined over XML documents using pathexpressions. The approach described in this paper has the following main features that distinguish it from the previous works: (1) The view specification language is powerful and standardized enough to be used in realistic applications. (2) The size of the auxiliary data maintained with the views depends on the expression size and the answer size regardless of the source data size.(3) No source schema is assumed to exist; the source data can be any general well-formed XML document. Experimental evaluation is conducted to assess the performance benefits of the proposed approach.
368|XKvalidator: a constraint validator for XML|The role of XML in data exchange is evolving from one of merely conveying the structure of data to one that also conveys its semantics. In particular, several proposals for key and foreign key constraints have recently appeared, and aspects of these proposals have been adopted within XMLSchema. In this paper, we examine the problem of checking keys and foreign keys in XML documents using a validator based on SAX. The algorithm relies on an indexing technique based on the paths found in key definitions, and can be used for checking the correctness of an entire document (bulk checking) as well as for checking updates as they are made to the document (incremental checking). The asymptotic performance of the algorithm is linear in the size of the document or update. Furthermore, experimental results demonstrate reasonable performance.
369|Simplification of integrity constraints with aggregates and arithmetic built-ins|Abstract. Both aggregates and arithmetic built-ins are widely used in current database query languages: Aggregates are second-order constructs such as COUNT and SUM of SQL; arithmetic built-ins include relational and other mathematical operators that apply to numbers, such as = and +. These features are also of interest in the context of database integrity constraints: correct and efficient integrity checking is crucial, as, without any guarantee of data consistency, the answers to queries cannot be trusted. In this paper we propose a method of practical relevance that can be used to derive, at database design time, simplified versions of such integrity constraints that can be tested before the execution of any update. In this way, virtually no time is spent for optimization or rollbacks at run time. Both set and bag semantics are considered. 1
370|Survey on Data Integrity Checking Protocols in Cloud Computing |Cloud computing is a internet based computing model that supports convenient,on-demand and pay-for-use model. In this computing, data owners host their data on cloud servers and clients can access the data from cloud servers. Due to the data outsourcing, efficient verification of the outsourced data becomes a formidable challenge for data security in Cloud Computing (CC). Therefore, an independent auditing service is required to make sure that the data is correctly hosted in the Cloud. Several protocols are introduced for performing integrity in cloud storage. This paper focuses on the different integrity checking protocols to address the above issue. This paper provides an overview of these protocols by presenting their characteristics, functionality, benefits and limitations.
371|Enabling public verifiability and data dynamics for storage security in cloud computing|Abstract. Cloud Computing has been envisioned as the next-generation architecture of IT Enterprise. It moves the application software and databases to the centralized large data centers, where the management of the data and services may not be fully trustworthy. This unique paradigm brings about many new security challenges, which have not been well understood. This work studies the problem of ensuring the integrity of data storage in Cloud Computing. In particular, we consider the task of allowing a third party auditor (TPA), on behalf of the cloud client, to verify the integrity of the dynamic data stored in the cloud. The introduction of TPA eliminates the involvement of client through the auditing of whether his data stored in the cloud is indeed intact, which can be important in achieving economies of scale for Cloud Computing. The support for data dynamics via the most general forms of data operation, such as block modification, insertion and deletion, is also a significant step toward practicality, since services in Cloud Computing are not limited to archive or backup data only. While prior works on ensuring remote data integrity often lacks the support of either public verifiability or dynamic data operations, this paper achieves both. We first identify the difficulties and potential security problems of direct extensions with fully dynamic data updates from prior works and then show how to construct an elegant verification scheme for seamless integration of these two salient features in our protocol design. In particular, to achieve efficient data dynamics, we improve the Proof of Retrievability model [1] by manipulating the classic Merkle Hash Tree (MHT) construction for block tag authentication. Extensive security and performance analysis show that the proposed scheme is highly efficient and provably secure. 1
372|Decidable containment of recursive queries|One of the most important reasoning tasks on queries is checking containment, i.e., verifying whether one query yields necessarily a subset of the result of another one. Query containment is crucial in several contexts, such as query optimization, query reformulation, knowledge-base verification, information integration, integrity checking, and cooperative answering. Containment is undecidable in general for Datalog, the fundamental language for expressing recursive queries. On the other hand, it is known that containment between monadic Datalog queries and between Datalog queries and unions of conjunctive queries are decidable. It is also known that containment between unions of conjunctive two-way regular path queries, which are queries used in the context of semistructured data models containing a limited form of recursion in the form of transitive closure, is decidable. In this paper, we combine the automata-theoretic techniques at the base of these two decidability results to show that containment of Datalog in union of conjunctive two-way regular path queries is decidable in 2EXPTIME. By sharpening a known lower bound result for containment of Datalog in union of conjunctive queries we show also a matching lower bound.
373|Simplification of integrity constraints for data integration|Abstract. When two or more databases are combined into a global one, integrity may be violated even when each database is consistent with its own local integrity constraints. Efficient methods for checking global integrity in data integration systems are called for: answers to queries can then be trusted, because either the global database is known to be consistent or suitable actions have been taken to provide consistent views. The present work generalizes simplification techniques for integrity checking in traditional databases to the combined case. Knowledge of local consistency is employed, perhaps together with given a priori constraints on the combination, so that only a minimal number of tuples needs to be considered. Combination from scratch, integration of a new source, and absorption of local updates are dealt with for both the local-as-view and global-as-view approaches to data integration. 1
374|View Serializable Updates of Concurrent Index Structures|Abstract. We present new algorithms for concurrent reading and updating of B *-trees and binary search trees. Our algorithms are based on the well-known link technique, and improve previously proposed solutions in several respects. We prove formally that our algorithms are correct. We show that they satisfy a view serializability criterion, which fails for previous solutions. This stronger serializability criterion is central to the proof that several subtle (but essential) optimizations incorporated in our algorithms are correct. 1
375|A theory of timed automata|Model checking is emerging as a practical tool for automated debugging of complex reactive systems such as embedded controllers and network protocols (see [23] for a survey). Traditional techniques for model checking do not admit an explicit modeling of time, and are thus, unsuitable for analysis of real-time systems whose correctness depends on relative magnitudes of different delays. Consequently, timed automata [7] were introduced as a formal notation to model the behavior of real-time systems. Its definition provides a simple way to annotate state-transition graphs with timing constraints using finitely many real-valued clock variables. Automated analysis of timed automata relies on the construction of a finite quotient of the infinite space of clock valuations. Over the years, the formalism has been extensively studied leading to many results establishing connections to circuits and logic, and much progress has been made in developing verification algorithms, heuristics, and tools. This paper provides a survey of the theory of timed automata, and their role in specification and verification of real-time systems.
376|Design and Synthesis of Synchronization Skeletons Using Branching Time Temporal Logic|We propose a method of constructing concurrent programs in which the synchroni-zation skeleton of the program ~s automatically synthesized from a high-level (branching time) Temporal Logic specification. The synchronization skeleton is an abstraction of the actual program where detail irrelevant to synchronization is
377|The algorithmic analysis of hybrid systems|We present a general framework for the formal specification and algorithmic analysis of hybrid systems. A hybrid system consists of a discrete program with an analog environment. We model hybrid systems as nite automata equipped with variables that evolve continuously with time according to dynamical laws. For verification purposes, we restrict ourselves to linear hybrid systems, where all variables follow piecewise-linear trajectories. We provide decidability and undecidability results for classes of linear hybrid systems, and we show that standard program-analysis techniques can be adapted to linear hybrid systems. In particular, we consider symbolic model-checking and minimization procedures that are based on the reachability analysis of an infinite state space. The procedures iteratively compute state sets that are definable as unions of convex polyhedra in multidimensional real space. We also present approximation techniques for dealing with systems for which the iterative procedures do not converge.  
378|The Theory of Hybrid Automata|A hybrid automaton is a formal model for a mixed discrete-continuous system. We classify hybrid automata acoording to what questions about their behavior can be answered algorithmically. The classification reveals structure on mixed discrete-continuous state spaces that was previously studied on purely discrete state spaces only. In particular, various classes of hybrid automata induce finitary trace equivalence (or similarity, or bisimilarity) relations on an uncountable state space, thus permitting the application of various model-checking techniques that were originally developed for finite-state systems.
379|What&#039;s Decidable about Hybrid Automata?|. Hybrid automata model systems with both digital and analog components, such as embedded control programs. Many verification tasks for such programs can be expressed as reachability problems for hybrid automata. By improving on previous decidability and undecidability results, we identify a boundary between decidability and undecidability for the reachability problem of hybrid automata. On the positive side, we give an (optimal) PSPACE reachability algorithm for the case of initialized rectangular automata, where all analog variables follow independent trajectories within piecewise-linear envelopes and are reinitialized whenever the envelope changes. Our algorithm is based on the construction of a timed automaton that contains all reachability information about a given initialized rectangular automaton. The translation has practical significance for verification, because it guarantees the termination of symbolic procedures for the reachability analysis of initialized rectangular autom...
380|Model-Checking in Dense Real-time|Model-checking is a method of verifying concurrent systems in which a state-transition graph model of the system behavior is compared with a temporal logic formula. This paper extends model-checking for the branching-time logic CTL to the analysis of real-time systems, whose correctness depends on the magnitudes of the timing delays. For specifications, we extend the syntax of CTL to allow quantitative temporal operators such as 93!5 , meaning &#034;possibly within 5 time units.&#034; The formulas of the resulting logic, Timed CTL  (TCTL), are interpreted over continuous computation trees, trees in which paths are maps from the set of nonnegative reals to system states. To model finitestate systems we introduce timed graphs --- state-transition graphs annotated with timing constraints. As our main result, we develop an algorithm for model-checking, for determining the truth of a TCTL-formula with respect to a timed graph. We argue that choosing a dense domain instead of a discrete domain to mo...
381|Automatic Symbolic Verification of Embedded Systems|We present a model-checking procedure and its implementation for the automatic verification of embedded systems. The system components are described as Hybrid Automata -- communicating machines with finite control and real-valued variables that represent continuous environment parameters such as time, pressure, and temperature. The system requirements are specified in a temporal logic with stop watches, and verified by symbolic fixpoint computation. The verification procedure -- implemented in the Cornell Hybrid Technology Tool, HyTech -- applies to hybrid automata whose continuous dynamics is governed by linear constraints on the variables and their derivatives. We illustrate the method and the tool by checking safety, liveness, time-bounded, and duration requirements of digital controllers, schedulers, and distributed algorithms.
382|The Tool KRONOS|KRONOS [6, 8] is a tool developed with the aim to assist the user to validate complex real-time systems. The tool checks whether a real-tinae system modeled by a timed automaton [4] satisfies a timing property specified by a formula of the temporal logic TCTL [3]. KRONOS implements the symbolic model-checking
383|The Benefits of Relaxing Punctuality|The most natural, compositional, way of modeling real-time systems uses a dense domain for time. The satis ability of timing constraints that are capable of expressing punctuality in this model, however, is known to be undecidable. We introduce a temporal language that can constrain the time difference between events only with finite, yet arbitrary, precision and show the resulting logic to be EXPSPACE-complete. This result allows us to develop an algorithm for the verification of timing properties of real-time systems with a dense semantics.
384|An Old-Fashioned Recipe for Real Time|this paper appeared in ACM Transactions on Programming Languages and Systems 16, 5 (September 1994) 1543-- 1571. The appendix was published electronically by the ACM.  Contents
385|What Good Are Digital Clocks?|. Real-time systems operate in &#034;real,&#034; continuous time  and state changes may occur at any real-numbered time point. Yet  many verification methods are based on the assumption that states  are observed at integer time points only. What can we conclude if a  real-time system has been shown &#034;correct&#034; for integral observations?  Integer time verification techniques suffice if the problem of whether  all real-numbered behaviors of a system satisfy a property can be  reduced to the question of whether the integral observations satisfy a  (possibly modified) property. We show that this reduction is possible  for a large and important class of systems and properties: the class of  systems includes all systems that can be modeled as timed transition  systems; the class of properties includes time-bounded invariance  and time-bounded response.  1 Introduction  Over the past few years, we have seen a proliferation of formal methodologies for software and hardware design that emphasize the treatm...
387|HYTECH: The next generation|Abstract. We describe a new implementation of HyTech 1,asymbolic model checker for hybrid systems. Given a parametric description of an embedded system as a collection of communicating automata, HyTech automatically computes the conditions on the parameters under which the system satis es its safety and timing requirements. While the original HyTech prototype was based on the symbolic algebra tool Mathematica, the new implementation is written in C ++ and builds on geometric algorithms instead of formula manipulation. The new HyTech o ers a cleaner and more expressive input language, greater portability, superior performance (typically two to three orders of magnitude), and new features such as diagnostic error-trace generation. We illustrate the e ectiveness of the new implementation by applying HyTech to the automatic parametric analysis of the generic railroad crossing benchmark problem [HJL93] and to an active structure control algorithm [ECB94]. 1
388|Event-Clock Automata: A Determinizable Class of Timed Automata|We introduce event-recording automata. An event-recording automaton is a timed automaton that contains, for every event a, a clock that records the time of the last occurrence of a. The class of event-recording automata is, on one hand, expressive enough to model (finite) timed transition systems and, on the other hand, determinizable and closed under all boolean operations. As a result, the language inclusion problem is decidable for event-recording automata. We present a translation from timed transition systems to event-recording automata, which leads to an algorithm for checking if two timed transition systems have the same set of timed behaviors. We also consider event-predicting automata, which contain clocks that predict the time of the next occurrence of an event. The class of event-clock automata, which contain both event-recording and event-predicting clocks, is a suitable specification language for real-time properties. We provide an algorithm for checking if a timed automa...
389|Computer-aided verification|How can a computer program developer ensure that a program actually implements its intended purpose? This article describes a method for checking the correctness of certain types of computer programs. The method is used commercially in the development of programs implemented as integrated circuits and is applicable to the development of “control-intensive ” software programs as well. “Divide-and-conquer ” techniques central to this method apply to a broad range of program verification methodologies. Classical methods for testing and quality control no longer are sufficient to protect us from communication network collapses, fatalities from medical machinery malfunction, rocket guidance failure, or a half-billion dollar commercial loss due to incorrect arithmetic in a popular integrated circuit. These sensational examples are only the headline cases. Behind them are multitudes of mundane programs whose failures merely infuriate their users and cause increased costs to their producers. A source of such problems is the growth in program complexity. The more a program controls, the more types of interactions it supports. For example, the telephone “call-forwarding ” service (forwarding incoming calls to a customer-designated number) interacts with the “billing ” program that must determine whether the forwarding number or the calling number gets charged for the additional connection to the customer-designated number. At the same time, call-forwarding interacts with the “connection ” program that deals with the issue of
390|Model-checking for Probabilistic Real-time Systems|Model-checking is a method of verifying concurrent systems in which a state-graph model of the system behavior is compared with a temporal logic formula. This paper extends modelchecking to stochastic real-time systems, whose behavior depends on probabilistic choice and quantitative time. The specification language is TCTL, a branching-time temporal logic for expressing real-time properties. We interpret the formulas of the logic over generalized semiMarkov processes. Our model can express constraints like &#034;the delay between the request and the response is distributed uniformly between 2 to 4 seconds&#034;. We present an algorithm that combines model-checking for real-time non-probabilistic systems with model-checking for finite-state discrete-time Markov chains. The correctness of the algorithm is not obvious, because it analyzes the projection of a Markov process onto a finite state space. The projection process is not Markov, so our most significant result is that the model-checking algo...
391|Liveness in Timed and Untimed Systems|When proving the correctness of algorithms in distributed systems, one generally considers safety conditions and liveness conditions. The Input/Output (I/O) automaton model and its timed version have been used successfully, but have focused on safety conditions and on a restricted form of liveness called fairness. In this paper we develop a new I/O automaton model, and a new timed I/O automaton model, that permit the verification of general liveness properties on the basis of existing verification techniques. Our models include a notion of environment-freedom which generalizes the idea of receptiveness of other existing formalisms, and enables the use of compositional verification techniques.
392|Time-Constrained Automata|) Michael Merritt  AT&amp;T Bell Laboratories 600 Mountain Avenue Murray Hill, NJ 07974 merritt@research.att.com  Francesmary Modugno  School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 fmm@cs.cmu.edu  Mark R. Tuttle  DEC Cambridge Research Lab One Kendall Sq., Bldg. 700 Cambridge, MA 02139 tuttle@crl.dec.com Abstract  In this paper, we augment the input-output automaton model in order to reason about time in concurrent systems, and we prove simple properties of this augmentation. The input-output automata model is a useful model for reasoning about computation in concurrent and distributed systems because it allows fundamental properties such as fairness and compositionality to be expressed easily and naturally. A unique property of the model is that systems are modeled as the composition of  autonomous components. This paper describes a way to add a notion of time to the model in a way that preserves these properties. The result is a simple, compositional model fo...
393|Modeling Urgency in Timed Systems|this paper and is the object of an ongoing work. Here, we restrict our attention to 1-safe TPNs.
394|Modularity for Timed and Hybrid Systems| In a trace-based world, the modular specification, verification,  and control of live systems require each module to be receptive;  that is, each module must be able to meet its liveness assumptions no  matter how the other modules behave. In a real-time world, liveness is  automatically present in the form of diverging time. The receptiveness  condition, then, translates to the requirement that a module must be  able to let time diverge no matter how the environment behaves. We  study the receptiveness condition for real-time systems by extending the  model of reactive modules to timed and hybrid modules. We define the  receptiveness of such a module as the existence of a winning strategy in  a game of the module against its environment. By solving the game on  region graphs, we present an (optimal) Exptime algorithm for checking  the receptiveness of propositional timed modules. By giving a fixpoint  characterization of the game, we present a symbolic procedure for checking  the re...
395|Efficient Timed Reachability Analysis using Clock Difference Diagrams|One of the major problems in applying automatic verification tools to industrial-size systems is the excessive amount of memory required during the state-space exploration of a model. In the setting of real-time, this problem of state-explosion requires extra attention as information must be kept not only on the discrete control structure but also on the values of continuous clock variables. In this
396|A Kleene Theorem for Timed Automata|In this paper we define timed regular expressions, an extension of regular expressions for specifying sets of densetime discrete-valued signals. We show that this formalism is equivalent in expressive power to the timed automata of Alur and Dill by providing a translation procedure from expressions to automata and vice versa. The result is extended to !-regular expressions (B uchi&#039;s theorem). 1. Introduction  Timed automata, i.e. automata equipped with clocks [AD94], have been studied extensively in recent years as they provide a rigorous model for reasoning about the quantitative temporal aspects of systems. Together with realtime logics and process algebras they constitute the underlying theoretical basis for the specification and verification of real-time systems. Kleene&#039;s theorem [K56], stating that the regular (or rational) subsets of \Sigma    are exactly the recognizable ones (those accepted by finite automata), is one of the cornerstones of automata theory. No such theorem has ...
397|Some progress in the symbolic verification of timed automata| In this paper we discuss the practical difficulty of analyzing the behavior of timed automata and report some results obtained using an experimental bdd-based extension of kronos. We have treated examples originating from timing analysis of asynchronous boolean networks and CMOS circuits with delay uncertainties and the results outperform those obtained by previous implementations of timed automata verification tools. 
398|Timing Verification by Successive Approximation|We present an algorithm for verifying that a model M with timing constraints satisfies a given temporal property T . The model M is given as a parallel composition of !-automata P i , where each automaton P i is constrained by bounds on delays. The property T is given as an !-automaton as well, and the verification problem is posed as a language inclusion question L(M ) ` L(T ). In constructing the composition M of the constrained automata P i , one needs to rule out the behaviors that are inconsistent with the delay bounds, and this step is (provably) computationally expensive. We propose an iterative solution which involves generating successive approximations M j to M , with containment L(M ) ` L(M j ) and monotone convergence L(M j ) ! L(M ) within a bounded number of steps. As the succession progresses, the approximations M j become more complex. At any step of the iteration one may get a proof or a counterexample to the original language inclusion question. The described algori...
399|It’s about Time: Real-time Logics Reviewed| We summarize and reorganize some of the last decade&#039;s research on real-time extensions of temporal logic. Our main focus is on tableau constructions for model checking linear temporal formulas with timing constraints. In particular, we find that a great deal of real-time verification can be performed in polynomial space, but also that considerable care must be exercised in order to keep the real-time verification problem in polynomial space, or even decidable.
400|The Regular Real-Time Languages|. A specification formalism for reactive systems defines a class of !-languages. We call a specification formalism fully decidable if it is constructively closed under boolean operations and has a decidable satisfiability (nonemptiness) problem. There are two important, robust classes of !-languages that are definable by fully decidable formalisms. The !-regular languages are definable by finite automata, or equivalently, by the Sequential Calculus. The counter-free !-regular languages are definable by temporal logic, or equivalently, by the first-order fragment of the Sequential Calculus. The gap between both classes can be closed by finite counting (using automata connectives), or equivalently, by projection (existential second-order quantification over letters).  A specification formalism for real-time systems defines a class of timed !-languages, whose letters have real-numbered time stamps. Two popular ways of specifying timing constraints rely on the use of clocks, and on the use...
401|Timing Analysis in COSPAN|. We describe how to model and verify real-time systems using the formal verification tool Cospan. The verifier supports automatatheoretic verification of coordinating processes with timing constraints. We discuss different heuristics, and our experiences with the tool for certain benchmark problems appearing in the verification literature. 1 Introduction  Model checking is a method of automatically verifying concurrent systems in which a finite-state model of a system is compared with a correctness requirement. This method has been shown to be very effective in detecting errors in high-level designs, and has been implemented in various tools. We consider the tool Cospan that is based on the theory of !-automata (!-automata are finite automata accepting infinite sequences, see [Tho90] for a survey, and [VW86, Kur94] for applications to verification). The system to be verified is modeled as a collection of coordinating processes described in the language S/R [Kur94]. The semantics of su...
402|Time Abstracted Bisimulation: Implicit Specifications and Decidability|ed Bisimulation: Implicit Specifications and Decidability   Kim G. Larsen  y  and Yi Wang  z January 8, 1997 Abstract  In the last few years a number of real--time process calculi have emerged with the purpose of capturing important quantitative aspects of real--time systems. In addition, a number of process equivalences sensitive to time--quantities have been proposed, among these the notion of timed (bisimulation) equivalence in In this paper, we introduce a time--abstracting (bisimulation) equivalence, and investigate its properties with respect to the real--time process calculus of [Wan90]. Seemingly, such an equivalence would yield very little information (if any) about the timing properties of a process. However, time--abstracted reasoning about a composite process may yield important information about the relative timing--properties of the components of the system. In fact, we show as a main theorem that such implicit reasoning will reveal all timing aspects of a process. More p...
403|Verifying Abstractions of Timed Systems|ions of Timed Systems  Serdar Ta¸siran  ?  Rajeev Alur  ??  Robert P. Kurshan  ??  Robert K. Brayton  ?  Abstract. Given two descriptions of a real-time system at different levels of abstraction, we consider the problem of proving that the refined representation is a correct implementation of the abstract one. To avoid the complexity of building a representation for the refined system in its entirety, we develop a compositional framework for the implementation check to be carried out in a module-by-module manner using assumeguarantee style proof rules. On the algorithmic side, we show that the problem of checking for timed simulation relations, a sufficient condition for correct implementation, is decidable. We study state homomorphisms as a way of specifying a correspondence between two modules. We present an algorithm for checking if a given mapping is a homomorphism preserving timed behaviors. We have implemented this check in the verifier  Cospan, and applied our method to the comp...
404|The Observational Power of Clocks|We develop a theory of equivalences for timed systems. Two systems are equivalent iff external observers cannot observe differences in their behavior. The notion of equivalence depends, therefore, on the distinguishing power of the observers. The power of an observer to measure time results in untimed, clock, and timed equivalences: an untimed observer cannot measure the time difference between events; a clock observer uses a clock to measure time differences with finite precision; a timed observer is able to measure time differences with arbitrary precision. We show that the distinguishing power of clock observers grows with the number of observers, and approaches, in the limit, the distinguishing power of a timed observer. More precisely, given any equivalence for untimed systems, two timed systems are k-clock congruent, for a nonnegative integer k, iff their compositions with every environment that uses k clocks are untimed equivalent. Both k-clock bisimulation congruence and k-cloc...
406|Analysis of Timed Systems Based on Time-Abstracting Bisimulations|. We adapt a generic minimal model generation algorithm to compute the coarsest finite model of the underlying infinite transition system of a timed automaton. This model is minimal modulo a timeabstracting bisimulation. Our algorithm uses a refinement method that avoids set complementation, and is considerably more efficient than previous ones. We use the constructed minimal model for verification purposes by defining abstraction criteria that allow to further reduce the model and to compare it to a specification. 1 Introduction  Behavioral equivalences based on bisimulation relations have proven useful for verifying the correctness of concurrent systems. They allow comparing an implementation to a usually more abstract specification both represented as labeled transition systems. This approach also allows reducing the size of the system by identifying equivalent states which is crucial to avoid the explosion of the state-space. Since the introduction of strong bisimulation in [Mil80]...
407|Model Checking of Real-Time Systems: A Telecommunications Application|We describe the application of model checking tools to analyze a real-time software challenge in the design of Lucent Technologies&#039; 5ESS telephone switching system. We use two tools: COSPAN for checking real-time properties, and TPWB for checking probabilistic specifications. We report on the feedback given by the tools, and based on our experience, discuss the advantages and the limitations of the approach used.
408|STARI: A Case Study in Compositional and Hierarchical Timing Verification|. In [TAKB96], we investigated techniques for checking if one real-time system correctly implements another and developed theory for hierarchical proofs and assume-guarantee style reasoning. In this study, using the techniques of [TAKB96], we verify the correctness of the timing of the communication chip STARI.  1 Introduction  We describe the application of the techniques and tools described in [TAKB96] to the verification of the high-bandwidth communication chip, STARI [G93]. STARI (by Greenstreet, [G93, G96]) is a self-timed FIFO that interfaces a transmitter and a receiver that operate at the same clock frequency but may have some skew between their clock signals (Figure 1). STARI can compensate for large, time varying skews and makes high bandwidth synchronous operation possible by eliminating the need for handshakes between the transmitter and the receiver. However, because there are no handshakes, certain timing properties need to be verified to show that the interface functions...
409|State Minimization for Concurrent System Analysis Based on State Space Exploration|A fundamental issue in the automated analysis of concurrent systems is the efficient generation of the reachable state space. Since it is not possible to explore all the reachable states of a system if the number of states is very large or infinite, we need to develop techniques for minimizing the state space. This paper presents our approach to cluster subsets of states into equivalent classes. We assume that concurrent systems are specified as communicating state machines with arbitrary data space. We describe a procedure for constructing a minimal reachability state graph from communicating state machines. As an illustration of our approach, we analyze a producer-consumer program written in Ada. 1 Introduction  One of the most prohibitive barriers in automatic analysis based on state space exploration of a concurrent system is state explosion [4, 13]. Two major sources of state explosion are process composition and data space size. The state space of a system grows exponentially wit...
410|Using Versions in Update Transactions: Application to Integrity Checking|This paper proposes an extension of the multiversion two phase locking protocol, called EMV2PL, which enables update transactions to use versions while guaranteeing the serializability of all transactions. The use of the protocol is restricted to transactions, called write-then-read transactions that consist of two consecutive parts: a write part containing both read and write operations in some arbitrary order, and an abusively called read part, containing read operations or write operations on data items already locked in the write part of the transaction. With EMV2PL, read operations in the read part use versions and read locks acquired in the write part can be released just before entering the read part. We prove the correctness of our protocol, and show that its implementation requires very few changes to classical implementations of MV2PL. After presenting various methods used by application developers to implement integrity checking, we show how EMV2PL can be effectively used to...
411|A critique of ANSI SQL isolation levels|Reads, and Phantoms. This paper shows that these phenomena and the ANSI SQL definitions fail to properly characterize several popular isolation levels, including the standard Ioeking implementations of the levels covered. Ambiguity in the statement of the phenomena is investigated and a more formal statement is arrived at; in addition new phenomena that better characterize isolation types are introduced. Finally, an important multiversion isolation type, called Snapshot Isolation, is defined. 1.
412|Implementing atomic actions on decentralized data|Synchronization of accesses to shared data and recovering the state of such data in the case of failures are really two aspects of the same problem--implementing atomic actions on a related set of data items. In this paper a mechanism that solves both problems simultaneously in a way that is compatible with requirements of decentralized systems is described. In particular, the correct construction and execution of a new atomic action can be accomplished without knowledge of all other atomic actions in the system that might execute concurrently. Further, the mechanisms degrade gracefully if parts of the system fail: only those atomic actions that require resources in failed parts of the system are prevented from executing, and there is no single coordinator that can fail and bring down the whole system.
413|Transaction Chopping: Algorithms and Performances Studies|Chopping transactions into pieces is good for performance but may lead to non-serializable executions. Many researchers have reacted to this fact by either inventing new concurrency control mechanisms, weakening serializability, or both. We adopt a different approach. We assume a user who -- has access only to user-level tools such as (i) choosing between degree 2 and degree 3 isolation levels (degree 2 corresponds to level 1 in the new ANSI SQL terminology) (ii) the ability to execute a portion of a transaction using multiversion read consistency, and (iii) the ability to reorder the instructions in transaction programs; and -- knows the set of transactions that may run during a certain interval (users are likely to have such knowledge for online or real-time transactional applications).
414|Commit LSN: A Novel and Simple Method for Reducing Locking and Latching in Transaction Processing Systems|mnha/vr~lhm.com Abstract This paper presents a novel and simple method, called CommitYLSN, for determining if a piece of data is in the commltted state in a transaction pro-cessing system. This method is a much cheaper alter-native to the locking approach used by the prior art for this purpose. The method takes advantage of the concept of a log sequence number (LSN). In many systems. an LSN is recorded in each page of the data base to relate the state of the page to the log of update actions for that page. Our method uses information about the LSN of the first log record (call it Commit-LSN) of the oldest update transaction still executing in the system to infer that all the updates in pages with page_LSN less than Commit LSN have been committed. This reduces locking and latching. In addition. the method may also increase the level of concurrency that could be supported. The Commit LSN method makes it possible to use fine-granulazty locking without unduly penalizing transactions which read numerous records. It also benefits update transactions by reducing the cost of fine-granularity lock-ing when contention is not present for data on a page. We discuss in detail many applications of this method and illustrate its potential benefits for various environ-ments. In order to apply the Commit-LSN method, ex-tensions are also proposed for those systems in which (1) LSNs are not associated with pages (AS1400, ’ SQLIDS, System R), (2) LSNs are used only partially (IMS), and/or (3) not all objects ’ changes are logged (AS1400, SQL/DS. System R). 1.
415|String theory and noncommutative geometry|We extend earlier ideas about the appearance of noncommutative geometry in string theory with a nonzero B-field. We identify a limit in which the entire string dynamics is described by a minimally coupled (supersymmetric) gauge theory on a noncommutative space, and discuss the corrections away from this limit. Our analysis leads us to an equivalence between ordinary gauge fields and noncommutative gauge fields, which is realized by a change of variables that can be described explicitly. This change of variables is checked by comparing the ordinary Dirac-Born-Infeld theory with its noncommutative counterpart. We obtain a new perspective on noncommutative gauge theory on a torus, its T-duality, and Morita equivalence. We also discuss the D0/D4 system, the relation to M-theory in DLCQ, and a possible noncommutative version of the six-dimensional (2, 0) theory. 8/99
417|A path integral approach to the Kontsevich quantization formula |Abstract. We give a quantum field theory interpretation of Kontsevich’s deformation quantization formula for Poisson manifolds. We show that it is given by the perturbative expansion of the path integral of a simple topological bosonic open string theory. Its Batalin–Vilkovisky quantization yields a superconformal field theory. The associativity of the star product, and more generally the formality conjecture can then be understood by field theory methods. As an application, we compute the center of the deformed algebra in terms of the center of the Poisson algebra. 1.
418|D-branes and the noncommutative torus|We show that in certain superstring compactifications, gauge theories on noncommutative tori will naturally appear as D-brane world-volume theories. This gives strong evidence that they are well-defined quantum theories. It also gives a physical derivation of the identification proposed by Connes, Douglas and Schwarz of Matrix theory compactification on the noncommutative torus with M theory compactification with constant background three-form tensor field. November 1997It is often stated that in toroidal compactification in superstring theory, there is a minimum radius for the torus, the string length. Tori with smaller radii can always be related to tori with larger radii by using T-duality. However, this picture is not correct in the presence of other background fields, as is clear from the following simple example. Consider compactification on T 2 with a constant Neveu-Schwarz two-form field B. We quote the standard result [1]: under simultaneous T-duality of all coordinates, the combination (G + B)ij is inverted, where G is the metric expressed in string units. Consider a square torus and take the limit R1 = R2 ? 0; B ? = 0 fixed. (1) The T-dual torus has G + B =
419|D-branes and deformation quantization|In this note we explain how world-volume geometries of D-branes can be reconstructed within the microscopic framework where D-branes are described through boundary conformal field theory. We extract the (non-commutative) world-volume algebras from the operator product expansions of open string vertex operators. For branes in a flat background with constant non-vanishing B-field, the operator products are computed perturbatively to all orders in the field strength. The resulting series coincides with Kontsevich’s presentation of the Moyal product. After extending these considerations to fermionic fields we conclude with some remarks on the generalization of our approach to curved backgrounds.
420|The tensor Goldstone multiplet for partially broken supersymmetry,” Phys|The partial spontaneous breaking of rigid N = 2 supersymmetry implies the existence of a massless N = 1 Goldstone multiplet. In this paper we show that the spin-(1/2, 1) Maxwell multiplet can play this role. We construct its full nonlinear transformation law and find the invariant Goldstone action. The spin-1 piece of the action turns out to be of Born-Infeld type, and the full superfield action is duality invariant. This leads us to conclude that the Goldstone multiplet can be associated with a D-brane solution of superstring theory for p = 3. In addition, we find that N = 1 chirality is preserved in the presence of the Goldstone-Maxwell multiplet. This allows us to couple it to N = 1 chiral and gauge field multiplets. We find that arbitrary Kähler and superpotentials are consistent with partially broken N = 2 supersymmetry. 1 To the memory of Viktor I. Ogievetsky 1 Introduction. The spontaneous breaking of rigid supersymmetry gives rise to a massless spin-1/2 Goldstone field, ?a(x) [1]. When N = 2 supersymmetry is broken to N = 1, the Goldstone fermion
421|Quantum Field Theory on Noncommutative Space-times and the |We study properties of a scalar quantum field theory on two-dimensional noncommutative space-times. Contrary to the common belief that noncommutativity of space-time would be a key to remove the ultraviolet divergences, we show that field theories on a noncommutative plane with the most natural Heisenberg-like commutation relations among coordinates or even on a noncommutative quantum plane with Eq(2)-symmetry have ultraviolet divergences, while the theory on a noncommutative cylinder is ultraviolet finite. Thus, ultraviolet behaviour of a field theory on noncommutative spaces is sensitive to the topology of the space-time, namely to its compactness. We present general arguments for the case of higher space-time dimensions and as well discuss the symmetry transformations of physical states on noncommutative space-times.
422|Nonlocal field theories and the noncommutative torus,” Phys|We argue that by taking a limit of SYM on a non-commutative torus one can obtain a theory on non-compact space with a finite non-locality scale. We also suggest that one can also obtain a similar generalization of the (2,0) field theory in 5+1 dimensions, and that the DLCQ of this theory is known. February
423|Nonrelativistic fermions, coadjoint orbits of W8 and string field theory at c = 1|We apply the method of coadjoint orbits of W8-algebra to the problem of non-relativistic fermions in one dimension. This leads to a geometric formulation of the quantum theory in terms of the quantum phase space distribution of the fermi fluid. The action has an infinite series expansion in the string coupling, which to leading order reduces to the previously discussed geometric action for the classical fermi fluid based on the group w 8 of area-preserving diffeomorphisms. We briefly discuss the strong coupling limit of the string theory which, unlike the weak coupling regime, does not seem to admit of a two dimensional space-time picture. Our methods are equally applicable to interacting fermions in one dimension.
424|Matrix theory on noncommutative torus|We consider the compactification of Matrix theory on tori with background antisymmetric tensor field. Douglas and Hull have recently discussed how noncommutative geometry appears on the tori. In this paper, we demonstrate the concrete construction of this compactification of Matrix theory in a similar way to that previously given by Taylor. March
425|Noncommutative Geometry and Spacetime Gauge Symmetries of String Theory, Chaos Solitons Fractals 10|ar
426|World Volume Noncommutativity versus Target Space Noncommutativity,” JHEP 9903|It is known that the noncommutativity of D-brane coordinate is responsible for describing the higher-dimensional D-branes in terms of more fundamental ones such as D-particles or D-instantons, while considering a noncommutative torus as a target space is conjectured to be equivalent to introducing the background antisymmetric tensor field in matrix models. In the present paper we clarify the dual nature of both descriptions. Namely the noncommutativity of conjugate momenta of the D-brane coordinates realizes the target space structure, whereas noncommutativity of the coordinates themselves realizes world volume structure. We explicitly construct a boundary state for the Dirichlet boundary condition where the string boundary is adhered to the D-brane on the noncommutative torus. There are non-trivial relations between the parameters appeared in the algebra of the coordinates and that of the momenta.
427|Basic concepts and taxonomy of dependable and secure computing| This paper gives the main definitions relating to dependability, a generic concept including as special case such attributes as reliability, availability, safety, integrity, maintainability, etc. Security brings in concerns for confidentiality, in addition to availability and integrity. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability and security (faults, errors, failures), their attributes, and the means for their achievement (fault prevention, fault tolerance, fault removal, fault forecasting). The aim is to explicate a set of general concepts, of relevance across a wide range of situations and, therefore, helping communication and cooperation among a number of scientific and technical communities, including ones that are concentrating on particular types of system, of system failures, or of causes of system failures.
428|On the Criteria To Be Used in Decomposing Systems into Modules|This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a “modularization ” is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.
429|The Byzantine Generals Problem|Reliable computer systems must handle malfunctioning components that give conflicting information to different parts of the system. This situation can be expressed abstractly in terms of a group of generals of the Byzantine army camped with their troops around an enemy city. Communicating only by messenger, the generals must agree upon a common battle plan. However, one of more of them may be traitors who will try to confuse the others. The problem is to find an algorithm to ensure that the loyal generals will reach agreement. It is shown that, using only oral messages, this problem is solvable if and only if more than two-thirds of the generals are loyal; so a single traitor can confound two loyal generals. With unforgeable written messages, the problem is solvable for any number of generals and possible traitors. Applications of the solutions to reliable computer systems are then discussed.
430|Efficient dispersal of information for security, load balancing, and fault tolerance|Abstract. An Information Dispersal Algorithm (IDA) is developed that breaks a file F of length L =  ( F ( into n pieces F,, 1 5 i 5 n, each of length ( F, 1 = L/m, so that every m pieces suffice for reconstructing F. Dispersal and reconstruction are computationally efficient. The sum of the lengths ( F, 1 is (n/m). L. Since n/m can be chosen to be close to I, the IDA is space eflicient. IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communi-cations between processors in parallel computers. For the latter problem provably time-efftcient and highly fault-tolerant routing on the n-cube is achieved, using just constant size buffers. Categories and Subject Descriptors: E.4 [Coding and Information Theory]: nonsecret encoding schemes
431|Understanding Fault-Tolerant Distributed Systems|We propose a small number of basic concepts that can be used to explain the architecture of fault-tolerant distributed systems and we discuss a list of architectural issues that we find useful to consider when designing or examining such systems. For each issue we present known solutions and design alternatives, we discuss their relative merits and we give examples of systems which adopt one approach or the other. The aim is to introduce some order in the complex discipline of designing and understanding fault-tolerant distributed systems.  
432|Why Do Computers Stop And What Can Be Done About It?|An analysis of the failure statistics of a commercially available fault-tolerant system shows that administration and software are the major contributors to failure. Various approaches to software fault-tolerance are then discussed -- notably process-pairs, transactions and reliable storage. It is pointed out that faults in production software are often soft (transient) and that a transaction mechanism combined with persistent processpairs provides fault-tolerant execution -- the key to software fault-tolerance.
433|On evaluating the performability of degradable computing systems|Abstract-If the performance of a computing system is &#034;degradable,&#034; performance and reliability issues must be dealt with simultaneously in the process of evaluating system effectiveness. For this purpose, a unified measure, called &#034;performability, &#034; is introduced and the foundations of performability modeling and evaluation are established. A critical step in the modeling process is the introduction of a &#034;capability function &#034; which relates low-level system behavior to user-oriented performance levels. A hierarchical modeling scheme is used to formulate the capability function and capability is used, in turn, to evaluate performability. These techniques are then illustrated for a specific application: the performability evaluation of an aircraft computer in the environment of an air transport mission. Index Terms-Degradable computing systems, fault-tolerant computing, hierarchical modeling, performability evaluation, performance evaluation, reliability evaluation. I.
434|Survivable Network Systems: An Emerging Discipline|: Society is growing increasingly dependent upon large-scale,  highly distributed systems that operate in unbounded network  environments. Unbounded networks, such as the Internet, have no  central administrative control and no unified security policy. Furthermore,  the number and nature of the nodes connected to such networks cannot  be fully known. Despite the best efforts of security practitioners, no  amount of system hardening can assure that a system that is connected  to an unbounded network will be invulnerable to attack. The discipline of  survivability can help ensure that such systems can deliver essential  services and maintain essential properties such as integrity,  confidentiality, and performance, despite the presence of intrusions.  Unlike the traditional security measures that require central control or  administration, survivability is intended to address unbounded network  environments. This report describes the survivability approach to helping  assure that a syste...
435|Failure Mode Assumptions and Assumption Coverage|. A method is proposed for the formal analysis of failure mode assumptions and for the evaluation of the dependability of systems whose design correctness is conditioned on the validity of such assumptions. Formal definitions are given for the types of errors that can affect items of service delivered by a system or component. Failure mode assumptions are then formalized as assertions on the types of errors that a component may induce in its enclosing system. The concept of assumption coverage is introduced to relate the notion of partiallyordered assumption assertions to the quantification of system dependability. Assumption coverage is shown to be extremely important in systems requiring very high dependability. It is also shown that the need to increase system redundancy to accommodate more severe modes of component failure can sometimes result in a decrease in dependability. 1 Introduction and Overview  The definition of assumptions about the types of faults, the rate at which comp...
436|Experimenting with Quantitative Evaluation Tools for Monitoring Operational Security|: This paper presents the results of an experiment of security evaluation. The evaluation method used is based on previous work involving modeling the system as a privilege graph exhibiting the security vulnerabilities and on the computation of measures representing the difficulty for a possible attacker to exploit these vulnerabilities and defeat the security objectives of the system. A set of tools has been developed to compute such measures and has been experimented to monitor a large real system during more than a year. The experiment results are presented and the validity of the measures is discussed. Finally, the practical usefulness of such tools for operational security monitoring is shown and a comparison with other existing approaches is given. 1 Introduction  Security is an increasing worry for most computing system administrators: computing systems are more and more vital for most companies and organizations, while these systems are made more and more vulnerable by new user...
437|Verifying and validating software requirements and design specifications|This paper presents the following guideline information on verification and validation (V&amp;V) of software requirements and design specifications: Definitions of the terms &#034;verification &#034; and &#034;validation, &#034; and an explanation of their context in the software life-cycle; A description of the basic sequence of functions performed during software requirements and design V&amp;V An explanation, with examples: of the major software requirements and design V&amp;V criteria: completeness, consistency, feasibility, and testability; An evaluation of the relative cost and effectiveness of the major software requirements and design V&amp;V techniques with respect to the above criteria; An example V&amp;V checklist for software system reliability and availability. Based on the above, the paper provides recommendations of the combination of software requirements and design V&amp;V techniques most suitable for small, medium, and large software specifications. I. OBJECTIVES The basic objectives in verification and validation (V&amp;V) of software requirements and design specifications are to identify and resolve software problems and high-risk issues early in the software life-cycle. The main reason for doing this is indicated in Figure 1, (1). It shows that savings of up to 100:1 are possible by finding and fixing problems early rather than late in the life-cycle. Besides the major cost savings, there are also significant payoffs in improved
438|The Capability Maturity Model for Software|This paper provides an overview of the latest version of the Capability Maturity Model for Software, CMM v1.1. Based on over six years of experience with software process improvement and the contributions of hundreds of reviewers, CMM v1.1 describes the software engineering and management practices that characterize organizations as they mature their processes for developing and maintaining software. This paper stresses the need for a process maturity framework to prioritize improvement actions, describes the process maturity framework of five maturity levels and the associated structural components, and discusses future directions for the CMM.  Keywords: capability maturity model, CMM, process maturity framework, software process improvement, process capability, process performance, maturity level, key process area, software process assessment, software capability evaluation. 1  1 Introduction  After two decades of unfulfilled promises about productivity and quality gains from applyi...
439|Implementing Fault Tolerant Applications using Reflective Object-Oriented|This paper shows how refection and object-oriented programming can be used to ease the implementation of classical fault tolerance mechanisms in distributed applications. When the underlying runtime system does not provide fault tolerance transparently, classical approaches to implementing fault tolerance mechanisms ofren imply mixing functional programming with non-functional programming (e.g. error processing mechanisms). The use of reflection improves the transparency of fault tolerance mechanisms to the programmer and more generally provides a clearer separation between functional and non-functional programming. The implementations of some classical replication techniques using a reflective approach are presented in detail and illustrated by several examples, which have been prototyped on a network of Unix workstations. Lessons learnt from our experiments are drawn and future work is discussed. 1
440|Formal Methods and their Role in the Certification of Critical Systems|This report is based on one prepared as a chapter for the FAA Digital Systems Validation Handbook (a guide to assist FAA Certification Specialists with Advanced Technology Issues).  1  Its purpose is to explain the use of formal methods in the specification and verification of software and hardware requirements, designs, and implementations, to identify the benefits, weaknesses, and difficulties in applying these methods to digital systems used in critical applications, and to suggest factors for consideration when formal methods are offered in support of certification. The presentation concentrates on the rationale for formal methods and on their contribution to assurance for critical applications within a context such as that provided by DO-178B (the guidelines for software used on board civil aircraft)  2  ; it is intended as an introduction for those to whom these topics are new. A more technical discussion of formal methods is provided in a companion report.  3 1  Digital Systems ...
441|Self-repairing computers|Computer systems and their &#034;organs&#034;--microprocessors, applications and communications networks--are becoming ever more powerful. But they are also becoming ever more complex and therefore more susceptible to failure. As the costs of administration, oversight and downtime expand in response, scientists and engineers in the computer industry are working to enhance the dependability of their products. Significantly, many of their efforts aim to take humans (and the errors they inevitably engender) out of the loop.
442|Formal Specification and Verification of a Fault-Masking and Transient-Recovery Model for Digital Flight-Control Systems|We present a formal model for fault-masking and transient-recovery among the replicated computers of digital flight-control systems. We establish conditions under which majority voting causes the same commands to be sent to the actuators as those that would be sent by a single computer that suffers no failures. The model and its analysis have been subjected to formal specification and mechanically checked verification using the Ehdm system. Keywords: digital flight control systems, formal methods, formal specification and verification, proof checking, fault tolerance, transient faults, majority voting, modular redundancy  Contents 1 Introduction 1  1.1 Digital Flight-Control Systems : : : : : : : : : : : : : : : : : : : : : 2 1.2 Fault Tolerance for DFCS : : : : : : : : : : : : : : : : : : : : : : : : 3 1.3 Formal Models for DFCS : : : : : : : : : : : : : : : : : : : : : : : : 11 1.3.1 Overview of the Fault-Masking Model Employed : : : : : : : 12  2 The Fault-Masking Model 17  2.1 A M...
443|NonStop availability in a client/server environment”, Tandem|The popularity of client/server computing has grown enonnously in the last few years. Client/server architectures have become the standard campus LAN environment at most companies, and client/server architectures are beginning to be used for mission-critical applications. Because of this computing paradigm shift, client/server availability has become a very important issue. This paper presents a predictive model of client/server availability applied to a representative client/server environment. The model is based on client/server outage data and measures client/server availability using a new metric: user outage minutes, which measures the amount of downtime for each user. User outage minutes are calculated for all outage types: physical, design, operations, and environmental failures, as well as outages due to planned reconfigurations. The model includes outages due to all client, server, and network devices in a typical client/server environment. The model has been validated by showing that its results are very similar to downtime surveys and other outage reports in the literature. The major results from the model are: • Each client in today&#039;s mission-critical client/server environment experiences an
444|Managing Update Conflicts in Bayou, a Weakly Connected Replicated Storage System|Bayou is a replicated, weakly consistent storage system designed for a mobile computing environment that includes portable machines with less than ideal network connectivity. To maximize availability, users can read and write any accessible replica. Bayou&#039;s design has focused on supporting apphcation-specific mechanisms to detect and resolve the update conflicts that naturally arise in such a system, ensuring that replicas move towards eventual consistency, and defining a protocol by which the resolution of update conflicts stabilizes. It includes novel methods for conflict detection, called dependency checks, and per-write conflict resolution based on client-provided merge procedures. To guarantee eventual consistency, Bayou servers must be able to rollback the effects of previously executed writes and redo them according to a global senalization order. Furthermore, Bayou permits clients to observe the results of all writes received by a server, Including tentative writes whose conflicts have not been ultimately resolved. This paper presents the motivation for and design of these mechanisms and describes the experiences gained with an initial implementation of the system.
445|Time, Clocks, and the Ordering of Events in a Distributed System|The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.  
446|Disconnected Operation in the Coda File System|Disconnected operation is a mode of operation that enables a client to continue accessing critical data during temporary failures of a shared data repository. An important, though not exclusive, application of disconnected operation is in supporting portable computers. In this paper, we show that disconnected operation is feasible, efficient and usable by describing its design and implementation in the Coda File System. The central idea behind our work is that caching of data, now widely used for performance, can also be exploited to improve availability.
447|Coda: A Highly available File System for a Distributed Workstation Environment|Abstract- Coda is a file system for a large-scale distributed computing environment composed of Unix workstations. It provides resiliency to server and network failures through the use of two distinct but complementary mechanisms. One mechanism, server replication, stores copies of a file at multiple servers. The other mechanism, disconnected operation, is a mode of execution in which a caching site temporarily assumes the role of a replication site. Disconnected operation is particularly useful for supporting portable workstations. The design of Coda optimizes for availability and performance, and strives to provide the highest degree of consistency attainable in the light of these objectives. Measurements from a prototype show that the performance cost of providing high availability in Coda is reasonable. Index Terms- Andrew, availability, caching, disconnected operation, distributed file system, performance, portable computers, scalability, server replication. I.
448|Concurrency Control in Groupware Systems|Abstract. Groupware systems are computer-based systems that support two or more users engaged in a common task, and that provide an interface to a shared environment. These systems frequently require fine-granularity sharing of data and fast response times. This paper distinguishes real-time groupware systems from other multi-user systems and dis-cusses their concurrency control requirements. An algorithm for concurrency control in real-time groupware systems is then presented. The advantages of this algorithm are its sim-plicity of use and its responsiveness: users can operate di-rectly on the data without obtaining locks. The algorithm must know some semantics of the operations. However the algorithm’s overall structure is independent of the semantic information, allowing the algorithm to be adapted to many situations. An example application of the algorithm to group text editing is given, along with a sketch of its proof of cor-rectness in this particular case. We note that the behavior desired in many of these systems is non-serializable. 1.
449|The locus distributed operating system|LOCUS Is a distributed operating system which supports transparent access to data through a network wide fllesystem, permits automatic replication of storaget supports transparent distributed process execution, supplies a number of high reliability functions such as nested transactions, and is upward compatible with Unix. Partitioned operation of subnetl and their dynamic merge is also supported. The system has been operational for about two years at UCLA and extensive experience In its use has been obtained. The complete system architecture is outlined in this paper, and that experience is summarized. 1
450|Session Guarantees for Weakly Consistent Replicated Data|Four per-session guarantees are proposed to aid users and applications of weakly consistent replicated data: Read Your Writes, Monotonic Reads, Writes Follow Reads, and Monotonic Writes. The intent is to present individual applications with a view of the database that is consistent with their own actions, even if they read and write from various, potentially inconsistent servers. The guarantees can be layered on existing systems that employ a read-any/ write-any replication scheme while retaining the principal benefits of such a scheme, namely high-availability, simplicity, scalability, and support for disconnected operation. These session guarantees were developed in the context of the Bayou project at Xerox PARC in which we are designing and building a replicated storage system to support the needs of mobile computing users who may be only intermittently connected.
451|Mobile Wireless Computing: Challenges in Data Management|Mobile computing is a new emerging computing paradigm posing many challenging data management problems. We identify these new challenges and investigate their technical significance. New research problems include management of location dependent data, information services to mobile users, frequent disconnections, wireless data broadcasting, and energy efficient data access. 1 Introduction  The rapidly expanding technology of cellular communications, wireless LAN, and satellite services will make it possible for mobile users to access information anywhere and at anytime. In the near future, tens of millions of users will be carrying a portable computer, often called a personal digital assistant or a personal communicator. Smaller units will run on AA batteries and may be diskless; larger units will run on Ni-Cd packs. These larger units will be powerful laptop computers with large memories and powerful processors. Regardless of size, all mobile computers will be equipped with a wireless...
452|Providing High Availability Using Lazy Replication|To provide high availability for services such as mail or bulletin boards, data must be replicated. One way to guarantee consistency of replicated data is to force service operations to occur in the same order at all sites, but this approach is expensive. For some applications a weaker causal operation order can preserve consistency while providing better performance. This paper describes a new way of implementing causal operations. Our technique also supports two other kinds of operations: operations that are totally ordered with respect to one another, and operations that are totally ordered with respect to all other operations. The method performs well in terms of response time, operation processing capacity, amount of stored state, and number and size of messages; it does better than replication methods based on reliable multicast techniques. This research was supported in part by the National Science Foundation under Grant CCR-8822158 and in part by the Advanced Research Projects ...
453|Implementation of the Ficus Replicated File System|As we approach nation-wide integration of computer systems, it is clear that file replication will play a key role, both to improve data availability in the face of failures, and to improve performance by locating data near where it will be used. We expect that future file systems will have an extensible, modular structure in which features such as replication can be &#034;slipped in&#034; as a transparent layer in a stackable layered architecture. We introduce the Ficus replicated file system for NFS and show how it is layered on top of existing file systems. The Ficus file system differs from previous file replication services in that it permits update during network partition if any copy of a file is accessible. File and directory updates are automatically propagated to accessible replicas. Conflicting updates to directories are detected and automatically repaired; conflicting updates to ordinary files are detected and reported to the owner. The frequency of communications outages rendering i...
454|Flexible and Safe Resolution of File Conflicts|In this paper we describe the support provided by the Coda File System for transparent resolution of conflicts arising from concurrent updates to a file in different network partitions. Such partitions often occur in mobile computing environments. Coda provides a framework for invoking customized pieces of code called application-specific resolvers (asrs) that encapsulate the knowledge needed for file resolution. If resolution succeeds, the user notices nothing more than a slight performance delay. Only if resolution fails does the user have to resort to manual repair. Our design combines a rule-based approach to ASR selection with  transactional encapsulation of ASR execution. This  paper shows how such an approach leads to flexible  and efficient file resolution without loss of security or robustness.
455|Database System Issues in Nomadic Computing|Mobile computers and wireless networks are emerging technologies that will soon be available to a  wide variety of computer users. Unlike earlier generations of laptop computers, the new generation  of mobile computers can be an integrated part of a distributed computing environment, one in  which users change physical location frequently. The result is a new computing paradigm, nomadic  computing. This paradigm will affect the design of much of our current systems software, including  that of database systems.  This paper discusses in some detail the impact of nomadic computing on a number of traditional  database system concepts. In particular, we point out how the reliance on short-lived batteries  changes the cost assumptions underlying query processing. In these systems, power consumption  competes with resource utilization in the definition of cost metrics. We also discuss how the likelihood  of temporary disconnection forces consideration of alternative transaction processing pr...
456|A Weak-Consistency Architecture for Distributed Information Services|services
457|Logbased directory resolution in the Coda file system|optimistic replicltion is m imparrrnt technique for achieving high avdability in distributed file systans. A key problem m optimistic rephtim is using d c bwledge of objects to resolve co&#034;mt updates from multiple partitions. In this paper, we describe how the Coda File System resolves pllrtitionsd updareg to dirCCtOIkS. The CUUd Idt Of Our Work is th.t &amp;g&amp;g Of updates is a simple yet efficient and powerful teclmique for directory resolution m Unix file systems. Mersurementr from our implementatkm show that the time for resolution is typically less than 1096 of the time for paforming the original set of partitioned updates. Analysis based on file traces fnnn our envinm &#034; indicate that a log size of 2 MB per hour of padtion should be ample for typical smers. 1.
458|A Flexible Object Merging Framework|The need to merge different versions of an object to a common state arises in collaborative computing due to several reasons including optimistic concurrency control, asynchronous coupling, and absence of access control. We have developed a flexible object merging framework that allows definition of the merge policy based on the particular application and the context of the collaborative activity. It performs automatic, semi-automatic, and interactive merges, supports semanticsdetermined merges, operates on objects with arbitrary structure and semantics, and allows fine-grained specification of merge policies. It is based on an existing collaborative applications framework and consists of a merge matrix,which defines merge functions and their parameters and allows definition of multiple merge policies, and a merge algorithm, which performs the merge based on the results computed by the merge functions. In conjunction with our framework we introduce a set of merge policies for several u...
459|An Architecture for On-the-fly File Integrity Checking|There are several ways for an intruder to obtain access to a remote  computing system, such as exploiting program vulnerabilities, stealing  passwords, and so. The intruder can modify system utilities in order to hide  his/her presence and to guarantee an open backdoor to the system. Many  techniques have been proposed to detect unauthorized file modifications, but  they usually work off-line and thus detect file modifications only when the  system is already compromised. This paper presents an architecture to deal with  this kind of problem. Through the combined use of digital signature techniques  and system call interceptions, it allows for transparent on-the-fly integrity check  of files in Unix systems. Its evaluation in real-world situations validates the  approach, by showing overheads under 10% for most situations.
461|Handbook of Applied Cryptography|As we draw near to closing out the twentieth century, we see quite clearly that the information-processing and telecommunications revolutions now underway will continue vigorously into the twenty-first. We interact and transact by directing flocks of digital packets towards each other through cyberspace, carrying love notes, digital cash, and secret corporate documents. Our personal and economic lives rely more and more on our ability to let such ethereal carrier pigeons mediate at a distance what we used to do with face-to-face meetings, paper documents, and a firm handshake. Unfortunately, the technical wizardry enabling remote collaborations is founded on broadcasting everything as sequences of zeros and ones that one&#039;s own dog wouldn&#039;t recognize. What is to distinguish a digital dollar when it is as easily reproducible as the spoken word? How do we converse privately when every syllable is bounced off a satellite and smeared over an entire continent? How should a bank know that it really is Bill Gates requesting from his laptop in Fiji a transfer of $10,000,000,000 to another bank? Fortunately, the magical mathematics of cryptography can help. Cryptography provides techniques for keeping information secret, for determining that information
462|Exokernel: An Operating System Architecture for Application-Level Resource Management|We describe an operating system architecture that securely multiplexes machine resources while permitting an unprecedented degree of application-specific customization of traditional operating system abstractions. By abstracting physical hardware resources, traditional operating systems have significantly limited the performance, flexibility, and functionality of applications. The exokernel architecture removes these limitations by allowing untrusted software to implement traditional operating system abstractions entirely at application-level. We have implemented a prototype exokernel-based system that includes Aegis, an exokernel, and ExOS, an untrusted application-level operating system. Aegis defines the low-level interface to machine resources. Applications can allocate and use machine resources, efficiently handle events, and participate in resource revocation. Measurements show that most primitive Aegis operations are 10–100 times faster than Ultrix,a mature monolithic UNIX operating system. ExOS implements processes, virtual memory, and inter-process communication abstractions entirely within a library. Measurements show that ExOS’s application-level virtual memory and IPC primitives are 5–50 times faster than Ultrix’s primitives. These results demonstrate that the exokernel operating system design is practical and offers an excellent combination of performance and flexibility. 1
463|Interposition Agents: Transparently Interposing User Code at the System Interface|1.1. Terminology Many contemporary operating systems utilize a system Many contemporary operating systems provide an call interface between the operating system and its clients. interface between user code and the operating system Increasing numbers of systems are providing low-level services based on special &#034;system calls&#034;. One can view mechanisms for intercepting and handling system calls in the system interface as simply a special form of structured user code. Nonetheless, they typically provide no higher- communication channel on which messages are sent, level tools or abstractions for effectively utilizing these allowing such operations as interposing programs that mechanisms. Using them has typically required record or modify the communications that take place on reimplementation of a substantial portion of the system this channel. In this paper, such a program that both uses interface from scratch, making the use of such facilities and provides the system interface will be refe...
464|Linux security modules: General security support for the linux kernel|Symposium
465|SLIC: An extensibility system for commodity operating systems |Modern commodity operating systems are large and complex systems developed over many years by large teams of programmers, containing hundreds of thousands of lines of code. Consequently, it is extremely difficult to add significant new functionality to these systems. In response to this problem, a number of recent research projects have explored novel operating system architectures to support untrusted extensions, including SPIN, VINO, Exokernel, and Fluke. Unfortunately, these architectures require substantialimplementation effort and are not generally available in commodity systems. In contrast, by leveraging the technique of interposition, we have designed and implemented a prototype extension system called SLIC which requires only trivial operating system
466|A Layered Approach to File System Development|This paper discusses the stackable layers approach to file system design. With this approach, a file system is constructed from several layers, each implementing one portion of the file system well. Each layer is bounded above and below by an identical interface framework. The symmetry of the interface, coupled with run-time stack definition, make layer configuration flexible and facilitate experimentation and new file system development.  Addition of new file system functionality to existing environments often requires changes to current interfaces. To address this issue, stackable layers are joined by an extensible interface. Any layer can add to such an interface; existing layers continue to function without modification.  Stackable architectures benefit from new development techniques. This paper examines development methods unique to stackable systems, and concludes with an analysis of the performance of layered file systems.  1 Introduction  The utility of modular structures in s...
467|The SWISS-MODEL Workspace: A web-based environment for protein structure homology modelling|Motivation: Homology models of proteins are of great interest for planning and analyzing biological experiments when no experimental three-dimensional structures are available. Building homology models requires specialized programs and up-to-date sequence and structural databases. Integrating all required tools, programs and databases into a single web-based workspace facilitates access to homology modelling from a computer with web connection without the need of downloading and installing large program packages and databases. Results: SWISS-MODEL Workspace is a web-based integrated service dedicated to protein structure homology modelling. It assists and guides the user in building protein homology models at different levels of complexity. A personal working environment is provided for each user where several modelling projects can be carried out in parallel. Protein sequence and structure databases necessary for modelling are accessible from the workspace and are updated in regular intervals. Tools for template selection, model building, and structure quality evaluation can be invoked from within the workspace. Workflow and usage of the workspace are illustrated by modelling human Cyclin A1 and human Transmembrane Protease
468|Gapped Blast and PsiBlast: a new generation of protein database search programs|The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSIBLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.
469|Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features|structure
470|The Pfam protein families database|Pfam is a large collection of protein families and domains. Over the past 2 years the number of families in Pfam has doubled and now stands at 6190 (version 10.0). Methodology improvements for searching the Pfam collection locally as well as via the web are described. Other recent innovations include modelling of discontinuous domains allow-ing Pfam domain de®nitions to be closer to those found in structure databases. Pfam is available on the web in the UK
471|Database resources of the National Center for Biotechnology Information|In addition to maintaining the GenBankÒ nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI’s Web site. NCBI resources include Entrez,
473|Hidden Markov models for detecting remote protein homologies|A new hidden Markov model method (SAM-T98) for nding remote homologs of protein sequences is described and evaluated. The method begins with a single target sequence and iteratively builds a hidden Markov model (hmm) from the sequence and homologs found using the hmm for database search. SAM-T98 is also used to construct model libraries automatically from sequences in structural databases. We evaluate the SAM-T98 method with four datasets. Three of the test sets are fold-recognition tests, where the correct answers are determined by structural similarity. The fourth uses a curated database. The method is compared against wu-blastp and against double-blast, a two-step method similar to ISS, but using blast instead of fasta. Results SAM-T98 had the fewest errors in all tests| dramatically so for the fold-recognition tests. At the minimum-error point on the SCOP-domains test, SAM-T98 got 880 true positives and 68 false positives, double-blast got 533 true positives with 71 false positives, and wu-blastp got 353 true positives with 24 false positives. The method is optimized to recognize superfamilies, and would require parameter adjustment to be used to nd family or fold relationships. One key to the performance of the hmm method is a new score-normalization technique that compares the score to the score with a reversed model rather than to a uniform null model. Availability A World Wide Web server, as well as information on obtaining the Sequence Alignment and PREPRINT to appear in Bioinformatics, 1999
474|SWISSMODEL: an automated protein homology-modeling server|SWISS-MODEL
475|Twilight Zone of Protein Sequence Alignments|l findings are applicable to automatic database searches.  Keywords: alignment quality analysis/evolutionary conservation/ genome analysis/protein sequence alignment/sequence space hopping  Introduction  Protein sequence alignments in twilight zone  Protein sequences fold into unique three-dimensional (3D) structures. However, proteins with similar sequences adopt similar structures (Zuckerkandl and Pauling, 1965; Doolittle, 1981; Doolittle, 1986; Chothia and Lesk, 1986). Indeed, most protein pairs with more than 30 out of 100 identical residues were found to be structurally similar (Sander and Schneider, 1991). This high robustness of structures with respect to residue exchanges explains partly the robustness of organisms with respect to gene-replication errors, and it allows for the variety in evolution (Zuckerkandl and Pauling, 1965; Zuckerkandl, 1976; Doolittle, 1979, 1986). Structure alignments have uncovered homologous protein pairs with less than 10% pairwise sequence identity (
476|SCOP database in 2004: refinements integrate structure and sequence family data|The Structural Classication of Proteins (SCOP) database is a comprehensive ordering of all proteins of known structure, according to their evolutionary and structural relationships. Protein domains in SCOP are hierarchically classied into families, superfamilies, folds and classes. The continual accumulation of sequence and structural data allows more rigorous analysis and provides important information for understanding the protein world and its evolutionary repertoire. SCOP participates in a project that aims to rationalize and integrate the data on proteins held in several sequence and structure databases. As part of this project, starting with release 1.63, we have initiated a renement of the SCOP classication, which introduces a number of changes mostly at the levels below superfamily. The pending SCOP reclassication will be carried out gradually through a number of future releases. In addition to the expanded set of static links to external resources, available at the level of domain entries, we have started modernization of the interface capabilities of SCOP allowing more dynamic links with other databases. SCOP can be accessed at http://scop.mrc-lmb.cam.ac.uk/scop.
477|Hidden Markov models for sequence analysis: extension and analysis of the basic method|Hidden Markov models (HMMs) are a highly effective means of modeling a family of unaligned sequences or a common motif within a set of unaligned sequences. The trained HMM can then be used for discrimination or multiple alignment. The basic mathematical description of an HMM and its expectation-maximization training procedure is relatively straight-forward. In this paper, we review the mathematical extensions and heuristics that move the method from the theoretical to the practical. Then, we experimentally analyze the effectiveness of model regularization, dynamic model modification, and optimization strategies. Finally it is demonstrated on the SH2 domain how a domain can be found from unaligned sequences using a special model type. The experimental work was completed with the aid of the Sequence Alignment and Modeling software suite. 1 Introduction  Since their introduction to the computational biology community (Haussler et al., 1993; Krogh et al., 1994a), hidden Markov models (HMMs...
478|PDBsum more: new summaries and analyses of the known 3D structures of proteins and nucleic acids|PDBsum is a database of mainly pictorial summaries of the 3D structures of proteins and nucleic acids in the Protein Data Bank. Its pages aim to provide an at-aglance view of the contents of every 3D structure, plus detailed structural analyses of each protein chain, DNA–RNA chain and any bound ligands and metals. In the past year, the database has been significantly improved, in terms of both appearance and new content. Moreover, it has moved to its new address at
479|The Protein Data Bank and structural genomics|The Protein Data Bank (PDB;
480|Intrinsic disorder in cell-signaling and cancer-associated proteins|The dominating concept that protein structure determines protein function is undergoing Abbreviations used: CDK, cyclin-dependent kinase; eIF4E, translation initiation factor (eIF) 4F; EU_SW,
481|Protein distance constraints predicted by neural networks and probability density functions. Prot|3To whom correspondence should be addressed We predict interatomic Ca distances by two independent data driven methods. The first method uses statistically derived probability distributions of the pairwise distance between two amino acids, whilst the latter method consists of a neural network prediction approach equipped with windows taking the context of the two residues into account. These two methods are used to predict whether distances in independent test sets were above or below given thresholds. We investigate which distance thresholds produce the most information-rich constraints and, in turn, the optimal performance of the two methods. The predictions are based on a data set derived using a new threshold which defines when sequence similarity implies structural similarity. We show that distances in proteins are predicted more accurately by neural networks than by probability density functions. We show that the accuracy of the predictions can be further increased by using sequence profiles. A threading method based on the predicted distances is presented. A homepage with software, predictions and data related to this paper is available at
482|EVA: evaluation of protein structure prediction servers|EVA
483|Formal Ontology and Information Systems|Research on ontology is becoming increasingly widespread in the computer science community, and its importance is being recognized in a multiplicity of research fields and application areas, including knowledge engineering, database design and integration, information retrieval and extraction. We shall use the generic term information systems, in its broadest sense, to collectively refer to these application perspectives. We argue in this paper that so-called ontologies present their own methodological and architectural peculiarities: on the methodological side, their main peculiarity is the adoption of a highly interdisciplinary approach, while on the architectural side the most interesting aspect is the centrality of the role they can play in an information system, leading to the perspective of ontology-driven information systems.
484|A translation approach to portable ontology specifications|To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse — definitions of classes, relations, functions, and other objects — is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms. 
485|WordNet: A Lexical Database for English|Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet 1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].
486|Toward Principles for the Design of Ontologies Used for Knowledge Sharing|Recent work in Artificial Intelligence is exploring the use of formal ontologies as a way of specifying content-specific agreements for the sharing and reuse of knowledge among software entities. We take an engineering perspective on the development of such ontologies. Formal ontologies are viewed as designed artifacts, formulated for specific purposes and evaluated against objective design criteria. We describe the role of ontologies in supporting knowledge sharing activities, and then present a set of criteria to guide the development of ontologies for these purposes. We show how these criteria are applied in case studies from the design of ontologies for engineering mathematics and bibliographic data. Selected design decisions are discussed, and alternative representation choices and evaluated against the design criteria.
487|Ontologies: Principles, methods and applications|This paper is intended to serve as a comprehensive introduction to the emerging field concerned with the design and use of ontologies. We observe that disparate backgrounds, languages, tools, and techniques are a major barrier to effective communication among people, organisations, and/or software systems. We show how the development and implementation of an explicit account of a shared understanding (i.e. an `ontology&#039;) in a given subject area, can improve such communication, which in turn, can give rise to greater reuse and sharing, inter-operability, and more reliable software. After motivating their need, we clarify just what ontologies are and what purposes they serve. We outline a methodology for developing and evaluating ontologies, first discussing informal techniques, concerning such issues as scoping, handling ambiguity, reaching agreement and producing de nitions. We then consider the bene ts of and describe, a more formal approach. We re-visit the scoping phase, and discuss the role of formal languages and techniques in the specification, implementation and evaluation of ontologies. Finally, we review the state of the art and practice in this emerging field,
488|Ontologies and knowledge bases: Towards a terminological clarification|The word “ontology ” has recently gained a good popularity within the knowledge engineering community. However, its meaning tends to remain a bit vague, as the term is used in very different ways. Limiting our attention to the various proposals made in the current debate in AI, we isolate a number of interpretations, which in our opinion deserve a suitable clarification. We elucidate the implications of such various interpretations, arguing for the need of clear terminological choices regarding the technical use of terms like “ontology”, “conceptualization ” and “ontological commitment”. After some comments on the use “Ontology ” (with the capital “o”) as a term which denotes a philosophical discipline, we analyse the possible confusion between an ontology intended as a particular conceptual framework at the knowledge level and an ontology intended as a concrete artifact at the symbol level, to be used for a given purpose. A crucial point in this clarification effort is the careful analysis of Gruber’ s definition of an ontology as a specification of a conceptualization. 1
489|Formal Ontology, Conceptual Analysis and Knowledge Representation|The purpose of this paper is to defend the systematic introduction of formal ontological principles in the current practice of knowledge engineering, to explore the various relationships between ontology and knowledge representation, and to present the recent trends in this promising research area. According to the &#034;modelling view&#034; of knowledge acquisition proposed by Clancey, the modeling activity must establish a correspondence between a knowledge base and two separate subsystems: the agent&#039;s behavior (i.e. the problem-solving expertize) and its own environment (the problem domain). Current knowledge modelling methodologies tend to focus on the former subsystem only, viewing domain knowledge as strongly dependent on the particular task at hand: in fact, AI researchers seem to have been much more interested in the nature of reasoning rather than in the nature of the real world. Recently, however, the potential value of task-independent knowlege bases (or &#034;ontologies&#034;) suitable to large scale integration has been underlined in many ways.  In this paper, we compare the dichotomy between reasoning and representation to the philosophical distinction between epistemology and ontology. We introduce the notion of the ontological level, intermediate between the epistemological and the conceptual level discussed by Brachman, as a way to characterize a knowledge representation formalism taking into account the intended meaning of its primitives. We then discuss some formal ontological distinctions which may play an important role for such purpose.   
490|Towards distributed use of large-scale ontologies|Large scale knowledge bases systems are difficult and expensive to construct. If we could share knowledge across systems, costs would be reduced. However, because knowledge bases are typically constructed from scratch, each with their own idiosyncratic structure, sharing is difficult. Recent research has focused on the use of ontologies to promote sharing. An ontology is a hierarchically structured set of terms for describing a domain that can be used as a skeletal foundation for a knowledge base. If two knowledge bases are built on a common ontology, knowledge can be more readily shared, since they share a common underlying structure. This paper outlines a set of desiderata for ontologies, and then describes how we have used a large-scale (50,000+ concept) ontology develop a specialized, domain-specific ontology semiautomatically. We then discuss the relation between ontologies and the process of developing a system, arguing that to be useful, an ontology needs to be created as a &#034;living document&#034;, whose development is tightly integrated with the system’s. We conclude with a discussion of Web-based ontology tools we are developing to support this approach.
491|Enterprise modeling|... This article motivates the need for enterprise models and introduces the concepts of generic and deductive enterprise models. It  reviews research to date on enterprise modeling and considers in detail the Toronto virtual enterprise effort at the University of Toronto.
492|Part-Whole Relations in Object-Centered Systems: An Overview|Knowledge bases, data bases and object-oriented systems (referred to in the paper as Object-Centered systems) all rely on attributes as the main construct used  to associate properties to objects; among these, a fundamental role is played by  the so-called part-whole relation. The representation of such a structural information usually requires a particular semantics together with specialized inference and  update mechanisms, but rarely do current modeling formalisms and methodologies  give it a specific  &#034;first-class&#034; dignity.  The main thesis of this paper is that the part-whole relation cannot simply be  considered as an ordinary attribute, its specific ontological nature requires to be  understood and integrated within data modeling formalisms and methodologies.  On the basis of such an ontological perspective, we survey the conceptual modeling  issues involving part-whole relations, and the various modeling frameworks provided  by knowledge representation and object-oriented formalisms.   
493|Semantic Matching: Formal Ontological Distinctions for Information Organization, Extraction, and Integration|The task of information extraction can be seen as a problem of semantic  matching between a user-defined template and a piece of information written  in natural language. To this purpose, the ontological assumptions of the  template need to be suitably specified, and compared with the ontological implications  of the text. So-called &#034;ontologies&#034;, consisting of theories of various  kinds expressing the meaning of shared vocabularies, begin to be used for this  task. This paper addresses the theoretical issues related to the design and use of  such ontologies for purposes of information retrieval and extraction. After a discussion  on the nature of semantic matching within a model-theoretical framework,  we introduce the subject of Formal Ontology, showing how the notions of  parthood, integrity, identity, and dependence can be of help in understanding,  organizing and formalizing fundamental ontological distinctions. We present  then some basic principles for ontology design, and we illustrate a preliminary  proposal for a top-level ontology develped according to such principles. As a  concrete example of ontology-based information retrieval, we finally report an  ongoing experience of use of a large linguistic ontology for the retrieval of object-oriented software components. 
494|The MOMIS approach to Information Integration|Introduction The web explosion, both at internet and intranet level, has transformed the electronic information system from single isolated node to an entry points into a worldwide network of information exchange and business transactions. Business and commerce has taken the opportunity of the new technologies to define the e-commerce activity. An electronic marketplace represents a virtual place where buyers and sellers meet to exchange goods and services, by sharing information that is often obtained as hypertext catalogs from different companies. Companies have equipped themselves with data storing systems building up informative systems containing data which are related one another, but which are often redundant, heterogeneous and not always substantial. The problems that have to be faced in this field are mainly due to both structural and application heterogeneity, as well as to the lack of a common ontology, causing semantic differences between information sources. Moreo
496|Ontology Reuse and Application|In this paper, we describe an investigation into the reuse and application  of an existing ontology for the purpose of specifying and formally  developing software for aircraft design. Our goals were to clearly identify  the processes involved in the task, and assess the cost-effectiveness  of reuse. Our conclusions are that (re)using an ontology is far from  an automated process, and instead requires significant effort from the  knowledge engineer. We describe and illustrate some intrinsic properties  of the ontology translation problem and argue that fully automatic  translators are unlikely to be forthcoming in the foreseeable future. Despite  the effort involved, our subjective conclusions are that in this case  knowledge reuse was cost-effective, and that it would have taken significantly  longer to design the knowledge content of this ontology from  scratch in our application. Our preliminary results are promising for  achieving larger-scale knowledge reuse in the future.
497|Domain Specific Ontologies for Semantic Information Brokering on the Global Information Infrastructure|Recent emerging technologies such as internetworking and the World Wide Web (WWW) have significantly expanded the types, availability, and volume of data accessible to an information management system. In this new environment it is imperative to view an information source at the level of its relevant semantic concepts. We propose that these semantic concepts be chosen from pre-existing domain specific ontologies. Domain specific ontologies are used as tools/mechanisms for specifying the ontological commitments or agreements between information users and providers on the information infrastructure. We use domain specific ontologies to tackle the information explosion by the: (a) Re-use and organization of knowledge in pre-existing real world ontologies, achieved by mapping semantic concepts in the ontologies to data structures in the underlying repositories; and (b) Knowledge integration and development of mechanisms to translate information requests across ontologies. We thus provide s...
498|A Connection Based Approach to Commonsense Topological Description and Reasoning|The standard mathematical approaches to topology, point-set topology and algebraic  topology, treat points as the fundamental, undefined entities, and construct extended  spaces as sets of points with additional structure imposed on them. Point-set topology  in particular generalises the concept of a `space&#039; far beyond its intuitive meaning. Even  algebraic topology, which concentrates on spaces built out of `cells&#039; topologically equivalent  to n-dimensional discs, concerns itself chiefly with rather abstract reasoning concerning  the association of algebraic structures with particular spaces, rather than the kind of  topological reasoning which is required in everyday life, or which might illuminate the  metaphorical use of topological concepts such as `connection&#039; and `boundary&#039;.  This paper explores an alternative to these approaches, RCC theory, which takes  extended spaces (`regions&#039;) rather than points as fundamental. A single relation, C (x; y)  (read `Region x connects with reg...
499|Ontological Tools for Geographic Representation|Abstract. This paper is concerned with certain ontological issues in the foundations of geographic representation. It sets out what these basic issues are, describes the tools needed to deal with them, and draws some implications for a general theory of spatial representation. Our approach has ramifications in the domains of mereology, topology, and the theory of location, and the question of the interaction of these three domains within a unified spatial representation theory is addressed. In the final part we also consider the idea of nonstandard geographies, which may be associated with geography under a classical conception in the same sense in which non-standard logics are associated with classical logic. 1.
500|The Basic Tools of Formal Ontology|The term ‘formal ontology ’ was first used by the philosopher Edmund Husserl in his Logical Investigations to signify the study of those formal structures and relations – above all relations of part and whole – which are exemplified in the subject-matters of the different material sciences. We follow Husserl in presenting the basic concepts of formal ontology as falling into three groups: the theory of part and whole, the theory of dependence, and the theory of boundary, continuity and contact. These basic concepts are presented in relation to the problem of providing an account of the formal ontology of the mesoscopic realm of everyday experience, and specifically of providing an account of the concept of individual substance.
501|An Ontological Theory of Physical Objects|We discuss an approach to a theory of physical objects and present a logical theory based on a fundamental distinction between objects and their substrates, i.e. chunks of matter and regions of space. The purpose is to establish the basis of a general ontology of space, matter and physical objects for the domain of mechanical artifacts. An extensional mereological framework is assumed for substrates, whereas physical objects are allowed to change their spatial and material substrate while keeping their identity. Besides the parthood relation, simple self-connected region and congruence (or sphere) are adopted as primitives for the description of space. Only threedimensional regions are assumed in the domain. This paper is a revision and slight modification of [Borgo et al. 1996]. 1.
502|Spatial Entities|this paper. However one basic motivation seems easily available. Without going into much detail (see Varzi [1994]), the point is simply that mereological reasoning by itself cannot do justice to the notion of a whole---a self-connected whole, such as a stone or a rope, as opposed to a scattered entity made up of several disconnected parts, such as a broken glass or an archipelago. Parthood is a relational concept, wholeness a global property. And in spite of a widespread tendency to present mereology as a theory of parts and wholes, the latter notion (in its ordinary understanding) cannot be explained in terms of the former. For every whole there is a set of (possibly potential) parts; for every set of parts (i.e., arbitrary objects) there is in principle a complete whole, viz. its mereological sum, or fusion. But there is no way, mereologically, to draw a distinction between &#034;good&#034; and &#034;bad&#034; wholes; there is no way one can rule out wholes consisting of widely scattered or ill assorted entities (the sum consisting of our four eyes and Caesar&#039;s left foot) by reasoning exclusively in terms of parthood. If we allow for the possibility of scattered entities, then we lose the possibility of discriminating them from integral, connected wholes. On the other hand, we cannot just keep the latter without some means of discriminating them from the former.
503|The Ontological Nature of Subject Taxonomies|. Subject based classification is an important part of information retrieval, and has a long history in libraries, where a subject taxonomy was used to determine the location of books on the shelves. We have been studying the notion of subject itself, in order to determine a formal ontology of subject for a large scale digital library card catalog system. Deep analysis reveals a lot of ambiguity regarding the usage of subjects in existing systems and terminology, and we attempt to formalize these notions into a single framework for representing it. 1 Introduction Until recently, library card catalog systems have worked successfully because the amount of material referenced by the system was fairly small. Digital libraries, both formal as in the United States National Digital Library, or informal as in the World Wide Web, promise the potential of billions of electronic documents, and will render the existing card catalog paradigm useless. It has begun already, as web users find themsel...
504|Logical Modelling of Product Knowledge: Towards a Well-Founded Semantics for STEP|The main purpose of the STEP standard is to make possible the integration of product knowledge within the whole enterprise. Under this perspective, the mere exchange of geometric data is not enough, and qualitative knowledge of different kinds needs to be acquired and represented. Here, however, serious semantic problems arise, since the interpretation of the modelling primitives proposed by the standard heavily relies on implicit background knowledge. This problem has been recently underlined in [Metzger 1996], where it is argued that this background knowledge is stable enough and well agreed-upon only in the case of low-level geometric concepts. In the case of more abstract geometric concepts like design features, or non-geometric concepts like part or action, their meaning is not clear enough to be effectively shared by different application protocols. As a result, different interpretations are assumed for the same term in differ
505|Basic Problems of Mereotopology|Mereotopology is today regarded as a major tool for ontological analysis,  and for many good reasons. There are, however, a number of open questions that call  for an answer. Some of them are philosophical, others have direct import for applications,  but all are crucial for a proper assessment of the strengths and limits of  mereotopology. This paper is an attempt to put some order into this still untamed area  of research. I will not attempt any answers. But I shall try to give an idea of the problems,  and of their relevance for the systematic development of formal ontological  theories.
506|The Neutral Representation Project|The evolving complexity of many modern artifacts,  such as aircraft, has led to a serious fragmentation of  knowledge among software systems required for their  design and manufacture. In the case of aircraft design,  views of the same generic design knowledge are redundantly  encoded in multiple software systems, each  system using its own idiosyncractic ontology, and each  system containing that knowledge in an implicit, taskand  vendor-specific form. This situation is expensive,  due to high cost of developing from scratch, maintaining  and keeping synchronized the many systems used  in design.  Boeing&#039;s &#034;Neutral Representation&#034; project aims to address  these concerns by prototyping languages and  methods for making these underlying ontologies and  knowledge explicit, and hence more sharable and  maintainable. We are approaching this goal through  three tasks: Building explicit, neutral, machinesensible  representations of design knowledge; structuring  that knowledge into reusable components, indexed  by the ontologies which they use; and linking those  representations with existing design systems. In this  paper we present the work done this year, and discuss  issues related to ontological engineering and knowledge  sharing which have arisen.  
507|A Simple Estimator of Cointegrating Vectors in Higher Order Cointegrated Systems|Efficient estimators of cointegrating vectors are presented for systems involving deterministic components and variables of differing, higher orders of integration. The estimators are computed using GLS or OLS, and Wald Statistics constructed from these estimators have asymptotic x2 distributions. These and previously proposed estimators of cointegrating vectors are used to study long-run U.S. money (Ml) demand. Ml demand is found to be stable over 1900-1989; the 95 % confidence intervals for the income elasticity and interest rate semielasticity are (.88,1.06) and (-.13,-.08), respectively. Estimates based on the postwar data alone, however, are unstable, with variances which indicate substantial sampling uncertainty.
508|Statistical Analysis of Cointegrated Vectors|We consider a nonstationary vector autoregressive process which is integrated of order 1, and generated by i.i.d. Gaussian errors. We then derive the maximum likelihood estimator of the space of cointegration vectors and the likelihood ratio test of the hypothesis that it has a given number of dimensions. Further we test linear hypotheses about the cointegration vectors. The asymptotic distribution of these test statistics are found and the first is described by a natural multivariate version of the usual test for unit root in an autoregressive process, and the other is a x2 test. 1.
510|Testing for Common Trends|Cointegrated multiple time series share at least one common trend. Two tests are developed for the number of common stochastic trends (i.e., for the order of cointegration) in a multiple time series with and without drift. Both tests involve the roots of the ordinary least squares coefficient matrix obtained by regressing the series onto its first lag. Critical values for the tests are tabulated, and their power is examined in a Monte Carlo study. Economic time series are often modeled as having a unit root in their autoregressive representation, or (equivalently) as containing a stochastic trend. But both casual observation and economic theory suggesthat many series might contain the same stochastic trendso that they are cointegrated. If each of n series is integrated of order 1 but can be jointly characterized by k &lt; n stochastic trends, then the vecto representation of these series has k unit roots and n- k distinct stationary linear combinations. Our proposed tests can be viewed alternatively as tests of the number of common trends, linearly independent cointegrating vectors, or autoregressive unit roots of the vector process. Both of the proposed tests are asymptotically similar. The firstest (qf) is developed under the assumption that certain components of the process have a finite-order vector autoregressive (VAR) representation, and the nuisance parameters are handled by estimating this VAR. The second test (q,) entails computing the eigenvalues of a corrected sample first-order autocorrelation matrix, where the correction is essentially a sum of the autocovariance matrices. Previous researchers have found that U.S. postwar interest rates, taken individually, appear to be integrated of order 1. In addition, the theory of the term structure implies that yields on similar assets of different maturities will be cointegrated. Applying these tests to postwar U.S. data on the federal funds rate and the three- and twelve-month treasury bill rates providesupport for this prediction: The three interest rates appear to be cointegrated.
511|Inference in Linear Time Series Models with Some Unit Roots|This paper considers estimation and hypothesis testing in linear time series models when some or all of the variables have unit roots. Our motivating example is a vector autoregression with some unit roots in the companion matrix, which might include polynomials in time as regressors. In the general formulation, the variable might be integrated or cointegrated of arbitrary orders, and might have drifts as well. We show that parameters that can be written as coefficients on mean zero, nonintegrated regressors have jointly normal asymptotic distributions, converging at the rate T&#039;/2. In general, the other coefficients (including the coefficients on polynomials in time) will have nonnormal asymptotic distributions. The results provide a formal characterization of which t or F tests-such as Granger causality tests-will be asymptotically valid, and which will have nonstandard limiting distributions.
512|Stochastic Trends and Economic Fluctuations|Are business cycles mainly the result of permanent shocks to productivity? This paper uses a long-run restriction implied by a large class of real-business-cycle models-identifying permanent productivity shocks as shocks to the common stochastic trend in output, consumption, and investment-to provide new evidence on this question. Econometric tests indicate that this common-stochastic-trend / cointegration implication is consistent with postwar U.S. data. However, in systems with nominal variables, the estimates of this common stochastic trend indicate that permanent productivity shocks typically explain less than half of the business-cycle variability in output, consumption, and investment. (JEL E32, C32) A central, surprising, and controversial result of some current research on real business cycles is the claim that a common stochastic trend-the cumulative effect of permanent shocks to productivity-underlies the bulk of economic fluctuations. If confirmed, this finding would imply that many other forces have been relatively unimportant over historical business cycles, including the monetary and fiscal policy shocks stressed in traditional macroeconomic analysis. This paper shows that the hypothesis of a common stochastic productivity trend has a set of econometric implications that allows us to test for its presence, measure its importance, and extract estimates of its realized value. Applying these procedures to consumption, investment, and output for the postwar United States, we find results that both support and contradict this claim in the real-businesscycle literature. The U.S. data are consis-
513|Kerberos: An Authentication Service for Open Network Systems|In an open network computing environment, a workstation cannot be trusted to identify its users correctly to network services.  Kerberos provides an alternative approach whereby a trusted third-party authentication service is used to verify users’ identities. This paper gives an overview of the Kerberos authentication model as implemented for MIT’s Project Athena. It describes the protocols used by clients, servers, and Kerberos to achieve authentication. It also describes the management and replication of the database required.   The views of Kerberos as seen by the user, programmer, and administrator are described.  Finally, the role of Kerberos in the larger Athena picture is given, along with a list of applications that presently use Kerberos for user authentication.  We describe the addition of Kerberos authentication to the Sun Network File System as a case study for integrating Kerberos with an existing application.
514|Berkeley UNIX+ on 1000 Workstations: Athena Changes to 4.3BSD |4.3BSD UNIX as shipped is designed for use on individually-managed, networked  timesharing systems. A large network of individual workstations and server machines,  all managed centrally, has many important differences from such a model. This paper  discusses some of the changes necessary for 4.3 in this new world, including the file system  layout, configuration files, and software. The integration with Athena&#039;s authentication  system, name service, and service management system are also discussed.  1. Overview  &#034;By 1988, create a new educational computing environment environment at MIT built around high-performance graphics workstations, high-speed networking, and servers of various types.&#034; This one-sentence statement is a highlevel description of the technical goals of Project Athena. While the primary goals are to enhance education, attaining them has required a significant effort to engineer a software environment for use in a large network of workstations and servers.  The Athena...
515|Lattice-Based Access Control Models|The objective of this article is to give a tutorial on lattice-based  access control models for computer security. The paper begins with a review  of Denning&#039;s axioms for information flow policies, which provide a theoretical  foundation for these models. The structure of security labels in the military and  government sectors, and the resulting lattice is discussed. This is followed by a  review of the Bell-LaPadula model, which enforces information flow policies by  means of its simple-security and *-properties. It is noted that information flow  through covert channels is beyond the scope of such access controls. Variations  of the Bell-LaPadula model are considered. The paper next discusses the Biba  integrity model, examining its relationship to the Bell-LaPadula model. The  paper then reviews the Chinese Wall policy, which arises in a segment of the  commercial sector. It is shown how this policy can be enforced in a lattice  framework.
516|Role-Based Access Control|While Mandatory Access Controls (MAC) are appropriate for multilevel secure military applications, Discretionary Access Controls (DAC) are often perceived as meeting the security processing needs of industry and civilian government. This paper argues that reliance on DAC as the principal method of access control is unfounded and inappropriate for many commercial and civilian government organizations. The paper describes a type of non-discretionary access control - role-based access control (RBAC) - that is more central to the secure processing needs of non-military systems then DAC. 1 Introduction  The U.S. government has been involved in developing security technology for computer and communications systems for some time. Although advances have been great, it is generally perceived that the current state of security technology has, to some extent failed to address the needs of all. [1], [2] This is especially true of organizations outside the Department of Defense (DoD). [3] The curre...
517|The Typed Access Matrix Model|The access matrix model as formalized by Harrison, Ruzzo, and Ullman (HRU) has broad expressive power. Unfortunately, HRU has weak safety properties (i.e., the determination of whether or not a given subject can ever acquire access to a given object). Most security policies of practical interest fall into the undecidable cases of HRU. This is true even for monotonic policies (i.e., where access rights can be deleted only if the deletion is itself reversible). In this paper we define the typed access matrix (TAM) model by introducing strong typing into HRU (i.e., each subject or object is created to be of a particular type which thereafter does not change). We prove that monotonic TAM (MTAM) has strong safety properties similar to Sandhu&#039;s Schematic Protection Model. Safety in MTAM&#039;s decidable case is, however, NP-hard. We develop a model called ternary MTAM which has polynomial safety for its decidable case, and which nevertheless retains the full expressive power of MTAM. There is compelling evidence that the decidable safety cases of ternary MTAM are quite adequate for modeling practial monotonic security policies.
518|Access Rights Administration in Role-Based Security Systems|This paper examines the concept of role-based protection and, in particular, role  organization. From basic role relationships, a model for role organization is developed.  The role graph model, its operator semantics based on graph theory and algorithms for  role administration are proposed. The role graph model, in our view, presents a very  generalized form of role organization for access rights administration. It is shown how  the model simulates other organizational structures such as hierarchies [TDH92] and  privilege graphs [Bal90]. 
519|Conceptual Foundations for a Model of Task-based Authorizations|In this paper we describe conceptual foundations to address integrity issues in computerized information systems from the enterprise perspective. Our motivation for this effort stems from the recognition that existing models are formulated at too low a level of abstraction, to be useful for modeling organizational requirements, policy aspects, and internal controls, pertaining to maintenance of integrity in information systems. In particular, these models are primarily concerned with the integrity of internal data components within computer systems, and thus lack the constructs necessary to model enterprise level integrity principles. The starting point in our investigation is the notion of authorization functions and tasks associated with business activities carried out in the enterprise. These functions identify the authorization requirements while the authorization tasks embody the concepts required to carry out such authorizations. We believe a model of task-based autho...
520|A Lattice Interpretation Of The Chinese Wall Policy|The Chinese Wall policy was identi#ed and so named by Brewer and Nash #2#.  This policy arises in the segment of the commercial sector which provides consulting  services to other companies. Consultants naturally have to deal with con#dential company  information for their clients. The objective of the Chinese Wall policy is to prevent  information #ows which cause con#ict of interest for individual consultants. Brewer and  Nash develop a mathematical model of the Chinese Wall policy, on the basis of which  they claim that this policy #cannot be correctly represented by a Bell-LaPadula model.&#034;  In this paper we demonstrate that the Brewer-Nash model is too restrictivetobeemployed  in a practical system. This is due to their treatment of users and subjects as  synonymous concepts, with the consequence that they do not distinguish security policy  as applied to human users versus security policy as applied to computer subjects. By  maintaining a careful distinction between users, princip...
521|Delegation Of Authority|This paper is concerned with the specification of discretionary access control policy for commercial security and the delegation of access control authority in a way which gives flexibility while retaining management control. Large distributed processing systems have very large numbers of users and resource objects so that it is impractical to specify access control policy in terms of individual objects or individual users. We need to be able to specify it as relationships between groups of users and groups of objects. The systems typically consist of multiple interconnected networks and span a number of different organisations. Authority cannot be delegated or imposed from one central point, but has to be negotiated between independent managers who wish to cooperate but who may have a very limited trust in each other. The paper proposes the use of access rules to specify, in terms of their domain memberships, what operations a user can perform on a target object. The delegation of aut...
522|Adaptive clustering for mobile wireless networks|This paper describes a self-organizing, multihop, mobile radio network, which relies on a code division access scheme for multimedia support. In the proposed network architecture, nodes are organized into nonoverlapping clusters. The clusters are independently controlled and are dynamically reconfigured as nodes move. This network architecture has three main advantages. First, it provides spatial reuse of the bandwidth due to node clustering. Secondly, bandwidth can be shared or reserved in a controlled fashion in each cluster. Finally, the cluster algorithm is robust in the face of topological changes caused by node motion, node failure and node insertion/removal. Simulation shows that this architecture provides an efficient, stable infrastructure for the integration of different types of traffic in a dynamic radio network. 1.
523|Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers  (1994) |An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize adhoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.  
524|Maisie: A Language for the Design of Efficient Discrete-event Simulations|Maisie is a C-based discrete-event simulation language that was designed to cleanly separate a simulation model from the underlying algorithm (sequential or parallel) used for the execution of the model. With few modifications, a Maisie program may be executed using a sequential simulation algorithm, a parallel conservative algorithm or a parallel optimistic algorithm. The language constructs allow the runtime system to implement optimizations that reduce recomputation and state saving overheads for optimistic simulations and synchronization overheads for conservative implementations. This paper presents the Maisie simulation language, describes a set of optimizations and illustrates the use of the language in the design of efficient parallel simulations. 1 Introduction Distributed (or parallel) simulation refers to the execution of a simulation program on parallel computers. A number of algorithms[25, 10, 11, 21, 20] have been suggested for distributed simulation and many experimental...
525|Radio Link Admission Algorithms for Wireless Networks with Power Control and Active Link Quality Protection|In this paper we present a distributed power control scheme, which maintains the SIRs of operational (active) links above their required thresholds at all time (link quality protection), while new users are being admitted; furthermore, when new users cannot be successfully admitted, existing ones do not suffer fluctuations of their SIRs below their required thresholds values. We also present two admission /rejection control algorithms, which exercise voluntary drop-out of links inadmissible to the network, so as to reduce interference and possibly facilitate the admission of other links.
526|Asynchronous Multimedia Multihop Wireless Networks+|Personal communications and mobile computing will require a wireless network infrastructure which is fast deployable, possibly multihop, and capable of multimedia service support. The first infrastructure of this type was the Packet Radio Network (PRNET), developed in the 70&#039;s to address the battlefield and disaster recovery communication requirements. PRNET was totally asynchronous and was based on a completely distributed architecture. It handled datagram traffic reasonably well, but did not offer efficient multimedia support. Recently, under the WAMIS and Glomo ARPA programs several mobile, multimedia, multihop (M  3  ) wireless network architectures have been developed, which assume some form of synchronous, time division infrastructure. The synchronous time frame leads to efficient multimedia support implementations. However, it introduces more complexity and is less robust in the face of mobility and channel fading. In this paper, we examine the impact of synchronization on wirel...
527|Multicluster, Mobile, Multimedia Radio Network|A multi-cluster, multi-hop packet radio network architecture for wireless adaptive mobile information systems is presented...
528|Providing Connection-oriented Network Services to Mobile Hosts|Mobile computers using wireless networks, along with multimedia applications, are two emerging trends in computer systems. This new mobile multimedia computing environment presents many challenges, due to the requirements of multimedia applications and the mobile nature of hosts. We present several alternative schemes for maintaining network connections used to provide multimedia service, as hosts move through a nano-cellular radio network. These algorithms modify existing connections by partially reestablishing them to perform handoffs. Using a simple analytical model, we compare the schemes on the basis of the service disruption caused by handoffs, required buffering, and excess resources required to perform the handoffs. Two technological trends of the 1990s are the emerging use of wireless computers and network support for multimedia services. The products of these trends hold forth the promise of being combined in innovative ways to provide applications such as mobile digital video and audio conferencing. The new computing environment presented by wireless multimedia personal communication systems such as discussed in
529|A distributed architecture for multimedia in dynamic wireless networks|The paper presents a self-organizing, wireless mobile radio net-work for multimedia support. The proposed architecture is distri-buted and it has the capability of rapid deployment and dynamic reconfiguration. Without the need of base stations, this architec-ture can operate in areas without a wired backbone infrastruc-ture. This architecture provides an instant infrastructure for real-time traffic transmission. Based on the instant infrastruc-ture, a stable and loop-free routing protocol is implemented. 1.
530|Ptolemy: A Framework for Simulating and Prototyping Heterogeneous Systems|Ptolemy is an environment for simulation and prototyping of heterogeneous systems. It uses modern object-oriented software technology (C++) to model each subsystem in a natural and efficient manner, and to integrate these subsystems into a whole. Ptolemy encompasses practically all aspects of designing signal processing and communications systems, ranging from algorithms and communication strategies, simulation, hardware and software design, parallel computing, and generating real-time prototypes. To accommodate this breadth, Ptolemy must support a plethora of widely-differing design styles. The core of Ptolemy is a set of object-oriented class definitions that makes few assumptions about the system to be modeled; rather, standard interfaces are provided for generic objects and more specialized, application-specific objects are derived from these. A basic abstraction in Ptolemy is the Domain, which realizes a computational model appropriate for a particular type of subsystem. Current e...
531|Telos: enabling ultra-low power wireless research|Abstract — We present Telos, an ultra low power wireless sensor module (“mote”) for research and experimentation. Telos is the latest in a line of motes developed by UC Berkeley to enable wireless sensor network (WSN) research. It is a new mote design built from scratch based on expe-riences with previous mote generations. Telos ’ new design consists of three major goals to enable experimentation: minimal power consumption, easy to use, and increased software and hardware robustness. We discuss how hardware components are selected and integrated in order to achieve these goals. Using a Texas Instruments MSP430 microcontroller, Chipcon IEEE 802.15.4-compliant radio, and USB, Telos ’ power profile is almost one-tenth the consumption of previous mote platforms while providing greater performance and throughput. It eliminates programming and support boards, while enabling experimentation with WSNs in both lab, testbed, and deployment settings. I.
532|The design of an acquisitional query processor for sensor networks|We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. 1.
533|Maté: A Tiny Virtual Machine for Sensor Networks|Composed of tens of thousands of tiny devices with very limited resources (&#034;motes&#034;), sensor networks are subject to novel systems problems and constraints. The large number of motes in a sensor network means that there will often be some failing nodes; networks must be easy to repopu-late. Often there is no feasible method to recharge motes, so energy is a precious resource. Once deployed, a network must be reprogrammable although physically unreachable, and this reprogramming can be a significant energy cost. We present Maté, a tiny communication-centric virtual machine designed for sensor networks. Mat~&#039;s high-level in-terface allows complex programs to be very short (under 100 bytes), reducing the energy cost of transmitting new programs. Code is broken up into small capsules of 24 instructions, which can self-replicate through the network. Packet sending and reception capsules enable the deploy-ment of ad-hoc routing and data aggregation algorithms. Maté&#039;s concise, high-level program representation simplifies programming and allows large networks to be frequently re-programmed in an energy-efficient manner; in addition, its safe execution environment suggests a use of virtual ma-chines to provide the user/kernel boundary on motes that have no hardware protection mechanisms. 
534|The dynamic behavior of a data dissemination protocol for network programming at scale |To support network programming, we present Deluge, a reliable data dissemination protocol for propagating large data objects from one or more source nodes to many other nodes over a multihop, wireless sensor network. Deluge builds from prior work in density-aware, epidemic maintenance protocols. Using both a real-world deployment and simulation, we show that Deluge can reliably disseminate data to all nodes and characterize its overall performance. On Mica2dot nodes, Deluge can push nearly 90 bytes/second, oneninth the maximum transmission rate of the radio supported under TinyOS. Control messages are limited to 18 % of all transmissions. At scale, the protocol exposes interesting propagation dynamics only hinted at by previous dissemination work. A simple model is also derived which describes the limits of data propagation in wireless networks. Finally, we argue that the rates obtained for dissemination are inherently lower than that for single path propagation. It appears very hard to significantly improve upon the rate obtained by Deluge and we identify establishing a tight lower bound as an open problem.
535|An analysis of a large scale habitat monitoring application|Habitat and environmental monitoring is a driving application for wireless sensor networks. We present an analysis of data from a second generation sensor networks deployed during the summer and autumn of 2003. During a 4 month deployment, these networks, consisting of 150 devices, produced unique datasets for both systems and biological analysis. This paper focuses on nodal and network performance, with an emphasis on lifetime, reliability, and the the static and dynamic aspects of single and multi-hop networks. We compare the results collected to expectations set during the design phase: we were able to accurately predict lifetime of the single-hop network, but we underestimated the impact of multihop traffic overhearing and the nuances of power source selection. While initial packet loss data was commensurate with lab experiments, over the duration of the deployment, reliability of the backend infrastructure and the transit network had a dominant impact on overall network performance. Finally, we evaluate the physical design of the sensor node based on deployment experience and a post mortem analysis. The results shed light on a number of design issues from network deployment, through selection of power sources to optimizations of routing decisions.  
536|Simulating the power consumption of large-scale sensor network applications|Developing sensor network applications demands a new set of tools to aid programmers. A number of simulation environments have been developed that provide varying degrees of scalability, realism, and detail for understanding the behavior of sensor networks. To date, however, none of these tools have addressed one of the most important aspects of sensor application design: that of power consumption. While simple approximations of overall power usage can be derived from estimates of node duty cycle and communication rates, these techniques often fail to capture the detailed, low-level energy requirements of the CPU, radio, sensors, and other peripherals. In this paper, we present PowerTOSSIM, a scalable simulation environment for wireless sensor networks that provides an accurate, per-node estimate of power consumption. PowerTOSSIM is an extension to TOSSIM, an event-driven simulation environment for TinyOS applications. In PowerTOSSIM, TinyOS components corresponding to specific hardware peripherals (such as the radio, EEPROM, LEDs, and so forth) are instrumented to obtain a trace of each device’s activity during the simulation run. PowerTOSSIM employs a novel code-transformation technique to estimate the number of CPU cycles executed by each node, eliminating the need for expensive instruction-level simulation of sensor nodes. PowerTOSSIM includes a detailed model of hardware energy consumption based on the Mica2 sensor node platform. Through instrumentation of actual sensor nodes, we demonstrate that PowerTOSSIM provides accurate estimation of power consumption for a range of applications and scales to support very large simulations.
537|Lessons From A Sensor Network Expedition|Habitat monitoring is an important driving application for wireless sensor networks (WSNs). Although researchers anticipate some challenges arising in the real-world deployments of sensor networks, a number of problems can be discovered only through experience. This paper evaluates a sensor network system described in an earlier work and presents a set of experiences from a four month long deployment on a remote island o# the coast of Maine. We present an in-depth analysis of the environmental and node health data. The close integration of WSNs with their environment provides biological data at densities previous impossible; however, we show that the sensor data is also useful for predicting system operation and network failures. Based on over one million data and health readings, we analyze the node and network design and develop network reliability profiles and failure models.
539|COTS Dust|Contents  Preface iv 1.0 Introduction 1  1.1 Smart Dust Scenarios 2  1.1.1 Forest Fire Warning 2 1.1.2 Enemy Troop Monitoring 3  1.2 Smart Dust Capabilities 3  1.2.1 Distributed Sensor Networks and Ad-hoc Networking 4 1.2.2 High Level Interpretation of Spatial Sensor Data 4 1.2.3 Distributed Processing 5 1.2.4 COTS Dust 6  2.0 COTS Dust Architecture 7  2.1 Power 8 2.2 Computation 9  2.2.1 Static vs. Dynamic Current 11 2.2.2 Strong Thumb 11  2.3 Sensors 12  2.3.1 Magnetometer (2/3 Axis) 13 2.3.2 Accelerometers (2/3 Axis) 14 2.3.3 Light Sensor 16 2.3.4 Temperature Sensor 17 2.3.5 Pressure Sensor 17 2.3.6 Humidity Sensor 19  2.4 Communication 19  2.4.1 Acoustic Communication 20 2.4.2 RF Communication 23 2.4.3 Optical Communication 27 2.4.4 Optical Communication vs. RF Communication 32  3.0 COTS Dust Systems 35  3.1 Mouse Collars 35 3.2 Radio Frequency Mote (RF Mote) 39  3.2.1 RF Communica
540|Electronic Markets and Electronic Hierarchies|This paper analyzes the fundamental changes in market structures that may result from the increasing use of information technology. First, an analytic framework is presented and its usefulness is demonstrated in explaining several major historical changes in American business structures. Then, the framework is used to help explain how electronic markets and electronic hierarchies will allow closer integration of adjacent steps in the value added chains of our economy. The most surprising prediction is that information technology will lead to an overall shift toward proportionately more coordination by markets rather than by internal decisions within firms. Finally, several examples of companies where these changes are already occurring are used to illustrate the likely paths by which new market structures will evolve and the ways in which individual companies can take advantage of these changes.
541|The telegraph’s effect on nineteenth century markets and firms|The second half of the nineteenth century was a period of extensive, seemingly contradictory change in the economy of the United States. Until that time, the economy was characterized by small, predominantly singlefunction firms operating in local and regional markets. Then, under the influence of improved communication and transportation, local and regional market areas merged into larger national ones. In some industries, fast and efficient national markets arose to coordinate related economic activities within the expanded market areas. In others, small firms grew, merged, and vertically integrated into large, multifunctional firms coordinating various functions or stages of economic activity internally. Since integrated firms were a new development in the US, there was a relative shift towards firm coordination and away from market coordination, though in absolute terms markets were handling more transactions than ever. Many developments affected the trade-offs between the integrated firm and the market as modes of coordinating various functions. This paper explores the telegraph&#039;s seemingly contradictory effects on these trade-offs. By radically reducing the time and cost for long-distance communication, it facilitated the emergence of large and efficient markets. In addition, however, it provided an important method by which large firms could efficiently coordinate various activities previously coordinated by markets. The role of the telegraph in the economic expansion of the second half of the nineteenth century has frequently been mentioned, but rarely studied. Alfred Chandler [2] has examined it more closely than most general business
542|Quantum Gravity|We describe the basic assumptions and key results of loop quantum gravity, which is a background independent approach to quantum gravity. The emphasis is on the basic physical principles and how one deduces predictions from them, at a level suitable for physicists in other areas such as string theory, cosmology, particle physics, astrophysics and condensed matter physics. No details are given, but references are provided to guide the interested reader to the literature. The present state of knowledge is summarized in a list of 35 key results on topics including the hamiltonian and path integral quantizations, coupling to matter, extensions to supergravity and higher dimensional theories, as well as applications to black holes, cosmology and Plank scale phenomenology. We describe the near term prospects for observational tests of quantum theories of gravity and the expectations that loop quantum gravity may provide predictions for their outcomes. Finally, we provide answers to frequently asked questions and a list of key open problems.
544|Background independent quantum gravity: a status report|The goal of this article is to present an introduction to loop quantum gravity —a background independent, non-perturbative approach to the problem of unification of general relativity and quantum physics, based on a quantum theory of geometry. Our presentation is pedagogical. Thus, in addition to providing a bird’s eye view of the present status of the subject, the article should also serve as a vehicle to enter the field and explore it in detail. To aid non-experts, very little is assumed beyond elements of general relativity, gauge theories and quantum field theory. While the article is essentially selfcontained, the emphasis is on communicating the underlying ideas and the significance of results rather than on presenting systematic derivations and detailed proofs. (These can be found in the listed references.) The subject can be approached in different ways. We have chosen one which is deeply rooted in well established physics and also has sufficient mathematical precision to ensure that there are no hidden infinities. In order to keep the article to a reasonable size, and to avoid overwhelming non-experts, we have had to leave out several interesting topics, results and viewpoints; this is meant to be an introduction to the subject rather than an exhaustive review of it.
545|Relativistic Spin Networks and Quantum Gravity|Abstract. Relativistic spin networks are defined by considering the spin covering of the group SO(4), SU(2)  × SU(2). Relativistic quantum spins are related to the geometry of the 2-dimensional faces of a 4-simplex. This extends the idea of Ponzano and Regge that SU(2) spins are related to the geometry of the edges of a 3-simplex. This leads us to suggest that there may be a 4-dimensional state sum model for quantum gravity based on relativistic spin networks which parallels the construction of 3-dimensional quantum gravity from ordinary spin networks.
546|Spin foam models|While the use of spin networks has greatly improved our understanding of the kinematical aspects of quantum gravity, the dynamical aspects remain obscure. To address this problem, we define the concept of a ‘spin foam ’ going from one spin network to another. Just as a spin network is a graph with edges labeled by representations and vertices labeled by intertwining operators, a spin foam is a 2-dimensional complex with faces labeled by representations and edges labeled by intertwining operators. Spin foams arise naturally as higher-dimensional analogs of Feynman diagrams in quantum gravity and other gauge theories in the continuum, as well as in lattice gauge theory. When formulated as a ‘spin foam model’, such a theory consists of a rule for computing amplitudes from spin foam vertices, faces, and edges. The product of these amplitudes gives the amplitude for the spin foam, and the transition amplitude between spin networks is given as a sum over spin foams. After reviewing how spin networks describe ‘quantum 3-geometries’, we describe how spin foams describe ‘quantum 4-geometries’. We conclude by presenting a spin foam model of 4-dimensional Euclidean quantum gravity, closely related to the state sum model of Barrett and Crane, but not assuming the presence of an underlying spacetime manifold.
547|Quantum Geometry of Isolated Horizons and Black Hole Entropy|Using the classical Hamiltonian framework of [1] as the point of departure, we carry out a non-perturbative quantization of the sector of general relativity, coupled to matter, admitting non-rotating isolated horizons as inner boundaries. The emphasis is on the quantum geometry of the horizon. Polymer excitations of the bulk quantum geometry pierce the horizon endowing it with area. The intrinsic geometry of the horizon is then described by the quantum Chern-Simons theory of a U(1) connection on a punctured 2-sphere, the horizon. Subtle mathematical features of the quantum Chern-Simons theory turn out to be important for the existence of a coherent quantum theory of the horizon geometry. Heuristically, the intrinsic geometry is flat everywhere except at the punctures. The distributional curvature of the U(1) connection at the punctures gives rise to quantized deficit angles which account for the overall curvature. For macroscopic black holes, the logarithm of the number of these horizon microstates is proportional to the area, irrespective of the values of (non-gravitational) charges. Thus, the black hole entropy can be accounted for entirely by the quantum states of the horizon geometry. Our analysis is applicable to all non-rotating black holes, including the astrophysically interesting ones which are very far from extremality. Furthermore, cosmological horizons (to which statistical mechanical considerations are known to apply) are naturally incorporated. An effort has been made to make the paper self-contained by including short reviews of the background material.
548|Quantum Spin Dynamics (QSD)  (1996) |An anomaly-free operator corresponding to the Wheeler-DeWitt constraint  of Lorentzian, four-dimensional, canonical, non-perturbative vacuum gravity  is constructed in the continuum. This operator is entirely free of factor ordering  singularities and can be defined in symmetric and non-symmetric form.  We work in the real connection representation and obtain a well-defined  quantum theory. We compute the complete solution to the Quantum Einstein  Equations for the non-symmetric version of the operator and a physical inner  product thereon.  The action of the Wheeler-DeWitt constraint on spin-network states is  by annihilating, creating and rerouting the quanta of angular momentum  associated with the edges of the underlying graph while the ADM-energy is  essentially diagonalized by the spin-network states. We argue that the spinnetwork  representation is the &#034;non-linear Fock representation&#034; of quantum  gravity, thus justifying the term &#034;Quantum Spin Dynamics (QSD)&#034;.  1 Introduction  A...
550|Quantum gravity with a positive cosmological constant|A quantum theory of gravity is described in the case of a positive cosmological constant in 3 + 1 dimensions. Both old and new results are described, which support the case that loop quantum gravity provides a satisfactory quantum theory of gravity. These include the existence of a ground state, discoverd by Kodama, which both is an exact solution to the constraints of quantum gravity and has a semiclassical limit which is deSitter spacetime. The long wavelength excitations of this state are studied and are shown to reproduce both gravitons and, when matter is included, quantum field theory on deSitter spacetime. Furthermore, one may derive directly from the Wheeler-deWitt equation corrections to the energy-momentum relations for matter fields of the form E 2 = p 2 +m 2 +alPlE 3 +... where a is a computable dimensionless constant. This may lead in the next few years to experimental tests of the theory. To study the excitations of the Kodama state exactly requires the use of the spin network representation, which is quantum deformed due to the cosmological constant. The theory may be developed within a single horizon, and the boundary states described exactly in terms of a boundary Chern-Simons theory. The Bekenstein bound is recovered and the N bound of Banks is given a background independent explanation. The paper is written as an introduction to loop quantum gravity, requiring no prior knowledge of the subject. The deep relationship between quantum gravity and topological field theory is stressed throughout.
551|Quantum geometry with intrinsic local causality|The space of states and operators for a large class of background independent theories of quantum spacetime dynamics is defined. The SU(2) spin networks of quantum general relativity are replaced by labelled compact two-dimensional surfaces. The space of states of the theory is the direct sum of the spaces of invariant tensors of a quantum group Gq over all compact (finite genus) oriented 2-surfaces. The dynamics is background independent and locally causal. The dynamics constructs histories with discrete features of spacetime geometry such as causal structure and multifingered time. For SU(2) the theory satisfies the Bekenstein bound and the holographic hypothesis is recast in this formalism.
552|Black hole entropy in loop quantum gravity |We calculate the black hole entropy in Loop Quantum Gravity as a function of the horizon area and provide the exact formula for the leading and sub-leading terms. By comparison with the Bekenstein– Hawking formula we uniquely fix the value of the ’quantum of area’ in the theory. The Bekenstein–Hawking formula gives the leading term in the entropy of the black hole in the form
553|Coherent States for Canonical Quantum General Relativity and the Infinite Tensor Product Extension |We summarize a recently proposed concrete programme for investigating the (semi)classical limit of canonical, Lorentzian, continuum quantum general relativity in four spacetime dimensions. The analysis is based on a novel set of coherent states labelled by graphs. These fit neatly together with an Infinite Tensor Product (ITP) extension of the currently used Hilbert space. The ITP construction enables us to give rigorous meaning to the infinite volume (thermodynamic) limit of the theory which has been out of reach so far. 1
554|Worldsheet formulations of gauge theories and gravity. talk given at the 7th Marcel Grossmann Meeting Stanford|The evolution operator for states of gauge theories in the graph representation (closely related to the loop representation) is formulated as a weighted sum over worldsheets interpolating between initial and final graphs. As examples, lattice SU(2) BF and Yang-Mills theories are expressed as worldsheet theories, and (formal) worldsheet forms of several continuum U(1) theories are given. It is argued that the world sheet framework should be ideal for representing GR, at least euclidean GR, in 4 dimensions, because it is adapted to both the 4-diffeomorphism invariance of GR, and the discreteness of 3-geometry found in the loop representation quantization of the theory. However, the weighting of worldsheets in GR has not yet been found. 1
555|Notes for a Brief History of Quantum Gravity|I sketch the main lines of development of the research in quantum gravity, from the first explorations in the early thirties to nowadays.  
556|Dual Formulation of Spin Network Evolution|We illustrate the relationship between spin networks and their dual representation by labelled triangulations of space in 2+1 and 3+1 dimensions. We apply this to the recent proposal for causal evolution of spin networks. The result is labelled spatial triangulations evolving with transition amplitudes given by labelled spacetime simplices. The formalism is very similar to simplicial gravity, however, the triangulations represent combinatorics and not an approximation to the spatial manifold. The distinction between future and past nodes which can be ordered in causal sets also exists here. Spacelike and timelike slices can be defined and the foliation is allowed to vary. We clarify the choice of the two rules in the causal spin network evolution, and the assumption of trivalent spin networks for 2+1 spacetime dimensions and four-valent for 3+1. As a direct application, the problem of the exponential growth of the causal model is remedied. The result is a clear and more rigid graphical...
557|A holographic formulation of quantum general relativity”, Phys |We show that there is a sector of quantum general relativity, in the Lorentzian signature case, which may be expressed in a completely holographic formulation in terms of states and operators defined on a finite boundary. The space of boundary states is built out of the conformal blocks of SU(2)L ? SU(2)R, WZW field theory on the n-punctured sphere, where n is related to the area of the boundary. The Bekenstein bound is explicitly satisfied. These results are based on a new lagrangian and hamiltonian formulation of general relativity based on a constrained Sp(4) topological field theory. The hamiltonian formalism is polynomial, and also left-right symmetric. The quantization uses balanced SU(2)L ?SU(2)R spin networks and so justifies the state sum model of Barrett and Crane. By extending the formalism to Osp(4/N) a holographic formulation of extended supergravity is obtained, as will be described in detail in a subsequent paper.
558|Spectrum of the volume operator in quantum gravity Nucl. Phys. B 460 143; Lewandowski J 1997 Volume and quantizations Class. Quantum Grav. 14 71, Ashtekar A and Lewandowski J 1998 Quantum theory of geometry II: Volume operators Adv|The volume operator is an important kinematical quantity in the non-perturbative approach to four-dimensional quantum gravity in the connection formulation. We give a general algorithm for computing its spectrum when acting on four-valent spin network states, evaluate some of the eigenvalue formulae explicitly, and discuss the role played by the Mandelstam constraints. 1 The volume operator has emerged as an important quantity in the kinematics of 3+1dimensional quantum gravity in the loop representation. It is the quantum analogue of the classical volume function, measuring the volume of three-dimensional spatial regions. Although not an observable of the pure gravity theory (in the sense of commuting with
559|Quantum Theory from Quantum Gravity|We provide a mechanism by which, from a background independent model with no quantum mechanics, quantum theory arises in the same limit in which spatial properties appear. Starting with an arbitrary abstract graph as the microscopic model of spacetime, our ansatz is that the microscopic dynamics can be chosen so that 1) the model has a low low energy limit which reproduces the non-relativistic classical dynamics of a system of N particles in flat spacetime, 2) there is a minimum length, and 3) some of the particles are in a thermal bath or otherwise evolve stochastically. We then construct simple functions of the degrees of freedom of the theory and show that their probability distributions evolve according to the Schrödinger equation. The non-local hidden variables required to satisfy the conditions of Bell’s theorem are the links in the fundamental graph that connect nodes adjacent in the graph but distant in the approximate metric of the low energy limit. In the presence of these links, distant stochastic fluctuations are transferred into universal quantum fluctuations.
560|Supersymmetric spin networks and quantum supergravity|We define supersymmetric spin networks, which provide a complete set of gauge invariant states for supergravity and supersymmetric gauge theories. The particular case of Osp(1/2) is studied in detail and applied to the non-perturbative quantization of supergravity. The supersymmetric extension of the area operator is defined and partly diagonalized. The spectrum is discrete as in quantum general relativity, and the two cases could be distinguished by measurements of quantum geometry. 1
561|Operators for quantized directions|Abstract. Inspired by the spin geometry theorem, two operators are defined which measure angles in the quantum theory of geometry. One operator assigns a discrete angle to every pair of surfaces passing through a single vertex of a spin network. This operator, which is effectively the cosine of an angle, is defined via a scalar product density operator and the area operator. The second operator assigns an angle to two “bundles ” of edges incident to a single vertex. While somewhat more complicated than the earlier geometric operators, there are a number of properties that are investigated including the full spectrum of several operators and, using results of the spin geometry theorem, conditions to ensure that semiclassical geometry states replicate classical angles.
562|Observational limits on quantum geometry effects |Abstract. Using a form of modified dispersion relations derived in the context of quantum geometry, we investigate limits set by current observations on potential corrections to Lorentz invariance. We use a phenomological model in which there are separate parameters for photons, leptons, and hadrons. Constraints on these parameters are derived using thresholds for the processes of photon stability, photon absorption, vacuum Cerenkov radiation, pion stability, and the GZK cutoff. Although the allowed region in parameter space is tightly constrained, non-vanishing corrections to Lorentz symmetry due to quantum geometry are consistent with current astrophysical observations. 1.
563|Quantum geometry and thermal radiation from black holes, Classical Quantum Gravity 16|A quantum mechanical description of black hole states proposed recently within non-perturbative quantum gravity is used to study the emission and absorption spectra of quantum black holes. We assume that the probability distribution of states of the quantum black hole is given by the “area” canonical ensemble, in which the horizon area is used instead of energy, and use Fermi’s golden rule to find the line intensities. For a non-rotating black hole, we study the absorption and emission of s-waves considering a special set of emission lines. To find the line intensities we use an analogy between a microscopic state of the black hole and a state of the gas of atoms.
565|Time, measurement and information loss in quantum cosmology, available on gr-qc/9301016|A framework for a physical interpretation of quantum cosmology appropriate to a nonperturbative hamiltonian formulation is proposed. It is based on the use of matter fields to define a physical reference frame. In the case of the loop representation it is convenient to use a spatial reference frame that picks out the faces of a fixed simplicial complex and a clock built with a free scalar field. Using these fields a procedure is proposed for constructing physical states and operators in which the problem of constructing physical operators reduces to that of integrating ordinary differential equations within the algebra of spatially diffeomorphism invariant operators. One consequence is that we may conclude that the spectra of operators that measure the areas of physical surfaces are discrete independently of the matter couplings or dynamics of the gravitational field. Using the physical observables and the physical inner product, it becomes possible to describe singularities, black holes and loss of information in a nonperturbative formulation of quantum gravity, without making reference to a background metric. While only a dynamical calculation can answer the question of whether quantum effects eliminate singularities, it is conjectured that, if they do not, loss of information is a likely result because the physical operator algebra that corresponds to measurements made at late times must be incomplete. Finally, I show that it is possible to apply Bohr’s original operational interpretation of quantum mechanics to quantum cosmology, so that one is free to use either a Copenhagen interpretation or a corresponding relative state interpretation in a canonical formulation of quantum cosmology. 1 SMOLIN: Time and measurement in quantum cosmology Contents
566|Statistical geometry of random weave states |I describe the first steps in the construction of semiclassical states for non-perturbative canonical quantum gravity using ideas from classical, Riemannian statistical geometry and results from quantum geometry of spin network states. In particular, I concentrate on how those techniques are applied to the construction of random spin networks, and the calculation of their contribution to areas and volumes.
567|Nonperturbative dynamics for abstract (p,q) string networks, Phys.Rev. D58  (1998) |We describe abstract (p,q) string networks which are the string networks of Sen without the information about their embedding in a background spacetime. The non-perturbative dynamical formulation invented for spin networks, in terms of causal evolution of dual triangulations, is applied on them. The formal transition amplitudes are sums over discrete causal histories that evolve (p,q) string networks. The dynamics depend on two free SL(2,Z) invariant functions which describe the amplitudes for the local evolution moves.
568|Quantum gravity vacuum and invariants of embedded spin networks |We show that the path integral for the three-dimensional SU(2) BF theory with a Wilson loop or a spin network function inserted can be understood as the Rovelli-Smolin loop transform of a wavefunction in the Ashtekar connection representation, where the wavefunction satisfies the constraints of quantum general relativity with zero cosmological constant. This wavefunction is given as a product of the delta functions of the SU(2) field strength and therefore it can be naturally associated to a flat connection spacetime. The loop transform can be defined rigorously via the quantum SU(2) group, as a spin foam state sum model, so that one obtains invariants of spin networks embedded in a three-manifold. These invariants define a flat connection vacuum state in the q-deformed spin network basis. We then propose a modification of this construction in order to obtain a vacuum state corresponding to the flat metric spacetime.
569|How far are we from the quantum theory of gravity?|An assessment is offered of the progress that the major approaches to quantum gravity have made towards the goal of constructing a complete and satisfactory theory. The emphasis is on loop quantum gravity and string theory, although other approaches are discussed, including dynamical triangulation models (euclidean and lorentzian) regge calculus models, causal sets, twistor theory, non-commutative geometry and models based on analogies to condensed matter systems. We proceed by listing the questions the theories are expected to be able to answer. We then compile two lists: the first details the actual results so far achieved in each theory, while the second lists conjectures which remain open. By comparing them we can evaluate how far each theory has progressed, and what must still be done before each theory can be considered a satisfactory quantum theory of gravity. We find there has been impressive recent progress on several fronts. At the same time, important issues about loop quantum gravity are so far unresolved, as are key conjectures of string theory. However, there is a reasonable expectation that experimental tests of lorentz invariance at Planck scales may in the near future make it possible to rule out one or more candidate quantum theories
