ID|Title|Summary
1|Issues in Data Stream Management|Traditional databases store sets of relatively static records with no pre-defined notion of time, unless timestamp attributes are explicitly added. While this model adequately represents commercial catalogues or repositories of personal information, many current and emerging applications require support for online analysis of rapidly changing data streams. Limitations of traditional DBMSs in supporting streaming applications have been recognized, prompting research to augment existing technologies and build new systems to manage streaming data. The purpose of this paper is to review recent work in data stream management systems, with an emphasis on application requirements, data models, continuous query languages, and query evaluation.
3|The space complexity of approximating the frequency moments|The frequency moments of a sequence containing mi elements of type i, for 1 = i = n, are the numbers Fk = ?n i=1 mki. We consider the space complexity of randomized algorithms that approximate the numbers Fk, when the elements of the sequence are given one by one and cannot be stored. Surprisingly, it turns out that the numbers F0, F1 and F2 can be approximated in logarithmic space, whereas the approximation of Fk for k = 6 requires n?(1) space. Applications to data bases are mentioned as well. 
4|NiagaraCQ: A Scalable Continuous Query System for Internet Databases|Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system’s performance and scalability. 1.
5|The design of an acquisitional query processor for sensor networks|We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. 1.
6|TelegraphCQ: Continuous Dataflow Processing for an Uncertan World|Increasingly pervasive networks are leading towards a world where data is constantly in motion. In such a world, conventional techniques for query processing, which were developed under the assumption of a far more static and predictable computational environment, will not be sufficient. Instead, query processors based on adaptive dataflow will be necessary. The Telegraph project has developed a suite of novel technologies for continuously adaptive query processing. The next generation Telegraph system, called TelegraphCQ, is focused on meeting the challenges that arise in handling large streams of continuous queries over high-volume, highly-variable data streams. In this paper, we describe the system architecture and its underlying technology, and report on our ongoing implementation effort, which leverages the PostgreSQL open source code base. We also discuss open issues and our research agenda.
7|Query Processing for Sensor Networks|Hardware for sensor nodes that combine physical sensors, actuators, embedded processors, and communication components has advanced significantly over the last decade, and made the large-scale deployment of such sensors a reality. Applications range from monitoring applications such as inventory maintenance over health care to military applications.
8|Eddies: Continuously Adaptive Query Processing|In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.  In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry  during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates prom...
9|Approximate Frequency Counts over Data Streams|We present algorithms for computing frequency counts exceeding a user-specified threshold over data streams. Our algorithms are simple and have provably small memory footprints. Although the output is approximate, the error is guaranteed not to exceed a user-specified parameter. Our algorithms can easily be deployed for streams of singleton items like those found in IP network monitoring. We can also handle streams of variable sized sets of items exemplified by a sequence of market basket transactions at a retail store. For such streams, we describe an optimized implementation to compute frequent itemsets in a single pass.
10|Finding frequent items in data streams|Abstract. We present a 1-pass algorithm for estimating the most frequent items in a data stream using very limited storage space. Our method relies on a novel data structure called a count sketch, which allows us to estimate the frequencies of all the items in the stream. Our algorithm achieves better space bounds than the previous best known algorithms for this problem for many natural distributions on the item frequencies. In addition, our algorithm leads directly to a 2-pass algorithm for the problem of estimating the items with the largest (absolute) change in frequency between two data streams. To our knowledge, this problem has not been previously studied in the literature. 1
11|Mining time-changing data streams|Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a station-ary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying pro-cesses generating them changed during this time, sometimes radically. Although a number of algorithms have been pro-posed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes ques-tionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.
13|Fjording the Stream: An Architecture for Queries over Streaming Sensor Data|If industry visionaries are correct, our lives will soon be full of sensors, connected together in loose conglomerations via wireless networks, each monitoring and collecting data about the environment at large. These sensors behave very differently from traditional database sources: they have intermittent connectivity, are limited by severe power constraints, and typically sample periodically and push immediately, keeping no record of historical information. These limitations make traditional database systems inappropriate for queries over sensors. We present the Fjords architecture for managing multiple queries over many sensors, and show how it can be used to limit sensor resource demands while maintaining high query throughput. We evaluate our architecture using traces from a network of traffic sensors deployed on Interstate 80 near Berkeley and present performance results that show how query throughput, communication costs, and power consumption are necessarily coupled in sensor environments.
14|Maintaining Stream Statistics over Sliding Windows (Extended Abstract)  (2002) |Mayur Datar  Aristides Gionis  y  Piotr Indyk  z  Rajeev Motwani  x  Abstract  We consider the problem of maintaining aggregates and statistics over data streams, with respect to the last N data elements seen so far. We refer to this model as the sliding window model. We consider the following basic problem: Given a stream of bits, maintain a count of the number of 1&#039;s in the last N elements seen from the stream. We show that using O(  1  ffl log  2  N) bits of memory, we can estimate the number of 1&#039;s to within a factor of 1 + ffl. We also give a matching lower bound of \Omega\Gamma  1  ffl log  2  N) memory bits for any deterministic or randomized algorithms. We extend our scheme to maintain the sum of the last N positive integers. We provide matching upper and lower bounds for this more general problem as well. We apply our techniques to obtain efficient algorithms for the Lp norms (for p 2 [1; 2]) of vectors under the sliding window model. Using the algorithm for the basic counting problem, one can adapt many other techniques to work for the sliding window model, with a multiplicative overhead of O(  1  ffl log N) in memory and a 1 + ffl factor loss in accuracy. These include maintaining approximate histograms, hash tables, and statistics or aggregates such as sum and averages.
15|Continuously Adaptive Continuous Queries over Streams|We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive crossquery sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators -- both selections and join state -- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.
16|Adaptive Filters for Continuous Queries over Distributed Data Streams|We consider an environment where distributed  data sources continuously stream updates to a  centralized processor that monitors continuous  queries over the distributed data. Significant communication  overhead is incurred in the presence of  rapid update streams, and we propose a new technique  for reducing the overhead. Users register  continuous queries with precision requirements at  the central stream processor, which installs filters  at remote data sources. The filters adapt to changing  conditions to minimize stream rates while  guaranteeing that all continuous queries still receive  the updates necessary to provide answers of  adequate precision at all times. Our approach enables  applications to trade precision for communication  overhead at a fine granularity by individually  adjusting the precision constraints of continuous  queries over streams in a multi-query workload.
17|Surfing wavelets on streams: One-pass summaries for approximate aggregate queries|Abstract We present techniques for computing small spacerepresentations of massive data streams. These are inspired by traditional wavelet-based approx-imations that consist of specific linear projections of the underlying data. We present general&amp;quot;sketch &amp;quot; based methods for capturing various linear projections of the data and use them to pro-vide pointwise and rangesum estimation of data streams. These methods use small amounts ofspace and per-item time while streaming through the data, and provide accurate representation asour experiments with real data streams show.
18|Dataflow query execution in a parallel main-memory environment|Abstract. In this paper, the performance and characteristics of the execution of various join-trees on a parallel DBMS are studied. The results of this study are a step into the direction of the design of a query optimization strategy that is fit for parallel execution of complex queries. Among others, synchronization issues are identified to limit the performance gain from parallelism. A new hash-join algorithm is introduced that has fewer synchronization constraints han the known hash-join algorithms. Also, the behavior of individual join operations in a join-tree is studied in a simulation experiment. The results how that the introduced Pipelining hash-join algorithm yields a better performance for multi-join queries. The format of the optimal join-tree appears to depend on the size of the operands of the join: A multi-join between small operands performs best with a bushy schedule; larger operands are better off with a linear schedule. The results from the simulation study are confirmed with an analytic model for dataflow query execution. Ke~,ords: parallel query processing, multi-join queries, simulation, analytical modeling 1.
19|Space-Efficient Online Computation of Quantile Summaries|An &amp;epsilon;-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of &amp;epsilon;N . We present a new online...
20|Supporting Aggregate Queries over Ad-Hoc Wireless Sensor Networks|We show how the database community&#039;s notion of a generic query interface for data aggregation can be applied to ad-hoc networks of sensor devices. As has been noted in the sensor network literature, aggregation is important as a data-reduction tool; networking approaches, however, have focused on application specific solutions, whereas our innetwork aggregation approach is driven by a general purpose, SQL-style interface that can execute queries over any type of sensor data while providing opportunities for significant optimization. We present a variety of techniques to improve the reliability and performance of our solution. We also show how grouped aggregates can be efficiently computed and offer a comparison to related systems and database projects.
21|Processing Complex Aggregate Queries over Data Streams|Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (i.e., data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed.
22|Frequency estimation of internet packet streams with limited space| We consider a router on the Internet analyzing the statistical properties of a TCP/IP packet stream. A fundamental difficulty with measuring traffic behavior on the Internet is that there is simply too much data to be recorded for later analysis, on the order of gigabytes a second. As a result, network routers can collect only relatively few statistics about the data. The central problem addressed here is to use the limited memory of routers to determine essential features of the network traffic stream. A particularly difficult and representative subproblem is to determine the top k categories to which the most packets belong, for a desired value of k and for a given notion of categorization such as the destination IP address. We present an algorithm that deterministically finds (in particular) all categories having a frequency above 1/(m + 1) using m counters, which we prove is best possible in the worst case. We also present a sampling-based algorithm for the case that packet categories follow an arbitrary distribution, but their order over time is permuted uniformly at random. Under this model, our algorithm identifies flows above a frequency threshold of roughly 1 /  v nm with high probability, where m is the number of counters and n is the number of packets observed. This guarantee is not far off from the ideal of identifying all flows (probability 1/n), and we prove that it is best possible up to a logarithmic factor. We show that the algorithm ranks the identified flows according to frequency within any desired constant factor of accuracy. 
23|Streaming Queries over Streaming Data|Recent work on querying data streams has focused  on systems where newly arriving data is  processed and continuously streamed to the user  in real-time. In many emerging applications, however,  ad hoc queries and/or intermittent connectivity  also require the processing of data that arrives  prior to query submission or during a period of  disconnection. For such applications, we have developed  PSoup, a system that combines the processing  of ad-hoc and continuous queries by treating  data and queries symmetrically, allowing new  queries to be applied to old data and new data to  be applied to old queries. PSoup also supports intermittent  connectivity by separating the computation  of query results from the delivery of those  results. PSoup builds on adaptive query processing  techniques developed in the Telegraph project  at UC Berkeley. In this paper, we describe PSoup  and present experiments that demonstrate the effectiveness  of our approach.
24|Multi-Dimensional Regression Analysis of Time-Series Data Streams|Real-time production systems and other dynamic environments often generate tremendous (potentially infinite) amount of stream data; the volume of data is too huge to be stored on disks or scanned multiple times. Can we perform on-line, multi-dimensional analysis and data mining of such data to alert people about dramatic changes of situations and to initiate timely, high-quality responses? This is a challenging task. In this paper,
25|XJoin: A Reactively-Scheduled Pipelined Join Operator|Wide-area distribution raises significant performance problems for traditional query processing techniques as data access becomes less predictable due to link congestion, load imbalances, and temporary outages. Pipelined query execution is a promising approach to coping with unpredictability in such environments as it allows scheduling to adjust to the arrival properties of the data. We have developed a non-blocking join operator, called XJoin, which has a small memory footprint, allowing many such operators to be active in parallel. XJoin is optimized to produce initial results quickly and can hide intermittent delays in data arrival by reactively scheduling background processing. We show that XJoin is an effective solution for providing fast query responses to users even in the presence of slow and bursty remote sources.  
26|Rate-Based Query Optimization for Streaming Information Sources|Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences. We present a preliminary validation of our rate-based optimization framework on a prototype XML query engine, though it is generic enough to be used in other database contexts. The results show that rate-based optimization is feasible and can indeed yield correct decisions.
27|Sampling From a Moving Window Over Streaming Data|We introduce the problem of sampling from a moving window of recent items from a data stream and develop the &#034;chain-sample&#034; and &#034;priority-sample&#034; algorithms for this problem.
28|Processing sliding window multi-joins in continuous queries over data streams|We study sliding window multi-join processing in continuous queries over data streams. Several algorithms are reported for performing continuous, incremental joins, under the assumption that all the sliding windows fit in main memory. The algorithms include multiway incremental nested loop joins (NLJs) and multi-way incremental hash joins. We also propose join ordering heuristics to minimize the processing cost per unit time. We test a possible implementation of these algorithms and show that, as expected, hash joins are faster than NLJs for performing equi-joins, and that the overall processing cost is influenced by the strategies used to remove expired tuples from the sliding windows. 1
29|Random sampling techniques for space efficient online computation of order statistics of large datasets|In a recent paper [MRL98], we had described a general framework for single pass approximate quantile nding algorithms. This framework included several known algorithms as special cases. We had identi ed a new algorithm, within the framework, which had a signi cantly smaller requirement for main memory than other known algorithms. In this paper, we address two issues left open in our earlier paper. First, all known and space e cient algorithms for approximate quantile nding require advance knowledge of the length of the input sequence. Many important database applications employing quantiles cannot provide this information. In this paper, we present anovel non-uniform random sampling scheme and an extension of our framework. Together, they form the basis of a new algorithm which computes approximate quantiles without knowing the input sequence length. Second, if the desired quantile is an extreme value (e.g., within the top 1 % of the elements), the space requirements of currently known algorithms are overly pessimistic. We provide a simple algorithm which estimates extreme values using less space than required by the earlier more general technique for computing all quantiles. Our principal observation here is that random sampling is quanti ably better when estimating extreme values than is the case with the median.  
30|Characterizing Memory Requirements for Queries over Continuous Data|This paper deals with continuous conjunctive queries with arithmetic comparisons and optional aggregation over multiple data streams. An algorithm is presented for determining whether or not any given query can be evaluated using a bounded amount of memory for all possible instances of the data streams. For queries that can be evaluated using bounded memory, an execution strategy based on constant-sized synopses of the data streams is proposed. For queries that cannot be evaluated using bounded memory, data stream scenarios are identified in which evaluating the queries requires memory linear in the size of the unbounded streams
31|Maintaining Variance and k-Medians over Data Stream Windows|The sliding window model is useful for discounting stale data in data stream applications. In this model, data elements arrive continually and only the most recent N elements are used when answering queries. We present a novel technique for solving two important and related problems in the sliding window model --- maintaining variance and maintaining a k-- median clustering. Our solution to the problem of maintaining variance provides a continually updated estimate of the variance of the last N values in a data stream with relative error of at most # using O(    # 2 log N) memory. We present a constant-factor approximation algorithm which maintains an approximate k--median solution for the last N data points using O(      N) memory, where # &lt; 1/2 is a parameter which trades o# the space bound with the approximation factor of O(2    ).
32|Tribeca: A System for Managing Large Databases of Network Traffic|The engineers who analyze traffic on high bandwidth networks must filter and aggregate either recorded traces of network packets or live traffic from the network itself. These engineers perform operations similar to database queries, but cannot use conventional data managers because of performance concerns and a semantic mismatch between the analysis operations and the operations supported by commercial DBMSs. Traffic analysis does not require fast random access, transactional update, or relational joins. Rather, it needs fast sequential access to a stream of traffic records and the ability to filter, aggregate, define windows, demultiplex, and remultiplex the stream. Tribeca is an extensible, stream-oriented DBMS designed to support network traffic analysis. It combines ideas from temporal and sequence databases with an implementation optimized for databases stored on high speed ID-1 tapes or arriving in real time from the network. The paper describes Tribeca&#039;s query language, executo...
33|Chain: Operator Scheduling for Memory Minimization in Data Stream Systems|In many applications involving continuous data streams, data arrival is bursty and data rate fluctuates over time. Systems that seek to give rapid or real-time query responses in such an environment must be prepared to deal gracefully with bursts in data arrival without compromising system performance. We discuss one strategy for processing bursty streams — adaptive, load-aware scheduling of query operators to minimize resource consumption during times of peak load. We show that the choice of an operator scheduling strategy can have significant impact on the run-time system memory usage. We then present Chain scheduling, an operator scheduling strategy for data stream systems that is near-optimal in minimizing run-time memory usage for any collection of singlestream queries involving selections, projections, and foreign-key joins with stored relations. Chain scheduling also performs well for queries with sliding-window joins over multiple streams, and multiple queries of the above types. A thorough experimental evaluation is provided where we demonstrate the potential benefits of Chain scheduling, compare it with competing scheduling strategies, and validate our analytical conclusions. 1.
34|Comparing data streams using hamming norms (how to zero in)  (2003) | Massive data streams are now fundamental to many data processing applications. For example, Internet routers produce large scale diagnostic data streams. Such streams are rarely stored in traditional databases and instead must be processed “on the fly” as they are produced. Similarly, sensor networks produce multiple data streams of observations from their sensors. There is growing focus on manipulating data streams and, hence, there is a need to identify basic operations of interest in managing data streams, and to support them efficiently. We propose computation of the Hamming norm as a basic operation of interest. The Hamming norm formalizes ideas that are used throughout data processing. When applied to a single stream, the Hamming norm gives the number of distinct items that are present in that data stream, which is a statistic of great interest in databases. When applied to a pair of streams, the Hamming norm gives an important measure of (dis)similarity: the number of unequal item counts in the two streams. Hamming norms have many uses in comparing data streams. We present a novel approximation technique for estimating the Hamming norm for massive data streams; this relies on what we call the “l0 sketch ” and we prove its accuracy. We test our approximation method on a large quantity of synthetic and real stream data, and show that the estimation is accurate to within a few percentage points.
35|Wavelet synopses with error guarantees|ABSTRACT Recent work has demonstrated the effectiveness of the wavelet de-composition in reducing large amounts of data to compact sets of
36|Scheduling for Shared Window Joins Over Data Streams|Continuous Ouery (CO) systems typically exploit  commonality among query expressions to achieve  improved efficiency through shared processing. Recently  proposed CO systems have introduced window  specifications in order to support unbounded  data streams. There has been, however, little investigation  of sharing for windowed query operators.
37|Online Data Mining for Co-Evolving Time Sequences|In many applications, the data of interest comprises multiple sequences that evolve over time. Examples include currency exchange rates, network traffic data. We develop a fast method to analyze such co-evolving time sequences  jointly to allow (a) estimation/forecasting of missing /delayed/future values, (b) quantitative data mining,and (c) outlier detection. Our method, MUSCLES, adapts to changing correlations among time sequences. It can handle indefinitely long sequences efficiently using an incremental algorithm and requires only small amount of storage and less I/O operations. To make it scale for a large number of sequences, we present a variation, the Selective MUSCLES method and propose an efficient algorithm to reduce the problem size. Experiments on real datasets show that MUSCLES outperforms popular competitors in prediction accuracy up to 10 times, and discovers interesting correlations. Moreover, Selective MUSCLES scales up very well for large numbers of sequences, reducing response time up to 110 times over MUSCLES, and sometimes even improves the prediction quality.
38|Using State Modules for Adaptive Query Processing|We present a query architecture in which join operators are decomposed into their constituent data structures (State Modules, or SteMs), and dataflow among these SteMs is managed adaptively by an Eddy routing operator. Breaking the encapsulation of joins serves two purposes. First, it allows the Eddy to observe multiple physical operations embedded in a join algorithm, allowing for better calibration and control of these operations. Second, the SteM on a relation serves as a shared materialization point, enabling multiple competing access methods to share results, which can be leveraged by multiple competing join algorithms. Our architecture extends prior work significantly, allowing continuously adaptive decisions for most major aspects of traditional query optimization: choice of access methods and join algorithms, ordering of operators, and choice of a query spanning tree. SteMs introduce...
39|Distributed streams algorithms for sliding windows|Massive data sets often arise as physically distributed, parallel data streams, and it is important to estimate various aggregates and statistics on the union of these streams. This paper presents algorithms for estimating aggregate functions over a “sliding window ” of the N most recent data items in one or more streams. Our results include: 1. For a single stream, we present the first ?-approximation scheme for the number of 1’s in a sliding window that is optimal in both worst case time and space. We also present the first ?-approximation scheme for the sum of integers in [0..R] in a sliding window that is optimal in both worst case time and space (assuming R is at most polynomial in N). Both algorithms are deterministic and use only logarithmic memory words. 2. In contrast, we show that any deterministic algorithm that estimates, to within a small constant relative error, the number of 1’s (or the sum of integers) in a sliding window on the union of distributed streams requires ?(N) space.
40|QuickSAND: Quick Summary and Analysis of Network Data|Monitoring and analyzing traffic data generated from large ISP networks imposes challenges both at the data gathering phase as well as the data analysis itself. Still both tasks are crucial for responding to day to day challenges of engineering large networks with thousands of customers. In this paper we build on the premise that approximation is a necessary evil of handling massive datasets such as network data. We propose building compact summaries of the traffic data called sketches at distributed network elements and centers. These sketches are able to respond well to queries that seek features that stand out of the data. We call such features &#034;heavy hitters.&#034; In this paper, we describe sketches and show how to use sketches to answer aggregate and trend-related queries and identify heavy hitters. This may be used for exploratory data analysis of network operations interest. We support our proposal by experimentally studying AT&amp;T WorldNet data and performing a feasibility study on the Cisco NetFlow data collected at several routers.  1 
41| An Abstract Semantics and Concrete Language for Continuous Queries over Streams and Relations |Despite the recent surge of research in query processing over data streams, little attention has been devoted to defining precise semantics for continuous queries over streams. We first present an abstract semantics based on several building blocks: formal definitions for streams and relations, mappings among them, and any relational query language. From these basics we define a precise interpretation for continuous queries over streams and relations. We then propose a concrete language, CQL (for Continuous semantics using SQL as the relational query language and window specifications derived from SQL-99 to map from streams to relations. We identify some equivalences that can be used to rewrite CQL queries for optimization, and we discuss some additional implementation issues arising from the language and its semantics. We are implementing CQL as part of a general-purpose Data Stream Management System at Stanford.  
42|Exploiting k-Constraints to Reduce Memory Overhead in Continuous Queries over Data Streams|We consider the problem of efficiently processing continuous queries over multiple continuous data streams inthe presence of constraints on the datastreams. We specify several types of constraints, and for each constrainttype we identify an “ adherence parameter ” that captures how closely a given stream or joining pair of streams adheres to a constraint of that type. We then present a query execution algorithm that takes-constraints over streams into account in order to reduce memory overhead. In general, the tighter the adherence parameters are in the-constraints, the less memory required. Furthermore, if input streams do not adhere to constraints within the specified adherence parameters, our algorithm automatically degrades gracefully to provide continuous approximate answers. We have implemented our approach in a testbed continuous query processor and preliminary experimental results are reported. 1
43|Reverse Nearest Neighbor Aggregates Over Data Streams|Reverse Nearest Neighbor (RNN) queries  have been studied for finite, stored data  sets and are of interest for decision support.
44|Data Stream Management Issues – A Survey|Traditional databases store sets of relatively static records with no pre-defined notion of time, unless timestamp attributes are explicitly added. While this model adequately represents commercial catalogues or repositories of personal information, many current and emerging applications require support for on-line analysis of rapidly changing data streams. Limitations of traditional DBMSs in supporting streaming applications have been recognized, prompting research to augment existing technologies and build new systems to manage streaming data. The purpose of this paper is to review recent work in data stream management systems, with an emphasis on data models, continuous query languages, and query evaluation and optimization techniques. We also give examples of streaming queries in various applications and review related work in modeling lists and sequences. 1
45|ATLaS: A Native Extension of SQL for Data Mining and Stream Computations|A lack of power and extensibility in their query languages has seriously limited the generality of DBMSs and hampered their ability to support new application domains. Considerable efforts by database researchers and commercial DBMS vendors have led to major extensions; yet there remain important applications---particularly data mining---that are not supported well in SQL-3. Thus, there is a pressing need for more general mechanisms for extending SQL and dealing with new application areas, particularly database-centric data mining. To satisfy this need, we allow database users to add new table functions and stream-oriented aggregate functions by defining them in SQL---rather than in external procedural languages as O-R DBMSs currently do. This simple extension turns SQL into a powerful database language, which can express a wide range of applications, including recursive queries, ROLAP aggregates, time-series queries, stream-oriented processing, and data mining functions. In addition to adding great power and flexibility to SQL, these extensions are conducive to performance and data independence. The paper also discusses the system we have developed to support these SQL extensions, and the architecture and techniques used in its realization.
46|Aurora: a new model and architecture for data stream management|This paper describes the basic processing model and architecture of Aurora, a new system to manage data streams for monitoring applications. Monitoring applications differ substantially from conventional business data processing. The fact that a software system must process and react to continual inputs from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area. In this paper, we present Aurora, a new DBMS currently under construction at Brandeis University, Brown University, and M.I.T. We first provide an overview of the basic Aurora model and architecture and then describe in detail a stream-oriented set of operators.
47|Maintenance of Materialized Views: Problems, Techniques, and Applications|In this paper we motivate and describe materialized views, their applications, and the problems  and techniques for their maintenance. We present a taxonomy of view maintenanceproblems  basedupon the class of views considered, upon the resources used to maintain the view, upon the types of modi#cations to the base data that areconsidered during maintenance, and whether the technique works for all instances of databases and modi#cations. We describe some of the view maintenancetechniques proposed in the literature in terms of our taxonomy. Finally, we consider new and promising application domains that are likely to drive work in materialized  views and view maintenance.  1 Introduction  What is a view? A view is a derived relation de#ned in terms of base #stored# relations. A view thus de#nes a function from a set of base tables to a derived table; this function is typically recomputed every time the view is referenced.  What is a materialized view? A view can be materialized by storin...
48|The Design, Implementation and Evaluation of SMART: A Scheduler for Multimedia Applications|This paper argues for the need to design a new processor scheduling algorithm that can handle the mix of applications we see today. We present a scheduling algorithm which we have implemented in the Solaris UNIX operating system [Eykholt et al. 1992], and demonstrate its improved performance over existing schedulers in research and practice on real applications. In particular, we have quantitatively compared against the popular weighted fair queueing and UNIX SVR4 schedulers in supporting multimedia applications in a realistic workstation environment...
49|An Adaptive Query Execution System for Data Integration|Query processing in data integration occurs over networkbound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the  Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila  to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that the Tukwila architecture extends previous innovations in adaptive execution (such as...
50|Main memory database systems: An overview|Abstract-Memory resident database systems (MMDB’s) store their data in main physical memory and provide very high-speed access. Conventional database systems are optimized for the particular characteristics of disk storage mechanisms. Memory resident systems, on the other hand, use different optimizations to structure and organize data, as well as to make it reliable. This paper surveys the major memory residence optimizations and briefly discusses some of the memory resident systems that have been designed or implemented. Index Terms- Access methods, application programming in-terface, commit processing, concurrency control, data clustering, data representation, main memory database system (MMDB), query processing, recovery. Invited Paper I.
51|Temporal and Real-Time Databases: A Survey|A temporal database contains time-varying data. In a real-time database transactions have deadlines or timing constraints. In this paper we review the substantial research in these two heretofore separate research areas. We first characterize the time domain, then investigate temporal and real-time data models. We evaluate temporal and real-time query languages along several dimensions. Temporal and real-time DBMS implementation is examined. We conclude with a summary of the major accomplishments of the research to date, and list several research questions that should be addressed next. Keywords: object-oriented database, relational databases, query language, temporal data model, time-constrained database, transaction time, user-defined time, valid time 1 Introduction  Time is an important aspect of all real-world phenomena. Events occur at specific points in time; objects and the relationships among objects exist over time. The ability to model this temporal dimension of the real worl...
52|Active Database Systems|, Exception, Clock, Externalg Granularity ae fMember, Subset, Setg Type ae fPrimitive, Composite g Operators ae for, and, seq, closure, times, not g Consumption mode ae fRecent, Chronicle, Cumulative, Continuous g Role 2 fMandatory, Optional, Noneg Condition Role 2 fMandatory, Optional, Noneg Context ae fDB T , BindE , DBE , DBC g Action Options ae fStructure Operation, Behavior Invocation, Update-Rules, Abort Inform, External, Do Instead g Context ae fDB T , BindE , BindC , DBE , DBC , DBA g ---behavior invocation, in which case the event is raised by the execution of some user-defined operation (e.g. the message display is sent to an object of type widget). It is common for event languages to allow events to be raised before or after an operation has been executed. ---transaction, in which case the event is raised by transaction commands (e.g. abort, commit, begin-transaction) ---abstract or user-defined, in which case a programming mechanism is used that allows an appli...
53|Query Processing, Approximation, and Resource Management In a Data Stream Management System|This paper describes our ongoing work developing the Stanford Stream Data Manager (STREAM), a system for executing continuous queries over multiple continuous data streams. The STREAM system supports a declarative query language, and it copes with high data rates and query workloads by providing approximate answers when resources are limited. This paper describes specific contributions made so far and enumerates our next steps in developing a general-purpose Data Stream Management System.
55|SEQ: A Model for Sequence Databases|This paper presents the model which is the basis for a system to manage various kinds of sequence data. The model separates the data from the ordering information, and includes operators based on two distinct abstractions of a sequence. The main contributions of the model are: (a) it can deal with different types of sequence data, (b) it supports an expressive range of sequence queries, (c) it draws from many of the diverse existing approaches to modeling sequence data. 1
56|Stream Control Transmission Protocol|This document is an Internet-Draft and is in full conformance with all provisions of Section 10 of RFC 2026. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months and may be updated, replaced, or obsoleted by other documents at any time. It is inappropriate to use Internet- Drafts as reference material or to cite them other than as ‘‘work in progress.’’ The list of current Internet-Drafts can be accessed at
57|On Estimating End-to-End Network Path Properties|The more information about current network conditions available to a transport protocol, the more efficiently it can use the network to transfer its data. In networks such as the Internet, the transport protocol must often form its own estimates of network properties based on measurements performed by the connection endpoints. We consider two basic transport estimation problems: determining the setting of the retransmission timer (RTO) for a reliable protocol, and estimating the bandwidth available to a connection as it begins. We look at both of these problems in the context of TCP, using a large TCP measurement set [Pax97b] for trace-driven simulations. For RTO estimation, we evaluate a number of different algorithms, finding that the performance of the estimators is dominated by their minimum values, and to a lesser extent, the timer granularity, while being virtually unaffected by how often round-trip time measurements are made or the settings of the parameters in the exponentially-weighted moving average estimators commonly used. For bandwidth estimation, we explore techniques previously sketched in the literature [Hoe96, AD98] and find that in practice they perform less well than anticipated. We then develop a receiver-side algorithm that performs significantly better. 1
59|TCP congestion control with a misbehaving receiver|In this paper, we explore the operation of TCP congestion control when the receiver can misbehave, as might occur with a greedy Web client. We first demonstrate that there are simple attacks that allow a misbehaving receiver to drive a standard TCP sender arbitrarily fast, without losing end-to-end reliability. These attacks are widely applicable because they stem from the sender behavior specified in RFC 2581 rather than implementation bugs. We then show that it is possible to modify TCP to eliminate this undesirable behavior entirely, without requiring assumptions of any kind about receiver behavior. This is a strong result: with our solution a receiver can only reduce the data transfer rate by misbehaving, thereby eliminating the incentive to do so. 1
60|Randomness Requirements for Security|This document is intended to become a Best Current Practice. Comments should be sent to the authors. Distribution is unlimited. This document is an Internet-Draft and is in full conformance with all provisions of Section 10 of RFC 2026. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months and may be updated, replaced, or obsoleted by other documents at any time. It is inappropriate to use Internet-Drafts as reference material or to cite them other than as &#034;work in progress. &#034; The list of current Internet-Drafts can be accessed at http://www.ietf.org/ietf/1id-abstracts.txt The list of Internet-Draft Shadow Directories can be accessed at
61|Models and issues in data stream systems|In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues. 
62|Randomized Algorithms|Randomized algorithms, once viewed as a tool in computational number theory, have by now found widespread application. Growth has been fueled by the two major benefits of randomization: simplicity and speed. For many applications a randomized algorithm is the fastest algorithm available, or the simplest, or both. A randomized algorithm is an algorithm that uses random numbers to influence the choices it makes in the course of its computation. Thus its behavior (typically quantified as running time or quality of output) varies from
63|Multiparty Communication Complexity|A given Boolean function has its input distributed among many parties. The aim is to determine which parties to tMk to and what information to exchange with each of them in order to evaluate the function while minimizing the total communication. This paper shows that it is possible to obtain the Boolean answer deterministically with only a polynomial increase in communication with respect to the information lower bound given by the nondeterministic communication complexity of the function.
64|Fast subsequence matching in time-series databases|We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an ecient and eective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.  
65|Mining high-speed data streams|Categories and Subject ?????????? ? ?¨?????????????????????????¦???¦??????????¡¤?? ¡  ? ¡????????????????¦¡¤????§?£????
66|Online Aggregation|Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in postgres. 1 Introduction  Aggregation is an incre...
67|Efficient Filtering of XML Documents for Selective Dissemination of Information|Information Dissemination applications are gaining increasing  popularity due to dramatic improvements in  communications bandwidth and ubiquity. The sheer  volume of data available necessitates the use of selective  approaches to dissemination in order to avoid overwhelming  users with unnecessaryinformation. Existing  mechanisms for selective dissemination typically rely  on simple keyword matching or &#034;bag of words&#034; information  retrieval techniques. The advent of XML as a  standard for information exchangeand the development  of query languages for XML data enables the development  of more sophisticated filtering mechanisms that  take structure information into account. We have developed  several index organizations and search algorithms  for performing efficient filtering of XML documents for  large-scale information dissemination systems. In this  paper we describe these techniques and examine their  performance across a range of document, workload, and  scale scenarios.  1 
68|External Memory Algorithms and Data Structures|Data sets in large applications are often too massive to fit completely inside the computer&#039;s internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this paper, we survey the state of the art in the design and analysis of external memory algorithms and data structures (which are sometimes referred to as &#034;EM&#034; or &#034;I/O&#034; or &#034;out-of-core&#034; algorithms and data structures). EM algorithms and data structures are often designed and analyzed using the parallel disk model (PDM). The three machine-independent measures of performance in PDM are the number of I/O operations, the CPU time, and the amount of disk space. PDM allows for multiple disks (or disk arrays) and parallel CPUs, and it can be generalized to handle tertiary storage and hierarchical memory. We discuss several important paradigms for how to solve batched and online problems efficiently in external memory. Programming tools and environments are available for simplifying the programming task. The TPIE system (Transparent Parallel I/O programming Environment) is both easy to use and efficient in terms of execution speed. We report on some experiments using TPIE in the domain of spatial databases. The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.  
69|Random sampling with a reservoir|We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O(n(1 + log(N/n))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.
70|Stable Distributions, Pseudorandom Generators, Embeddings and Data Stream Computation|In this paper we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular: ffl we show how to maintain (using only O(log n=ffl  2  ) words of storage) a sketch C(p) of a point p 2 l  n  1 under dynamic updates of its coordinates, such that given sketches C(p) and C(q) one can estimate jp \Gamma qj 1 up to a factor of (1 + ffl) with large probability. This solves the main open problem of [10].  ffl we obtain another sketch function C  0  which maps l  n  1 into a normed space l  m  1 (as opposed to C), such that m = m(n) is much smaller than n; to our knowledge this is the first dimensionality reduction lemma for l 1 norm  ffl we give an explicit embedding of l  n  2 into l  n  O(log n) 1  with distortion (1 + 1=n  \Theta(1)  ) and a nonconstructive embedding of l  n  2 into l  O(n)  1 with distortion  (1 + ffl) such that the embedding can be represented using only O(n log  2  n) bits (as opposed to at least...
71|Continuous Queries over Data Streams|In many recent applications, data may take the form of continuous data streams, rather than finite stored data sets. Several aspects of data management need to be re-considered in the presence of data streams, offering a new research direction for the database community. In this pa-per we focus primarily on the problem of query processing, specifically on how to define and evaluate continuous queries over data streams. We address semantic issues as well as efficiency concerns. Our main contributions are threefold. First, we specify a general and flexible architecture for query processing in the presence of data streams. Second, we use our basic architecture as a tool to clarify alternative semantics and processing techniques for continuous queries. The architecture also captures most previous work on continuous queries and data streams, as
72|Multiple-Query Optimization|Some recently proposed extensions to relational database systems, as well as to deductive database systems, require support for multiple-query processing. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions may expand to more than one actual query that has to be run over the database. It is an interesting problem then to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of the paper is a systematic look at the problem, along with the presentation and analysis of algorithms that can be used for multiple-query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably.
73|Trajectory Sampling for Direct Traffic Observation|Traffic measurement is a critical component for the control and engineering of communication networks. We argue that traffic measurement should make it possible to obtain the spatial flow of traffic through the domain, i.e., the paths followed by packets between any ingress and egress point of the domain. Most resource allocation and capacity planning tasks can benefit from such information. Also, traffic measurements should be obtained without a routing model and without knowledge of network state. This allows the traffic measurement process to be resilient to network failures and state uncertainty.  We propose a method that allows the direct inference of traffic flows through a domain by observing the trajectories of a subset of all packets traversing the network. The key advantages of the method are that (i) it does not rely on routing state, (ii) its implementation cost is small, and (iii) the measurement reporting traffic is modest and can be controlled precisely. The key idea of the method is to sample packets based on a hash function computed over the packet content. Using the same hash function will yield the same sample set of packets in the entire domain, and enables us to reconstruct packet trajectories.  I. 
74|  Wavelet-Based Histograms for Selectivity Estimation |Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations. In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data values give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using
75|Approximate Query Processing Using Wavelets|Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today’s decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing
76| Approximate Computation of Multidimensional Aggregates of Sparse Data Using Wavelets |Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries. In this paper, we present anovel method that provides approximate answers to high-dimensional OLAP aggregation queries in massive sparse data sets in a time-efficient and space-efficient manner. We construct a compact data cube, which is an approximate and space-efficient representation of the underlying multidimensional array, based upon a multiresolution wavelet decomposition. In the on-line phase, each aggregation query can generally be answered using the compact data cube in one I/O or a small number of I/Os, depending upon the desired accuracy. We present two I/O-efficient algorithms to construct the compact data cube for the important case of sparse high-dimensional arrays, which often arise in practice. The traditional histogram methods are infeasible for the massive high-dimensional data sets in OLAP applications. Previously developed wavelet techniques are efficient only for dense data. Our on-line query processing algorithm is very fast and capable of refining answers as the user demands more accuracy. Experiments on real data show that our method provides significantly more accurate results for typical OLAP aggregation queries than other efficient approximation techniques such as random sampling.
77|Computing on Data Streams|In this paper we study the space requirement of algorithms that make  only one (or a small number of) pass(es) over the input data. We study such  algorithms under a model of data streams that we introduce here. We give  a number of upper and lower bounds for problems stemming from queryprocessing,  invoking in the process tools from the area of communication  complexity.
78|Continual Queries for Internet Scale Event-Driven Information Delivery|In this paper we introduce the concept of continual queries, describe the design of a distributed  event-driven continual query system -- OpenCQ, and outline the initial implementation of OpenCQ  on top of the distributed interoperable information mediation system DIOM [21, 19]. Continual  queries are standing queries that monitor update of interest and return results whenever the update  reaches specified thresholds. In OpenCQ, users may specify to the system the information they would  like to monitor (such as the events or the update thresholds they are interested in). Whenever the  information of interest becomes available, the system immediately delivers it to the relevant users;  otherwise, the system continually monitors the arrival of the desired information and pushes it to the  relevant users as it meets the specified update thresholds. In contrast to conventional pull-based data  management systems such as DBMSs and Web search engines, OpenCQ exhibits two important featu...
79|Join synopses for approximate query answering|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses (join samples) as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known. We also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations. One of our key contributions is a detailed analysis of the error bounds obtained for approximate answers that demonstrates the trade-offs in various methods, as well as the advantages in certain scenarios of a new subsampling method we propose. Our extensive set of experiments on the TPC-D benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper. 1
80|An efficient cost-driven index selection tool for Microsoft SQL Server|In this paper we describe novel techniques that make it possible to build an industrial-strength tool for automating the choice of indexes in the physical design of a SQL database. The tool takes as input a workload of SQL queries, and suggests a set of suitable indexes. We ensure that the indexes chosen are effective in reducing the cost of the workload by keeping the index selection tool and the query optimizer &amp;quot;in step&amp;quot;. The number of index sets that must be evaluated to find the optimal configuration is very large. We reduce the complexity of this problem using three techniques. First, we remove a large number of spurious indexes from consideration by taking into account both query syntax and cost information. Second, we introduce optimizations that make it possible to cheaply evaluate the “goodness ” of an index set. Third, we describe an iterative approach to handle the complexity arising from multicolumn indexes. The tool has been implemented on Microsoft SQL Server 7.0. We performed extensive experiments over a range of workloads, including TPC-D. The results indicate that the tool is efficient and its choices are close to optimal. 1.
81|Reductions in Streaming Algorithms, with an Application to Counting Triangles in Graphs |We introduce reductions in the streaming model as a tool in the design of streaming algorithms. We develop
82|Computing iceberg queries efficiently|Many applications compute aggregate functions...
83|Data-Streams and Histograms|Histograms have been used widely to capture data distribution, to represent the data by a small number of step functions. Dynamic programming algorithms which provide optimal construction of these histograms exist, albeit running in quadratic time and linear space. In this paper we provide linear time construction of 1 + epsilon approximation of optimal histograms, running in polylogarithmic space. Our results extend to the context of data-streams, and in fact generalize to give 1 + epsilon approximation of several problems in data-streams which require partitioning the index set into intervals. The only assumptions required are that the cost of an interval is monotonic under inclusion (larger interval has larger cost) and that the cost can be computed or approximated in small space. This exhibits a nice class of problems for which we can have near optimal data-stream algorithms.
84|Making Views Self-Maintainable for Data Warehousing|A data warehouse stores materialized views over data from one or more sources in order to provide fast access to the integrated data, regardless of the availability of the data sources. Warehouse views need to be maintained inresponse to changes to the base data in the sources. Except for very simple views, maintaining a warehouse view requires access to data that is not available in the view itself. Hence, to maintain the view, one either has to query the data sources or store auxiliary data in the warehouse. We show that by using key and referential integrity constraints, we often can maintain a select-project-join view without going to the data sources or replicating the base relations in their entirety in the warehouse. We derive a set of auxiliary views such that the warehouse view and the auxiliary views together are self-maintainable|they can be maintained without going to the data sources or replicating all base data. In addition, our technique can be applied to simplify traditional materialized view maintenance by exploiting key and referential integrity constraints. 1
85|Approximate Medians and other Quantiles in One Pass and with Limited Memory|We present new algorithms for computing approximate quantiles of large datasets in a single pass. The approximation guarantees are explicit, and apply without regard to the value distribution or the arrival distributions of the dataset. The main memory requirements are smaller than those reported earlier by an order of magnitude.  We also discuss methods that couple the approximation algorithms with random sampling to further reduce memory requirements. With sampling, the approximation guarantees are explicit but probabilistic, i.e., they apply with respect to a (user controlled) confidence parameter.  We present the algorithms, their theoretical analysis and simulation results.  1 Introduction  This article studies the problem of computing order statistics  of large sequences of online or disk-resident data using as little main memory as possible. We focus on computing  quantiles, which are elements at specific positions in the sorted order of the input. The OE-quantile, for OE 2 [0; ...
86|Random Sampling for Histogram Construction: How much is enough?|Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining &#034;How much sampling is enough?&#034; We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a  conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose ...
87|Tracking join and self-join sizes in limited storage|This paper presents algorithms for tracking (approximate) join and self-join sizes in limited storage, in the presence of insertions and deletions to the data set(s). Such algorithms detect changes in join and self-join sizes without an expensive recomputation from the base data, and without the large space overhead required to maintain such sizes exactly. Query optimizers rely on fast, high-quality estimates of join sizes in order to select between various join plans, and estimates of self-join sizes are used to indicate the degree of skew in the data. For self-joins, we considertwo approaches proposed in [Alon, Matias, and Szegedy. The Space Complexity of Approximating the Frequency Moments. JCSS, vol. 58, 1999, p.137-147], which we denote tug-of-war and sample-count. Wepresent fast algorithms for implementing these approaches, and extensions to handle deletions as well as insertions. We also report on the rst experimental study of the two approaches, on a range of synthetic and real-world data sets. Our study shows that tug-of-war provides more accurate estimates for a given storage limit than sample-count, which in turn is far more accurate than a standard sampling-based approach. For example, tug-of-war needed only 4{256 memory words, depending on the data set, in order to estimate the self-join size
88|Data Cube Approximation and Histograms via Wavelets (Extended Abstract)  (1998) |) Jeffrey Scott Vitter   Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  jsv@cs.duke.edu Min Wang  y Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  minw@cs.duke.edu Bala Iyer  Database Technology Institute IBM Santa Teresa Laboratory P.O. Box 49023 San Jose, CA 95161 USA  balaiyer@vnet.ibm.com Abstract  There has recently been an explosion of interest in the analysis of data in data warehouses in the field of On-Line Analytical Processing (OLAP). Data warehouses can be extremely large, yet obtaining quick answers to queries is important. In many situations, obtaining the exact answer to an OLAP query is prohibitively expensive in terms of time and/or storage space. It can be advantageous to have fast, approximate answers to queries. In this paper, we present an I/O-efficient technique based upon a multiresolution wavelet decomposition that yields an approximate a...
89|The design and implementation of a sequence database system|This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. There are three distinct contributions made in this paper. (1) We describe the specification of sequence queries using the SEQUIN query language. (2) We quantitatively demonstrate the importance of various storage and optimization techniques by studying their effect on performance. (3) We present a novel nested design paradigm used in PREDATOR to combine sequence ‘and relational data.
90|Congressional samples for approximate answering of group-by queries|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex decision support queries using precomputed summary statistics, such as samples. Decision support queries routinely segment the data into groups and then aggregate the information in each group (group-by queries). Depending on the data, there can be a wide disparity between the number of data items in each group. As a result, approximate answers based on uniform random samples of the data can result in poor accuracy for groups with very few data items, since such groups will be represented in the sample by very few (often zero) tuples. In this paper, we propose a general class of techniques for obtaining fast, highly-accurate answers for group-by queries. These techniques rely on precomputed non-uniform (biased) samples of the data. In particular, we proposecongressional samples, ahybrid union of uniform and biased samples. Given a xed amount of space, congressional samples seek to maximize the accuracy for all possible group-by queries on a set of columns. We present a one pass algorithm for constructing a congressional sample and use this technique to also incrementally maintain the sample up-to-date without accessing the base relation. We also evaluate query rewriting strategies for providing approximate answers from congressional samples. Finally, we conduct an extensive set of experiments on the TPC-D database, which demonstrates the e cacy of the techniques proposed. 1
91|Histogram-Based Approximation of Set-Valued Query Answers|Answering queries approximately has recently  been proposed as a way to reduce query response  times in on-line decision support systems,  when the precise answer is not necessary  or early feedback is helpful. Most of the  work in this area uses sampling-based techniques  and handles aggregate queries, ignoring  queries that return relations as answers. In  this paper, we extend the scope of approximate  query answering to general queries. We propose  a novel and intuitive error measure for  quantifying the error in an approximate query  answer, which can be a multiset in general.
92|Data Integration using Self-Maintainable Views|.  In this paper we de#ne the concept of self-maintainable views  # these are views that can be maintained using only the contents of  the view and the database modi#cations, without accessing any of the  underlying databases. We derive tight conditions under which several  types of select-project-join are self-maintainable upon insertions, deletions  and updates. Self-Maintainability is a desirable property for e#-  ciently maintaining large views in applications where fast response and  high availability are important. One example of suchanenvironment  is data warehousing wherein views are used for integrating data from  multiple databases.  1 Introduction  Most large organizations have related data in distinct databases. Many of these databases may be legacy systems, or systems separated for organizational reasons like funding and ownership. Integrating data from such distinct databases is a pressing business need. A common approach for integration is to de#ne an integrated view and...
93|Monitoring XML Data on the Web|We consider the monitoring of a flow of incoming documents. More precisely, we present here the monitoring used in a very large warehouse built from XML documents found on the web. The flow of documents consists in XML pages (that are warehoused) and HTML pages (that are not). Our contributions are the following:  ffl a subscription language which specifies the monitoring of pages when fetched, the periodical evaluation of continuous queries and the production of XML reports. ffl the description of the architecture of the system we implemented that makes it possible to monitor a flow of millions of pages per day with millions of subscriptions on a single PC, and scales up by using more machines. ffl a new algorithm for processing alerts that can be used in a wider context. We support
95|Expiring data in a warehouse|Data warehouses collect data into materi-alized views for analysis. After some time, some of the data may no longer be needed or may not be of interest. In this pa-per, we handle this by expiring or remov-ing unneeded materialized view tuples. A framework supporting such expiration is presented. Within it, a user or adminis-trator can declaratively request expirations and can specify what type of modifications are expected from external sources. The lat-ter can significantly increase the amount of data that can be expired. We present effi-cient algorithms for determining what data can be expired (data not needed for main-tenance of other views), taking into account the types of updates that may occur. 1
97|Sequence Query Processing|Many applications require the ability to manipulate sequences of data. We motivate the importance of sequence query processing, and present a framework for the optimization of sequence queries based on several novel techniques. These include query transformations, optimizations that utilize meta--data, and caching of intermediate results. We present a bottom-up algorithm that generates an efficient query evaluation plan based on cost estimates. This work also identifies a number of directions in which future research can be directed.  1 Introduction  Many real life applications manipulate data that is inherently sequential. Such data is logically viewed and queried in terms of a sequence abstraction and is often physically stored as a sequence. Databases should (a) allow sequences to be queried in a declarative manner, utilizing the ordered semantics of the data, and (b) take advantage of the opportunities available for query optimization. Relational databases are inadequate in this re...
98|Sampling Algorithms: Lower Bounds and Applications (Extended Abstract)  (2001) |]  Ziv Bar-Yossef  y  Computer Science Division  U. C. Berkeley  Berkeley, CA 94720  zivi@cs.berkeley.edu  Ravi Kumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  ravi@almaden.ibm.com  D. Sivakumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  siva@almaden.ibm.com  ABSTRACT  We develop a framework to study probabilistic sampling algorithms that approximate general functions of the form f : A  n  ! B, where A and B are arbitrary sets. Our goal is to obtain lower bounds on the query complexity of functions, namely the number of input variables x i that any sampling algorithm needs to query to approximate f(x1 ; : : : ; xn ).  We define two quantitative properties of functions --- the block sensitivity and the minimum Hellinger distance --- that give us techniques to prove lower bounds on the query complexity. These techniques are quite general, easy to use, yet powerful enough to yield tight results. Our applications include the mean and higher statistical moments, the median and other selection functions, and the frequency moments, where we obtain lower bounds that are close to the corresponding upper bounds.  We also point out some connections between sampling and streaming algorithms and lossy compression schemes.  1. 
99|Approximating a Data Stream for Querying and Estimation: Algorithms and Performance Evaluation|Obtaining fast and good quality approximations to data distributions is a problem of central interest to database management. A variety of popular database applications including, approximate querying, similarity searching and data mining in most application domains, rely on such good quality approximations. Histogram based approximation is a very popular method in database theory and practice to succinctly represent a data distribution in a space efficient manner. In this paper, we place the problem of histogram construction into perspective and we generalize it by raising the requirement of a finite data set and/or known data set size. We consider the case of an infinite data set on which data arrive continuously forming an infinite data stream. In this context, we present the first single pass algorithms capable of constructing histograms of provable good quality. We present algorithms for the fixed window variant of the basic histogram construction problem, supporting incremental maintenance of the histograms. The proposed algorithms trade accuracy for speed and allow for a graceful tradeoff between the two, based on application requirements. In the case of approximate queries on infinite data streams, we present a detailed experimental evaluation comparing our algorithms with other applicable techniques using real data sets, demonstrating the superiority of our proposal. 1
100|A robust, optimization-based approach for approximate answering of aggregate queries|The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are “similar ” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work. 1
101|Online Dynamic Reordering for Interactive Data Processing|Abstract We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing.
103|On Sampling and Relational Operators|A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. We highlight the primary difficulties, summarize the results of some recent work in this area, and indicate directions for future work.   
104|SplitStream: High-Bandwidth Multicast in Cooperative Environments|In tree-based multicast systems, a relatively small number of interior nodes carry the load of forwarding multicast messages. This works well when the interior nodes are highly available, d d cated infrastructure routers but it poses a problem for application-level multicast in peer-to-peer systems. SplitStreamadV esses this problem by striping the content across a forest of interior-nodno# sjoint multicast trees that d stributes the forward ng load among all participating peers. For example, it is possible to construct efficient SplitStream forests in which each peer contributes only as much forwarding bandH d th as it receives. Furthermore, with appropriate content encod ngs, SplitStream is highly robust to failures because a nod e fai ure causes the oss of a single stripe on average. We present thed#&#039; gnand implementation of SplitStream and show experimental results obtained on an Internet testbed and via large-scale network simulation. The results show that SplitStreamd istributes the forward ing load among all peers and can accommod&#039;9 peers with different band0 d capacities while imposing low overhead for forest constructionand maintenance. 
105|Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications|A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.
106|SCRIBE: A large-scale and decentralized application-level multicast infrastructure|This paper presents Scribe, a scalable application-level multicast infrastructure. Scribe supports large numbers of groups, with a potentially large number of members per group. Scribe is built on top of Pastry, a generic peer-to-peer object location and routing substrate overlayed on the Internet, and leverages Pastry&#039;s reliability, self-organization, and locality properties. Pastry is used to create and manage groups and to build efficient multicast trees for the dissemination of messages to each group. Scribe provides best-effort reliability guarantees, but we outline how an application can extend Scribe to provide stronger reliability. Simulation results, based on a realistic network topology model, show that Scribe scales across a wide range of groups and group sizes. Also, it balances the load on the nodes while achieving acceptable delay and link stress when compared to IP multicast.
107|Overcast: Reliable Multicasting with an Overlay Network|Overcast is an application-level multicasting system that can be incrementally deployed using today&#039;s Internet infrastructure. These properties stem from Overcast&#039;s implementation as an overlay network. An overlay network consists of a collection of nodes placed at strategic locations in an existing network fabric. These nodes implement a network abstraction on top of the network provided by the underlying substrate network. Overcast  provides
108|Bayeux: An architecture for scalable and fault-tolerant wide-area data dissemination|The demand for streaming multimedia applications is growing at an incredible rate. In this paper, we propose Bayeux, an efficient application-level multicast system that scales to arbitrarily large receiver groups while tolerating failures in routers and network links. Bayeux also includes specific mechanisms for load-balancing across replicate root nodes and more efficient bandwidth consumption. Our simulation results indicate that Bayeux maintains these properties while keeping transmission overhead low. To achieve these properties, Bayeux leverages the architecture of Tapestry, a fault-tolerant, wide-area overlay routing and location network.
109|Distributing Streaming Media Content Using Cooperative Networking|In this paper, we discuss the problem of distributing streaming media content, both live and on-demand, to a large number of hosts in a scalable way. Our work is set in the context of the traditional client-server framework. Specifically, we consider the problem that arises when the server is overwhelmed by the volume of requests from its clients. As a solution, we propose Cooperative Networking (CoopNet), where clients cooperate to distribute content, thereby alleviating the load on the server. We discuss the proposed solution in some detail, pointing out the interesting research issues that arise, and present a preliminary evaluation using traces gathered at a busy news site during the flash crowd that occurred on September 11, 2001.
110|Application-Level Multicast Using Content-Addressable Networks|Most currently proposed solutions to application-level multicast organize  the group members into an application-level mesh over which a DistanceVector  routing protocol, or a similar algorithm, is used to construct source-rooted  distribution trees. The use of a global routing protocol limits the scalability of  these systems. Other proposed solutions that scale to larger numbers of receivers  do so by restricting the multicast service model to be single-sourced. In this paper,  we propose an application-level multicast scheme capable of scaling to large  group sizes without restricting the service model to a single source. Our scheme  builds on recent work on Content-Addressable Networks (CANs). Extending the  CAN framework to support multicast comes at trivial additional cost and, because  of the structured nature of CAN topologies, obviates the need for a multicast  routing algorithm. Given the deployment of a distributed infrastructure such as a  CAN, we believe our CAN-based multicast scheme offers the dual advantages of  simplicity and scalability.
111|Informed content delivery across adaptive overlay networks|Abstract—Overlay networks have emerged as a powerful and highly flexible method for delivering content. We study how to optimize throughput of large transfers across richly connected, adaptive overlay networks, focusing on the potential of collaborative transfers between peers to supplement ongoing downloads. First, we make the case for an erasure-resilient encoding of the content. Using the digital fountain encoding approach, end hosts can efficiently reconstruct the original content of size from a subset of any symbols drawn from a large universe of encoding symbols. Such an approach affords reliability and a substantial degree of application-level flexibility, as it seamlessly accommodates connection migration and parallel transfers while providing resilience to packet loss. However, since the sets of encoding symbols acquired by peers during downloads may overlap substantially, care must be taken to enable them to collaborate effectively. Our main contribution is a collection of useful algorithmic tools for efficient summarization and approximate reconciliation of sets of symbols between pairs of collaborating peers, all of which keep message complexity and computation to a minimum. Through simulations and experiments on a prototype implementation, we demonstrate the performance benefits of our informed content-delivery mechanisms and how they complement existing overlay network architectures. Index Terms—Bloom filter, content delivery, digital fountain, erasure code, min-wise sketch, overlay, peer-to-peer, reconciliation. I.
112|An Evaluation of Scalable Application-Level Multicast Built Using Peer-to-Peer Overlays|Structured peer-erg163 overlay networks such as CAN, Chord, Pastry, and Tapestry can be used to implement Internet-g683 application-3 vel multicast. There are two general approaches to accomplishingthis: tree buildingand flooding. This paper evaluates these two approaches usingtwo different types of structured overlay: 1) overlays which use a form of generalized hypercube routing, e.g., Chord, Pastry and Tapestry, and 2) overlays which use a numerical distance metric to route through a Cartesian hyper-erg15 e.g., CAN. Pastry and CAN are chosen as the representatives of each type of overlay. To the best of our knowledge, this paper reports the firstheadto -d- comparison ofCAN-B91g versus Pastry-gZ4B overlay networks, usingmulticast communication workloads runningon an identical simulation infrastructure. The two approaches to multicast are independent of overlay network choice, and we provide a comparison of floodingversus tree-2696 multicast on both overlays. Results show that the tree-2613 approach consistently outperforms the floodingapproach. Finally, for treebased multicast, we show that Pastry provides better performance than CAN.
113|Exploiting network proximity in peer-to-peer overlay networks|In CAN, each node measures its network delay to a set of
114|Distributed Video Streaming with Forward Error Correction|With the explosive growth of video applications over the Internet, many approaches have been proposed to stream video effectively over packet switched, best-effort networks. Many use techniques from source and channel coding, or implement transport protocols, or modify system architectures in order to deal with delay, loss, and time-varying nature of the Internet. In our previous work, we proposed a framework with a receiver driven protocol to coordinate simultaneous video streaming from multiple senders to a single receiver in order to achieve higher throughput, and to increase tolerance to packet loss and delay due to network congestion. The receiver-driven protocol employs two algorithms: rate allocation and packet partition. The rate allocation algorithm determines the sending rate for each sender; the packet partition algorithm ensures no senders send the same packets, and at the same time, minimizes the probability of late packets. In this paper, we propose a novel rate allocation scheme to be used with Forward Error Correction (FEC) in order to minimize the probability of packet loss in bursty loss environments such as those caused by network congestion. Using both simulations and actual Internet experiments, we demonstrate the effectiveness of our rate allocation scheme in reducing packet loss, and hence, achieving higher visual quality for the streamed video.
115|Using Random Subsets to Build Scalable Network Services|In this paper, we argue that a broad range of large-scale network services would benefit from a scalable mechanism for delivering state about a random subset of global participants. Key to this approach is ensuring that membership in the subset changes periodically and with uniform representation over all participants. Random subsets could help overcome inherent scaling limitations to services that maintain global state and perform global network probing. It could further improve the routing performance of peer-to-peer distributed hash tables by locating topologically-close nodes. This paper presents the design, implementation, and evaluation of RanSub, a scalable protocol for delivering such state.
116|Unbalanced Multiple Description Video Communication Using Path Diversity|Multiple description (MD) coders provide important error resilience  properties. Specifically, MD coders are designed to provide good performance when the loss is limited to a single description, but it is not known in advance which description. In [1], we combined MD video coding with a path diversity transmission system for packet networks such as the Internet, where different descriptions are explicitly transmitted through different network paths, to improve the effectiveness of MD coding over a packet network by increasing the likelihood that the loss probabilities for each description are independent. The available bandwidth in each path may be similar or different, resulting in the requirement of balanced or unbalanced operation, where the bit rate of each description may differ based on the available bandwidth along its path. We design a MD video communication system that is effective in both balanced and unbalanced operation. Specifically, unbalanced MD streams are created by carefully adjusting the frame rate of each description, thereby achieving unbalanced rates of almost 2:1 while preserving MD&#039;s effectiveness and error recovery capability.  
117|Proximity neighbor selection in tree-based structured peer-to-peer overlays|Structured peer-to-peer (p2p) overlay networks provide a useful substrate for building distributed applications. They assign object keys to overlay nodes and provide a primitive to route a message to the node responsible for a key. Proximity neighbor selection (PNS) can be used to achieve both low delay routes and low bandwidth usage but it introduces high overhead. This paper presents a detailed evaluation of PNS and heuristic approximations. We describe a new heuristic called constrained gossiping (PNS-CG) and show that it achieves performance similar to perfect PNS with low overhead. We also compare constrained gossiping with previous heuristics and show that it achieves better performance with lower overhead.
118|Scalable Application-Level Anycast for Highly Dynamic Groups|We present an application-level implementation of anycast  for highly dynamic groups. The implementation can handle group sizes  varying from one to the whole Internet, and membership maintenance  is e#cient enough to allow members to join for the purpose of receiving  a single message. Key to this e#ciency is the use of a proximity-aware  peer-to-peer overlay network for decentralized, lightweight group maintenance;  nodes join the overlay once and can join and leave many groups  many times to amortize the cost of maintaining the overlay. An anycast  implementation with these properties provides a key building block  for distributed applications. In particular, it enables management and  location of dynamic resources in large scale peer-to-peer systems. We  present several resource management applications that are enabled by  our implementation.
119|Fcast Multicast File Distribution|Reliable data multicast is problematic. ACK/NACK schemes do not scale to large audiences, and simple data replication wastes network bandwidth. Fcast, &#034;file multicasting&#034;, combines multicast with Forward Error Correction (FEC) to address both these problems. Like classic multicast, Fcast scales to large audiences, and like other FEC schemes, it uses bandwidth very efficiently. Some of the benefits of this combination were known previously, but Fcast contributes new caching methods that improve disk throughput, and new optimizations for small file transfers. This paper describes Fcast&#039;s design, implementation, and API.
120|Data stream management and mining |Abstract. This paper provides an introduction to the field of data stream management and mining. The increase of data production in operational information systems prevents from monitoring these systems with the old paradigm of storing data before analyzing it. New approaches have been developed recently to process data ‘on the fly ’ considering they are produced in the form of structured data streams. These approaches cover both querying and mining data.
121|Probabilistic Counting Algorithms for Data Base Applications|This paper introduces a class of probabilistic counting lgorithms with which one can  estimate the number of distinct elements in a large collection of data (typically a large file  stored on disk) in a single pass using only a small additional storage (typically less than a  hundred binary words) and only a few operations per element scanned. The algorithms are  based on statistical observations made on bits of hashed values of records. They are by con-  struction totally insensitive to the replicafive structure of elements in the file; they can be used  in the context of distributed systems without any degradation of performances and prove  especially useful in the context of data bases query optimisation. ; 1985 Academic Press, Inc
122|A Framework for Clustering Evolving Data Streams|The clustering problem is a difficult problem for the data stream domain. This is because the large volumes of data arriving in a stream renders most traditional algorithms too inefficient. In recent years, a...
123|Gigascope: a stream database for network applications|We have developed Gigascope, a stream database for network ap-plications including traffic analysis, intrusion detection, router con-figuration analysis, network research, network monitoring, and and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the AT&amp;T network, including at OC48 routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance is-sues. We conclude with a discussion of stream database research problems we have found in our application. 1.
124|SASE: Complex event processing over streams|RFID technology is gaining adoption on an increasing scale for tracking and monitoring purposes. Wide deployments of RFID devices will soon generate an unprecedented volume of data. Emerging applications require the RFID data to be filtered and correlated for complex pattern detection and transformed to events that provide meaningful, actionable information to end applications. In this work, we design and develop SASE, a com-plex event processing system that performs such data-information transformations over real-time streams. We design a complex event language for specifying application logic for such transformation, devise new query processing techniques to effi-ciently implement the language,  and develop a comprehensive system that collects, cleans, and processes RFID data for deliv-ery of relevant, timely information as well as storing necessary data for future querying. We demonstrate an initial prototype of SASE through a real-world retail management scenario. 1.
125|Using Data Stream Management Systems to analyze Electric Power Consumption Data |Abstract. With the development of AMM (Automatic Metering Management), it will be possible for electric power suppliers to acquire from the customers their electric power consumption up to every second. This will generate data arriving in multiple, continuous, rapid, and time-varying data streams. Data Stream Management Systems (DSMS)- currently available as prototypes- aim at facilitating the management of such data streams. This paper describes an experimental study which analyzes the advantages and limitations of using a DSMS for the management of electric power consumption data.
126|CoolStreaming/DONet: A Data-driven Overlay Network for Peer-to-Peer Live Media Streaming|This paper presents DONet, a Data-driven Overlay Network for live media streaming. The core operations in DONet are very simple: every node periodically exchanges data availability information with a set of partners, and retrieves unavailable data from one or more partners, or supplies available data to partners. We emphasize three salient features of this data-driven design: 1) easy to implement, as it does not have to construct and maintain a complex global structure; 2) efficient, as data forwarding is dynamically determined according to data availability while not restricted by specific directions; and 3) robust and resilient, as the partnerships enable adaptive and quick switching among multi-suppliers. We show through analysis that DONet is scalable with bounded delay. We also address a set of practical challenges for realizing DONet, and propose an efficient member- and partnership management algorithm, together with an intelligent scheduling algorithm that achieves real-time and continuous distribution of streaming contents.
127|A Case for End System Multicast|Abstract — The conventional wisdom has been that IP is the natural protocol layer for implementing multicast related functionality. However, more than a decade after its initial proposal, IP Multicast is still plagued with concerns pertaining to scalability, network management, deployment and support for higher layer functionality such as error, flow and congestion control. In this paper, we explore an alternative architecture that we term End System Multicast, where end systems implement all multicast related functionality including membership management and packet replication. This shifting of multicast support from routers to end systems has the potential to address most problems associated with IP Multicast. However, the key concern is the performance penalty associated with such a model. In particular, End System Multicast introduces duplicate packets on physical links and incurs larger end-to-end delays than IP Multicast. In this paper, we study these performance concerns in the context of the Narada protocol. In Narada, end systems selforganize into an overlay structure using a fully distributed protocol. Further, end systems attempt to optimize the efficiency of the overlay by adapting to network dynamics and by considering application level performance. We present details of Narada and evaluate it using both simulation and Internet experiments. Our results indicate that the performance penalties are low both from the application and the network perspectives. We believe the potential benefits of transferring multicast functionality from end systems to routers significantly outweigh the performance penalty incurred. I.
128|Scalable Application Layer Multicast|We describe a new scalable application-layer multicast protocol, specifically designed for low-bandwidth, data streaming applications with large receiver sets. Our scheme is based upon a hierarchical clustering of the application-layer multicast peers and can support a number of different data delivery trees with desirable properties. We present extensive simulations of both our protocol and the Narada application-layer multicast protocol over Internet-like topologies. Our results show that for groups of size 32 or more, our protocol has lower link stress (by about 25%), improved or similar endto-end latencies and similar failure recovery properties. More importantly, it is able to achieve these results by using orders of magnitude lower control traffic. Finally, we present results from our wide-area testbed in which we experimented with 32-100 member groups distributed over 8 different sites. In our experiments, averagegroup members established and maintained low-latency paths and incurred a maximum packet loss rate of less than 1 % as members randomly joined and left the multicast group. The average control overhead during our experiments was less than 1 Kbps for groups of size 100.
129|Bullet: High Bandwidth Data Dissemination Using an Overlay Mesh|In recent years, overlay networks have become an effective alternative to IP multicast for efficient point to multipoint communication across the Internet. Typically, nodes self-organize with the goal of forming an efficient overlay tree, one that meets performance targets without placing undue burden on the underlying network. In this paper, we target high-bandwidth data distribution from a single source to a large number of receivers. Applications include large-file transfers and real-time multimedia streaming. For these applications, we argue that an overlay mesh, rather than a tree, can deliver fundamentally higher bandwidth and reliability relative to typical tree structures. This paper presents Bullet, a scalable and distributed algorithm that enables nodes spread across the Internet to self-organize into a high bandwidth overlay mesh. We construct Bullet around the insight that data should be distributed in a disjoint manner to strategic points in the network. Individual Bullet receivers are then responsible for locating and retrieving the data from multiple points in parallel. Key contributions of this work include: i) an algorithm that sends data to di#erent points in the overlay such that any data object is equally likely to appear at any node, ii) a scalable and decentralized algorithm that allows nodes to locate and recover missing data items, and iii) a complete implementation and evaluation of Bullet running across the Internet and in a large-scale emulation environment reveals up to a factor two bandwidth improvements under a variety of circumstances. In addition, we find that, relative to tree-based solutions, Bullet reduces the need to perform expensive bandwidth probing.
130|Topologically-aware overlay construction and server selection|  A number of large-scale distributed Internet applications could potentially benefit from some level of knowledge about the relative proximity between its participating host nodes. For example, the performance of large overlay networks could be improved if the application-level connectivity between the nodes in these networks is congruent with the underlying IP-level topology. Similarly, in the case of replicated web content, client nodes could use topological information in selecting one of multiple available servers. For such applications, one need not find the optimal solution in order to achieve significant practical benefits. Thus, these applications, and presumably others like them, do not require exact topological information and can instead use sufficiently informative hints about the relative positions of Internet hosts. In this paper, we present a binning scheme whereby nodes partition themselves into bins such that nodes that fall within a given bin are relatively close to one another in terms of network latency. Our binning strategy is simple (requiring minimal support from any measurement infrastructure), scalable (requiring no form of global knowledge, each node only needs knowledge of a small number of well-known landmark nodes) and completely distributed (requiring no communication or cooperation between the nodes being binned). We apply this binning strategy to the two applications mentioned above: overlay network construction and server selection. We test our binning strategy and its application using simulation and Internet measurement traces. Our results indicate that the performance of these applications can be significantly improved by even the rather coarse-grained knowledge of topology offered by our binning scheme.
131|Peer-to-Peer Membership Management for Gossip-Based Protocols|Gossip-based protocols for group communication have attractive scalability and reliability properties. The probabilistic  gossip schemes studied so far typically assume that each group member has full knowledge of the global membership and chooses  gossip targets uniformly at random. The requirement of global knowledge impairs their applicability to very large-scale groups. In this  paper, we present SCAMP (Scalable Membership protocol), a novel peer-to-peer membership protocol which operates in a fully  decentralized manner and provides each member with a partial view of the group membership. Our protocol is self-organizing in the  sense that the size of partial views naturally converges to the value required to support a gossip algorithm reliably. This value is a  function of the group size, but is achieved without any node knowing the group size. We propose additional mechanisms to achieve  balanced view sizes even with highly unbalanced subscription patterns. We present the design, theoretical analysis, and a detailed  evaluation of the basic protocol and its refinements. Simulation results show that the reliability guarantees provided by SCAMP are  comparable to previous schemes based on global knowledge. The scale of the experiments attests to the scalability of the protocol.
132|PROMISE: Peer-to-Peer Media Streaming Using CollectCast|We present the design, implementation, and evaluation of PROMISE,  a novel peer-to-peer media streaming system encompassing the key functions of peer lookup, peer-based aggregated streaming, and dynamic adaptations to network and peer conditions. Particularly, PROMISE is based on a new application level P2P service called CollectCast. CollectCast performs three main functions: (1) inferring and leveraging the underlying network topology and performance information for the selection of senders; (2) monitoring the status of peers and connections and reacting to peer/connection failure or degradation with low overhead; (3) dynamically switching active senders and standby senders, so that the collective network performance out of the active senders remains satisfactory. Based on both real-world measurement and simulation, we evaluate the performance of PROMISE, and discuss lessons learned from our experience with respect to the practicality and further optimization of PROMISE.
133|Video Multicast over the Internet|Multicast (multipoint) distribution of video is an important component of many existing and  future networked services. Today&#039;s Internet lacks support for quality of service (QoS) assurance  which makes the transmission of real-time traffic (such as video) challenging. In addition,  the heterogeneity of the Internet&#039;s transmission resources and end-systems makes it extremely  difficult, if not impossible, to agree on acceptable traffic characteristics among multiple receivers  of the same video stream. In this paper we survey techniques that have been proposed for  transmitting video in this environment. These techniques generally involve adaptation of the  video traffic carried over the network to match receiver requirements and network conditions.  In addition to their applicability to the near-term capabilities of the Internet, they also are of  relevance to a future, QoS-aware Internet environment because of the inevitable inaccuracies  in traffic and resource reservation specifica...
134|Resilient Multicast using Overlays|(PRM): a multicast data recovery scheme that improves data delivery ratios while maintaining low end-to-end latencies. PRM has both a proactive and a reactive components; in this paper we describe how PRM can be used to improve the performance of application-layer multicast protocols especially when there are high packet losses and host failures. Through detailed analysis in this paper, we show that this loss recovery technique has efficient scaling properties—the overheads at each overlay node asymptotically decrease to zero with increasing group sizes. As a detailed case study, we show how PRM can be applied to the NICE application-layer multicast protocol. We present detailed simulations of the PRM-enhanced NICE protocol for 10 000 node Internet-like topologies. Simulations show that PRM achieves a high delivery ratio ( 97%) with a low latency bound (600 ms) for environments with high end-to-end network losses (1%–5%) and high topology change rates (5 changes per second) while incurring very low overheads ( 5%). Index Terms—Multicast, networks, overlays, probabilistic forwarding, protocols, resilience. I.
135|Routing in Overlay Multicast Networks|Multicast services can be provided either as a basic network service or as an application-layer service. Higher level multicast implementations often provide more sophisticated features, and can provide multicast services, where no network layer support is available. Overlay multicast networks offer an intermediate option, potentially combining the flexibility and advanced features of application layer multicast with the greater efficiency of network layer multicast. Overlay multicast networks play an important role in the Internet. Indeed, since Internet Service Providers have been slow to enable IP multicast in their networks, Internet multicast is only widely available as an overlay service. This paper introduces several routing algorithms that are suitable for overlay multicast networks and evaluates their performance. The algorithms seek to optimize the endto -end delay and the interface bandwidth usage at the routing sites within the overlay network. The interface bandwidth is typically a key resource for an overlay network provider, and needs to be carefully managed in order to maximize the number of sessions that can be served. The simultaneous optimization of both delay and bandwidth is an NP-hard problem. We propose several heuristic algorithms and simulate their performance under various traffic conditions and on various network topologies.
136|A Peer-to-Peer Architecture for Media Streaming|We have witnessed the success of peer-to-peer (P2P) applications in both commercial and research fields. However, a practical application has received little attention to date: media streaming. Given the fact that the current Internet does not support IP Multicast while content-distribution-networks technologies are costly, P2P could be a promising start for enabling large-scale streaming systems. In our so-called Zigzag approach, we propose a method for clustering peers into a hierarchy called the administrative organization for easy management, and a method for building the multicast tree atop this hierarchy for efficient content transmission. In Zigzag, the multicast tree has a height logarithmic with the number of clients, and a node degree bounded by a constant. This helps reduce the number of processing hops on the delivery path to a client while avoiding network bottleneck. Consequently, the end-to-end delay is kept small. Although one could build a tree satisfying such properties easily, an efficient control protocol between the nodes must be in place to maintain the tree under the effects of network dynamics. Zigzag handles such situations gracefully requiring a constant amortized worst-case control overhead. Especially, failure recovery is done regionally with impact on at most a constant number of existing clients and with mostly no burden on the server.
137|oStream: Asynchronous Streaming Multicast in Application-Layer Overlay Networks|Although initially proposed as the deployable alternative to IP multicast, application-layer overlay network actually revolutionizes the way network applications can be built, since each overlay node is an end host, which is able to carry out more functionalities than simply forwarding packets. This paper addresses the on-demand media distribution problem in the context of overlay network. We take advantage of the strong...
138|On Peer-to-Peer Media Streaming  |In this paper, we study a peer-to-peer media streaming system with the following characteristics: (1) its streaming capacity grows dynamically; (2) peers do not exhibit serverlike behavior; (3) peers are heterogeneous in their bandwidth contribution; and (4) each streaming session may involve multiple supplying peers. Based on these characteristics, we investigate two problems: (1) how to assign media data to multiple supplying peers in one streaming session and (2) how to fast amplify the system’s total streaming capacity. Our solution to the first problem is an optimal media data assignment algorithm ÇÌËÔ Ô, which results in minimum buffering delay in the consequent streaming session. Our solution to the second problem is a distributed differentiated admission control protocol ???Ô Ô. By differentiating between requesting peers with different outbound bandwidth, ???Ô Ô achieves fast system capacity amplification; benefits all requesting peers in admission rate, waiting time, and buffering delay; and creates an incentive for peers to offer their truly available out-bound bandwidth. 
139|P2Cast: peer-to-peer patching scheme for VoD service|Providing video on demand (VoD) service over the Internet in a scalable way is a challenging problem. In this paper, we propose P2Cast- an architecture that uses a peer-to-peer approach to cooperatively stream video using patching techniques, while only relying on unicast connections among peers. We address the following two key technical issues in P2Cast: (1) constructing an application overlay appropriate for streaming; and (2) providing continuous stream playback (without glitches) in the face of disruption from an early departing client. Our simulation experiments show that P2Cast can serve many more clients than traditional client-server unicast service, and that it generally out-performs multicast-based patching if clients can cache more than ¡£¢¥¤ of a stream’s initial portion. We handle disruptions by delaying the start of playback and applying the shifted forwarding technique. A threshold on the length of time during which arriving clients are served in a single session in P2Cast serves as a knob to adjust the balance between the scalability and the clients ’ viewing quality in P2Cast.
140|From Epidemics to Distributed Computing |Abstract — Epidemic algorithms have been recently recognized as robust and scalable means to disseminate information in large-scale settings. Information is disseminated reliably in a distributed system the same way an epidemic would be propagated throughout a group of individuals: each process of the system chooses random peers to whom it relays the information it has received. The underlying peer-to-peer communication paradigm is the key to the scalability of the dissemination scheme. Epidemic algorithms have been studied theoretically and their analysis is built on sound mathematical foundations. Although promising, their general applicability to large scale distributed systems has yet to go through addressing many issues. These constitute an exciting research agenda. Index Terms — Scalability, peer-to-peer, epidemics, information
141|An Architecture for Internet Content Distribution as an Infrastructure Service|The IP Multicast service model extends the traditional best effort Internet datagram delivery service for efficient multi-point packet delivery. However, in spite of a decade of research on multicast protocols and applications, a globally deployed multicast service is nowhere in sight, hindered by multitudes of problems such as manageability, lack of a robust inter-domain multicast routing protocol, scalability, and heterogeneity. In this work, we propose a new model for Internet multicast where we view multi-point delivery not as a network primitive but rather as an application-level infrastructure service. Our architecture relies on a collection of strategically placed network agents that collaboratively provides the multicast service for a session. Clients locate a nearby agent and tap into the session via that agent. Agents organize themselves into an overlay network of unicast connections and build data distribution trees on top of this overlay structure. This model effectively pa...
142|Layered Peer-to-Peer Streaming|In this paper, we propose a peer-to-peer streaming solution to address the on-demand media distribution problem. We identify two issues, namely the asynchrony of user requests and heterogeneity of peer network bandwidth. Our key techniques to address these two issues are cache-andrelay and layer-encoded streaming. A unique challenge of layered peer-to-peer streaming is that the bandwidth and data availability (number of layers received) of each receiving peer are constrained and heterogeneous, which further limits the bandwidth and data availability of its downstream node when it acts as the supplying peer. This challenge distinguishes our work from existing studies on layered multicast. Our experiments show that our solution is e#cient at utilizing bandwidth resource of supplying peers, scalable at saving server bandwidth consumption, and optimal at maximizing streaming qualities of all peers.
143|Cache-and-Relay Streaming Media Delivery for Asynchronous Clients|We consider the problem of delivering popular streaming media to a large number of asynchronous clients. We propose and evaluate a cache-and-relay end-system multicast approach, whereby a client joining a multicast session caches the stream, and if needed, relays that stream to neighboring clients which may join the multicast session at some later time. This cache-and-relay approach is fully distributed, scalable, and efficient in terms of network link cost. In this paper we analytically derive bounds on the network link cost of our cache-and-relay approach, and we evaluate its performance under assumptions of limited client bandwidth and limited client cache capacity. When client bandwidth is limited, we show that although finding an optimal solution is NP-hard, a simple greedy algorithm performs surprisingly well in that it incurs network link costs that are very close to a theoretical lower bound. When client cache capacity is limited, we show that our cache-and-relay approach can still significantly reduce network link cost. We have evaluated our cache-and-relay approach using simulations over large, synthetic random networks, power-law degree networks, and small-world networks, as well as over large real router-level Internet maps.
144|  A Comparative Study of Application Layer Multicast Protocols | Due to the sparse deployment of IP multicast in the Internet today, some researchers have proposed application layer multicast as a new approach to implement widearea multicast services. In this approach multicast functionality is implemented at the end-hosts instead of network routers. Unlike network-layer multicast, application layer multicast requires no infrastructure support and can be easily deployed in the Internet. In this paper, we describe a set of application layer multicast protocols that have been proposed in recent literature, classify them based on some properties and present a comparison of performance and applicability of these schemes.
145|A Proactive Approach to Reconstructing Overlay Multicast Trees|Overlay multicast constructs a multicast delivery tree among end hosts. Unlike traditional IP multicast, the nonleaf nodes in the tree are normal end hosts, which are potentially more susceptible to failures than routers and may leave the multicast group voluntarily. In these cases, all downstream nodes will be affected. Thus an important problem in overlay multicast is how to recover from node departures in order to minimize the disruption of service to those affected nodes. In this paper, we propose a proactive approach to restore overlay multicast trees. Rather than letting downstream nodes try to find a new parent after a node departure, each non-leaf node precalculates a parent-to-be for each of its children. When this nonleaf node is gone, all its children can find their respective new parents immediately. The salient feature of the approach is that each non-leaf node can compute a rescue plan for its children independently, and in most cases, rescue plans from multiple non-leaf nodes can work together for their children when they fail or leave at the same time. We develop a protocol for nodes to communicate with new parents so that the delivery tree can be quickly restored. Extensive simulations demonstrate that our proactive approach can recover from node departures 5 times faster than reactive methods in some cases, and 2 times faster on average.
146|PROP: a Scalable and Reliable P2P Assisted Proxy Streaming System|The demand of delivering streaming media content in the Internet has become increasingly high for scientific, educational, and commercial applications. Three representative technologies have been developed for this purpose, each of which has its merits and serious limitations. Infrastructurebased CDNs with dedicated network bandwidths and powerful media replicas can provide high quality streaming services but at a high cost. Server-based proxies are costeffective but not scalable due to the limited proxy capacity and its centralized control. Client-based P2P networks are scalable but do not guarantee high quality streaming service due to the transient nature of peers. To address these limitations, we present a novel and efficient design of a scalable and reliable media proxy system supported by P2P networks. This system is called PROP abbreviated from our technical theme of &#034;collaborating and coordinating PROxy and its P2P clients&#034;. Our objective is to address both scalability and reliability issues of streaming media delivery in a costeffective way. In the PROP system, the clients&#039; machines in an intranet are self-organized into a structured P2P system to provide a large media storage and to actively participate in the streaming media delivery, where the proxy is also embedded as an important member to ensure quality of streaming service. The coordination and collaboration in the system are efficiently conducted by our P2P management structure and replacement policies. We have comparatively evaluated our system by trace-driven simulations with synthetic workloads and with a real-life workload trace extracted from the media server logs in an enterprise network. The results show that our design significantly improves the quality of media streaming and the system scalabil...
147|A hybrid architecture for cost-effective on-demand media streaming|We propose a new architecture for on-demand media streaming centered around the peer-to-peer (P2P) paradigm. The key idea of the architecture is that peers share some of their resources with the system. As peers contribute resources to the system, the overall system capacity increases and more clients can be served. The proposed ar-chitecture employs several novel techniques to: (1) use the often-underutilized peers ’ resources, which makes the proposed architecture both deployable and cost-effective, (2) aggregate contributions from multiple peers to serve a requesting peer so that supplying peers are not overloaded, (3) make a good use of peer heterogeneity by as-signing relatively more work to the powerful peers, and (4) organize peers in a network-aware fashion, such that nearby peers are grouped into a logical entity called a cluster. The network-aware peer organization is validated by statistics collected and analyzed from real Internet data. The main benefit of the network-aware peer organization is that it allows to develop efficient searching (to locate nearby suppliers) and dispersion (to disseminate new files into the system) algorithms. We present network-aware searching and dispersion algorithms that result in: (i) fast dissemination of new media files, (ii) reduction of the load on the underlying network, and (iii) better streaming service. We demonstrate the potential of the proposed architecture for a large-scale on-demand media streaming service through an extensive simulation study on large, Internet-like, topologies. Starting with a limited streaming capacity
148|Data Stream Management Systems  |In many application fields, such as production lines or stock analysis, it is substantial to create and process high amounts of data at high rates. Such continuous data flows with unknown size and end are also called data streams. The processing and analysis of data streams are a challenge for common data management systems as they have to operate and deliver results in real time. Data Stream Management Systems (DSMS), as an advancement of database management systems, have been implemented to deal with these issues. DSMS have to adapt to the notion of data streams on various levels, such as query languages, processing or optimization. In this chapter we give an overview of the basics of data streams, architecture principles of DSMS and the used query languages. Furthermore, we specifically detail data quality aspects in DSMS as these play an important role for various applications based on data streams. Finally, the chapter also includes a list of research and commercial DSMS and their key properties.
149|The CQL Continuous Query Language: Semantic Foundations and Query Execution|CQL, a Continuous Query Language, is supported by the STREAM prototype Data Stream  Management System at Stanford. CQL is an expressive SQL-based declarative language for  registering continuous queries against streams and updatable relations. We begin by presenting  an abstract semantics that relies only on &#034;black box&#034; mappings among streams and relations.
150|Graphscope: parameter-free mining of large time-evolving graphs|How can we find communities in dynamic networks of social interactions, such as who calls whom, who emails whom, or who sells to whom? How can we spot discontinuity time-points in such streams of graphs, in an on-line, any-time fashion? We propose GraphScope, that addresses both prob-lems, using information theoretic principles. Contrary to the majority of earlier methods, it needs no user-defined param-eters. Moreover, it is designed to operate on large graphs, in a streaming fashion. We demonstrate the efficiency and effectiveness of our GraphScope on real datasets from sev-eral diverse domains. In all cases it produces meaningful time-evolving patterns that agree with human intuition.
151|On graph problems in a semi-streaming model|Abstract. We formalize a potentially rich new streaming model, the semi-streaming model, that we believe is necessary for the fruitful study of efficient algorithms for solving problems on massive graphs whose edge sets cannot be stored in memory. In this model, the input graph, G = (V, E), is presented as a stream of edges (in adversarial order), and the storage space of an algorithm is bounded by O(n · polylog n), where n = |V |. We are particularly interested in algorithms that use only one pass over the input, but, for problems where this is provably insufficient, we also look at algorithms using constant or, in some cases, logarithmically many passes. In the course of this general study, we give semi-streaming constant approximation algorithms for the unweighted and weighted matching problems, along with a further algorithm improvement for the bipartite case. We also exhibit log n / log log n semistreaming approximations to the diameter and the problem of computing the distance between specified vertices in a weighted graph. These are complemented by ?(log (1-?) n) lower bounds. 1
152|Exploiting punctuation semantics in continuous data streams|Abstract—As most current query processing architectures are already pipelined, it seems logical to apply them to data streams. However, two classes of query operators are impractical for processing long or infinite data streams. Unbounded stateful operators maintain state with no upper bound in size and, so, run out of memory. Blocking operators read an entire input before emitting a single output and, so, might never produce a result. We believe that a priori knowledge of a data stream can permit the use of such operators in some cases. We discuss a kind of stream semantics called punctuated streams. Punctuations in a stream mark the end of substreams allowing us to view an infinite stream as a mixture of finite streams. We introduce three kinds of invariants to specify the proper behavior of operators in the presence of punctuation. Pass invariants define when results can be passed on. Keep invariants define what must be kept in local state to continue successful operation. Propagation invariants define when punctuation can be passed on. We report on our initial implementation and show a strategy for proving implementations of these invariants are faithful to their relational counterparts. Index Terms—Continuous queries, stream semantics, continuous data streams, query operators, stream iterators. 1
153|A Middleware for Fast and Flexible Sensor Network Deployment|A key problem in current sensor network technology is the heterogeneity of the available software and hardware platforms which makes deployment and application development a tedious and time consuming task. To minimize the unnecessary and repetitive implementation of identical functionalities for different platforms, we present our Global Sensor Networks (GSN) middleware which supports the flexible integration and discovery of sensor networks and sensor data, enables fast deployment and addition of new platforms, provides distributed querying, filtering, and combination of sensor data, and supports the dynamic adaption of the system configuration during operation. GSN’s central concept is the virtual sensor abstraction which enables the user to declaratively specify XML-based deployment descriptors in combination with the possibility to integrate sensor network data through plain SQL queries over local and remote sensor data sources. In this demonstration, we specifically focus on the deployment aspects and allow users to dynamically reconfigure the running system, to add new sensor networks on the fly, and to monitor the effects of the changes via a graphical interface. The GSN implementation is available from
154|Stream: The stanford data stream management system|Traditional database management systems are best equipped to run onetime queries over finite stored data sets. However, many modern applications such as network monitoring, financial analysis, manufacturing, and sensor networks require long-running, or continuous, queries over continuous unbounded
155|Processing flows of information: from data stream to complex event processing|A large number of distributed applications requires continuous and timely processing of information as it flows from the periphery to the center of the system. Examples include intrusion detection systems which analyze network traffic in real-time to identify possible attacks; environmental monitoring applications which process raw data coming from sensor networks to identify critical situations; or applications performing online analysis of stock prices to identify trends and forecast future values. Traditional DBMSs, which need to store and index data before processing it, can hardly fulfill the requirements of timeliness coming from such domains. Accordingly, during the last decade, different research communities developed a number of tools, which we collectively call Information flow processing (IFP) systems, to support these scenarios. They differ in their system architecture, data model, rule model, and rule language. In this article, we survey these systems to help researchers, who often come from different backgrounds, in understanding how the various approaches they adopt may complement each other. In particular, we propose a general, unifying model to capture the different aspects of an IFP system and use it to provide a complete and precise classification of the systems and mechanisms proposed so far.
156|The 8 requirements of real-time stream processing|Applications that require real-time processing of high-volume data steams are pushing the limits of traditional data processing infrastructures. These stream-based applications include market feed processing and electronic trading on Wall Street, network and infrastructure monitoring, fraud detection, and command and control in military environments. Furthermore, as the “sea change ” caused by cheap micro-sensor technology takes hold, we expect to see everything of material significance on the planet get “sensor-tagged ” and report its state or location in real time. This sensorization of the real world will lead to a “green field ” of novel monitoring and control applications with high-volume and low-latency processing requirements. Recently, several technologies have emerged—including off-theshelf stream processing engines—specifically to address the challenges of processing high-volume, real-time data without requiring the use of custom code. At the same time, some existing software technologies, such as main memory DBMSs and rule engines, are also being “repurposed ” by marketing departments to address these applications. In this paper, we outline eight requirements that a system software should meet to excel at a variety of real-time stream processing applications. Our goal is to provide high-level guidance to information technologists so that they will know what to look for when evaluation alternative stream processing solutions. As such, this paper serves a purpose comparable to the requirements papers in relational DBMSs and on-line analytical processing. We also briefly review alternative system software technologies in the context of our requirements. The paper attempts to be vendor neutral, so no specific commercial products are mentioned. 1.
157|Semantics and evaluation techniques for window aggregates in data streams|A windowed query operator breaks a data stream into possibly overlapping subsets of data and computes a result over each. Many stream systems can evaluate window aggregate queries. However, current stream systems suffer from a lack of an explicit definition of window semantics. As a result, their implementations unnecessarily confuse window definition with physical stream properties. This confusion complicates the stream system, and even worse, can hurt performance both in terms of memory usage and execution time. To address this problem, we propose a framework for defining window semantics, which can be used to express almost all types of windows of which we are aware, and which is easily extensible to other types of windows that may occur in the future. Based on this definition, we explore a one-pass query evaluation strategy, the Window-ID (WID) approach, for various types of window aggregate queries. WID significantly reduces both required memory space and execution time for a large class of window definitions. In addition, WID can leverage punctuations to gracefully handle disorder. Our experimental study shows that WID has better execution-time performance than existing window aggregate query evaluation options that retain and reprocess tuples, and has better latency-accuracy tradeoffs for disordered input streams compared to using a fixed delay for handling disorder. 1.
158|Staying FIT: Efficient Load Shedding Techniques for Distributed Stream Processing|In distributed stream processing environments, large numbers of continuous queries are distributed onto multiple servers. When one or more of these servers become overloaded due to bursty data arrival, excessive load needs to be shed in order to preserve low latency for the query results. Because of the load dependencies among the servers, load shedding decisions on these servers must be well-coordinated to achieve end-to-end control on the output quality. In this paper, we model the distributed load shedding problem as a linear optimization problem, for which we propose two alternative solution approaches: a solver-based centralized approach, and a distributed approach based on metadata aggregation and propagation, whose centralized implementation is also available. Both of our solutions are based on generating a series of load shedding plans in advance, to be used under certain input load conditions. We have implemented our techniques as part of the Borealis distributed stream processing system. We present experimental results from our prototype implementation showing the performance of these techniques under different input and query workloads. 1.
159|XSQ: A streaming XPath engine|We have implemented and released the XSQ system for evaluating XPath queries on streaming XML data. XSQ supports XPath features such as multiple predicates, closures, and aggregation, which pose interesting challenges for streaming evaluation. Our implementation is based on using a hierarchical arrangement of augmented finite state automata. A design goal of XSQ is buffering data for the least amount of time possible. We present a detailed experimental study that characterizes the performance of XSQ and related systems, and that illustrates the performance implications of XPath features such as closures.
160|Dense Subgraph Maintenance under Streaming Edge Weight Updates for Realtime Story Identification|Recent years have witnessed an unprecedented proliferation of social media. People around the globe author, every day, millions of blog posts, micro-blog posts, social network status updates, etc. This rich stream of information can be used to identify, on an ongoing basis, emerging stories, and events that capture popular attention. Stories can be identified via groups of tightly-coupled realworld entities, namely the people, locations, products, etc., that are involved in the story. The sheer scale, and rapid evolution of the data involved necessitate highly efficient techniques for identifying important stories at every point of time. The main challenge in real-time story identification is the maintenance of dense subgraphs (corresponding to groups of tightlycoupled entities) under streaming edge weight updates (resulting from a stream of user-generated content). This is the first work to study the efficient maintenance of dense subgraphs under such streaming edge weight updates. For a wide range of definitions of density, we derive theoretical results regarding the magnitude of change that a single edge weight update can cause. Based on these, we propose a novel algorithm, DYNDENS, which outperforms adaptations of existing techniques to this setting, and yields meaningful results. Our approach is validated by a thorough experimental evaluation on large-scale real and synthetic datasets. 
161|Exploiting PredicateWindow Semantics over Data Streams |The continuous sliding-window query model is used widely in data stream management systems where the focus of a continuous query is limited to a set of the most recent tuples. In this paper, we show that an interesting and important class of queries over data streams cannot be answered using the sliding-window query model. Thus, we introduce a new model for continuous window queries, termed the predicatewindow query model that limits the focus of a continuous query to the stream tuples that qualify a certain predicate. Predicate-window queries have some distinguishing characteristics, e.g., (1) The window predicate can be defined over any attribute in the stream tuple (ordered or unordered). (2) Stream tuples qualify and disqualify the window predicate in an out-of-order manner. In this paper, we discuss the applicability of the predicate-window query model. We will show how the existing sliding-window query models fail to answer some of the predicate-window queries. Finally, we discuss the challenges in supporting the predicate-window query model in data stream management systems. 1.
162| Extending XQuery with Window Functions |This paper presents two extensions for XQuery. The first extension allows the definition and processing of different kinds of windows over an input sequence; i.e., tumbling, sliding, and landmark windows. The second extension extends the XQuery data model (XDM) to support infinite sequences. This extension makes it possible to use XQuery as a language for continuous queries. Both extensions have been integrated into a Java-based open source XQuery engine. This paper gives details of this implementation and presents the results of running the Linear Road benchmark on the extended XQuery engine. 
163|AN OVERVIEW OF THE APPLICATIONS OF MULTISETS|  This paper presents a systemization of representation of multisets and basic operations under multisets, and an overview of the applications of multisets in mathematics, computer science and related areas.
164|PODS: a new model and processing algorithms for uncertain data streams|Uncertain data streams, where data is incomplete, imprecise, and even misleading, have been observed in a variety of environments. Feeding uncertain data streams to existing stream systems can produce results of unknown quality, which is of paramount concern to monitoring applications. In this paper, we present the Pods system that supports uncertain data stream processing for data that is naturally captured using continuous random variables. The Pods system employs a unique data model that is flexible and allows efficient computation. Built on this model, we develop evaluation techniques for complex relational operators, including aggregates and joins, by exploring advanced statistical theory and approximation techniques. Our evaluation results show that our techniques can achieve high performance in stream processing while satisfying accuracy requirements, and these techniques significantly outperform a state-of-the-art sampling-based method. Furthermore, initial results of a case study show that our modeling and aggregation techniques can allow a tornado detection system to produce better quality results yet with lower execution time. 1.
165|Logical Foundations of Continuous Query Languages for Data Streams |Abstract. Data Stream Management Systems (DSMS) have attracted much interest from the database community, and extensions of relational database languages were proposed for expressing continuous queries on data streams. However, while relational databases were built on the solid bedrock of logic, the same cannot be said for DSMS. Thus, a logic-based reconstruction of DSMS languages and their unique computational model is long overdue. Indeed, the banning of blocking queries and the fact that stream data are ordered by their arrival timestamps represent major new aspects that have yet to be characterized by simple theories. In this paper, we show that these new requirements can be modeled using the familiar deductive database concepts of closed-world assumption and explicit local stratification. Besides its obvious theoretical interest, this approach leads to the design of a powerful version of Datalog for data streams. This language is called Streamlog and takes the query and application languages of DSMS to new levels of expressive power, by removing the unnecessary limitations that severely impair current commercial systems and research prototypes. 1
166|A general algebra and implementation for monitoring event streams|Recently there has been considerable research on Data Stream Management Systems (DSMS) to support analysis of data that arrives rapidly in high-speed streams. Most of these systems have very expressive query languages in order to address a wide range of applications. In this paper, we take a different approach. Instead of starting with a very powerful data stream query language, we begin with a well-known class of languages — event languages. Through the addition of several simple, but powerful language constructs (namely parameterization and aggregates), we add pieces that extend their expressiveness towards full-fledged languages for processing data streams. Our resulting contributions are a novel algebra for expressing data stream queries, and a corresponding transformation of algebra expressions into finite state automata that can be implemented very efficiently. Our language is simple and natural, and it can express surprisingly powerful data stream queries. We formally introduce the language including a formal mapping of algebra expressions to finite state automata. Furthermore, we show the efficacy of our approach via an initial performance evaluation, including a comparison with the Stanford STREAM System. 1
167|Relational Languages and Data Models for Continuous Queries on Sequences and Data Streams|Most data stream management systems are based on extensions of the relational data model and query languages, but rigorous analyses of the problems and limitations of this approach, and how to overcome them, are still wanting. In this article, we elucidate the interaction between stream-oriented extensions of the relational model and continuous query language constructs, and show that the resulting expressive power problems are even more serious for data streams than for databases. In particular, we study the loss of expressive power caused by the loss of blocking query operators, and characterize nonblocking queries as monotonic functions on the database. Thus we introduce the notion of N B-completeness to assure that a query language is as suitable for continuous queries as it is for traditional database queries. We show that neither RA nor SQL are N B-complete on unordered sets of tuples, and the problem is even more serious when the data model is extended to support order—a sine-qua-non in data stream applications. The new limitations of SQL, compounded with well-known problems in applications such as sequence queries and data mining, motivate our proposal of extending the language with user-defined aggregates (UDAs). These can be natively coded in SQL, according to simple syntactic rules that set nonblocking aggregates apart from blocking ones. We first prove that SQL with UDAs is Turing complete. We then prove that SQL with monotonic UDAs and union operators can express all monotonic set functions computable by a Turing machine (N B-completeness) and
168|Designing an Inductive Data Stream Management System: the Stream Mill Experience |There has been much recent interest in on-line data mining. Existing mining algorithms designed for stored data are either not applicable or not effective on data streams, where real-time response is often needed and data characteristics change frequently. Therefore, researchers have been focusing on designing new and improved algorithms for on-line mining tasks, such as classification, clustering, frequent itemsets mining, pattern matching, etc. Relatively little attention has been paid to designing DSMSs, which facilitate and integrate the task of mining data streams—i.e., stream systems that provide Inductive functionalities analogous to those provided by Weka and MS OLE DB for stored data. In this paper, we propose the notion of an Inductive DSMS—a system that besides providing a rich library of inter-operable functions to support the whole mining process, also supports the essentials of DSMS, including optimization of continuous queries, load shedding, synoptic constructs, and non-stop computing. Ease-of-use and extensibility are additional desiderata for the proposed Inductive DSMS. We first review the many challenges involved in realizing such a system and then present our approach of extending the Stream Mill DSMS toward that goal. Our system features (i) a powerful query language where mining methods are expressed via aggregates for generic streams and arbitrary windows, (ii) a library of fast and light mining algorithms, and (iii) an architecture that makes it easy to customize and extend existing mining methods and introduce new ones. 1.
169|constitutive expression|IFNg is a pro-in¯ammatory cytokine that potentiates p53-independent apoptosis in a variety of cell types. STAT1 is the primary mediator of IFNg action. ZBP-89 is a transcription factor that binds to the G/C-rich elements and mediates p53-independent apoptosis. In this study, site-directed mutagenesis revealed that a G-rich element from +171 to +179 within the ®rst intron of the STAT1 gene is critical for optimal STAT1 promoter activity. Electrophoretic mobility shift assays and promoter analysis revealed that ZBP-89 binds directly to this STAT1 G-rich element along with Sp1 and Sp3. Reduction of ZBP-89 with siRNA attenuated both basal and IFNg-induced STAT1 expression and subsequently diminished the activation of apoptotic markers, e.g. caspase-3 and PARP. Taken together, we conclude that ZBP-89 is required for constitutive STAT1 expression and in this way contributes to the ability of cells to be activated by IFNg.
170|A flexible framework for multisensor data fusion using data stream management technologies |Many applications use sensors to capture an image of the real world, which is needed for automatical processes. E. g. fu-ture driver assistance systems will be based on dynamic in-formation about the car’s environment, the car’s state and the driver’s state. Since there exists no single sensor that can sense the required information, different sensors like radar, video and eye-tracker are used. Typically some provide re-dundant information about the same real world entity, while others measure different things. Thus, the fusion of infor-mation from different sensors is necessary to get a consistant image of the real world. In most sensor fusion systems the sensor configuration is known a priori and the fusion algo-rithms are adapted for these sensor configurations. Thus, changing a sensor fusion system to enable it to process sensor readings from another sensor configuration is hardly possible or completely impossible. Since in development processes of automotive applications different sensor equipment and en-vironmental requirements exist and change frequently a new approach for adapting sensor fusion systems is necessary. Hence, in this work a framework for sensor fusion systems will be developed that allows a flexible adaption of fusion mechanisms. Due to realtime requirements of automotive applications and the flexibility of query processing technolo-gies, data stream management technology will be used to develop a flexible framework for multisensor data fusion.
172|A dynamic theory of organizational knowledge creation|to stimulate the next wave of research on organization learning. It provides a conceptual framework for research on the differences and similarities of learning by individuals, groups, and organizations.
174|Working Knowledge|While knowledge is viewed by many as an asset, it is often difficult  to locate particular items within a large electronic corpus. This paper presents an  agent based framework for the location of resources to resolve a specific query,  and considers the associated design issue. Aspects of the work presented complements  current research into both expertise finders and recommender systems. The  essential issues for the proposed design are scalability, together with the ability  to learn and adapt to changing resources. As knowledge is often implicit within  electronic resources, and therefore difficult to locate, we have proposed the use  of ontologies, to extract the semantics and infer meaning to obtain the results required.
175|The resource-based view of the firm in two environments: The Hollywood film studios from 1936 to 1965|This article continues to operationally define and test the resource-hased view of the firm in a study of the major U.S. film studios from 1936 to 1965. We found that property-hased resources in the form of exclusive long-term contracts with stars and theaters helped financial performance in the stable, predictable environment of 1936-50. In con-trast, knowledge-based resources in the form of production and coordi-native talent and budgets boosted financial performance in the more uncertain (changing and unpredictable) post-television environment of 1951-65. The resource-based view of the firm provides a useful complement to Porter&#039;s (1980) well-known structural perspective of strategy. This view shifts the emphasis from the competitive environment of firms to the resources that firms have developed to compete in that environment. Unfortunately, although it has generated a great deal of conceptualizing (see reviews by Black and Boal [1994] and Peteraf [1993]), the resource-based view is just
176|Knowledge Management Systems: Emerging Views and Practices from the Field|The knowledge-based theory of the firm suggests that knowlege is the organizational asset that enables sustainable competitive advantage in hypercompetitive environments. The emphasis on knowledge in today’s organizations is based on the assumption that barriers to the transfer and replication of knowledge endow it with strategic importance. Many organizations are developing information systems designed specifically to facilitate the sharing and integration of knowledge. Such systems are referred to as Knowledge Management Systems (KMS). Because KMS are just beginning to appear in organizations, there exists little research and field data to guide the development and implementation of these systems or to guide expectations of the potential benefits of such systems. The current study provides an analysis of current practices and outcomes of KMS and the nature of KMS as they are evolving in fifty organizations. The findings suggest that interest in KMS across a variety of industries is very high, the technological foundations are varied, and the major concerns revolve around achieving the correct amount and type of accurate knowledge and garnering support for contributing to the KMS. Implications for practice and suggestions for future research are drawn from the study findings.
177|Risk-management: coordinating corporate investment and financing policies|This paper develops a general framework for analyzing corporate risk management policies. We begin by observing that if external sources of finance are more costly to corporations than internally generated funds, there will typically be a benefit to hedging: hedging adds value to the extent that it helps ensure that a corporation has sufficient internal funds available to take advantage of attractive investment opportunities. We then argue that this simple observation has wide ranging impli-cations for the design of risk management strategies. We delineate how these strategies should depend on such factors as shocks to investment and financing opportunities. We also discuss exchange rate hedging strategies for multinationals, as well as strategies involving &#034;nonlinear&#034; instruments like options.  
178|Theory of the Firm: Managerial Behavior, Agency Costs and Ownership Structure|This paper integrates elements from the theory of agency, the theory of property rights and the theory of finance to develop a theory of the ownership structure of the firm. We define the concept of agency costs, show its relationship to the ‘separation and control’ issue, investigate the nature of the agency costs generated by the existence of debt and outside equity, demonstrate who bears costs and why, and investigate the Pareto optimality of their existence. We also provide a new definition of the firm, and show how our analysis of the factors influencing the creation and issuance of debt and equity claims is a special case of the supply side of the completeness of markets problem. 
179|Corporate Financing and Investment Decisions when Firms Have Information that Investors Do Not Have|This paper considers a firm that must issue common stock to raise cash to undertake a valuable investment opportunity. Management is assumed to know more about the firm’s value than potential investors. Investors interpret the firm’s actions rationally. An. equilibrium mode1 of the issue-invest decision is developed under these assumptions. The mode1 shows that firms may refuse to issue stock, and therefore may pass up valuable investment opportunities. The model suggests explanations for several aspects of corporate financing behavior, including the tendency to rely on internal sources of funds, and to prefer debt to equity if external financing is required. Extensions and applications of the model are discussed. 
181|Financial Intermediation and Delegated Monitoring|This paper develops a theory of financial intermediation based on minimizing the cost of monitoring information which is useful for resolving incentive problems between borrowers and lenders. It presents a characterization of the costs of providing incentives for delegated monitoring by a financial intermediary. Diversification within an intermediary serves to reduce these costs, even in a risk neutral economy. The paper presents some more general analysis of the effect of diversification on resolving incentive problems. In the environment assumed in the model, debt contracts with costly bankruptcy are shown to be optimal. The analysis has implications for the portfolio structure and capital structure of intermediaries.
182|Optimal contracts and competitive markets with costly state verification|The insight of Arrow [4] and Debreu [7] that uncertainty is easily incor-porated into general equilibrium models is double-edged. It is true that one need only index commodities by the state of nature, and classical results on the existence and optimality of competitive equilibria can be made to
183|Multimarket Oligopoly: Strategic Substitutes and complements |A firm’s actions in one market can change competitors’ strategies in a second market by affecting its own marginal costs in that other mar-ket. Whether the action provides costs or benefits in the second market depends on (a) whether it increases or decreases marginal costs in the second market and (b) whether competitors’ products are strategic substitutes or strategic complements. The latter distinction is determined by whether more “aggressive” play (e.g., lower price or higher quantity) by one firm in a market lowers or raises compet-ing firms’ marginal profitabilities in that market. Many recent results in oligopoly theory can be most easily understood in terms of strategic substitutes and complements. 
185|Incentive-compatible debt contracts: The one-period problem|In a simple model of borrowing and lending with asymmetric information we show that the optimal, incentive-compatible debt contract is the standard debt contract. The second-best level of investment never exceeds the first-best and is strictly less when there is a positive probability of costly bankruptcy. We also compare the second-best with the results of interest-rate-taking behaviour and consider the effects of risk aversion. Finally we provide conditions under which increasing the borrower&#039;s initial net wealth must reduce total investment in the venture. 1.
186|Managing Energy and Server Resources in Hosting Centers|Interact hosting centers serve multiple service sites from a common hardware base. This paper presents the design and implementation of an architecture for resource management in a hosting center op-erating system, with an emphasis on energy as a driving resource management issue for large server clusters. The goals are to provi-sion server resources for co-hosted services in a way that automati-cally adapts to offered load, improve the energy efficiency of server dusters by dynamically resizing the active server set, and respond to power supply disruptions or thermal events by degrading service in accordance with negotiated Service Level Agreements (SLAs). Our system is based on an economic approach to managing shared server resources, in which services &amp;quot;bid &amp;quot; for resources as a func-tion of delivered performance. The system continuously moni-tors load and plans resource allotments by estimating the value of their effects on service performance. A greedy resource allocation algorithm adjusts resource prices to balance supply and demand, allocating resources to their most efficient use. A reconfigurable server switching infrastructure directs request traffic to the servers assigned to each service. Experimental results from a prototype confirm that the system adapts to offered load and resource avail-ability, and can reduce server energy usage by 29 % or more for a typical Web workload. 1.
187|Generating Representative Web Workloads for Network and Server Performance Evaluation|One role for workload generation is as a means for understanding how servers and networks respond to variation in load. This enables management and capacity planning based on current and projected usage. This paper applies a number of observations of Web server usage to create a realistic Web workload generation tool which mimics a set of real users accessing a server. The tool, called Surge (Scalable URL Reference Generator) generates references matching empirical measurements of 1) server file size distribution; 2) request size distribution; 3) relative file popularity; 4) embedded file references; 5) temporal locality of reference; and 6) idle periods of individual users. This paper reviews the essential elements required in the generation of a representative Web workload. It also addresses the technical challenges to satisfying this large set of simultaneous constraints on the properties of the reference stream, the solutions we adopted, and their associated accuracy. Finally, we present evidence that Surge exercises servers in a manner significantly different from other Web server benchmarks.
188|Wide-area Internet traffic patterns and characteristics|Abstract – The Internet is rapidly growing in number of users, traffic levels, and topological complexity. At the same time it is increasingly driven by economic competition. These developments render the characterization of network usage and workloads more difficult, and yet more critical. Few recent studies have been published reporting Internet backbone traffic usage and characteristics. At MCI, we have implemented a high-performance, low-cost monitoring system that can capture traffic and perform analyses. We have deployed this monitoring tool on OC-3 trunks within internetMCI’s backbone and also within the NSF-sponsored vBNS. This paper presents observations on the patterns and characteristics of wide-area Internet traffic, as recorded by MCI’s OC-3 traffic monitors. We report on measurements from two OC-3 trunks in MCI’s commercial Internet backbone over two time ranges (24-hour and 7-day) in the presence of up to 240,000 flows. We reveal the characteristics of the traffic in terms of packet sizes, flow duration, volume, and percentage composition by protocol and application, as well as patterns seen over the two time scales. 1
189|Agile Application-Aware Adaptation for Mobility|In this paper we show that application-aware adaptation, a collaborative partnership between the operating system and applications, offers the most general and effective approach to mobile information access. We describe the design of Odyssey, a prototype implementing this approach, and show how it supports concurrent execution of diverse mobile applications. We identify agility as a key attribute of adaptive systems, and describe how to quantify and measure it. We present the results of our evaluation of Odyssey, indicating performance improvements up to a factor of 5 on a benchmark of three applications concurrently using remote services over a network with highly variable bandwidth.  
190|Resource Containers: A New Facility for Resource Management in Server Systems|General-purpose operating systems provide inadequate support for resource management in large-scale servers. Applications lack sufficient control over scheduling and management of machine resources, which makes it difficult to enforce priority policies, and to provide robust and controlled service. There is a fundamental mismatch between the original design assumptions underlying the resource management mechanisms of current general-purpose operating systems, and the behavior of modern server applications. In particular, the operating system’s notions of protection domain and resource principal coincide in the process abstraction. This coincidence prevents a process that manages large numbers of network connections, for example, from properly allocating system resources among those connections. We propose and evaluate a new operating system abstraction called a resource container, which separates the notion of a protection domain from that of a resource principal. Resource containers enable fine-grained resource management in server systems and allow the development of robust servers, with simple and firm control over priority policies. 1
191|Lottery Scheduling: Flexible Proportional-Share Resource Management|This paper presents lottery scheduling, a novel randomized resource allocation mechanism. Lottery scheduling provides efficient, responsive control over the relative execution rates of computations. Such control is beyond the capabilities of conventional schedulers, and is desirable in systems that service requests of varying importance, such as databases, media-based applications, and networks. Lottery scheduling also supports modular resource management by enabling concurrent modules to insulate their resource allocation policies from one another. A currency abstraction is introduced to flexibly name, share, and protect resource rights. We also show that lottery scheduling can be generalized to manage many diverse resources, such as I/O bandwidth, memory, and access to locks. We have implemented a prototype lottery scheduler for the Mach 3.0 microkernel, and found that it provides flexible and responsive control over the relative execution rates of a wide range of applications. The overhead imposed by our unoptimized prototype is comparable to that of the standard Mach timesharing policy. 
192|A measurement-based admission control algorithm for integrated services packet networks|Many designs for integrated service networks offer a bounded delay packet delivery service to support real-time applications. To provide bounded delay service, networks must use admission control to regulate their load. Previous work on admission control mainly focused on algorithms that compute the worst case theoretical queueing delay to guarantee an absolute delay bound for all packets. In this paper we describe a measurement-based admission control algorithm for predictive service, which allows occasional delay violations. We have tested our algorithm through simulations on a wide variety of network topologies and driven with various source models, including some that exhibit long-range dependence, both in themselves and in their aggregation. Our simulation results suggest that, at least for the scenarios studied here, the measurement-based approach combined with the relaxed service commitment of predictive service enables us to achieve a high
193|Locality-Aware Request Distribution in Cluster-based Network Servers|We consider cluster-based network servers in which a front-end directs incoming requests to one of a number of back-ends. Specifically, we consider content-based request distribution: the front-end uses the content requested, in addition to information about the load on the back-end nodes, to choose which back-end will handle this request. Content-based request distribution can improve locality in the back-ends&#039; main memory caches, increase secondary storage scalability by partitioning the server&#039;s database, and provide the ability to employ back-end nodes that are specialized for certain types of requests. As a specific policy for content-based request distribution, we introduce a simple, practical strategy for locality-aware request distribution (LARD). With LARD, the front-end distributes incoming requests in a manner that achieves high locality in the back-ends&#039; main memory caches as well as load balancing. Locality is increased by dynamically subdividing the server&#039;s working set o...
194|Energy-aware adaptation for mobile applications|In this paper, we demonstrate that a collaborative relationship between the operating system and applications can be used to meet user-specified goals for battery duration. We first show how applications can dynamically modify their behavior to conserve energy. We then show how the Linux operating system can guide such adaptation to yield a batterylife of desired duration. By monitoring energy supply and demand, it is able to select the correct tradeoff between energy conservation and application quality. Our evaluation shows that this approach can meet goals that extend battery life by as much as 30%.
195|Dynamic Thermal Management for High-Performance Microprocessors|With the increasing clock rate and transistor count of today’s microprocessors, power dissipation is becoming a critical component of system design complexity. Thermal and power-delivery issues are becoming especially critical for high-performance computing systems. In this work, we investigate dynamic thermal management as a technique to control CPUpower dissipation. With the increasing usage of clock gating techniques, the average power dissipation typically seen by common applications is becoming much less than the chip’s rated maximum power dissipation. However; system designers still must design thermal heat sinks to withstand the worst-case scenario. We define and investigate the major components of any dynamic thermal management scheme. Specijcally we explore the tradeoffs between several mechanisms for responding to periods of thermal trauma and we consider the effects of hardware and sofnyare implementations. With appropriate dynamic thermal management, the CPU can be designed for a much lower maximum power rating, with minimal performance impact for typical applications. 1
196|Performance Guarantees for Web Server End-Systems: A Control-Theoretical Approach|The Internet is undergoing substantial changes from a communication and browsing infrastructure to a medium for conducting business and marketing a myriad of services. The World Wide Web provides a uniform and widely-accepted application interface used by these services to reach multitudes of clients. These changes place the web server at the center of a gradually emerging eservice infrastructure with increasing requirements for service quality and reliability guarantees in an unpredictable and highly-dynamic environment.
197|Workload characterization of the 1998 world cup web site|Web, workload characterization, performance, servers, caching, World Cup This paper presents a detailed workload characterization study of the 1998 World Cup Web site. Measurements from this site were collected over a three month period. During this time the site received 1.35 billion requests, making this the largest Web workload analyzed to date. By examining this extremely busy site and through comparison with existing characterization studies we are able to determine how Web server workloads are evolving. We find that improvements in the caching architecture of the World-Wide Web are changing the workloads of Web servers, but that major improvements to that architecture are still necessary. In particular, we uncover evidence that a better consistency mechanism is required for World-Wide Web caches.
198|A Feedback-driven Proportion Allocator for Real-Rate Scheduling|In this paper we propose changing the decades-old practice of allocating CPU to threads based on priority to a scheme based on proportion and period. Our scheme allocates to each thread a percentage of CPU cycles over a period of time, and uses a feedback-based adaptive scheduler to assign automatically both proportion and period. Applications with known requirements, such as isochronous software devices, can bypass the adaptive scheduler by specifying their desired proportion and/or period. As a result, our scheme provides reservations to applications that need them, and the benefits of proportion and period to those that do not. Adaptive scheduling using proportion and period has several distinct benefits over either fixed or adaptive priority based schemes: finer grain control of allocation, lower variance in the amount of cycles allocated to a thread, and avoidance of accidental priority inversion and starvation, including defense against denial-of-service attacks. This paper descr...
199|CPU Reservations and Time Constraints: Efficient, Predictable Scheduling of Independent Activities|Workstations and personal computers are increasingly being used for applications with real-time characteristics such as speech understanding and synthesis, media computations and I/O, and animation, often concurrently executed with traditional non-real-time workloads. This paper presents a system that can schedule multiple independent activities so that: . activities can obtain minimum guaranteed execution rates with application-specified reservation granularities via CPU Reservations, . CPU Reservations, which are of the form &#034;reserve X units of time out of every Y units&#034;, provide not just an average case execution rate of X/Y over long periods of time, but the stronger guarantee that from any instant of time, by Y time units later, the activity will have executed for at least X time units, . applications can use Time Constraints to schedule tasks by deadlines, with on-time completion guaranteed for tasks with accepted constraints, and . both CPU Reservations and Time Constraints...
200|Load Balancing and Unbalancing for Power and Performance in Cluster-Based Systems|In this paper we address power conservation for clusters of workstations or PCs. Our approach is to develop systems that dynamically  turn cluster nodes on -- to be able to handle the load imposed on the system efficiently -- and off -- to save power under lighter load. The key component of our systems is an algorithm that makes load balancing and unbalancing decisions by considering both the total load imposed on the cluster and the power and performance implications of turning nodes off. The algorithm is implemented in two different ways: (1) at the application level for a cluster-based, localityconscious network server; and (2) at the operating system level for  an operating system for clustered cycle servers. Our experimental  results are very favorable, showing that our systems conserve both power and energy in comparison to traditional systems.
201|Power Aware Page Allocation|One of the major challenges of post-PC computing is the need to reduce energy consumption, thereby extending the lifetime of the batteries that p ower these mobile devices. Memory is a particularly important tar get for e orts to improve energy e ciency. Memory technolo gy is becoming available that o ers power management featur es such as the ability to put individual chips in any one of several di erent power modes. In this paper we explor e the interaction of page plac ement with static and dynamic hardware policies to exploit these emer ginghardwar efeatur es. In p articular, we c onsider p age allo cation p olicies that ancbe employed by an informed operating system to complement the hardware power management strategies. We perform experiments using two complementary simulation envir onments: a tracedriven simulator with workload traces that are representative of mobile computing and an execution-driven simulator with a detaile d processor/memory model and a more memoryintensive set of benchmarks (SPEC2000). Our r esults make a compelling case for a cooperative hardwar e/software approach for exploiting power-aware memory, with down to as little as 45 % of the Energy Delay for the best static policy and 1 % to 20 % of the Ener gyDelay for a traditional fullpower memory. 1.
202|Cluster Reserves: A Mechanism for Resource Management in Cluster-based Network Servers|In network (e.g., Web) servers, it is often desirable to isolate the performance of different classes of requests from each other. That is, one seeks to achieve that a certain minimal proportion of server resources are available for a class of requests, independent of the load imposed by other requests. Recent work demonstrates how to achieve this performance isolation in servers consisting of a single, centralized node; however, achieving performance isolation in a distributed, cluster based server remains a problem. This paper introduces a new abstraction, the cluster reserve, which represents a resource principal in a cluster based network server. We present a design and evaluate a prototype implementation that extends existing techniques for performance isolation on a single node server to cluster based servers. In our design, the dynamic cluster-wide resource management problem is formulated as a constrained optimization problem, with the resource allocations on individual machin...
203|WebOS: Operating System Services for Wide Area Applications |In this paper, we demonstrate the power of providing a common set of Operating System services to wide-area applications, including mechanisms for naming, persistent storage, remote process execution, resource management, authentication, and security. On a single machine, application developers can rely on the local operating system to provide these abstractions. In the wide area, however, application developers are forced to build these abstractions themselves or to do without. This ad-hoc approach often results in individual programmers implementing non-optimal solutions, wasting both programmer effort and system resources. To address these problems, we are building a system, WebOS, that provides basic operating systems services needed to build applications that are geographically distributed, highly available, incrementally scalable, and dynamically reconfigurable. Experience with a number of applications developed under WebOS indicates that it simplifies system development and improves resource utilization. In particular, we use WebOS to implement Rent-A-Server to provide dynamic replication of overloaded Web services across the wide area in response to client demands. 
204|Oceano - SLA Based Management of a Computing Utility|Contact address:
205|Interposed Request Routing for Scalable Network Storage|This paper presents Slice, a new storage system architecture for highspeed LANs incorporating network-attached block storage. Slice interposes a request switching filter -- called a /proxy -- along the network path between the client and the network storage system (e.g., in a network adapter or switch). The purpose of the/proxy is to route requests among a server ensemble that implements the file service. We present a prototype that uses this approach to virtualize the standard NFS file protocol to provide scalable, high-bandwidth file service to ordinary NFS clients. The paper presents and justifies the architecture, proposes and evaluates several request routing policies realizable within the architecture, and explores the effects of these policies on service structure
206|Mobile Network Estimation|Mobile systems must adapt their behavior to changing network conditions. To do this, they must accurately estimate available network capacity. Producing quality estimates is challenging because network observations are noisy, particularly in mobile, ad hoc networks. Current systems depend on simple, exponentially-weighted moving average (EWMA) filters. These filters are either able to detect true changes quickly or to mask observed noise and transients, but cannot do both. In this paper, we present four filters designed to react quickly to persistent changes while tolerating transient noise. Such filters are ##### when possible, but ###### when necessary, adapting their behavior to prevailing conditions. These filters are evaluated in a variety of networking situations, including persistent and transient change, congestion, and topology changes. We find that one filter, based on techniques from ########### ####### ##### ##, provides performance superior to the other three. Compared to two EWMA filters, one agile and the other stable, it is able to offer the agility of the former in four of five scenarios and the stability of the latter in three of four scenarios.
207|Every Joule is Precious: The Case for Revisiting Operating System Design for Energy Efficiency|this paper, we propose the systematic re-examination of all aspects of operating system design and implementation from the point of view of energy efficiency rather than the more traditional OS metric of maximizing performance. In [7], we made the case for energy as a first-class OS-managed resource. We emphasized the benefits of higher-level control over energy usage policy and the application/OS interactions required to achieve them. This paper explores the implications that this major shift in focus can have upon the services, policies, mechanisms, and internal structure of the OS itself based on our initial experiences with rethinking system design for energy efficiency.
208|Sharing and Protection in a Single Address Space Operating System|The appearance of 64-bit address space architectures, such as the DEC Alpha, HP PA-RISC,  and MIPS R4000, signals a radical shift in the amount of address space available to operating  systems and applications. This shift provides the opportunity to reexamine fundamental operating  system structure specifically, to change the way that operating systems use address  space. This paper
209|Demand-driven Service Differentiation in Cluster-based Network Servers|Service differentiation that provides prioritized service qualities to multiple classes of client requests can effectively utilize available server resources. This paper studies how demand-driven service differentiation in terms of end-user performance can be supported in cluster-based network servers. Our objective is to deliver better services to high priority request classes without over-sacrificing low priority classes. To achieve this objective, we propose a dynamic scheduling scheme, called DDSD, that adapts to fluctuating request resource demands by periodically repartitioning servers. This scheme also employs priority-based admission control to drop excessive user requests and achieve soft performance guarantees. For each scheduling period, our scheme monitors the system status and uses a queuing model to approximate server behaviors and guide resource allocation. Our experiments show that the proposed technique achieves demand-driven service differentiation while maximizing resource utilization and that it can substantially outperform static server partitioning. 
210|Energy is just another resource: Energy accounting and energy pricing in the Nemesis OS|In this position paper, we argue that, with an appropriate operating system structure, energy in mobile computers can be treated and managed as just another resource. In particular, we investigate how energy management could be added to the Nemesis OS which provides detailed and accurate resource accounting capabilities in order to provide Quality of Service (QoS) guarantees for all resources to applications. We argue that, with such an operating system, accounting of energy to individual processes can be achieved. Furthermore, we investigate how an economic model, proposed for congestion avoidance in computer network, and recently applied to CPU resource management, can be used as a dynamic, decentralised energy management system, forming a collaborative environment between operating system and applications. 1
211|Isolation with Flexibility: A Resource Management Framework for Central Servers|Proportional-share resource management is becoming increasingly important in today&#039;s computing environments. In particular, the growing use of the computational resources of central service providers argues for a proportional-share approach that allows resource principals to obtain allocations that reflect their relative importance. In such environments, resource principals must be isolated from one another to prevent the activities of one principal from impinging on the resource rights of others. However, such isolation limits the flexibility with which resource allocations can be modified to reflect the actual needs of applications. We present extensions to the lottery-scheduling resource management framework that increase its flexibility while preserving its ability to provide secure isolation. To demonstrate how this extended framework safely overcomes the limits imposed by existing proportional-share schemes, we have implemented a prototype system that uses the framework to manage...
212|High-Performance Web Site Design Techniques|Performance and high availability are critical at Web sites that receive large numbers of requests. This article presents several techniques—including redundant hardware, load balancing, Web server acceleration, and efficient management of dynamic data—that can be used at popular sites to improve performance and availability. We describe how we deployed several of these techniques at the official Web site for the 1998 Olympic Winter Games in Nagano, Japan, which was one of the most popular sites up to its time. In fact, the Guinness Book of World Records recognized the site on 14 July 1998 for setting two records: ¦ “Most Popular Internet Event Ever Recorded, ” based on the officially audited figure of 634.7 million requests over the 16 days of the Olympic Games; and ¦ “Most Hits on an Internet Site in One Minute, ” based on the officially audited figure of 110,414 hits received in a single minute around the time of the women’s freestyle figure skating.
213|A Measurement-Based Admission-Controlled Web Server|Current HTTP servers process requests using a first come first serve queuing policy. What this implies is that the web server must process each request as it arrives. The result is that the more requests a client makes, the more replies the server will generate in response. Unfortunately, the bandwidth of the network and the processing capabilities of the server are often limited resulting in an aggressive client, or sets of clients, consuming the majority of the server&#039;s resources, limiting other clients&#039; ability to use their fair allocation. While the traditional behavior of a web server works efficiently for a web site that is non-discriminating towards all clients, guaranteeing service for preferred clients from the server itself is not yet possible. This paper describes the algorithm we have designed and implemented on the Apache HTTP server, which has been shown to be effective in allocating configurable fixed percentages of bandwidth across numerous simultaneous clients, indepen...
214|Virtual Services - A New Abstraction for Server Consolidation|Modern server operating systems (OS&#039;s) do not address the issue of interference between competing applications. This deficiency is a major road-block for Internet and Application Service Providers who want to multiplex server resources among their business clients. To insulate applications from each other, we introduce Virtual Services (VSs). Besides providing per-service resource budgets, VSs drastically reduce cross-service interference in the presence of shared backend services, such as databases and name services.  VSs provide dynamic per-service resource partitioning and management in a manner completely transparent to applications. To accomplish this goal, we introduce a kernel-based work classification mechanism called gates. Gates track work that propagates from one service to another and are configured by the system administrator via simple rules. They automate the binding of processes and sockets to VSs, and ensure that any work done on behalf of a VS, even if it is done by s...
215|Energy Needs In An Internet Economy: A Closer Look At Data Centers|Introduction  II. Data Center Fundamentals  III. The Current Debate  IV. Defining Common Metrics  V. Estimating Data Center Loads  VI. Using Measured Data to Confirm Power Needs  VII. Reasons for Exaggerated Forecasts  VIII. Implications of Findings  IX. Conclusions  Appendices:  Appendix A. Detailed PDU Data  Appendix B. Frequently Used Terms  References  Mitchell-Jackson 05/16/2001 2  I. Introduction  Founded on the dispersion of the personal computer, the expansion of the nation&#039;s fiber optic network, and the spread of the World Wide Web, a &#034;new economy&#034; has emerged. But new technologies and new ways of conducting business are not the only changes. This Internet economy also brings with it new requirements for power and power systems.  In many ways, the Internet has facilitated a move toward more decentralized systems. Customers can now use the Internet to purchase products from stores around the country rather than being limited to stores in their neigh
216|Differentiated and Predictable Quality of Service in Web Server Systems|As the World Wide Web experiences increasing commercial and mission-critical use, server systems are expected to deliver high and predictable performance. The phenomenal improvement in microprocessor speeds, coupled with the deployment of clusters of commodity workstations has enabled server systems to meet the continually increasing performance demands in a cost-effective and scalable manner. However, as the volume, variety and sophistication of services oered by server systems increase, eective support for providing dierentiated and predictable quality of service has also become important. For example, it is often desirable to dierentiate between the resources allocated to virtual web sites hosted on a server system so as to provide predictable performance to individual sites, regardless of the load imposed upon others. Server systems lack adequate support for providing predictable performance to hosted services in terms of metrics that are meaningful to server applications, such...
217|Exokernel: An Operating System Architecture for Application-Level Resource Management|We describe an operating system architecture that securely multiplexes machine resources while permitting an unprecedented degree of application-specific customization of traditional operating system abstractions. By abstracting physical hardware resources, traditional operating systems have significantly limited the performance, flexibility, and functionality of applications. The exokernel architecture removes these limitations by allowing untrusted software to implement traditional operating system abstractions entirely at application-level. We have implemented a prototype exokernel-based system that includes Aegis, an exokernel, and ExOS, an untrusted application-level operating system. Aegis defines the low-level interface to machine resources. Applications can allocate and use machine resources, efficiently handle events, and participate in resource revocation. Measurements show that most primitive Aegis operations are 10–100 times faster than Ultrix,a mature monolithic UNIX operating system. ExOS implements processes, virtual memory, and inter-process communication abstractions entirely within a library. Measurements show that ExOS’s application-level virtual memory and IPC primitives are 5–50 times faster than Ultrix’s primitives. These results demonstrate that the exokernel operating system design is practical and offers an excellent combination of performance and flexibility. 1
218|Active Messages: a Mechanism for Integrated Communication and Computation|The design challenge for large-scale multiprocessors is (1) to minimize communication overhead, (2) allow communication to overlap computation, and (3) coordinate the two without sacrificing processor cost/performance. We show that existing message passing multiprocessors have unnecessarily high communication costs. Research prototypes of message driven machines demonstrate low communication overhead, but poor processor cost/performance. We introduce a simple communication mechanism, Active Messages, show that it is intrinsic to both architectures, allows cost effective use of the hardware, and offers tremendous flexibility. Implementations on nCUBE/2 and CM-5 are described and evaluated using a split-phase shared-memory extension to C, Split-C. We further show that active messages are sufficient to implement the dynamically scheduled languages for which message driven machines were designed. With this mechanism, latency tolerance becomes a programming/compiling concern. Hardware support for active messages is desirable and we outline a range of enhancements to mainstream processors.  
219|Efficient Software-Based Fault Isolation|One way to provide fault isolation among cooperating software modules is to place each in its own address space. However, for tightly-coupled modules, this solution incurs prohibitive context switch overhead. In this paper, we present a software approach to implementing fault isolation within a single address space. Our approach has two parts. First, we load the code and data for a distrusted module into its own fault domain, a logically separate portion of the application&#039;s address space. Second, we modify the object code of a distrusted module to prevent it from writing or jumping to an address outside its fault domain. Both these software operations are portable and programming language independent. Our approach poses a tradeo relative to hardware fault isolation: substantially faster communication between fault domains, at a cost of slightly increased execution time for distrusted modules. We demonstrate that for frequently communicating modules, implementing fault isolation in software rather than hardware can substantially improve end-to-end application performance.
220|Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism|Threads are the vehicle,for concurrency in many approaches to parallel programming. Threads separate the notion of a sequential execution stream from the other aspects of traditional UNIX-like processes, such as address spaces and I/O descriptors. The objective of this separation is to make the expression and control of parallelism sufficiently cheap that the programmer or compiler can exploit even fine-grained parallelism with acceptable overhead. Threads can be supported either by the operating system kernel or by user-level library code in the application address space, but neither approach has been fully satisfactory. This paper addresses this dilemma. First, we argue that the performance of kernel threads is inherently worse than that of user-level threads, rather than this being an artifact of existing implementations; we thus argue that managing par- allelism at the user level is essential to high-performance parallel computing. Next, we argue that the lack of system integration exhibited by user-level threads is a consequence of the lack of kernel support for user-level threads provided by contemporary multiprocessor operating systems; we thus argue that kernel threads or processes, as currently conceived, are the wrong abstraction on which to support user- level management of parallelism. Finally, we describe the design, implementation, and performance of a new kernel interface and user-level thread package that together provide the same functionality as kernel threads without compromis- ing the performance and flexibility advantages of user-level management of parallelism.
221|Extensibility, safety and performance in the SPIN operating system|This paper describes the motivation, architecture and performance of SPIN, an extensible operating system. SPIN provides an extension infrastructure, together with a core set of extensible services, that allow applications to safely change the operating system&#039;s interface and implementation. Extensions allow an application to specialize the underlying operating system in order to achieve a particular level of performance and functionality. SPIN uses language and link-time mechanisms to inexpensively export ne-grained interfaces to operating system services. Extensions are written in a type safe language, and are dynamically linked into the operating system kernel. This approach o ers extensions rapid access to system services, while protecting the operating system code executing within the kernel address space. SPIN and its extensions are written in Modula-3 and run on DEC Alpha workstations. 1
222|Lightweight remote procedure call|Lightweight Remote Procedure Call (LRPC) is a communication facility designed and optimized for communication between protection domains on the same machine. In contemporary small-kernel operating systems, existing RPC systems incur an unnecessarily high cost when used for the type of communication that predominates-between protection domains on the same machine. This cost leads system designers to coalesce weakly related subsystems into the same protection domain, trading safety for performance. By reducing the overhead of same-machine communication, LRPC encourages both safety and performance. LRPC combines the control transfer and communication model of capability systems with the programming semantics and large-grained protection model of RPC. LRPC achieves a factor-of-three performance improvement over more traditional approaches based on independent threads exchanging messages, reducing the cost of same-machine communication to nearly the lower bound imposed by conventional hardware. LRPC has been integrated into the Taos operating system of the DEC SRC Firefly multiprocessor workstation.
223|Experiences with the amoeba distributed operating system|The Amoeba distributed operating system has been in development and use for over eight years now. In this paper we describe the present system and our experience with it—what we did right, but also what we did wrong. Among the things done right were basing the system on objects, using a single uniform mechanism (capabilities) for naming and protecting them in a location independent way, and designing a completely new, and very fast file system. Among the things done wrong were having threads not be pre-emptable, initially building our own homebrew window system, and not having a multicast facility at the outset.
224|The Packet Filter: An Efficient Mechanism for User-level Network Code|Code to implement network protocols can be either inside the kernel of an operating system or in user-level processes. Kernel-resident code is hard to develop, debug, and maintain, but user-level implementations typically incur significant overhead and perform poorly.  The performance of user-level network code depends on the mechanism used to demultiplex received packets. Demultiplexing in a user-level process increases the rate of context switches and system calls, resulting in poor performance. Demultiplexing in the kernel eliminates unnecessary overhead.  This paper describes the packet filter, a kernel-resident, protocolindependent packet demultiplexer. Individual user processes have great flexibility in selecting which packets they will receive. Protocol implementations using the packet filter perform quite well, and have been in production use for several years.  
225|Hints for Computer Systems Design|Studying the design and implementation of a number of computer has led to some general hints for system design. They are described here and illustrated by many examples, ranging from hardware such as the Alto and the Dorado to application programs such as Bravo and Star. 1.
226|Overview of the CHORUS Distributed Operating Systems|The CHORUS technology has been designed for building new generations of open, distributed, scalable operating systems. CHORUS has the following main characteristics:  # a communication-based architecture, relying on a minimal Nucleus which integrates distributed processing and communication at the lowest level, and which implements generic services used by a set of subsystem servers to extend standard operating system interfaces. A UNIX subsystem has been developed; other subsystems such as objectoriented systems are planned;  # a real-time Nucleus providing real-time services which are accessible to system programmers;   # a modular architecture providing scalability, and allowing, in particular, dynamic configuration of the system and its applications over a wide range of hardware and network configurations, including parallel and multiprocessor systems. CHORUS-V3 is the current version of the CHORUS Distributed Operating System, developed by Chorus systemes. Earlier versions were st...
227|Virtual Memory Primitives for User Programs|Memory Management Units (MMUs) are traditionally used by operating systems to implement disk-paged virtual memory. Some operating systems allow user programs to specify the protection level (inaccessible, readonly. read-write) of pages, and allow user programs t.o handle protection violations. bur. these mechanisms are not. always robust, efficient, or well-mat. ched to the needs of applications.
228|Improving IPC by kernel design|Inter-process communication (ipc) has to be fast and e ective, otherwise programmers will not use remote procedure calls (RPC), multithreading and multitasking adequately. Thus ipc performance is vital for modern operating systems, especially µ-kernel based ones. Surprisingly, most µ-kernels exhibit poor ipc performance, typically requiring 100 µs for a short message transfer on a modern processor, running with 50 MHz clock rate. In contrast, we achieve 5 µs; a twentyfold improvement. This paper describes the methods and principles used, starting from the architectural design and going down to the coding level. There is no single trick to obtaining this high performance; rather, a synergetic approach in design and implementation on all levels is needed. The methods and their synergy are illustrated by applying them to a concrete example, the L3-kernel (an industrial-quality operating system in daily use at several hundred sites). The main ideas are to guide the complete kernel design by the ipc requirements, and to make heavy use of the concept of virtual address space inside the-kernel itself. As the L3 experiment shows, significant performance gains are possible: compared with Mach, they range from a factor of 22 (8-byte messages) to 3 (4-Kbyte messages). Although hardware specific details in uence both the design and implementation, these techniques are applicable to the whole class of conventional general
229|Experiences with a High-Speed Network Adaptor: A Software Perspective|This paper describes our experiences, from a software perspective, with the OSIRIS network adaptor. It first identifies the problems we encountered while programming OSIRIS and optimizing network performance, and outlines how we either addressed them in the software, or had to modify the hardware. It then describes the opportunities provided by OSIRIS that we were able to exploit in the host operating system (OS); opportunities that suggested techniques for making the OS more effective in delivering network data to application programs. The most novel of these techniques, called application device channels, gives application programs running in user space direct access to the adaptor. The paper concludes with the lessons drawn from this work, which we believe will benefit the designers of future network adaptors. 1 Introduction  With the emergence of high-speed network facilities, several research efforts are focusing on the design and implementation of network adaptors [5, 2, 3, 16, 2...
230|Stride Scheduling: Deterministic Proportional-Share Resource Management|This paper presents stride scheduling, a deterministic scheduling technique that efficiently supports the same flexible resource management abstractions introduced by lottery scheduling. Compared to lottery scheduling, stride scheduling achieves significantly improved accuracy over relative throughput rates, with significantly lower response time variability. Stride scheduling implements proportional-share control over processor time and other resources by cross-applying elements of rate-based flow control algorithms designed for networks. We introduce new techniques to support dynamic changes and higher-level resource management abstractions. We also introduce a novel hierarchical stride scheduling algorithm that achieves better throughput accuracy and lower response time variability than prior schemes. Stride scheduling is evaluated using both simulations and prototypes implemented for the Linux kernel.
231|Accent: A Communication Oriented Network Operating System Kernel|Accent is a communication oriented operating system kernel being built at Carnegie-Mellon University to support the distributed personal computing project, Spice, and the development of a fault-tolerant distributed sensor network (DSN). Accent is. built around a single, powerful abstraction of communication between processes, with all kernel functions, such as device access and virtual memory management accessible through messages and distributable throughout a network. In this paper, specific attention is given to system supplied facilities which support transparent network access and fault-tolerant behavior. Many of these facilities are already being provided under a modified version of VAX/UNIX. The Accent system itself is currently being implemented on the Three Rivers Corp. PERQ. Keywords: Inter-process communication, networking,
232|Implementation and Performance of Application-Controlled File Caching|Traditional file system implementations do not allow applications to control file caching replacement decisions.  We have implemented two-level replacement, a scheme that allows applications to control their own cache replacement, while letting the kernel control the allocation of cache space among processes. We designed an interface to let applications exert control on replacement via a set of directives to the kernel. This is effective and requires low overhead.  We demonstrate that for applications that do not perform well under traditional caching policies, the combination of good application-chosen replacement strategies, and our kernel allocation policy LRU-SP, can reduce the number of block I/Os by up to 80%, and can reduce the elapsed time by up to 45%. We also show that LRU-SP is crucial to the performance improvement for multiple concurrent applications: LRUSP fairly distributes cache blocks and offers protection against foolish applications.   
233|PATHFINDER: A Pattern-Based Packet Classifier|This paper describes a pattern-based approach to building packet classifiers. One novelty of the approach is that it can be implemented efficiently in both software and hardware. A performance study shows that the software implementation is about twice as fast as existing mechanisms, and that the hardware implementation is currently able to keep up with OC-12 (622Mbps) network links and is likely to operate at gigabit speeds in the near future. 1 Introduction A packet classifier is a mechanism that inspects incoming network packets, and based on the values found in select header fields, determines how each is to be processed. A classifier can be thought of as assigning a tag (type) to each packet, or alternatively, identifying the flow or path to which the packet belongs. We call this mechanism a packet classifier rather than a packet filter because it more accurately describes the function being performed---it classifies all packets, rather than filtering out select packets. Packet ...
234|Efficient Packet Demultiplexing for Multiple Endpoints and Large Messages|This paper describes a new packet filter mechanism that efficiently dispatches incoming network packets to one of multiple endpoints, for example address spaces. Earlier packet filter systems iteratively applied each installed filter against every incoming packet, resulting in high processing overhead whenever multiple filters existed. Our new packet filter provides an associative match function that enables similar but not identical filters to be combined together into a single filter. The filter mechanism, which we call the Mach Packet Filter (MPF), has been implemented for the Mach 3.0 operating system and is being used to support endpoint-based protocol processing, whereby each address space implements its own suite of network protocols. With large numbers of registered endpoints, MPF outperforms the earlier BSD Packet Filter (BPF) by over a factor of four. MPF also allows a filter program to dispatch fragmented packets, which was quite difficult with previous filter mechanisms.  
235|Sharing and protection in a single-address-space operating system|This article explores memory sharing and protection support in Opal, a single-address-space operating system designed for wide-address (64-bit) architectures. Opal threads execute within protection domains in a single shared virtual address space. Sharing is simplified, because addresses are context independent. There is no loss of protection, because addressability and access are independent; the right to access a segment is determined by the protection domain in which a thread executes. This model enables beneficial code- and data-sharing patterns that are currently prohibitive, due in part to the inherent restrictions of multiple address spaces, and in part to Unix programming style. We have designed and implemented an Opal prototype using the Mach 3.0 microkernel as a base. Our implementation demonstrates how a single-address-space structure can be supported alongside of other environments on a modern microkernel operating system, using modern wide-address architectures. This article justifies the opal model and its goals for sharing and protection, presents the system and its abstractions, describes the prototype implementation,
236|Threads and Input/Output in the Synthesis kernel|The Synthesis operating system kernel combines several techniques to provide high performance, including kernel code synthesis, ne-grain scheduling, and optimistic synchronization. Kernel code synthesis reduces the execution path for frequently used kernel calls. Optimistic synchronization increases concurrency within the kernel. Their combination results in signi cant performance improvement over traditional operating system implementations. Using hardware and software emulating a SUN 3/160 running SUNOS, Synthesis achieves several times to several dozen times speedup for Unix kernel calls and context switch times of 21 microseconds or faster. 1
237|Limits to Low-Latency Communication on High-Speed Networks|The throughput of local area networks is rapidly increasing. For example, the bandwidth of new ATM networks and FDDI token rings is an order of magnitude greater than that of Ethernets. Other network technologies promise a bandwidth increase of yet another order of magnitude in a few years. However, in distributed systems, lowered latency rather than increased throughput is often of primary concern. This paper examines the system-level effects of newer high-speed network technologies on low-latency, cross-machine communications. To evaluate a number of influences, both hardware and software, we designed and imple-mented a new remote procedure call system targeted at providing low latency. We then ported this system to several hardware platforms (DECstation and SPARCstation) with several differ-ent networks and controllers (ATM, FDDI, and Ethernet). Comparing these systems allows us to explore the performance impact of alternative designs in the communication system with respect to achieving low latency, e.g., the network, the network controller, the host architecture and cache system, and the kernel and user-level runtime software. Our RPC system, which achieves substantially reduced call times (170 pseconds on an ATM network using DECstation 5000/200 hosts), allow us to isolate those components of next-
238|Pilot: An Operating System for a Personal Computer|this paper was presented at the 7th ACM Symposium on Operating Systems Principles, Pacific Grove, Calif., Dec. 10-12, 1979
239|Synthesis: An Efficient Implementation of Fundamental Operating System Services|This dissertation shows that operating systems can provide fundamental services an order of magnitude more efficiently than traditional implementations. It describes the implementation of a new operating system kernel, Synthesis, that achieves this level of performance. The Synthesis kernel combines several new techniques to provide high performance without sacrificing the expressive power or security of the system. The new ideas include: ffl Run-time code synthesis --- a systematic way of creating executable machine code at runtime to optimize frequently-used kernel routines --- queues, buffers, context switchers, interrupt handlers, and system call dispatchers --- for specific situations, greatly reducing their execution time. ffl Fine-grain scheduling --- a new process-scheduling technique based on the idea of feedback that performs frequent scheduling actions and policy adjustments (at submillisecond intervals) resulting in an adaptive, self-tuning system that can support real-ti...
240|Architectural support for translation table management in large address space machines|Virtual memoy page translation tables provide mappings from virtual to physical addresses. When the hardware controlled Tratmlation L.ookaside Buffers (TLBs) do not contain a translation, these tables provide the translation. Approaches to the structure and management of these tables vary from full hardware implementations to complete software based algon”thms. The size of the virtual aaliress space used by processes is rapidly growing beyond 32 bits of address. As the utilized address space increases, new problems and issues surjace. Traditional methoak for managing the page translation tables are inappropriate for large address space architectures. The Hashed Page Table (HPI’), described here, provides a very fast and space ejicient translation table that reduces ovdwad by splitting TLB management responsibilities between hardware and software. Measurements demonstrate its applicability to a diverse range of operating systems and workloads and, in particular, to large virtual address space machines. In simulations of over 4 billion instructions, improvements of 5 to IO % were observed. 1.
241|Design Tradeoffs for Software-Managed TLBs|this paper appeared in the Proceedings of the 20th Annual International Symposium on Computer Architecture, San Diego, May 1993. Authors&#039; address: Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109-2122 This work was supported by Defense Advanced Research Projects Agency under DARPA/ARO Contract Number DAAL03-90-C-0028 and a National Science Foundation Graduate Fellowship.  Uhlig et al. . 2 within the kernel. These and related operating system trends place greater stress upon the TLB by increasing miss rates and, hence, decreasing overall system performance. This paper explores these issues by examining design trade-offs for software-managed TLBs and their impact, in conjunction with various operating systems, on overall system performance. To examine issues which cannot be adequately modeled with simulation, we have developed a system analysis tool called Monster, which enables us to monitor actual systems. We have also developed a novel TLB simulator called Tapeworm, which is compiled directly into the operating system so that it can intercept all TLB misses caused by both user process and OS kernel memory references. The information that Tapeworm extracts from the running system is used to obtain TLB miss counts and to simulate different TLB configurations. The remainder of this paper is organized as follows: Section 2 examines previous TLB and OS research related to this work. Section 3 describes our analysis tools, Monster and Tapeworm. The MIPS R2000 TLB structure and its performance under Ultrix, OSF/1 and Mach 3.0 are explored in Section 4. Hardware- and software-based performance improvements are presented in Section 5. Section 6 summarizes our conclusions. 2 RELATED WORK By caching page table entries, TLBs g...
242|SPIN - an extensible microkernel for application-specific operating system services|Application domains, such as multimedia, databases, and parallel computing, require operating system services with high performance and high functionality. Existing operating systems provide fixed interfaces and implementations to system services and resources. This makes them inappropriate for applications whose resource demands and usage patterns are poorly matched by the services provided. The SPIN operating system enables system services to be defined in an application-specific fashion through an extensible microkernel. It offers fine-grained control over a machine&#039;s logical and physical resources to applications through run-time adaptation of the system to application requirements. 1
243|Hardware and Software Support for Efficient Exception Handling|Program-synchronous exceptions, for example, breakpoints, watchpoints, illegal opcodes, and memory access violations, provide information about exceptional conditions, interrupting the program and vectoring to an operating system handler. Over the last decade, however, programs and run-time systems have increasingly employed these mechanisms as a performance optimization to detect normal and expected conditions. Unfortunately, current architecture and operating system structures are designed for exceptional or erroneous conditions, where performance is of secondary importance, rather than normal conditions. Consequently, this has limited the practicality of such hardware-based detection mechanisms. We propose both hardware and software structures that permit efficient handling of synchronous exceptions by user-level code. We demonstrate a software implementation that reduces exceptiondelivery cost by an order-of-magnitude on current RISC processors, and show the performance benefits o...
244|Fast Mutual Exclusion for Uniprocessors|In this paper we describe restartable atomic sequences, an optimistic mechanism for implementing simple atomic operations (such as Test-And-Set) on a uniprocessor. A thread that is suspended within a restartable atomic sequence is resumed by the operating system at the beginning of the sequence, rather than at the point of suspension. This guarantees that the thread eventually executes the sequence atomically. A restartable atomic sequence has signi cantly less overhead than other software-based synchronization mechanisms, such askernel emulation or software reservation. Consequently, it is an attractive alternative for use on uniprocessors that do not support atomic operations. Even on processors that do support atomic operations in hardware, restartable atomic sequences can have lower overhead. We describe di erent implementations of restartable atomic sequences for the Mach 3.0 and Taos operating systems. These systems &#039; thread management packages
245|Software Prefetching and Caching for Translation Lookaside Buffers|A number of interacting trends in operating system structure, processor architecture, and memory systems are increasing both the rate of translation lookaside buffer (TLB) misses and the cost of servicing a miss. This paper presents two novel software schemes, implemented under Mach 3.0, to decrease both the number and the cost of kernel TLB misses (i.e., misses on kernel data structures, including user page tables). The first scheme is a new use of prefetching for TLB entries on the IPC path, and the second scheme is a new use of software caching of TLB entries for hierarchical page table organizations.  For a range of applications, prefetching decreases the number of kernel TLB misses by 40% to 50%, and caching decreases TLB penalties by providing a fast path for over 90% of the misses. Our caching scheme also decreases the number of nested TLB traps due to the page table hierarchy, reducing the number of kernel TLB miss traps for applications by 20% to 40%. Prefetching and caching, ...
246|Tools for the Development of Application-Specific Virtual Memory Management|While many applications incur few page faults, some scientific and database applications perform poorly when running on top of a traditional virtual memory implementation. To help address this problem, several systems have been built to allow each program the flexibility to use its own application-specific page replacement policy, in place of the generic policy provided by the operating system. This has the potential to improve performance for the class of applications limited by virtual memory behavior; however, to realize this performance gain, application developers must re-implement much of the virtual memory system, a non-trivial programming task. Our goal is to make it easy for programmers to develop new application-specific page replacement policies. To do this, we have implemented (i) an extensible object-oriented user-level virtual memory system and (ii) a graphical performance monitor for virtual memory behavior. Together, these help the user to identify problems with an appl...
248|The Operating System Kernel as a Secure Programmable Machine|To provide modularity and performance, operating system kernels should have only minimal embedded functionality. Today&#039;s operating systems are large, inefficient and, most importantly, inflexible. In our view, most operating system performance and flexibility problems can be eliminated simply by pushing the operating system interface lower. Our goal is to put abstractions traditionally implemented by the kernel out into user-space, where user-level libraries and servers abstract the exposed hardware resources. To achieve this goal, we have defined a new operating system structure, exokernel, that safely exports the resources defined by the underlying hardware. To enable applications to benefit from full hardware functionality and performance, they are allowed to download additions to the supervisor-mode execution environment. To guarantee that these extensions are safe, techniques such as code inspection, inlined cross-domain procedure calls, and secure languages are used. To test and ...
249|An open operating system for a single-user machine|The file system and modularization of a single-user operating system are described. The main points of interest are the openness of the system, which establishes no sharp boundary between itself and the user&#039;s programs, and the techniques used to make the system robust. 1.
250|How to Use a 64-Bit Virtual Address Space|Most operating systems execute programs in private address spaces communicating through messages or files. The traditional private address space model was developed for 16- and 32-bit architectures, on which virtual addresses are a scarce resource. The recent appearance of architectures with flat 64-bit virtual addressing opens an opportunity to reconsider our use of virtual address spaces. In this paper we argue for an alternative addressing model, in which all programs and data reside in a single global virtual address space shared by multiple protection domains. Hardware-based memory protection exists within the single address space, providing firewalls as strong as in conventional systems. We explore the tradeoffs in the use of a global virtual address space relative to the private address space model. We contend that a shared address space can eliminate obstacles to efficient sharing and exchange of data structures that are inherent in private address space systems. The shared add...
251|Vino: an integrated platform for operating systems and database research|In 1981, Stonebraker wrote: Operating system services in many existing systems are either too slow or inappropriate. Current DBMSs usually provide their own and make little or no use of those o ered by the operating system. [STON81] The standard operating system model has changed little since that time, and we believe that, at its core, it is the wrong model for DBMS and other resource-intensive applications. The standard model is in exible, uncooperative, and irregular in its treatment of resources. We describe the design of a new system, the VINO kernel, which addresses the limitations of standard operating systems. It focuses on three key ideas: Applications direct policy. Kernel mechanisms are reusable by applications. All resources share a common extensible interface. VINO&#039;s power and exibility make it an ideal platform for the design and implementation of traditional and modern database management systems. 1
252|SPACE: A New Approach to Operating System Abstraction|Object-oriented operating systems, as well as conventional O/S designs, present an overly restrictive level of abstraction to the programmer. Models of objects, processes, concurrency, etc., are embedded within the system in such a way that they are difficult to extend or replace. SPACE is an extensible operating system being developed for research into object-oriented and distributed systems design. SPACE uses capability mechanisms based on the manipulation of address spaces to provide low-level kernel primitives from which higherlevel abstractions can be constructed. Standard micro-kernel abstractions such as processes, virtual memory, interprocess communication, and object models are built outside the kernel in SPACE, using the SPACE-kernel primitives: spaces, domains, and portals. Multiple versions of the standard O/S abstractions can coexist and interact.
254|FUGU: Implementing Translation and Protection in a  Multiuser Multimodel Multiprocessor|Multimodel multiprocessors provide both shared memory and message passing primitives to the  user for efficient communication. In a multiuser machine, translation permits machine resources to be  virtualized and protection permits users to be isolated. The challenge in a multiuser multiprocessor is  to provide translation and protection sufficient for general-purpose computing without compromising  communication performance, particularly the performance of communication between parallel threads  belonging to the same computation. FUGU is a proposed architecture that integrates translation and  protection with a set of communication mechanisms originally designed for high performance on a  single-user, physically-addressed, large-scale, multimodel multiprocessor. Communication
255|MULTILISP: a language for concurrent symbolic computation|Multilisp is a version of the Lisp dialect Scheme extended with constructs for parallel execution. Like Scheme, Multilisp is oriented toward symbolic computation. Unlike some parallel programming languages, Multilisp incorporates constructs for causing side effects and for explicitly introducing parallelism. The potential complexity of dealing with side effects in a parallel context is mitigated by the nature of the parallelism constructs and by support for abstract data types: a recommended Multilisp programming style is presented which, if followed, should lead to highly parallel, easily understandable programs. Multilisp is being implemented on the 32-processor Concert multiprocessor; however, it is ulti-mately intended for use on larger multiprocessors. The current implementation, called Concert Multilisp, is complete enough to run the Multilisp compiler itself and has been run on Concert prototypes including up to eight processors. Concert Multilisp uses novel techniques for task scheduling and garbage collection. The task scheduler helps control excessive resource utilization by means of an unfair scheduling policy; the garbage collector uses a multiprocessor algorithm based on the incremental garbage collector of Baker.
256|A Methodology for Implementing Highly Concurrent Data Objects| A concurrent object is a data structure shared by concurrent processes. Conventional techniques for implementing concurrent objects typically rely on critical sections: ensuring that only one process at a time can operate on the object. Nevertheless, critical sections are poorly suited for asynchronous systems: if one process is halted or delayed in a critical section, other, nonfaulty processes will be unable to progress. By contrast, a concurrent object implementation is lock free if it always guarantees that some process will complete an operation in a finite number of steps, and it is wait free if it guarantees that each process will complete an operation in a finite number of steps. This paper proposes a new methodology for constructing lock-free and wait-free implementations of concurrent objects. The object’s representation and operations are written as stylized sequential programs, with no explicit synchronization. Each sequential operation is automatically transformed into a lock-free or wait-free operation using novel synchronization and memory management algorithms. These algorithms are presented for a multiple instruction/multiple data (MIMD) architecture in which n processes communicate by applying atomic read, wrzte, load_linked, and store_conditional operations to a shared memory.
257|The v distributed system|The V distributed System was developed at Stanford University as part of a research project to explore issues in distributed systems. Aspects of the design suggest important directions for the design of future operating systems and communication systems. 
258|The Amber System: Parallel Programming on a Network of Multiprocessors|Microprocessor-based shared-memory multiprocessors are becoming widely available and promise to provide cost-effective high-performance computing. This paper describes a programming system called Amber which permits a single application program to use a homogeneous network of multiprocessors in a uniform way, making the network appear to the application as an integrated, non-uniform memory access, shared-memory multiprocessor. This simplifies the development of applications and allows compute-intensive parallel programs to effectively harness the potential of multiple nodes. Amber programs are written using an object-oriented subset of the C++ programming language, supplemented with primitives for managing concurrency and distribution. Amber provides a network-wide shared-object virtual memory in which coherence is provided by hardware means for locally-executing threads, and by software means for remote accesses. Amber runs on top of the Topaz operating system on a network of DEC SRC ...
259|Experience with Processes and Monitors in Mesa|The use of monitors for describing concurrency has been much discussed in the literature. When monitors are used in real systems of any size, however, a number of problems arise which have not been adequately dealt with: the semantics of nested monitor calls; the various ways of defining the meaning of WAIT; priority scheduling; handling of timeouts, aborts and other exceptional conditions; interactions with process creation and destruction; monitoring large numbers of small objects. These problems are addressed by the facilities described here for concurrent programming in Mesa. Experience with several substantial applications gives us some confidence in the validity of our solutions.
260|Performance of Firefly RPC|In this paper, we report on the performance of the remote procedure call implementation for the Firefly multiprocessor and analyze the implemen-tation to account precisely for all measured latency. From the analysis and measurements, we estimate how much faster RPC could be if certain improve-ments were made. The elapsed time for an inter-machine call to a remote procedure that accepts no arguments and produces no results is 2.66 milliseconds. The elapsed time for an RPC that has a single 1440-byte result (the maximum result that will fit in a single packet) is 6.35 milliseconds. Maximum inter-machine throughput using RPC is 4.65 megabits/second, achieved with 4 threads making parallel RPCs that return the maximum sized single packet result. CPU utilization at maximum throughput is about 1.2 on the calling machine and a little less on the server. These measurements are for RPCs from user space on one machine to user space on another, using the installed system and a 10 megabit/second E...
261|The Performance Implications of Thread Management Alternatives for Shared-Memory Multiprocessors|Threads (“lightweight ” processes) have become. a common element of new languages and operating systems. This paper examines the performance implications of several data structure and algorithm alternatives for thread management in shared-memory multiprocessors. Both experimental measurements and analytical model projections are presented For applications with fine-grained parallelism, small differences in thread management are shown to have significant performance impact, often posing a tradeoff between throughput and latency. Per-processor data structures can be used to improve throughput, and in some circumstances to avoid locking, improving latency as well. The method used by processors to queue for locks is also shown to affect performance significantly. Normal methods of critical resource waiting can substantially degrade performance with moderate numbers of waiting processors. We present an Ethernet-style backoff algo rithm that largely eliminates~tiis effect. 1.
262|First-Class User-Level Threads|It is often desirable, for reasons of clarity, portability, and efficiency, to write parallel programs in which the number of processes is independent of the number of available processors. Several modern operating systems support more than one process in an address space, but the overhead of creating and synchronizing kernel processes can be high. Many runtime environments implement lightweight processes (threads) in user space, but this approach usually results in second-class status for threads, making it difficult or impossible to perform scheduling operations at appropriate times (e.g. when the current thread blocks in the kernel). In addition, a lack of common assumptions may also make it difficult for parallel programs or library routines that use dissimilar thread packages to communicate with each other, or to synchronize access to shared data. We describe a set of kernel mechanisms and conventions designed to accord first-class status to user-level threads, allowing them to be used in any reasonable way that traditional kernel-provided processes can be used, while leaving the details of their implementation to userlevel code. The key features of our approach are (1) shared memory for asynchronous communication between the kernel and the user, (2) software interrupts for events that might require action on the part of a user-level scheduler, and (3) a scheduler interface convention that facilitates interactions in user space between dissimilar kinds of threads. We have incorporated these mechanisms in the Psyche parallel operating system, and have used them to implement several different kinds of user-level threads. We argue for our approach in terms of both flexibility and performance.
263|Empirical Studies of Competitive Spinning for a Shared-Memory Multiprocessor|A common operation in multiprocessor programs is acquiring a lock to protect access to shared data. Typically, the requesting thread is blocked if the lock it needs is held by another thread. The cost of blocking one thread and activating another can be a substantial part of program execution time. Alternatively, the thread could spin until the lock is free, or spin for a while and then block. This may avoid context-switch overhead, but processor cycles may be wasted in unproductive spinning. This paper studies seven strategies for determining whether and how long to spin before blocking. Of particular interest are competitive strategies, for which the performance can be shown to be no worse than some constant factor times an optimal off-line strategy. The performance of five competitive strategies is compared with that of always blocking, always spinning, or using the optimal off-line algorithm. Measurements of lock-waiting time distributions for five parallel programs were used to co...
264|Synchronization Primitives for a Multiprocessor: A Formal Specification|Formal specifications of operating system interfaces can be a useful part of their documentation. We illustrate this by documenting the Threads synchronization primitives of the Taos operating system. We start with an informal description, present a way to formally specify interfaces in concurrent systems, and then give a formal specification of the synchronization primitives. We briefly discuss both the implementation and what we have learned from using the specification for more than a year. Our main conclusion is that programmers untrained in reading formal specifications have found this one helpful in getting their work done. iii  Introduction  The careful documentation of interfaces is an important step in the production of software upon which other software is to be built. If people are to use software without having to understand its implementation, documentation must convey semantic as well as syntactic information. When the software involves concurrency, adequate documentatio...
265|Multi-model parallel programming in Psyche|Many different parallel programming models, including lightweight processes that communicate with shared memory and heavyweight processes that communicate with messages, have been used to implement parallel applications. Unfortunately, operating systems and languages designed for parallel programming typically support only one model. Multi-model parallel programming is the simultaneous use of several different models, both across programs and within a single program. This paper describes multi-model parallel programming in the Psyche multiprocessor operating system. We explain why multi-model programming is desirable and present an operating system interface designed to support it. Through a series of three examples, we illustrate how the Psyche operating system supports different models of parallelism and how the different models are able to interact. 1.
266|Self-Similarity Through High-Variability: Statistical Analysis of Ethernet LAN Traffic at the Source Level|A number of recent empirical studies of traffic measurements from a variety of working packet networks have convincingly demonstrated that actual network traffic is self-similar or long-range dependent in nature (i.e., bursty over a wide range of time scales) -- in sharp contrast to commonly made traffic modeling assumptions. In this paper, we provide a plausible physical explanation for the occurrence of self-similarity in LAN traffic. Our explanation is based on new convergence results for processes that exhibit high variability  (i.e., infinite variance) and is supported by detailed statistical analyses of real-time traffic measurements from Ethernet LAN&#039;s at the level of individual sources. This paper is an extended version of [53] and differs from it in significant ways. In particular, we develop here the mathematical results concerning the superposition of strictly alternating ON/OFF sources. Our key mathematical result states that the superposition of many ON/OFF sources (also k...
267|Web Server Workload Characterization: The Search for Invariants (Extended Version)  (1996) |The phenomenal growth in popularity of the World Wide Web (WWW, or the Web) has made WWW traffic the largest contributor to packet and byte traffic on the NSFNET backbone. This growth has triggered recent research aimed at reducing the volume of network traffic produced by Web clients and servers, by using caching, and reducing the latency for WWW users, by using improved protocols for Web interaction. Fundamental to the goal of improving WWW performance is an understanding of WWW workloads. This paper presents a workload characterization study for Internet Web servers. Six different data sets are used in this study: three from academic environments, two from scientific research organizations, and one from a commercial Internet provider. These data sets represent three different orders of magnitude in server activity, and two different orders of magnitude in time duration, ranging from one week of activity to one year of activity. Throughout the study, emphasis is placed on finding wor...
268|Experimental Queueing Analysis with Long-Range Dependent Packet Traffic|Recent traffic measurement studies from a wide range of working packet networks have convincingly established the presence of significant statistical features that are characteristic of fractal traffic processes, in the sense that these features span many time scales. Of particular interest in packet traffic modeling is a property called long-range dependence, which is marked by the presence of correlations that can extend over many time scales. In this paper, we demonstrate empirically that, beyond its statistical significance in traffic measurements, long-range dependence has considerable impact on queueing performance, and is a dominant characteristic for a number of packet traffic engineering problems. In addition, we give conditions under which the use of compact and simple traffic models that incorporate long-range dependence in a parsimonious manner (e.g.,  fractional Brownian motion) is justified and can lead to new insights into the traffic management of high-speed networks. 1...
269|Characteristics of WWW Client-based Traces|The explosion of WWW traffic necessitates an accurate picture of WWW use, and in particular requires a good understanding of client requests for WWW documents. To address this need, we have collectedtraces of actual executions of NCSA Mosaic, reflecting over half a million user requests for WWW documents. In this paper we describe the methods we used to collect our traces, and the formats of the collected data. Next, we present a descriptive statistical summary of the traces we collected, which identifies a number of trends and reference patterns in WWW use. In particular, we show that many characteristics of WWW use can be modelled using power-law distributions, including the distribution of document sizes, the popularity of documents as a function of size, the distribution of user requests for documents, and the number of references to documents as a function of their overall rank in popularity (Zipf&#039;s law). Finally, we show how the power-law distributions derived from our traces can beused to guide system designers interested in caching WWW documents.
270|Characterizing Reference Locality in the WWW|As the World Wide Web (Web) is increasingly adopted as the infrastructure for large-scale distributed information systems, issues of performance modeling become ever more critical. In particular, locality of reference is an important property in the performance modeling of distributed information systems. In the case of the Web, understanding the nature of reference locality will help improve the design of middleware, such as caching, prefetching, and document dissemination systems. For example, good measurements of reference locality would allow us to generate synthetic reference streams with accurate performance characteristics, would allow us to compare empirically measured streams to explain differences, and would allow us to predict expected performance for system design and capacity planning. In this paper we propose models for both temporal and spatial locality of reference in streams of requests arriving at Web servers. We show that simple models based only on document popularity (likelihood of reference) are insufficient for capturing either temporal or spatial locality. Instead, we rely on an equivalent, but numerical, representation of a reference stream: a stack distance trace. We show that temporal locality can be characterized by
271|Empirically-Derived Analytic Models of Wide-Area TCP Connections: Extended Report|We analyze 2.5 million TCP connections that occurred during 14 wide-area traffic traces. The traces were gathered at five &#034;stub&#034; networks and two internetwork gateways, providing a diverse look at wide-area traffic. We derive analytic models describing the random variables associated with telnet, nntp, smtp, and ftp connections, and present a methodology for comparing the effectiveness of the analytic models with empirical models such as tcplib [DJ91]. Overall we find that the analytic models provide good descriptions, generally modeling the various distributions as well as empirical models and in some cases better.
272|Network Behavior of a Busy Web Server and its Clients|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There are two other research laboratories located in Palo Alto, the Network Systems
273|Empirical Model of WWW Document Arrivals at Access Link|The cable and telephone industries have already begun constructing the information super highway. The network capacity planning for data services, however, cannot start until a realistic traffic model is developed based on actual traffic data. Since the future data services are perceived to resemble today&#039;s World Wide Web (WWW) browsers, we propose in this paper a traffic model for the access network based on today&#039;s WWW traffic. The model is an ON-OFF two-state model with the ON period consisting of a sequence of document transmission requests from an individual subscriber. Actual traffic data with over 20,000 data points was used to fit distributions to the model. The ON and OFF periods are found to be of heavy-tailed Weibull and Pareto distributions, respectively. The inter-arrival times of requests within the ON periods can be described with another Weibull distribution. This empirical model can be used as a realistic basis for network capacity planning for the future access networ...
274|Using collaborative filtering to weave an information tapestry|predicated on the belief that information filtering can be more effective when humans are involved in the filtering process. Tapestry was designed to support both content-based filtering and collaborative filtering, which entails people collaborating to help each other perform filtering by recording their reactions to documents they read. The reactions are called annotations; they can be accessed by other people’s filters. Tapestry is intended to handle any incoming stream of electronic documents and serves both as a mail filter and repository; its components are the indexer, document store, annotation store, filterer, little box, remailer, appraiser and reader/browser. Tapestry’s client/server architecture, its various components, and the Tapestry query language are described.
275|Browsing electronic mail: Experiences interfacing a mail system to a DBMS|Abstract: A database management system provides the ideal support for electronic mail applications. The Walnut mail system built at the Xerox Palo Alto Research Center was recently redesigned to take better advantage of its underlying database facilities. The ability to pose ad-hoc queries with a “fill-in-the-form” browser allows people to browse their mail quickly and effectively, while database access paths guarantee fast retrieval of stored information. Careful consideration of the systems ’ usage was reflected in both the database schema representation and the user-interface for browsing mail.
276|The x-Kernel: An Architecture for Implementing Network Protocols|This paper describes a new operating system kernel, called the x-kernel, that provides an  explicit architecture for constructing and composing network protocols. Our experience  implementing and evaluating several protocols in the x-kernel shows that this architecture  is both general enough to accommodate a wide range of protocols, yet efficient enough to  perform competitively with less structured operating systems.  1 Introduction  Network software is at the heart of any distributed system. It manages the communication hardware that connects the processors in the system and it defines abstractions through which processes running on those processors exchange messages. Network software is extremely complex: it must hide the details of the underlying hardware, recover from transmission failures, ensure that messages are delivered to the application processes in the appropriate order, and manage the encoding and decoding of data. To help manage this complexity, network software is divi...
277|The sprite network operating system|Sprite is a new operating system for networked uniprocessor and multiprocessor workstations with large physical memories. It implements a set of kernel calls much like those of 4.3 BSD UNIX, with extensions to allow processes on the same workstation to share memory and to allow processes to migrate between workstations. The implementation of the Sprite kernel contains several interesting features, including a remote procedure call facility for communication between kernels, the use of prefix tables to implement a single file name space and to provide flexibility in administering the network file system, and large variable-size file caches on both client and server machines, which provide high performance even for diskless workstations.
278|A Stream Input-Output System|In a new version of the Unix operating system, a flexible coroutine-based design replaces the traditional rigid connection between processes and terminals or networks. Processing modules may be inserted dynamically into the stream that connects a user&#039;s program to a device. Programs may also connect directly to programs, providing interprocess communication. Introduction  The part of the Unix operating system that deals with terminals and other character devices has always been complicated. In recent versions of the system it has become even more so, for two reasons. 1) Network connections require protocols more ornate than are easily accommodated in the existing structure. A notion of &#034;line disciplines&#034; was only partially successful, mostly because in the traditional system only one line discipline can be active at a time. 2) The fundamental data structure of the traditional character I/O system, a queue of individual characters (the &#034;clist&#034;), is costly because it accepts and dispense...
279|Preserving and Using Context Information in Interprocess Communication|ion  Psync is based on a conversation abstraction that provides a shared message space through which a collection of processes exchange messages. The general form of this message space is defined by a directed acyclic graph that preserves the partial order of the exchanged messages. For the purpose of this section, we view a conversation as an abstract data type that is implemented in shared memory; Section 3 gives an algorithm for implementing a conversation in an unreliable network.  A conversation behaves much like any connection-oriented IPC abstraction: A well-defined set of processes---called participants---explicitly open a conversation, exchange messages through it, and close the conversation. Only processes that have been identified as participants may exchange message through the conversation, and this set is fixed for the duration of the conversation. Processes begin a conversation with the operations:  conv = active open(participant set)  conv = passive open(pid)  The first...
280|Distributed Process Groups in the V Kernel|The V kernel supports an abstraction of processes, with operations for interprocess communication, process management, and memory management. This abstraction is used as a software base for constructing distributed systems. As a distributed kernel, the V kernel makes intermachine bound-aries largely transparent. In this environment of many cooperating processes on different machines, there are many logical groups of processes. Examples include the group of tile servers, a group of processes executing a particular job, and a group of processes executing a distributed parallel computation. In this paper we describe the extension of the V kernel to support process groups. Operations on groups include group interprocess communication, which provides an application-level abstraction of network multicast. Aspects of the implementation and performance, and initial experience with applications are discussed.
281|An overview of the SR language and implementation|SR is a language for programming distributed systems ranging from operating systems to application programs. On the basis of our experience with the initial version, the language has evolved consider-ably. In this paper we describe the current version of SR and give an overview of its implementation. The main language constructs are still resources and operations. Resources encapsulate processes and variables that they share; operations provide the primary mechanism for process interaction. One way in which SR has changed is that both resources and processes are now created dynamically. Another change is that inheritance is supported. A third change is that the mechanisms for operation invocation-call and send-and operation implementation-proc and in-have been extended and integrated. Consequently, all of local and remote procedure call, rendezvous, dynamic process creation, asynchronous message passing, multicast, and semaphores are supported. We have found this flexibility to be very useful for distributed programming. Moreover, by basing SR on a small number of well-integrated concepts, the language has proved easy to learn and use, and it has a reasonably efficient implementation.
282|Performance of the World&#039;s Fastest Distributed Operating System|Distributed operating systems have been in the experimental stage for a  number of years now, but few have progressed to the point of actually being  used in a production environment. It is our belief that the reason lies primarily  with the performance of these systems---they tend to be fairly slow  compared to traditional single computer systems. The Amoeba system has  been designed with high performance in mind. In this paper some performance  measurements of Amoeba are presented and comparisons are made  with UNIX on the SUN, as well as with some other interesting systems. In  particular, short remote procedure calls take 1.4 msec and long data transfers  achieve a user-to-user bandwidth of 677 kbytes/sec. Furthermore, the file  server is so fast that it is limited by the communication bandwidth to 677  kbytes/sec. The real speed of the file server is too high to measure. To the  best of our knowledge, these are the best figures yet reported in the literature  for the class of hard...
283|Law and finance|This paper examines legal rules covering protection of corporate shareholders and creditors, the origin of these rules, and the qual-ity of their enforcement in 49 countries. The results show that common-law countries generally have the strongest, and French-civil-law countries the weakest, legal protections of investors, with German- and Scandinavian-civil-law countries located in the mid-dle. We also find that concentration of ownership of shares in the largest public companies is negatively related to investor protec-tions, consistent with the hypothesis that small, diversified share-holders are unlikely to be important in countries that fail to protect their rights. I. Overview of the Issues In the traditional finance of Modigliani and Miller (1958), securities are recognized by their cash flows. For example, debt has a fixed promised stream of interest payments, whereas equity entitles its
284|What Do We Know about Capital Structure? Some Evidence from International Data|We investigate the determinants of capital structure choice by analyzing the financing decisions of public firms in the major industrialized countries. At an aggregate level, firm leverage is fairly similar across the G-7 countries. We find that factors identified by previous studies as correlated in the cross-section with firm leverage in the U.S., are similarly correlated in other countries as well. However, a deeper examination of the U.S. and foreign evidence suggests that the theoretical underpinnings of the observed correlations are still largely unresolved.
285|The Legal Environment, Banks, and Long-Run Economic Growth|This paper examines the relationship between the legal system and banking development and traces this connection through to long-run rates of per capita GDP growth, capital stock growth, and productivity growth. The data indicate that countries where the legal system (1) emphasizes creditor rights and (2) rigorously enforces contracts have better developed banks than countries where laws do not give a high priority to creditors and where enforcement is lax. Furthermore, the exogenous component of banking development -- the component defined by the legal environment -- is positively and robustly associated with per capita growth, physical capital accumulation, and productivity growth. 
286|A survey of corporate governance|in the 10th Workshop on Corporate Governance and Investment for useful comments. The paper was written while Ken
287|Tinydb: An acquisitional query processing system for sensor networks|We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. Categories and Subject Descriptors: H.2.3 [Database Management]: Languages—Query languages; H.2.4 [Database Management]: Systems—Distributed databases; query processing
288|Directed Diffusion: A scalable and robust communication paradigm for sensor networks|Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.
289|System architecture directions for networked sensors|Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world or spread throughout our environment. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances. 
290|ANALYSIS OF WIRELESS SENSOR NETWORKS FOR HABITAT MONITORING|  We provide an in-depth study of applying wireless sensor networks (WSNs) to real-world habitat monitoring. A set of system design requirements were developed that cover the hardware design of the nodes, the sensor network software, protective enclosures, and system architecture to meet the requirements of biologists. In the summer of 2002, 43 nodes were deployed on a small island off the coast of Maine streaming useful live data onto the web. Although researchers anticipate some challenges arising in real-world deployments of WSNs, many problems can only be discovered through experience. We present a set of experiences from a four month long deployment on a remote island. We analyze the environmental and node health data to evaluate system performance. The close integration of WSNs with their environment provides environmental data at densities previously impossible. We show that the sensor data is also useful for predicting system operation and network failures. Based on over one million 2 Polastre et. al. data readings, we analyze the node and network design and develop network reliability profiles and failure models.
291|The Cricket Location-Support System|This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, locationdependent applications. It allows applications running on mobile and static nodes to learn their physical location by using listeners that hear and analyze information from beacons spread throughout the building. Cricket is the result of several design goals, including user privacy, decentralized administration, network heterogeneity, and low cost. Rather than explicitly tracking user location, Cricket helps devices learn where they are and lets them decide whom to advertise this information to; it does not rely on any centralized management or control and there is no explicit coordination between beacons; it provides information to devices regardless of their type of network connectivity; and each Cricket device is made from off-the-shelf components and costs less than U.S. $10. We describe the randomized algorithm used by beacons to transmit information, the use of concurrent radio and ultrasonic signals to infer distance, the listener inference algorithms to overcome multipath and interference, and practical beacon configuration and positioning techniques that improve accuracy. Our experience with Cricket shows that several location-dependent applications such as in-building active maps and device control can be developed with little effort or manual configuration.  1 
292|The nesC language: A holistic approach to networked embedded systems|We present nesC, a programming language for networked embedded systems that represent a new design space for application developers. An example of a networked embedded system is a sensor network, which consists of (potentially) thousands of tiny, lowpower “motes, ” each of which execute concurrent, reactive programs that must operate with severe memory and power constraints. nesC’s contribution is to support the special needs of this domain by exposing a programming model that incorporates event-driven execution, a flexible concurrency model, and component-oriented application design. Restrictions on the programming model allow the nesC compiler to perform whole-program analyses, including data-race detection (which improves reliability) and aggressive function inlining (which reduces resource consumption). nesC has been used to implement TinyOS, a small operating system for sensor networks, as well as several significant sensor applications. nesC and TinyOS have been adopted by a large number of sensor network research groups, and our experience and evaluation of the language shows that it is effective at supporting the complex, concurrent programming style demanded by this new class of deeply networked systems.
293|Timing-Sync Protocol for Sensor Networks|Wireless ad-hoc sensor networks have emerged as an interesting and important research area in the last few years. The applications envisioned for such networks require collaborative execution of a distributed task amongst a large set of sensor nodes. This is realized by exchanging messages that are timestamped using the local clocks on the nodes. Therefore, time synchronization becomes an indispensable piece of infrastructure in such systems. For years, protocols such as NTP have kept the clocks of networked systems in perfect synchrony. However, this new class of networks has a large density of nodes and very limited energy resource at every node; this leads to scalability requirements while limiting the resources that can be used to achieve them. A new approach to time synchronization is needed for sensor networks.
294|The Cougar Approach to In-Network Query Processing in Sensor Networks|The widespread distribution and availability of smallscale sensors, actuators, and embedded processors is transforming the physical world into a computing platform. One such example is a sensor network consisting of a large number of sensor nodes that combine physical sensing capabilities such as temperature, light, or seismic sensors with networking and computation capabilities. Applications range from environmental control, warehouse inventory, and health care to military environments. Existing sensor networks assume that the sensors are preprogrammed and send data to a central frontend where the data is aggregated and stored for offline querying and analysis. This approach has two major drawbacks. First, the user cannot change the behavior of the system on the fly. Second, conservation of battery power is a major design factor, but a central system cannot make use of in-network programming, which trades costly communication for cheap local computation.
295|A Transmission Control Scheme for Media Access in Sensor Networks|We study the problem of media access control in the novel regime of sensor networks, where unique application behavior and tight constraints in computation power, storage, energy resources, and radio technology have shaped this design space to be very different from that found in traditional mobile computing regime. Media access control in sensor networks must not only be energy efficient but should also allow fair bandwidth allocation to the infrastructure for all nodes in a multihop network. We propose an adaptive rate control mechanism aiming to support these two goals and find that such a scheme is most effective in achieving our fairness goal while being energy effcient for both low and high duty cycle of network traffic.
296|Routing indices for peer-to-peer systems|Finding information in a peer-to-peer system currently requires either a costly and vulnerable central index, or ooding the network with queries. In this paper we introduce the concept of Routing Indices (RIs), which allow nodes to forward queries to neighbors that are more likely to have answers. If a node cannot answer a query, it forwards the query to a subset of its neighbors, based on its local RI, rather than by selecting neighbors at random or by ooding the network by forwarding the query to all neighbors. We present three RI schemes: the compound, the hop-count, and the exponential routing indices. We evaluate their performance via simulations, and nd that RIs can improve performance by one or two orders of magnitude vs. a ooding-based system, and by up to 100 % vs. a random forwarding system. We also discuss the tradeo s between the di erent RIschemes and highlight the e ects of key design variables on system performance.
297|The Gamma database machine project|This paper describes the design of the Gamma database machine and the techniques employed in its implementation. Gamma is a relational database machine currently operating on an Intel iPSC/2 hypercube with 32 processors and 32 disk drives. Gamma employs three key technical ideas which enable the architecture to be scaled to 100s of processors. First, all relations are horizontally partitioned across multiple disk drives enabling relations to be scanned in parallel. Second, novel parallel algorithms based on hashing are used to implement the complex relational operators such as join and aggregate functions. Third, dataflow scheduling techniques are used to coordinate multioperator queries. By using these techniques it is possible to control the execution of very complex queries with minimal coordination- a necessity for configurations involving a very large number of processors. In addition to describing the design of the Gamma software, a thorough performance evaluation of the iPSC/2 hypercube version of Gamma is also presented. In addition to measuring the effect of relation size and indices on the response time for selection, join, aggregation, and update queries, we also analyze the performance of Gamma relative to the number of processors employed when the sizes of the input relations are kept constant (speedup) and when the sizes of the input relations are increased proportionally to the number of processors (scaleup). The speedup results obtained for both selection and join queries are linear; thus, doubling the number of processors
299|Optimization of nonrecursive queries|State-of-the-art optimization approaches for relational database systems, e.g., those used in systems such as OBE, SQL/DS, and commercial INGRES. when used for queries in non-traditional database applications, suffer from two problems. First, the time complexity of their optimization algorithms, being combinatoric, is exponential in the number of relations to be joined in the query. Their cost is therefore prohibitive in situa-tions such as deductive databases and logic oriented languages for knowledge bases, where hundreds of joins may be required. The second problem with the traditional approaches is that, al-beit effective in their specific domain, it is not clear whether they can be generalized to different scenarios (e.g. parallel evalua-tion) since they lack a formal model to define the assumptions and critical factors on which their valiclity depends. This paper
300|Extensible/Rule Based Query Rewrite Optimization in Starburst|This paper describes the Query Rewrite facility of the Starburst extensible database system, a novel phase of query optimization. We present a suite of rewrite rules used in Starburst to transform queries into equivalent queries for faster execution, and also describe the production rule engine which is used by Starburst to choose and execute these rules. Examples are provided demonstrating that these Query Rewrite transformations lead to query execution time improvements of orders of magnitude, suggesting that Query Rewrite in general --- and these rewrite rules in particular --- are an essential step in query optimization for modern database systems.  1 Introduction  In traditional database systems, query optimization typically consists of a single phase of processing in which access methods, join orders and join methods are chosen to provide an efficient plan for executing a user&#039;s declarative query. We refer to this phase as plan optimization. In this paper we present a distinct ph...
301|Querying in highly mobile distributed environments|New challenges to the database field brought about by the rapidly growing area of wireless personal communication are presented. Preliminary solutions to two of the major problems, control of update volume and integration of query processing and data acquisition, are discussed along with the simulation results. 1
302|Adaptive Query Processing: Technology in Evolution|As query engines are scaled and federated, they must cope with highly unpredictable and changeable environments. In the Telegraph project, we are attempting to architect and implement a continuously adaptive query engine suitable for global-area systems, massive parallelism, and sensor networks. To set the stage for our research, we present a survey of prior work on adaptive query processing, focusing on three characterizations of adaptivity: the frequency of adaptivity, the effects of adaptivity, and the extent of adaptivity. Given this survey, we sketch directions for research in the Telegraph project.  
303|Database System Issues in Nomadic Computing|Mobile computers and wireless networks are emerging technologies that will soon be available to a  wide variety of computer users. Unlike earlier generations of laptop computers, the new generation  of mobile computers can be an integrated part of a distributed computing environment, one in  which users change physical location frequently. The result is a new computing paradigm, nomadic  computing. This paradigm will affect the design of much of our current systems software, including  that of database systems.  This paper discusses in some detail the impact of nomadic computing on a number of traditional  database system concepts. In particular, we point out how the reliance on short-lived batteries  changes the cost assumptions underlying query processing. In these systems, power consumption  competes with resource utilization in the definition of cost metrics. We also discuss how the likelihood  of temporary disconnection forces consideration of alternative transaction processing pr...
304|Optimization techniques for queries with expensive methods|Object-Relational database management systems allow knowledgeable users to de ne new data types, as well as new methods (operators) for the types. This exibility produces an attendant complexity, which must be handled in new ways for an Object-Relational database management system to be e cient. In this paper we study techniques for optimizing queries that contain time-consuming methods. The focus of traditional query optimizers has been on the choice of join methods and orders; selections have been handled by \pushdown &#034; rules. These rules apply selections in an arbitrary order before as many joins as possible, using the assumption that selection takes no time. However, users of Object-Relational systems can embed complex methods in selections. Thus selections may take signi cant amounts of time, and the query optimization model must be enhanced. In this paper, we carefully de ne a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial Object-Relational database management system Illustra, and discuss practical issues that a ect our earlier assumptions. We compare Predicate Migration to a variety of simpler optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we presentmaybe useful for constrained workloads.
305|Cost-based Query Scrambling for Initial Delays|Remote data access from disparate sources across a wide-area network such as the Internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites. Traditional, static, query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays. Query scrambling has been proposed to address this problem. Scrambling modifies query execution plans on-the-fly when delays are encountered during runtime. In its original formulation, scrambling was based on simple heuristics, which although providing good performance in many cases, were also shown to be susceptible to problems resulting from bad scrambling decisions. In this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices. We propose three different approaches to using query optimization for scramblin...
306|Adaptive Parallel Aggregation Algorithms|Aggregation and duplicate removal are common in SQL queries. However, in the parallel query processing literature, aggregate processing has received surprisingly little attention; furthermore, for each of the traditional parallel aggregation algorithms, there is a range of grouping selectivities where the algorithm performs poorly. In this work, we propose new algorithms that dynamically adapt, at query evaluation time, in response to observed grouping selectivities. Performance analysis via analytical modeling and an implementation on a workstation-cluster shows that the proposed algorithms are able to perform well for all grouping selectivities. Finally, we study the effect of data skew and show that for certain data sets the proposed algorithms can even outperform the best of traditional approaches. 1 Introduction  SQL queries are replete with aggregate and duplicate elimination operations. One measure of the perceived importance of aggregation is that in the proposed TPCD benchmark...
308|Bluetooth and Sensor Networks: A Reality Check|The current generation of sensor nodes rely on commodity components. The choice of the radio is particularly important as it impacts not only energy consumption but also software design (e.g., network self-assembly, multihop routing and in-network processing). Bluetooth is one of the most popular commodity radios for wireless devices. As a representative of the frequency hopping spread spectrum radios, it is a natural alternative to broadcast radios in the context of sensor networks. The question is whether Bluetooth can be a viable alternative in practice. In this paper, we report our experience using Bluetooth for the sensor network regime. We describe our tiny Bluetooth stack that allows TinyOS applications to run on Bluetooth-based sensor nodes, we present a multihop network assembly procedure that leverages Bluetooth&#039;s device discovery protocol, and we discuss how Bluetooth favorably impacts in-network query processing. Our results show that despite obvious limitations the Bluetooth sensor nodes we studied exhibit interesting properties, such as a good energy per bit sent ratio. This reality check underlies the limitations and some promises of Bluetooth for the sensor network regime.
309|Aggregation and Relevance in Deductive Databases|In this paper we present a technique to optimize queries on deductive databases that use aggregate operations such as min, max, and “largest Ic values. ” Our approach is based on an extended notion of relevance of facts to queries that takes aggregate operations into account. The approach has two parts: a rewriting part that labels predica.tes with “ag-gregate selections, ” and an evaluat,ion part. t.hat, makes use of “aggregate selections ” to detect that facts are irrelevant and discards them. The rewriting complements standard rewrit-ing algorithms like Magic sets, and the evaluation essentially refines Semi-Naive evaluation. 1
310|Query Optimization In Compressed Database Systems|Over the lastd ecad es, improvements in CPU speed have outpaced improvements in main memory and d isk access rates by ord ers of magnitud , enabling the use ofd ata compression techniques to improve the performance ofd atabase systems. Previous work d scribes the benefits of compression for numerical attributes, whered8 a is stored in compressed  format ond isk. Despite the abund3&amp; e of stringvalued  attributes in relational schemas there is little work on compression for string attributes in ad atabase context. Moreover, none of the previous work suitablyad2 esses the role of the query optimizer: During query execution, dD a is either eagerly d compressed when it is read into main memory, or dD a lazily stays compressed in main memory and is d compressed ond emand only. In this paper, we present an e#ective approach for dD abase compression based on lightweight, attribute-level compression techniques. We propose a Hierarchical ictionary Encod  ing strategy that intelligently selects the most e#ective compression method for string-valued attributes. We show that eager and lazy d compression strategies prod1 e suboptimal plans for queries involving compressed string attributes. We then formalize the problem of compressionaware query optimizationand propose one provably optimal  and two fast heuristic algorithms for selecting a query plan for relational schemas with compressed attributes; our algorithms can easily be integrated into existing cost-based query optimizers. Experiments using TPC-Hd atad emonstrate the impact of our string compression method s and show the importance of compression-aware query optimization. Our approach results in up to an or d r speed up over existing approaches.  1. 
311|The Design and Implementation of the Ariel Active Database Rule System|This paper describes the design and implementation of the Ariel DBMS and it&#039;s tightlycoupled forward-chaining rule system. The query language of Ariel is a subset of POSTQUEL, extended with a new production-rule sublanguage. Ariel supports traditional relational database query and update operations efficiently, using a System Rlike query processing strategy. In addition, the Ariel rule system is tightly coupled with query and update processing. Ariel rules can have conditions based on a mix of patterns, events, and transitions. For testing rule conditions, Ariel makes use of a discrimination network composed of a special data structure for testing single-relation selection conditions efficiently, and a modified version of the TREAT algorithm, called A-TREAT, for testing join conditions. The key modification to TREAT (which could also be used in the Rete algorithm) is the use of virtual ff-memory nodes which save storage since they contain only the predicate associated with the memory n...
312|DOMINO: Databases fOr MovINg Objects tracking|Consider a database that represents information about moving objects and their location. For example, for a database representing the location of taxi-cabs a typical query may be: retrieve the free cabs that are currently within 1 mile of 33 N. Michigan Ave., Chicago (to pick-up a customer); or for a trucking company database a typical query may be: retrieve the trucks that are currently within 1 mile of truck ABT312 (which needs assistance); or for a database representing the current location of objects in a battlefield a typical query may be: retrieve the friendly helicopters that are in a given region, or, retrieve the friendly helicopters that are expected to enter the region within the next 10 minutes. The queries may originate from the moving objects, or from stationary users. We will refer to applications with the above characteristics as moving-objects-database (MOD) applications, and to queries as the ones mentioned above as MOD queries.
313|From ethnography to design in a vineyard, in|This paper summarizes the process from ethnographic study of a vineyard to concept development and interaction design for a ubiquitous computing solution. It provides examples of vineyard interfaces and the lessons learned that could be generally applied to the interaction design of ubiquitous systems. These are: design for multiple perspectives on data, design for multiple access points, and design for varying levels of attention.
314|Query Optimization in Mobile Environments|We consider the issue of optimizing queries for a distributed processing in mobile environment. An interesting characteristic of mobile machines is that they depend on battery as a source of energy which may not be substantial enough. Hence, the appropriate optimization criterion in a mobile environment considers both resource utilization and energy consumption at the mobile client. In this scenario, the optimal plan for a query depends on the residual battery level of the mobile client and the load at the server. We approach this problem by compiling a query into a sequence of candidate plans, such that for any state of the client-server system, the optimal plan is one of the candidate plans. A general solution is proposed by adapting the partial order dynamic programming search algorithm [17, 18] (p.o dp) such that the coverset of the query is the set of candidate plans. We propose two novel algorithms, namely, the linear combinations algorithm and the linearset algorithm (referred t...
315|Online Dynamic Reordering|We present a mechanism for providing dynamic user control during long running, data-intensive operations. Users  can see partial results and dynamically direct the processing to suit their interests, thereby enhancing the interactivity  in several contexts such as online aggregation and large-scale spreadsheets.  In this paper, we develop a pipelining, dynamically tunable reorder operator for providing user control. Users  specify preferences for different data items based on prior results, so that data of interest is prioritized for early  processing. The reordering mechanism is efficient and non-blocking, and can be used over arbitrary data streams  from files and indexes, as well as continuous data feeds. We also investigate several policies for the reordering  based on the performance goals of various typical applications. We present performance results for reordering in the  context of an Online Aggregation implementation in Informix, and in the context of sorting and scrolling in...
316|Overview of the scalable video coding extension of the H.264/AVC standard|With the introduction of the H.264/AVC video coding standard, significant improvements have recently been demonstrated in video compression capability. The Joint Video Team of the ITU-T VCEG and the ISO/IEC MPEG has now also standardized a Scalable Video Coding (SVC) extension of the H.264/AVC standard. SVC enables the transmission and decoding of partial bit streams to provide video services with lower temporal or spatial resolutions or reduced fidelity while retaining a reconstruction quality that is high relative to the rate of the partial bit streams. Hence, SVC provides functionalities such as graceful degradation in lossy transmission environments as well as bit rate, format, and power adaptation. These functionalities provide enhancements to transmission and storage applications. SVC has achieved significant improvements in coding efficiency with an increased degree of supported scalability relative to the scalable profiles of prior video coding standards. This paper provides an overview of the basic concepts for extending H.264/AVC towards SVC. Moreover, the basic tools for providing temporal, spatial, and quality scalability are described in detail and experimentally analyzed regarding their efficiency and complexity.
317|Factor Graphs and the Sum-Product Algorithm|A factor graph is a bipartite graph that expresses how a &#034;global&#034; function of many variables factors into a product of &#034;local&#034; functions. Factor graphs subsume many other graphical models including Bayesian networks, Markov random fields, and Tanner graphs. Following one simple computational rule, the sum-product algorithm operates in factor graphs to compute---either exactly or approximately---various marginal functions by distributed message-passing in the graph. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative &#034;turbo&#034; decoding algorithm, Pearl&#039;s belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform algorithms. 
318|A taxonomy and evaluation of dense two-frame stereo correspondence algorithms|Abstract. Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods. Our taxonomy is designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms. We have also produced several new multi-frame stereo data sets with ground truth and are making both the code and data sets available on the Web. Finally, we include a comparative evaluation of a large set of today’s best-performing stereo algorithms.
319|Overview of the H.264/AVC Video Coding Standard| H.264/AVC is newest video coding standard of the
320|Graphical models, exponential families, and variational inference|The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide varietyof algorithms — among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations — can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.
321|Learning low-level vision|We show a learning-based method for low-level vision problems. We set-up a Markov network of patches of the image and the underlying scene. A factorization approximation allows us to easily learn the parameters of the Markov network from synthetic examples of image/scene pairs, and to e ciently propagate image information. Monte Carlo simulations justify this approximation. We apply this to the \super-resolution &amp;quot; problem (estimating high frequency details from a low-resolution image), showing good results. For the motion estimation problem, we show resolution of the aperture problem and lling-in arising from application of the same probabilistic machinery.
322|Stereo matching using belief propagation| In this paper, we formulate the stereo matching problem as a Markov network and solve it using Bayesian belief propagation. The stereo Markov network consists of three coupled Markov random fields that model the following: a smooth field for depth/disparity, a line process for depth discontinuity, and a binary process for occlusion. After eliminating the line process and the binary process by introducing two robust functions, we apply the belief propagation algorithm to obtain the maximum a posteriori (MAP) estimation in the Markov network. Other low-level visual cues (e.g., image segmentation) can also be easily incorporated in our stereo model to obtain better stereo results. Experiments demonstrate that our methods are comparable to the state-of-the-art stereo algorithms for many test cases.
323|Nonparametric Belief Propagation|In applications  of  graphical models arising in fields such as computer  vision, the hidden variables  of  interest are most naturally  specified by continuous, non--Gaussian distributions. However, due  to the limitations  of  existing  inf#6F6F3  algorithms, it is of#]k necessary  tof#3# coarse, discrete approximations to such models. In this  paper, we develop a nonparametric belief propagation (NBP) algorithm,  which uses stochastic methods to propagate kernel--based  approximations to the true continuous messages. Each NBP message  update is based on an efficient sampling procedure which  can accomodate an extremely broad class  of potentialf#l3]k[[z3  allowing easy adaptation to new application areas. We validate  our method using comparisons to continuous BP for Gaussian networks,  and an application to the stereo vision problem.
324|Long-Term Memory Motion-Compensated Prediction|Long-term memory motion-compensated prediction extends the spatial displacement vector utilized in block-based hybrid video coding by a variable time delay permitting the use of more frames than the previously decoded one for motioncompensated prediction. The long-term memory covers several seconds of decoded frames at the encoder and decoder. The use of multiple frames for motion compensation in most cases provides significantly improved prediction gain. The variable time delay has to be transmitted as side information requiring an additional bit rate which may be prohibitive when the size of the long-term memory becomes too large. Therefore, we control the bit rate of the motion information by employing rateconstrained motion estimation. Simulation results are obtained by integrating long-term memory prediction into an H.263 codec. Reconstruction PSNR improvements up to 2 dB for the Foreman sequence and 1.5 dB for the Mother--Daughter sequence are demonstrated in comparison to the TMN-2.0 H.263 coder. The PSNR improvements correspond to bit-rate savings up to 34 and 30%, respectively. Mathematical inequalities are used to speed up motion estimation while achieving full prediction gain.
325|Three-Dimensional Lifting Schemes For Motion Compensated Video|Three dimensional wavelet decompositions are efficient tools  for scalable video coding. In this paper, we show the interest of a lifting formulation of these decompositions. The temporal wavelet transform is inherently non-linear, due to the motion estimation step, and the lifting formalism allows us to provide several improvements to the scheme initially proposed by Choi and Woods: a better processing of the uncovered areas is proposed and an overlapped motion compensated temporal filtering method is introduced in the multiresolution decomposition. As shown by simulations, the proposed method results in higher coding efficiency, while keeping the scalability functionalities.
326|Performance Analysis of SVC|Abstract—This paper provides a performance analysis of the Scalable Video Coding (SVC) extension of H.264/AVC. A short overview presenting the main functionalities of SVC is given and main issues in encoder control and bit stream extraction are out-lined. Some aspects of rate-distortion optimization in the context of SVC are discussed and strategies for derivation of optimized configurations relative to the investigated scalability scenarios are presented. Based on these methods, rate-distortion results for several SVC configurations are presented and compared to rate-distortion optimized H.264/AVC single layer coding. For reference, a comparison to rate-distortion optimized MPEG-4 Visual (Advanced Simple Profile) coding results is provided. The results show that the performance gap between single layer coding and scalable video coding can be very small and that SVC clearly outperforms previous video coding technology such as MPEG-4 ASP. Index Terms—H.264/AVC, rate-distortion optimization, perfor-mance, scalability, video coding. I.
327|A comparison of algorithms for inference and learning in probabilistic graphical models|Computer vision is currently one of the most exciting areas of artificial intelligence re-search, largely because it has recently become possible to record, store and process large amounts of visual data. While impressive achievements have been made in pattern clas-sification problems such as handwritten character recognition and face detection, it is even more exciting that researchers may be on the verge of introducing computer vision systems that perform scene analysis, decomposing image input into its constituent objects, lighting conditions, motion patterns, and so on. Two of the main challenges in computer vision are finding efficient models of the physics of visual scenes and finding efficient algorithms for inference and learning in these models. In this paper, we advocate the use of graph-based probability models and their associated inference and learning algorithms for computer vision and scene analysis. We review exact techniques and various approximate, computationally efficient techniques, including iterative conditional modes, the expectation maximization (EM) algorithm, the mean field method, variational techniques, structured variational techniques, Gibbs sampling, the sum-product algorithm and “loopy ” belief propagation. We describe how each technique can be applied in a model of multiple, occluding objects, and contrast the behaviors and performances of the techniques using a unifying cost function, free energy.
328|Motion Compensated Lifting Wavelet And Its Application In Video Coding|A motion compensated lifting (MCLIFT) framework is proposed for the 3D wavelet video coder. By using bi-directional motion compensation in each lifting step of the temporal direction, the video frames are effectively de-correlated. With proper entropy coding and bitstream packaging schemes, the MCLIFT wavelet video coder can be scalable in frame rate and quality level. Experimental results show that the MCLIFT video coder outperforms the 3D wavelet video coder with the same entropy coding scheme by an average of 1.1-1.6dB, and outperforms MPEG-4 coder by an average of 0.9-1.4dB.
329|A Locally Optimal Design Algorithm for Block-Based Multi-Hypothesis Motion-Compensated Prediction|Multi-hypothesis motion-compensated prediction extends traditional motion  -compensated prediction used in video coding schemes. Known algorithms  for block-based multi-hypothesis motion-compensated prediction are, for example,  overlapped block motion compensation (OBMC) and bidirectionally predicted  frames (B-frames). This paper presents a generalization of these algorithms  in a rate-distortion framework. All blocks which are available for prediction  are called hypotheses. Further, we explicitly distinguish between the search  space and the superposition of hypotheses. Hypotheses are selected from a search  space and their spatio-temporal positions are transmitted by means of spatiotemporal  displacement codewords. Constant predictor coe#cients are used to  combine linearly hypotheses of a multi-hypothesis. The presented design algorithm  provides an estimation criterion for optimal multi-hypotheses, a rule for  optimal displacement codes, and a condition for optimal predictor coe#cients.
330|T.Wiegand,“ Constrained interlayer prediction for single-loop decoding in spatial scalability |Abstract—The scalability extension of H.264/AVC uses an oversampled pyramid representation for spatial scalability, where for each spatial resolution a separate motion compensation or MCTF loop is deployed. When the reconstructed signal at a lower resolution is used to predict the next higher resolution, the motion compensation or MCTF loops including the deblocking filter operations of both resolutions have to be executed. This imposes a large complexity burden on the decoding of the higher resolution signals, especially when multiple spatial layers are utilized. In this paper, we investigate the approach to only allow prediction between spatial layers for parts of the lower resolution pictures that are intra-coded in order to avoid decoding that requires multiple motion compensation or MCTF loops. Experimental results evaluate the effectiveness of the proposed approach. Keywords–Scalability; H.264/AVC; inter-layer prediction; single-loop decoding I.
331|The interdisciplinary study of coordination|This survey characterizes an emerging research area, sometimes called coordination theory, that focuses on the interdisciplinary study of coordination. Research in this area uses and extends ideas about coordination from disciplines such as computer science, organization theory, operations research, economics, linguistics, and psychology. A key insight of the framework presented here is that coordination can be seen as the process of managing dependencies among activities. Further progress, therefore, should be possible by characterizing different kinds of dependencies and identifying the coordination processes that can be used to manage them. A variety of processes are analyzed from this perspective, and commonalities across disciplines are identified. Processes analyzed include those for managing shared resources, producer/consumer relationships, simultaneity constraints, and tank/subtask dependencies. Section 3 summarizes ways of applying a coordination perspective in three different domains: (1) understanding the effects of information technology on human organizations and markets, (2) designing cooperative work tools, and (3) designing distributed and parallel computer systems. In the final section, elements of a research
332|Electronic Markets and Electronic Hierarchies|This paper analyzes the fundamental changes in market structures that may result from the increasing use of information technology. First, an analytic framework is presented and its usefulness is demonstrated in explaining several major historical changes in American business structures. Then, the framework is used to help explain how electronic markets and electronic hierarchies will allow closer integration of adjacent steps in the value added chains of our economy. The most surprising prediction is that information technology will lead to an overall shift toward proportionately more coordination by markets rather than by internal decisions within firms. Finally, several examples of companies where these changes are already occurring are used to illustrate the likely paths by which new market structures will evolve and the ways in which individual companies can take advantage of these changes.
333|Frameworks for Cooperation in Distributed Problem Solving|Abstract — Two forms of cooperation in distributed problem solving are considered: task-sharing and result-sharing. In the former, nodes assist each other by sharing the computational load for the execution of subtasks of the overall problem. In the latter, nodes assist each other by sharing partial results which are based on somewhat different perspectives on the overall problem. Different perspectives arise because the nodes use different knowledge sources (KS’s) (e.g., syntax versus acoustics in the case of a speech-understanding system) or different data (e.g., data that is sensed at different locations in the case of a distributed sensing system). Particular attention is given to control and to internode communication for the two forms of cooperation. For each, the basic methodology is presented and systems in which it has been used are described. The two forms are then compared and the types of applications for which they are suitable are considered. I. DISTRIBUTED PROBLEM SOLVING
334|Offices Are Open Systems|This paper is intended as a contribution to analysis of the implications of viewing offices as open systems. It takes a prescriptive stance on how to estalish the informationprocessing foundations for taking action and making decisions in office work from an open systems perspective. We propose due process as a central activity in organizational information processing. Computer systems are beginning to play important roles in mediating the ongoing activities of organizations. We expect that these roles will gradually increase in importance as computer systems take on more of the authority and responsibility for ongoing activities. At the same time, we expect computer systems to acquire more of the characteristics and structure of human organizations.
335|Experiments with Oval: A Radically Tailorable Tool for Cooperative Work|This article describes a series of tests of the generality of a “radically tailorable” tool for cooperative work. Users of this system can create applications by combining and modifying four kinds of building blocks: objects, uiezus, agents, and links. We found that user-level tailoring of these primitives can provide most of the functionality found in well-known cooperative work systems such as gIBIS, Coordinator, Lotus Notes, and Information Lens. These primitives, therefore, appear to provide an elementary “tailoring language” out of which a wide variety of integrated information management and collaboration applications can be constructed by end users.
336|A microeconomic approach to optimal resource allocation in distributed computer systems|Abstract-Decentralized algorithms are examined for opti-mally distributing a divisible resource in a distributed computer system. In order to study this problem in a specific context, we consider the problem of optimal file allocation. In this case, the optimization criteria include both the communication cost and average processing delay associated with a file access. Our algorithms have their origins in the field of mathematical economics. They are shown to have several attractive properties, including their simplicity and distributed nature, the computation of feasible and increasingly better resource allocations as the result of each iteration, and in the case of file allocation, rapid convergence. Conditions are formally derived under which the algorithms are guaranteed to converge and their convergence behavior is additionally examined through simulation. Index Terms-Distributed algorithms, distributed systems, file allocation, resource allocation, optimization I.
337|Social Analyses of Computing: Theoretical Perspectives in Recent|Recent empirical studies of computing use in organizations and in public life are examined. The roles of computer technologies in the workplace, in decision making, in altering power relationships, and in influencing personal privacy are examined. In addition, studies that examine the social accountability of computing arrangements to broad publics are reviewed. All studies of computing in social ife make important assumptions about the social world in which computing is embedded. Two broad perspectives are contrasted. Systems rationalism, a collection of approaches including management science, managerial rationalism, and the systems approach, is found to be most helpful in stable settings, when there is considerable consensus over important social values. Segmented-institutionalist nalyses, which assume social conflict rather than consensus, are particularly powerful as the social world of computing use becomes more dynamic and as a wider variety of groups is involved.
338|Mathematische Modelle|The quest for runware: on compositional, executable and intuitive
339|Organizations|Proceedings DEFORM’06 is associated to BMVC’06, the 7th British Machine Vision Conference Preface These are the proceedings of DEFORM’06, the Workshop on Image Registration in Deformable Environments, associated to BMVC’06, the 17th British Machine Vision Conference, held in Edinburgh, UK, in
340|Inter-Organization Computer Networks: Indications of Shifts in Interdependence|As firms increasingly adopt inter-organization computer networks (IONS) to improve coordination, researchers must be concerned about the long term impact of IONS on organizational relationships. This paper reports on an exploratory study of the use of IONS in design and manufacturing activities in the semiconductor industry. We identify the potential interactions between firms that can be facilitated by IONS, and focus on the implications for customer and producer interdependence. Our analysis suggests that the long term impacts of IONS are not technologically determined, and that their use ought to be regarded differently than those of other media. 
341|A Hierarchical Internet Object Cache| This paper discusses the design andperformance  of a hierarchical proxy-cache designed to make Internet information systems scale better. The design was motivated by our earlier trace-driven simulation study of Internet traffic. We believe that the conventional wisdom, that the benefits of hierarchical file caching do not merit the costs, warrants reconsideration in the Internet environment.  The cache implementation supports a highly concurrent stream of requests. We present performance measurements that show that the cache outperforms other popular Internet cache implementations by an order of magnitude under concurrent load. These measurements indicate that hierarchy does not measurably increase access latency. Our software can also be configured as a Web-server accelerator; we present data that our httpd-accelerator is ten times faster than Netscape&#039;s Netsite and NCSA 1.4 servers.  Finally, we relate our experience fitting the cache into the increasingly complex and operational world of Internet information systems, including issues related to security, transparency to cache-unaware clients, and the role of file systems in support of ubiquitous wide-area information systems.  
342|Scale and performance in a distributed file system|The Andrew File System is a location-transparent distributed tile system that will eventually span more than 5000 workstations at Carnegie Mellon University. Large scale affects performance and complicates system operation. In this paper we present observations of a prototype implementation, motivate changes in the areas of cache validation, server process structure, name translation, and low-level storage representation, and quantitatively demonstrate Andrew’s ability to scale gracefully. We establish the importance of whole-file transfer and caching in Andrew by comparing its performance with that of Sun Microsystem’s NFS tile system. We also show how the aggregation of files into volumes improves the operability of the system.
343|The Case for Geographical Push-Caching|Most existing wide-area caching schemes are client initiated. Decisions on when and where to cache information are made without the benefit of the server&#039;s global knowledge of the situation. We believe that the server should play a role in making these caching decisions, and we propose  geographical push-caching as a way of bringing the server back into the loop. The World Wide Web is an excellent example of a wide-area system that will benefit from geographical push-caching, and we present an architecture that allows a Web server to autonomously replicate HTML pages. 1 Introduction  The World-Wide Web [1] operates for the most part as a cache-less distributed system. When two neighboring clients retrieve a document from the same server, the document is sent twice. This is inefficient, especially considering the ease with which Web browsers allow users to transfer large multimedia documents. To combat this problem, some Web browsers have begun to add local client caches. These prevent ...
344|The Harvest Information Discovery and Access System|It is increasingly difficult to make effective use of Internet information, given the rapid growth in data volume, user base, and data diversity. In this paper we introduce Harvest, a system that provides a scalable, customizable architecture for gathering, indexing, caching, replicating, and accessing Internet information.  
345|Alex -- a global filesystem|The Alex filesystem provides users and applications transparent read access to files in Internet anonymous FTP sites. Today there are thousands of anonymous FTP sites with a total of a few million files and roughly a terabyte of data. The standard approach to accessing these files involves logging in to the remote machine. This means that an application can not access remote files and that users do not have any of their aliases or local tools available when connected to a remote site. Users who want to use an application on a remote file must first manually make a local copy of the file. Not only is this inconvenient, it creates two more problems. First, there is no mechanism for automatically updating this local copy when the remote file changes. The users must keep track of where they get their files from and check to see if there are updates, and then fetch these. Second, many different users at the same site may have made copies of the same remote file, thus wasting disk space. Alex addresses the problems with the above approach while maintaining compatibility with the existing FTP protocol so that the large collection of currently available files can be accessed. To get reasonable performance, long term file caching must be used. Thus consistency must be addressed. Traditional solutions to the cache consistency problem do not work in the Internet FTP domain: callbacks are not an
346|Multi-level Caching in Distributed File Systems or Your cache ain’t nuthin’ but trash|We are investigating the potential for intermediate file servers to address scaling problems in increasingly large distributed file systems. To this end, we have run trace-driven simulations based on data from DEC-SRC and our own data collection to determine the potential of caching-only intermediate servers. The degree of sharing among clients is central to the effectiveness of an intermediate server. This turns out to be quite low in the traces available to us. All told, fewer than 10 % of block accesses are to files shared by more than one file system client. Trace-driven simulation shows that even with an infinite cache at the intermediate, cache hit rates are disappointingly low. For client caches as small as 20 MB, we observe hit rates less than 19%. As client cache sizes increase, the hit rate at the intermediate approaches the degree of sharing among all clients. On the other hand, the intermediate does appear to be effective in reducing the peak load presented to upstream file servers.
347|Demand-based Document Dissemination for the World-Wide Web|We analyzed the logs of our departmental HTTP server
348|Equation-based congestion control for unicast applications|This paper proposes a mechanism for equation-based congestion control for unicast traffic. Most best-effort traffic in the current Internet is well-served by the dominant transport protocol, TCP. However, traffic such as best-effort unicast streaming multimedia could find use for a TCP-friendly congestion control mechanism that refrains from reducing the sending rate in half in response to a single packet drop. With our mechanism, the sender explicitly adjusts its sending rate as a function of the measured rate of loss events, where a loss event consists of one or more packets dropped within a single round-trip time. We use both simulations and experiments over the Internet to explore performance. We consider equation-based congestion control a promising avenue of development for congestion control of multicast traffic, and so an additional motivation for this work is to lay a sound basis for the further development of multicast congestion control.
349|Modeling TCP Throughput: A Simple Model and its Empirical Validation|In this paper we develop a simple analytic characterization of the steady state throughput, as a function of loss rate and round trip time for a bulk transfer TCP flow, i.e., a flow with an unlimited amount of data to send. Unlike the models in [6, 7, 10], our model captures not only the behavior of TCP’s fast retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP’s timeout mechanism on throughput. Our measurements suggest that this latter behavior is important from a modeling perspective, as almost all of our TCP traces contained more timeout events than fast retransmit events. Our measurements demonstrate that our model is able to more accurately predict TCP throughput and is accurate over a wider range of loss rates. This material is based upon work supported by the National Science Foundation under grants NCR-95-08274, NCR-95-23807 and CDA-95-02639. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
350|Promoting the Use of End-to-End Congestion Control in the Internet| This paper considers the potentially negative impacts of an increasing deployment of non-congestion-controlled best-effort traffic on the Internet.’ These negative impacts range from extreme unfairness against competing TCP traffic to the potential for congestion collapse. To promote the inclusion of end-to-end congestion control in the design of future protocols using best-effort traffic, we argue that router mechanisms are needed to identify and restrict the bandwidth of selected high-bandwidth best-effort flows in times of congestion. The paper discusses several general approaches for identifying those flows suitable for bandwidth regulation. These approaches are to identify a high-bandwidth flow in times of congestion as unresponsive, “not TCP-friendly,” or simply using disproportionate bandwidth. A flow that is not “TCP-friendly ” is one whose long-term arrival rate exceeds that of any conformant TCP in the same circumstances. An unresponsive flow is one failing to reduce its offered load at a router in response to an increased packet drop rate, and a disproportionate-bandwidth flow is one that uses considerably more bandwidth than other flows in a time of congestion.
351|A Proposal to add Explicit Congestion Notification (ECN) to IP  (1999) |This note describes a proposed addition of ECN (Explicit Congestion Notification) to IP. TCP is currently the dominant transport protocol used in the Internet. We begin by describing TCP&#039;s use of packet drops as an indication of congestion. Next we argue that with the addition of active queue management (e.g., RED) to the Internet infrastructure, where routers detect congestion before the queue overflows, routers are no longer limited to packet drops as an indication of congestion. Routers could instead set a Congestion Experienced (CE) bit in the packet header of packets from ECN-capable transport protocols. We describe when the CE bit would be set in the routers, and describe what modifications would be needed to TCP to make it ECN-capable. Modifications to other transport protocols (e.g., unreliable unicast or multicast, reliable multicast, other reliable unicast transport protocols) could be considered as those protocols are developed and advance through the standards process. 1. C...
352|RAP: An end-to-end rate-based congestion control mechanism for realtime streams in the internet|Abstract-End-to-end congestion control mechanisms have been critical to the robustness and stability of the Internet. Most of today’s Internet trafftc is TCP, and we expect this to remain so in the future. Thus, having “TCP-friendly ” behavior is crucial for new applications. However, the emergence of non-congestion-controlled realtime applications threatens unfairness to competing TCP traffic and possible congestion collapse. We present an end-to-end TCP-friendly Rate Adaptation Protocol (RAP), which employs an additive-increase, multiplicativedecrease (AIMD) algorithm. It is well suited for unicast playback of realtime streams and other semi-reliable rate-based applications. Its primary goal is to be fair and TCP-friendly while separating network congestion control from application-level reliability. We evaluate RAP through extensive simulation, and conclude that bandwidth is usually evenly shared between TCP and RAP traffic. Unfairness to TCP traffic is directly determined by how TCP diverges from the AIMD algorithm. Basic RAP behaves in a TCPfriendly fashion in a wide range of likely conditions, but we also devised a fine-grain rate adaptation mechanism to extend this range further. Finally, we show that deploying RED queue management can result in an ideal fairness between TCP and RAP traffic. I.
353|Tcp-like congestion control for layered multicast data transfer|Abstract—We present a novel congestion control algorithm suitable for use with cumulative, layered data streams in the MBone. Our algorithm behaves similarly to TCP congestion control algorithms, and shares bandwidth fairly with other instances of the protocol and with TCP flows. It is entirely receiver driven and requires no per-receiver status at the sender, in order to scale to large numbers of receivers. It relies on standard functionalities of multicast routers, and is suitable for continuous stream and reliable bulk data transfer. In the paper we illustrate the algorithm, characterize its response to losses both analytically and by simulations, and analyse its behaviour using simulations and experiments in real networks. We also show how error recovery can be dealt with independently from congestion control by using FEC techniques, so as to provide reliable bulk data transfer.
354|Observations on the dynamics of a congestion control algorithm: The effects of two-way traffic|We use simulation to study the dynamics of the congestion cent rol algorithm embedded in the BSD 4.3-Tahoe TCP implementation. We investigate the simple case of a few TCP connections, originating and terminating at the same pair of hosts, using a single bottleneck link. This work is an extension of our earlier work ([16]), where one-way traffic (i.e., all of the sources are on the same host and all of the destinations are on the other host) was studied. In this paper we investigate the dynamics that results from two-way traffic (in which there are data sources on both hosts). We find that the one-way traffic clustering and loss-synchronization phenomena d~cussed in [16] persist in this new situation, albeit in a slightly modified form. In addition, there are two new phenomena not present in the earlier study: (1) ACK-compression, which is due to the interaction of data and ACK packets and gives rise to rapid fluctuations in queue length, and (2) an out-of-phase queue-synchronization mode, which keeps link utilization less than optimal even in the limit of very large buffers. These phenomena are helpful in understanding results from an earlier study of network oscillations ([19]). 1
355|Connections with Multiple Congested Gateways in Packet-Switched Networks Part 1: One-way Traffic|In this paper we explore the bias in TCP/IP networks against connections with multiple congested gateways. We consider the interaction between the bias against connections with multiple congested gateways, the bias of the TCP window modification algorithm against connections with longer roundtrip times, and the bias of Drop Tail and Random Drop gateways against bursty traffic. Using simulations and a heuristic analysis, we show that in a network with the window modification algorithm in 4.3 tahoe BSD TCP and with Random Drop or Drop Tail gateways, a longer connection with multiple congested gateways can receive unacceptably low throughput. We show that in a network with no bias against connections with longer roundtrip times and with no bias against bursty traffic, a connection with multiple congested gateways can receive an acceptable level of throughput. We discuss the application of several current measures of fairness to networks with multiple congested gateways, and show that diff...
356|On the Relationship Between File Sizes, Transport Protocols, and Self-Similar Network Traffic|Recent measurements of local-area and wide-area traffic have shown that network traffic exhibits variability at a wide range of scales. In this paper, we examine a mechanism that gives rise to self-similar network traffic and present some of its performance implications. The mechanism we study is the transfer of files or messages whose size is drawn from a heavy-tailed distribution. First, we show that in a “realistic ” client/server network environment—i.e., one with bounded resources and coupling among traffic sources competing for resources—the degree to which file sizes are heavy-tailed can directly determine the degree of traffic self-similarity at the link level. We show that this causal relationship is robust with respect to changes in network resources (bottleneck bandwidth and
357|Automated Packet Trace Analysis of TCP Implementations|We describe tcpanaly, a tool for automatically analyzing a TCP implementation&#039;s behavior by inspecting packet traces of the TCP&#039;s activity. Doing so requires surmounting a number of hurdles, including detecting packet filter measurement errors, coping with ambiguities due to the distance between the measurement point and the TCP, and accommodating a surprisingly large range of behavior among different TCP implementations. We discuss why our efforts to develop a fully general tool failed, and detail a number of significant differences among 8 major TCP implementations, some of which, if ubiquitous, would devastate Internet performance. The most problematic TCPs were all independently written, suggesting that correct TCP implementation is fraught with difficulty. Consequently, it behooves the Internet community to develop testing programs and reference implementations. 1 Introduction  There can be a world of difference between the behavior we expect of a transport protocol, and what we g...
358|Congestion Avoidance in Computer Networks With a Connectionless Network Layer: Concepts, Goals and Methodology|Congestion occurs in a computer network when the resource demands exceed the capacity. Packets may be lost due to too much queuing in the network. During congestion, the network throughput may drop and the path delay may become very high. A congestion control scheme helps the network to recover from the congestion state. A congestion avoidance scheme allows a network to operate in the region of low delay and high throughput. Such schemes prevent a network from entering the congested state. Congestion avoidance is a prevention mechanism while congestion control is a recovery mechanism.
359|Real-Time Internet Video Using Error Resilient Scalable Compression and TCP-Friendly Transport Protocol|We introduce a point to point real-time video transmission scheme over the Internet combining a low-delay TCP-friendly transport protocol in conjunction with a novel compression method that is error resilient and bandwidth-scalable. Compressed video is packetized into individually decodable packets of equal expected visual importance. Consequently, relatively constant video quality can be achieved at the receiver under lossy conditions. Furthermore, the packets can be truncated to instantaneously meet the time varying bandwidth imposed by a TCP-friendly transport protocol. As a result, adaptive flows that are friendly to other Internet traffic are produced. Actual Internet experiments together with simulations are used to evaluate the performance of the compression, transport, and the combined schemes.
360|A Model Based TCP-Friendly Rate Control Protocol |As networked multimedia applications become widespread, it becomes increasingly important to ensure that these applications can coexist with current TCP-based applications. The TCP protocol is designed to reduce its sending rate when congestion is detected. Networked multimedia applications should exhibit similar behavior, if they wish to co-exist with TCP-based applications [9]. Using TCP for multimedia applications is not practical, since the protocol combines error control and congestion control, an appropriate combination for non-real time reliable data transfer, but inappropriate for loss-tolerant real time applications. In this paper we present a protocol that operates by measuring loss rates and round trip times and then uses them to set the transmission rate to that which TCP would achieve under similar conditions. The analysis in [13] is used to determine this &#034;TCP-friendly&#034; rate. This protocol represents a rst step towards developing a comprehensive protocol for congestion control for time-sensitive multimedia data streams. We evaluate the protocol under various tra c conditions, using simulations and implementation. The simulations are used to study the behavior of the protocol under controlled conditions. The implementation and experimentation involve over 300 experiments over the Internet, using several machines in the US and UK. Our experimental and simulation results show that the protocol is fair to TCP and to other sessions running TFRCP, and that the formula-based approach to achieving TCP-friendliness is indeed practical. 
361|Congestion control principles|Status of this Memo This document is an Internet-Draft and is in full conformance with all provisions of Section 10 of RFC2026. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months and may be updated, replaced, or obsoleted by other documents at any time. It is inappropriate to use Internet- Drafts as reference material or to cite them other than as &#034;work in progress.&#034; The list of current Internet-Drafts can be accessed at
362|The Loss-Delay Based Adjustment Algorithm: A TCP-Friendly Adaptation Scheme|Many distributed multimedia applications have the ability to adapt to fluctuations in the network conditions. By adjusting temporal and spatial quality to available bandwidth, or manipulating the playout time of continuous media in response to variations in delay, multimedia flows can keep an acceptable QoS level at the end systems. In this paper, we present a new scheme called the loss-delay based adjustment algorithm (LDA) for adapting the transmission rate of multimedia applications to the congestion level of the network. The LDA algorithm was designed to reduce losses and improve utilization in a TCP-friendly way that avoids starving competing TCP connections. It relies on the end-toend Real Time Transport Protocol (RTP) for feedback information. In addition, we enhanced RTP with functionalities for determining the bottleneck bandwidth of a connection. The bottleneck bandwidth is then used for dynamically determining the adaptation parameters. Simulations and measurements of the LD...
363|Experiments with a Layered Transmission Scheme over the Internet|Combining hierarchical coding of data with receiver-driven control appears to be an attractive scheme for the multicast transmission of audio/video flows in a heterogeneous multicast environment such as the Internet. However, little experimental data is available regarding the actual performance of such schemes over the Internet. Previous work such as that on receiver driven layered multicast uses join experiments to choose the best quality signal a receiver can subscribe to. In this paper, we present a receiver-based multicast rate control mechanism based on a recently proposed TCP-friendly unicast mechanism. We have implemented this mechanism and evaluate its performance in conjunction with a simple layered audio coding scheme. We find that it has interesting convergence and performance properties, but also bring out its limitations.
364|A Comparison of Equation-based and AIMD Congestion Control|This paper considers AIMD-based (Additive-Increase Multiplicative-Decrease) congestion control mechanisms that are TCP-compatible (i.e., that compete reasonably fairly with TCP), but that reduce their sending rate less sharply than does TCP in response to a single packet drop. The paper then briefly compares these smoother AIMD-based congestion control mechanisms with TFRC (TCP-Friendly Rate Control), which makes use of equation-based congestion control. 
365|Dummynet and forward error correction|In this paper we presentacouple of tools developed by theauthor on FreeBSD, and available from the author&#039;s Web page in source format. The rst one, called dummynet, is a tool designed for the performance evaluation of network protocols and applications. Despite its original design goal, there has been a lot of interest on using dummynet as a bandwidth manager in network servers. dummynet simulates the e ect of nite queues, bandwidth limitations, and queueing delays, and is embedded in the protocol stack of the host, allowing even complex experiments to be run on a single machine, using existing applications and protocol implementations. The second tool is a software implementation of an erasure code especially suited for use in network protocols. Erasure codes are used in Forward Error Correction (FEC) techniques to reduce or remove the need for retransmissions in presence of communication errors. FEC has been rarely used in network protocols, because of the encoding/decoding overhead, and also because the underlying theory of error correcting codes is generally not well known to network researchers. In this paper we discuss the theory behind a simple erasure code, and provide performance data to show that the encoding/decoding overhead is acceptable for many applications even on low-end machines. 1
366|TCP-friendly Congestion Control for Real-time Streaming Applications|This paper introduces and analyzes a class of nonlinear congestion control algorithms called binomial algorithms, motivated in part by the needs of streaming audio and video applications for which a drastic reduction in transmission rate upon congestion is problematic. Binomial algorithms generalize TCP-style additive-increase by increasing inversely proportional to a power ? of the current window (for TCP, ?  ? ) ; they generalize TCP-style multiplicative-decrease by decreasing proportional to a power of the current window (for TCP,  ?). We show that there are an infinite number of deployable TCP-friendly binomial algorithms, all of which satisfy ?  ? , and that all binomial algorithms converge to fairness under a synchronized-feedback assumption provided ?  ?  ? ? ?  ?. Our simulation results show that binomial algorithms interact well with TCP across a RED gateway. We focus on two particular algorithms, IIAD (inverse-increase/additive-decrease,  ?  ?  ?  ? ) and SQRT ( ?  ?  ? ??), showing that they are well-suited to applications that do not react well to large TCP-style window reductions. We also find that TCP-friendliness in terms of the relationship between throughput and loss rate of an algorithm does not necessarily imply fairness relative to TCP performance, especially for drop-tail bottleneck gateways. 1
367|TCP Congestion Window Validation|This memo defines an Experimental Protocol for the Internet community. It does not specify an Internet standard of any kind. Discussion and suggestions for improvement are requested. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (2000). All Rights Reserved. TCP’s congestion window controls the number of packets a TCP flow may have in the network at any time. However, long periods when the sender is idle or application-limited can lead to the invalidation of the congestion window, in that the congestion window no longer reflects current information about the state of the network. This document describes a simple modification to TCP’s congestion control algorithms to decay the congestion window cwnd after the transition from a sufficiently-long application-limited period, while using the slow-start threshold ssthresh to save information about the previous value of the congestion window. An invalid congestion window also results when the congestion window is increased (i.e., in TCP’s slow-start or congestion avoidance phases) during application-limited periods, when the previous value of the congestion window might never have been fully utilized. We propose that the TCP sender should not increase the congestion window when the TCP sender has been application-limited (and therefore has not fully used the current congestion window). We have explored these algorithms both with simulations and with experiments from an implementation in FreeBSD.
368|Equation-Based Congestion Control|In this thesis, we introduce and analyze the TCP-Friendly Rate Control Protocol (TFRC), a rate-based end-to-end congestion control protocol. TFRC uses a model for steady state TCP throughput to limit the sending rate and assure fair behavior against competing flows. Instead of reacting to single congestion events (in the form of packet loss) like TCP, the TFRC protocol changes its sending rate in response to the loss rate, sampled over a certain amount of time. While TFRC achieves the same long-term throughput as a conformant TCP flow, its short-term sending rate is more stable. This makes the protocol suitable for traffic where sudden rate changes are undesirable (e.g. video or audio streams). Furthermore, rate-based congestion control is a promising basis for large scale multicast transport protocols. Since no router support is necessary, the protocol can be readily deployed in today’s Internet.  
369|Bro: A System for Detecting Network Intruders in Real-Time|We describe Bro, a stand-alone system for detecting network intruders in real-time by passively monitoring a network link over which the intruder&#039;s traffic transits. We give an overview of the system&#039;s design, which emphasizes highspeed (FDDI-rate) monitoring, real-time notification, clear separation between mechanism and policy, and extensibility. To achieve these ends, Bro is divided into an “event engine” that reduces a kernel-filtered network traffic stream into a series of higher-level events, and a “policy script interpreter” that interprets event handlers written in a specialized language used to express a site&#039;s security policy. Event handlers can update state information, synthesize new events, record information to disk, and generate real-time notifications via syslog. We also discuss a number of attacks that attempt to subvert passive monitoring systems and defenses against these, and give particulars of how Bro analyzes the six applications integrated into it so far: Finger, FTP, Portmapper, Ident, Telnet and Rlogin. The system is publicly available in source code form.  
370|End-to-End Internet Packet Dynamics|  We discuss findings from a large-scale study of Internet packet dynamics conducted by tracing 20 000 TCP bulk transfers between 35 Internet sites. Because we traced each 100-kbyte transfer at both the sender and the receiver, the measurements allow us to distinguish between the end-toend behaviors due to the different directions of the Internet paths, which often exhibit asymmetries. We: 1) characterize the prevalence of unusual network events such as out-of-order delivery and packet replication; 2) discuss a robust receiver-based algorithm for estimating “bottleneck bandwidth ” that addresses deficiencies discovered in techniques based on “packet pair;” 3) investigate patterns of packet loss, finding that loss events are not well modeled as independent and, furthermore, that the distribution of the duration of loss events exhibits infinite variance; and 4) analyze variations in packet transit delays as indicators of congestion periods, finding that congestion periods also span a wide range of time scales. 
371|A Methodology for Testing Intrusion Detection Systems|Intrusion Detection Systems (IDSs) attempt to identify unauthorized use, misuse, and abuse of computer systems. In response to the growth in the use and development of IDSs, we have developed a methodology for testing IDSs. The methodology consists of techniques from the field of software testing which we have adapted for the specific purpose of testing IDSs. In this paper, we identify a set of general IDS performance objectives which is the basis for the methodology. We present the details of the methodology, including strategies for test-case selection and specific testing procedures. We include quantitative results from testing experiments on the Network Security Monitor (NSM), an IDS developed at UC Davis. We present an overview of the software platform that we have used to create user-simulation scripts for testing experiments. The platform consists of the UNIX tool expect and enhancements that we have developed, including mechanisms for concurrent scripts and a record-and-replay ...
372|Collaborative Load Shedding for Media-Based Applications|The VuSystem is a software-intensive video environment developed on operating systems without real-time features. In the course of our work with this system, interesting questions have been brought up about how resources on a workstation can be best divided between applications, and what resource management features (if any) are required. This paper&#039;s main purpose is to outline a rationale for collaborative load shedding and to create an awareness within the operating system community; it also discusses some our early experiences with simple collaborative load shedding schemes.  1 1.0 Introduction  The VuSystem[3,4] is a software-intensive system for manipulating video streams on workstations. The VuSystem runs on a wide variety of workstations, including Suns, DECStations, DEC Alpha workstations, and Silicon Graphics workstations. Its performance varies  from workstation to workstation, simply trying to do &#034;the best&#034; it can on each platform. This system works very well for workstation...
373|A Scalable Content-Addressable Network|Hash tables – which map “keys ” onto “values” – are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internet-like scales. The CAN is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation. 
374|Pastry: Scalable, distributed object location and routing for large-scale peer-to-peer systems|This paper presents the design and evaluation of Pastry, a scalable, distributed object location and routing scheme for wide-area peer-to-peer applications. Pastry provides application-level routing and object location in a potentially very large overlay network of nodes connected via the Internet. It can be used to support a wide range of peer-to-peer applications like global data storage, global data sharing, and naming. An insert operation in Pastry stores an object at a user-defined number of diverse nodes within the Pastry network. A lookup operation reliably retrieves a copy of the requested object if one exists. Moreover, a lookup is usually routed to the node nearest the client issuing the lookup (by some measure of proximity), among the nodes storing the requested object. Pastry is completely decentralized, scalable, and self-configuring; it automatically adapts to the arrival, departure and failure of nodes. Experimental results obtained with a prototype implementation on a simulated network of 100,000 nodes confirm Pastry&#039;s scalability, its ability to self-configure and adapt to node failures, and its good network locality properties.
375|Tapestry: An infrastructure for fault-tolerant wide-area location and routing|In today’s chaotic network, data and services are mobile and replicated widely for availability, durability, and locality. Components within this infrastructure interact in rich and complex ways, greatly stressing traditional approaches to name service and routing. This paper explores an alternative to traditional approaches called Tapestry. Tapestry is an overlay location and routing infrastructure that provides location-independent routing of messages directly to the closest copy of an object or service using only point-to-point links and without centralized resources. The routing and directory information within this infrastructure is purely soft state and easily repaired. Tapestry is self-administering, faulttolerant, and resilient under load. This paper presents the architecture and algorithms of Tapestry and explores their advantages through a number of experiments. 1
376|Resilient Overlay Networks|A Resilient Overlay Network (RON) is an architecture that allows distributed Internet applications to detect and recover from path outages and periods of degraded performance within several seconds, improving over today’s wide-area routing protocols that take at least several minutes to recover. A RON is an application-layer overlay on top of the existing Internet routing substrate. The RON nodes monitor the functioning and quality of the Internet paths among themselves, and use this information to decide whether to route packets directly over the Internet or by way of other RON nodes, optimizing application-specific routing metrics. Results from two sets of measurements of a working RON deployed at sites scattered across the Internet demonstrate the benefits of our architecture. For instance, over a 64-hour sampling period in March 2001 across a twelve-node RON, there were 32 significant outages, each lasting over thirty minutes, over the 132 measured paths. RON’s routing mechanism was able to detect, recover, and route around all of them, in less than twenty seconds on average, showing that its methods for fault detection and recovery work well at discovering alternate paths in the Internet. Furthermore, RON was able to improve the loss rate, latency, or throughput perceived by data transfers; for example, about 5 % of the transfers doubled their TCP throughput and 5 % of our transfers saw their loss probability reduced by 0.05. We found that forwarding packets via at most one intermediate RON node is sufficient to overcome faults and improve performance in most cases. These improvements, particularly in the area of fault detection and recovery, demonstrate the benefits of moving some of the control over routing into the hands of end-systems. 
377|Multicast Routing in Datagram Internetworks and Extended LANs|Multicasting, the transmission of a packet to a group of hosts, is an important service for improving the efficiency and robustness of distributed systems and applications. Although multicast capability is available and widely used in local area networks, when those LANs are interconnected by store-and-forward routers, the multicast service is usually not offered across the resulting internetwork. To address this limitation, we specify extensions to two common internetwork routing algorithms-distance-vector routing and link-state routing-to support low-delay datagram multicasting beyond a single LAN. We also describe modifications to the single-spanning-tree routing algorithm commonly used by link-layer bridges, to reduce the costs of multicasting in large extended LANs. Finally, we discuss how the use of multicast scope control and hierarchical multicast routing allows the multicast service to scale up to large internetworks.
378|How to Model an Internetwork|Graphs are commonly used to model the structure of internetworks, for the study of problems ranging from routing to resource reservation. A variety of graph models are found in the literature, including regular topologies such as rings or stars, &#034;well-known&#034; topologies such as the original ARPAnet, and randomly generated topologies. Less common is any discussion of how closely these models correlate with real network topologies. We consider the problem of efficiently generating graph models that accurately reflect the topological properties of real internetworks. We compare properties of graphs generated using various methods with those of real internets. We also propose efficient methods for generating topologies with particular properties, including a Transit-Stub model that correlates well with Internet structure. Improved models for internetwork structure have the potential to impact the significance of simulation studies of internetworking solutions, providing basis for the validi...
379|SCRIBE: The design of a large-scale event notification infrastructure|This paper presents Scribe, a large-scale event notification infrastructure  for topic-based publish-subscribe applications. Scribe supports large numbers  of topics, with a potentially large number of subscribers per topic. Scribe is  built on top of Pastry, a generic peer-to-peer object location and routing substrate  overlayed on the Internet, and leverages Pastry&#039;s reliability, self-organization and  locality properties. Pastry is used to create a topic (group) and to build an efficient  multicast tree for the dissemination of events to the topic&#039;s subscribers  (members). Scribe provides weak reliability guarantees, but we outline how an  application can extend Scribe to provide stronger ones.
381|ALMI: An Application Level Multicast Infrastructure|The IP multicast model allows scalable and efficient multi-party communication, particularly for groups of large size. However, deployment of IP multicast requires substantial infrastructure modifications and is hampered by a host of unresolved open problems. To circumvent this situation, we have designed and implemented ALMI, an application level group communication middleware, which allows accelerated application deployment and simplified network configuration, without the need of network infrastructure support. ALMI is tailored toward support of multicast groups of relatively small size (several I Os of members) with many to many semantics. Session participants are connected via a vir- tual multicast tree, which consists of unicast connections between end hosts and is formed as a minimum spanning tree (MST) using application-specific performance metric. Using simulation, we show that the performance penalties, introduced by this shift of multicast to end systems, is a relatively small increase in traffic load and that ALMI multicast trees approach the efficiency of IP multicast trees. We have also implemented ALMi as a Java based middleware package and performed experiments over the Internet. Experimental results show that ALMI is able to cope with network dynamics and keep the mul- ticast tree efficient.
382|Host Multicast: A Framework for Delivering Multicast To End Users|While the advantages of multicast delivery over multiple unicast deliveries is undeniable, the deployment of the IP multicast protocol has been limited to &#034;islands&#034; of network domains under single administrative control. Deployment of inter-domain multicast delivery has been slow due to both technical and administrative reasons. In this paper we propose a Host Multicast Tree Protocol (HMTP) that (1) automates the interconnection of IP-multicast enabled islands and (2) provides multicast delivery to end hosts where IP multicast is not available. With HMTP, end-hosts and proxy gateways of IP multicast-enabled islands can dynamically create shared multicast trees across different islands. Members of an HMTP multicast group self-organize into an efficient, scalable and robust multicast tree. The tree structure is adjusted periodically to accommodate changes in group membership and network topology. Simulation results show that the multicast tree has low cost, and data delivered over it experiences moderately low latency. I. 
383|Detour: a Case for Informed Internet Routing and Transport|Despite its obvious success, robustness, and scalability, the Internet suffers from a number of end-to-end performance and availability problems. In this paper, we attempt to quantify the Internet&#039;s inefficiencies and then we argue that Internet behavior can be improved by spreading intelligent routers at key access and interchange points to actively manage traffic. Our Detour prototype aims to demonstrate practical benefits to end users, without penalizing non-Detour users, by aggregating traffic information across connections and using more efficient routes to improve Internet performance. 1 Introduction  By any metric, the Internet has scaled remarkably; from 4 nodes in 1969 to an estimated 25 million hosts and 100 million users today. This reflects a sustained growth rate over three decades of roughly 80% per year, all while providing nearly continuous service. As a system, the Internet&#039;s growth has been matched only by the major infrastructure projects of the early 1900&#039;s: the ele...
384|Scalable Secure Group Communication over IP Multicast|We introduce and analyze a scalable re-keying scheme for implementing secure group communications IP multicast. We show that our scheme incurs constant processing, message, and storage overhead for a re-key operation when a single member joins or leaves the group, and logarithmic overhead for bulk simultaneous changes to the group membership. These bounds hold even when group dynamics are not known a-priori.
385|Missing data: Our view of the state of the art|Statistical procedures for missing data have vastly improved, yet misconception and unsound practice still abound. The authors frame the missing-data problem, review methods, offer advice, and raise issues that remain unresolved. They clear up common misunderstandings regarding the missing at random (MAR) concept. They summarize the evidence against older procedures and, with few exceptions, dis-courage their use. They present, in both technical and practical language, 2 general approaches that come highly recommended: maximum likelihood (ML) and Bayes-ian multiple imputation (MI). Newer developments are discussed, including some for dealing with missing data that are not MAR. Although not yet in the main-stream, these procedures may eventually extend the ML and MI methods that currently represent the state of the art. Why do missing data create such difficulty in sci-entific research? Because most data analysis proce-dures were not designed for them. Missingness is usu-ally a nuisance, not the main focus of inquiry, but
386|Statistical Analysis with Missing Data|Subsample ignorable likelihood for regression
387|Bayesian Data Analysis|I actually own a copy of Harold Jeffreys’s Theory of Probability but have only read small bits of it, most recently over a decade ago to confirm that, indeed, Jeffreys was not too proud to use a classical chi-squared p-value when he wanted to check the misfit of a model to data (Gelman, Meng and Stern, 2006). I do, however, feel that it is important to understand where our probability models come from, and I welcome the opportunity to use the present article by Robert, Chopin and Rousseau as a platform for further discussion of foundational issues. 2 In this brief discussion I will argue the following: (1) in thinking about prior distributions, we should go beyond Jeffreys’s principles and move toward weakly informative priors; (2) it is natural for those of us who work in social and computational sciences to favor complex models, contra Jeffreys’s preference for simplicity; and (3) a key generalization of Jeffreys’s ideas is to explicitly include model checking in the process of data analysis.
388|Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation|We propose a remedy for the discrepancy between the way political scientists analyze data with missing values and the recommendations of the statistics community. Methodologists and statisticians agree that &#034;multiple imputation&#034; is a superior approach to the problem of missing data scattered through one&#039;s explanatory and dependent variables than the methods currently used in applied data analysis. The reason for this discrepancy lies with the fact that the computational algorithms used to apply the best multiple imputation models have been slow, difficult to implement, impossible to run with existing commercial statistical packages, and demanding of considerable expertise.  In this paper, we adapt an existing algorithm, and use it to implement a generalpurpose, multiple imputation model for missing data. This algorithm is considerably faster and easier to use than the leading method recommended in the statistics literature. We also quantify the risks of current missing data practices, ...
389|Multiple imputation for multivariate missing-data problems: a data analyst&#039;s perspective|Analyses of multivariate data are frequently hampered by missing values. Until re-cently, the only missing-data methods available to most data analysts have been relatively ad hoc practices such as listwise deletion. Recent dramatic advances in theoretical and com-putational statistics, however, have produced a new generation of flexible procedures with a sound statistical basis. These procedures involve multiple imputation (Rubin, 1987), a simu-lation technique that replaces each missing datum with a set of m&gt;1 plausible values. The m versions of the complete data are analyzed by standard complete-data methods, and the results are combined using simple rules to yield estimates, standard errors, and p-values that formally incorporate missing-data uncertainty. New computational algorithms and software described in a recent book (Schafer, 1997) allow us to create proper multiple imputations in complex multivariate settings. This article reviews the key ideas of multiple imputation, discusses the software programs currently available, and demonstrates their use on data from
390|Application of random-effects pattern-mixture models for missing data in longitudinal studies|Random-effects regression models have become increasingly popular for analysis of longitudinal data. A key advantage of the random-effects approach is that it can be applied when subjects are not measured at the same number of timepoints. In this article we describe use of random-effects pattern-mixture models to further handle and describe the influence of missing data in longitudinal studies. For this approach, subjects are first divided into groups depending on their missing-data pattern and then variables based on these groups are used as model covariates. In this way, researchers are able to examine the effect of missing-data patterns on the outcome (or outcomes) of interest. Furthermore, overall estimates can be obtained by averaging over the missing-data patterns. A psychiatric clinical trials data set is used to illustrate the random-effects pattern-mixture approach to longitudinal data analysis with missing data.  
391|Multiple Imputation for Missing Data: A Cautionary Tale|: Two algorithms for producing multiple imputations for missing data are evaluated with simulated data. Software using a propensity score classifier with the approximate Bayesian boostrap produces badly biased estimates of regression coefficients when data on predictor variables are missing at random or missing completely at random. On the other hand, a regression-based method employing the data augmentation algorithm produces estimates with little or no bias.  4 Multiple imputation (MI) appears to be one of the most attractive methods for generalpurpose handling of missing data in multivariate analysis. The basic idea, first proposed by Rubin (1977) and elaborated in his (1987) book, is quite simple: 1. Impute missing values using an appropriate model that incorporates random variation. 2. Do this M times (usually 3-5 times), producing M &#034;complete&#034; data sets. 3. Perform the desired analysis on each data set using standard complete-data methods. 4. Average the values of the parameter ...
392|On Structural Equation Modeling with Data that are not Missing Completely at Random|A general latent variable model is given which includes the specification of a missing data mechanism. This framework allows for an elucidating discussion of existing general multivariate theory bearing on maximum likelihood estimation with missing data. Here, missing completely at random is not a prerequisite for unbiased estimation in large samples, as when using the traditional listwise or pairwise present data approaches. The theory is connected with old and new results in the area of selection and factorial invariance. It is pointed out that in many applications, maximum likelihood estimation with missing data may be carried out by existing structural equation modeling software, such as LISREL and LISCOMP. Several sets of artifical data are generated within the general model framework. The proposed estimator is compared to the two traditional ones and found superior. Key words: maximum likelihood, ignorability, selectivity, factor analysis, factorial invariance,
393|Multiple Imputation in Practice: Comparison of Software Packages for Regression Models With Missing Variables |This article reviews multiple imputation, describes assumptions that it requires, and reviews software packages that implement this procedure. We apply the methods and compare the results using two examples---a child psychopathology dataset with missing outcomes and an artificial dataset with missing covariates. We conclude with some discussion of the strengths and weaknesses of these implementations as well as advantages and limitations of imputation
394|Imputation of the 1989 Survey of Consumer Finances: Stochastic Relaxation and Multiple Imputation” mimeo, Board of Governors of the Federal Reserve System|acknowledges the support for this work by staff in the Division of Research and Statistics including
395|ANALYSIS WITH MISSING DATA IN PREVENTION RESEARCH|Missing data are pervasive in alcohol and drug abuse prevention evaluation efforts: Researchers administer surveys, and some items are left unanswered. Slow readers often leave large portions incomplete at the end of the survey. Researchers administer the surveys at several points in time, and people fail to show up at one or more waves of measurement. Researchers often design their measures to include a certain amount of “missingness”; some measures are so expensive (in money or time) that researchers can afford to administer them only to some respondents. Missing data problems have been around for years. Until recently, researchers have fumbled with partial solutions and put up only the weakest counterarguments to the admonitions of the critics of prevention and applied psychological research. Things have changed, however. Statistically sound solutions are now available for virtually every missing data problem,
396|Inference with Imputed Conditional Means|In this paper, we develop analytic techniques that can be used to produce appropriate inferences from a data set in which imputation for missing values has been carried out using predictive means. Our derivations are based on asymptotic expansions of point estimators and their associated variance estimators, and the resulting formulas can be thought of as first-order approximations to the estimators that would be used with multiple imputation. The procedures developed can be used either for univariate missing data or for multivariate missing data in which the variables are either missing or observed together, and they are designed for situations in which the complete-data estimator is a smooth function of linear statistics. We illustrate properties of our methods in several examples, including abstract problems as well as applications to large data sets from studies carried out by the federal government. Key Words: Linearization; Missing data; Multiple Imputation; Nonresponse; Taylor s...
397|Maximum Likelihood Analysis of Generalized Linear Models with Missing Covariates|Missing data is a common occurrence in most medical research data collection enterprises. There is an extensive literature concerning missing data, much of which has focused on missing outcomes. Covariates in regression models are often missing, particularly if information is being collected from multiple sources. The method of weights is an implementation of the EM algorithm 8 for general maximum-likelihood analysis of regression models, including generalized linear models 32 (GLMs) with incomplete covariates. In this paper, we will describe the method of weights in detail, illustrate its application with several examples, discuss its advantages and limitations, and review extensions and applications of the method.
398|Multiple imputation and posterior simulation for multivariate missing data in longitudinal studies (pp  (1995) |SUMMARY. This paper outlines a multiple imputation method for handling missing data in designed lon-gitudinal studies. A random coefficients model is developed to accommodate incomplete multivariate con-tinuous longitudinal data. Multivariate repeated measures are jointly modeled; specifically, an i.i.d. normal model is assumed for time-independent variables and a hierarchical random coefficients model is assumed for time-dependent variables in a regression model conditional on the time-independent variables and time, with heterogeneous error variances across variables and time points. Gibbs sampling is used to draw model parameters and for imputations of missing observations. An application to data from a study of startle reactions illustrates the model. A simulation study compares the multiple imputation procedure to the weighting approach of Robins, Rotnitzky, and Zhao (1995, Journal of the American Statistical Association 90, 106-121) that can be used to address similar data structures. KEY WORDS: Gibbs sampling; Missing data; Multiple imputation; Multivariate longitudinal data 1. Background In designed longitudinal studies, missing data often occur be-cause subjects miss visits during the study, because some vari-ables may not be measured at particular visits, or because
399|Space/Time Trade-offs in Hash Coding with Allowable Errors|this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash- coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency
400|Taming the Underlying Challenges of Reliable Multihop Routing in Sensor Networks|The dynamic and lossy nature of wireless communication poses major challenges to reliable, self-organizing multihop networks. These non-ideal characteristics are more problematic with the primitive, low-power radio transceivers found in sensor networks, and raise new issues that routing protocols must address. Link connectivity statistics should be captured dynamically through an efficient yet adaptive link estimator and routing decisions should exploit such connectivity statistics to achieve reliability. Link status and routing information must be maintained in a neighborhood table with constant space regardless of cell density. We study and evaluate link estimator, neighborhood table management, and reliable routing protocol techniques. We focus on a many-to-one, periodic data collection workload. We narrow the design space through evaluations on large-scale, high-level simulations to 50-node, in-depth empirical experiments. The most effective solution uses a simple time averaged EWMA estimator, frequency based table management, and cost-based routing.
401|Dynamic source routing in ad hoc wireless networks|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any established infrastructure or centralized administration. In such an environment, it may be necessary for one mobile host to enlist the aid of other hosts in forwarding a packet to its destination, due to the limited range of each mobile host’s wireless transmissions. This paper presents a protocol for routing in ad hoc networks that uses dynamic source routing. The protocol adapts quickly to routing changes when host movement is frequent, yet requires little or no overhead during periods in which hosts move less frequently. Based on results from a packet-level simulation of mobile hosts operating in an ad hoc network, the protocol performs well over a variety of environmental conditions such as host density and movement rates. For all but the highest rates of host movement simulated, the overhead of the protocol is quite low, falling to just 1 % of total data packets transmitted for moderate movement rates in a network of 24 mobile hosts. In all cases, the difference in length between the routes used and the optimal route lengths is negligible, and in most cases, route lengths are on average within a factor of 1.01 of optimal. 1.
402|Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers  (1994) |An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize adhoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.  
404|Topology Control of Multihop Wireless Networks using Transmit Power Adjustment| We consider the problem of adjusting the transmit powers of nodes in a multihop wireless network (also called an ad hoc network) to create a desired topology. We formulate it as a constrained optimization problem with two constraints- connectivity and biconnectivity, and one optimization objective- maximum power used. We present two centralized algorithms for use in static networks, and prove their optimality. For mobile networks, we present two distributed heuristics that adaptively adjust node transmit powers in response to topological changes and attempt to maintain a connected topology using minimum power. We analyze the throughput, delay, and power consumption of our algorithms using a prototype software implementation, an emulation of a power-controllable radio, and a detailed channel model. Our results show that the performance of multihop wireless networks in practice can be substantially increased with topology control.  
405|Wake on Wireless: An Event Driven Energy Saving Strategy for Battery Operated Devices|The demand for an all-in-one phone with integrated personal information management and data access capabilities is beginning to accelerate. While personal digital assistants (PDAs) with built-in cellular, WiFi, and Voice-Over-IP technologies have the ability to serve these needs in a single package, the rate at which energy is consumed by PDA-based phones is very high. Thus, these devices can quickly drain their own batteries and become useless to their owner. In this paper, we introduce a technique to increase the battery lifetime of a PDA-based phone by reducing its idle power, the power a device consumes in a “standby ” state. To reduce the idle power, we essentially shut down the device and its wireless network card when the device is not being used—the device is powered only when an incoming call is received. Using this technique, we can increase the battery lifetime by up to 115%. In this paper, we describe the design of our “wake-on-wireless ” energy-saving strategy and the prototype device we implemented. To evaluate our technique, we compare it with alternative approaches. Our results show that our technique can provide a significant lifetime improvement over other technologies. Categories and Subject Descriptors
406|Real-World Experiences with an Interactive Ad Hoc Sensor Network|While it is often suggested that moderate-scale ad hoc sensor networks are a promising approach to solving real-world problems, most evaluations of sensor network protocols have focused on simulation, rather than realworld, experiments. In addition, most experimental results have been obtained in limited scale. This paper describes a practical application of moderate-scale ad hoc sensor networks. We explore several techniques for reducing packet loss, including quality-based routing and passive acknowledgment, and present an empirical evaluation of the effect of these techniques on packet loss and data freshness.
407|Eigrp - A Fast Routing Protocol Based On Distance Vectors|Early routing protocols were based on distance vectors; they were very simple and easy to implement  but had the severe drawbacks of counting to infinity and routing loops. These problems were reduced  using such techniques as split horizon and hold-downs; however, for these techniques to work in practice,  long convergence times are introduced. Routing protocols based on link states have been implemented  to address the problem of slow convergence in distance-vector protocols, but they add complexity in  configuration and troubleshooting. We present a new distance-vector protocol that converges as quickly  as current link-state protocols, while maintaining loop freedom at every instant. The protocol is based  on three main elements: a transport algorithm that supports the reliable exchange of messages among  routers, the diffusing update algorithm, which computes shortest paths distributedly, and modules  that permit the operation of the new routing protocol in a multiprotocol environment.
408|The mote connectivity protocol|Abstract — An attractive architecture for sensor networks is to have the sensing devices mounted on small computers, called motes. Motes are battery-powered, and can communicate in a wireless fashion by broadcasting messages over radio frequency. In mote networks, the connectivity of a mote u can be defined by those motes that can receive messages from u with high probability and those motes from which u can receive messages with high probability. In this paper, we describe a protocol that can be triggered by any mote in a mote network in order that each mote in the network computes its connectivity. The protocol is simple and has several energy saving features. We implemented this protocol over TinyOS and discuss the results of some execution runs of this implementation. I.
409|Cloud Computing and Emerging IT Platforms: Vision, Hype, and Reality for Delivering Computing as the 5th Utility |With the significant advances in Information and Communications Technology (ICT) over the last half century, there is an increasingly perceived vision that computing will one day be the 5th utility (after water, electricity, gas, and telephony). This computing utility, like all other four existing utilities, will provide the basic level of computing service that is considered essential to meet the everyday needs of the general community. To deliver this vision, a number of computing paradigms have been proposed, of which the latest one is known as Cloud computing. Hence, in this paper, we define Cloud computing and provide the architecture for creating Clouds with market-oriented resource allocation by leveraging technologies such as Virtual Machines (VMs). We also provide insights on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain Service Level Agreement (SLA)-oriented resource allocation. In addition, we reveal our early thoughts on interconnecting Clouds for dynamically creating global Cloud exchanges and markets. Then, we present some representative Cloud platforms, especially those developed in industries along with our current work towards realizing market-oriented resource allocation of Clouds as realized in Aneka enterprise Cloud technology. Furthermore, we highlight the difference between High Performance Computing (HPC) workload and Internet-based services workload. We also describe a meta-negotiation infrastructure to establish global Cloud
410|MapReduce: Simplified Data Processing on Large Clusters|MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.  
411|Xen and the art of virtualization|Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100 % binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service. This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort. Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead — at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests.
412|Improving MapReduce Performance in Heterogeneous Environments |MapReduce is emerging as an important programming model for large-scale data-parallel applications such as web indexing, data mining, and scientific simulation. Hadoop is an open-source implementation of MapReduce enjoying wide adoption and is often used for short jobs where low response time is critical. Hadoop’s performance is closely tied to its task scheduler, which implicitly assumes that cluster nodes are homogeneous and tasks make progress linearly, and uses these assumptions to decide when to speculatively re-execute tasks that appear to be stragglers. In practice, the homogeneity assumptions do not always hold. An especially compelling setting where this occurs is a virtualized data center, such as Amazon’s Elastic Compute Cloud (EC2). We show that Hadoop’s scheduler can cause severe performance degradation in heterogeneous environments. We design a new scheduling algorithm, Longest Approximate Time to End (LATE), that is highly robust to heterogeneity. LATE can improve Hadoop response times by a factor of 2 in clusters of 200 virtual machines on EC2. 1
413|Market-oriented cloud computing: Vision, hype, and reality for delivering IT services as computing utilities, in|This keynote paper: presents a 21 st century vision of computing; identifies various computing paradigms promising to deliver the vision of computing utilities; defines Cloud computing and provides the architecture for creating market-oriented Clouds by leveraging technologies such as VMs; provides thoughts on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain SLAoriented resource allocation; presents some representative Cloud platforms especially those developed in industries along with our current work towards realising market-oriented resource allocation of Clouds by leveraging the 3 rd generation Aneka enterprise Grid technology; reveals our early thoughts on interconnecting Clouds for dynamically creating an atmospheric computing environment along with pointers to future community research; and concludes with the need for convergence of competing IT paradigms for delivering our 21 st century vision. 1.
414|Sharp: An architecture for secure resource peering|This paper presents Sharp, a framework for secure distributed resource management in an Internet-scale computing infrastructure. The cornerstone of Sharp is a construct to represent cryptographically protected resource claims— promises or rights to control resources for designated time intervals—together with secure mechanisms to subdivide and delegate claims across a network of resource managers. These mechanisms enable flexible resource peering: sites may trade their resources with peering partners or contribute them to a federation according to local policies. A separation of claims into tickets and leases allows coordinated resource management across the system while preserving site autonomy and local control over resources. Sharp also introduces mechanisms for controlled, accountable oversubscription of resource claims as a fundamental tool for dependable, efficient resource management. We present experimental results from a Sharp prototype for PlanetLab, and illustrate its use with a decentralized barter economy for global PlanetLab resources. The results demonstrate the power and practicality of the architecture, and the effectiveness of oversubscription for protecting resource availability in the presence of failures.
415|SNAP: A protocol for negotiating service level agreements and coordinating resource management in distributed systems|A fundamental problem with distributed applications is how to map activities such as computation or data transfer onto a set of resources that will meet the application’s requirement for performance, cost, security, or other quality of service metrics. An application or client must engage in a multi-phase negotiation process with resource managers, as it discovers, reserves, acquires, configures, monitors, and potentially renegotiates resource access. We present a generalized resource management model in which resource interactions are mapped onto a well defined set of symmetric and resource independent service level agreements. We instantiate this model in (the Service Negotiation and Acquisition Protocol (SNAP) which provides integrated support for lifetime management and an at-most-once creation semantics for SLAs. The result is a resource management framework for distributed systems that we believe is more powerful and general than current approaches. We explain how SNAP can be deployed within the context of the Globus Toolkit. 1
416|Entropia: Architecture and Performance of an Enterprise Desktop Grid System|The exploitation of idle cycles on pervasive desktop PC systems offers the opportunity to increase the available computing power by orders of magnitude (10x - 1000x). However, for desktop PC distributed computing to be widely accepted within the enterprise, the systems must achieve high levels of efficiency, robustness, security, scalability, manageability, unobtrusiveness, and openness/ease of application integration. We describe the Entropia distributed computing system as a case study, detailing its internal architecture and philosophy in attacking these key problems. Key aspects of the Entropia system include the use of: 1) binary sandboxing technology for security and unobtrusiveness, 2) a layered architecture for efficiency, robustness, scalability and manageability, and 3) an open integration model to allow applications from many sources to be incorporated. Typical applications for the Entropia System includes molecular docking, sequence analysis, chemical structure modeling, and risk management. The applications come from a diverse set of domains including virtual screening for drug discovery, genomics for drug targeting, material property prediction, and portfolio management. In all cases, these applications scale to many thousands of nodes and have no dependences between tasks. We present representative performance results from several applications that illustrate the high performance, linear scaling, and overall capability presented by the Entropia system.
417|A Computational Economy for Grid Computing and its Implementation in the Nimrod-G Resource Broker|: Computational Grids, coupling geographically distributed resources such as PCs, workstations, clusters, and scientific instruments, have emerged as a next generation computing platform for solving large-scale problems in science, engineering, and commerce. However, application development, resource management, and scheduling in these environments continue to be a complex undertaking. In this article, we discuss our efforts in developing a resource management system for scheduling computations on resources distributed across the world with varying quality of service. Our service-oriented grid computing system called Nimrod-G manages all operations associated with remote execution including resource discovery, trading, scheduling based on economic principles and a user defined quality of service requirement. The Nimrod-G resource broker is implemented by leveraging existing technologies such as Globus, and provides new services that are essential for constructing industrial-strength Grids. We discuss results of preliminary experiments on scheduling some parametric computations using the Nimrod-G resource broker on a world-wide grid testbed that spans five continents. 1.
418| The Grid Economy |This chapter identifies challenges in managing resources in a Grid computing environment and proposes computational economy as a metaphor for effective management of resources and application scheduling. It identifies distributed resource management challenges and requirements of economybased Grid systems, and discusses various representative economy-based systems, both historical and emerging, for cooperative and competitive trading of resources such as CPU cycles, storage, and network bandwidth. It presents an extensible, service-oriented Grid architecture driven by Grid economy and an approach for its realization by leveraging various existing Grid technologies. It also presents commodity and auction models for resource allocation. The use of commodity economy model for resource management and application scheduling in both computational and data grids is also presented. 
419|Virtual Workspaces: Achieving Quality of Service and Quality|By defining standardized protocols for discovering, accessing, monitoring, and managing remote computers, storage systems, networks, and other resources, Grid technologies make it possible—in principle—to allocate resources to applications dynamically, in an on-demand fashion [1]. However, while Grids offer users access to many diverse and powerful resources, they do little to ensure that once a
420|The Case for Cooperative Networking|... CoopNet) where end-hosts cooperate to improve network performance perceived by all. In CoopNet, cooperation among peers complements traditional client-server communication rather than replacing it. We focus on the Web flash crowd problem and argue that CoopNet offers an effective solution. We present an evaluation of the CoopNet approach using simulations driven by traffic traces gathered at the MSNBC website during the flash crowd that occurred on September 11, 2001.
421|Tycoon: An Implementation of a Distributed, Market-based Resource Allocation System”, Multiagent Grid Systems |Distributed clusters like the Grid and PlanetLab enable the same statistical multiplexing efficiency gains for computing as the Internet provides for networking. One major challenge is allocating resources in an economically efficient and low-latency way. A common solution is proportional share, where users each get resources in proportion to their pre-defined weight. However, this does not allow users to differentiate the value of their jobs. This leads to economic inefficiency. In contrast, systems that require reservations impose a high latency (typically minutes to hours) to acquire resources. We present Tycoon, a market based distributed resource allocation system based on proportional share. The key advantages of Tycoon are that it allows users to differentiate the value of their jobs, its resource acquisition latency is limited only by communication delays, and it imposes no manual bidding overhead on users. We present experimental results using a prototype implementation of our design. 1
422|Sharing Networked Resources with Brokered Leases|This paper presents the design and implementation of Shirako, a system for on-demand leasing of shared networked resources. Shirako is a prototype of a serviceoriented architecture for resource providers and consumers to negotiate access to resources over time, arbitrated by brokers. It is based on a general lease abstraction: a lease represents a contract for some quantity of a typed resource over an interval of time. Resource types have attributes that define their performance behavior and degree of isolation. Shirako decouples fundamental leasing mechanisms from resource allocation policies and the details of managing a specific resource or service. It offers an extensible interface for custom resource management policies and new resource types. We show how Shirako enables applications to lease groups of resources across multiple autonomous sites, adapt to the dynamics of resource competition and changing load, and guide configuration and deployment. Experiments with the prototype quantify the costs and scalability of the leasing mechanisms, and the impact of lease terms on fidelity and adaptation. 1
423|Balancing Risk and Reward in a Market-based Task Service|This paper investigates the question of scheduling tasks according to a user-centric value metric—called yield or utility. User value is an attractive basis for allocating shared computing resources, and is fundamental to economic approaches to resource management in linked clusters or grids. Even so, commonly used batch schedulers do not yet support value-based scheduling, and there has been little study of its use in a market-based grid setting. In part this is because scheduling to maximize timevarying value is a difficult problem where even simple formulations are intractable. We present improved heuristics for value-based task scheduling using a simple but rich formulation of value, in which a task’s yield decays linearly with its waiting time. We also show the role of value-based scheduling heuristics in a framework for market-based bidding and admission control, in which clients negotiate for task services from multiple grid sites. Our approach follows an investment metaphor: the heuristics balance the risk of future costs against the potential for gains in accepting and scheduling tasks. In particular, we show the importance of opportunity cost, and the impact of risk due to uncertainty in the future job mix. 1
424|Resource Allocation in Federated Distributed Computing Infrastructures|Introduction  We consider the problem of allocating combinations of heterogeneous, distributed resources among selfinterested parties. In particular, we consider this problem in the context of distributed computing infrastructures, where resources are shared among users from di#erent administrative domains. Examples of such infrastructures include PlanetLab [15] and computational grids [7].  End-users derive utility from receiving a share of resources. When there is an excess demand for resources, it isn&#039;t possible to completely satisfy all resource requests. Therefore, we argue that it is important for these infrastructures to allocate resources in a way that maximizes aggregate end-user utility. Such an allocation system is known as economically e#cient. Because a user&#039;s utility function for resources isn&#039;t typically known a priori, determining an allocation policy to maximize utility is di#cult in the presence of excess demand. As use of these infrastructures becomes more widespread
425|A Grid Service Broker for Scheduling e-Science Applications on Global Data Grids|The next generation of scientific experiments and studies, popularly called e-Science, is carried out by large collaborations of researchers distributed around the world engaged in analysis of huge collections of data generated by scientific instruments. Grid computing has emerged as an enabler for e-Science as it permits the creation of virtual organizations that bring together communities with common objectives. Within a community, data collections are stored or replicated on distributed resources to enhance storage capability or efficiency of access. In such an environment, scientists need to have the ability to carry out their studies by transparently accessing distributed data and computational resources. In this paper, we propose and develop a Grid broker that mediates access to distributed resources by (a) discovering suitable data sources for a given analysis scenario, (b) suitable computational resources, (c) optimally mapping analysis jobs to resources, (d) deploying and monitoring job execution on selected resources, (e) accessing data from local or remote data source during job execution and (f) collating and presenting results. The broker supports a declarative and dynamic parametric programming model for creating grid applications. We have used this model in grid-enabling a high energy physics analysis application (Belle Analysis Software Framework). The broker has been used in deploying Belle experiment data analysis jobs on a grid testbed, called Belle Analysis Data Grid, having resources distributed across Australia interconnected through GrangeNet.
426|Drafting behind akamai (travelocity-based detouring  (2006) |To enhance web browsing experiences, content distribution networks (CDNs) move web content “closer ” to clients by caching copies of web objects on thousands of servers worldwide. Additionally, to minimize client download times, such systems perform extensive network and server measurements, and use them to redirect clients to different servers over short time scales. In this paper, we explore techniques for inferring and exploiting network measurements performed by the largest CDN, Akamai; our objective is to locate and utilize quality Internet paths without performing extensive path probing or monitoring. Our contributions are threefold. First, we conduct a broad measurement study of Akamai’s CDN. We probe Akamai’s network from 140 PlanetLab vantage points for two months. We find that Akamai redirection times, while slightly higher than advertised, are
427|Analysis and Characterization of Large-Scale Web Server Access Patterns and Performance|In this paper we develop a general methodology for characterizing the access patterns of Web server requests based on a time-series analysis of finite collections of observed data from real systems. Our approach is used together with the access logs from the IBM Web site for the Olympic Games to demonstrate some of its advantages over previous methods and to construct a particular class of benchmarks for large-scale heavily-accessed Web server environments. We then apply an instance of this class of benchmarks to analyze aspects of large-scale Web server performance, demonstrating some additional problems with commonly used methods to evaluate Web server performance at different request traffic intensities.
428|Power Aware Scheduling of Bag-of-Tasks Applications with Deadline Constraints on DVS-enabled Clusters |Power-aware scheduling problem has been a recent issue in cluster systems not only for operational cost due to electricity cost, but also for system reliability. As recent commodity processors support multiple operating points under various supply voltage levels, Dynamic Voltage Scaling (DVS) scheduling algorithms can reduce power consumption by controlling appropriate voltage levels. In this paper, we provide power-aware scheduling algorithms for bagof-tasks applications with deadline constraints on DVSenabled cluster systems in order to minimize power consumption as well as to meet the deadlines specified by application users. A bag-of-tasks application should finish all the sub-tasks before the deadline, so that the DVS scheduling scheme should consider the deadline as well. We provide the DVS scheduling algorithms for both time-shared and space-shared resource sharing policies. The simulation results show that the proposed algorithms reduce much power consumption compared to static voltage schemes. 1.
429|Market-oriented Grids and Utility Computing: The state-of-the-art and future directions|Traditional resource management techniques (resource allocation, admission control and scheduling) have been found to be inadequate for many shared Grid and distributed systems that face unpredictable and bursty workloads. They provide no incentive for users to request resources judiciously and appropriately, and they do not capture the true value and importance (the utility) of user jobs. Consequently, researchers and practitioners have been examining the appropriateness of ‘market-inspired ’ resource management techniques in ensuring that users are treated fairly, without unduly favouring one set of users over another. Such techniques aim to smooth out access patterns and reduce the chance of transient overload, by providing incentives for users to be flexible about their resource requirements and job deadlines. We examine the recent evolution of these systems, looking at the state of the art in price setting and negotiation, grid economy management and utilitydriven scheduling and resource allocation, and identify the advantages and limitations of these systems. We then look to the future of these systems, examining the emerging ‘Catallaxy ’ market paradigm and present Mercato, a decentralised, Catallaxy inspired architecture that encapsulates the future directions that need to be pursued to address the limitations of current generation of market oriented Grids and Utility Computing systems. 1
430|Pricing for Utility-driven Resource Management and Allocation in Clusters|Users perceive varying levels of utility for each different job completed by the cluster. Therefore, there is a need for existing cluster resource management systems (RMS) to provide a means for the user to express its perceived utility during job submission. The cluster RMS can then obtain and consider these user-centric needs such as Qualityof-Service requirements in order to achieve utility-driven resource management and allocation. We advocate the use of computational economy for this purpose. In this paper, we describe an architectural framework for a utility-driven cluster RMS. We present a user-level job submission specification for soliciting user-centric information that is used by the cluster RMS for making better resource allocation decisions. In addition, we propose a dynamic pricing function
431|A Grid Resource Broker Supporting Advance Reservations and Benchmark-based Resource Selection |Abstract. This contribution presents algorithms, methods, and soft-ware for a Grid resource manager, responsible for resource brokering and scheduling in early production Grids. The broker selects computing resources based on actual job requirements and a number of criteria iden-tifying the available resources, with the aim to minimize the total time to delivery for the individual application. The total time to delivery includes the time for program execution, batch queue waiting, input/output data transfer, executable staging, etc. Main features include support for mak-ing advance reservations, to make resource selections based on computer benchmark results and network performance predictions, and to enable a basic adaptation facility.
432|Aneka: Next-Generation Enterprise Grid Platform for e-Science and e-Business|In this paper, we present the design of Aneka, a.NET based service-oriented platform for desktop grid computing that provides: (i) a configurable service container hosting pluggable services for discovering, scheduling and balancing various types of workloads and (ii) a flexible and extensible framework/API supporting various programming models including threading, batch processing, MPI and dataflow. Users and developers can easily use different programming models and the services provided by the container to run their applications over desktop Grids managed by Aneka. We present the implementation of both the essential and advanced services within the platform. We evaluate the system with applications using the grid task and dataflow models on top of the infrastructure and conclude with some future directions of the current system. 1.
433|MetaCDN: Harnessing ‘Storage Clouds ’ for high performance content delivery |Content Delivery Networks (CDNs) such as Akamai and Mirror Image place web server clusters in numerous geographical locations to improve the responsiveness and locality of the content it hosts for end-users. However, their services are priced out of reach for all but the largest enterprise customers. An alternative approach to content delivery could be achieved by leveraging existing infrastructure provided by ‘Storage Cloud ’ providers, who offer internet accessible data storage and delivery at a fraction of the cost. In this paper, we introduce MetaCDN, a system that exploits ‘Storage Cloud ’ resources, creating an integrated overlay network that provides a low cost, high performance CDN for content creators. MetaCDN removes the complexity of dealing with multiple storage providers, by intelligently matching and placing users ’ content onto one or many storage providers based on their quality of service, coverage and budget preferences. MetaCDN makes it trivial for content creators and consumers to harness the performance and coverage of numerous ‘Storage Clouds ’ by providing a single unified namespace that makes it easy to integrate into origin websites, and is transparent for end-users. We then demonstrate the utility of this new approach to content delivery by showing that the participating ‘Storage Clouds ’ used by MetaCDN provide high performance (in terms of throughput and response time) and reliable content delivery for content consumers. 1
434|A Negotiation Mechanism for Advance Resource Reservation using the Alternate Offers Protocol |Abstract—Service Level Agreements (SLAs) between grid users and providers have been proposed as mechanisms for ensuring that the users ’ Quality of Service (QoS) requirements are met, and that the provider is able to realise utility from its infrastructure. This paper presents a bilateral protocol for SLA negotiation using the Alternate Offers mechanism wherein a party is able to respond to an offer by modifying some of its terms to generate a counter offer. We apply this protocol to the negotiation between a resource broker and a provider for advance reservation of compute nodes, and implement and evaluate it on a real grid system. I.
435|Handling Flash Crowds from Your Garage|The garage innovator creates new web applications which may rocket to popular success – or sink when the flash crowd that arrives melts the web server. In the web context, utility computing provides a path by which the innovator can, with minimal capital, prepare for overwhelming popularity. Many components required for web computing have recently become available as utilities. We analyze the design space of building a loadbalanced system in the context of garage innovation. We present six experiments that inform this analysis by highlighting limitations of each approach. We report our experience with three services we deployed in “garage” style, and with the flash crowds that each drew. 1
436|2007a, ‘Integrated Risk Analysis for a Commercial Computing Service|Abstract. Recent technological advances in grid computing enable the virtualization and dynamic delivery of computing services on demand to realize utility computing. In utility computing, computing services will always be available to the users whenever the need arises, similar to the availability of real-world utilities, such as electrical power, gas, and water. With this new outsourcing service model, users are able to define their service needs through Service Level Agreements (SLAs) and only have to pay when they use the services. They do not have to invest on or maintain computing infrastructures themselves and are not constrained to specific computing service providers. Thus, a commercial computing service will face two new challenges: (i) what are the objectives or goals it needs to achieve in order to support the utility computing model, and (ii) how to evaluate whether these objectives are achieved or not. To address these two new challenges, this paper first identifies four essential objectives that are required to support the utility computing model: (i) manage wait time for SLA acceptance, (ii) meet SLA requests, (iii) ensure reliability of accepted SLA, and (iv) attain profitability. It then describes two evaluation methods that are simple and intuitive: (i) separate and (ii) integrated risk analysis to analyze the effectiveness of resource management policies in achieving the objectives. Evaluation results based on simulation successfully demonstrate the applicability of separate and integrated risk analysis to assess policies in terms of the objectives. These evaluation results which constitute an a posteriori risk analysis of policies can later be used to generate an a priori risk analysis of policies by identifying possible risks for future utility computing situations.
437|Towards a Meta-Negotiation Architecture for SLA-Aware Grid Services |In novel market-oriented resource sharing models resource consumers pay for the resource usage and expect that non-functional requirements for the application execution, termed as Quality of Service (QoS), are satisfied. QoS is negotiated between two parties following the specific negotiation protocols and is recorded using Service Level Agreements (SLAs) standard. However, most of the existing work assumes that the communication partners know about the SLA negotiation protocols and about the SLA templates before entering the negotiation. However, this is a contradictory assumption, if we consider computational Grids and novel commercially oriented Computing Clouds where consumers and providers meet each other dynamically and on demand. In this paper we present novel meta-negotiation and SLA-mapping solutions for Grid workflows bridging the gap between current QoS models and Grid workflows, one of the most successful Grid programming paradigms. We illustrate the open research issues with a real world case study. Thereafter, we present document models for the specification of meta-negotiations and SLA-mappings. We discuss the architecture for the management of meta-negotiations and SLA-mappings as well as integration of the architecture into a Grid workflow management framework.
438|Autonomic metered pricing for a utility computing service|An increasing number of providers are offering utility computing services which require users to pay only when they use. Most of these providers currently charge users for metered usage based on fixed prices. In this paper, we analyze the pros and cons of charging fixed prices as compared to variable prices. In particular, charging fixed prices do not differentiate pricing based on different user requirements. Hence, we highlight the importance of deploying an autonomic pricing mechanism that self-adjusts pricing parameters to consider both application and service requirements of users. Performance results observed in the actual implementation of an enterprise Cloud show that the autonomic pricing mechanism is able to achieve higher revenue than various other common fixed and variable pricing mechanisms.
439|Bigtable: A distributed storage system for structured data|Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.  
440|The Google File System |We have designed and implemented the Google File Sys-tem, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous dis-tributed file systems, our design has been driven by obser-vations of our application workloads and technological envi-ronment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore rad-ically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our ser-vice as well as research and development efforts that require large data sets. The largest cluster to date provides hun-dreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use. Categories and Subject Descriptors D [4]: 3—Distributed file systems
441|The ubiquitous B-tree|B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and nnplemented using B-trees This paper reviews B-trees and shows why they have been so successful It discusses the major variations of the B-tree, especially the B+-tree,
442|Parallel database systems: the future of high performance database systems|Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1.
443|Operating System Support for Planetary-Scale Network Services|PlanetLab is a geographically distributed overlay network designed to support the deployment and evaluation of planetary-scale network services. Two high-level goals shape its design. First, to enable a large research community to share the infrastructure, PlanetLab provides distributed virtualization, whereby each service runs in an isolated slice of PlanetLab’s global resources. Second, to support competition among multiple network services, PlanetLab decouples the operating system running on each node from the networkwide services that define PlanetLab, a principle referred to as unbundled management. This paper describes how Planet-Lab realizes the goals of distributed virtualization and unbundled management, with a focus on the OS running on each node. 1
444|Paxos made live: an engineering perspective|We describe our experience building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected algorithmic and engineering problems encountered, and the solutions we found for them. Our measurements indicate that we have built a competitive system. 1
445|Integrating compression and execution in column-oriented database systems|Column-oriented database system architectures invite a reevaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads. In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in roworiented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.
446|Weaving Relations for Cache Performance|Relational database systems have traditionally optimzed for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM) (a.k.a., slotted pages). Recent research, however, indicates that cache utilization and performance is becoming increasingly important on modern platforms. In this paper, we first demonstrate that in-page data placement is the key to high cache performance and that NSM exhibits low cache utilization on modern platforms. Next, we propose a new data organization model called PAX (Partition Attributes Across), that significantly improves cache performance by grouping together all values of each attribute within each page. Because PAX only affects layout inside the pages, it incurs no storage penalty and does not affect I/O behavior. According to our experimental results, when compared to NSM (a) PAX exhibits superior cache and memory bandwidth utilization, saving at least 75% of NSM&#039;s stall time due to data cache accesses, (b) range selection queries and updates on memoryresident relations execute 17-25% faster, and (c) TPC-H queries involving I/O execute 11-48% faster.
447|Data Placement in Bubba|Thus paper examines the problem of data placement in Bubba, a highly-parallel system for data-intensive applications bemg developed at MCC “Highly-parallel” lmplres that load balancmng IS a cntlcal performance issue “Data-mtenave” means data IS so large that operatrons should be executed where the data resides As a result, data placement becomes a cntlcal performance issue In general, determmmg the optimal placement of data across processmg nodes for performance IS a difficult problem We describe our heuristic approach to solvmg the data placement problem w Bubba We then present expenmental results using a specific workload to provide msrght into the problem Several researchers have argued the benefits of deelustering (1 e, spreading each base relation over many nodes) We show that as declustermg IS increased. load balancing contmues to improve However, for transactions mvolvmg complex Joins, further declusterrng reduces throughput because of communications, startup and termmatron overhead We argue that data placement, especially declustermg, m a highly-parallel system must be considered early in the design, so that mechanrsms can be included for supportmg variable declustermg, for mmtmlzmg the most significant overheads associated with large-scale declustenng, and for gathering the required statistics.
448|DB2 Parallel Edition|The rate of increase in database size and response time requirements has outpaced  advancements in processor and mass storage technology. One way to satisfy the increasing  demand for processing power and I/O bandwidth in database applications is to have a  number of processors, loosely or tightly coupled, serving database requests concurrently.  Technologies developed during the last decade have made commercial parallel database  systems a reality and these systems have made an inroad into the stronghold of traditionally  mainframe based large database applications. This paper describes the parallel database  project initiated at IBM Research at Hawthorne and the DB2/AIX-PE product based on  it.  1 Introduction  Large scale parallel processing technology has made giant strides in the past decade and there is no doubt that it has established a place for itself. However, almost all of the applications harnessing this technology are scientific or engineering applications. The lack of com...
449|Routing Techniques in Wireless Sensor Networks: A Survey|Wireless Sensor Networks (WSNs) consist of small nodes with sensing, computation, and wireless communications capabilities. Many routing, power management, and data dissemination protocols have been specifically designed for WSNs where energy awareness is an essential design issue. The focus, however, has been given to the routing protocols which might differ depending on the application and network architecture. In this paper, we present a survey of the state-of-the-art routing techniques in WSNs. We first outline the design challenges for routing protocols in WSNs followed by a comprehensive survey of different routing techniques. Overall, the routing techniques are classified into three categories based on the underlying network structure: flat, hierarchical, and location-based routing. Furthermore, these protocols can be classified into multipath-based, query-based, negotiation-based, QoS-based, and coherent-based depending on the protocol operation. We study the design tradeoffs between energy and communication overhead savings in every routing paradigm. We also highlight the advantages and performance issues of each routing technique. The paper concludes with possible future research areas. 1
450|GPSR: Greedy perimeter stateless routing for wireless networks| We present Greedy Perimeter Stateless Routing (GPSR), a novel routing protocol for wireless datagram networks that uses the positions of touters and a packer&#039;s destination to make packet forwarding decisions. GPSR makes greedy forwarding decisions using only information about a router&#039;s immediate neighbors in the network topology. When a packet reaches a region where greedy forwarding is impossible, the algorithm recovers by routing around the perimeter of the region. By keeping state only about the local topology, GPSR scales better in per-router state than shortest-path and ad-hoc routing protocols as the number of network destinations increases. Under mobility&#039;s frequent topology changes, GPSR can use local topology information to find correct new routes quickly. We describe the GPSR protocol, and use extensive simulation of mobile wireless networks to compare its performance with that of Dynamic Source Routing. Our simulations demonstrate GPSR&#039;s scalability on densely deployed wireless networks.
451|Energy-efficient communication protocol for wireless microsensor networks|Wireless distributed microsensor systems will enable the reliable monitoring of a variety of environments for both civil and military applications. In this paper, we look at communication protocols, which can have significant impact on the overall energy dissipation of these networks. Based on our findings that the conventional protocols of direct transmission, minimum-transmission-energy, multihop routing, and static clustering may not be optimal for sensor networks, we propose LEACH (Low-Energy Adaptive Clustering Hierarchy), a clustering-based protocol that utilizes randomized rotation of local cluster base stations (cluster-heads) to evenly distribute the energy load among the sensors in the network. LEACH uses localized coordination to enable scalability and robustness for dynamic networks, and incorporates data fusion into the routing protocol to reduce the amount of information that must be transmitted to the base station. Simulations show that LEACH can achieve as much as a factor of 8 reduction in energy dissipation compared with conventional routing protocols. In addition, LEACH is able to distribute energy dissipation evenly throughout the sensors, doubling the useful system lifetime for the networks we simulated. 
452|A Survey on Sensor Networks|Recent advancement in wireless communica- tions and electronics has enabled the develop- ment of low-cost sensor networks. The sensor networks can be used for various application areas (e.g., health, military, home). For different application areas, there are different technical issues that researchers are currently resolving. The current state of the art of sensor networks is captured in this article, where solutions are discussed under their related protocol stack layer sections. This article also points out the open research issues and intends to spark new interests and developments in this field.
453|SPINS: Security Protocols for Sensor Networks|As sensor networks edge closer towards wide-spread deployment, security issues become a central concern. So far, the main research focus has been on making sensor networks feasible and useful, and less emphasis was placed on security. We design a suite of security building blocks that are optimized for resource-constrained environments and wireless communication. SPINS has two secure building blocks: SNEP and TESLA. SNEP provides the following important baseline security primitives: Data con£dentiality, two-party data authentication, and data freshness. A particularly hard problem is to provide efficient broad-cast authentication, which is an important mechanism for sensor networks. TESLA is a new protocol which provides authenticated broadcast for severely resource-constrained environments. We implemented the above protocols, and show that they are practical even on minimalistic hardware: The performance of the protocol suite easily matches the data rate of our network. Additionally, we demonstrate that the suite can be used for building higher level protocols. 
454|Geography-informed Energy Conservation for Ad Hoc Routing|We introduce a geographical adaptive fidelity (GAF) algorithm that reduces energy consumption in ad hoc wireless networks. GAF conserves energy by identifying nodes that are equivalent from a routing perspective and then turning off unnecessary nodes, keeping a constant level of routing fidelity. GAF moderates this policy using application- and system-level information; nodes that source or sink data remain on and intermediate nodes monitor and balance energy use. GAF is independent of the underlying ad hoc routing protocol; we simulate GAF over unmodified AODV and DSR. Analysis and simulation studies of GAF show that it can consume 40% to 60% less energy than an unmodified ad hoc routing protocol. Moreover, simulations of GAF suggest that network lifetime increases proportionally to node density; in one example, a four-fold increase in node density leads to network lifetime increase for 3 to 6 times (depending on the mobility pattern). More generally, GAF is an example of adaptive fidelity, a technique proposed for extending the lifetime of self-configuring systems by exploiting redundancy to conserve energy while maintaining application fidelity.   
455|Secure Routing in Wireless Sensor Networks: Attacks and Countermeasures|We  consider routing  security in wireless sensor networks. Many sensor  network routing  protocols have been proposed, but none of them have been designed with security as agq1( We propose  securitygcur forrouting  in sensor networks, show how attacks agacks ad-hoc and peer-to-peer networks can be adapted into powerful attacks agacks  sensor networks, introduce two classes of novel attacks agacks sensor networks----sinkholes and HELLO floods, and analyze the security of all the major sensor  networkrouting  protocols. We describe crippling  attacks against all of them and sug@(5 countermeasures anddesig considerations. This is the first such analysis of  secure routing  in sensor networks.
456|Minimum energy mobile wireless networks| We describe a distributed position-based network protocol optimized for minimum energy consumption in mobile wireless networks that support peer-to-peer communications. Given any number of randomly deployed nodes over an area, we illustrate that a simple local optimization scheme executed at each node guarantees strong connectivity of the entire network and attains the global minimum energy solution for stationary networks. Due to its localized nature, this protocol proves to be self-reconfiguring and stays close to the minimum energy solution when applied to mobile networks. Simulation results are used to verify the performance of the protocol. 
457|Protocols for self-organization of a wireless sensor network|We present a suite of algorithms for self-organization of wireless sensor networks, in which there is a scalably large number of mainly static nodes with highly constrained energy resources. The protocols further support slow mobility by a subset of the nodes, energy-efficient routing, and formation of ad hoc subnetworks for carrying out cooperative signal processing functions among a set of the nodes.
458|Energy Aware Routing for Low Energy Ad Hoc Sensor Networks|The recent interest in sensor networks has led to a number of routing schemes that use the limited resources available at sensor nodes more efficiently. These schemes typically try to find the minimum energy path to optimize energy usage at a node. In this paper we take the view that always using lowest energy paths may not be optimal from the point of view of network lifetime and long-term connectivity. To optimize these measures, we propose a new scheme called energy aware routing that uses sub-optimal paths occasionally to provide substantial gains. Simulation results are also presented that show increase in network lifetimes of up to 40% over comparable schemes like directed diffusion routing. Nodes also burn energy in a more equitable way across the network ensuring a more graceful degradation of service with time.
459|Rumor Routing Algorithm for Sensor Networks|Advances in micro-sensor and radio technology will enable small but smart sensors to be deployed for a wide range of environmental monitoring applications. In order to constrain communication overhead, dense sensor networks call for new and highly efficient methods for distributing queries to nodes that have observed interesting events in the network. A highly efficient data-centric routing mechanism will offer significant power cost reductions [17], and improve network longevity. Moreover, because of the large amount of system and data redundancy possible, data becomes disassociated from specific node and resides in regions of the network [10][7][8]. This paper describes and evaluates through simulation a scheme we call Rumor Routing, which allows for queries to be delivered to events in the network. Rumor Routing is tunable, and allows for tradeoffs between setup overhead and delivery reliability. It&#039;s intended for contexts in which geographic routing criteria are not applicable because a coordinate system is not available or the phenomenon of interest is not geographically correlated.
460|An Energy Efficient Hierarchical Clustering Algorithm for Wireless Sensor Networks|A wireless network consisting of a large number of small sensors with low-power transceivers can be an effective tool for gathering data in a variety of environments. The data collected by each sensor is communicated through the network to a single processing center that uses all reported data to determine characteristics of the environment or detect an event. The communication or message passing process must be designed to conserve the Hmited energy resources of the sensors. Clustering sensors into groups, so that sensors communicate information only to clusterheads and then the clusterheads communicate the aggregated information to the processing center, may save energy. In this paper, we propose a distributed, randomized clustering algorithm to organize the sensors in a wireless sensor network into clusters. We then extend this algorithm to generate a hierarchy of clusterheads and observe that the energy savings increase with the number of levels in the hierarchy. Results in stochastic geometry are used to derive solutions for the values of parameters of our algorithm that minimize the total energy spent in the network when all sensors report data through the clusterheads to the processing center.
461|SPEED: A Stateless Protocol for Real-Time Communication In Sensor Networks|In this paper, we present a real-time communication protocol for sensor networks, called SPEED. The protocol provides three types of real-time communication services, namely, real-time unicast, real-time area-multicast and real-time area-anycast. SPEED is specifically tailored to be a stateless, localized algorithm with minimal control overhead End-to-end soft real-time communication is achieved by maintaining a desired delivery speed across the sensor network through a novel combination of feedback control and non-deterministic geographic forwarding. SPEED is a highly efficient and scalable protocol for sensor networks where the resources of each node are scarce. Theoretical analysis, simulation experiments and a real implementation on Berkeley motes are provided to validate our claims.
462|Scalable information-driven sensor querying and routing for ad hoc heterogeneous sensor networks|This paper describes two novel techniques, informationdriven sensor querying (IDSQ) and constrained anisotropic diffusion routing (CADR), for energy-efficient data querying and routing in ad hoc sensor networks for a range of collaborative signal processing tasks. The key idea is to introduce an information utility measure to select which sensors to query and to dynamically guide data routing. This allows us to maximize information gain while minimizing detection latency and bandwidth consumption for tasks such as localization and tracking. Our simulation results have demonstrated that the information-driven querying and routing techniques are more energy efficient, have lower detection latency, and provide anytime algorithms to mitigate risks of link/node failures. 1
463|Negotiation-based Protocols for Disseminating Information in Wireless Sensor Networks|Abstract. In this paper, we present a family of adaptive protocols, called SPIN (Sensor Protocols for Information via Negotiation), that efficiently disseminate information among sensors in an energy-constrained wireless sensor network. Nodes running a SPIN communication protocol name their data using high-level data descriptors, called meta-data. They use meta-data negotiations to eliminate the transmission of redundant data throughout the network. In addition, SPIN nodes can base their communication decisions both upon application-specific knowledge of the data and upon knowledge of the resources that are available to them. This allows the sensors to efficiently distribute data given a limited energy supply. We simulate and analyze the performance of four specific SPIN protocols: SPIN-PP and SPIN-EC, which are optimized for a point-to-point network, and SPIN-BC and SPIN-RL, which are optimized for a broadcast network. Comparing the SPIN protocols to other possible approaches, we find that the SPIN protocols can deliver 60 % more data for a given amount of energy than conventional approaches in a point-to-point network and 80 % more data for a given amount of energy in a broadcast network. We also find that, in terms of dissemination rate and energy usage, the SPIN protocols perform close to the theoretical optimum in both point-to-point and broadcast networks.
464|Maximum Lifetime Routing In Wireless Sensor Networks|Routing in power-controlled wireless sensor networks is formulated as an optimization problem with the goal of maximizing the system lifetime. Considering that the information is delivered in the form of packets, we identified the problem as an integer programming problem. It is known that the system lifetime can be significantly extended by using a link metric that utilizes the information about the residual energy of the sensor nodes as well as the energy expenditure in transmission of a unit information over the wireless links. In this paper, some of the routing algorithms are proposed and examined in order to find the best link cost function and the method of shortest path calculation. The performance comparison is made through simulation in a typical battlefield scenario where sensors detecting a moving target vehicle periodically send a reporting packet to one of the gateway nodes. The results are also compared with the optimal solution obtained by the linear programming relaxation of the problem, which showed close-to-optimal performance.
465|A Taxonomy of Wireless Micro-Sensor Network Models|... This paper examines this emerging field to classify wireless micro-sensor networks according to different communication functions, data delivery models, and network dynamics. This taxonomy will aid in defining appropriate communication infrastructures for different sensor network application sub-spaces, allowing network designers to choose the protocol architecture that best matches the goals of their application. In addition, this taxonomy will enable new sensor network models to be defined for use in further research in this area.
466|Worst-Case Optimal and Average-Case Efficient Geometric Ad-Hoc Routing|In this paper we present GOAFR, a new geometric ad-hoc routing algorithm combining greedy and face routing. We evaluate this algorithm by both rigorous analysis and comprehensive simulation. GOAFR is the first ad-hoc algorithm to be both asymptotically optimal and average-case e#cient. For our simulations we identify a network density range critical for any routing algorithm. We study a dozen of routing algorithms and show that GOAFR outperforms other prominent algorithms, such as GPSR or AFR.
467|APTEEN: A hybrid protocol for efficient routing and comprehensive information retrieval in wireless sensor networks|Wireless sensor networks with thousands of tiny sensor nodes, are expected to find wide applicability and increas-ing deployment in coming years, as they enable reliable monitoring and analysis of the environment. In this paper, we propose a hybrid routing protocol (APTEEN) which al-lows for comprehensive information retrieval. The nodes in such a network not only react to time-critical situations, but also give an overall picture of the network at periodic in-tervals in a very energy efficient manner. Such a network enables the user to request past, present and future data from the network in the form of historical, one-time and per-sistent queries respectively. We evaluated the performance of these protocols and observe that these protocols are ob-served to outperform existing protocols in terms of energy consumption and longevity of the network. 1.
468|A Scalable Solution to Minimum Cost Forwarding in Large Sensor|Wireless sensor networks offer a wide range of challenges to networking research, including unconstrained network scale, limited computing, memory and energy resources, and wireless channel errors. In this paper, we study the problem of delivering messages from any sensor to an interested client user along the minimum-cost path in a large sensor network. We propose a new cost field based approach to minimum cost forwarding. In the design, we present a novel backoff-based cost field setup algorithm that finds the optimal costs of all nodes to the sink with one single message overhead at each node. Once the field is established, the message, carrying dynamic cost information, flows along the minimum cost path in the cost field. Each intermediate node forwards the message only if it finds itself to be on the optimal path, based on dynamic cost states. Our design does not require an intermediate node to maintain explicit &#034;forwarding path&#034; states. It requires a few simple operations and scales to any network size. We show the correctness and effectiveness of the design by both simulations and analysis.
469|Scalable coordination for wireless sensor networks: self-configuring localization systems|Pervasive networks of micro-sensors and actuators offer to revolutionize the ways in which we understand and construct complex physical systems. Sensor networks must be scalable, long-lived and robust systems, overcoming energy limitations and a lack of pre-installed infrastructure. We explore three themes in the design of self-configuring sensor networks: tuning density to trade operational quality against lifetime; using multiple sensor modalities to obtain robust measurements; and exploiting fixed environmental characteristics. We illustrate these themes through the problem of localization, which is a key building block for sensor systems that itself requires coordination.
470|Constrained Random Walks on Random Graphs: Routing Algorithms for Large Scale Wireless Sensor Networks|We consider a routing problem in the context of large scale networks with uncontrolled dynamics. A case of uncontrolled dynamics that has been studied extensively is that of mobile nodes, as this is typically the case in cellular and mobile ad-hoc networks. In this paper however we study routing in the presence of a different type of dynamics: nodes do not move, but instead switch between active and inactive states at random times. Our interest in this case is motivated by the behavior of sensor nodes powered by renewable sources, such as solar cells or ambient vibrations. In this paper we formalize the corresponding routing problem as a problem of constructing suitably constrained random walks on random dynamic graphs. We argue that these random walks should be designed so that their resulting invariant distribution achieves a certain load balancing property, and we give simple distributed algorithms to compute the local parameters for the random walks that achieve the sought behavior. A truly novel feature of our formulation is that the algorithms we obtain are able to route messages along all possible routes between a source and a destination node, without performing explicit route discovery/repair computations, and without maintaining explicit state information about available routes at the nodes. To the best of our knowledge, these are the first algorithms that achieve true multipath routing (in a statistical sense), at the complexity of simple stateless operations.
471|The ACQUIRE Mechanism for Efficient Querying in Sensor Networks|We propose a novel and efficient mechanism for obtaining information in sensor networks which we refer to as ACQUIRE. In ACQUIRE an active query is forwarded through the network, and intermediate nodes use cached local information (within a look-ahead of d hops) in order to partially resolve the query. When the query is fully resolved, a completed response is sent directly back to the querying node. We take a...
472|Lightweight sensing and communication protocols for target enumeration and aggregation|The development of lightweight sensing and communication protocols is a key requirement for designing resource constrained sensor networks. This paper introduces a set of efficient protocols and algorithms, DAM, EBAM, and EMLAM, for constructing and maintaining sensor aggregates that collectively monitor target activity in the environment. A sensor aggregate comprises those nodes in a network that satisfy a grouping predicate for a collaborative processing task. The parameters of the predicate depend on the task and its resource requirements. Since the foremost purpose of a sensor network is to selectively gather information about the environment, the formation of appropriate sensor aggregates is crucial for optimally allocating resources to sensing and communication tasks. This paper makes minimal assumptions about node onboard processing and communication capabilities so as to allow possible implementations on resource-constrained hardware. Factors affecting protocol performance are discussed. The paper presents simulation results showing how the protocol performance varies as key network and task parameters are varied. It also provides probabilistic analyses of network behavior consistent with the simulation results. The protocols have been experimentally validated on a sensor network testbed comprising 25 Berkeley MICA sensor motes.
473|Trade-Off between Traffic Overhead and Reliability in Multipath Routing for Wireless Sensor Networks|In wireless sensor networks (WSN) data produced by one or more sources usually has to be routed through several intermediate nodes to reach the destination. Problems arise when intermediate nodes fail to forward the incoming messages. The reliability of the system can be increased by providing several paths from source to destination and sending the same packet through each of them (the algorithm is known as multipath routing). Using this technique, the traffic increases significantly. In this paper, we analyze a new mechanism that enables the tradeoff between the amount of traffic and the reliability. The data packet is split in k subpackets (k = number of disjoined paths from source to destination). If only Ek subpackets (Ek &lt;k)are necessary to rebuild the original data packet (condition obtained by adding redundancy to each subpacket), then the trade-off between traffic and reliability can be controlled.
474|Hierarchical Power-aware Routing in Sensor Networks |This paper discusses online power-aware routing in large sensor networks. We seek to optimize the lifetime of the network. We develop an approximation algorithm called max-min zPmin that has a good empirical competitive ratio. To ensure scalability, we introduce a hierarchical algorithm, which is called zone-based routing.  
475|A high-performance, portable implementation of the MPI message passing interface standard|MPI (Message Passing Interface) is a specification for a standard library for message passing that was defined by the MPI Forum, a broadly based group of parallel computer vendors, library writers, and applications specialists. Multiple implementations of MPI have been developed. In this paper, we describe MPICH, unique among existing implementations in its design goal of combining portability with high performance. We document its portability and performance and describe the architecture by which these features are simultaneously achieved. We also discuss the set of tools that accompany the free distribution of MPICH, which constitute the beginnings of a portable parallel programming environment. A project of this scope inevitably imparts lessons about parallel computing, the specification being followed, the current hardware and software environment for parallel computing, and project management; we describe those we have learned. Finally, we discuss future developments for MPICH, including those necessary to accommodate extensions to the MPI Standard now being contemplated by the MPI Forum. 1
476|CoCheck: Checkpointing and Process Migration for MPI|Checkpointing of parallel applications can be used as the core technology to provide process migration. Both, checkpointing and migration, are an important issue for parallel applications on networks of workstations. The CoCheck environment which we present in this paper introduces a new approach to provide checkpointing and migration for parallel applications. In difference to existing systems CoCheck rather sits on top of the message passing library than inside and achieves consistency at a level above the message passing system. It uses an existing single process checkpointer which is available for a wide range of systems. Hence, CoCheck can be easily adapted to both, different message passing systems and new machines. 
477|User&#039;s Guide for mpich, a Portable Implementation of MPI Version 1.2.1|1 1 Introduction 2 2 Linking and running programs 2 2.1 Scripts to Compile and Link Applications . . . . . . . . . . . . . . . . . . . 3 2.1.1 Fortran 90 and the MPI module . . . . . . . . . . . . . . . . . . . . 4 2.2 Compiling and Linking without the Scripts . . . . . . . . . . . . . . . . . . 4 2.3 Running with mpirun . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3.1 SMP Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3.2 Multiple Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.4 More detailed control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3 Special features of different systems 6 3.1 Workstation clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.1.1 Checking your machines list . . . . . . . . . . . . . . . . . . . . . . . 7 3.1.2 Using the Secure Shell . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.1.3 Using the Secure Server . . . . . . . . . . . . . . . . ....
478|Monitors, Messages, and Clusters: the p4 Parallel Programming System |p4 is a portable library of C and Fortran subroutines for programming parallel computers. It is the current version of a system that has been in use since 1984. It includes features for explicit parallel programming of shared-memory machines, distributed-memory machines (including heterogeneous networks of workstations), and clusters, by which we mean sharedmemory multiprocessors communicating via message passing. We discuss here the design goals, history, and system architecture of p4 and describe briefly a diverse collection of applications that have demonstrated the utility of p4. 1 Introduction  p4 is a library of routines designed to express a wide variety of parallel algorithms portably, efficiently and simply. The goal of portability requires it to use widely accepted models of computation rather than specific vendor implementations of those models. The goal of efficiency requires it to use models of computation relatively close to those provided by the machines themselves and t...
479|The Nexus Task-parallel Runtime System|A runtime system provides a parallel language compiler with an interface to the low-level facilities required to support interaction between concurrently executing program components. Nexus is a portable runtime system for task-parallel programming languages. Distinguishing features of Nexus include its support for multiple threads of control, dynamic processor acquisition, dynamic address space creation, a global memory model via interprocessor references, and asynchronous events. In addition, it supports heterogeneity at multiple levels, allowing a single computation to utilize different programming languages, executables, processors, and network protocols. Nexus is currently being used as a compiler target for two task-parallel languages: Fortran M and Compositional C++ . In this paper, we present the Nexus design, outline techniques used to implement Nexus on parallel computers, showhow it is used in compilers, and compare its performance with that of another runtime system.  
480|Resource Management and Checkpointing for PVM|Checkpoints cannot only be used to increase fault tolerance, but also to migrate  processes. The migration is particularly useful in workstation environments  where machines become dynamically available and unavailable. We introduce the  CoCheck environment which not only allows the creation of checkpoints, but also  provides process migration. The creation of checkpoints of PVM applications is  explained and we show how this service can be used in a resource manager.  
481|The Design and Evolution of Zipcode|Zipcode is a message-passing and process-management system that was designed for multicomputers and homogeneous networks of computers in order to support libraries and large-scale multicomputer software. The system has evolved significantly over the last five years, based on our experiences and identified needs. Features of Zipcode that were originally unique to it, were its simultaneous support of static process groups, communication contexts, and virtual topologies, forming the &#034;mailer&#034; data structure. Point-to-point and collective operations reference the underlying group, and use contexts to avoid mixing up messages. Recently, we have added &#034;gather-send&#034; and &#034;receive-scatter&#034; semantics, based on persistent Zipcode &#034;invoices,&#034; both as a means to simplify message passing, and as a means to reveal more potential runtime optimizations. Key features in Zipcode appear in the forthcoming MPI standard.  Keywords: Static Process Groups, Contexts, Virtual Topologies, Point-to-Point Communica...
482| Performance Analysis of MPI Programs |The Message Passing Interface (MPI) standard has recently been completed. MPI  is a specification for a library of functions that implement the message-passing model of  parallel computation. One novel feature of MPI is its very general &#034;profiling interface,&#034;  that allows users to attach assorted profiling tools to the MPI library even though they  do not have access to the MPI source code. We describe the MPI profiling interface  and describe three profiling libraries that make use of it. These libraries are distributed  with the portable, publicly available implementation of MPI.   
484|Migrating from PVM to MPI, part I: The Unify System |This paper presents a new kind of portability system, Unify, which modifies the PVM message passing system to provide (currrently a subset of) the Message Passing Interface (MPI) standard notation for message passing. Unify is designed to reduce the effort of learning MPI while providing a sensible means to make use of MPI libraries and MPI calls while applications continue to run in the PVM environment. We are convinced that this strategy will reduce the costs of porting completely to MPI, while providing a gradual environment within which to evolve. Furthermore, it will permit immediate use of MPI-based parallel libraries in applications, even those that use PVM for user code. We describe several paradigms for supporting MPI and PVM message passing notations in a single environment, and note related work on MPI and PVM implementations. We show the design options that existed within our chosen paradigm (which is an MPI interface added to the base PVM system), and why we chose that par...
485|Reuse, Portability and Parallel Libraries|Parallel programs are typically written in an explicitly parallel fashion using either message passing or shared memory primitives. Message passing is attractive for performance and portability since shared memory machines can efficiently execute message passing programs, however message passing machines cannot in general effectively execute shared memory programs. In order to write a parallel program using message passing, the programmer is often obliged to develop a significant amount of code which manages distributed data and events and parallel input/output, and such code may have little or nothing to do with the application. However many parallel applications have common structural elements and much of this additional code can be encapsulated within a parallel library and reused in several programs. We discuss the requirements the library writer and user makes of the basic message passing interface and describe how we have addressed these requirements in our Common High-Level Inte...
486|Bursty and Hierarchical Structure in Streams|A fundamental problem in text data mining is to extract meaningful structure from document streams that arrive continuously over time. E-mail and news articles are two natural examples of such streams, each characterized by topics that appear, grow in intensity for a period of time, and then fade away. The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale. Underlying much of the text mining work in this area is the following intuitive premise --- that the appearance of a topic in a document stream is signaled by a &#034;burst of activity,&#034; with certain features rising sharply in frequency as the topic emerges.
487|A tutorial on hidden Markov models and selected applications in speech recognition|Although initially introduced and studied in the late 1960s and early 1970s, statistical methods of Markov source or hidden Markov modeling have become increasingly popular in the last several years. There are two strong reasons why this has occurred. First the models are very rich in mathematical structure and hence can form the theoretical basis for use in a wide range of applications. Sec-ond the models, when applied properly, work very well in practice for several important applications. In this paper we attempt to care-fully and methodically review the theoretical aspects of this type of statistical modeling and show how they have been applied to selected problems in machine recognition of speech.  
488|Mining Sequential Patterns|We are given a large database of customer transactions, where each transaction consists of customer-id, transaction time, and the items bought in the transaction. We introduce the problem of mining sequential patterns over such databases. We present three algorithms to solve this problem, and empirically evaluate their performance using synthetic data. Two of the proposed algorithms, AprioriSome and AprioriAll, have comparable performance, albeit AprioriSome performs a little better when the minimum number of customers that must support a sequential pattern is low. Scale-up experiments show that both AprioriSome and AprioriAll scale linearly with the number of customer transactions. They also have excellent scale-up properties with respect to the number of transactions per customer and the number of items in a transaction.  
489|ATTENTION,  INTENTIONS,  AND THE STRUCTURE OF DISCOURSE|In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse. In this theory, discourse structure is composed of three separate but interre-lated components: the structure of the sequence of utterances (called the linguistic structure), a struc-ture of purposes (called the intentional structure), and the state of focus of attention (called the attentional state). The linguistic structure consists of segments of the discourse into which the utter-ances naturally aggregate. The intentional structure captures the discourse-relevant purposes, expressed in each of the linguistic segments as well as relationships among them. The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds. The attentional state, being dynamic, records the objects, properties, and relations that are salient at each point of the discourse. The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases, referring expressions, and interruptions. The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses. Various properties of discourse are described, and explanations for the behavior of cue phrases, referring expressions, and interruptions are explored. This theory provides a framework for describing the processing of utterances in a discourse. Discourse processing requires recognizing how the utterances of the discourse aggregate into segments, recognizing the intentions expressed in the discourse and the relationships among intentions, and track-ing the discourse through the operation of the mechanisms associated with attentional state. This processing description specifies in these recognition tasks the role of information from the discourse and from the participants &#039; knowledge of the domain. 1
490|A Bayesian approach to filtering junk E-mail|In addressing the growing problem of junk E-mail on the Internet, we examine methods for the automated construction of filters to eliminate such unwanted messages from a user’s mail stream. By casting this problem in a decision theoretic framework, we are able to make use of probabilistic learning methods in conjunction with a notion of differential misclassification cost to produce filters Which are especially appropriate for the nuances of this task. While this may appear, at first, to be a straight-forward text classification problem, we show that by considering domain-specific features of this problem in addition to the raw text of E-mail messages, we can produce much more accurate filters. Finally, we show the efficacy of such filters in a real world usage scenario, arguing that this technology is mature enough for deployment.
491|Email overload: exploring personal information management of email|Email is one of the most successful computer applications yet devised. Our empirical data show however, that although email was origirally designed as a communications application, it is now used for additional functions, that it was not designed for, such as task management and personal archiving. We call this email overload. We demonstrate that email overload creates problems for personal information management: users often have cluttered inboxes containing hundreds of messages, including outstanding tasks, partially read documents and conversational threads. Furthermore, user attemtps to rationalise their inboxes by filing are often unsuccessful, with the consequence that important messages get overlooked, or &#034;lost&#034; in archives. We explain how email overloading arises and propose technical solutions to the problem.
492|Principles of Mixed-Initiative User Interfaces|Recent debate has centered on the relative promise of focusing user-interface research on developing new  metaphors and tools that enhance users&#039; abilities to directly manipulate objects versus directing effort toward developing interface agents that provide automation. In this paper, we review principles that show promise for allowing engineers to enhance human---computer interaction through an elegant coupling of automated services with direct manipulation. Key ideas will be highlighted in terms of the LookOut system for scheduling and meeting management.  Keywords  Intelligent agents, direct manipulation, user modeling, probability, decision theory, UI design  INTRODUCTION  There has been debate among researchers about where great opportunities lay for innovating in the realm of human--- computer interaction [10]. One group of researchers has expressed enthusiasm for the development and application of new kinds of automated services, often referred to as interface &#034;agents.&#034; The effo...
493|The Hierarchical Hidden Markov Model: Analysis and Applications|. We introduce, analyze and demonstrate a recursive hierarchical generalization of the widely used hidden Markov models, which we name Hierarchical Hidden Markov Models (HHMM). Our model is motivated by the complex multi-scale structure which appears in many natural sequences, particularly in language, handwriting and speech. We seek a systematic unsupervised approach to the modeling of such structures. By extendingthe standard forward-backward(BaumWelch) algorithm, we derive an efficient procedure for estimating the model parameters from unlabeled data. We then use the trained model for automatic hierarchical parsing of observation sequences. We describe two applications of our model and its parameter estimation procedure. In the first application we show how to construct hierarchical models of natural English text. In these models different levels of the hierarchy correspond to structures on different length scales in the text. In the second application we demonstrate how HHMMs can b...
494|On-line New Event Detection and Tracking| We define and describe the related problems of new event detection and event tracking within a stream of broadcast news stories. We focus on a strict on-line setting-i.e., the system must make decisions about one story before looking at any subsequent stories. Our approach to detection uses a single pass clustering algorithm and a novel thresholding model that incorporates the properties of events as a major component. Our ap-proach to tracking is similar to typical information filtering methods. We discuss the value of “surprising” features that have unusual occurrence characteristics, and briefly explore on-line adaptive filtering to handle evolving events in the news. New event detection and event tracking are part of the Topic Detection and Tracking (TDT) initiative. 
495|Learning Rules that Classify E-Mail|wcohen~research.att.com Two methods for learning text classifiers are compared on classification problems that might arise in filtering and filing personM e-mail messages: a &#034;traxiitionM IR &#034; method based on TF-IDF weighting, and a new method for learning sets of &#034;keyword-spotting rules &#034; based on the RIPPER rule learning algorithm. It is demonstrated that both methods obtain significant generalizations from a small number of examples; that both methods are comparable in generalization performance on problems of this type; and that both methods axe reasonably efficient, even with fairly large training sets. However, the greater comprehensibility of the rules may be advantageous in a system that allows users to extend or otherwise modify a learned classifier.
496|SwiftFile: An Intelligent Assistant for Organizing E-Mail|While most e-mail clients allow users to file messages  into folders, the process they must go through to file  each message is often tedious and slow. For each message,  the user must first decide which folder is most  appropriate. Then, the user must inform the e-mail  reader of that choice by selecting the appropriate icon  or menu item from among what is typically a set of several  dozen choices. The combined effort of choosing a  folder and conveying that choice to the application often  discourages users from filing their mail, resulting in  unmanageable inboxes that contain hundreds or even  thousands of unfiled messages. SwiftFile encourages  users to file their mail by simplifying the task. Using  an adaptive classifier, it predicts the three folders that  are most likely to be appropriate for a given message  and provides shortcut buttons that permit the user to  effortlessly file it into a predicted folder. For typical  users, SwiftFile&#039;s predictions are accurate over 80% to...
497|ThemeRiver: Visualizing Theme Changes over Time|ThemeRiver ™ is a prototype system that visualizes thematic variations over time within a large collection of documents. The “river ” flows from left to right through time, changing width to depict changes in thematic strength of temporally associated documents. Colored “currents ” flowing within the river narrow or widen to indicate decreases or increases in the strength of an individual topic or a group of topics in the associated documents. The river is shown within the context of a timeline and a corresponding textual presentation of external events.
498|Linear Time Inference in Hierarchical HMMs|The hierarchical hidden Markov model (HHMM) is a generalization of  the hidden Markov model (HMM) that models sequences with structure  at many length/time scales [FST98]. Unfortunately, the original inference  algorithm is rather complicated, and takes O(T    ) time, where T is  the length of the sequence, making it impractical for many domains. In  this paper, we show how HHMMs are a special kind of dynamic Bayesian  network (DBN), and thereby derive a much simpler inference algorithm,  which only takes O(T ) time. Furthermore, by drawing the connection  between HHMMs and DBNs, we enable the application of many standard  approximation techniques to further speed up inference.
499|Concept Features in Re:Agent, an Intelligent Email Agent|An important issue in the application of machine learning techniques to information management tasks is the nature of features extracted from textual information. We have created an intelligent email agent that can learn actions such as filtering, prioritizing, downloading to palmtops, and forwarding email to voicemail using automatic feature extraction. Our agent&#039;s newfeature extraction approach is based on first learning concepts present within the mail, then using these concepts as features for learning actions to perform on the messages. What features should be chosen? This paper describes the concept features approach and considers two sources for learning conceptual features: groups defined by the user and groups defined by the agent&#039;s task. Additionally, features may be defined by vectorized examples or keywords. Experimental results are provided for an email sorting task. Keywords Electronic mail, intelligent agents, machine learning, information management, feature selection...
500|Interface Agents that Learn: An Investigation of Learning Issues in a Mail Agent Interface|In recent years, interface agents have been developed to assist users with various tasks. Some systems employ machine learning techniques to allow the agent to adapt to the user&#039;s changing requirements. With the increase in the volume of data on the Internet, agents have emerged which are able to monitor and learn from their users to identify topics of interest. One such agent, described here, has been developed to filter mail messages. We examine the issues involved in constructing an autonomous interface agent which employs a learning component, and explore the use of two different learning techniques in this context. Submitted to Applied Artificial Intelligence Journal. October 26, 1 INTRODUCTION 1 1 Introduction Agents were once seen as anthropomorphic entities which would assist users with daily tasks. They could be used, for example, to locate information of interest to their user (Kay 1984). Ten years later, many definitions of agents have been proposed. The basic concept of ...
501|ifile: An Application of Machine Learning to E-Mail Filtering|The rise of the World Wide Web and the ever-increasing amounts of machine-readable text has caused text classification to become a important aspect of machine learning. One specific application that has the potential to affect almost every user of the Internet is e-mail filtering. The WorldTalk Corporation estimates that over 60 million business people use e-mail [6]. Many more use e-mail purely on a personal basis and the pool of e-mail users is growing daily. And yet, automated techniques for learning to filter e-mail have yet to significantly affect the e-mail market. Here, I attack problems that plague practical e-mail ltering and suggest solutions that will bring us closer to the acceptance of using automated classification techniques to filter personal e-mail. I also present a filtering system, ifile, that is both effective and efficient, and which has been adapted to a popular e-mail client. Results are presented from a number of experiments and show that a system such as ifile could become a...
502|Extracting Significant Time Varying Features from Text|We propose a simple statistical model for the frequency of occurrence of features in a stream of text. Adoption of this model allows us to use classical significance tests to filter the stream for interesting events. We tested the model by building a system and running it on a news corpus. By a subjective evaluation, the system worked remarkably well: almost all of the groups of identified tokens corresponded to news stories and were appropriately placed in time. A preliminary objective evaluation was also used to measure the quality of the system and it showed some of the weaknesses and the power of our approach.  1 Introduction  We are interested in information organization and exploration for supporting human decision making. Much information comes in the form of streams, where a stream is a collection of tokens arriving in a fixed order, with each token having a time stamp. Examples of data that are in the form of streams are e-mail, Usenet postings, news corpora, financial and cor...
503|Improving Text Categorization Methods for Event Tracking|Automated tracking of events from chronologically ordered document streams is a new challenge for statistical text classification. Existing learning techniques must be adapted or improved in order to effectively handle difficult situations where the number of positive training instances per event is extremely small, the majority of training documents are unlabelled, and most of the events have a short duration in time. We adapted several supervised text categorization methods, specifically several new variants of the k-Nearest Neighbor (kNN) algorithm and a Rocchio approach, to track events. All of these methods showed significant improvement (up to 71% reduction in weighted error rates) over the performance of the original kNN algorithm on TDT benchmark collections, making kNN among the top-performing systems in the recent TDT3 official evaluation. Furthermore, by combining these methods, we significantly reduced the variance in performance of our event tracking system over different ...
504|Topic Islands - A Wavelet-Based Text Visualization System|We present a novel approach to visualize and explore unstructured text. The underlying technology, called TOPIC-O-GRAPHY  TM  , applies wavelet transforms to a custom digital signal constructed from words within a document. The resultant multiresolution wavelet energy is used to analyze the characteristics of the narrative flow in the frequency domain, such as theme changes, which is then related to the overall thematic content of the text document using statistical methods. The thematic characteristics of a document can be analyzed at varying degrees of detail, ranging from section-sized text partitions to partitions consisting of a few words. Using this technology, we are developing a visualization system prototype known as TOPIC ISLANDS  TM  to browse a document, generate fuzzy document outlines, summarize text by levels of detail and according to user interests, define meaningful subdocuments, query text content, and provide summaries of topic evolution.  Keywords: text visualizati...
505|Visualizing Sequential Patterns for Text Mining|A sequential pattern in data mining is a finite series of elements such as A  B  C  D where A, B, C, and D are elements of the same domain. The mining of sequential patterns is designed to find patterns of discrete events that frequently happen in the same arrangement along a timeline. Like association and clustering, the mining of sequential patterns is among the most popular knowledge discovery techniques that apply statistical measures to extract useful information from large datasets. As our computers become more powerful, we are able to mine bigger datasets and obtain hundreds of thousands of sequential patterns in full detail. With this vast amount of data, we argue that neither data mining nor visualization by itself can manage the information and reflect the knowledge effectively. Subsequently, we apply visualization to augment data mining in a study of sequential patterns in large text corpora. The result shows that we can learn more and more quickly in an integrated visual...
506|Topic Detection and Tracking Pilot Study|Topic Detection and Tracking (TDT) is a DARPA-sponsored initiative to investigate the state of the art in finding and following new events in a stream of broadcast news stories. The TDT problem consists of three major tasks: (1) segmenting a stream of data, especially recognized speech, into distinct stories; (2) identifying those news stories that are the first to discuss a new event occurring in the news; and (3) given a small number of sample news stories about an event, finding all following stories in the stream. The TDT Pilot Study ran from September 1996 through October 1997. The primary participants were DARPA, Carnegie Mellon University, Dragon Systems, and the University of Massachusetts at Amherst. This report summarizes the findings of the pilot study. The TDT work continues in a new project involving larger training and test corpora, more active participants, and a more broadly defined notion of &#034;topic&#034; than was used in the pilot study. The following individuals participat...
507|Ishmail: Immediate Identification of Important Information|This paper describes Ishmail, a program designed for people who get a lot of electronic mail. Most email programs do not address the main problem experienced by people who get a lot of email: information overload. Given a deluge of email, how does one maintain control over incoming message traffic and reduce the time required to find important messages? Some email programs support classification of messages into separate mailboxes, but this is only a partial solution. Ishmail is unique in that it not only sorts messages into mailboxes, but it orders mailboxes by a combination of user-specified priorities and alarms. While most mail programs only alert users about unread messages, Ishmail supports independent alarms on each mailbox with customizable thresholds and filters. Users control their alarms, mailboxes, and messages through customizable summaries that act as both views and interactive controls. Three additional unique features of Ishmail are 1) the ability to read messages safel...
508|Mail-by-Example: A visual query interface for managing large volumes of electronic messages|Email is a rich source of quality and up-to-date information, and users need advanced facilities to store, organize and retrieve information in large volumes of electronic messages. In this paper, we present MBE (Mail-by-Example), a visual interface integrated to the Lotus Notes email environment that enables users to define ad hoc queries for retrieving messages, folders, or information about them. MBE is based on a “by-example ” query style (QBE), to suit the requirements of typical users of email environments, i.e. unfamiliar with database concepts and query formulation. The paper describes the striking characteristics of MBE, the various information access modules, and the current implementation using Lotus Notes environment. It also reports an experiment to test the usability of the interface, which confirmed its easiness and user-friendliness with regard to its target users profile. Users who participated in the experiment expressed a generalized satisfaction towards MBE features. 1
509|The Data Grid: Towards an Architecture for the Distributed Management and Analysis of Large Scientific Datasets|In an increasing number of scientific disciplines, large data collections are emerging as important  community resources. In this paper, we introduce design principles for a data management  architecture called the Data Grid. We describe two basic services that we believe are fundamental  to the design of a data grid, namely, storage systems and metadata management. Next, we explain  how these services can be used to develop higher-level services for replica management and replica  selection. We conclude by describing our initial implementation of data grid functionality.   
510|Globus: A Metacomputing Infrastructure Toolkit|Emerging high-performance applications require the ability to exploit diverse, geographically distributed resources. These applications use high-speed networks to integrate supercomputers, large databases, archival storage devices, advanced visualization devices, and/or scientific instruments to form networked virtual supercomputers or metacomputers. While the physical infrastructure to build such systems is becoming widespread, the heterogeneous and dynamic nature of the metacomputing environment poses new challenges for developers of system software, parallel tools, and applications. In this article, we introduce Globus, a system that we are developing to address these challenges. The Globus system is intended to achieve a vertically integrated treatment of application, middleware, and network. A low-level toolkit provides basic mechanisms such as communication, authentication, network information, and data access. These mechanisms are used to construct various higher-level metacomp...
511|A Security Architecture for Computational Grids|State-of-the-art and emerging scientific applications require fast access to large quantities of data and commensurately fast computational resources. Both resources and data are often distributed in a wide-area network with components administered locally and independently. Computations may involve hundreds of processes that must be able to acquire resources dynamically and communicate e#ciently. This paper analyzes the unique security requirements of large-scale distributed (grid) computing and develops a security policy and a corresponding security architecture. An implementation of the architecture within the Globus metacomputing toolkit is discussed.  
512|A Directory Service for Configuring High-Performance Distributed Computations|High-performance execution in distributed computing environments often requires careful selection and configuration not only of computers, networks, and other resources but also of the protocols and algorithms used by applications. Selection and configuration in turn require access to accurate, up-to-date information on the structure and state of available resources. Unfortunately, no standard mechanism exists for organizing or accessing such information. Consequently, different tools and applications adopt ad hoc mechanisms, or they compromise their portability and performance by using default configurations. We propose a solution to this problem: a Metacomputing Directory Service that provides efficient and scalable access to diverse, dynamic, and distributed information about resource structure and state. We define an extensible data model to represent the information required for distributed computing, and we present a scalable, high-performance, distributed implementation. The dat...
513|The SDSC Storage Resource Broker|This paper describes the architecture of the SDSC Storage Resource Broker (SRB). The SRB is middleware that provides applications a uniform API to access heterogeneous distributed storage resources including, filesystems, database systems, and archival storage systems. The SRB utilizes a metadata catalog service, MCAT, to provide a &#034;collection&#034;- oriented view of data. Thus, data items that belong to a single collection may, in fact, be stored on heterogeneous storage systems. The SRB infrastructure is being used to support digital library projects at SDSC. This paper describes the architecture and various features of the SDSC SRB. 1 Introduction  The San Diego Supercomputer Center (SDSC) is involved in developing infrastructure for a high performance distributed computing environment as part of its National Partnership for Advanced Computational Infrastructure (NPACI) project funded by the NSF. The NSF program in Partnerships for Advanced Computational Infrastructure (PACI), which fund...
514| A Distributed Resource Management Architecture that Supports Advance Reservations and Co-Allocation |The realization of end-to-end quality of service (QoS) guarantees in emerging network-based applications requires mechanisms that support first dynamic discovery and then advance or immediate reservation of resources that will often be heterogeneous in type and implementation and independently controlled and administered.We propose the Globus Architecture for Reservation and Allocation (GARA) to address these four issues.GARA treats both reservations and computational elements such as processes, network flows, and memory blocks as first class entities, allowing them to be created, monitored, and managed independently and uniformly.It simplifies management of heterogeneous resource types by defining uniform mechanisms for computers, networks, disk, memory, and other resources. Layering on these standard mechanisms, GARA enables the construction of application-level co-reservation and coallocation libraries that applications can use to dynamically assemble collections of resources, guided by both application QoS requirements and the local administration policy of individual resources.We describe a prototype GARA implementation that supports three different resource types— parallel computers, individual CPUs under control of the Dynamic Soft Real-Time scheduler, and Integrated Services networks—and provide performance results that quantify the costs of our techniques. 
515|Forecasting Network Performance to Support Dynamic Scheduling Using the Network Weather Service|The Network Weather Service is a generalizable and extensible facility designed to provide dynamic resource performance forecasts in metacomputing environments. In this paper, we outline its design and detail the predictive performance of the forecasts it generates. While the forecasting methods are general, we focus on their ability to predict the TCP/IP end-to-end throughput and latency that is attainable by an application using systems located at different sites. Such network forecasts are needed both to support scheduling [5], and by the metacomputing software infrastructure to develop quality-of-service guarantees [10, 17]. Keywords: scheduling, metacomputing, quality-ofservice, statistical forecasting, network performance monitoring  1. Introduction  As network technology advances, the resulting improvements in interprocess communication speeds make it possible to use interconnected but separate computer systems as a high-performance computational platform or metacomputer. Effect...
516|The NetLogger Methodology for High Performance Distributed Systems Performance Analysis|We describe a methodology that enables the real-time diagnosis of performance problems in complex high-performance distributed systems. The methodology includes tools for generating precision event logs that can be used to provide detailed end-to-end application and system level monitoring; a Java agent-based system for managing the large amount of logging data; and tools for visualizing the log data and real-time state of the distributed system. We developed these tools for analyzing a high-performance distributed system centered around the transfer of large amounts of data at high speeds from a distributed storage server to a remote visualization client. However, this methodology should be generally applicable to any distributed system.
517|The Stanford Digital Library Metadata Architecture|. The overall goal of the Stanford Digital Library project is to provide an infrastructure that affords interoperability among heterogeneous, autonomous digital library services. These services include both search services and remotely usable information processing facilities. In this paper, we survey and categorize the metadata required for a diverse set of Stanford Digital Library services that we have built. We then propose an extensible metadata architecture that meets these requirements. Our metadata architecture fits into our established infrastructure and promotes interoperability among existing and de-facto metadata standards. Several pieces of this architecture are implemented; others are under construction. The architecture includes attribute model proxies, attribute model translation services, metadata information facilities for search services, and local metadata repositories. In presenting and discussing the pieces of the architecture, we show how they address our motivati...
518|The Internet2 Distributed Storage Infrastructure Project: An Architecture for Internet Content Channels|ess to certain services by applying resources to those services alone. This structure inhibits the development of a viable economic model for differential investment in new infrastructure. But the need for further action on this front becomes all the more urgent as some of the underlying causes of these problems get worse over time. For instance, new media types and complex services delivered across the Web, as well as the exponential growth of the Web user population, are putting an ever-increasing load on network backbones and servers. Moreover, the Web and the Internet are constantly being used in new ways and for mission critical activities, such as the ubiquitous delivery of new software (e.g. Netscape Communicator). The I2-DSI project is based on the premise that any new infrastructure designed to improve Internet performance should also accommodate the relentless movement toward innovation and expansion of services. The I2-DSI strategy is to promote the development of innovative
519|Efficient Organization And Access Of Multi-Dimensional Datasets On Tertiary Storage Systems|This paper addresses the problem of urgently needed data management techniques for efficiently retrieving requested subsets of large datasets from mass storage devices. This problem is especially critical for scientific investigators who need ready access to the large volume of data generated by large-scale supercomputer simulations and physical experiments as well as the automated collection of observations by monitoring devices and satellites. This problem also negates the benefits of fast networks, because the time to access a subset from a large dataset stored on a mass storage system is much greater than the time to transmit that subset over a fast network. This paper focuses on very large spatial and temporal datasets generated by simulation of climate models, but the techniques described here are applicable to any large multidimensional grid data. The main requirement is to efficiently access relevant informationcontained within much larger datasets for analysis and interactive ...
520|Object-relational Queries into Multidimensional Databases with the Active Data Repository|As computational power and storage capacity increase, processing and analyzing large volumes of multi-dimensional datasets play an increasingly important role in many domains of scientific research. Scientific applications that make use of very large scientific datasets have several important characteristics: datasets consist of complex data and are usually multi-dimensional; applications usually retrieve a subset of all the data available in the dataset; various applicationspecific operations are performed on the data items retrieved. Such applications can be supported by object-relational database management systems (OR-DBMSs). In addition to providing functionality to define new complex datatypes and user-defined functions, an OR-DBMS for scientific datasets should contain runtime support that will provide optimized storage for very large datasets and an execution environment for user-defined functions involving expensive operations. In this paper we describe an infrastructure, the ...
521|Storage Management for High Energy Physics Applications|In many scientific domains large volumes of data are often generated by experimental devices or simulation programs. Examples are atmospheric data transmitted by satellites, climate modeling simulations, and high energy physics experiments. The volumes of data may reach hundreds of terabytes and therefore it is impractical to store them on disk systems. Rather they are stored on robotic tape systems that are managed by some mass storage system (MSS). A major bottleneck in analyzing the simulated/collected data is the retrieval of subsets from the tertiary storage system. This bottleneck results from the fact that the requested subsets are spread over many tape volumes, because the data are stored as files on tapes according to a predetermined order, usually according to the order they are generated. In this paper we describe the architecture and implementation of a Storage Manager designed to support the ordering of the data on tapes to optimize access patterns to the data. We also des...
523|Available Instruction-Level Parallelism for Superscalar and Superpipelined Machines|Superscalar machines can issue several instructions per cycle. Superpipelined machines can issue only one instruction per cycle, but they have cycle times shorter than the latency of any functional unit. In this paper these two techniques are shown to be roughly equivalent ways of exploiting instruction-level parallelism. A parameterizable code reorganization and simulation system was developed and used to measure instruction-level parallelism for a series of benchmarks. Results of these simulations in the presence of various compiler optimizations are presented. The average degree of superpipelining metric is introduced. Our simulations suggest that this metric is already high for many machines. These machines already exploit all of the instruction-level parallelism available in many non-numeric applications, even without parallel instruction issue or higher degrees of pipelining.  
524|Aspects of cache memory and instruction buffer performance|Public reporting burden for the collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden, to Washington Headquarters Services, Directorate for Information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington VA 22202-4302. Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to a penalty for failing to comply with a collection of information if it does not display a currently valid OMB control number.
525|Long address traces from RISC machines: Generation and analysis|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
526|A Characterization of Processor Performance in the VAX-11/780|This paper reports the results of a study of VAX-11/780 processor performance using a novel hardware monitoring technique. A micro-PC histogram monitor was built for these measurements. It keeps a count of the number of microcode cycles executed at each microcode location. Measurement experiments were performed on live timesharing workloads as well as on synthetic workloads of several types. The histogram counts allow the calculation of the frequency of various architectural events, such as the frequency of different types of opcodes and operand specifiers, as well as the frequency of some implementation-specific events, such as translation buffer misses. The measurement technique also yields the amount of processing time spent in various activities, such as ordinary microcode computation, memory management, and processor stalls of different kinds. This, paper reports in detail the amount of time the &#039;average&#039;fVAX instruction spends in these activities. 1.
527|Architectural And Organizational Tradeoffs in the Design of the MultiTitan CPU|This paper describes the architectural and organizational tradeoffs made during the design of the MultiTitan, and provides data supporting the decisions made. These decisions covered the entire space of processor design, from the instruction set and virtual memory architecture through the pipeline and organization of the machine. In particular, some of the tradeoffs involved the use of an on-chip instruction cache with off-chip TLB and floating-point unit, the use of direct-mapped instead of associative caches, the use of a 64-bit vs. 32-bit data bus, and the implementation of hardware pipeline interlocks. This is a preprint of a paper that will be presented at the 16th Annual International Symposium on Computer Architecture, IEEE and ACM, Jerusalem, Israel, May 28-June 1, 1989. An early draft of this paper appeared as WRL Technical Note TN-8. Copyright 1989 ACM i 1. Introduction The MultiTitan is a high-performance general-purpose 32-bit microprocessor developed at Digital Eq...
528|Improving performance of small on-chip instruction caches|Most current single-chip processors employ an on-chip instruction cache to improve performance. A miss in this insk-uc-tion cache will cause an external memory reference which must compete with data references for access to the external memory, thus affecting the overall performance of the processor. One com-mon way to reduce the number of off-chip instruction requests is to increase the size of the on-chip cache. An alternative approach is presented in this paper, in which a combination of an instruction cache, instruction queue and instruction queue buffer is used to achieve the same effect with a much smaller instruction cache size. Such an approach is significant for emerging technologies where high circuit densities are initially difficult to achieve yet a high level of performance is desired. or for more mature technologies where chip area can be used to provide more functionality. The viability of this approach is demonstrated by its implementation in au exist-ing single-chip processor. 1.
530|The Theory of Finance|bl ic Di sc lo su re A ut ho riz ed Pu bl ic
531|Absolute Priority, and the Pricing of Risky Debt Claims |In practice, there are substantial deviations from the doctrine of ‘absolute priority’, which governs the rights of the fum’s claimholders in the event of bankruptcy. To determine whether or not the possibility of such deviations is reflected in the prices of the lirm’s securities, this study examines the risk and return characteristics of financial claims against firms in court-supervised bankruptcy proceedings. Debt claims against bankrupt firms are indeed ‘risky’, exhibiting levels of systematic risk similar to that of common stocks in general. While some of the findings are anomalous, the data are generally consistent with the view that the capital market ‘properly ’ prices risky debt claims to reflect both their risk characteristics and the possibility of departures from the doctrine of absolute priority. 1. Introduction and
532|Separation of ownership and control|This paper analyzes the survival of organizations in which decision agents do not bear a major share of the wealth effects of their decisions. This is what the literature on large corporations calls separation of â??ownershipâ? and â??control.â? Such separation of decision and risk bearing functions is also common to organizations like large professional partnerships, financial mutuals and nonprofits. We contend that separation of decision and risk bearing functions survives in these organizations in part because of the benefits of specialization of management and risk bearing but also because of an effective common approach to controlling the implied agency problems. In particular, the contract structures of all these organizations separate the ratification and monitoring of decisions from the initiation and implementation of the decisions.
534|Environmental Impacts of a North American Free Trade Agreement,” in Mexican-U.S. Free Trade Agreement, edited by|This paper was prepared for the conference on the U.S.- Mexico
536|The proper scope of government: Theory and an application to prisons|When should a government provide a service in-house, and when should it contract out provision? We develop a model in which the provider can invest in improving the quality of service or reducing cost. If contracts are incomplete, the private provider has a stronger incentive to engage in both quality improvement and cost reduction than a government employee has. However, the private contractor’s incentive to engage in cost reduction is typically too strong because he ignores the adverse effect on noncontractible quality. The model is applied to understanding the costs and benefits of prison privatization.  
537|A constant recontracting model of sovereign debt|Few sovereign debtors have repudiated their obligations entirely. But despite the significant sanctions at the disposal of lenders, many borrowers have been able to consistently negotiate for reduced repayments. This paper presents a model of the on-going bargaining process that determines repayment levels. We derive a bargaining equilibrium in which countries with large debts achieve negotiated partial default. The ability to credibly threaten more draconian penalties in the event of repudiation may be of no benefit to lenders. Furthermore, unanticipated increases in world interest rates may actually help the borrowers by making lenders more impatient for a negotiated settlement. Finally, Western governments may be induced to make payments to facilitate reschedulings even though efficient agreements will be reached without their intervention.
538|Reputation acquisition in debt markets|you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at
539|Anatomy of Financial Distress: An Examination of Junk-Bond Issuers. The Quarterly|This paper examines the events following the onset of financial distress for 102 public junk bond issuers. We find that out-of-court debt relief mainly comes from junk bond holders; banks almost never forgive principal, though they do defer payments and waive debt covenants. Asset sales are an important means of avoiding Chapter 11 reorganization; however, they may be limited by industry factors. If a company simply restructures its bank debt, but either does not restructure its public debt or does not sell major assets or merge, the company goes bankrupt. The structure of a company&#039;s liabilities affects the likelihood that it goes bankrupt; companies whose bank and private debt are secured as well as companies with complex public debt structures are more prone to go bankrupt. Finally, there is no evidence that more profitable distressed companies are more successful in dealing with financial distress; they are not less likely to go bankrupt, sell assets, or reduce capital expenditures.
540|The Economics of Bankruptcy Reform|We propose a new bankruptcy procedure. Initially, a firm&#039;s debts are cancelled, and cash and non-cash bids are solicited for the &#039;new &#034; (all-equity) firm. Former claimants are given shares, or options to buy shares, in the new firm on the basis of absolute priority. Options are exercised once the bids are in. Finally, a shareholder vote is taken to select one of the bids. In essence, our procedure is a variant on the U.S. Chapter 7, in which non-cash bids are possible; this allows for reorganization. We believe our scheme is superior to Chapter 11 since it is simpler, quicker, market-based, avoids conflicts, and places appropriate discipline on management.
541|Executive Compensation, Management Turnover, and Firm Performance: An Empirical Investigation|This paper investigates the internal managerial control mechanisms at the disposal of a corpora-tion&#039;s compensation-setting board or committee. The hypotheses tested are that both compensation changes and management changes are methods used to control top management, and that the use of these control methods is motivated by changes in the firm&#039;s stock price performance. Public data from the period 1977-1980 support our hypotheses. We conclude that the firm&#039;s board creates managerial incentives consistent with those of the firm&#039;s owners, both by setting compensation a d following management change policies which benefit shareholders. 1.
542|Hostile takeovers in the 1980s: the return to corporate specialization|HOSTILE TAKEOVERS invite strong reactions, both positive and negative, from academics as well as the general public. Yet fairly little is known about what drives these takeovers, which characteristically involve significant wealth gains to target firms &#039; shareholders. The question is where these wealth gains come from. We examine the sample of all 62 hostile takeover contests between 1984 and 1986 that involved a purchase price of $50 million or more. In these contests, 50 targets were acquired and 12 remained independent. We use a sample of hostile takeovers exclusively to avoid using evidence from friendly acquisitions to judge hostile ones, as many studies have done. We examine such post-takeover operational changes as divestitures, layoffs, tax savings, and investment cuts to understand how the bidding firm could justify paying the takeover premium. We also examine the possibility of wealth losses by bidding firms &#039; stockholders as the explanation for target shareholder gains. The analysis of post-takeover changes is complicated because once the target and the bidding firms are merged, it becomes impossible to attribute to the target the changes recorded in joint accounting data. As a consequence, we do not use such data, but rather focus on discussion in annual reports, 1OK forms, newspapers, magazines, Moody&#039;s and
543|The Deisgn Philosophy of the DARPA Internet Protocols|The Intemet protocol suite, TCP/IP, was first proposed fifteen years ago, It was developed by the Defense Advanced Research Projects Agency (DARPA), and has been used widely in military and commercial systems. While there have been papers and specifications that describe how the protocols work, it is sometimes difficult to deduce from these why the protocol is as it is. For example, the Intemet protocol is based on a connectionless ordatagram mode of service. The motivation for this has been greatly misunderstood. This paper attempts to capture some of the early reasoning which shaped the Intemet protocols.
544|A Protocol for Packet Network Intercommunication|A protocol that supports the sharing of resources that exists in different packet switching networks is presented. The protocol provides for variation in individual network packet sizes, transmission failures, sequencing, flow control, end-to-end error checking, and the creation and destruction of logical process-to-process connections. Some implementation issues are considered, and problems such as internetwork routing, accounting, and timeouts are exposed.
545|The JPEG still picture compression standard|This paper is a revised version of an article by the same title and author which appeared in the April 1991 issue of Communications of the ACM. For the past few years, a joint ISO/CCITT committee known as JPEG (Joint Photographic Experts Group) has been working to establish the first international compression standard for continuous-tone still images, both grayscale and color. JPEG’s proposed standard aims to be generic, to support a wide variety of applications for continuous-tone images. To meet the differing needs of many applications, the JPEG standard includes two basic compression methods, each with various modes of operation. A DCT-based method is specified for “lossy’ ’ compression, and a predictive method for “lossless’ ’ compression. JPEG features a simple lossy technique known as the Baseline method, a subset of the other DCT-based modes of operation. The Baseline method has been by far the most widely implemented JPEG method to date, and is sufficient in its own right for a large number of applications. This article provides an overview of the JPEG standard, and focuses in detail on the Baseline method. 1
546|Disconnected Operation in the Coda File System|Disconnected operation is a mode of operation that enables a client to continue accessing critical data during temporary failures of a shared data repository. An important, though not exclusive, application of disconnected operation is in supporting portable computers. In this paper, we show that disconnected operation is feasible, efficient and usable by describing its design and implementation in the Coda File System. The central idea behind our work is that caching of data, now widely used for performance, can also be exploited to improve availability.
547|Receiver-driven Layered Multicast|State of the art, real-time, rate-adaptive, multimedia applications adjust their transmission rate to match the available network capacity. Unfortunately, this source-based rate-adaptation performs poorly in a heterogeneous multicast environment because there is no single target rate — the conflicting bandwidth requirements of all receivers cannot be simultaneously satisfied with one transmission rate. If the burden of rate-adaption is moved from the source to the receivers, heterogeneity is accommodated. One approach to receiver-driven adaptation is to combine a layered source coding algorithm with a layered transmission system. By selectively forwarding subsets of layers at constrained network links, each user receives the best quality signal that the network can deliver. We and others have proposed that selective-forwarding be carried out using multiple IP-Multicast groups where each receiver specifies its level of subscription by joining a subset of the groups. In this paper, we extend the multiple group framework with a rate-adaptation protocol called Receiver-driven Layered Multicast, or RLM. Under RLM, multicast receivers adapt to both the static heterogeneity of link bandwidths as well as dynamic variations in network capacity (i.e., congestion). We describe the RLM protocol and evaluate its performance with a preliminary simulation study that characterizes user-perceived quality by assessing loss rates over multiple time scales. For the configurations we simulated, RLM results in good throughput with transient short-term loss rates on the order of a few percent and long-term loss rates on the order of one percent. Finally, we discuss our implementation of a software-based Internet video codec and its integration with RLM.
548|Coda: A Highly available File System for a Distributed Workstation Environment|Abstract- Coda is a file system for a large-scale distributed computing environment composed of Unix workstations. It provides resiliency to server and network failures through the use of two distinct but complementary mechanisms. One mechanism, server replication, stores copies of a file at multiple servers. The other mechanism, disconnected operation, is a mode of execution in which a caching site temporarily assumes the role of a replication site. Disconnected operation is particularly useful for supporting portable workstations. The design of Coda optimizes for availability and performance, and strives to provide the highest degree of consistency attainable in the light of these objectives. Measurements from a prototype show that the performance cost of providing high availability in Coda is reasonable. Index Terms- Andrew, availability, caching, disconnected operation, distributed file system, performance, portable computers, scalability, server replication. I.
549|Managing Update Conflicts in Bayou, a Weakly Connected Replicated Storage System|Bayou is a replicated, weakly consistent storage system designed for a mobile computing environment that includes portable machines with less than ideal network connectivity. To maximize availability, users can read and write any accessible replica. Bayou&#039;s design has focused on supporting apphcation-specific mechanisms to detect and resolve the update conflicts that naturally arise in such a system, ensuring that replicas move towards eventual consistency, and defining a protocol by which the resolution of update conflicts stabilizes. It includes novel methods for conflict detection, called dependency checks, and per-write conflict resolution based on client-provided merge procedures. To guarantee eventual consistency, Bayou servers must be able to rollback the effects of previously executed writes and redo them according to a global senalization order. Furthermore, Bayou permits clients to observe the results of all writes received by a server, Including tentative writes whose conflicts have not been ultimately resolved. This paper presents the motivation for and design of these mechanisms and describes the experiences gained with an initial implementation of the system.
550|Vnodes: An architecture for multiple file system types|sun!srk
551|Adapting to Network and Client Variability via On-Demand Dynamic Distillation|The explosive growth of the Internet and the proliferation of smart cellular phones and handheld wireless devices is widening an already large gap between Internet clients. Clients vary in their hardware resources, software sophistication, and quality of connectivity, yet server support for client variation ranges from relatively poor to none at all. In this paper we introduce some design principles that we believe are fundamental to providing “meaningful” Internet access for the entire range of clients. In particular, we show how to perform on-demand datatype-specific lossy compression on semantically typed data, tailoring content to the specific constraints of the client. We instantiate our design principles in a proxy architecture that further exploits typed data to enable application-level management of scarce network resources. Our proxy architecture generalizes previous work addressing all three aspects of client variation by applying well-understood techniques in a novel way, resulting in quantitatively better end-to-end performance, higher quality display output, and new capabilities for lowend clients. 1
553|Rover: A Toolkit for Mobile Information Access|The Rover toolkit combines relocatable dynamic objects and queued remote procedure calls to provide unique services for &#034;roving&#034; mobile applications. A relocatable dynamic object is an object with a well-defined interface that can be dynamically loaded into a client computer from a server computer (or vice versa) to reduce clientserver communication requirements. Queued remote procedure call is a communication system that permits applications to continue to make non-blocking remote procedure call requests even when a host is disconnected, with requests and responses being exchanged upon network reconnection. The challenges of mobile environments include intermittent connectivity, limited bandwidth, and channeluse optimization. Experimental results from a Rover-based mail reader, calendar program, and two non-blocking versions of WorldWide Web browsers show that Rover&#039;s services are a good match to these challenges. The Rover toolkit also offers advantages for workstation applications by providing a uniform distributed object architecture for code shipping, object caching, and asynchronous object invocation.
554|World-Wide Web Proxies|A WWW proxy server, proxy for short, provides access to the Web for people on closed subnets who can only access the Internet through a firewall machine. The hypertext server developed at CERN, cern_httpd, is capable of running as a proxy, providing seamless external access to HTTP, Gopher, WAIS and FTP. cern_httpd has had gateway features for a long time, but only this spring they were extended to support all the methods in the HTTP protocol used by WWW clients. Clients don&#039;t lose any functionality by going through a proxy, except special processing they may have done for nonnative Web protocols such as Gopher and FTP. A brand new feature is caching performed by the proxy, resulting in shorter response times after the first document fetch. This makes proxies useful even to the people who do have full Internet access and don&#039;t really need the proxy just to get out of their local subnet. This paper gives an overview of proxies and reports their current status.  1.0 Introduction  The pri...
555|Mobile Information Access|The ability to access information on demand when mobile will be a critical capability in the 21st century. In this paper, we examine the fundamental forces at work in mobile computing systems and explain how they constrain the problem of mobile information access. From these constraints, we derive the importance of adaptivity as a crucial requirement of mobile clients. We then develop a taxonomy of adaptation strategies, and summarize our research in application-transparent and  application-aware adaptation in the Coda and Odyssey systems respectively.  
556|Processor Capacity Reserves for Multimedia Operating Systems|Multimedia applications have timing requirements that cannot generally be satisfied using time-sharing scheduling algorithms and system structures. To effectively support these types of programs, operating systems must support processor capacity reservation. A capacity reservation and enforcement mechanism isolates programs from the timing and execution characteristics of other programs in the same way that a memory protection system isolates programs from memory access by other programs. In this paper, we characterize the timing requirements and processor capacity reservation requirements for multimedia applications, we describe a scheduling framework to support reservation and admission control, and we introduce a novel reserve abstraction, specifically designed for the microkernel architecture, for controlling processor usage. 
557|Trace-Based Mobile Network Emulation|Subjecting a mobile computing system to wireless network conditions that are realistic yet reproducible is a challenging problem. In this paper, we describe a technique called trace modulation that re-creates the observed end-to-end characteristics of a real wireless network in a controlled and repeatable manner. Trace modulation is transparent to applications and accounts for all network traffic sent or received by the system under test. We present results that show that it is indeed capable of reproducing wireless network performance faithfully.  1 Introduction  How does one subject a mobile computing system to realistic yet reproducible wireless networking conditions? Reproducible behavior is important for three reasons. First, it is essential for thorough evaluation of the performance of a mobile computing system. Second, it is necessary for comparative evaluations of alternative system designs. Third, it is valuable in debugging mobile systems because it enables the re-creation of...
558|The Case for Wireless Overlay Networks|Wireless data services, other than those for electronic mail or paging, have thus far been more promising than successful. We believe that future mobile information systems must be built upon heterogeneous wireless overlay networks&#039;, extending traditional wired and internetworked processing &#034;islands&#034; to hosts on the move over coverage areas ranging from in-room, in-building, campus, metropolitan, and wide-areas. Unfortunately, network planners continue to think in terms of homogeneous wireless communications systems and technologies. In this paper, we describe a new wireless data networking architecture that integrates diverse wireless technologies into a seamless internetwork. In addition, we describe the applications support services needed to make it possible for applications to continue to operate as mobile hosts roam across such networks. The architecture described herein is being implemented in a testbed at the University of California, Berkeley under joint government/industry sponsorship.
559|MOBISAIC: An Information System for A Mobile Wireless Computing Environment|Mobisaic is a World Wide Web information system designed to serve users in a mobile wireless computing environment. Mobisaic extends the Web by allowing documents to both refer and react to potentially changing contextual information, such as current location in the wireless network. Mobisaic relies on clientside processing of HTML documents that support two new concepts: Dynamic Uniform Resource Locators (URLs) and Active Documents. A dynamic URL is one whose results depend upon the state of the user&#039;s mobile context at the time it is resolved. An active document is one that automatically updates its contents in response to changes in a user&#039;s mobile context. This paper describes the design of Mobisaic, the mechanism it uses for representing a user&#039;s mobile context, and the extensions made to the syntax and function of Uniform Resource Locators and HyperText Markup Language documents to support mobility. 
560|Interactive Translation of Conversational Speech|We present JANUS-II, a large scale system effort aimed at interactive spoken language translation. JANUS-II now accepts spontaneous conversational speech in a limited domain in English, German or Spanish and produces output in German, English, Spanish, Japanese and Korean. The challenges of coarticulated, disfluent, ill-formed speech are manifold, and have required advances in acoustic modeling, dictionary learning, language modeling, semantic parsing and generation, to achieve acceptable performance. A semantic “interlingua ” that represents the intended meaning of an input sentence, facilitates the generation of culturally and contextually appropriate translation in the presence of irrelevant or erroneous information. Application of statistical, contextual, prosodic and discourse constraints permits a progressively narrowing search for the most plausible interpretation of an utterance. During translation, JANUS-II produces paraphrases that are used for interactive correction of translation errors. Beyond our continuing efforts to improve robustness and accuracy, we have also begun to study possible forms of deployment. Several system prototypes have been implemented to explore translation needs in different settings: speech translation in one-on-one video conferencing, as portable mobile
561|Packet-Pair Flow Control|This paper presents the packet-pair rate-based feedback flow control scheme. This scheme is designed for networks where individual connections do not reserve bandwidth and for the available bitrate (best-effort) component of integrated networks. We assume a round-robin-like queue service discipline in the output queues of the network&#039;s switches, and propose a linear stochastic model for a single conversation in a network of such switches. These model motivates the Packet-Pair rate probing technique, which forms the basis for provably stable discrete and continuous time rate-based flow control schemes. We present a novel state estimation scheme based on fuzzy logic. We then address several practical concerns: dealing with system startup, retransmission and timeout strategy, and dynamic setpoint probing. We present a finite state machine as well as source code for a model implementation. The dynamics of a single source, the interactions of multiple sources, and the behavior of packet-pai...
562|Issues in Wireless Mobile Computing|le workstation as an X terminal. Currently, several companies sell a poor man&#039;s version of this --- the typical &#034;workstation&#034; is a small alphanumeric pager-like device which limits the user to reading and composing Email. Such an approach has its uses; however, there are some fundamental problems with the idea of building the world entirely with mobile X terminals. One problem is the very limited cellular bandwidth and its inefficient allocation by a circuit-switched, TDMA or FDMA phone network operating with a large cell size. Severe bandwidth constraints limit the number of simultaneous users, the nature of the applications that can be run, or both. (Presently in many areas callers are regularly denied service because no channel is available.) The present Email services may represent the limit of what can be done with analog phone service designed for voice. Another, hopefully temporary, problem with building on cellular telephone systems is that different systems are not always comp
563|Adaptive Service in Mobile Computing Environments|Emerging communication-intensive applications require significant levels of networking resources for efficient operation. In the context of mobile computing environments, limited and dynamically varying available resources, stringent application requirements, and user mobility make it difficult to provide sustained quality of service to applications. This paper addresses the above issues, and proposes a new service model in cellular mobile computing environments. The major results of this paper are the following: ffl An Adaptive Service model in mobile computing environments, which enables the network and applications to adaptively (re)negotiate QoS depending on dynamic network conditions ffl An algorithmic framework that provides cost-effective resource adaptation in the presence of resource and network dynamics ffl A unified implementation architecture for adaptive service support A brief overview of an ongoing testbed implementation is provided. 1 INTRODUCTION Recent years have ...
564|Replication in Ficus Distributed File Systems|Ficus is a replicated general filing environment for Unix intended to scale to very large (nationwide) networks. The system employs an optimistic &#034;one copy availability&#034; model in which conflicting updates to the file system&#039;s directory information are automatically reconciled, while con icting file updates are reliably detected and reported. The system architecture is based on a stackable layers methodology which permits a high degree of modularity and extensibility of file system services. This paper presents the motivations for replication and summarizes the case for optimistic concurrency control for large scale distributed file systems. It presents a brief description of the Ficus file system and concludes with a number of outstanding issues which must be addressed. stackable file system architecture; and an optimistic view of update, in which any file or directory may be updated, so long as some copy is available. Conicts are addressed when reconnection occurs. The system, which we call Ficus, is now operational. It has been constructed in a manner that can be added to an operating system that provides a faithful implementation of the vnode interface [10], even using conventional Unix le systems such as UFS for file storage. Various optimizations are possible through extensions to the basic structure. In the next section, we summarize the motivations for our replication work, providing a view of the goals of the research. The approach to modularity and optimistic operation are then described. Next, the actual Ficus project is summarized, and some observations about the utility of these approaches are offered. Finally, anumber of other issues are raised. 
565|Dynamic Documents: Mobile Wireless Access to the WWW|We propose dynamic documents as an approach to extending and customizing the WWW/Mosaic for mobile computing platforms. Dynamic documents are programs executed on a mobile platform to generate a document; they are implemented as Tcl scripts. We have modified the NCSA Mosaic web client to run the dynamic documents it retrieves through a modified Tcl interpreter. The interpreter is designed to execute only commands that do not violate safety.  To hide the latencies of slow links we have modified the Mosaic client to perform caching and prefetching. The policies for caching and prefetching can be under control of dynamic documents, allowing the strategies to be document-specific. Using dynamic documents, we have built an adaptive email browser that employs application-specific caching and prefetching strategies. Both the browser and the displayed email messages are dynamically customized to the mobile computing environment in which they run.  
566|W4 - the Wireless World Wide Web|4. Choosing wireless communications  In order to separate promises from practice in PDAs From the beginning, we were not interested in wire- and wireless communications, we decided to try to build a less local area communications. We wanted the focus to wireless PDA-based client to access the World Wide Web. be on the PDA and not on the installation and manage- This paper describes our design choices and experience ment of the network. If we got something built, we with W4, the Wireless World Wide Web. wanted to take it on the road. Much of the &#034;look and feel&#034; of mobile computing is lost if you can&#039;t bring the demo to 1. Introduction the customer.  A number of people see personal digital assistants, Press releases from a number of vendors during the PDA&#039;s, with wireless communication as the &#034;next big summer of 1993 suggested that &#034;real soon&#034; there would be thing&#034; in the computer industry. However, the current several communications options to choose from. Winter PDA&#039;s obvious limitatio...
567|Exploiting Weak Connectivity in a Distributed File System|views and conclusions contained in this document are those of the authors and should not be interpreted as
568|Using Dynamic Sets to Reduce the Aggregate Latency of Data Access|Many users of large distributed systems are plagued by high latency when accessing remote data. Latency is particularly problematic for the critical application of search and retrieval, which tends to access many objects and may suffer a long wait for each object accessed. Existing techniques like caching, inferential prefetching, and explicit prefetching are not suited to search, are ineffective at reducing latency for search applications, or greatly increase the complexity of the programming model. This dissertation shows that extending the file system interface to support a new abstraction called dynamic sets can address the problem of latency for search without incurring the penalties of the other techniques. A dynamic set is a lightweight and transitory collection of objects with well-defined semantics. An application creates a dynamic set on-demand to hold the objects it wishes to process. Adding dynamic sets to the system&#039;s interface results in two benefits. First, creation of a...
569|QoS Support for Mobile Computing|In recent years small, completely portable computers have become available on the marketplace. There is demand for such computers, termed walkstations, to access network services while retaining their mobility, and to operate effectively in a wide range of conditions. Future office environments are expected to support wireless networks with bandwidths which are several orders of magnitude greater than are available outdoors. In such environments, there will be powerful compute servers available for a walkstation&#039;s use. This dissertation describes a novel architecture called Notus and its support for applications operating in a mobile environment. The concept of the traded handoff is introduced, where applications are able to participate in the handoff process, rebuilding connections to the most appropriate service. This is expected to benefit walkstations which roam over large distances, where connections to servers would otherwise be strained, and also between heterogeneous networks where cooperation between the networks in performing a handoff might
570|Exploiting Non-Determinism in Set Iterators to Reduce I/O Latency|A key goal of distributed systems is to provide prompt access to shared information repositories. The high latency of remote access is a serious impediment to this goal. We propose a new file system abstraction called dynamic sets that allows the system to transparently reduce I/O latency without relying on reference locality, without modifying DFS serversand protocols, and without unduly complicating the programming model. We present this abstraction, and describe an implementation of it that runs on local and distributed file systems, as well as the World Wide Web. Substantial performance gains are demonstrated -- up to 50% savings in runtime for search on NFS, and up to 90% reduction in I/O latency for Web searches. 1 Introduction  A central problem facing distributed systems is the high latency to access remote data. Latency is problematic because it reduces the benefit typical applications can receive from faster CPUs, and reduces the productivity of users who are forced to wait f...
571|The SpectrumWare Testbed for ATM-based Software Radios|This paper describes the SpectrumWare testbed and two experimental systems we are developing: an ATM-based wideband receiver and the receive side of a GSM base station. These projects leverage recent advances in processor and analog-to-digital conversion technology that have enabled a software-oriented approach to wireless communication and distributed sample processing. 1 Introduction  Traditionally, analog hardware constrains systems to a particular modulation scheme and set of channels assignments. However, advances in processors and A/D conversion technology [1] have made it possible to implement &#034;virtual radios &#034; that sample wide bands of the RF spectrum, and process these samples in application software. In the SpectrumWare   1  architecture, the sample stream is converted into network packets (or ATM cells) at the antenna and forwarded to a cluster of workstations [8]. The workstation-based software can demodulate and decode any number of channels within the band, and apply diff...
572|Access path selection in a relational database management system|ABSTRACT: In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research&#039;Laboratory. 1.
573|A Performance Evaluation of Four Parallel Join Algorithms in a Shared-Nothing Multiprocessor Environment|The join operator has been a cornerstone of relational database systems since their inception. As such, much time and effort has gone into making joins efficient. With the obvious trend towards multiprocessors, attention has focused on efficiently parallelizing the join operation. In this paper we analyze and compare four parallel join algorithms. Grace and Hybrid hash represent the class of hash-based join methods, Simple hash represents a looping algorithm with hashing, and our last algorithm is the more traditional sort-merge. The Gamma database machine serves as the host for the performance comparison. Gamma’s shared-nothing architecture with commercially available components is becoming increasingly common, both in research and in industry. 1.
574|Dynamic Query Evaluation Plans|Traditional query optimizers assume accurate knowledge of run-time parameters such as selectivities and resource availability during plan optimization, i.e., at compile-time. In reality, however, this assumption is often not justified. Therefore, the “static ” plans produced by traditional optimizers may not be optimal for many of their actual run-time invocations. Instead, we propose a novel optimization model that assigns the bulk of the optimization effort to compile-time and delays carefully selected optimization decisions until run-time. Our previous work defined the run-time primitives, “dynamic plans ” using “choose-plan” operators, for executing such delayed decisions, but did not solve the problem of constructing dynamic plans at compile-time. The present paper introduces techniques that solve this problem. Experience with a working prototype optimizer demonstrates (i) that the additional optimization and start-up overhead of dynamic plans compared to static plans is dominated by their advantage at run-time, (ii) that dynamic plans are as robust as the “brute-force” remedy of run-time optimization, i.e., dynamic plans maintain their optimality even if parameters change between compile-time and run-time, and (iii) that the start-up overhead of dynamic plans is signifmantly less than the time required for complete optimization at run-time. In other words, our proposed techniques are superior to both techniques considered to-date, namely compile-time optimization into a single static plan as well as run-time optimization. Finally, we believe that the concepts and technology described can be transferred to commercial query optimizers in order to improve the performance of embedded queries with host variables in the query predicate and to adapt to run-time system loads unpredictable at compile-time. 1.
575|The Design Of Xprs|This paper presents an overview of the techniques we are using to build a DBMS at Berkeley that will simultaneously provide high performance and high availability in transaction processing environments, in applications with complex ad-hoc queries and in applications with large objects such as images or CAD layouts. We plan to achieve these goals using a general purpose DBMS and operating system and a shared memory multiprocessor. The hardware and software tactics which we are using to accomplish these goals are described in this paper and include a novel &#034;fast path&#034; feature, a special purpose concurrency control scheme, a two-dimensional file system, exploitation of parallelism and a novel method to efficiently mirror disks. 
576|A benchmark of NonStop SQL release 2 demonstrating near-linear speedup and scaleup on large databases|its second release, NonStop SQL transparently and automatically implements parallelism within an SQL statement. This parallelism allows query execution speed to increase almost linearly as processors and discs are added to the system-- speedup. In addition, this parallelism can help jobs restricted to a fIxed &amp;quot;batch window&amp;quot;. When the job doubles in size, its elapsed processing time will not change ifproportionately more equipment is available to process the job-- scaleup. This paper describes the parallelism features of NonStop SQL and an audited benchmark that demonstrates these speedup and scaleup claims.
577|A survey of general-purpose computation on graphics hardware|The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware acompelling platform for computationally demanding tasks in awide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general-purpose computation to graphics hardware. We begin with the technical motivations that underlie general-purpose computation on graphics processors (GPGPU) and describe the hardware and software developments that have led to the recent interest in this field. We then aim the main body of this report at two separate audiences. First, we describe the techniques used in mapping general-purpose computation to graphics hardware. We believe these techniques will be generally useful for researchers who plan to develop the next generation of GPGPU algorithms and techniques. Second, we survey and categorize the latest developments in general-purpose application development on graphics hardware.  
578|FFTW: An Adaptive Software Architecture For The FFT|FFT literature has been mostly concerned with minimizing the number of floating-point operations performed by an algorithm. Unfortunately, on present-day microprocessors this measure is far less important than it used to be, and interactions with the processor pipeline and the memory hierarchy have a larger impact on performance. Consequently, one must know the details of a computer architecture in order to design a fast algorithm. In this paper, we propose an adaptive FFT program that tunes the computation automatically for any particular hardware. We compared our program, called FFTW, with over 40 implementations of the FFT on 7 machines. Our tests show that FFTW&#039;s self-optimizing approach usually yields significantly better performance than all other publicly available software. FFTW also compares favorably with machine-specific, vendor-optimized libraries. 1. INTRODUCTION The discrete Fourier transform (DFT) is an important tool in many branches of science and engineering [1] and...
579|Modeling the Interaction of Light Between Diffuse Surfaces|A method is described which models the interaction of light between diffusely reflecting surfaces. Current light reflection models used in computer graphics do not account for the object-to-object reflection between diffuse surfaces, and thus incorrectly compute the global illumination effects. The new procedure, based on methods used in thermal engineering, includes the effects of diffuse light sources of finite area, as well as the &#034;color-bleeding&#034; effects which are caused by the diffuse reflections. A simple environment is used to illustrate these simulated effects and is presented with photographs of a physical model. The procedure is applicable to environments composed of ideal diffuse reflectors and can account for direct illumination from a variety of light sources. The resultant surface intensities are independent of observer position, and thus environments can be preprocessed for dynamic sequences.
580|Chromium: A Stream-Processing Framework for Interactive Rendering on Clusters|We describe Chromium, a system for manipulating streams of graphics API commands on clusters of workstations. Chromium&#039;s stream filters can be arranged to create sort-first and sort-last parallel graphics architectures that, in many cases, support the same applications while using only commodity graphics accelerators. In addition, these stream filters can be extended programmatically, allowing the user to customize the stream transformations performed by nodes in a cluster. Because our stream processing mechanism is completely general, any cluster-parallel rendering algorithm can be either implemented on top of or embedded in Chromium. In this paper, we give examples of real-world applications that use Chromium to achieve good scalability on clusters of workstations, and describe other potential uses of this stream processing technology. By completely abstracting the underlying graphics architecture, network topology, and API command processing semantics, we allow a variety of applications to run in different environments.
581|A progressive refinement approach to fast radiosity image generation|A reformulated radiosity algorithm is presented that produces initial images in time linear to the number of patches. The enormous memory costs of the radiosity algorithm are also elim-inated by computing form-factors on-the-fly. The technique is based on the approach of rendering by progressive refinement. The algorithm provides a useful solution almost immediately which progresses gracefully and continuously to the complete radiosity solution. In this way the competing demands of real-ism and interactivity are accommodated. The technique brings the use of radiosity for interactive rendering within reach and has implications for the use and development of current and future graphics workstations.
582|Fast Computation of Generalized Voronoi Diagrams Using Graphics Hardware|We present a new approach for computing generalized 2D and 3D Voronoi diagrams using interpolation-based polygon rasterization hardware. We compute a discrete Voronoi diagram by rendering a three dimensional distance mesh for each Voronoi site. The polygonal mesh is a bounded-error approximation of a (possibly) non-linear function of the distance between a site and a 2D planar grid of sample points. For each sample point, we compute the closest site and the distance to that site using polygon scan-conversion and the Z-buffer depth comparison. We construct distance meshes for points, line segments, polygons, polyhedra, curves, and curved surfaces in 2D and 3D. We generalize to weighted and farthest-site Voronoi diagrams, and present efficient techniques for computing the Voronoi boundaries, Voronoi neighbors, and the Delaunay triangulation of points. We also show how to adaptively refine the solution through a simple windowing operation. The algorithm has been implemented on SGI workstations and PCs using OpenGL, and applied to complex datasets. We demonstrate the application of our algorithm to fast motion planning in static and dynamic environments, selection in complex user-interfaces, and creation of dynamic mosaic effects.
583|Reflection from Layered Surfaces due to Subsurface Scattering|The reflection of light from most materials consists of two major terms: the specular and the diffuse. Specular reflection may be modeled from first principles by considering a rough surface consisting of perfect reflectors, or micro-facets. Diffuse reflection is generally considered to result from multiple scattering either from a rough surface or from within a layer near the surface. Accounting for diffuse reflection by Lambert&#039;s Cosine Law, as is universally done in computer graphics, is not a physical theory based on first principles. This paper presents
584|Brook for GPUs: Stream Computing on Graphics Hardware|In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming coprocessor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.
585|GPUTeraSort: High Performance Graphics Coprocessor Sorting for Large Database Management|We present a new algorithm, GPUTeraSort, to sort billionrecord wide-key databases using a graphics processing unit (GPU) Our algorithm uses the data and task parallelism on the GPU to perform memory-intensive and computeintensive tasks while the CPU is used to perform I/O and resource management. We therefore exploit both the highbandwidth GPU memory interface and the lower-bandwidth CPU main memory interface and achieve higher memory bandwidth than purely CPU-based algorithms. GPUTera-Sort is a two-phase task pipeline: (1) read disk, build keys, sort using the GPU, generate runs, write disk, and (2) read, merge, write. It also pipelines disk transfers and achieves near-peak I/O performance. We have tested the performance of GPUTeraSort on billion-record files using the standard Sort benchmark. In practice, a 3 GHz Pentium IV PC with $265 NVIDIA 7800 GT GPU is significantly faster than optimized CPU-based algorithms on much faster processors, sorting 60GB for a penny; the best reported PennySort price-performance. These results suggest that a GPU co-processor can significantly improve performance on large data processing tasks. 1.
586|Problem-oriented software engineering|This paper introduces a formal conceptual framework for software development, based on a problem-oriented perspective that stretches from requirements engineering through to program code. In a software problem the goal is to develop a machine—that is, a computer executing the software to be developed—that will ensure satisfaction of the requirement in the problem world. We regard development steps as transformations by which problems are moved towards software solutions. Adequacy arguments are built as problem transformations are applied: adequacy arguments both justify proposed development steps and establish traceability relationships between problems and solutions. The framework takes the form of a sequent calculus. Although itself formal, it can accommodate both formal and informal steps in development. A number of transformations are presented, and illustrated by application to small examples.  
587|Interactive Order-Independent Transparency|this document is to enable OpenGL developers to implement this technique with NVIDIA OpenGL extensions and GeForce3 hardware. Since shadow mapping is integral to the technique a very basic introduction is provided, but the interested reader is encouraged to explore the referenced material for more detail
588|A Multigrid Solver for Boundary Value Problems Using Programmable Graphics Hardware|We present a method for using programmable graphics hardware to solve a variety of boundary value problems. The time-evolution of such problems is frequently governed by partial differential equations, which are used to describe a wide range of dynamic phenomena including heat transfer and fluid mechanics. The need to solve these equations efficiently arises in many areas of computational science. Finite difference methods are commonly used for solving partial differential equations; we show that this approach can be mapped onto a modern graphics processor. We demonstrate an implementation of the multigrid method, a fast and popular approach to solving boundary value problems, on two modern graphics architectures. Our initial tests with available hardware show speedups of roughly 15x compared to traditional software implementation. This work presents a novel use of computer hardware and raises the intriguing possibility that we can make the inexpensive power of modern commodity graphics hardware accessible to and useful for the simulation commuuity.
589|The Direct3D 10 system |We present a system architecture for the 4 th generation of PCclass programmable graphics processing units (GPUs). The new pipeline features significant additions and changes to the prior generation pipeline including a new programmable stage capable of generating additional primitives and streaming primitive data to memory, an expanded, common feature set for all of the programmable stages, generalizations to vertex and image memory resources, and new storage formats. We also describe structural modifications to the API, runtime, and shading language to complement the new pipeline. We motivate the design with descriptions of frequently encountered obstacles in current systems. Throughout the paper we present rationale behind prominent design choices and alternatives that were ultimately rejected, drawing on insights collected during a multi-year collaboration with application developers and hardware designers.
590|Fast computation of database operations using graphics processors|We present new algorithms for performing fast computation of several common database operations on commodity graphics processors. Specifically, we consider operations such as conjunctive selections, aggregations, and semi-linear queries, which are essential computational components of typical database, data warehousing, and data mining applications. While graphics processing units (GPUs) have been designed for fast display of geometric primitives, we utilize the inherent pipelining and parallelism, single instruction and multiple data (SIMD) capabilities, and vector processing functionality of GPUs, for evaluating boolean predicate combinations and semi-linear queries on attributes and executing database operations efficiently. Our algorithms take into account some of the limitations of the programming model of current GPUs and perform no data rearrangements. Our algorithms have been implemented on a programmable GPU (e.g. NVIDIA’s GeForce FX 5900) and applied to databases consisting of up to a million records. We have compared their performance with an optimized implementation of CPU-based algorithms. Our experiments indicate that the graphics processor available on commodity computer systems is an effective co-processor for performing database operations.
591|Physically-Based Visual Simulation on Graphics Hardware|In this paper, we present a method for real-time visual simulation of diverse dynamic phenomena using programmable graphics hardware. The simulations we implement use an extension of cellular automata known as the coupled map lattice (CML). CML represents the state of a dynamic system as continuous values on a discrete lattice. In our implementation we store the lattice values in a texture, and use pixel-level programming to implement simple next-state computations on lattice nodes and their neighbors. We apply these computations successively to produce interactive visual simulations of convection, reaction-diffusion, and boiling. We have built an interactive framework for building and experimenting with CML simulations running on graphics hardware, and have integrated them into interactive 3D graphics applications.
592|Understanding the Efficiency of GPU Algorithms for Matrix-Matrix Multiplication|Utilizing graphics hardware for general purpose numerical computations has become a topic of considerable  interest. The implementation of streaming algorithms, typified by highly parallel computations with little reuse  of input data, has been widely explored on GPUs. We relax the streaming model&#039;s constraint on input reuse and  perform an in-depth analysis of dense matrix-matrix multiplication, which reuses each element of input matrices  O(n) times. Its regular data access pattern and highly parallel computational requirements suggest matrix-matrix  multiplication as an obvious candidate for efficient evaluation on GPUs but, surprisingly we find even nearoptimal  GPU implementations are pronouncedly less efficient than current cache-aware CPU approaches. We find  the key cause of this inefficiency is that the GPU can fetch less data and yet execute more arithmetic operations  per clock than the CPU when both are operating out of their closest caches. The lack of high bandwidth access to  cached data will impair the performance of GPU implementations of any computation featuring significant input  reuse.
593|Lu-gpu: Efficient algorithms for solving dense linear systems on graphics hardware|We present a novel algorithm to solve dense linear systems using graphics processors (GPUs). We reduce matrix decomposition and row operations to a series of rasterization problems on the GPU. These include new techniques for streaming index pairs, swapping rows and columns and parallelizing the computation to utilize multiple vertex and fragment processors. We also use appropriate data representations to match the rasterization order and cache technology of graphics processors. We have implemented our algorithm on different GPUs and compared the performance with optimized CPU implementations. In particular, our implementation on a NVIDIA GeForce 7800 GPU outperforms a CPU-based ATLAS implementation. Moreover, our results show that our algorithm is cache and bandwidth efficient and scales well with the number of fragment processors within the GPU and the core GPU clock rate. We use our algorithm for fluid flow simulation and demonstrate that the commodity GPU is a useful co-processor for many scientific applications. 1
594|Sequential point trees|Figure 1: Continuous detail levels of a Buddha generated in vertex programs on the GPU. The colors denote the LOD level used and the bars describe the selected amount of points selected for the GPU (top row) and the average CPU load required for rendering (bottom row). In this paper we present sequential point trees, a data structure that allows adaptive rendering of point clouds completely on the graphics processor. Sequential point trees are based on a hierarchical point representation, but the hierarchical rendering traversal is replaced by sequential processing on the graphics processor, while the CPU is available for other tasks. Smooth transition to triangle rendering for optimized performance is integrated. We describe optimizations for backface culling and texture adaptive point selection. Finally, we discuss implementation issues and show results.
595|A memory model for scientific algorithms on graphics processors|We present a memory model to analyze and improve the performance of scientific algorithms on graphics processing units (GPUs). Our memory model is based on texturing hardware, which uses a 2D block-based array representation to perform the underlying computations. We incorporate many characteristics of GPU architectures including smaller cache sizes, 2D block representations, and use the 3C’s model to analyze the cache misses. Moreover, we present techniques to improve the performance of nested loops on GPUs. In order to demonstrate the effectiveness of our model, we highlight its performance on three memory-intensive scientific applications – sorting, fast Fourier transform and dense matrix-multiplication. In practice, our cache-efficient algorithms for these applications are able to achieve memory throughput of 30–50 GB/s on a NVIDIA 7900 GTX GPU. We also compare our results with prior GPU-based and CPU-based implementations on highend processors. In practice, we are able to achieve 2–5× performance improvement.
596|Radiosity on graphics hardware|Radiosity is a widely used technique for global illumination. Typically the computation is performed offline and the result is viewed interactively. We present a technique for computing radiosity, including an adaptive subdivision of the model, using graphics hardware. Since our goal is to run at interactive rates, we exploit the computational power and programmability of modern graphics hardware. Using our system on current hardware, we have been able to compute and display a radiosity solution for a 10,000 element scene in less than one second. Key words: Graphics Hardware, Global Illumination. 1
597|Fast and Simple 2D Geometric Proximity Queries Using Graphics Hardware|We present a new approach for computing generalized proximity information of arbitrary 2D objects using graphics hardware. Using multi-pass rendering techniques and accelerated distance computation, our algorithm performs proximity queries not only for detecting collisions, but also for computing intersections, separation distance, penetration depth, and contact points and normals. Our hybrid geometry and image-based approach balances computation between the CPU and graphics subsystems. Geometric object-space techniques coarsely localize potential intersection regions or closest features between two objects, and image-space techniques compute the low-level proximity information in these regions. Most of the proximity information is derived from a distance field computed using graphics hardware. We demonstrate the performance in collision response computation for rigid and deformable body dynamics simulations. Our approach provides proximity information at interactive rates for a variet...
598|GPU algorithms for radiosity and subsurface scattering|All in-text	references	underlined	in	blue	are	linked	to	publications	on	ResearchGate, letting you	access	and	read	them	immediately.
599|Fast and approximate stream mining of quantiles and frequencies using graphics processors|We present algorithms for fast quantile and frequency estimation in large data streams using graphics processors (GPUs). We exploit the high computation power and memory bandwidth of graphics processors and present a new sorting algorithm that performs ras-terization operations on the GPUs. We use sorting as the main computational component for histogram approximation and con-struction of -approximate quantile and frequency summaries. Our algorithms for numerical statistics computation on data streams are deterministic, applicable to xed or variable-sized sliding windows and use a limited memory footprint. We use GPU as a co-processor and minimize the data transmission between the CPU and GPU by taking into account the low bus bandwidth. We implemented our algorithms on a PC with a NVIDIA GeForce FX 6800 Ultra GPU and a 3:4 GHz Pentium IV CPU and applied them to large data streams consisting of more than 100 million values. We also compared the performance of our GPU-based algorithms with op-timized implementations of prior CPU-based algorithms. Overall, our results demonstrate that the graphics processors available on a commodity computer system are efcient stream-processor and useful co-processors for mining data streams.
600|Performance and accuracy of hardware-oriented native-, emulated- and mixed-precision solvers in FEM simulations |In a previous publication, we have examined the fundamental difference between computational precision and result accuracy in the context of the iterative solution of linear systems as they typically arise in the Finite Element discretization of Partial Differential Equations (PDEs) [1]. In particular, we evaluated mixed- and emulatedprecision schemes on commodity graphics processors (GPUs), which at that time only supported computations in single precision. With the advent of graphics cards that natively provide double precision, this report updates our previous results. We demonstrate that with new co-processor hardware supporting native double precision, such as NVIDIA’s G200 architecture, the situation does not change qualitatively for PDEs, and the previously introduced mixed precision schemes are still preferable to double precision alone. But the schemes achieve significant quantitative performance improvements with the more powerful hardware. In particular, we demonstrate that a Multigrid scheme can accurately solve a common test problem in Finite Element settings with one million unknowns in less than 0.1 seconds, which is truely outstanding performance. We support these conclusions by exploring the algorithmic design space enlarged by the availability of double precision directly in the hardware. 1 Introduction and
601|Detection of Collisions and Self-collisions Using Image-space Techniques|Image-space techniques have shown to be very efficient for collision detection in dynamic simulation and animation environments. This paper proposes a new image-space technique for efficient collision detection of arbitrarily shaped, water-tight objects. In contrast to existing approaches that do not consider self-collisions, our approach combines the image-space object representation with information on face orientation to overcome this limitation. While
602|Applications of Pixel Textures in Visualization and Realistic Image Synthesis|With fast 3D graphics becoming more and more available even on low end platforms, the focus in developing new graphics hardware is beginning to shift towards higher quality rendering and additional functionality instead of simply higher performance implementations of the traditional graphics pipeline. On this search for improved quality it is important to identify a powerful set of orthogonal features to be implemented in hardware, which can then be flexibly combined to form new algorithms.  Pixel textures are an OpenGL extension by Silicon Graphics that fits into this category. In this paper, we demonstrate the benefits of this extension by presenting several different algorithms exploiting its functionality to achieve high quality, high performance solutions for a variety of different applications from scientific visualization and realistic image synthesis. We conclude that pixel textures are a valuable, powerful feature that should become a standard in future graphics systems.   
603|Accelerating 3D convolution using graphics hardware|Many volume filtering operations used for image enhancement, data processing or feature detection can be written in terms of three-dimensional convolutions. It is not possible to yield interactive frame rates on todays hardware when applying such convolutions on volume data using software filter routines. As modern graph-ics workstations have the ability to render two-dimensional convo-luted images to the frame buffer, this feature can be used to accel-erate the process significantly. This way generic 3D convolution can be added as a powerful tool in interactive volume visualization toolkits.
604|GPU-ABiSort: Optimal parallel sorting on stream architectures|In this paper, we present a novel approach for parallel sorting on stream processing architectures. It is based on adaptive bitonic sorting. For sorting n values utilizing p stream processor units, this approach achieves the optimal time complexity O((n log n)/p). While this makes our approach competitive with common sequential sorting algorithms not only from a theoretical viewpoint, it is also very fast from a practical viewpoint. This is achieved by using efficient linear stream memory accesses and by combining the optimal time approach with algorithms optimized for small input sequences. We present an implementation on modern programmable graphics hardware (GPUs). On recent GPUs, our optimal parallel sorting approach has shown to be remarkably faster than sequential sorting on the CPU, and it is also faster than previous non-optimal sorting approaches on the GPU for sufficiently large input sequences. Because of the excellent scalability of our algorithm with the number of stream processor units p (up to n / log 2 n or even n / log n units, depending on the stream architecture), our approach profits heavily from the trend of increasing number of fragment processor units on GPUs, so that we can expect further speed improvement with upcoming GPU generations.
605|Towards Fast Non-Rigid Registration|A fast multiscale and multigrid method for the matching of images in 2D and 3D is presented. Especially in medical imaging this problem - denoted as the registration problem - is of fundamental importance in the handling of images from multiple image modalities or of image time series. The paper restricts to the simplest matching energy to be minimized, i.e., E[] =    R    jf 1   f2 j    , where f1 , f2 are the intensity maps of the two images to be matched and  is a deformation. The focus is on a robust and efficient solution strategy. Matching of
606|Interactive Time-Dependent Tone Mapping Using Programmable Graphics Hardware|Modern graphics architectures have replaced stages of the graphics pipeline with fully programmable modules. Therefore, it is now possible to perform fairly general computation on each vertex or fragment in a scene. In addition, the nature of the graphics pipeline makes substantial computational power available if the programs have a suitable structure. In this paper, we show that it is possible to cleanly map a state-of-the-art tone mapping algorithm to the pixel processor. This allows an interactive application to achieve higher levels of realism by rendering with physically based, unclamped lighting values and high dynamic range texture maps. We also show that the tone mapping operator can easily be extended to include a time-dependent model, which is crucial for interactive behavior. Finally, we describe the ways in which the graphics hardware limits our ability to compress dynamic range efficiently, and discuss modifications to the algorithm that could alleviate these problems.
607|Fast summed-area table generation and its applications|We introduce a technique to rapidly generate summed-area tables using graphics hardware. Summed area tables, originally introduced by Crow, provide a way to filter arbitrarily large rectangular regions of an image in a constant amount of time. Our algorithm for generating summed-area tables, similar to a technique used in scientific computing called recursive doubling, allows the generation of a summed-area table in O(log n) time. We also describe a technique to mitigate the precision requirements of summed-area tables. The ability to calculate and use summed-area tables at interactive rates enables numerous interesting rendering effects. We present several possible applications. First, the use of summed-area tables allows real-time rendering of interactive, glossy environmental reflections. Second, we present glossy planar reflections with varying blurriness dependent on a reflected object’s distance to the reflector. Third, we show a technique that uses a summed-area table to render glossy transparent objects. The final application demonstrates an interactive depth-of-field effect using summedarea tables. Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
608|STAPL: An adaptive, generic parallel C++ library| The Standard Template Adaptive Parallel Library (STAPL) is a parallel library designed as a superset of the ANSI C++ Standard Template Library (STL). It is sequentially consistent for functions with the same name, and executes on uni- or multi-processor systems that utilize shared or distributed memory. STAPL is implemented using simple parallel extensions of C++ that currently provide a SPMD model of parallelism, and supports nested parallelism. The library is intended to be general purpose, but emphasizes irregular programs to allow the exploitation of parallelism for applications which use dynamically linked data structures such as particle transport calculations, molecular dynamics, geometric modeling, and graph algorithms. STAPL provides several different algorithms for some library routines, and selects among them adaptively at runtime. STAPL can replace STL automatically by invoking a preprocessing translation phase. In the applications studied, the performance of translated code was within 5 % of the results obtained using STAPL directly. STAPL also provides functionality to allow the user to further optimize the code and achieve additional performance gains. We present results obtained using STAPL for a molecular dynamics code and a particle transport code.  
609|Hardware Acceleration in Commercial Databases: A Case Study of Spatial Operations|Traditional databases have focused on the issue  of reducing I/O cost as it is the bottleneck  in many operations. As databases become  increasingly accepted in areas such as Geographic  Information Systems (GIS) and Bioinformatics,  commercial DBMS need to support  data types for complex data such as spatial  geometries and protein structures. These  non-conventional data types and their associated  operations present new challenges. In  particular, the computational cost of some  spatial operations can be orders of magnitude  higher than the I/O cost. In order to improve  the performance of spatial query processing,  innovative solutions for reducing this  computational cost are beginning to emerge.
610|Nonlinear optimization framework for image-based modeling on programmable graphics hardware|Graphics hardware is undergoing a change from fixed-function pipelines to more programmable organizations that resemble general-purpose stream processors. In this paper, we show that certain general algorithms, not normally associated with computer graphics, can be mapped to such designs. Specifically, we cast nonlinear optimization as a data streaming process that is well matched to modern graphics processors. Our framework is particularly well suited for solving image-based modeling problems since it can be used to represent a large and diverse class of these problems using a common formulation. We successfully apply this approach to two distinct image-based modeling problems: light field mapping approximation and fitting the Lafortune model to spatial bidirectional reflectance distribution functions. Comparing the performance of the graphics hardware implementation to a CPU implementation, we show more than 5-fold improvement.
611|Generic Mesh Refinement on GPU|Many recent publications have shown that a large variety of computation involved in computer graphics can be  moved from the CPU to the GPU, by a clever use of vertex or fragment shaders. Nonetheless there is still one  kind of algorithms that is hard to translate from CPU to GPU: mesh refinement techniques. The main reason  for this, is that vertex shaders available on current graphics hardware do not allow the generation of additional  vertices on a mesh stored in graphics hardware. In this paper, we propose a general solution to generate mesh  refinement on GPU. The main idea is to define a generic refinement pattern that will be used to virtually create  additional inner vertices for a given polygon. These vertices are then translated according to some procedural  displacement map defining the underlying geometry (similarly, the normal vectors may be transformed according  to some procedural normal map). For illustration purpose, we use a tesselated triangular pattern, but many other  refinement patterns may be employed. To show its flexibility, the technique has been applied on a large variety  of refinement techniques: procedural displacement mapping, as well as more complex techniques such as curved  PN-triangles or ST-meshes.
612|A cache-efficient sorting algorithm for database and data mining computations using graphics processors|We present a fast sorting algorithm using graphics processors (GPUs) that adapts well to database and data mining applications. Our algorithm uses texture mapping and blending functionalities of GPUs to implement an efficient bitonic sorting network. We take into account the communication bandwidth overhead to the video memory on the GPUs and reduce the memory bandwidth requirements. We also present strategies to exploit the tile-based computational model of GPUs. Our new algorithm has a memoryefficient data access pattern and we describe an efficient instruction dispatch mechanism to improve the overall sorting performance. We have used our sorting algorithm to accelerate join-based queries and stream mining algorithms. Our results indicate up to an order of magnitude improvement over prior CPU-based and GPU-based sorting algorithms. 1
613|Computer vision signal processing on graphics processing units|In some sense, computer graphics and computer vision are inverses of one another. Special purpose computer vision hardware is rarely found in typical mass-produced personal computers, but graphics processing units (GPUs) found on most personal computers, often exceed (in number of transistors as well as in compute power) the capabilities of the Central Processing Unit (CPU). This paper shows speedups attained by using computer graphics hardware for implementation of computer vision algorithms by efficiently mapping mathematical operations of computer vision onto modern computer graphics architecture. As an example computer vision algorithm, we implement a real–time projective camera motion tracking routine on modern, GeForce FX class GPUs. Algorithms are implemented using OpenGL and the nVIDIA Cg fragment shaders. Trade–offs between computer vision requirements and GPU resources are discussed. Algorithm implementation is examined closely, and hardware bottlenecks are addressed to examine the performance of GPU architecture for computer vision. It is shown that significant speedups can be achieved, while leaving the CPU free for other signal processing tasks. Applications of our work include wearable, computer mediated reality systems that use both computer vision and computer graphics, and require realtime processing with low–latency and high throughput provided by modern GPUs. 1.
614|Fast and reliable collision culling using graphics hardware|Figure 1: Tree with falling leaves: In this scene, leaves fall from the tree and undergo non-rigid motion. They collide with other leaves and branches. The environment consists of more than 40K triangles and 150 leaves. Our algorithm, FAR, can compute all the collisions in about 35 msec per time step. We present a reliable culling algorithm that enables fast and accurate collision detection between triangulated models in a complex environment. Our algorithm performs fast visibility queries on the GPUs for eliminating a subset of primitives that are not in close proximity. To overcome the accuracy problems caused by the limited viewport resolution, we compute the Minkowski sum of each primitive with a sphere and perform reliable 2.5D overlap tests between the primitives. We are able to achieve more effective collision culling as compared to prior object-space culling algorithms. We integrate our culling algorithm with CULLIDE [8] and use it to perform reliable GPU-based collision queries at interactive rates on all types of models, including non-manifold geometry, deformable models, and breaking objects.
615|Solving the Euler Equations on Graphics Processing Units |Abstract. The paper describes how one can use commodity graphics cards (GPUs) as a high-performance parallel computer to simulate the dynamics of ideal gases in two and three spatial dimensions. The dynamics is described by the Euler equations, and numerical approximations are computed using state-of-the-art high-resolution finite-volume schemes. These schemes are based upon an explicit time discretisation and are therefore ideal candidates for parallel implementation. 1
616|An in-depth look at computer performance growth|Abstract — It is a common belief that computer performance growth is over 50 % annually, or that performance doubles every 18-20 months. By analyzing publicly available results from the SPEC integer (CINT) benchmark suites, we conclude that this was true between 1985 and 1996 – the early years of the RISC paradigm. During the last 7.5 years (1996-2004), however, performance growth has slowed down to 41%, with signs of a continuing decline. Meanwhile, clock frequency has improved with about 29 % annually. The improvement in clock frequency was enabled both by an annual device speed scaling of 20 % as well as by longer pipelines with a lower gate-depth in each stage. This paper takes a fresh look at – and tries to remove the confusion about – performance scaling that exists in the computer architecture community. I.
617|Fast Interpolated Cameras by combining a GPU based Plane Sweep with a Max-Flow Regularisation Algorithm|The paper presents a method for the high speed calculation of crude depth maps. Performance and applicability are illustrated for view interpolation based on two input video streams, but the algorithm is perfectly amenable to multi-camera environments. First a 
618|Kohonen Feature Mapping through Graphics Hardware|This work describes the utilization of the inherent parallelism of commonly available hardware graphics accelerators for the realization of the Kohonen feature map. The result is an essential reduction of computing time compared to standard software implementations.  Keywords. Kohonen feature map, computer graphics, hardware, OpenGL , frame buffer. 1 Introduction  The Kohonen feature map (KFM) [3] is a particular kind of an artificial neural network (ANN) model, which consists of one layer of n-dimensional units  (neurons). They are fully connected with the network input. Additionally, there exist lateral connections through which a topological structure is imposed. For the standard model, the topology is a regular two-dimensional map instantiated by connections between each unit and its direct neighbors. The KFM is used for unsupervised learning tasks [2]. Through n-dimensional training samples, the units organize in a way that they match the distribution of samples in their n-dimensi...
619|  A Relational Debugging Engine for the Graphics Pipeline |  We present a new, unified approach to debugging graphics software. We propose a representation of all graphics state over the course of program execution as a relational database, and produce a query-based framework for extracting, manipulating, and visualizing data from all stages of the graphics pipeline. Using an SQLbased query language, the programmer can establish functional relationships among all the data, linking OpenGL state to primitives to vertices to fragments to pixels. Based on the Chromium library, our approach requires no modification to or recompilation of the program to be debugged, and forms a superset of many existing techniques for debugging graphics software.
620|A graphics hardware accelerated algorithm for nearest neighbor search|Abstract. We present a GPU algorithm for the nearest neighbor search, an important database problem. The search is completely performed using the GPU: No further post-processing using the CPU is needed. Our experimental results, using large synthetic and real-world data sets, showed that our GPU algorithm is several times faster than its CPU version. 1
621|Toward real time fractal image compression using graphics hardware|Abstract. In this paper, we present a parallel fractal image compression using the programmable graphics hardware. The main problem of fractal compression is the very high computing time needed to encode images. Our implementation exploits SIMD architecture and inherent parallelism of recently graphic boards to speed-up baseline approach of fractal encoding. The results we present are achieved on cheap and widely available graphics boards. 1
622|Application of the Two-Sided Depth Test to CSG Rendering|Shadow mapping is a technique for doing real-time shadowing. Recent work has shown that shadow mapping hardware can be used as a second depth test in addition to the z-test. In this paper, we explore the computational power provided by this second depth test by examining the problem of rendering objects described as CSG (Constructive Solid Geometry) expressions. We provide an algorithm that asymptotically improves the number of rendering passes required to display a CSG object by a factor of n by exploiting the two-sided depth test. Interestingly, a matching lower bound can be proved demonstrating that our algorithm is optimal.
623|Hardware Based Wavelet Transformations|Abstract Many filtering and feature extraction algorithms usewavelet or related multiscale representations of volume data for edge detection and processing. Due tothe computational complexity of these approaches no interactive visualization of the extraction process ispossible nowadays. Using the hardware of modern graphics workstations for wavelet decomposition andreconstruction is a first important step for removing lags in the visualization cycle. 1 Introduction Feature extraction has been proven to be a usefulutility for segmentation and registration in volume visualization [6, 14]. Many edge detectionalgorithms used in this step employ wavelets or related basis functions for the internal represen-tation of the volume. Additionally, wavelets can be used for fast volume visualization [4] usingthe Fourier rendering approach [7, 13]. Wavelet decomposition and reconstruction isusually implemented by applying multiple convolution and down- / up-sampling steps to thevolume data. The convolution steps will not scale with new computer hardware as well aspure computational problems, as they are already mainly memory-bound. When using typ-ical tensor-product wavelets the complete volume data has to be accessed three times for eachwavelet filtering step.
624|Efficient 3D Audio Processing with the GPU|Introduction  Audio processing applications are among the most computeintensive and often rely on additional DSP resources for realtime performance. However, programmable audio DSPs are in general only available to product developers. Professional audio boards with multiple DSPs usually support specific effects and products while consumer &#034;game-audio&#034; hardware still only implements fixed-function pipelines which evolve at a rather slow pace.  The widespread availability and increasing processing power of GPUs could offer an alternative solution. GPU features, like multiply-accumulate instructions or multiple execution units, are similar to those of most DSPs [3]. Besides, 3D audio rendering applications require a significant number of geometric calculations, which are a perfect fit for the GPU. Our feasibility study investigates the use of GPUs for efficient audio processing.  GPU-accelerated audio rendering  We consider a combination of two simple operations commonly used for 3D audio
625|MANOCHA D.: Efficient relational database management using graphics processors|We present algorithms using graphics processing units (GPUs) to efficiently perform database management queries. Our algorithms use efficient data memory representations and storage models on GPUs to perform fast database computations. We present relational database algorithms that successfully exploit the high memory bandwidth and the inherent parallelism available in GPUs. We implement these algorithms on commodity GPUs and compare their performance with optimized CPU-based algorithms. We show that the GPUs can be used as a co-processor to accelerate many database and data mining queries. 1.
627|Random Early Detection Gateways for Congestion Avoidance|This paper presents Random Early Detection (RED) gate-ways for congestion avoidance in packet-switched networks. The gateway detects incipient congestion by com-puting the average queue size. The gateway could notify connections of congestion either by dropping packets ar-riving at the gateway or by setting a bit in packet headers. When the average queue size exceeds a preset threshold,the gateway drops or marks each arriving packet with a certain probability, where the exact probability is a func-tion of the average queue size. RED gateways keep the average queue size low while allowing occasional bursts of packets in the queue. During congestion, the probability that the gateway notifies a particular connection to reduce its window is roughly proportional to that connection&#039;s share of the bandwidth throughthe gateway. RED gateways are designed to accompany a transport-layer congestion control protocol such as TCP.The RED gateway has no bias against bursty traffic and avoids the global synchronization of many connectionsdecreasing their window at the same time. Simulations of a TCP/IP network are used to illustrate the performance of RED gateways. 
628|A Reliable Multicast Framework for Light-weight Sessions and Application Level Framing|This paper... reliable multicast framework for application level framing and light-weight sessions. The algorithms of this framework are efficient, robust, and scale well to both very large networks and very large sessions. The framework has been prototype in wb, a distributed whiteboard application, and has been extensively tested on a global scale with sessions ranging from a few to more than 1000 participants. The paper describes the principles that have guided our design, including the 1P multicast group delivery model, an end-to-end, receiver-based model of reliability, and the application level framing protocol model. As with unicast communications, the performance of a reliable multicast delivery algorithm depends on the underlying topology and operational environment. We investigate that dependence via analysis and simulation, and demonstrate an adaptive algorithm that uses the results of previous loss recovery events to adapt the control parameters used for future loss recovery. Whh the adaptive algorithm, our reliable multicast delivery algorithm provides good performance over a wide range of underlying topologies. 
630|Link-Sharing and Resource Management Models for Packet Networks| This paper discusses the use of link-sharing mechanisms in packet networks and presents algorithms for hierarchical link-sharing. Hierarchical link-sharing allows multiple agencies, protocol families, or traflic types to share the bandwidth on a tink in a controlled fashion. Link-sharing and real-time services both require resource management mechanisms at the gateway. Rather than requiring a gateway to implement separate mechanisms for link-sharing and real-time services, the approach in this paper is to view link-sharing and real-time service requirements as simultaneous, and in some respect complementary, constraints at a gateway that can be implemented with a unified set of mechanisms. White it is not possible to completely predict the requirements that might evolve in the Internet over the next decade, we argue that controlled link-sharing is an essential component that can provide gateways with the flexibility to
631|Analysis, Modeling and Generation of Self-Similar VBR Video Traffic|We present a detailed statistical analysis of a 2-hour long empirical sample of VBR video. The sample was obtained by applying a simple intraframe video compression code to an action movie. The main findings of our analysis are (1) the tail behavior of the marginal bandwidth distribution can be accurately described using &#034;heavy-tailed&#034; distributions (e.g., Pareto); (2) the autocorrelation of the VBR video sequence decays hyperbolically (equivalent to long-range dependence) and can be modeled using self-similar processes. We combine our findings in a new (non-Markovian) source model for VBR video and present an algorithm for generating synthetic traffic. Trace-driven simulations show that statistical multiplexing results in significant bandwidth efficiency even when long-range dependence is present. Simulations of our source model show long-range dependence and heavy-tailed marginals to be important components which are not accounted for in currently used VBR video traffic models. 1 I...
632| An Architecture for Wide-Area Multicast Routing |Existing multicast routing mechanisms were intended for use within regions where a group is widely represented or bandwidth is universally plentiful. When group members, and senders to those group members, are distributed sparsely across a wide area, these schemes are not efficient; data packets or membership report information are occasionally sent over many links that do not lead to receivers or senders, respectively. Wehave developed a multicast routing architecture that efficiently establishes distribution trees across wide area internets, where many groups will be sparsely represented. Efficiency is measured in terms of the state, control message processing, and data packet processing, required across the entire network in order to deliver data packets to the members of the group. Our Protocol Independent Multicast (PIM) architecture: (a) maintains the traditional IP multicast service model of receiver-initiated membership; (b) can be configured to adapt to different multicast group and network characteristics; (c) is not dependent on a specific unicast routing protocol; and (d) uses soft-state mechanisms to adapt to underlying network conditions and group dynamics. The robustness, flexibility, and scaling properties of this architecture make it well suited to large heterogeneous inter-networks.
633|vic: A Flexible Framework for Packet Video|The deployment of IP Multicast has fostered the development of a suite of applications, collectively known as the MBone tools, for real-time multimedia conferencingover the Internet. Two of these tools --- nv from Xerox PARC and ivs from INRIA --- provide video transmission using softwarebased codecs. We describe a new video tool, vic, that extends the groundbreaking work of nv and ivs with a more flexible system architecture. This flexibility is characterized by network layer independence, support for hardware-based codecs, a conference coordination model, an extensible user interface, and support for diverse compression algorithms. We also propose a novel compression scheme called &#034;IntraH. 261&#034;. Created as a hybrid of the nv and ivs codecs, IntraH. 261 provides a factor of 2-3 improvement in compression gain over the nv encoder (6 dB of PSNR) as well as a substantial improvement in run-time performance over the ivs H.261 coder. KEYWORDS Conferencing protocols; digital video; image ...
634|Scalable Feedback Control for Multicast Video Distribution in the Internet|We describe a mechanism for scalable control of multicast continuous media streams. The mechanism uses a novel probing mechanism to solicit feedback information in a scalable manner and to estimate the number of receivers. In addition, it separates the congestion signal from the congestion control algorithm, so as to cope with heterogeneous networks. This mechanism has been implemented in the IVS videoconference system using options within RTP to elicit information about the quality of the video delivered to the receivers. The H.261 coder of IVS then uses this information to adjust its output rate, the goal being to maximize the perceptual quality of the image received at the destinations while minimizing the bandwidth used by the video transmission. We find that our prototype control mechanism is well suited to the Internet environment. Furthermore, it prevents video sources from creating congestion in the Internet. Experiments are underway to investigate how the scalable probing mech...
635|The synchronization of periodic routing messages|Abstract — The paper considers a network with many apparently-independent periodic processes and discusses one method by which these processes can inadvertent Iy become synchronized. In particular, we study the synchronization of periodic routing messages, and offer guidelines on how to avoid inadvertent synchronization. Using simulations and analysis, we study the process of synchronization and show that the transition from unsynchronized to synchronized traffic is not one of gradual degradation but is instead a very abrupt ‘phase transition’: in general, the addition of a single router will convert a completely unsynchronized traffic stream into a completely synchronized one. We show that synchronization can be avoided by the addition of randomization to the tra~c sources and quantify how much randomization is necessary. In addition, we argue that the inadvertent synchronization of periodic processes is likely to become an increasing problem in computer networks.
636|On Traffic Phase Effects in Packet-Switched Gateways|this paper we define the notion of traffic phase in a packet-switched network and describe how phase differences between competing traffic streams can be the dominant factor in relative throughput. Drop Tail gateways in a TCP/IP network with strongly periodic traffic can result in systematic discrimination against some connections. We demonstrate this
637|Bottleneck flow control|Abstract-The problem of optimally choosing message rates for users of a store-and-forward network is analyzed. Multiple users sharing the links of the network each attempt to adjust their message rates to achieve an ideal network operating point or an “ideal tradeoff point between high throughput and low delay. ” Each user has a fixed path or virtual circuit. In this environment, a basic definition of “ideal delay-throughput tradeoff ” is given and motivated. This definition concentrates on a fair allocation of network resources at network bottlenecks. This “ideal policy ” is implemented via a decentralized algorithm that achieves the unique set of optimal throughputs. All sharers constrained by the same bottleneck are treated fairly by being assigned equal throughputs. A generalized definition of ideal tradeoff is then introduced to provide more flexibility in the choice of message rates. With this definition, the network may accommodate users with different types of message traffic. A transformation technique reduces the problem of optimizing this performance measure to the problem of optimizing the basic measure. V I.
638|Issues With Multicast Video Distribution in Heterogeneous Packet Networks|he video delivered to some of participants will be of low quality (since packet losses and/or delays will be high on some branches of the tree). The second case is safest, but it results in disturbing the bulk of the participants.  In practice, it is not clear how to choose an adequate value for this fraction. In IVS, we set the fraction to a few percent [2, 3]. Ideally, however, the source should be able to single out the parts of the multicast tree that experience congestion. In order not to disturb the bulk of participants in the tree, these branches should be treated separately. Two possible solutions include video gateways, and using some form of layered coding.  Video gateways or layered coding schemes are not new ideas. Our contribution is to identify and discuss the issues associated with using these techniques in the Internet. We illustrate our points with the H.261[10] software coder of IVS.  2 Video gateways  Video gateways take 
639|Optimistic Strategies for Large-Scale Dissemination of Multimedia Information|We are investigating alternative transport protocol strategies for realizing large scale dissemination services across a wide area network. Communication requirements of such applications are distinct from those based on conventional client-server interactions. Conventional flow and error control methods based on the  retransmissions-with-timeout paradigm are not appropriate for such applications. Instead, we are interested in using optimistic flow and error control strategies that take into account application-specific error tolerance and media rates of multimedia applications. This paper describes transport level policies that use a combination of redundant transmissions, rate-based flow control, and selective feedback from receivers. A simulation-based performance evaluation demonstrates that relatively simple techniques succeed well in meeting the QOS requirements of a multimedia multicast and in scaling to hundreds of recipients. 1 Introduction  Our research is motivated by the co...
640|SEDA: An Architecture for Well-Conditioned, Scalable Internet Services|  We propose a new design for highly concurrent Internet services, whichwe call the staged event-driven architecture (SEDA). SEDA is intended
641|The click modular router| Click is a new software architecture for building flexible and configurable routers. A Click router is assembled from packet processing modules called elements. Individual elements implement simple router functions like packet classification, queueing, scheduling, and interfacing with network devices. A router configuration is a directed graph with elements at the vertices; packets flow along the edges of the graph. Configurations are written in a declarative language that supports user-defined abstractions. This language is both readable by humans and easily manipulated by tools. We present language tools that optimize router configurations and ensure they satisfy simple invariants. Due to Click’s architecture and language, Click router configurations are modular and easy to extend. A standards-compliant Click IP router has sixteen elements on its forwarding path. We present extensions to this router that support dropping policies, fairness among flows, quality-of-service, and
642|Oceanstore: An architecture for global-scale persistent storage|OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development. 1
643|Freenet: A Distributed Anonymous Information Storage and Retrieval System|We describe Freenet, an adaptive peer-to-peer network application  that permits the publication, replication, and retrieval of data  while protecting the anonymity of both authors and readers. Freenet operates  as a network of identical nodes that collectively pool their storage  space to store data files and cooperate to route requests to the most  likely physical location of data. No broadcast search or centralized location  index is employed. Files are referred to in a location-independent  manner, and are dynamically replicated in locations near requestors and  deleted from locations where there is no interest. It is infeasible to discover  the true origin or destination of a file passing through the network,  and difficult for a node operator to determine or be held responsible for  the actual physical contents of her own node.
644|Cluster-Based Scalable Network Services|This paper has benefited from the detailed and perceptive comments of our reviewers, especially our shepherd Hank Levy. We thank Randy Katz and Eric Anderson for their detailed readings of early drafts of this paper, and David Culler for his ideas on TACC&#039;s potential as a model for cluster programming. Ken Lutz and Eric Fraser configured and administered the test network on which the TranSend scaling experiments were performed. Cliff Frost of the UC Berkeley Data Communications and Networks Services group allowed us to collect traces on the Berkeley dialup IP network and has worked with us to deploy and promote TranSend within Berkeley. Undergraduate researchers Anthony Polito, Benjamin Ling, and Andrew Huang implemented various parts of TranSend&#039;s user profile database and user interface. Ian Goldberg and David Wagner helped us debug TranSend, especially through their implementation of the rewebber
645|Flash: An efficient and portable Web server|This paper presents the design of a new Web server architecture called the asymmetric multiprocess event-driven (AMPED) architecture, and evaluates the performance of an implementation of this architecture, the Flash Web server. The Flash Web server combines the high performance of single-process event-driven servers on cached workloads with the performance of multi-process and multithreaded servers on disk-bound workloads. Furthermore, the Flash Web server is easily portable since it achieves these results using facilities available in all modern operating systems. The performance of different Web server architectures is evaluated in the context of a single implementation in order to quantify the impact of a server&#039;s concurrency architecture on its performance. Furthermore, the performance of Flash is compared with two widely-used Web servers, Apache and Zeus. Results indicate that Flash can match or exceed the performance of existing Web servers by up to 50 % across a wide range of real workloads. We also present results that show the contribution of various optimizations embedded in Flash.  
646|The Design and Implementation of an Operating System to Support Distributed Multimedia Applications|Support for multimedia applications by general purpose computing platforms has been the subject of considerable research. Much of this work is based on an evolutionary strategy in which small changes to existing systems are made. The approach adopted here is to start ab initio with no backward compatibility constraints. This leads to a novel structure for an operating system. The structure aims to decouple applications from one another and to provide multiplexing of all resources, not just the CPU, at a low level. The motivation for this structure, a design based on the structure, and its implementation on a number of hardware platforms is described. I. Introduction G ENERAL purpose multimedia computing platforms should endow text, images, audio and video with equal status: interpreting an audio or video stream should not be a privileged task of special functions provided by the operating system, but one of ordinary user programs. Support for such processing on a platform on which ot...
647|Making Paths Explicit in the Scout Operating System|This paper makes a case for paths as an explicit abstraction in operating system design. Paths provide a unifying infrastructure for several OS mechanisms that have been introduced in the last several years, including fbufs, integrated layer processing, packet classifiers, code specialization, and migrating threads. This paper articulates the potential advantages of a path-based OS structure, describes the specific path architecture implemented in the Scout OS, and demonstrates the advantages in a particular application domain---receiving, decoding, and displaying MPEG-compressed video. 1 Introduction  Layering is a fundamental structuring technique with a long history in system design. From early work on layered operating systems and network architectures [12, 32], to more recent advances in stackable systems [27, 15, 14, 26], layering has played a central role in managing complexity, isolating failure, and enhancing configurability. This paper describes a complementary, but equally f...
648|Application performance and flexibility on Exokernel systems|The exokernel operating system architecture safely gives untrusted software efficient control over hardware and software resources by separating management from protection. This paper describes an exokernel system that allows specialized applications to achieve high performance without sacrificing the performance of unmodified UNIX programs. It evaluates the exokernel architecture by measuring end-to-end application performance on Xok, an exokernel for Intel x86-based computers, and by comparing Xok’s performance to the performance of two widely-used 4.4BSD UNIX systems (Free-BSD and OpenBSD). The results show that common unmodified UNIX applications can enjoy the benefits of exokernels: applications either perform comparably on Xok/ExOS and the BSD UNIXes, or perform significantly better. In addition, the results show that customized applications can benefit substantially from control over their resources (e.g., a factor of eight for a Web server). This paper also describes insights about the exokernel approach gained through building three different exokernel systems, and presents novel approaches to resource multiplexing. 1
651|On the Duality of Operating System Structures|Because the original of the following paper by Lauer and Needham is not
652|Scalable, Distributed Data Structures for Internet Service Construction|This paper presents a new persistent data management layer designed to simplify cluster-based Internet service construction. This self-managing layer, called a distributed data structure (DDS), presents a conventional single-site data structure interface to service authors, but partitions and replicates the data across a cluster. We have designed and implemented a distributed hash table DDS that has properties necessary for Internet services (incremental scaling of throughput and data capacity, fault tolerance and high availability, high concurrency, consistency, and durability). The hash table uses two-phase commits to present a coherent view of its data across all cluster nodes, allowing any node to service any task. We show that the distributed hash table simplies Internet service construction by decoupling service-specic logic from the complexities of persistent, consistent state management, and by allowing services to inherit the necessary service properties from the DDS rather ...
653|Using Control Theory to Achieve Service Level Objectives In Performance Management|A widely used approach to achieving service level objectives for a target  system (e.g., an email server) is to add a controller that manipulates  the target system&#039;s tuning parameters. We describe a methodology for  designing such controllers for software systems that builds on classical  control theory. The classical approach proceeds in two steps: system  identification and controller design. In system identification, we construct  mathematical models of the target system. Traditionally, this has been  based on a first-principles approach, using detailed knowledge of the target  system. Such models can be di#cult to build, and too complex to validate,  use, and maintain. In our methodology, a statistical (ARMA) model is fit  to historical measurements of the target being controlled. These models  are easier to obtain and use and allow us to apply control-theoretic design  techniques to a larger class of systems. When applied to a Lotus Notes  groupware server, we obtain model fits with R    no lower than 75% and  as high as 98%.
654|Connection Scheduling in Web Servers|Under high loads, a Web server may be servicing manyhundreds of connections concurrently. In traditional
655|Defending against Denial of Service Attacks in Scout|We describe a two-dimensional architecture for defending against denial of service attacks. In one dimension, the architecture accounts for all resources consumed by each I/O path in the system; this accounting mechanism is implemented as an extension to the path object in the Scout operating system. In the second dimension, the various modules that define each path can be configured in separate protection domains; we implement hardware enforced protection domains, although other implementations are possible. The resulting system---which we call Escort---is the first example of a system that simultaneously does end-to-end resource accounting (thereby protecting against resource based denial of service attacks where principals can be identified) and supports multiple protection domains (thereby allowing untrusted modules to be isolated from each other). The paper describes the Escort architecture and its implementation in Scout, and reports a collection of experiments that measure the c...
656|A Prototype Implementation of Archival Intermemory|An Archival Intermemory solves the problem of highly survivable digital data storage in the spirit of the Internet. In this paper we describe a prototype implementation of Intermemory, including an overall system architecture and implementations of key system components. The result is a working Intermemory that tolerates up to 17 simultaneous node failures, and includes a Web gateway for browser-based access to data. Our work demonstrates the basic feasibility of Intermemory and represents significant progress towards a deployable system.
657|Scalable kernel performance for Internet servers under realistic loads|UNIX Internet servers with an event-driven architecture often perform poorly under real workloads, even if they perform well under laboratory benchmarking conditions. We investigated the poor performance of event-driven servers. We found that the delays typical in wide-area networks cause busy servers to manage a large number of simultaneous connections. We also observed that the select system call implementation in most UNIX kernels scales poorly with the number of connections being managed by a process. The UNIX algorithm for allocating file descriptors also scales poorly. These algorithmic problems lead directly to the poor performance of event-driven servers.  We implemented scalable versions of the select system call and the descriptor allocation algorithm. This led to an improvement of up to 58% in Web proxy and Web server throughput, and dramatically improved the scalability of the system.  
658|Using Cohort Scheduling to Enhance Server Performance|A server application is commonly organized as a collection of concurrent threads, each of which executes the code necessary to process a request. This software architecture, which causes frequent control transfers between unrelated pieces of code, decreases instruction and data locality, and consequently reduces the effec- tiveness of hardware mechanisms such as caches, TLBs, and branch predictors. Numerous measurements demonstrate this effect in server applications, which often utilize only a fraction of a modern processor&#039;s computational throughput.
659|Kernel mechanisms for service differentiation in overloaded Web servers|We have implemented these mechanisms in AIX 5.0. Through numerous experiments we demonstrate their effectiveness in achieving the desired degree of service differentiation during overload. We also show that the kernel mechanisms are more efficient and scalable than application level controls implemented in the Web server.
660|ASHs: Application-Specific Handlers for High-Performance Messaging|Application-specific safe message handlers (ASHs) are designed to provide applications with hardware-level network performance. ASHs are user-written code fragments that safely and efficiently execute in the kernel in response to message arrival. ASHs can direct message transfers (thereby eliminating copies) and send messages (thereby reducing send-response latency). In addition, the ASH system provides support for dynamic integrated layer processing (thereby eliminating duplicate message traversals) and dynamic protocol composition (thereby supporting modularity). ASHs provide this high degree of flexibility while still providing network performance as good as, or (if they exploit application-specific knowledge) even better than, hard-wired in-kernel implementations. A combination of user-level microbenchmarks and end-to-end system measurements using TCP demonstrate the benefits of the ASH system.
661|Techniques for Developing and Measuring High-Performance Web Servers over ATM Networks|High-performance Web servers are essential to meet the growing demands of the Internet and large-scale intranets. Satisfying these demands requires a thorough understanding of key factors affecting Web server performance. This paper presents empirical analysis illustrating how dynamic and static adaptivity can enhance Web server performance. Two research contributions support this conclusion.  First, the paper presents results from a comprehensive empirical study of Web servers (such as Apache, Netscape Enterprise, PHTTPD, Zeus, and JAWS) over high-speed ATM networks. This study illustrates their relative performance and precisely pinpoints the server design choices that cause performance bottlenecks. We found that once network and disk I/O overheads are reduced to negligible constant factors, the main determinants of Web server performance are its protocol processing path and concurrency strategy. Moreover, no single strategy performs optimally for all load conditions and traffic type...
662|Scheduling computations on a software-based router|ABSTRACT Recent efforts to add new services to the Internet have increased the interest in software-based routers that are easy to extend and evolve. This paper describes our experiences implementing a software-based router, with a particular focus on the main difficulty we encountered: how to schedule the router&#039;s CPU cycles. The scheduling decision is complicated by the desire to differentiate the level of service for different packet flows, which leads to two fundamental conflicts: (1) assigning processor shares in a way that keeps the processes along the forwarding path in balance while meeting QoS promises, and (2) adjusting the level of batching in a way that minimizes overhead while meeting QoS promises. 1.
663|Virtualization Considered Harmful: OS Design Directions for Well-Conditioned Services|We argue that existing OS designs are ill-suited for the needs of Internet service applications. These applications demand massive concurrency (supporting a large number of requests per second) and must be well-conditioned to load (avoiding degradation of performance and predictability when demand exceeds capacity). The transparency and virtualization provided by existing operating systems leads to limited concurrency and lack of control over resource usage. We claim that Internet services would be far better supported by operating systems by reconsidering the role of resource virtualization. We propose a new design for server applications, the staged event-driven architecture (SEDA). In SEDA, applications are constructed as a set of eventdriven  stages separated by queues. We present the SEDA architecture and its consequences for operating system design. 1. 
664|The 1999 Southern California seismic network bulletin|together with the Caltech Seismological Laboratory, perates a network of more than 350 remote seismometers in southern California called the Southern California Seismic Network
666|Statecharts: A Visual Formalism For Complex Systems|We present a broad extension of the conventional formalism of state machines and state diagrams, that is relevant to the specification and design of complex discrete-event systems, such as multi-computer real-time systems, communication protocols and digital control units. Our diagrams, which we call statecharts, extend conventional state-transition diagrams with essentially three olements, dealing, respectively, with the notions of hierarchy, concurrency and communication. These transform the language of state diagrams into a highly structured&#039; and economical description language. Statecharts are thus compact and expressive--small diagrams can express complex behavior--as well as compositional and modular. When coupled with the capabilities of computerized graphics, statecharts enable viewing the description at different levels of detail, and make even very large specifications manageable and comprehensible. In fact, we intend to demonstrate here that statecharts counter many of the objections raised against conventional state diagrams, and thus appear to render specification by diagrams an attractive and plausible approach. Statecharts can be used either as a stand-alone behavioral description or as part of a more general design methodology that deals also with the system&#039;s other aspects, such as functional decomposition and data-flow specification. We also discuss some practical experience that was gained over the last three years in applying the statechart formalism to the specification of a particularly complex system.
667|On the Criteria To Be Used in Decomposing Systems into Modules|This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a “modularization ” is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.
669|Foundations for the Study of Software Architecture|The purpose of this paper is to build the foundation for software architecture. We first develop an intuition for software architecture by appealing to several well-established architectural disciplines. On the basis of this intuition, we present a model of software architec-ture that consists of three components: elements, form, and rationale. Elements are either processing, data, or connecting elements. Form is defined in terms of the properties of, and the relationships among, the elements-- that is, the constraints on the elements. The ratio-nale provides the underlying basis for the architecture in terms of the system constraints, which most often derive from the system:requirements. We discuss the compo-nents of the model in the context of both architectures and architectural styles and present an extended exam-ple to illustrate some important architecture and style considerations. We conclude by presenting some of the benefits of our approach to software architecture, sum-marizing our contributions, and relating our approach to other current work.  
670|The 4+1 view model of architecture|The 4+1 View Model organizes a description of a software architecture using five concurrent views, each of which
addresses a specific set of concerns. Architects capture their design decisions in four views and use the fifth view to illustrate and validate them.
671|Specifying Distributed Software Architectures|There is a real need for clear and sound design specifications of distributed systems at the architectural level. This is the level of the design which deals with the high-level organisation of computational elements and the interactions between those elements. The paper presents the Darwin notation for specifying this high-level organisation. Darwin is in essence a declarative binding language which can be used to define hierarchic compositions of interconnected components. Distribution is dealt with orthogonally to system structuring. The language supports the specification of both static structures and dynamic structures which may evolve during execution. The central abstractions managed by Darwin are components and services. Services are the means by which components interact. In addition to its use in specifying the architecture of a distributed system, Darwin has an operational semantics for the elaboration of specifications such that they may be used at runtime to di...
672|The design and implementation of hierarchical software systems with reusable components|We present a domain-independent model of hierarchical software system design and construction that is based on interchangeable software components and largescale reuse. The model unifies the conceptualizations of two independent projects, Genesis and Avoca, that are successful examples of software component/building-block technologies and domain modeling. Building-block technologies exploit large-scale reuse, rely on open architecture software, and elevate the granularity of programming to the subsystem level. Domain modeling formalizes the similarities and differences among systems of a domain. We believe our model is a blue-print for achieving software component technologies in many domains.
673|The X Window System|The X Window System, Version 11, is the standard window system on Linux and UNIX systems. X11, designed in 1987, was “state of the art ” at that time. From its inception, X has been a network transparent window system in which X client applications can run on any machine in a network using an X server running on any display. While there have been some significant extensions to X over its history (e.g. OpenGL support), X’s design lay fallow over much of the 1990’s. With the increasing interest in open source systems, it was no longer sufficient for modern applications and a significant overhaul is now well underway. This paper describes revisions to the architecture of the window system used in a growing fraction of desktops and embedded systems 1
674|Specification and analysis of system architecture using Rapide|  Rapide is an event-based concurrent, object-oriented language specifically designed for prototyping system architectures. Two principle design goals are (1) to provide constructs for defining executable prototypes of architectures, and (2) to adopt an execution model in which the concurrency, synchronization, dataflow, and timing properties of a prototype are explicitly represented. This paper describes the partially ordered event set (poset) execution model and outlines with examples some of the event-based features for defining communication architectures and relationships between architectures. Various features of Rapide are illustrated by excerpts from a prototype of the X/Open distributed transaction processing reference architecture.
675|A Formal Approach to Software Architecture|As software systems become more complex, the overall system structure---or software architecture---becomes a central design problem. A system&#039;s architecture provides a model of the system that suppresses implementation detail, allowing the architect to concentrate on the analyses and decisions that are most crucial to structuring the system to satisfy its requirements.  Unfortunately, current representations of software architecture are informal and ad hoc. While architectural concepts are often embodied in infrastructure to support specific architectural styles and in the initial conceptualization of a system configuration, the lack of an explicit, independently-characterized architecture or architectural style significantly limits the benefits of software architectural design in current practice.  In this dissertation, I show that an Architecture Description Language based on a formal, abstract model of system behavior can provide a practical means of describing and analyzing softwar...
676|Abstractions for Software Architecture and Tools to Support Them|Architectures for software use rich abstractions and idioms to describe system components, the nature of interactions among the components, and the patterns that guide the composition  of components into systems. These abstractions are higher-level than the elements usually  supported by programming languages and tools. They capture packaging and interaction issues  as well as computational functionality. Well-established (if informal) patterns guide architectural design of systems. We sketch a model for defining architectures and present an implementation of the basic level of that model. Our purpose is to support the abstractions  used in practice by software designers. The implementation provides a testbed for experiments  with a variety of system construction mechanisms. It distinguishes among different  types of components and different ways these components can interact. It supports abstract  interactions such as data flow and scheduling on the same footing as simple procedure...
677|Correct Architecture Refinement|A method is presented for the stepwise refinement of an abstract architecture into a relatively correct lower-level architecture that is intended to implement it. A refinement step involves the application of a predefined refinement pattern that provides a routine solution to a standard architectural design problem. A pattern contains an abstract architecture schema and a more detailed schema intended to implement it. The two schemas usually contain very different architectural concepts (from different architectural styles). Once a refinement pattern is proven correct, instances of it can be used without proof in developing specific architectures. Individual refinements are compositional, permitting incremental development and local reasoning. A special correctness criterion is defined for the domain of software architecture, as well as an accompanying proof technique. A useful syntactic form of correct composition is defined. The main points are illustrated by means of familiar archit...
678|Exploiting Style in Architectural Design Environments|As the design of software architectures emerges as a discipline within software engineering, it will become increasingly important to support architectural description and analysis with tools and environments. In this paper we describe a system for developing architectural design environments that exploit architectural styles to guide software architects in producing specific systems. The primary contributions of this research are: (a) a generic object model for representing architectural designs; (b) the characterization of architectural styles as specializations of this object model; and (c) a toolkit for creating an open architectural design environment from a description of a specific architectural style. We use our experience in implementing these concepts to illustrate how style-oriented architectural design raises new challenges for software support environments. 
679|A Field Guide to Boxology: Preliminary Classification of Architectural Styles for Software Systems|Software architects use a number of commonly-recognized “styles” to guide their design of system structures. Each of these is appropriate for some classes of problems, but none is suitable for all problems. How, then, does a software designer choose an architecture suitable for the problem at hand? Two kinds of information are required: (1) careful discrimination among the candidate architectures and (2) design guidance on how to make appropriate choices. Here we support careful discrimination with a preliminary classification of styles. We use a two-dimensional classification strategy with control and data issues as the dominant organizing axes. We position the major styles within this space and use finer-grained discriminations to elaborate variations on the styles. This provides a framework for organizing design guidance, which we partially flesh out with rules of thumb.
680|Using Style to Understand Descriptions of Software Architecture|The software architecture of most systems is described informally and diagrammatically. In order for these descriptions to be meaningful at all, figures are understood by interpreting the boxes and lines in specific, conventionalized ways [5]. The imprecision of these interpretations has a number of limitations. In this paper we consider these conventionalized interpretations as architectural styles and provide a formal framework for their uniform definition. In addition to providing a template for precisely defining new architectural styles, this framework allows for the proof that the notational constraints on a style are sufficient to guarantee the meanings of all described systems and provides a unified semantic base through which different stylistic interpretations can be compared.
681|Paradigms for process interaction in distributed programs|Distributed computations are concurrent programs in which processes communicate by message passing. Such programs typically execute on network architectures such as networks of workstations ordistributed memory parallel machines (i. e, multicomputers such ashypercubes). Several paradigms—examples or models—for process interaction
682|Formalizing design spaces: Implicit invocation mechanisms |An important goal of software engineering is to exploit commonalities in system design in order to reduce the complexity of building new systems, support largescale reuse, and provide automated assistance for system development. A significant roadblock to accomplishing this goal is that common properties of systems are poorly understood. In this paper we argue that formal specification can help solve this problem. A formal definition of a design framework can identify the common properties of a family of systems and make clear the dimensions of specialization. New designs can then be built out of old ones in a principled way, at reduced cost to designers and implementors. To illustrate these points, we present a formalization of a system integration technique called implicit invocation. We show how many previously unrelated systems can be viewed as instances of the same underlying framework. Then we briefly indicate how the formalization allows us to reason about certain properties of those systems as well as the relationships between different systems. 1
683|Software Requirements Negotiation and Renegotiation Aids: A Theory-W Based Spiral Approach|A major problem in requirements engineering is obtaining requirements that address the concerns of multiple stakeholders. An approach to such a problem is the Theory-W based Spiral Model. One key element of this model is stakeholder collaboration and negotiation to obtain win-win requirements. This paper focuses on the problem of developing a support system for such a model. In particular it identifies needs and capabilities required to address the problem of negotiation and renegotiation that arises when the model is applied to incremental requirements engineering. The paper formulates elements of the support system, called WinWin, for providing such capabilities. These elements were determined by experimenting with versions of WinWin and understanding their merits and deficiencies. The key elements of WinWin are described and their use in incremental requirements engineering are demonstrated, using an example renegotiation scenario from the domain of software engineering environments...
684|Reconciling the Needs of Architectural Description with Object-Modelling Notations|Abstract. Complex software systems require expressive notations for representing their software architectures. Two competing paths have emerged. One is to use a specialized notation for architecture – or architecture description language (ADL). The other is to adapt a general-purpose modeling notation, such as UML. The latter has a number of benefits, including familiarity to developers, close mapping to implementations, and commercial tool support. However, it remains an open question as to how best to use object-oriented notations for architectural description, and, indeed, whether they are sufficiently expressive, as currently defined. In this paper we take a systematic look at these questions, examining the space of possible mappings from ADLs into object notations. Specifically, we describe (a) the principle strategies for representing architectural structure in UML; (b) the benefits and limitations of each strategy; and (c) aspects of architectural description that are intrinsically difficult to model in UML using the strategies. 1
685|Using object-oriented typing to support architectural design in the C2 style|Abstract-- Software architectures enable large-scale software development. Component reuse and substitutability, two key aspects of large-scale development, must be planned for during software design. Object-oriented (OO) type theory supports reuse by structuring inter-component relationships and verifying those relationships through type checking in an architecture definition language (ADL). In this paper, we identify the issues and discuss the ramifications of applying OO type theory to the C2 architectural style. This work stems from a series of experiments that were conducted to investigate component reuse and substitutability in C2. We also discuss the limits of applicability of OO typing to C2 and how we addressed them in the C2 ADL. 1
686|Describing Software Architecture with UML|: This paper describes our experience using UML, the Unified Modeling Language, to describe the software architecture of a system. We found that it works well for communicating the static structure of the architecture: the elements of the architecture, their relations, and the variability of a structure. These static properties are much more readily described with it than the dynamic properties. We could easily describe a particular sequence of activities, but not a general sequence. In addition, the ability to show peer-to-peer communication is missing from UML.  Keywords: software architecture, UML, architecture descriptions, multiple views  1. INTRODUCTION  UML, the Unified Modeling Language, is a standard that has wide acceptance and will likely become even more widely used. Although its original purpose was for detailed design, its ability to describe elements and the relations between them makes it potentially applicable much more broadly. This paper describes our experience usin...
687|Software Interconnection Models|We present a formulation of interconnection models and present the unit and syntactic models --- the primary models used for managing the evolution of large software systems. We discuss various tools that use these models and evaluate how well these models support the management of system evolution. We then introduce the semantic interconnection model. The semantic interconnection model incorporates the advantages of the unit and syntactic interconnection models and provides extremely useful extensions to them. By refining the grain of interconnections to the level of semantics (that is, to the predicates that define aspects of behavior) we provide tools that are better suited to manage the details of evolution in software systems and that provide a better understanding of the implications of changes. We do this by using the semantic interconnection model to formalize the semantics of program construction, the semantics of changes, and the semantics of version equivalence and compatibi...
688|Using tool abstraction to compose systems|paradigms support the evolution of large-scale software systems. Data abstraction eases design changes in the representation of data structures, while tool abstraction does the same with system functions. M anaging complexity and supporting evolution are two fundamental “i, problems with large-scale software systems. ’ Although modularization,. has long been accepted as the basic approach to managing complexity, as David Parnas observed nearly 20 years ago, not all modularizations are equally good at handling evolution.’ Data abstraction is a popular, important style of modularization. In this style, an abstract data type is defined by an explicit interface that specifies operations on
689|Assessing the Suitability of a Standard Design Method for Modeling Software Architectures| Software architecture descriptions are high-level models of software systems.  Most existing special-purpose architectural notations have a great deal of  expressive power but are not well integrated with common development  methods. Conversely, mainstream development methods are accessible to  developers, but lack the semantics needed for extensive analysis. In our  previous work, we described an approach to combining the advantages of  these two ways of modeling architectures. While this approach suggested a  practical strategy for bringing architectural modeling into wider use, it  introduced specialized extensions to a standard modeling notation, which  could also hamper wide adoption of the approach. This paper attempts to  assess the suitability of a standard design method &#034;as is&#034; for modeling  software architectures.  
690|Experience with a course on architectures for software systems|Abstract. As software systems grow in size and complexity their design problem extends beyond algorithms and data structures to issues of system design. This area receives little or no treatment in existing computer science curricula. Although courses about speci c systems are usually available, there is no systematic treatment of the organizations used to assemble components into systems. These issues { the software architecture level of software design { are the subject of a new course that we taught for the rst time in Spring 1992. This paper describes the motivation for the course, the content and structure of the current version, and our plans for improving the next version. 1
691|Aladdin: A Tool for Architecture-Level Dependence Analysis of Software Systems|The emergence of formal architecture description languages provides an opportunity to perform analyses at high levels of abstraction, as well as early in the development process. Previous research has primarily focused on developing techniques such as algebraic and transition-system analysis to detect component mismatches or global behavioral incorrectness. In this paper, we present Aladdin, a tool that implements chaining, a static dependence analysis technique for use with architectural descriptions. Dependence analysis has been used widely at the implementation level to aid program optimization, anomaly checking, program understanding, testing, and debugging. We investigate the definition and application of dependence analysis at the architectural level. We illustrate the utility of chaining, through the use of Aladdin, by showing how the technique can be used to answer various questions one might pose of a Rapide architecture specification. 
692|The impact of Mesa on system design|The Mesa programming language supports program modularity in ways that permit subsystems to be developed separately but to be bound together with complete type safety. Separate and explicit interface definitions provide an effective means of communication, both between programs and between programmers. A configuration language describes the organization of a system and controls the scopes of interfaces. These facilities have had a profound impact on the way we design systems and Organize development projects. This paper reports our recent experience with Mesa, particularly its use in the development of an operating system. It illustrates techniques for designing interfaces, for using the interface language as a specification language, and for organizing a ~ystem to achieve the practical benefits of program modularity without sacrificing strict type-checking.
693|A Survey of active network Research|Active networks are a novel approach to network architecture in which the switches of the network perform customized computations on the messages flowing through them. This approach is motivated by both lead user applications, which perform user-driven computation at nodes within the network today, and the emergence of mobile code technologies that make dynamic network service innovation attainable. In this paper, we discuss two approaches to the realization of active networks and provide a snapshot of the current research issues and activities. Introduction – What Are Active Networks? In an active network, the routers or switches of the network perform customized computations on the messages flowing through them. For example, a user of an active network could send a “trace ” program to each router and arrange for the program to be executed when their packets are processed. Figure 1 illustrates how the routers of an IP
694|Safe Kernel Extensions Without Run-Time Checking |Abstract This paper describes a mechanism by which an operating system kernel can determine with certainty that it is safe to execute a binary supplied by an untrusted source. The kernel first defines a safety policy and makes it public. Then, using this policy, an application can provide binaries in a special form called proof-carrying code, or simply PCC. Each PCC binary contains, in addition to the native code, a formal proof that the code obeys the safety policy. The kernel can easily validate the proof without using cryptography and without consulting any external trusted entities. If the validation succeeds, the code is guaranteed to respect the safety policy without relying on run-time checks. The main practical difficulty of PCC is in generating the safety proofs. In order to gain some preliminary experience with this, we have written several network packet filters in hand-tuned DEC Alpha assembly language, and then generated PCC binaries for them using a special prototype assembler. The PCC binaries can be executed with no run-time overhead, beyond a one-time cost of 1 to 3 milliseconds for validating the enclosed proofs. The net result is that our packet filters are formally guaranteed to be safe and are faster than packet filters created using Berkeley Packet Filters, Software Fault Isolation, or safe languages such as Modula-3.
695|Improving TCP/IP performance over wireless networks|TCP is a reliable transport protocol tuned to perform well in traditional networks made up of links with low bit-error rates. Networks with higher bit-error rates, such as those with wireless links and mobile hosts, violate many of the assumptions made by TCP, causing degraded end-to-end performance. In tbis paper, we describe the design and implementation of a simple protocol, called the snoop protocol, that improves TCP performance in wireless networks. The protocol modifies network-layer software mainly at a base station and preserves end-to-end TCP semantics. The main idea of the protocol is to cache packets at the base station and perform local retransmissions across the wireless link. We have implemented the snoop protocol on a wireless testbed consisting of IBM ThinkPad laptops and i486 base
696|An Application Level Video Gateway|The current model for multicast transmission of video over the Internet assumes that a fixed average bandwidth is uniformly present throughout the network. Consequently, sources limit their transmission rates to accommodate the lowest bandwidth links, even though high-bandwidth connectivity might be available to many of the participants. We propose an architecture where a video transmission can be decomposed into multiple sessions with different bandwidth requirements using an application-level gateway. Our video gateway transparently connects pairs of sessions into a single logical conference by manipulating the data and control information of the video streams. In particular, the gateway performs bandwidth adaptation through transcoding and rate-control. We describe an efficient algorithm for transcoding Motion-JPEG to H.261 that runs in real-time on standard workstations. By making the Real-time Transport Protocol (RTP) an integral component of our architecture, the video gateway in...
697|An Architecture for Active Networking|Active networking offers a change in the usual network paradigm: from passive carrier of bits to a more general computation engine. The implementation of such a change is likely to enable radical new applications that cannot be foreseen today. Large-scale deployment, however, involves significant challenges in interoperability, security, and scalability. In this paper we define an active networking architecture in which user control the invocation of pre-defined, network-based functions through control information in packet headers. After defining our active networking architecture, we consider a problem (namely, network congestion) that may benefit in the near-term from active networking, and thus may help justify migration to this new paradigm. Given an architecture allowing applications to exercise some control over network processing, the bandwidth allocated to each application&#039;s packets can be reduced in a manner that is tailored to the application, rather than being applied gener...
698|`C: A Language for High-Level, Efficient, and Machine-independent Dynamic Code Generation|Dynamic code generation allows specialized code sequences to be crafted using runtime information. Since this  information is by definition not available statically, the use of dynamic code generation can achieve performance  inherently beyond that of static code generation. Previous attempts to support dynamic code generation have been  low-level, expensive, or machine-dependent. Despite the growing use of dynamic code generation, no mainstream  language provides flexible, portable, and efficient support for it. We describe

699|Towards Programmable Networks|Intermediate nodes (e.g., routers, switches) of current networks, in contrast with end nodes (e.g., PCs workstations), are vertically integrated closed systems. Their functions, mostly implemented by embedded software, are rigidly built-in by intermediate nodes vendors. Vendors must follow designs dictated by slow and intractable standard committees rather than pursue rapid introduction of innovative costeffective technologies. There is thus a need for new technologies that would enable programming intermediate nodes with the same simplicity of programming end-nodes. This paper describes the NetScript project, pursuing agent-based middleware for programming functions of intermediate network nodes. Delegated agents are used to deploy functions in intermediate nodes. The NetScript programming language provides means to script processing of packet streams; it is particularly suitable to program routing, packet analyzers or signalling functions. This paper describes an architecture for pro...
700|The ACTIVE IP Option|In this paper, we discuss our work on an active network architecture in which passive packets are replaced with active capsules --- encapsulated program fragments that are executed at each switch they traverse. This approach allows application-specific processing to be injected into the network. The accessibility of computation and storage &#034;within&#034; the network provides a substrate that can be tailored to build global applications, including those that invoke customized multicast and merge processing. We describe an extension to the IP options mechanism that supports the embedding of program fragments in datagrams and the evaluation of these fragments as they traverse the Internet. The active option provides a generic approach to the extension of the IP network service.
701|Protocol boosters|This paper describes a new methodology for protocol design, using incremental construction of the protocol from elements called “protocol boosters ” on an as-needed basis. Protocol boosters allow: (1) dynamic protocol customization to heterogeneous environments, and (2) rapid protocol evolution. Unlike alternative adaptation approaches, such as link layer, conversion, and termination protocols, protocol boosters are both robust (end-to-end protocol messages are not modified) and maximize efficiency (does not replicate the functionality of the end-to-end protocol). We give examples of error and congestion control boosters and give initial results from booster implementations. 1
702|Liquid Software: A New Paradigm for Networked Systems|This paper introduces the idea of dynamically moving functionality in a network---between clients and servers, and between hosts at the edge of the network and nodes inside the network. At the heart of moving functionality is the ability to support mobile code---code that is not tied to any single machine, but instead can easily move from one machine to another. Mobile code has been studied mostly for application-level code. This paper explores its use for all facets of the network, and in a much more general way. Issues of efficiency, interface design, security, and resource allocation, among others, are addressed. We use the term liquid software to describe the complete picture---liquidsoftware is an entire infrastructure for dynamically moving functionality throughout a network. We expect liquid software to enble new paradigms, such as active networks that allow users and applications to customize the network by interjecting code into it. Department of Computer Science The Univers...
703|Designing an Academic Firewall: Policy, Practice, . . .|Corporate network firewalls are well-understood and are becoming commonplace. These firewalls establish a security perimeter that aims to block (or heavily restrict) both incoming and outgoing network communication. We argue that these firewalls are neither effective nor appropriate for academic or corporate research environments needing to maintain information security while still supporting the free exchange of ideas. In this paper, we present the Stanford University Research Firewall (SURF), a network firewall design that is suitable for a research environment. While still protecting information and computing resources behind the firewall, this firewall is less restrictive of outward information flow than the traditional model; can be easily deployed; and can give internal users the illusion of unrestricted e-mail, anonymous FTP, and WWW connectivity to the greater Internet. Our experience demonstrates that an adequate firewall for a research environment can be constructed for minim...
704|The process group approach to reliable distributed computing|The difficulty of developing reliable distributed softwme is an impediment to applying distributed computing technology in many settings. Expeti _ with the Isis system suggests that a structured approach based on virtually synchronous _ groups yields systems that are substantially easier to develop, exploit sophisticated forms of cooperative computation, and achieve high reliability. This paper reviews six years of resemr,.hon Isis, describing the model, its impl_nentation challenges, and the types of applicatiom to which Isis has been appfied. 1 In oducfion One might expect the reliability of a distributed system to follow directly from the reliability of its con-stituents, but this is not always the case. The mechanisms used to structure a distributed system and to implement cooperation between components play a vital role in determining how reliable the system will be. Many contemporary distributed operating systems have placed emphasis on communication performance, overlooking the need for tools to integrate components into a reliable whole. The communication primitives supported give generally reliable behavior, but exhibit problematic semantics when transient failures or system configuration changes occur. The resulting building blocks are, therefore, unsuitable for facilitating the construction of systems where reliability is impo/tant. This paper reviews six years of research on Isis, a syg_,,m that provides tools _ support the construction of reliable distributed software. The thesis underlying l._lS is that development of reliable distributed software can be simplified using process groups and group programming too/_. This paper motivates the approach taken, surveys the system, and discusses our experience with real applications.
706|Transis: A Communication Sub-System for High Availability|This paper describes Transis, a communication sub-system for high availability. Transis is a transport layer package that supports a variety of reliable multicast message passing services between processors. It provides highly tuned multicast and control services for scalable systems with arbitrary topology. The communication domain comprises of a set of processors that can initiate multicast messages to a chosen subset. Transis delivers them reliably and maintains the  membership of connected processors automatically, in the presence of arbitrary communication delays, of message losses and of processor failures and joins. The contribution of this paper is in providing an aggregate definition of communication and control services over broadcast domains. The main benefit is the efficient implementation of these services using the broadcast capability. In addition, the membership algorithm has a novel approach in handling partitions and remerging; in allowing the regular flow of messages...
707|Fail-Stop Processors: An Approach to Designing Fault-Tolerant Computing Systems|This paper was originally submitted to ACM Transactions on Programming Languages and Systems. The responsible editor was Susan L. Graham. The authors and editor kindly agreed to transfer the paper to the ACM Transactions on Computer Systems
708|Memory access buffering in multiprocessors|In highly-pipelined machines, instructions and data are prefetched and buffered in both the processor and the cache. This is done to reduce the average memory access la-tency and to take advantage of memory interleaving. Lock-up free caches are designed to avoid processor blocking on a cache miss. Write buffers are often included in a pipelined machine to avoid processor waiting on writes. In a shared memory multiprocessor, there are more advantages in buffering memory requests, since each memory access has to traverse the memory- processor interconnection and has to compete with memory requests issued by different processors. Buffering, however, can cause logical problems in multipro-cessors. These problems are aggravated if each processor has a private memory in which shared writable data may be present, such as in a cache-based system or in a system with a distributed global memory. In this paper, we analyze the benefits and problems associated with the buffering of memory requests in shared memory multiprocessors. We show that the logical problem of buffering is directly related to the problem of synchronization. A simple model is presented to evaluate the performance improvement result-ing from buffering. 1.
709|Using Process Groups to Implement Failure Detection in Asynchronous Environments|Agreement on the membership of a group of processes in a distributed system is a basic problem that arises in a wide range of applications. Such groups occur when a set of processes co-operate to perform some task, share memory, monitor one another, subdivide a computation, and so forth. In this paper we discuss the Group Membership Problem as it relates to failure detection in asynchronous, distributed systems. We present a rigorous, formal specification for group membership under this interpretation. We then present a solution for this problem that improves upon previous work.
710|An Efficient Reliable Broadcast Protocol|Many distributed and parallel applications can make good use of  broadcast communication. In this paper we present a (software) protocol  that simulates reliable broadcast, even on an unreliable network. Using  this protocol, application programs need not worry about lost messages.  Recovery of communication failures is handled automatically and transparently  by the protocol. In normal operation, our protocol is more  efficient than previously published reliable broadcast protocols. An initial  implementation of the protocol on 10 MC68020 CPUs connected by a 10  Mbit/sec Ethernet performs a reliable broadcast in 1.5 msec.   
711|The Distributed V Kernel and its Performance for Diskless Workstations|The distributed V kernel is a message-oriented kernel that provides uniform local and network interprocess communication. It is primarily being used in an environment of diskless workstations connected by a high-speed local network to a set of file servers. We describe a performance evaluation of the kernel, with particular emphasis on the cost of network file access. Our results show that over a local network: 1. Diskless workstations can access remote files with minimal performance penalty. 2. The V message facility can be used to access remote files at comparable cost to any well-tuned specialized file access protocol. We conclude that it is feasible to build a distributed system with all network communication using the V message facility even when most of the network nodes have no secondary storage. 1.
712|Highly-available distributed services and fault-tolerant distributed garbage collection|This paper describes two techniques that are only loosely related. The first is a method for constructing a highly available service for use in a distributed system. The service presents its clients with a consistent view of its state, but the view may contain old information. Clients can indicate how recent the information must be. The method can be used in any application in which the property of interest is stable: once the property becomes true, it remains true forever. The paper also describes a fault-tolerant garbage collection method for a distributed heap. The method is practical and efficient. Each computer that contains part of the heap does local garbage collection independently, using whatever algorithm it chooses, and without needing to communicate with the other computers that contain parts of the heap. The highly available central service is used to store information about inter.computer references. 1.
713|Designing Application Software in Wide Area Network Settings|T(&#039;VED FOR PUBLIC RELEASE
714|Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks|Dryad is a general-purpose distributed execution engine for coarse-grain data-parallel applications. A Dryad applica-tion combines computational “vertices ” with communica-tion “channels ” to form a dataflow graph. Dryad runs the application by executing the vertices of this graph on a set of available computers, communicating as appropriate through files, TCP pipes, and shared-memory FIFOs. The vertices provided by the application developer are quite simple and are usually written as sequential programs with no thread creation or locking. Concurrency arises from Dryad scheduling vertices to run simultaneously on multi-ple computers, or on multiple CPU cores within a computer. The application can discover the size and placement of data at run time, and modify the graph as the computation pro-gresses to make efficient use of the available resources. Dryad is designed to scale from powerful multi-core sin-gle computers, through small clusters of computers, to data centers with thousands of computers. The Dryad execution engine handles all the difficult problems of creating a large distributed, concurrent application: scheduling the use of computers and their CPUs, recovering from communication or computer failures, and transporting data between ver-tices.
715|PVM: A Framework for Parallel Distributed Computing|The PVM system is a programming environment for the development and execution of large concurrent or parallel applications that consist of many interacting, but relatively independent, components. It is intended to operate on a collection of heterogeneous computing elements interconnected by one or more networks. The participating processors may be scalar machines, multiprocessors, or special-purpose computers, enabling application components to execute on the architecture most appropriate to the algorithm. PVM provides a straightforward and general interface that permits the description of various types of algorithms (and their interactions), while the underlying infrastructure permits the execution of applications on a virtual computing environment that supports multiple parallel computation models. PVM contains facilities for concurrent, sequential, or conditional execution of application components, is portable to a variety of architectures, and supports certain forms of error dete...
716|Cilk: An Efficient Multithreaded Runtime System|Cilk (pronounced &#034;silk&#034;) is a C-based runtime system for multithreaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the &#034;work&#034; and &#034;critical-path length&#034; of a Cilk computation can be used to model performance accurately. Consequently, a Cilk programmer can focus on reducing the computation&#039;s work and critical-path length, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of &#034;fully strict&#034; (well-structured) programs, the Cilk scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The Cilk
717|Distributed Computing in Practice: The Condor Experience|Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational grid. In this chapter, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course traveled by research ideas as they grow into production systems.
719|Interpreting the Data: Parallel Analysis with Sawzall |Very large data sets often have a flat but regular structure and span multiple disks and machines. Examples include telephone call records, network logs, and web document repositories. These large data sets are not amenable to study using traditional database techniques, if only because they can be too large to fit in a single relational database. On the other hand, many of the analyses done on them can be expressed using simple, easily distributed computations: filtering, aggregation, extraction of statistics, and so on. We present a system for automating such analyses. A filtering phase, in which a query is expressed using a new procedural programming language, emits data to an aggregation phase. Both phases are distributed over hundreds or even thousands of computers. The results are then collated and saved to a file. The design—including the separation into two phases, the form of the programming language, and the properties of the aggregators—exploits the parallelism inherent in having data and computation distributed across many machines. 1
720|Programming Parallel Algorithms|In the past 20 years there has been treftlendous progress in developing and analyzing parallel algorithftls. Researchers have developed efficient parallel algorithms to solve most problems for which efficient sequential solutions are known. Although some ofthese algorithms are efficient only in a theoretical framework, many are quite efficient in practice or have key ideas that have been used in efficient implementations. This research on parallel algorithms has not only improved our general understanding ofparallelism but in several cases has led to improvements in sequential algorithms. Unf:ortunately there has been less success in developing good languages f:or prograftlftling parallel algorithftls, particularly languages that are well suited for teaching and prototyping algorithms. There has been a large gap between languages
721|Programmable stream processors|The Imagine Stream Processor is a single-chip programmable media processor with 48 parallel ALUs. At 400 MHz, this translates to a peak arithmetic rate of 16 GFLOPS on single-precision data and 32 GOPS on 16bit fixed-point data. The scalability of Imagine’s programming model and architecture enable it to achieve such high arithmetic rates. Imagine executes applications that have been mapped to the stream programming model. The stream model decomposes applications into a set of computation kernels that operate on data streams. This mapping exposes the inherent locality and parallelism in the application, and Imagine exploits the locality and parallelism to provide a scalable architecture that supports 48 ALUs on a single chip. This paper presents the Imagine architecture and programming model in the first half, and explores the scalability of the Imagine architecture in the second half. 1.
722|Accelerator: using data parallelism to program GPUs for general-purpose uses|GPUs are difficult to program for general-purpose uses. Programmers can either learn graphics APIs and convert their applications to use graphics pipeline operations or they can use stream programming abstractions of GPUs. We describe Accelerator, a system that uses data parallelism to program GPUs for general-purpose uses instead. Programmers use a conventional imperative programming language and a library that provides only high-level data-parallel operations. No aspects of GPUs are exposed to programmers. The library implementation compiles the data-parallel operations on the fly to optimized GPU pixel shader code and API calls. We describe the compilation techniques used to do this. We evaluate the effectiveness of using data parallelism to program GPUs by providing results for a set of compute-intensive benchmarks. We compare the performance of Accelerator versions of the benchmarks against hand-written pixel shaders. The speeds of the Accelerator versions are typically within 50 % of the speeds of hand-written pixel shader code. Some benchmarks significantly outperform C versions on a CPU: they are up to 18 times faster than C code running on a CPU.
723|The CODE 2.0 Graphical Parallel Programming Language |CODE 2.0 is a graphical parallel programming system that targets the three goals of ease of use, portability, and production of efficient parallel code. Ease of use is provided by an integrated graphical/textual interface, a powerful dynamic model of parallel computation, and an integrated concept of program component reuse. Portability is approached by the declarative expression of synchronization and communication operators at a high level of abstraction in a manner which cleanly separates overall computation structure from the primitive sequential computations that make up a program. Execution efficiency is approached through a systematic class hierarchy that supports hierarchical translation refinement including special case recognition. This paper reports results obtained through experimental use of a prototype implementation of the CODE 2.0 system. CODE 2.0 represents a major conceptual advance over its predecessor systems (CODE 1.0 and CODE 1.2) in terms of the expressive power ...
724|Stream Computations Organized for Reconfigurable Execution (SCORE): Introduction and Tutorial  (2000) |A primary impediment to wide-spread exploitation of reconfigurable computing is the lack of a unifying computational model which allows application portability and longevity without sacrificing a substantial fraction of the raw capabilities. We introduce SCORE (Stream Computation Organized for Reconfigurable Execution), a streambased compute model which virtualizes reconfigurable computing resources (compute, storage, and communication) by dividing a computation up into fixed-size &#034;pages&#034; and time-multiplexing the virtual pages on available physical hardware. Consequently, SCORE applications can scale up or down automatically to exploit a wide range of hardware sizes. We hypothesize that the SCORE model will ease development and deployment of reconfigurable applications and expand the range of applications which can benefit from reconfigurable execution. Further, we believe that a well engineered SCORE implementation can be efficient, wasting little of the capabilities of the raw hardw...
725|Paralex: An Environment for Parallel Programming in Distributed Systems|Modern distributed systems consisting of powerful workstations and high-speed interconnection networks are an economical alternative to special-purpose super computers. The technical issues that need to be addressed in exploiting the parallelism inherent in a distributed system include heterogeneity, high-latency communication, fault tolerance and dynamic load balancing. Current software systems for parallel programming provide little or no automatic support towards these issues and require users to be experts in fault-tolerant distributed computing. The Paralex system is aimed at exploring the extent to which the parallel application programmer can be liberated from the complexities of distributed systems. Paralex is a complete programming environment and makes extensive use of graphics to define, edit, execute and debug parallel scientific applications. All of the necessary code for distributing the computation across a network and replicating it to achieve fault tolerance and dynamic load balancing is automatically generated by the system. In this paper we give an overview of Paralex and present our experiences with a prototype implementation.
726|Parallel and Distributed Haskells|Parallel and distributed languages specify computations on multiple processors and have a computation language to describe the algorithm, i.e. what to compute, and a coordination language to describe how to organise the computations across the processors. Haskell has been used as the computation language for a wide variety of parallel and distributed languages, and this paper is a comprehensive survey of implemented languages. We outline parallel and distributed language concepts and classify Haskell extensions using them. Similar example programs are used to illustrate and contrast the coordination languages, and the comparison is facilitated by the common computation language. A lazy language is not an obvious choice for parallel or distributed computation, and we address the question of why Haskell is a common functional computation language.
727|Highly Available, Fault-Tolerant, Parallel Dataflows|We present a technique that masks failures in a cluster to provide high availability and fault-tolerance for long-running, parallelized dataflows. We can use these dataflows to implement a variety of continuous query (CQ) applications that require high-throughput, 24x7 operation. Examples include network monitoring, phone call processing, click-stream processing, and online financial analysis. Our main contribution is a scheme that carefully integrates traditional query processing techniques for partitioned parallelism with the process-pairs approach for high availability. This delicate integration allows us to tolerate failures of portions of a parallel dataflow  without sacrificing result quality. Upon failure, our technique provides quick fail-over, and automatically recovers the lost pieces on the fly. This piecemeal recovery provides minimal disruption to the ongoing dataflow computation and improved reliability as compared to the straight-forward application of the process-pairs technique on a per dataflow basis. Thus, our technique provides the high availability necessary for critical CQ applications. Our techniques are encapsulated in a reusable dataflow operator called Flux, an extension of the Exchange that is used to compose parallel dataflows. Encapsulating the fault-tolerance logic into Flux minimizes modifications to existing operator code and relieves the burden on the operator writer of repeatedly implementing and verifying this critical logic. We present experiments illustrating these features with an implementation of Flux in the TelegraphCQ code base [8].
728|A comparison of stream-oriented high availability algorithms|Recently, significant efforts have focused on developing novel data-processing systems to support a new class of applications that commonly require sophisticated and timely processing of high-volume data streams. Early work in stream processing has primarily focused on streamoriented languages and resource-constrained, one-pass query-processing. High availability, an increasingly important goal for virtually all data processing systems, is yet to be addressed. In this paper, we first describe how the standard highavailability approaches used in data management systems can be applied to distributed stream processing. We then propose a novel stream-oriented approach that exploits the unique data-flow nature of streaming systems. Using analysis and a detailed simulation study, we characterize the performance of each approach and demonstrate that the stream-oriented algorithm significantly reduces runtime overhead at the expense of a small increase in recovery time. 1
729|U-Net: A User-Level Network Interface for Parallel and Distributed Computing|The U-Net communication architecture provides processes with a virtual view of a network interface to enable userlevel access to high-speed communication devices. The architecture, implemented on standard workstations using offthe-shelf ATM communication hardware, removes the kernel from the communication path, while still providing full protection. The model presented by U-Net allows for the construction of protocols at user level whose performance is only limited by the capabilities of network. The architecture is extremely flexible in the sense that traditional protocols like TCP and UDP, as well as novel abstractions like Active Messages can be implemented efficiently. A U-Net prototype on an 8-node ATM cluster of standard workstations offers 65 microseconds round-trip latency and 15 Mbytes/sec bandwidth. It achieves TCP performance at maximum network bandwidth and demonstrates performance equivalent to Meiko CS-2 and TMC CM-5 supercomputers on a set of Split-C benchmarks. 1
730|An Analysis of TCP Processing Overhead|networks, have been getting faster, perceived throughput at the application has not always increased accordingly. Various performance bottlenecks have been encountered, each of which has to be analyzed and corrected. One aspect of networking often suspected of contributing to low throughput is the transport layer of the protocol suite. This layer, especially in connectionless protocols, has considerable functionality, and is typically executed in software by the host processor at the end points of the network. It is thus a likely source of processing overhead. While this theory is appealing, a preliminary examination suggested to us that other aspects of networking may be a more serious source of overhead. To test this proposition, a detailed study was made of a popular transport protocol, Transmission Control Protocol (TCP) [I]. This paper provides results of that
731|Fbufs: A High-Bandwidth Cross-Domain Transfer Facility|We have designed and implemented a new operating system facility for I/O buffer management and data transfer across protection domain boundaries on shared memory machines. This facility, called fast buffers (fbufs), combines virtual page remapping with shared virtual memory, and exploits locality in I/O traffic to achieve high throughput withoutcompromising protection, security, or modularity. Its goal is to help deliver the high bandwidth afforded by emerging high-speed networks to user-level processes, both in monolithic and microkernel-based operating systems.  This paper outlines the requirements for a cross-domain transfer facility, describes the design of the fbuf mechanism that meets these requirements, and experimentally quantifies the impact of fbufs on network performance. 1 Introduction  Optimizing operations that cross protection domain boundaries has received a great deal of attention recently [2, 3]. This is because an efficient cross-domain invocation facility enables a ...
732|Dynamics of TCP Traffic over ATM Networks|We investigate the performance of TCP connections over ATM networks without ATM-level congestion control, and compare it to the performance of TCP over packet-based networks. For simulations of congested networks, the effective throughput of TCP over ATM can be quite low when cells are dropped at the congested ATM switch. The low throughput is due to wasted bandwidth as the congested link transmits cells from `corrupted&#039; packets, i.e., packets in which at least one cell is dropped by the switch. We investigate two packet discard strategies which alleviate the effects of fragmentation. Partial Packet Discard, in which remaining cells are discarded after one cell has been dropped from a packet, somewhat improves throughput. We introduce Early Packet Discard, a strategy in which the switch drops whole packets prior to buffer overflow. This mechanism prevents fragmentation and restores throughput to maximal levels.  
733|Protocol Service Decomposition for High-Performance Networking|In this paper we describe a new approach to implementing network protocols that enables them to have high performance and high flexibility, while retaining complete conformity to existing application programming interfaces. The key insight behind our work is that an application&#039;s interface to the network is distinct and separable from its interface to the operating system. We have separated these interfaces for two protocol implementations, TCP/IP and UDP/IP, running on the Mach 3.0 operating system and UNIX server. Specifically, library code in the application&#039;s address space implements the network protocols and transfers data to and from the network, while an operating system server manages the heavyweight abstractions that applications use when manipulating the network through operations other than send and receive. On DECstation 5000/200 This research was sponsored in part by the Advanced Research Projects Agency, Information Science and Technology Office, under the title &#034;Research...
734|The importance of Non-Data Touching Processing Overheads|We present detailed measurements of various processing overheads of the TCP/IP and UDP/IP protocol stacks on a DECstation 5000/200 running the Ultrix 4.2a operating system. These overheads include data-touching operations, such as the checksum computation and data movemen ~ which are well known to be major time consumers. In this stud y, we also considered overheads due to non-data touching operations, such as network buffer manipulation, protocol-specific processing, operating system functions, data structure manipulations (other than network buffers), and error checking. We show that when one considers realistic message size dktributions, where the majority of messages are small, the cumulative time consumed by the nondata touching overheads represents the majority of processing time. We assert that it will be difficult to significantly reduce the cumulative processing time due to non-data touching overheads. The goal of this study is to determine the relative importance of various processing overheads in network software, in particular, the TCP/IP and UDPAP protocol stacks. In the prrsg significant focus has been placed on maximizing throughput noting that “data
735|Distributed Network Computing over Local ATM Networks|Communication between processors has long been the bottleneck of distributed network computing. However, recent progress in switch-based high-speed Local Area Networks (LANs) may be changing this situation. Asynchronous Transfer Mode (ATM) is one of the most widely-accepted and emerging high-speed network standards which can potentially satisfy the communication needs of distributed network computing. In this paper, we investigate distributed network computing over local ATM networks. We first study the performance characteristics involving end-to-end communication in an environment that includes several types of workstations interconnected via a Fore Systems&#039; ASX-100 ATM Switch. We then compare the communication performance of four different Application Programming Interfaces (APIs). The four APIs were Fore Systems ATM API, BSD socket programming interface, Sun&#039;s Remote Procedure Call (RPC), and the Parallel Virtual Machine (PVM) message passing library. Each API represents distribute...
736|Separating Data and Control Transfer in Distributed Operating Systems|Advances in processor architecture and technology have resulted in workstations in the 100+ MIPS range. As well, newer local-area networks such as ATM promise a ten- to hundred-fold increase in throughput, much reduced latency, greater scalability, and greatly increased reliability, when compared to current LANs such as Ethernet. We believe that these new network and processor technologies will permit tighter coupling of distributed systems at the hardware level, and that distributed systems software should be designed to benefit from that tighter coupling. In this paper, we propose an alternative way of structuring distributed systems that takes advantage of a communication model based on remote network access (reads and writes) to protected memory segments. A key feature of the new structure, directly supported by the communication model, is the separation of data transfer and control transfer. This is in contrast to the structure of traditional distributed systems, which are typical...
737|Protocol Implementation Using Integrated Layer Processing|Integrated Layer Processing (ILP) is an implementation concept which &amp;quot;permit[s] the implementor the option of performing all the [data] manipulation steps in one or two integrated processing loops &amp;quot; [1]. To estimate the achievable benefits of ILP, a file transfer application with an encryption function on top of a user-level TCP has been implemented and the performance of the application in terms of throughput and packet processing times has been measured. The results show that it is possible to obtain performance benefits by integrating marshalling, encryption and TCP checksum calculation. They also show that the benefits are smaller than in simple experiments, where ILP effects have not been evaluated in a complete protocol environment. Simulations of memory access and cache hit rate show that the main benefit of ILP is reduced memory accesses rather than an improved cache hit rate. The results further show that data manipulation characteristics may significantly influence the cache behavior and the achievable performance gain of ILP. 1
738|User-space protocols deliver high performance to applications on a low-cost gb/s lan|Two important questions in high-speed networking are firstly, how to provide GbitJs networking at low cost and secondly, how to provide a flexible low-level network inter-face so that applications can control their data from the instant it arrives. We describe some work that addresses both of these ques-tions. The Jetstream Gbit/s LAN is an experimental, low-cost network interface that provides the services required by delay-sensitive traffic as well as meeting the performance needs of current applications. Jetstream is a combination of traditional shared-medium LAN technology and more recent ATM cell- and switch-based technology. Jetstream frames contain a channel identifier so that the net-work driver can immediately associate an incoming frame with its application. We have developed such a driver that enables applications to control how their data should be managed without the need to first move the data into the application’s address space. Consequently, applications can elect to read just a part of a frame and then instruct the driver to move the remainder directly to its destination. Individual channels can elect to receive frames that have failed their CRC, while applications can specify frame-drop policies on a per-channel basis. Measured results show that both kernel- and user-space pro-tocols can achieve very good throughput: applications using both TCP and our own reliable byte-stream protocol have demonstrated throughputs in excess of 200 Mbit/s. The ben-efits of running protocols in user-space are well known- the drawback has often been a severe penalty in the perform-ance achieved. In this paper we show that it is possible to have the best of both worlds. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given
739|Experience with Active Messages on the Meiko CS-2|Active messages provide a low latency communication architecture which on modern parallel machines achieves more than an order of magnitude performance improvement over more traditional communication libraries. This paper discusses the experience we gained while implementing active messages on the Meiko CS-2, and discusses implementations for similar architectures. During our work we have identified that architectures which only support efficient remote write operations (or DMA transfers as in the case of the CS-2) make it difficult to transfer both data and control as required by active messages. Traditional network interfaces avoid this problem because they have a single point of entry which essentially acts as a queue. To efficiently support active messages on modern network communication co-processors, hardware primitives are required which support this queue behavior. We overcame this problem by producing specialized code which runs on the communications co-processor and supports ...
740|Virtual-memorymapped network interfaces|In today’s multicomputers, software overhead dominates the message-passing latency cost. We designed two multicomputer network interfaces that signif~cantiy reduce this overhead. Both support vMual-memory-mapped communication, allowing user processes to communicate without expensive buffer management and without making system calls across the protection boundary separating user processes from the operating system kerneL Here we compare the two interfaces and discuss the performance trade-offs between them.
741|Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the World Wide Web|We describe a family of caching protocols for distrib-uted networks that can be used to decrease or eliminate the occurrence of hot spots in the network. Our protocols are particularly designed for use with very large networks such as the Internet, where delays caused by hot spots can be severe, and where it is not feasible for every server to have complete information about the current state of the entire network. The protocols are easy to implement using existing network protocols such as TCP/IP, and require very little overhead. The protocols work with local control, make efficient use of existing resources, and scale gracefully as the network grows. Our caching protocols are based on a special kind of hashing that we call consistent hashing. Roughly speaking, a consistent hash function is one which changes minimally as the range of the function changes. Through the development of good consistent hash functions, we are able to develop caching protocols which do not require users to have a current or even consistent view of the network. We believe that consistent hash functions may eventually prove to be useful in other applications such as distributed name servers and/or quorum systems.  
742|  Parity-Based Loss Recovery for Reliable Multicast Transmission |We investigate how FEC (Forward Error Correction) can be combined with ARQ (Automatic Repeat Request) to achieve scalable reliable multicast transmission. We consider the two scenarios where FEC is introduced as a transparent layer underneath a reliable multicast layer that uses ARQ, and where FEC and ARQ are both integrated into a single layer that uses the retransmission of parity data to recover from the loss of original data packets. Toevaluate the performance improvements due to FEC, we consider different types of loss behaviors (spatially or temporally correlated loss, homogeneous or heterogeneous loss) and loss rates for up to 10 6 receivers. Our results show that introducing FEC as a layer below ARQ can improve multicast transmission efficiency and scalability and that there are substantial additional improvements when the two are integrated.
743|IP Multicast Channels: Express Support for Large-scale Single-source Applications|In the IP multicast model, a set of hosts can be aggregated into a group of hosts with one address, to which any host can send. However, Internet TV, distance learning, file distribution and other emerging large-scale multicast applications strain the current realization of this model, which lacks a basis for charging, lacks access control, and is difficult to scale.  This paper proposes an extension to IP multicast to support the channel model of multicast and describes a specific realization called EXPlicitly REquested SingleSource (EXPRESS) multicast. In this model, a multicast  channel has exactly one explicitly designated source,  and zero or more channel subscribers. A single protocol supports both channel subscription and efficient collection of channel information such as subscriber count. We argue that EXPRESS addresses the aforementioned problems, justifying this multicast service model in the Internet.   
744|Autonet: A high-speed, self-configuring local area network using point-to-point links|Read it as an adjunct to the lectures on distributed systems, links, and switching. It gives a fairly complete description of a working highly-available switched network providing daily service to about 100 hosts. The techniques used to obtain high reliability and fault-tolerance are characteristic of many distributed systems, not just of networks. The paper also makes clear the essential role of software in modern networks.
745|Adaptive web caching: towards a new global caching architecture|An adaptive, highly scalable, and robust web caching system is needed to effectively handle the exponential growth and extreme dynamic environment of the World Wide Web. Our work presented last year sketched out the basic design of such a system. This sequel paper reports our progress over the past year. To assist caches making web query forwarding decisions, we sketch out the basic design of a URL routing framework. To assist fast searching within each cache group, we let neighbor caches share content information. Equipped with the URL routing table and neighbor cache contents, a cache in the revised design can now search the local group, and forward all missing queries quickly and efficiently, thus eliminating both the waiting delay and the overhead associated with multicast queries. The paper also presents a proposal for incremental deployment that provides a smooth transition from the currently deployed cache infrastructure to the new
746|Speculative Data Dissemination and Service to Reduce Server Load, Network Traffic and Service Time in . . .|We present two server-initiated protocols to improve the performance of distributed information systems (e.g. WWW). Our first protocol is a hierarchical data dissemination mechanism that allows information to propagate from its producers to servers that are closer to its consumers. This dissemination reduces network traffic and balances load amongst servers by exploiting geographic and temporal locality of reference properties exhibited in client access patterns. Our second protocol relies on &#034;speculative service&#034;, whereby a request for a document is serviced by sending, in addition to the document requested, a number of other documents that the server speculates will be requested inthenear future. This speculation reduces service time by exploiting the spatial locality of reference property. We present results of trace-driven simulations that quantify the attainable performance gains for both protocols.  
747|FLIP: an Internetwork Protocol for Supporting Distributed Systems|Most modern network protocols give adequate support for traditional applications such as file transfer and remote login. Distributed applications, however, have different requirements (e.g., efficient atmost-once remote procedure call even in the face of processor failures). Instead of using ad-hoc protocols to meet each of the new requirements, we have designed a new protocol, called the Fast Local Internet Protocol (FLIP), that provides a clean and simple integrated approach to these new requirements. FLIP is an unreliable message protocol that provides both point-to-point communication and multicast communication, and requires almost no network management. Furthermore, by using FLIP we have simplified higher-level protocols such as remote procedure call and group communication, and enhanced support for process migration and security. A prototype implementation of FLIP has been built as part of the new kernel for the Amoeba distributed operating system, and is in daily use. Measurements of its performance are presented. 1.
748|Seamlessly Selecting the Best Copy from Internet-Wide Replicated Web Servers|. The explosion of the web has led to a situation where a majority of the traffic on the Internet is web related. Today, practically all of the popular web sites are served from single locations. This necessitates frequent long distance network transfers of data (potentially repeatedly) which results in a high response time for users, and is wasteful of the available network bandwidth. Moreover, it commonly creates a single point of failure between the web site and its Internet provider. This paper presents a new approach to web replication, where each of the replicas resides in a different part of the network, and the browser is automatically and  transparently directed to the &#034;best&#034; server. Implementing this architecture for popular web sites will result in a better response-time and a higher availability of these sites. Equally important, this architecture will potentially cut down a significant fraction of the traffic on the Internet, freeing bandwidth for other uses. 1. Introducti...
749|Awareness and Coordination in Shared Workspaces|Awareness of individual and group activities is critical to successful collaboration and is commonly supported in CSCW systems by active, information generation mecha-nisms separate from the shared workspace. These mechanisms pena~ise information providers, presuppose rel-evance to the recipient, and make access difficult, We discuss a study of shared editor use which suggests that awareness information provided and exploited passively through the shared workspace, allows users to move smoothly between close and loose collaboration, and to assign and coordinate work dynamically. Passive awareness mechanisms promise effective support for collaboration requiring this sort of behaviour, whilst avoiding problems with active approaches.
750|Portholes: Supporting Awareness in a Distributed Work Group |We are investigating ways in which media space technologies can support distributed work groups through access to information that supports general awareness. Awareness involves knowing who is “around”, what activities are cxcurring, who is talking with whom, it provides a view of one another in the daily work environments. Awareness may lead to informal interactions, spontaneous connections, and the development of shared cultures-all important aspects of maintaining working relationships which are denied to groups distributed across multiple sites.
751|Design for conversation: lessons from Cognoter|When studying the use of Cognoter, a multi-user idea organizing tool, we noticed that users encountered unexpected communicative breakdowns. Many of these difficulties stemmed from an incorrect model of conversation implicit in the design of the software. Drawing on recent work in psychology and sociology, we were able to create a more realistic model of the situation our users faced and apply it to the system to understand the breakdowns. We discovered that users encountered difficulties coordinating their conversational actions. They also had difficulty determining that they were talking about the same objects and actions in the workspace. This work led to the redesign of the tool and to the identification of areas for further exploration.
752|DISTRIBUTED SYSTEMS|Growth of distributed systems has attained unstoppable momentum. If we better understood how to think about, analyze, and design distributed systems, we could direct their implementation with more confidence.
753|The Active Badge Location System|cation is the `pager system&#039;. In order to locate a person a signal is sent out by a central facility that addresses a particular receiver unit (beeper) and produces an audible signal. In addition, it may display a number to which the called-party should phone back (some systems allow a vocal message to be conveyed about the call-back number). It is then up to the recipient to use the conventional telephone system to call-back confirming the signal and determine the required action. Although useful in practice there are still circumstances where it is not ideal. For instance, if the called party does not reply the controller has no idea if they: 1) are in an area where the signal does not penetrate 2) have been completely out of the area for some time 3) have been too busy to reply or 4) have misheard or misread the call-back number. Moreover, in the case where there are a number of people who could respond to a crisis situation, it is not known which one is the nearest to the crisis an
756|Domain names - Implementation and Specification|This RFC describes the details of the domain system and protocol, and assumes that the reader is familiar with the concepts discussed in a companion RFC, &#034;Domain Names- Concepts and Facilities &#034; [RFC-1034]. The domain system is a mixture of functions and data types which are an official protocol and functions and data types which are still experimental. Since the domain system is intentionally extensible, new data types and experimental behavior should always be expected in parts of the system beyond the official protocol. The official protocol parts include standard queries, responses and the Internet class RR data formats (e.g., host addresses). Since the previous RFC set, several definitions have changed, so some previous definitions are obsolete. Experimental or obsolete features are clearly marked in these RFCs, and such information should be used with caution. The reader is especially cautioned not to depend on the values which appear in examples to be current or complete, since their purpose is
757|Assigned Numbers|Status of this Memo
758|Requirements for Internet Hosts -- Application and Support|This RFC is an official specification for the Internet community. It incorporates by reference, amends, corrects, and supplements the primary protocol standards documents relating to hosts. Distribution of this document is unlimited. Summary This RFC is one of a pair that defines and discusses the requirements for Internet host software. This RFC covers the application and support protocols; its companion RFC-1122 covers the communication protocol layers: link layer, IP layer, and transport layer.
759|RTP profile for audio and video conferences with minimal control|This document is an Internet-Draft. Internet-Drafts are working
760|Address Allocation for Private Internets|Status of this Memo This document specifies an Internet Best Current Practices for the Internet Community, and requests discussion and suggestions for improvements. Distribution of this memo is unlimited. 1.
761|Security mechanisms in high-level network protocols|The implications of adding security mechanisms to high-level network protocols operating in an open-system environment are analyzed. First the threats to security that may arise in such an environment are described, and then a set of goals for communications security measures is established. This is followed by a brief description of the two basic
762|Dynamic QoS Control of Multimedia Applications based on RTP|We describe a mechanism for dynamic adjustment of the bandwidth requirements of multimedia applications.  The sending application uses RTP receiver reports to compute packet loss and jitter. Based  on these metrics the congestion state seen by the receivers is determined and the bandwidth is adjusted  by a linear regulator with dead zone.  The suggested mechanism has been implemented and controls the bandwidth of the vic video conferencing  system. Currently we are evaluating the proposed algorithms by simulations and experiments on  the Internet and on our local ATM network.  1 Introduction  Multimedia applications pose unique challenges for network control and management: they offer high data rates, stringent real-time contraints, long connection durations and relatively inflexible demands on bandwidth. Due to the long connection duration, the standard cycle of QoS negotiation purely at the beginning of a session leads either to high call rejection probability at busy times or unnece...
763|Issues in Designing a Transport Protocol for Audio and Video Conferences and other. . .|This memorandum is a companion document to the current version of the RTP protocol specification draft-ietf-avt-rtp-*.ftxt,psg. It discusses protocol aspects of transporting real-time services (for example, voice or video) over packet-switched networks such as the Internet. It compares and evaluates design alternatives for a real-time transport protocol, providing rationales for the design decisions made for RTP. Also covered are issues of port assignment and multicast address allocation. An appendix provides a comprehensive glossary of terms related to multimedia conferencing. This document is a product of the Audio-Video Transport working group within the Internet Engineering Task Force. Comments are solicited and should be addressed to the working group&#039;s mailing list at rem-conf@es.net and/or the author(s).  INTERNET-DRAFT draft-ietf-avt-issues-02.ps May 9, 1994 Contents  1 Introduction 4 2 Goals 7 3 Services 9  3.1 Control and Data : : : : : : : : : : : : : : : : : : : : : : : : ...
764|Randomness Recommendations for Security|Security systems today are built on increasingly strong cryptographic algorithms that foil pattern analysis attempts. However, the security of these systems is dependent on generating secret quantities for passwords, cryptographic keys, and similar quantities. The use of pseudo-random processes to generate secret quantities can result in pseudo-security. The sophisticated attacker of these security  systems may find it easier to reproduce the environment that produced the secret quantities, searching the resulting small set of  possibilities, than to locate the quantities in the whole of the number space.
765|Security services for multimedia conferencing|Multimedia conferencing is the use of mixed media such as real-time audio, video, and groupware for group tele-collaboration. Multimedia conferencing may well become as widespread an application on network computers as electronic mail. The demand for security in multimedia conferencing is high because the users ’ expectation of being monitored is high. In this paper we discuss security services for protecting multimedia conferencing, how well these services scale to large conferences, and what
766|The modern industrial revolution, exit, and the failure of internal control systems|Since 1973 technological, political, regulatory, and economic forces have been changing the worldwide economy in a fashion comparable to the changes experienced during the nineteenth century Industrial Revolution. As in the nineteenth century, we are experiencing declining costs, increaing average (but decreasing marginal) productivity of labor, reduced growth rates of labor income, excess capacity, and the requirement for downsizing and exit. The last two decades indicate corporate internal control systems have failed to deal effectively with these changes, especially slow growth and the requirement for exit. The next several decades pose a major challenge for Western firms and political systems as these forces continue to work their way through the worldwide economy.  
767|Capitalism, Socialism and Democracy|or method of economic change and not only never is but never can be stationary.
768|Agency costs of free cash flow, corporate finance and takeovers|The interests and incentives of managers and shareholders conflict over such issues as the optimal size of the firm and the payment of cash to shareholders. These conflicts are especially severe in firms with large free cash flows—more cash than profitable investment opportunities. The theory developed here explains 1) the benefits of debt in reducing agency costs of free cash flows, 2) how debt can substitute for dividends, 3) why “diversification ” programs are more likely to generate losses than takeovers or expansion in the same line of business or liquidation-motivated takeovers, 4) why the factors generating takeover activity in such diverse activities as broadcasting and tobacco are similar to those in oil, and 5) why bidders and some targets tend to perform abnormally well prior to takeover.
769|Does corporate performance improve after mergers|Center at MIT and the Division of Research at HBS for financial support. This
770|Relative performance evaluation for chief executive officers|Measured individual performance often depends on random factors which also affect the performances of other workers in the same firm, industry, or market. In these cases, relative performance evaluation (RPE) can provide incentives while partially insulating workers from the common uncertainty. Basing pay on relative performance, however, generates incentives to sabotage the measured performance of co-workers, to collude with co-workers and shirk, and to apply for jobs with inept co-workers. RPE contracts also are less desirable when the output of co-workers is expensive to measure or in the presence of production externalities, as in the case of team production. The purpose of this paper is to review the benefits and costs of RPE and to test for the presence of RPE in one occupation where the benefits plausibly exceed the costs: toplevel management. Rewarding chief executive officers (CEOs) based on performance measured relative to the industry or market creates incentives to take actions increasing shareholder wealth while insuring executives against the vagaries of the stock and product markets that are beyond their control. We expect RPE to be a common feature of
771|The market for corporate control: The empirical evidence since 1980|Your use of the JSTOR archive indicates your acceptance of JSTOR&#039;s Terms and Conditions of Use, available at
772|Takeovers: Their causes and consequences|Your use of the JSTOR archive indicates your acceptance of JSTOR&#039;s Terms and Conditions of Use, available at
773|A Test of the Free Cash Flow Hypothesis: The Case of Bidder Returns|We develop a measure of free cash flow using Tobin’s q to distinguish between firms that hav,e good investment opportunities and those that do not. In a sample of successful tender offers, bidder returns are significantly negatively related to cash flow for low q bidders but not for high q bidders: further. the relation between cash Row and bidder returns differs significantly for low q and high q bidders. This result holds for several cash Row measures suggested in the literature and also in multivariate regressions controlling for bidder and contest-specific characteristics. 1.
774|Telnet Data Entry Terminal Option|The sender of this command REQUESTS or AGREES to send and receive subcommands to control the Data Entry Terminal. IAC WONT DET The sender of this command REFUSES to send and receive subcommands to control the Data Entry Terminal. IAC DO DET The sender of this command REQUESTS or AGREES to send and receive subcommands to control the Data Entry Terminal. IAC DONT DET The sender of this command REFUSES to send and receive subcommands to control the Data Entry Terminal. The DET option uses five classes of subcommands 1) to establish the requirements and capabilities of the application and the terminal, 2) to format the screen, and to control the 3) edit, 4) erasure, and 5) transmission functions. The subcommands that perform these functions are described below. The Network Virtual Data Entry Terminal (NVDET) The NVDET consists of a keyboard and a rectangular display. The keyboard is capable of generating all of the characters of the ASCII character set. In addition, the keyboard may possess a number of function keys which when pressed cause a FN subcommand to be sent.
775|USER ACCEPTANCE OF INFORMATION TECHNOLOGY: TOWARD A UNIFIED VIEW|Information technology (IT) acceptance research has yielded many competing models, each with different sets of acceptance determinants. In this paper, we (1) review user acceptance literature and discuss eight prominent models, (2) empirically compare the eight models and their extensions, (3) formulate a unified model that integrates elements across the eight models, and (4) empirically validate the unified model. The eight models reviewed are the theory of reasoned action, the technology acceptance model, the motivational model, the theory of planned behavior, a model combining the technology acceptance model and the theory of planned behavior, the model of PC utilization, the innovation diffusion theory, and the social cognitive theory. Using data from four organizations over a six-month period with three points of measurement, the eight models explained between 17 percent and 53 percent of the variance in user intentions to use information technology. Next, a unified model, called the Unified Theory of Acceptance and Use of Technology (UTAUT), was formulated, with four core determinants of intention and usage, and up to four moderators of key relationships. UTAUT was then tested using the original data and found to outperform the eight individual models (adjusted R 2 of 69 percent). UTAUT was then confirmed with data from two new organizations with similar
776|The theory of planned behavior|Research dealing with various aspects of * the theory of planned behavior (Ajzen, 1985, 1987) is reviewed, and some unresolved issues are discussed. In broad terms, the theory is found to be well supported by empirical evidence. Intentions to perform behaviors of different kinds can be predicted with high accuracy from attitudes toward the behavior, subjective norms, and perceived behavioral control; and these intentions, together with perceptions of behavioral control, account for considerable variance in actual behavior. Attitudes, subjective norms, and perceived behavioral control are shown to be related to appropriate sets of salient behavioral, normative, and control beliefs about the behavior, but the exact nature of these relations is still uncertain. Expectancy — value formulations are found to be only partly successful in dealing with these relations. Optimal rescaling of expectancy and value measures is offered as a means of dealing with measurement limitations. Finally, inclusion of past behavior in the prediction equation is shown to provide a means of testing the theory*s sufficiency, another issue that remains unresolved. The limited available evidence concerning this question shows that the theory is predicting behavior quite well in comparison to the ceiling imposed by behavioral reliability. © 1991 Academic Press. Inc.
777|Toward an integrative theory of training motivation: A meta-analytic path analysis of 20 years of research|This article meta-analytically summarizes the literature on training motivation, its antecedents, and its relationships with training outcomes such as declarative knowledge, skill acquisition, and transfer. Significant predictors of training motivation and outcomes included individual characteristics (e.g., locus of control, conscientiousness, anxiety, age, cognitive ability, self-efficacy, valence, job involvement) and situational characteristics (e.g., climate). Moreover, training motivation explained incremental variance in training outcomes beyond the effects of cognitive ability. Meta-analytic path analyses further showed that the effects of personality, climate, and age on training outcomes were only partially mediated by self-efficacy, valence, and job involvement. These findings are discussed in terms of their practical significance and their implications for an integrative theory of training motivation. Traditionally, training researchers have focused on the methods and settings that maximize the reaction, learning, and behavior change of trainees (Tannenbaum &amp; Yukl, 1992). This research has sought to understand the impact of training media, instructional settings, sequencing of content, and other factors on training effectiveness. However, several reviews of training research have emphasized that because the influence of these variables on individuals&#039; learning and behavior varies, research must examine how personal characteristics relate to training effectiveness (Campbell, 1988; Tannenbaum &amp; Yukl, 1992). For example, Pintrich, Cross,
778|A longitudinal field investigation of gender differences in individual technology adoption decision-making processes|This research investigated gender differences in the over-looked context of individual adoption and sustained usage of technology in the workplace using the theory of planned behavior (TPB). User reactions and technology usage behavior were stud-ied over a 5-month period among 355 workers being introduced to a new software technology application. When compared to women&#039;s decisions, the decisions of men were more strongly influ-enced by their attitude toward using the new technology. In con-trast, women were more strongly influenced by subjective norm and perceived behavioral control. Sustained technology usage behavior was driven by early usage behavior, thus fortifying the lasting influence of gender-based early evaluations of the new technology. These findings were robust across income, organiza-tion position, education, and computer self-efficacy levels. q 2000
779|Computer technology training in the workplace: a longitudinal investigation of the effect|How does a person&#039;s mood during technology training influence motivation, intentions, and, ultimately, usage of the new technol-ogy? Do these mood effects dissipate or are they sustainable over time? A repeated-measures field study (n 5 316) investigated the effect of mood on employee motivation and intentions toward using a specific computer technology at two points in time: imme-diately after training and 6 weeks after training. Actual usage behavior was assessed for 12 weeks after training. Each individ-ual was assigned to one of three mood treatments: positive, nega-tive, or control. Results indicated that there were only short-term boosts in intrinsic motivation and intention to use the technology among individuals in the positive mood intervention. However, a long-term lowering of intrinsic motivation and intention was observed among those in the negative mood condition. q 1999 Academic Press You spent a wonderful week in Hawaii. You are upbeat. You return to work for an important executive technology training program. Alternatively, you had an argument with your spouse. Your typical 20-min commute took over an hour due to bad traffic. You reach work and head straight for an important executive training program on a new computer software application. Will your mood, altered by either of these two highly plausible scenarios, affect your
780|Minimum energy mobile wireless networks revisited|Energy conservation is a critical issue in designing wireless ad hoc networks, as the nodes are powered by batteries only. Given a set of wireless network nodes, the directed weighted transmission graph Gt has an edge uv if and only if node v is in the transmission range of node u and the weight of uv is typically defined as II,,vll + c for a constant 2 &lt;_ t ~ &lt; 5 and c&gt; O. The minimum power topology Gm is the smallest subgraph of Gt that contains the shortest paths between all pairs of nodes, i.e., the union of all shortest paths. In this paper, we described a distributed position-based networking protocol to construct an enclosure graph G~, which is an approximation of Gin. The time complexity of each node u is O(min(dG ~ (u)dG ~ (u), dG ~ (u) log dG ~ (u))), where dc(u) is the degree of node u in a graph G. The space required at each node to compute the minimum power topology is O(dG ~ (u)). This improves the previous result that computes Gm in O(dG, (u) a) time using O(dGt(U) 2) spaces. We also show that the average degree dG,(u) is usually a constant, which is at most 6. Our result is first developed for stationary network and then extended to mobile networks. I.
781|Near ground wideband channel measurement|Abstract- Frequency domain channel propagation measurements in the 800-1000 MHz band have been performed with ground-lying antennas. The range of path-loss exponent and shadowing variance for indoor and outdoor environment were determined. The range of these values roughly agree with those measured for higher elevation antennas. Frequency selectivity of the RF channel was also characterized by means of determining average coherence bandwidth (CBW). It was observed that there is a relationship between CBW and distance between transmitting and receiving antennas. I.
782|Low-power directsequence spread-spectrum modem architecture for distributed wireless sensor networks|Emerging CMOS and MEMS technologies enable the implementation of a large number of wireless distributed microsensors that can be easily and rapidly deployed to form highly redundant, self-configuring, and ad hoc sensor networks. To facilitate ease of deployment, these sensors should operate on battery for extended periods of time. A particular challenge in maintaining extended battery lifetime lies in achieving communications with low power. This paper presents a directsequence spread-spectrum modem architecture that provides robust communications for wireless sensor networks while dissipating very low power. The modem architecture has been verified in an FPGA implementation that dissipates only 33 mW for both transmission and reception. The implementation can be easily mapped to an ASIC technology with an estimated power performance of less than 1 mW.
783| A Stream Enabled Routing (SER) Protocol for Sensor Networks   (2002) |As the number of communication components can be integrated into a single chip increases, the possibility of high volume but low cost sensor nodes is realizable in the near future. Each sensor node can be designed to perform a single or multiple sensing operations, e.g., detecting temperature, seismic activity, object movement, and environmental pollution. As a result, a routing protocol must provide the quality of service (QoS) needed by the sensor nodes. A new routing protocol called Stream Enabled Routing (SER) is proposed to allow the sources choose the routes based on the instruction given by the sinks. It also takes into account the available energy of the sensor nodes. Also, SER allows the sink to give new instruction to the sources without setting up another path. Sources are the sensor nodes in the sensor field that are performing the sensing task. As a result, an interactive user-to-sources communication is achieved. In addition, the routing protocol is shown mathematically to perform well in the sensor network environment.
784|Human domination of Earth’s ecosystems|Human alteration of Earth is substantial and growing. Between one-third and one-half interact with the atmosphere, with aquatic of the land surface has been transformed by human action; the carbon dioxide con- systems, and with surrounding land. Morecentration in the atmosphere has increased by nearly 30 percent since the beginning of over, land trallsformation interacts strongly the Industrial Revolution; more atmospheric nitrogen is fixed by humanity than by all with most other components of global ennatural terrestrial sources combined; more than half of all accessible surface fresh water rironmental change. is put to use by humanity; and about one-quarter of the bird species on Earth have been The measurement of land transformadriven to extinction. By these and other standards, it is clear that we live on a human- tion on a global scale is challenging; changdominated planet. es can be measured more or less straightforwardly at a eiven site, but it is difficult to aggregate these changes regionally and globallv. In contrast to analvses of human al-A11 organisms modify their environment, reasonably well quantified; all are ongoing, teraiion of the global carbon cycle, we and humans are no exceotion. As the hu- These relativelv well-documented changes cannot install instruments on a tro~ical man population has and the power of in turn entrail; further alterations to;he mountain to collect evidence of land tians-technology has expanded, the scope and f~~nctioning of the Earth system, most no- formation. Remote sensing is a most useful
785|Max-Min D-Cluster Formation in Wireless Ad Hoc Networks|An ad hoc network may be logically represented as a set of clusters. The clusterheads form a d-hop dominating set. Each node is at most d hops from a clusterhead. Clusterheads form a virtual backbone and may be used to route packets for nodes in their cluster. Previous heuristics restricted themselves to 1-hop clusters. We show that the minimum d-hop dominating set problem is NP-complete. Then we present a heuristic to form d-clusters in a wireless ad hoc network. Nodes are assumed to have non-deterministic mobility pattern. Clusters are formed by diffusing node identities along the wireless links. When the heuristic terminates, a node either becomes a clusterhead, or is at most d wireless hops away from its clusterhead. The value of d is a parameter of the heuristic. The heuristic can be run either at regular intervals, or whenever the network configuration changes. One of the features of the heuristic is that it tends to re-elect existing clusterheads even when the network configurat...
786|Scalable routing strategies for ad hoc wireless networks| In this paper, we consider a large population of mobile stations that are interconnected by a multihop wireless network. The applications of this wireless infrastructure range from ad hoc networking (e.g., collaborative, distributed computing) to disaster recovery (e.g., fire, flood, earthquake), law enforcement (e.g., crowd control, search-and-rescue), and military (automated battlefield). Key characteristics of this system are the large number of users, their mobility, and the need to operate without the support of a fixed (wired or wireless) infrastructure. The last feature sets this system apart from existing cellular systems and in fact makes its design much more challenging. In this environment, we investigate routing strategies that scale well to large populations and can handle mobility. In addition, we address the need to support multimedia communications, with low latency requirements for interactive traffic and quality-of-service (QoS) support for real-time streams (voice/video). In the wireless routing area, several schemes have already been proposed and implemented (e.g., hierarchical routing, on-demand routing, etc.). We introduce two new schemes—fisheye state routing (FSR) and hierarchical state routing (HSR)—which offer some competitive advantages over the existing schemes. We compare the performance of existing and proposed schemes via simulation. 
787|Performance of a novel self-organization protocol for wireless ad hoc sensor networks|Abstract- The paper presents an ad-hoc architecture for wireless sensor networks and other wireless sys-tems similar to them. In this class of wireless system the physical resource at premium is energy. Band-width available to the system is in excess of system requirements. The approach to solve the problem of ad-hoc network formation here is to use available bandwidth in order to save energy. The method introduced solves the problem of connecting an ad-hoc network. This algorithm gives procedures for the joint formation of a time schedule (similar to a TDMA schedule) and activation of links therein for random network topologies.This self-organization method is energy-sensitive, distributed, scalable, and able to form a connected network rapidly. I.
788|Multicluster, Mobile, Multimedia Radio Network|A multi-cluster, multi-hop packet radio network architecture for wireless adaptive mobile information systems is presented...
789|Soft Handoffs in CDMA Mobile Systems|This article presents an overview of soft handoff, an idea which is becoming quite important because of its use in the IS-95 code-division multiple access (CDMA) cellular phone standard. The benefits and disadvantages of using soft handoff over hard handoff are discussed, with most results  drawn from the available literature. The two most well-known benefits are fade margin improvement and higher uplink capacity, while disadvantages  include increased downlink interference and more complex implementation. Handoff parameter optimization is extremely important, so  various studies on the trade-offs to be considered when selecting these parameters are surveyed, from both the link quality and resource  allocation perspectives. Finally, research directions and future trends are discussed.
790|Mobility Management in Hierarchical Multihop Mobile Wireless Networks |In this paper, we consider the mobility management in large, hierarchically organized multihop wireless networks. The examples of such networks range from battlefield networks, emergency disaster relief and law enforcement etc. We present a novel network addressing architecture to accommodate mobility using a “Home Agent ” concept akin to mobile IP. The performance of the mobility management scheme is investigated through simulations. I.
791|An Overview of the C++ Programming Language|This overview of C++ presents the key design, programming, and language-technical concepts using examples to give the reader a feel for the language. C++ is a general-purpose programming language with a bias towards systems programming that supports efficient low-level computation, data abstraction, object-oriented programming, and generic programming.  
792|The Spring nucleus: A microkernel for objects|The Spring system is a distributed operating system that supports a distributed, object-oriented application framework. Each individual Spring system is based around a microkernel known as the nucleus, which is structured to support fast cross-address-space object invocations. This paper discusses the design rationale for the nucleus&#039;s IPC facilities and how they fit into the overall Spring programming model. We then describe how the internal structure of the nucleus is organized to support fast crossaddress -space calls, including some specific details and performance information on the current implementation.  
793|High-Performance Scientific Computing Using C++|Concepts from mathematics and physics often map well to object-oriented software since the original concepts are of an abstract nature. We describe our experiences with developing high-performance shock-wave physics simulation codes in C++ and discuss the software engineering issues which we have encountered. The primary enabling technology in C++ for allowed us to share software between our development groups is operator overloading for a number of &#034;numeric&#034; objects. Unfortunately, this enabling feature can also impact the efficiency of our computations. We describe the techniques we have utilized for minimizing this difficulty.  Introduction  Developers of scientific software systems are tasked to implement abstract ideas and concepts. The software implementation of algorithms and ideas from physics, mechanics and mathematics should in principle be complementary to the mathematical abstractions. Often these ideas are very naturally implemented in an object-oriented style. For example...
794|Reliable Communication in the Presence of Failures|The design and correctness of a communication facility for a distributed computer system are reported on. The facility provides support for fault-tolerant process groups in the form of a family of reliable multicast protocols that can be used in both local- and wide-area networks. These protocols attain high levels of concurrency, while respecting application-specific delivery ordering constraints, and have varying cost and performance that depend on the degree of ordering desired. In particular, a protocol that enforces causal delivery orderings is introduced and shown to be a valuable alternative to conventional asynchronous communication protocols. The facility also ensures that the processes belonging to a fault-tolerant process group will observe consistent orderings of events affecting the group as a whole, including process failures, recoveries, migration, and dynamic changes to group properties like member rankings. A review of several uses for the protocols in the ISIS system, which supports fault-tolerant resilient objects and bulletin boards, illustrates the significant simplification of higher level algorithms made possible by our approach.
795|Time, Clocks, and the Ordering of Events in a Distributed System|The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.  
796|Atomic Broadcast: From Simple Message Diffusion to Byzantine Agreement|In distributed systems subject to random communication delays and component failures, atomic broadcast can be used to implement the abstraction of synchronous replicated storage, a distributed storage that displays the same contents at every correct processor as of any clock time. This paper presents a systematic derivation of a family of atomic broadcast protocols that are tolerant of increasingly general failure classes: omission failures, timing failures, and authentication-detectable Byzantine failures. The protocols work for arbitrary point-to-point network topologies, and can tolerate any number of link and process failures up to network partitioning. After proving their correctness, we also prove two lower bounds that show that the protocols provide in many cases the best possible termination times. Keywords and phrases: Atomic Broadcast, Byzantine Agreement, Computer Network, Correctnesss, Distributed System, Failure Classification, Fault-Tolerance, Lower Bound, Real-Time Syste...
797|Maintaining Availability in Partitioned Replicated Databases|In a replicated database, a data item may have copies residing on several sites. A replica control protocol is necessary to ensure that data items with several copies behave as if they consist of a single copy, as far as users can tell. We describe a new replica control protocol that allows the accessing of data in spite of site failures and network partitioning. This protocol provides the database designer with a large degree of flexibility in deciding the degree of data availability, as well as the cost of accessing data.
798|The Anatomy of a Context-Aware Application|We describe a platform for context-aware computing which enables applications to follow mobile users as they move around a building. The platform is particularly suitable for richly equipped, networked environments. The only item a user is required to carry is a small sensor tag, which identifies them to the system and locates them accurately in three dimensions. The platform builds a dynamic model of the environment using these location sensors and resource information gathered by telemetry software, and presents it in a form suitable for application programmers. Use of the platform is illustrated through a practical example, which allows a user&#039;s current working desktop to follow them as they move around the environment.
799|ContextAware Computing Applications|This paper describes systems thatel:amine and re-actto an indi7Jidltal&#039;s changing context. Such systems can promote and mediate people&#039;s mleractlOns with de-Vices, computers, and other people, and they can help navigate unfamiliar places. We bel1eve that a lunded amount of information coveTIng a per&#039;son&#039;s proximale environment is most important for this form of com-puting since the interesting part of the world around us is what we can see, hear, and touch. In this paper we define context-aware computing, and describe four cal-egones of conteL·t-aware applications: proximate selec-tion, automatic contextual reconfiguratlOn, contexlual information and commands, and context-triggered ac-tions. fnstances of these application types ha11e been prototyped on the PARCTAB, a wireless, palm-sl.:ed computer. 1
800|The quadtree and related hierarchical data structures|A tutorial survey is presented of the quadtree and related hierarchical data structures. They are based on the principle of recursive decomposition. The emphasis is on the representation of data used in applications in image processing, computer graphics, geographic information systems, and robotics. There is a greater emphasis on region data (i.e., two-dimensional shapes) and to a lesser extent on point, curvilinear, and threedimensional data. A number of operations in which such data structures find use are examined in greater detail.
801|A New Location Technique for the Active Office|Configuration of the computing and communications systems found at home and in the workplace is a complex task that currently requires the  attention of the user. Recently, researchers have begun to examine computers that would autonomously change their functionality based on  observations of who or what was around them. By determining their context, using input from sensor systems distributed throughout the  environment, computing devices could personalize themselves to their current user, adapt their behavior according to their location, or react  to their surroundings. The authors present a novel sensor system, suitable for large-scale deployment in indoor environments, which allows the  locations of people and equipment to be accurately determined. We also describe some of the context-aware applications that might make use  of this fine-grained location information.
802|A Distributed Location System for the Active Office|Computer and commmunications systems continue to proliferate... This article describes the technology of a system for locating people and equipment, and the design of a distributed system service supporting access to that information. The application interfaces which are made possible by, or benefit from this facility are presented.
803|A Demonstrated Optical Tracker With Scalable Work Area for Head-Mounted Display Systems|An optoelectronic head-tracking system for head-mounted displays is described. The system features a scalable work area that currently measures 10&#039; x 12&#039;, a measurement update rate of 20-100 Hz with 20-60 ms of delay, and a resolution specification of 2 mm and 0.2 degrees. The sensors consist of four head-mounted imaging devices that view infrared lightemitting diodes (LEDs) mounted in a 10&#039; x 12&#039; grid of modular 2&#039; x 2&#039; suspended ceiling panels. Photogrammetric techniques allow the head&#039;s location to be expressed as a function of the known LED positions and their projected images on the sensors. The work area is scaled by simply adding panels to the ceiling&#039;s grid. Discontinuities that occurred when changing working sets of LEDs were reduced by carefully managing all error sources, including LED placement tolerances, and by adopting an overdetermined mathematical model for the computation of head position: space resecfion by collinearity. The working system was demonstrated in the Tomorrow&#039;s Realities gallery at the ACM SIGGRAPH &#039;91 conference.
804|Matrix: A Realtime Object Identification and  Registration Method for Augmented Reality|This paper introduces a new technique for producing augmented reality systems that simultaneously identify real world objects and estimate their coordinate systems. This method utilizes a 2D matrix marker, a square shaped barcode, which can identify a large number of objects. It also acts as a landmark to register information on real world images. As a result, it costs virtually nothing to produce and attach codes on various kinds of real world objects, because the matrix code are printable. We have developed an augmented reality system based on this method, and demonstrated several potential applications.
805|Teleporting in an X Window System Environment|Teleporting is the ability to redirect a windowing environment to different computer displays. This paper describes the implementation of a teleporting system developed at Olivetti Research Laboratory (ORL). We outline two particular features of the system that make it powerful. First, it operates with existing applications, which will run without any modification. Second, it incorporates sophisticated techniques of personnel and equipment location which make it simple to use. Teleporting may represent a development in attempts to achieve a ubiquitous, personalised computing environment for all. 1 Introduction  In the near future, communication networks will make it possible to access computing services from almost anywhere in the world. In addition, the number of computers readily available within an office is increasing. We would like to allow individuals to make use of such networked computing facilities as they move from place to place, whilst retaining the familiarity of their own...
806|The Byzantine Generals Problem|Reliable computer systems must handle malfunctioning components that give conflicting information to different parts of the system. This situation can be expressed abstractly in terms of a group of generals of the Byzantine army camped with their troops around an enemy city. Communicating only by messenger, the generals must agree upon a common battle plan. However, one of more of them may be traitors who will try to confuse the others. The problem is to find an algorithm to ensure that the loyal generals will reach agreement. It is shown that, using only oral messages, this problem is solvable if and only if more than two-thirds of the generals are loyal; so a single traitor can confound two loyal generals. With unforgeable written messages, the problem is solvable for any number of generals and possible traitors. Applications of the solutions to reliable computer systems are then discussed.
807|Distributed Snapshots: Determining Global States of Distributed Systems|This paper presents an algorithm by which a process in a distributed system determines a global state of the system during a computation. Many problems in distributed systems can be cast in terms of the problem of detecting global states. For instance, the global state detection algorithm helps to solve an important class of problems: stable property detection. A stable property is one that persists: once a stable property becomes true it remains true thereafter. Examples of stable properties are “computation has terminated,”  “the system is deadlocked” and “all tokens in a token ring have disappeared.” The stable property detection problem is that of devising algorithms to detect a given stable property. Global state detection can also be used for checkpointing. 
808|The Protection of Information in Computer Systems|This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures--whether hardware or software--that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysis of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading. Glossary The following glossary provides, for reference, brief definitions for several terms as used in this paper in the context of protecting information in computers. Access The ability to make use of information stored in a computer system. Used frequently as a verb, to the horror of grammarians. Access control list A list of principals that are authorized to have access to some object. Authenticate To verify the identity of a person (or other agent external to the protection system) making a request.
809|Kerberos: An Authentication Service for Open Network Systems|In an open network computing environment, a workstation cannot be trusted to identify its users correctly to network services.  Kerberos provides an alternative approach whereby a trusted third-party authentication service is used to verify users’ identities. This paper gives an overview of the Kerberos authentication model as implemented for MIT’s Project Athena. It describes the protocols used by clients, servers, and Kerberos to achieve authentication. It also describes the management and replication of the database required.   The views of Kerberos as seen by the user, programmer, and administrator are described.  Finally, the role of Kerberos in the larger Athena picture is given, along with a list of applications that presently use Kerberos for user authentication.  We describe the addition of Kerberos authentication to the Sun Network File System as a case study for integrating Kerberos with an existing application.
810|A Resource Management Architecture for Metacomputing Systems|Metacomputing systems are intended to support remote and/or  concurrent use of geographically distributed computational resources.  Resource management in such systems is complicated by five concerns  that do not typically arise in other situations: site autonomy and heterogeneous  substrates at the resources, and application requirements for policy  extensibility, co-allocation, and online control. We describe a resource  management architecture that addresses these concerns. This architecture  distributes the resource management problem among distinct local  manager, resource broker, and resource co-allocator components and defines  an extensible resource specification language to exchange information  about requirements. We describe how these techniques have been  implemented in the context of the Globus metacomputing toolkit and  used to implement a variety of different resource management strategies.  We report on our experiences applying our techniques in a large testbed,  GUSTO, incorporating 15 sites, 330 computers, and 3600 processors.  
811|Matchmaking: Distributed Resource Management for High Throughput Computing|Conventional resource management systems use a system model to describe resources and a centralized scheduler to control their allocation. We argue that this paradigm does not adapt well to distributed systems, particularly those built to support high-throughput computing. Obstacles include heterogeneity of resources, which make uniform allocation algorithms difficult to formulate, and distributed ownership, leading to widely varying allocation policies. Faced with these problems, we developed and implemented the classified advertisement (classad) matchmaking framework, a flexible and general approach to resource management in distributed environment with decentralized ownership of resources. Novel aspects of the framework include a semi-structured data model that combines schema, data, and query in a simple but powerful specification language, and a clean separation of the matching and claiming phases of resource allocation. The representation and protocols result in a robust, scalabl...
812|BEOWULF: A Parallel Workstation For Scientific Computation|Network-of-Workstations technology is applied to the challenge of implementing very high performance workstations for Earth and space science applications. The Beowulf parallel workstation employs 16 PCbased processing modules integrated with multiple Ethernet networks. Large disk capacity and high disk to memory bandwidth is achieved through the use of a hard disk and controller for each processing module supporting up to 16 way concurrent accesses. The paper presents results from a series of experiments that measure the scaling characteristics of Beowulf in terms of communication bandwidth, file transfer rates, and processing performance. The evaluation includes a computational fluid dynamics code and an N-body gravitational simulation program. It is shown that the Beowulf architecture provides a new operating point in performance to cost for high performance workstations, especially for file transfers under favorable conditions.  1 INTRODUCTION  Networks Of Workstations, or NOW [?] ...
813|Dealing with disaster: Surviving misbehaved kernel extensions|Today’s extensible operating systems allow applications to modify kernel behavior by providing mechanisms for application code to run in the kernel address space. The advantage of this approach is that it provides improved application flexibility and performance; the disadvantage is that buggy or malicious code can jeopardize the integrity of the kernel. It has been demonstrated that it is feasible to use safe languages, software fault isolation, or virtual memory protection to safeguard the main kernel. However, such protection mechanisms do not address the full range of problems, such as resource hoarding, that can arise when application code is introduced into the kernel. In this paper, we present an analysis of extension mechanisms in the VINO kernel. VINO uses software fault isolation as its safety mechanism and a lightweight transaction system to cope with resource-hoarding. We explain how these two mechanisms are sufficient to protect against a large class of errant or malicious extensions, and we quantify the overhead that this protection introduces. We find that while the overhead of these techniques is high relative to the cost of the extensions themselves, it is low relative to the benefits that extensibility brings.
814|The locus distributed operating system|LOCUS Is a distributed operating system which supports transparent access to data through a network wide fllesystem, permits automatic replication of storaget supports transparent distributed process execution, supplies a number of high reliability functions such as nested transactions, and is upward compatible with Unix. Partitioned operation of subnetl and their dynamic merge is also supported. The system has been operational for about two years at UCLA and extensive experience In its use has been obtained. The complete system architecture is outlined in this paper, and that experience is summarized. 1
815|Core algorithms of the Maui scheduler|Abstract. The Maui scheduler has received wide acceptance in the HPC community as a highly configurable and effective batch scheduler. It is currently in use on hundreds of SP, O2K, and Linux cluster systems throughout the world including a high percentage of the largest and most cutting edge research sites. While the algorithms used within Maui have proven themselves effective, nothing has been published to date documenting these algorithms nor the configurable aspects they support. This paper focuses on three areas of Maui scheduling, specifically, backfill, job prioritization, and fairshare. It briefly discusses the goals of each component, the issues and corresponding design decisions, and the algorithms enabling the Maui policies. It also covers the configurable aspects of each algorithm and the impact of various parameter selections. 1
816|A worldwide flock of Condors: load sharing among workstation clusters|Condor is a distributed batch system for sharing the workload of compute-intensive
817|Stork: Making Data Placement a First Class Citizen in the Grid|Todays scientific applications have huge data requirements which continue to increase drastically every year. These data are generally accessed by many users from all across the the globe. This implies a major necessity to move huge amounts of data around wide area networks to complete the computation cycle, which brings with it the problem of efficient and reliable data placement. The current approach to solve this problem of data placement is either doing it manually, or employing simple scripts which do not have any automation or fault tolerance capabilities. Our goal is to make data placement activities first class citizens in the Grid just like the computational jobs. They will be queued, scheduled, monitored, managed, and even check-pointed. More importantly, it will be made sure that they complete successfully and without any human interaction. We also believe that data placement jobs should be treated differently from computational jobs, since they may have different semantics and different characteristics. For this purpose, we have developed Stork, a scheduler for data placement activities in the Grid.
818|Condor Technical Summary|Condor is a software package for executing long running &#034;batch&#034; type jobs on workstations which would otherwise be idle. Major features of Condor are automatic location and allocation of idle machines, and checkpointing and migration of processes. All of these features are achieved without any modifications to the UNIX kernel whatsoever. Also, users of Condor do not need to change their source programs to run with Condor, although such programs must be specially linked. The features of Condor for both users and workstation owners along with the limitations on the kinds of jobs which may be executed by Condor are described. The mechanisms behind our implementations of checkpointing and process migration are discussed in detail. Finally, the software which detects idle machines and allocates those machines to Condor users is described along with the techniques used to configure that software to meet the demands of a particular computing site or workstation owner.  1. Introduction to the ...
819|Solving Large Quadratic Assignment Problems on Computational Grids|The quadratic assignment problem (QAP) is among the hardest combinatorial optimization problems. Some instances of size n = 30 have remained unsolved for decades. The solution of these problems requires both improvements in mathematical programming algorithms and the utilization of powerful computational platforms. In this article we describe a novel approach to solve QAPs using a state-of-the-art branch-and-bound algorithm running on a federation of geographically distributed resources known as a computational grid. Solution of QAPs of unprecedented complexity, including the nug30, kra30b, and tho30 instances, is reported.
820|Resource Management through Multilateral Matchmaking|Federated distributed systems present new challenges to resource management, which cannot be met by conventional systems that employ relatively static resource models and centralized allocators. We previously argued that Matchmaking provides an elegant and robust resource management solution for these highly dynamic environments [5]. Although powerful and flexible, multiparty policies (e.g., co-allocation) cannot be accomodated by Matchmaking. In this paper we present Gang-Matching, a multilateral matchmaking formalism to address this deficiency.
821|Interfacing Condor and PVM to harness the cycles of workstation clusters|A continuing challenge to the scientific research and engineering communities is how to fully utilize computational hardware. In particular, the proliferation of clusters of high performance workstations has become an increasingly attractive source of compute power. Developments to take advantage of this environment have previously focused primarily on managing the resources, or on providing interfaces so that a number of machines can be used in parallel to solve large problems. Both approaches are desirable, and indeed should be complementary. Unfortunately, the resource management and parallel processing systems are usually developed by independent groups, and they usually do not interact well together. To bridge this gap, we have developed a framework for interfacing these two sorts of systems. Using this framework, we have interfaced PVM, a popular system for parallel programming with Condor, a powerful resource management system. This combined system is operational, and we have ma...
822|Parrot: Transparent User-Level Middleware for Data Intensive Computing|Distributed computing continues to be an alphabet-soup of services and protocols for managing computation and storage. To live in this environment, applications require middleware that can transparently adapt standard interfaces to new distributed systems; such software is known as an interposition agent. In this paper, we present several lessons learned about interposition agents via a progressive study of design possibilities. Although performance is an important concern, we pay special attention to less tangible issues such as portability, reliability, and compatibility. We begin with a comparison of seven methods of interposition, focusing on one method, the debugger trap, that requires special techniques to achieve acceptable performance on popular operating systems. Using this method, we implement a complete interposition agent, Parrot, that splices existing remote I/O systems into the namespace of standard applications. The primary design problem of Parrot is the mapping of fixed application semantics into the semantics of the available I/O systems. We offer a detailed discussion of how errors and other unexpected conditions must be carefully managed in order to keep this mapping intact. We conclude with a evaluation of the performance of the I/O protocols employed by Parrot, and use an Andrew-like benchmark to demonstrate that semantic differences have consequences in performance. 1. 
823|Protocols and services for distributed data-intensive science|Abstract. We describe work being performed in the Globus project to develop enabling protocols and services for distributed data-intensive science. These services include: * High-performance, secure data transfer protocols based on FTP, plus a range of libraries and tools that use these protocols * Replica catalog services supporting the creation and location of file replicas in distributed systems These components leverage the substantial body of &#034;Grid &#034; services and protocols developed within the Globus project and by its collaborators, and are being used in a number of data-intensive application projects.
824|Matchmaking Frameworks for Distributed Resource Management|Federated distributed systems present new challenges to resource management. Conventional resource managers are based on a relatively static resource model and a centralized allocator that assigns resources to customers. Distributed envi-ronments, particularly those built to support high-throughput computing (HTC), are often characterized by distributed management and distributed ownership. Distributed management introduces resource heterogeneity: Not only the set of available resources, but even the set of resource types is constantly changing. Distributed ownership introduces policy heterogeneity: Each resource may have its own idiosyncratic allocation policy. We propose a resource management framework based on a matchmaking paradigm to address these shortcomings. Matchmaking services enable discov-ery and exchange of goods and services in marketplaces. Agents that provide or require services advertise their presence by publishing constraints and pref-erences on the entities they would like to be matched with, as well as their own
825|Cheap cycles from the desktop to the dedicated cluster: combining opportunistic and dedicated scheduling with Condor|Clusters of commodity PC hardware running Linux are becoming widely used as computational resources. Most software for controlling clusters relies on dedicated scheduling algorithms. These algorithms assume the constant availability of resources to compute fixed schedules. Unfortunately, due to hardware and software failures, dedicated resources are not always available over the long-term. Moreover, these dedicated scheduling solutions are only applicable to certain classes of jobs, and they can only manage clusters or large SMP machines. The Condor High Throughput Computing System overcomes these limitations by combining aspects of dedicated and opportunistic scheduling into a single system. Both parallel and serial jobs are managed at the same time, allowing a simpler interface for the user and better resource utilization. This paper describes the Condor system, defines opportunistic scheduling, explains how Condor supports MPI jobs with a combination of dedicated and opportunistic scheduling, and shows the advantages gained by such an approach. An exploration of future work in these areas concludes the paper. By using both desktop workstations and dedicated clusters, Condor harnesses all available computational power to enable the best possible science at a low cost.  1 
826|Providing Resource Management Services to Parallel Applications|Because resource management (RM) services are vital to the performance of parallel applications, it is essential that parallel programming environments (PPEs) and RM systems work together. We believe that no single RM system is always the best choice for every application and every computing environment. Therefore, the interface between the PPE and the resource manager must be flexible enough to allow for customization and extension based on the environment. We present a framework for interfacing general PPEs and RM systems. This framework is based on clearly defining the responsibilities of these two components of the system. This framework has been applied to PVM, and two separate instances of RM systems have been implemented. One behaves exactly as PVM always has, while the second uses Condor to extend the set of RM services available to PVM applications. 1 Introduction To fulfill the promises of high performance computing, parallel applications must be provided with effective reso...
827|Gathering at the well: Creating communities for grid I/O|Grid applications have demanding I/O needs. Schedulers must bring jobs and data in close proximity in order to satisfy throughput, scalability, and policy requirements. Most systems accomplish this by making either jobs or data mobile. We propose a system that allows jobs and data to meet by binding execution and storage sites together into I/O communities which then participate in the wide-area system. The relationships between participants in a community may be expressed by the ClassAd framework. Extensions to the framework allow community members to express indirect relations. We demonstrate our implementation of I/O communities by improving the performance of a key high-energy physics simulation on an international distributed system. 1.
828|Bypass: A Tool for Building Split Execution Systems|Split execution is a common model for providing a friendly environment on a foreign machine. In this model, a remotely executing process sends some or all of its system calls back to a home environment for execution. Unfortunately, hand-coding split execution systems for experimentation and research is difficult and error-prone. We have built a tool, Bypass, for quickly producing portable and correct split execution systems for unmodified legacy applications. We demonstrate Bypass by using it to transparently connect a POSIX application to a simple data staging system based on the Globus toolkit. 1. Introduction The split execution model allows a process running on a foreign machine to behave as if it were running on its home machine. Split execution generally involves three software components: an application, an agent, and a shadow. Figure 1 shows these components. Kernel Agent Application Local System Calls Calls System Trapped Kernel Shadow Local System Calls Other...
829|Utilizing Widely Distributed Computational Resources Efficiently with Execution Domains|Wide-area computational grids have the potential to provide large amounts of computing capacity to the scientific community. Realizing this potential requires intelligent data management, enabling applications to harness remote computing resources with minimal remote data access overhead. We define execution domains, a framework which defines an affinity between CPU and data resources in the grid, so applications are scheduled to run on CPUs which have the needed access to datasets and storage devices. The framework also includes domain managers, agents which dynamically adjust the execution domain configuration to support the efficient execution of grid applications. In this paper, we present the execution domain framework and show how we apply it in the Condor resource management system.
830|Error Scope on a Computational Grid: Theory and Practice|Error propagation is a central problem in grid computing. We re-learned this while adding a Java feature to the Condor computational grid. Our initial experience with the system was negative, due to the large number of new ways in which the system could fail. To reason about this problem, we developed a theory of error propagation. Central to our theory is the concept of an error&#039;s scope, defined as the portion of a system that it invalidates. With this theory in hand, we recognized that the expanded system did not properly consider the scope of errors it discovered. We modified the system according to our theory, and succeeded in making it a more robust platform for distributed computing.
831|The DBC: Processing Scientific Data Over the Internet|We present the Distributed Batch Controller (DBC), a system built to support batch processing of large scientific datasets. The DBC implements a federation of autonomous workstation pools, which may be widely-distributed. Individual batch jobs are executed using idle workstations in these pools. Input data are staged to the pool before processing begins. We describe the architecture and implementation of the DBC, and present the results of experiments in which it is used to perform image compression. 1 Introduction  In this paper we present the DBC (Distributed Batch Controller), a system that processes data using widely-distributed computational resources. The DBC was built as a tool for enriching scientific data stored in two mass storage systems at NASA&#039;s Goddard Space Flight Center (GSFC). Enriching data means processing it to make it more useful. For example, satellite images may be classified according to some domain-specific criteria. These classifications can then be stored as ...
832|Principled design of the modern web architecture |The World Wide Web has succeeded in large part because its software architecture has been designed to meet the needs of an Internet-scale distributed hypermedia system. The modern Web architecture emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. In this paper, we introduce the Representational State Transfer (REST) architectural style, developed as an abstract model of the Web architecture to guide our redesign and definition of the Hypertext Transfer Protocol and Uniform Resource Identifiers. We describe the software engineering principles guiding REST and the interaction constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. We then compare the abstract model to the currently deployed Web architecture in order to elicit mismatches between the existing protocols and the applications they are intended to support.
833|Architectural Styles and the Design of Network-based Software Architectures|
The World Wide Web has succeeded in large part because its software architecture has been designed to meet the needs of an Internet-scale distributed hypermedia system. The Web has been iteratively developed over the past ten years through a series of modifications to the standards that define its architecture. In order to identify those aspects of the Web that needed improvement and avoid undesirable modifications, a model for the modern Web architecture was needed to guide its design, definition, and deployment.

Software architecture research investigates methods for determining how best to partition a system, how components identify and communicate with each other, how information is communicated, how elements of a system can evolve independently, and how all of the above can be described using formal and informal notations. My work is motivated by the desire to understand and evaluate the architectural design of network-based application software through principled use of architectural constraints, thereby obtaining the functional, performance, and social properties desired of an architecture. An architectural style is a named, coordinated set of architectural constraints.

This dissertation defines a framework for understanding software architecture via architectural styles and demonstrates how styles can be used to guide the architectural design of network-based application software. A survey of architectural styles for network-based applications is used to classify styles according to the architectural properties they induce on an architecture for distributed hypermedia. I then introduce the Representational State Transfer (REST) architectural style and describe how REST has been used to guide the design and development of the architecture for the modern Web.

REST emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. I describe the software engineering principles guiding REST and the interaction constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. Finally, I describe the lessons learned from applying REST to the design of the Hypertext Transfer Protocol and Uniform Resource Identifier standards, and from their subsequent deployment in Web client and server software.
834|Understanding Code Mobility|The technologies, architectures, and methodologies traditionally used to develop distributed applications exhibit a variety of limitations and drawbacks when applied to large scale distributed settings (e.g., the Internet). In particular, they fail in providing the desired degree of configurability, scalability, and customizability. To address these issues, researchers are investigating a variety of innovative approaches. The most promising and intriguing ones are those based on the ability of moving code across the nodes of a network, exploiting the notion of mobile code. As an emerging research field, code mobility is generating a growing body of scientific literature and industrial developments. Nevertheless, the field is still characterized by the lack of a sound and comprehensive body of concepts and terms. As a consequence, it is rather difficult to understand, assess, and compare the existing approaches. In turn, this limits our ability to fully exploit them in practice, and to further promote the research work on mobile code. Indeed, a significant symptom of this situation is the lack of a commonly accepted and sound definition of the term &#034;mobile code&#034; itself. This paper presents a conceptual framework for understanding code mobility. The framework is centered around a classification that introduces three dimensions: technologies, design paradigms, and applications. The contribution of the paper is twofold. First, it provides a set of terms and concepts to understand and compare the approaches based on the notion of mobile code. Second, it introduces criteria and guidelines that support the developer in the identification of the classes of applications that can leverage off of mobile code, in the design of these applications, and, finally, in the selection of the most appropriate implementation technologies. The presentation of the classification is intertwined with a review of the state of the art in the field. Finally, the use of the classification is exemplified in a case study.
835|A Note on Distributed Computing|We argue that objects that interact in a distributed system need to be dealt with in ways that are intrinsically different from objects that interact in a single address space. These differences are required because distributed systems require that the programmer be aware of latency, have a different model of memory access, and take into account issues of concurrency and partial failure. We look at a number of distributed systems that have attempted to paper over the distinction between local and remote objects, and show that such systems fail to support basic requirements of robustness and reliability. These failures have been masked in the past by the small size of the distributed systems that have been built. In the enterprise-wide distributed systems foreseen in the near future, however, such a masking will be impossible. We conclude by discussing what is required of both systems-level and application-level programmers and designers if one is to take distribution seriously.
836|A Caching Relay for the World Wide Web|We describe the design and performance of a caching relay for the World Wide Web. We model the distribution of requests for pages from the web and see how this distribution affects the performance of a cache. We use the data gathered from the relay to make some general characterizations about the web. (A version of this paper is available at http://www.research.digital.com/- SRC/personal/Steve Glassman/-  CachingTheWeb.html or .../CachingTheWeb.ps)  1 Overview  In January 1994, we set up a caching World Wide Web [10] relay for Digital Equipment Corporation &#039;s facilities in Palo Alto, California. We use a relay to reach the Web because Digital has a security firewall that restricts direct interaction between Digital internal computers and machines outside of Digital. We added caching to the relay because we wanted to improve the relay&#039;s performance and reduce its external network traffic. Clients use the relay for accessing the Web outside of Digital; requests for internal Digital pages...
837|A Component- and Message-Based Architectural Style for GUI Software|While a large fraction of application code is devoted to graphical user interface (GUI) functions, support for reuse in this domain has largely been confined to the creation of GUI toolkits (&#034;widgets&#034;). We present a novel architectural style directed at supporting larger grain reuse and flexible system composition. Moreover, the style supports design of distributed, concurrent applications. Asynchronous notification messages and asynchronous request messages are the sole basis for inter-component communication. A key aspect of the style is that components are not built with any dependencies on what typically would be considered lower-level components, such as user interface toolkits. Indeed, all components are oblivious to the existence of any components to which notification messages are sent. While our focus has been on applications involving graphical user interfaces, the style has the potential for broader applicability. Several trial applications using the style are described.
838|A Design Framework for Internet-Scale Event Observation and Notification|There is increasing interest in having software systems execute and interoperate over the Internet. Execution and interoperation at this scale imply a degree of loose coupling and heterogeneity among the components from which such systems will be built. One common architectural style for distributed; loosely-coupled, heterogeneous software systems is a structure based on event generation, observation and notification. The technology to support this approach is well-developed for local area networks, but it is illsuited to networks on the scale of the Internet. Hence, new technologies are needed to support the construction of large-scale, event-based software systems for the Internet. We have begun to design a new facility for event observation and notification that better serves the needs of Internet-scale applications. In this paper we present results from our first step in this design process, in which we defined a framework that captures many of the relevant design dimensions. Our framework comprises seven models-an object model, an event model, a naming model, an observation model, a time model, a notification model, and a resource model. The paper discusses each of these models in detail and illustrates them using an example involving an update to a Web page. The paper also evaluates three existing technologies with respect to the seven models.
839|Organization-Based Analysis of Web-Object Sharing and Caching|Performance-enhancing mechanisms in the World Wide Web primarily exploit repeated requests to Web documents by multiple clients. However, little is known about patterns of shared document access, particularly from diverse client populations. The principal goal of this paper is to examine the sharing of Web documents from an organizational point of view. An organizational analysis of sharing is important, because caching is often performed on an organizational basis; i.e., proxies are typically placed in front of large and small companies, universities, departments, and so on. Unfortunately, simultaneous multi-organizational traces do not currently exist and are difficult to obtain in practice.
840|Modeling the Performance of HTTP over Several Transport Protocols|This paper is a draft that will appear in IEEE/ACM Transactions on Networking. Final editing is still expected. Please replace it with the final version when published.
843|Uniform Resource Locators|Many protocols and systems for document search and retrieval are currently in use, and many more protocols or refinements of existing protocols are to be expected in a field whose expansion is explosive. These systems are aiming to achieve global search and readership of documents across differing computing platforms, and despite a plethora of protocols and data formats. As protocols evolve, gateways can allow global access to remain possible. As data formats evolve, format conversion programs can preserve global access. There is one area, however, in which it is impractical to make conversions, and that is in the names and addresses used to identify objects. This is because names and addresses of objects are passed on in so many ways, from the backs of envelopes to hypertext objects, and may have a long life. This paper discusses the requirements on a universal syntax which can be used to refer to objects available using existing protocols, and may be extended with technology. It make...
844|HTTP State Management Mechanism|This document specifies a way to create a stateful session with HTTP requests and responses. It describes two new headers, Cookie and Set-Cookie2, which carry state information between participating origin servers and user agents. The method described here differs from Netscape&#039;s Cookie proposal [Netscape],   but it can interoperate with HTTP/1.0 user agents that use Netscape&#039;s method. (See the HISTORICAL section.)  This document reflects implementation experience with RFC 2109 [RFC2109] and obsoletes it.   2. TERMINOLOGY  The terms user agent , client , server, proxy, and origin server have the same meaning as in the HTTP/1.1   specification [RFC2068].  Host name (HN) means either the host domain name (HDN) or the numeric Internet Protocol (IP) address of a host. The fully qualified domain name is preferred; use of numeric IP addresses is strongly discouraged.  The terms request-host and request-URI refer to the values the client would send to the server as, respectively, the host (bu...
845|Maintaining Distributed Hypertext Infostructures: Welcome to MOMspider&#039;s Web|Most documents made available on the World-Wide Web can be considered part of an infostructure --- an information resource database with a specifically designed structure. Infostructures often contain a wide variety of information sources, in the form of interlinked documents at distributed sites, which are maintained by a number of different document owners (usually, but not necessarily, the original document authors). Individual documents may also be shared by multiple infostructures. Since it is rarely static, the content of an infostructure is likely to change over time and may vary from the intended structure. Documents may be moved or deleted, referenced information may change, and hypertext links may be broken.  As it grows, an infostructure becomes complex and difficult to maintain. Such maintenance currently relies upon the error logs of each server (often never relayed to the document owners), the complaints of users (often not seen by the actual document maintainers), and pe...
846|A Set Of Principles For Conducting And  Evaluating Interpretive Field Studies In Information Systems|This article discusses the conduct and evaluation of interpretive research in information systems. While the conventions for evaluating information systems case studies conducted according to the natural science model of social science are now widely accepted, this is not the case for interpretive field studies. A set of principles for the conduct and evaluation of interpretive field research in information systems is proposed, along with their philosophical rationale. The usefulness of the principles is illustrated by evaluating three published interpretive field studies drawn from the IS research literature. The intention of the paper is to further reflection and debate on the important subject of grounding interpretive research methodology.
847|A Scientific Methodology for|The authors conducted formative research for the UCLA Eating and Activi ty Task Force, which wil l develop a social marketing campaign about body image in the next academic year.
848|Human resource bundles and manufacturing performance: Organizational logic and flexible production systems in the world auto industry|you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at.
849|The Learning Bureaucracy: New United Motors Manufacturing|by
850|The impact of economic performance of a transformation in workplace relations|This study examines how a transformation in patterns of conflict and cooperation affected economic performance in 25 work areas of a large, unionized manufacturing facility in the period 1984-87. Unlike most studies of industrial relations and economic performance, this study clearly distinguishes conflict from cooperation but evaluates the two together, rather than focusing on only one. An analysis of data collected from union and employer ecords and interviews strongly suggests that work areas with &#034;traditional &#034; labor-management relations, rooted in adversarial assumptions, had higher costs, more scrap, lower productiv-ity, and a lower return to direct labor hours worked than work areas with &#034;transformational &#034; relations, characterized by increased coopera-tion and improved ispute resolution. M OST recent studies that seek to link industrial relations activities to eco-nomic performance or productivity focus either on new developments in labor-management cooperation or on patterns of industrial conflict. In this study I develop and test the thesis that links between industrial relations and economic
851|Wireless sensor networks: a survey|This paper describes the concept of sensor networks which has been made viable by the convergence of microelectro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the potential sensor networks applications are explored, and a review of factors influencing the design of sensor networks is provided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are
852|Next century challenges: Scalable coordination in sensor networks|Networked sensors-those that coordinate amongst them-selves to achieve a larger sensing task-will revolutionize information gathering and processing both in urban envi-ronments and in inhospitable terrain. The sheer numbers of these sensors and the expected dynamics in these environ-ments present unique challenges in the design of unattended autonomous sensor networks. These challenges lead us to hypothesize that sensor network coordination applications may need to be structured differently from traditional net-work applications. In particular, we believe that localized algorithms (in which simple local node behavior achieves a desired global objective) may be necessary for sensor net-work coordination. In this paper, we describe localized al-gorithms, and then discuss directed diffusion, a simple com-munication model for describing localized algorithms. 1
853|Power-Aware Routing in Mobile Ad Hoc Networks|In this paper we present a case for using new power-aware metrics for determining routes in wireless  ad hoc networks. We present five different metrics based on battery power consumption at nodes. We  show that using these metrics in a shortest-cost routing algorithm reduces the cost/packet of routing  packets by 5-30% over shortest-hop routing (this cost reduction is on top of a 40-70% reduction in energy  consumption obtained by using PAMAS, our MAC layer protocol). Furthermore, using these new metrics  ensures that the mean time to node failure is increased significantly. An interesting property of using  shortest-cost routing is that packet delays do not increase. Finally, we note that our new metrics can  be used in most traditional routing protocols for ad hoc networks.  1 
854|Next Century Challenges: Mobile Networking for “Smart Dust”|Large-scale networks of wireless sensors are becoming an active topic of research. Advances in hardware technology and engineering design have led to dramatic reductions in size, power consumption and cost for digital circuitry, wire-less communications and Micro ElectroMechanical Systems (MEMS). This has enabled very compact, autonomous and mobile nodes, each containing one or more sensors, computation and communication capabilities, and a power supply. The missing ingredient is the networking and applications layers needed to harness this revolutionary capability into a complete system. We review the key elements of the emergent technology of “Smart Dust ” and outline the research challenges they present to the mobile networking and systems community, which must provide coherent connectivity to large numbers of mobile network nodes co-located within a small volume. 
855|I-TCP: Indirect TCP for mobile hosts|Abstract — IP-based solutions to accommodate mobile hosts within existing internetworks do not address the distinctive features of wireless mobile computing. IP-based transport protocols thus suffer from poor performance when a mobile host communicates with a host on the fixed network. This is caused by frequent disruptions in network layer connectivity due to — i) mobility and ii) unreliable nature of the wireless link. We describe the design and implementation of I-TCP, which is an indirect transport layer protocol for mobile hosts. I-TCP utilizes the resources of Mobility Support Routers (MSRs) to provide transport layer communication between mobile hosts and hosts on the fixed network. With I-TCP, the problems related to mobility and the unreliability of wireless link are handled entirely within the wireless link; the TCP/IP software on the fixed hosts is not modified. Using I-TCP on our testbed, the throughput between a fixed host and a mobile host improved substantially in comparison to regular TCP. 1
856|ASCENT: Adaptive self-configuring sensor networks topologies| Advances in microsensor and radio technology will enable small but smart sensors to be deployed for a wide range of environmental monitoring applications. The low per-node cost will allow these wireless networks of sensors and actuators to be densely distributed. The nodes in these dense networks will coordinate to perform the distributed sensing and actuation tasks. Moreover, as described in this paper, the nodes can also coordinate to exploit the redundancy provided by high density so as to extend overall system lifetime. The large number of nodes deployed in these systems will preclude manual configuration, and the environmental dynamics will preclude design-time preconfiguration. Therefore, nodes will have to self-configure to establish a topology that provides communication under stringent energy constraints. ASCENT builds on the notion that, as density increases, only a subset of the nodes are necessary to establish a routing forwarding backbone. In ASCENT, each node assesses its connectivity and adapts its participation in the multihop network topology based on the measured operating region. This paper motivates and describes the ASCENT algorithm and presents analysis, simulation, and experimental measurements. We show that the system achieves linear increase in energy savings as a function of the density and the convergence time required in case of node failures while still providing adequate connectivity. 
857|Instrumenting the world with wireless sensor networks|Pervasive micro-sensing and actuation may revolutionize the way in which we understand and manage complex physical systems: from airplane wings to complex ecosystems. The capabilities for detailed physical monitoring and manipulation offer enormous opportunities for almost every scientific discipline, and it will alter the feasible granularity of engineering. We identify opportunities and challenges for distributed signal processing in networks of these sensing elements and investigate some of the architectural challenges posed by systems that are massively distributed, physically-coupled, wirelessly networked, and energy limited. 
858|Smart Dust: Communicating with a Cubic-Millimeter Computer|building virtual keyboards;  . managing inventory control;  . monitoring product quality;  . constructing smart office spaces; and  . providing interfaces for the disabled.  SMART DUST REQUIREMENTS  Smart Dust requires both evolutionary and revolutionary  advances in miniaturization, integration, and  energy management. Designers can use microelectromechanical  systems (MEMS) to build small sensors,  optical communication components, and power supplies,  whereas microelectronics provides increasing  functionality in smaller areas, with lower energy consumption.  Figure 1 shows the conceptual diagram of  a Smart Dust mote. The power system consists of a  thick-film battery, a solar cell with a charge-integrating  capacitor for periods of darkness, or both.  Depending on its objective, the design integrates various  sensors, including light, temperature, vibration,  magnetic field, acoustic, and wind shear, onto the  mote. An integrated circuit provides sensor-signal pr
859|The Simulation and Evaluation of Dynamic Voltage Scaling Algorithms|The reduction of energy consumption in microprocessors can be accomplished without impacting the peak performance through the use of dynamic voltage scaling (DVS). This approach varies the processor voltage under software control to meet dynamically varying performance requirements. This paper presents a foundation for the simulation and analysis of DVS algorithms. These algorithms are applied to a benchmark suite specifically targeted for PDA devices. 2.
860|Comparing Algorithms for Dynamic Speed-Setting of a Low-Power CPU|To take advantage of the full potential of ubiquitous computing, we will need systems which minimize powerconsumption. Weiser et al. and others have suggested that this may be accomplished by a CPU which dynamically changes speed and voltage, thereby saving energy by spreading run cycles into idle time. Here we continue this research, using a simulation to compare a number of policies for dynamic speed-setting. Our work clarifies a fundamental power vs. delay tradeoff, as well as the role of prediction and of smoothing in dynamic speed-setting policies. We conclude that success seemingly depends more on simple smoothing algorithms than on sophisticated prediction techniques, but defer to the replication of these results on future variable-speed systems. 1 Introduction  Recent developments in ubiquitous computing make it likely that the future will see a proliferation of cordless computing devices. Clearly it will be advantageous for such devices to minimize power-consumption. The top p...
861|Building Efficient Wireless Sensor Networks with Low-Level Naming|In most distributed systems, naming of nodes for low-level communication leverages topological location (such as node addresses)  and is independent of any application. In this paper, we investigate an emerging class of distributed systems where low-level communication does not rely on network topological location. Rather, low-level communication is based on attributes that are external to the network topology and relevant to the application. When combined with dense deployment of nodes, this kind of named data enables in-network processing for data aggregation, collaborative signal processing, and similar problems. These approaches are essential for emerging applications such as sensor networks where resources such as bandwidth and energy are limited. This paper is the first description of the software architecture that supports  named data and in-network processing in an operational, multi-application sensor-network. We show that approaches such as in-network aggregation and nested queries can significantly affect network traffic. In one experiment aggregation reduces traffic by up to 42% and nested queries reduce loss rates by 30%. Although aggregation has been previously studied in simulation, this paper demonstrates nested queries as another form of in-network processing, and it presents the first evaluation of these approaches over an operational testbed.  
862|Power Efficient Organization of Wireless Sensor Networks|Abstract-- Wireless sensor networks have emerged recently as an effective way of monitoring remote or inhospitable physical environments. One of the major challenges in devising such networks lies in the constrained energy and computational resources available to sensor nodes. These constraints must be taken into account at all levels of system hierarchy. The deployment of sensor nodes is the first step in establishing a sensor network. Since sensor networks contain a large number of sensor nodes, the nodes must be deployed in clusters, where the location of each particular node cannot be fully guaranteed a priori. Therefore, the number of nodes that must be deployed in order to completely cover the whole monitored area is often higher than if a deterministic procedure were used. In networks with stochastically placed nodes, activating only the necessary number of sensor nodes at any particular moment can save energy. We introduce a heuristic that selects mutually exclusive sets of sensor nodes, where the members of each of those sets together completely cover the monitored area. The intervals of activity are the same for all sets, and only one of the sets is active at any time. The experimental results demonstrate that by using only a subset of sensor nodes at each moment, we achieve a significant energy savings while fully preserving coverage. I.
863|Achieving MAC Layer Fairness in Wireless Packet Networks|Link-layer fairness models that have been proposed for wireline and packet cellular networks cannot be generalized for shared channel wireless networks because of the unique characteristics of the wireless channel, such as location-dependent contention, inherent conflict between optimizing channel utilization and achieving fairness, and the absence of any centralized control. In this paper, we propose a general analytical framework that captures the unique characteristics of shared wireless channels and allows the modeling of a large class of systemwide fairness models via the specification of per-flow utility functions. We show that system-wide fairness can be achieved without explicit global coordination so long as each node executes a contention resolution algorithm that is designed to optimize its local utility function. We present a general mechanism for translating a given fairness model in our framework into a corresponding contention resolution algorithm. Using this translation...
864|Physical Layer Driven Protocol and Algorithm Design for Energy-Efficient Wireless Sensor Networks|The potential for collaborative, robust networks of microsensors has attracted a great deal of research attention. For the most part, this is due to the compelling applications that will be enabled once wireless microsensor networks are in place
866|Upper Bounds on the Lifetime of Sensor Networks|In this paper, we ask a fundamental question concerning the limits of energy e#ciency of sensor networks - What is the upper bound on the lifetime of a sensor network that collects data from a specified region using a certain number of energy-constrained nodes? The answer to this question is valuable for two main reasons. First, it allows calibration of real world data-gathering protocols and an understanding of factors that prevent these protocols from approaching fundamental limits. Secondly, the dependence of lifetime on factors like the region of observation, the source behavior within that region, basestation location, number of nodes, radio path loss characteristics, e#ciency of node electronics and the energy available on a node, is exposed. This allows architects of sensor networks to focus on factors that have the greatest potential impact on network lifetime. By employing a combination of theory and extensive simulations of constructed networks, we show that in all data gathe...
867|Robust Range Estimation Using Acoustic and Multimodal Sensing|Many applications of robotics and embedded sensor technology can benet from ne-grained localization. Fine-grained localization can simplify multi-robot collaboration, enable energy ecient multi-hop routing for low-power radio networks, and enable automatic calibration of distributed sensing systems. In this work we focus on range estimation, a critical prerequisite for ne-grained localization. While many mechanisms for range estimation exist, any individual mode of sensing can be blocked or confused by the environment. We present and analyze an acoustic ranging system that performs well in the presence of many types of interference, but can return incorrect measurements in non-line-of-sight conditions. We then suggest how evidence from an orthogonal sensory channel might be used to detect and eliminate these measurements. This work illustrates the more general research theme of combining multiple modalities to obtain robust results. 1 
868|Exposure In Wireless Ad-Hoc Sensor Networks|Wireless ad-hoc sensor networks will provide one of the missing connections between the Internet and the physical world. One of the fundamental problems in sensor networks is the calculation of coverage. Exposure is directly related to coverage in that it is a measure of how well an object, moving on an arbitrary path, can be observed by the sensor network over a period of time.  In addition to the informal definition, we formally define exposure and study its properties. We have developed an efficient and effective algorithm for exposure calculation in sensor networks, specifically for finding minimal exposure paths. The minimal exposure path provides valuable information about the worst case exposure-based coverage in sensor networks. The algorithm works for any given distribution of sensors, sensor and intensity models, and characteristics of the network. It provides an unbounded level of accuracy as a function of run time and storage. We provide an extensive collection of experimental results and study the scaling behavior of exposure and the proposed algorithm for its calculation.  I. 
869|Sensor Information Networking Architecture and Applications|This article introduces a sensor information networking architecture, called SINA, that facilitates querying, monitoring, and tasking of sensor networks. SINA plays the role of a middleware that abstracts a network of sensor nodes as a collection of massively distributed objects. The SINA&#039;s execution environment provides a set of configuration and communication primitives that enable scalable and energy-efficient organization of and interactions among sensor objects. On top the execution environment is a programmable substrate that provides mechanisms to create associations and coordinate activities among sensor nodes. Users then access information within a sensor network using declarative queries, or perform tasks using programming scripts.
870|Adaptive Frame Length Control for Improving Wireless Link Throughput, Range, and Energy Efficiency|Wireless network links are characterized by rapidly  time varying channel conditions and battery energy limitations at  the wireless mobile user nodes. Therefore static link control techniques  that make sense in comparatively well behaved wired links  do not necessarily apply to wireless links. New adaptive link layer  control techniques are needed to provide robust and energy efficient  operation even in the presence of orders of magnitude variations  in bit error rates and other radio channel conditions. For  example, recent research has advocated adaptive link layer techniques  such as adaptive error control [Lettieri97], channel state  dependent protocols [Bhagwat96, Fragouli97], and variable  spreading gain [Chien97]. In this paper we explore one such  adaptive technique: dynamic sizing of the MAC layer frame, the  atomic unit that is sent through the radio channel. A trade-off  exists between the desire to reduce header and physical layer  overhead by making frames large, and th...
871|Error Control and Energy Consumption in Communications for Nomadic Computing|We consider the problem of communications over a wireless channel in support of data transmissions from the  perspective of small portable devices that must rely on limited battery energy. We model the channel outages as statistically  correlated errors. Classic ARQ strategies are found to lead to a considerable waste of energy, due to the large number of  transmissions. The use of finite energy sources in the face of dependent channel errors leads to new protocol design criteria. As an  example, a simple probing scheme, which slows down the transmission rate when the channel is impaired, is shown to be more  energy efficient, with a slight loss in throughput. A modified scheme that yields slightly better performance but requires some  additional complexity is also studied. Some references on the modeling of battery cells are discussed to highlight the fact that  battery charge capacity is strongly influenced by the available &#034;relaxation time&#034; between current pulses. A formal approach ...
872|Intelligent Medium Access for Mobile Ad Hoc Networks with Busy Tones and Power Control|In a mobile ad-hoc networks (MANET), one essential issue is how to increase channel utilization while avoiding the hidden-terminal and the exposed terminal problems. Several MAC protocols, such as RTS/CTS-based and busytone-based schemes, have been proposed to alleviate these problems. In this paper, we explore the possibility of combining the concept of power control with the RTS/CTS-based and busy-tone-based protocols to further increase channel utilization. A sender will use an appropriate power level to transmit its packets so as to increase the possibility of channel reuse. The possibility of using discrete, instead of continuous, power levels is also discussed. Through analyses and simulations, we demonstrate the advantage of our new MAC protocol. This, together with the extra bene ts such as saving battery energy and reducing cochannel interference, does show a promising direction to enhance the performance of MANETs.
873|What is complexity|SFI Working Papers contain accounts of scientific work of the author(s) and do not necessarily represent the views of the Santa Fe Institute. We accept papers intended for publication in peer-reviewed journals or proceedings volumes, but not papers that have already appeared in print. Except for papers by our external faculty, papers must be based on work done at SFI, inspired by an invited visit to or collaboration at SFI, or funded by an SFI grant. ©NOTICE: This working paper is included by permission of the contributing author(s) as a means to ensure timely distribution of the scholarly and technical work on a non-commercial basis. Copyright and all rights therein are maintained by the author(s). It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author&#039;s copyright. These works may be reposted only with the explicit permission of the copyright holder. www.santafe.edu
874|Design Considerations for Distributed Microsensor Systems|Wireless distributed microsensor systems will enable the reliable monitoring and control of a variety of applications that range from medical and home security to machine diagnosis, chemical/biological detection and other military applications. The sensors have to be designed in a highly integrated fashion, optimizing across all levels of system abstraction, with the goal of minimizing energy dissipation. This paper addresses some of the key design considerations for future microsensor systems including the network protocols required for collaborative sensing and information distribution, system partitioning considering computation and communication costs, low energy electronics, power system design and energy harvesting techniques.  1. Introduction  Over the last few years, the design of micropower wireless sensor systems has gained increasing importance for a variety of civil and military applications. The Low Power Wireless Integrated Microsensors (LWIM) project has made major advan...
875|DataSpace: Querying and Monitoring Deeply Networked Collections in Physical Space|In this article we introduce a new conception of three-dimensional DataSpace, which is physical space enhanced by connectivity to the network.
876|Dynamic Voltage Scaling Techniques for Distributed Microsensor Networks|Distributed microsensor networks promise a versatile and robust platform for remote environment monitoring. Crucial to long system lifetimes for these microsensors are algorithms and protocols that provide the option of trading quality for energy savings. Dynamic voltage scaling on the sensor node&#039;s processor enables energy savings from these scalable algorithms. We demonstrate dynamic voltage scaling on the beginnings of a sensor node prototype, which currently consists of a commercial processor, a digitally adjustable DC-DC regulator, and a power-aware operating system.  1. Introduction  Distributed microsensor networks are emerging as a compelling new hardware platform for remote environment monitoring [1]. Researchers are considering a range of applications including remote climate monitoring, battlefield surveillance, and intra-machine monitoring [2]. A distributed microsensor network consists of many small, expendable, battery-powered wireless nodes. Once the nodes are deployed t...
877|Power-Aware Communication for Mobile Computers|Recently, the mobile community has focused on techniques for reducing energy consumption  for mobile hosts. These power management techniques typically target communication  devices such as wireless network interfaces, aiming to reduce usage, and thus energy consumption,  of the particular device itself. We observe that optimization of a single device&#039;s energy  consumption, without considering the effect of the strategy on the rest of the machine, can  have negative consequences. We propose power management techniques addressing mobile host  communications that encompass all components of a mobile host in an effort to optimize total  energy consumption. Specifically, we propose runtime adaptation of communication parameters  in order to minimize the energy consumed during active data transfer. Information about the  network environment is used to drive such adaptations in an effort to compensate for the effect  of dynamic service from wireless communication device on the energy consume...
878|A versatile architecture for the distributed sensor integration problem|Abstract-The computational issues related to information in-tegration in multisensor systems and distributed sensor networks has become an active area of research. From a computational viewpoint, the efficient extraction of information from noisy and faulty signals emanating from many sensors requires the solution of problems related a) to the architecture and fault tolerance of the distributed sensor network, b) to the proper synchronization of sensor signals, and c) to the integration of information to keep the communication and the centralized processing require-ments small. In this paper, we propose a versatile architecture for a distributed sensor network which consists of a multilevel network with the nodes (processing elementlsensor pairs) at each level interconnected as a deBruijn network. We show that this multilevel network has reasonable fault tolerance, admits simple and decentralized routing, and offers easy extensibility. We model information from sensors as real valued intervals and derive an interesting property related to information integration in the presence of faults. Using this property, the search for a fault is narrowed down to two potentially faulty sensors or communication links. In a distributed environment, information has to be integrated from “temporally close ” signals in the presence of imperfect clocks in a distributed environment. We apply the results of past research in this area to state various relationships between the clocks of the processing elements in the network for proper information integration. Index Terms- Abstract estimate, clock synchronization, dis-tributed sensor networks, deBruijn networks, fault tolerance, information integration. I.
879|The Mobile Patient: Wireless Distributed Sensor Networks for Patient Monitoring and Care|In this paper, the concept of a 3 layer distributed sensor network for patient monitoring and care is introduced. The envisioned network has a leaf node layer (consisting of patient sensors), a intermediate node layer (consisting of the supervisory processor residing with each patient) and the root node processor (residing at a central monitoring facility). The introduced paradigm has the capability of dealing with the bandwidth bottleneck at the wireless patient - root node link and the processing bottleneck at the central processor or root node of the network.
880|Energy-efficient link layer for wireless microsensor network |Wireless microsensors are being used to form large, dense networks for the purposes of long-term environmental sensing and data collection. Unfortunately, these networks are typically deployed in remote environments where energy sources are limited. Thus, designing fault-tolerant wire-less microsensor networks with long system lifetimes can be challenging. By applying energy-efficient techniques at all levels of the system hierarchy, system lifetime can be ex-tended. In this paper, energy-efficient techniques that adapt underlying communication parameters will be presented in the context of wireless microsensor networks. In particular, the effect of adapting link and physical layer parameters, such as output transmit power and error control coding, on system energy consumption will be examined. 1.
881|Diagnosis of Sensor Networks|As sensor nodes are embedded into physical environments and becoming integral parts of our daily lives, sensor networks will become the important nerve systems that monitor and actuate our physical environments. We define the process of monitoring the status of a sensor network and figuring out the problematic sensor nodes sensor network diagnosis. However, the high sensor node-to-manager ratio makes it extremely difficult to pay special attention to any individual node. In addition, the response implosion problem, which occurs when a high volume of incoming replies triggered by diagnosis queries cause the central diagnosing node to become a bottleneck, is one major obstacle to be overcome. In this paper, we describe approaches to addressing the response implosion problem in sensor network diagnosis. We will also present simulation experiments on the performance of these approaches, and discuss presentation schemes for diagnostic results.
882|A selforganizing approach to data forwarding in largescale sensor networks|Abstracf- The large number of networked sensors, frequent sensor failures and stringent energy constraints pose unique design challenges for data forwarding in wireless sensor networks. In this paper, we present a new approach to data forwarding in sensor networks that effectively addresses these design issues. Our approach organizes sensors into a dynamic, self-optimizing multicast tree-based forwarding hierarchy, which is data centric and robust to node failures. We demonstrate the effectiveness of our design through simulations. I.
883|All-Digital Impulse Radio For MUI/ISI-Resilient Multi-User Communications Over Frequency-Selective Multipath Channels|Impulse radio (IR) is an ultra-wideband system with attractive features for baseband asynchronous multiple access (MA), multimedia services, and tactical wireless communications. Implemented with analog components, the continuoustime IRMA model utilizes pulse-position modulation (PPM) and random time-hopping codes to alleviate multipath effects and suppress multiuser interference (MUI). We introduce a novel continuous-time Multiple Input Multiple Output (MIMO) PPMIRMA scheme, and derive its discrete-time equivalent model. Relying on a time-division-duplex access protocol and orthogonal user codes, we design composite linear and non-linear receivers for the downlink. The linear step eliminates MUI deterministically and accounts for frequency-selective multipath, while a Maximum Likelihood (ML) receiver performs symbol detection.  1. INTRODU7 ION  The idea of transmitting digital information using ultra-short impulses was first presented in [10] and called Impulse Radio.It  relies on PPM...
884|High performance messaging on workstations: Illinois Fast Messages (FM) for Myrinet  (1995) |In most computer systems, software overhead dominates the cost of messaging, reducing delivered performance, especially for short messages. Efficient software messaging layers are needed to deliver the hardware performance to the application level and to support tightly-coupled workstation clusters. Illinois Fast Messages (FM) 1.0 is a high speed messaging layer that delivers low latency and high bandwidth for short messages. For 128-byte packets, FM achieves bandwidths of 16.2 MB/s and one-way latencies 32 s on Myrinet-connected SPARCstations (user-level to user-level). For shorter packets, we have measured one-way latencies of 25 s, and for larger packets, bandwidth as high as to 19.6 MB/s — delivered bandwidth greater than OC-3. FM is also superior to the Myrinet API messaging layer, not just in terms of latency and usable bandwidth, but also in terms of the message half-power point (n 1 2 which is two orders of magnitude smaller (54 vs. 4,409 bytes). We describe the FM messaging primitives and the critical design issues in building a low-latency messaging layers for workstation clusters. Several issues are critical: the division of labor between host and network coprocessor, management of the input/output (I/O) bus, and buffer management. To achieve high performance, messaging layers should assign as much functionality as possible to the host. If the network interface has DMA capability, the I/O bus should be used asymmetrically, with
885|Dynamically Forecasting Network Performance Using the Network Weather Service|this paper, we outline its design and detail the predictive performance of the forecasts it generates. While the forecasting methods are general, we focus on their ability to predict the TCP/IP end-to-end throughput and latency that is attainable by an application using systems located at different sites. Such network forecasts are needed both to support scheduling [5], and by the metacomputing software infrastructure to develop quality-of-service guarantees [10, 17]. Keywords: scheduling, metacomputing, quality-of-service, statistical forecasting, network performance monitoring
886|The Nexus approach to integrating multithreading and communication|Lightweight threads have an important role to play in parallel systems: they can be used to exploit shared-memory parallelism, to mask communication and I/O latencies, to implement remote memory access, and to support task-parallel and irregular applications. In this paper, we address the question of how to integrate threads and communication in high-performance distributed-memory systems. We propose an approach based on global pointer and remote service request mechanisms, and explain how these mechanisms support dynamic communication structures, asynchronous messaging, dynamic thread creation and destruction, and a global memory model via interprocessor references. We also explain how these mechanisms can be implemented in various environments. Our global pointer and remote service request mechanisms have been incorporated in a runtime system called Nexus that is used as a compiler target for parallel languages and as a substrate for higher-level communication libraries. We report the results of performance studies conducted using a Nexus implementation; these results indicate that Nexus mechanisms can be implemented efficiently on commodity hardware and software systems. 1
887|Legion: The Next Logical Step Toward a Nationwide Virtual Computer|The coming of giga-bit networks makes possible the realization of a single nationwide virtual computer comprised of a variety of geographically distributed high-performance machines and workstations. To realize the potential that the physical infrastructure provides, software must be developed that is easy to use, supports large degrees of parallelism in applications code, and manages the complexity of the underlying physical system for the user. This paper describes our approach to constructing and exploiting such &#034;metasystems&#034;. Our approach inherits features of earlier work on parallel processing systems and heterogeneous distributed computing systems. In particular, we are building on Mentat, an object-oriented parallel processing system developed at the University of Virginia. This report is a preliminary document. We expect changes to occur as the architecture and design of the system mature. 
888|Legion -- a view from 50,000 feet|The coming of giga-bit networks makes possible the realization of a single nationwide virtual computer com-prised of a variety of geographically distributed high-pe6ormance machines and workstations. To realize the potential that the physical infrastructure provides, soft-ware must be developed that is easy to use, supports large degrees of parallelism in applications code, and manages the complexity of the underlying physical sys-tem for the usel: Legion is a metasystem project at the University of Virginia designed to provide users with a transparent intelface to the available resources, both at the programming interface level as well as at the user level. Legion addresses issues such as parallelism, fault-tolerance, security, autonomy, heterogeneity, resource management, and access transparency in a multi-language environment. In this paper we present a high-level overview of Legion, its vision, objectives, a brief sketch of how some of those objectives will be met, and the current status of the project.2 1 The Opportunity The dramatic increase in ubiquitously available net-work bandwidth will qualitatively change how the world computes, communicates, and collaborates. The rapid expansion of the world-wide web, and the changes that it has wrought are just the beginning. As high band-width connections become available they “shrink ” dis-tance and change our modes of computation, storage and interaction. Inevitably, users will operate in a wide-area environment that transparently consists of worksta-tions, personal computers, graphics rendering engines, supercomputers, and non-traditional devices: e.g., TVs,
889|An Abstract-Device Interface for Implementing  Portable Parallel-I/O Interfaces|In this paper, we propose a strategy for implementing parallel-I/O interfaces portably and efficiently. We have defined an abstract-device interface for parallel I/O, called ADIO. Any parallel-I/O API can be implemented on multiple file systems by implementing the API portably on top of ADIO, and implementing only ADIO on different file systems. This approach simplifies the task of implementing an API and yet exploits the specific high-performance features of individual file systems. We have used ADIO to implement the Intel PFS interface and subsets of MPI-IO and IBM PIOFS interfaces on PFS, PIOFS, Unix, and NFS file systems. Our performance studies indicate that the overhead of using ADIO as an implementation strategy...
890|Managing Multiple Communication Methods in High-Performance Networked Computing Systems|Modern networked computing environments and applications often require---or can benefit from---the use of multiple communication substrates, transport mechanisms, and protocols, chosen according to where communication is directed, what is communicated, or when communication is performed. We propose techniques that allow multiple communication methods to be supported transparently in a single application, with either automatic or user-specified selection criteria guiding the methods used for each communication. We explain how communication link and remote service request mechanisms facilitate the specification and implementation of multimethod communication. These mechanisms have been implemented in the Nexus multithreaded runtime system, and we use this system to illustrate solutions to various problems that arise when implementing multimethod communication. We also illustrate the application of our techniques by describing a multimethod, multithreaded implementation of the Message Pas...
891|Active Message Applications Programming Interface and Communication Subsystem Organization|High-performance network hardware is advancing, with multi-gigabit link bandwidths and sub-microsecond switch latencies. Network-interface hardware also continues to evolve, although the design space remains large and diverse. One critical abstraction, a simple, portable, and general-purpose communications interface, is required to make effective use of these increasingly high-performance networks and their capable interfaces. Without a new interface, the software overheads of existing ones will dominate communication costs, and many applications may not benefit from the advancements in network hardware. This document specifies a new active message communications interface for these networks. Its primitives, in essence an instruction set for communications, map efficiently onto underlying network hardware and compose effectively into higher-level protocols and applications. For high-performance implementations, the interface enables direct application-network interface interactions. In...
892|CC++: A Declarative Concurrent Object Oriented Programming Notation|CC++ is Compositional C++ , a parallel object-oriented notation that consists of C++ with six extensions. The goals of the CC++ project are to provide a theory, notation and tools for developing reliable scalable concurrent program libraries, and to provide a framework for unifying: 1. distributed reactive systems, batch-oriented numeric and symbolic applications, and user-interface systems, 2. declarative programs and object-oriented imperative programs, and 3. deterministic and nondeterministic programs. This paper is a brief description of the motivation for CC++ , the extensions to C++ , a few examples of CC++ programs with reasoning about their correctness, and an evaluation of CC++ in the context of other research on concurrent computation. A short description of C++ is provided.  
893|Software infrastructure for the I-WAY high-performance distributed computing experiment|High-speed wide area networks are expected to enable innovative applications that integrate geographically distributed, high-performance computing, database, graphics, and networking resources. However, there is as yet little understanding of the higher-level services required to support these applications, or of the techniques required to implement these services in a scalable, secure manner. We report on a large-scale prototyping effort that has yielded some insights into these issues. Building on the hardware base provided by the I-WAY, a national-scale Asynchronous Transfer Mode (ATM) network, we developed an integrated management and application programming system, called I-Soft. This system was deployed at most of the 17 I-WAY sites and used by many of the 60 applications demonstrated on the I-WAY network. In this article, we describe the I-Soft design and report on lessons learned from application experiments. 1
894|Sharing Visualization Experiences among Remote Virtual Environments|Virtual reality has become an increasingly familiar part of the science of visualization and communication of information. This, combined with the increase in connectivity of remote sites via high-speed networks, allows for the development of a collaborative distributed virtual environment. Such an environment enables the development of supercomputer simulations with virtual reality visualizations that can be displayed at multiple sites, with each site interacting, viewing, and communicating about the results being discovered. The early results of an experimental collaborative virtual reality environment are discussed in this paper. The issues that need to be addressed in the implementation, as well as preliminary results are covered. Also provided are a discussion of plans and a generalized application programmers interface for CAVE to CAVE will be provided. 1 Introduction  Sharing a visualization experience among remote virtual environments is a new area of research within the field ...
895|Near-real-time Satellite Image Processing: Metacomputing in CC++|Metacomputing entails the combination of diverse, heterogeneous elements to provide a seamless, integrated computing service. We describe one such metacomputing application using Compositional C that integrates specialized resources, high-speed networks, parallel computers, and VR display technology to process satellite imagery in near-real-time. From the virtual environment, the user can query an InputHandler object for the latest available satellite data, select a satellite pass for processing by CloudDetector objects on a parallel supercomputer, and have the results rendered by a VisualizationManager object which could be a simple workstation, an ImmersaDesk or a CAVE. With an ImmersaDesk or CAVE, the user can navigate over a terrain with elevation and through the cloudscape data as it is being pumped in from the supercomputer. We discuss further issues for the development of metacomputing capabilities with regards to the integration of run-time systems, operating systems, high-s...
896|A Secure Communications Infrastructure for High-Performance Distributed Computing|We describe a software infrastructure designed to support the development of applications  that use high-speed networks to connect geographically distributed supercomputers,  databases, and scientific instruments. Such applications may need to operate over open  networks and access valuable resources, and hence can require mechanisms for ensuring  integrity and confidentiality of communications and for authenticating both users and  resources. Yet security solutions developed for traditional client-server applications do  not provide direct support for the distinctive program structures, programming tools,  and performance requirements encountered in these applications. To address these requirements,  we are developing a security-enhanced version of a communication library  called Nexus, which is then used to provide secure versions of various parallel libraries  and languages, including the popular Message Passing Interface. These tools support the  wide range of process creation mechan...
897|Remote Engineering Tools for the Design of Pollution Control Systems for Commercial Boilers|this paper, we discuss a pilot project involving a collaboration between Nalco Fuel Tech (NFT), a small company that has developed state-of-the-art emission reduction systems for commercial boilers, and the computational science group at Argonne National Laboratory (ANL). The key objective of this project is the development of a real-time, interactive capability that allows the user to drive the computational model from within the virtual environment. In this case, the required interaction involves the placement of chemical injection systems in the boiler and a quick evaluation of  their effectiveness in reducing undesirable emissions from the combustion process. The numerical model of the combustion process and its interaction with the chemical reactions used to reduce emissions is complex, and the solution is computationally intensive. The timely solution of these models depends on advanced numerical methods and the effective use of high-performance computing resources. Our approach includes an implementation of an injector model that can be run on a variety of architectures ranging from the IBM SP to networks of workstations. This calculation is separate from the visualization process and numerical results are communicated to the virtual boiler using the CAVEcomm message-passing library developed at Argonne National Laboratory [4]. The virtual boiler environment consists of several components designed to aide in the efficient optimization of the injection system. In this paper we discuss the software that allows rapid virtual boiler construction, various mechanisms for data visualization, and a package for the interactive placement of injector nozzles. Once the computational design process is complete, engineers are sent to the boiler site to install the injection s...
898|MPI on the I-WAY: A Wide-Area, Multimethod Implementation of the Message Passing Interface|High-speed wide-area networks enable innovative ap-plications that integrate geographically distributed com-puting, database, graphics, and networking resources. The Message Passing Interface (MPI) can be used as a portable, high-performance programming model for such systems. However, the wide-area environment in-troduces challenging problems for the MPI implementor, because of the heterogeneity of both the underlying physical infrastructure and the authentication and software environment at different sites. In this article, we describe an MPI implementation that incorporates so-lutions to these problems. This implementation, which was developed for the I-WAY distributed-computing ex-periment, was constructed by layering MPICH on the Nexus multithreaded runtime system. Nexus provides automatic configuration mechanisms that can be used to select and configure authentication, process creation, and communication mechanisms in heterogeneous systems. 
899|The Macroscopic Behavior of the TCP Congestion Avoidance Algorithm|In this paper, we analyze a performance model for the TCP Congestion Avoidance algorithm. The model predicts the bandwidth of a sustained TCP connection subjected to light to moderate packet losses, such as loss caused by network congestion. It assumes that TCP avoids retransmission timeouts and always has sufficient receiver window and sender data. The model predicts the Congestion Avoidance performance of nearly all TCP implementations under restricted conditions and of TCP with SelectiveAcknowledgements over a much wider range of Internet conditions. We verify
900|TCP Selective Acknowledgment Options|TCP may experience poor performance when multiple packets are lost from one window of data. With the limited information available from cumulative acknowledgments, a TCP sender can only learn about a single lost packet per round trip time. An aggressive sender could choose to retransmit packets early, but such retransmitted segments may have already been successfully received. A Selective Acknowledgment (SACK) mechanism, combined with a selective repeat retransmission policy, can help to overcome these limitations. The receiving TCP sends back SACK packets to the sender informing the sender of data that has been received. The sender can then retransmit only the missing data segments. This memo proposes an implementation of SACK and discusses its performance and related issues. Acknowledgements Much of the text in this document is taken directly from RFC1072 &#034;TCP Extensions for Long-Delay Paths&#034; by Bob Braden and Van Jacobson. The authors would like to thank Kevin Fall (LBNL), Christian...
901|Characterizing End-to-End Packet Delay and Loss in the Internet|We use the measured round trip delays of small UDP probe packets sent at regular time intervals to characterize the end-to-end packet delay and loss behavior in the Internet. By varying the interval between probe packets, it is possible to study the structure of the Internet load over different time scales. In this paper, the time scales of interest range from a few milliseconds to a few minutes. Our observations agree with results obtained by others using simulation and experimental approaches. For example, our estimates of Internet workload are consistent with the hypothesis of a mix of bulk traffic with larger packet size, and interactive traffic with smaller packet size. The interarrival time distribution for Internet packets is consistent with an exponential distribution. We also observe a phenomenon of compression (or clustering) of the probe packets similar to the acknowledgement compression phenomenon recently observed in TCP. Our results also show interesting and less expected...
902|A Delay-Based Approach for Congestion Avoidance in Interconnected Heterogeneous Computer Networks|In heterogeneous networks, achieving congestion avoidance is difficult because the congestion feedback from one subnetwork may have no meaning to sources on other subnetworks. We propose using changes in round-trip delay as an implicit feedback. Using a black-box model of the network, we derive an expression for the optimal window as a function of the gradient of the delay-window curve. The problems of selfish optimum and social optimum are also addressed. It is shown that without a careful design, it is possible to get into a race condition during heavy congestion, where each user wants more resources than others, thereby leading to a diverging condition. It is shown that congestion avoidance using round-trip delay is a promising approach. The approach has the advantage that there is absolutely no overhead for the network itself. It is exemplified by a simple scheme. The performance of the scheme is analyzed using a simulation model. The scheme is shown to be efficient, fair, conver...
903|Simulation-based comparison of tahoe, reno and sack TCP|This paper uses simulations to explore the benefits of adding selective acknowledgments (SACK) and selective repeat to TCP. We compare Tahoe and Reno TCP, the two most common reference implementations for TCP, with two modified versions of Reno TCP. The first version is New-Reno TCP, a modified version of TCP without SACK that avoids some of Reno TCP&#039;s performance problems when multiple packets are dropped from a window of data. The second version is SACK TCP, a conservative extension of Reno TCP modified to use the SACK option being proposed in the Internet Engineering Task Force (IETF). We describe the congestion control algorithms in our simulated implementation of SACK TCP and show that while selective acknowledgments are not required to solve Reno TCP&#039;s performance problems when multiple packets are dropped, the absence of selective acknowledgments does impose limits to TCP&#039;s ultimate performance. In particular, we show that without selective acknowledgments, TCP implementations are constrained to either retransmit at most one dropped packet per round-trip time, or to retransmit packets that might have already been successfully delivered. 1
904|Real Time Video and Audio in the World Wide Web|The architecture of World Wide Web (WWW) browsers and servers support full file transfer for document retrieval. TCP is used for data transfers by Web browsers and their associated Hypertext Transfer Protocol (HTTP) servers. Full file transfer and TCP are unsuitable for continuous media, such as real time audio and video. In order for the WWW to support continuous media, we require the transmission of video and audio on-demand and in real time, as well as new protocols for real time data. We extend the architecture of the WWW to encompass the dynamic, real time information space of video and audio. Our WWW browser Vosaic, short for Video Mosaic, incorporates real time video and audio into standard hypertext pages and which are displayed in place. Video and audio transfers occur in real time; there is no file retrieval latency. The video and audio result in compelling Web pages. Real time video and audio data can be effectively served over the present day Internet with the proper transm...
905|Architectural Considerations for Playback of Quality Adaptive Video over the Internet|Lack of QoS support in the Internet has not prevented rapid growth of realtime streaming applications. Many such applications play back stored audio and video over the network. However most of these applications do not perform effective congestion control, and there is significant concern about the effects on co-existing wellbehaved traffic and the potential for congestion collapse. In addition, most such applications are unable to perform quality adaptation on-the-fly as available bandwidth changes during a session, and so do not make effective use of additional bandwidth when it is available. This paper aims to provide some architectural insights on the design of video playback applications in the Internet. We identify end-to-end congestion control, quality adaptation and error control as the three major building blocks for Internet video playback
906|Rate Control For Video Coding Over Variable Bit Rate Channels With Applications To Wireless Transmission|Video transmission over wireless links is an emerging application which involves a time-varying channel. In this paper we propose that rate control algorithms should be used at the video encoders, along with models of the channel behavior, to improve the performance of such systems. Rather than letting information be lost as the channel conditions change, in our scheme channel state information is fedback to the encoder. We propose a method, based on dynamic programming, to compute the rate-distortion performance for a given channel and source realization. We show how the rate-distortion performance changes with end-to-end delay and feedback delay.
907|Real-Time Dynamic Rate Shaping and Control for Internet Video Applications|We present a novel framework for supporting video in today&#039;s Internet. Our technique is novel in that it employs image processing and networking techniques which work together to provide the best quality video in a hostile environment. Unlike other real-time Internetbased services which do not consider congestion avoidance, our technique is as harmless as a file transfer. We describe the algorithms and protocols needed to support the architecture and present experimental results gathered in both a controlled environment and in external wide area connections across the Internet. 1 Introduction  The technique for developing video services without QoS involves an explicit attempt at avoiding network congestion. Clearly, network congestion hurts the performance of all users of the network. The goal is to send only the data that can fit into the network at a particular time. This requires both a networking and an image processing approach. From the networking perspective, an estimate of the...
908|The Unix Time-Sharing System|Unix is a general-purpose, multi-user, interactive operating system for the larger Digital Equipment Corporation PDP-11 and the Interdata 8/32 computers. It offers a number of features seldom found even in larger operating systems, including i A hierarchical file system incorporating demountable volumes, ii Compatible file, device, and inter-process I/O, iii The ability to initiate asynchronous processes, iv System command language selectable on a per-user basis, v Over 100 subsystems including a dozen languages, vi High degree of portability. This paper discusses the nature and implementation of the file system and of the user command interface. I.
909|A Note on the Confinement Problem|This not explores the problem of confining a program during its execution so that it cannot transmit information to any other program except its caller. A set of examples attempts to stake out the boundaries of the problem. Necessary conditions for a solution are stated and informally justified.
910|Programming Semantics for Multiprogrammed Computations|... between an assembly language and an advanced algebraic language.  
911|Protection and the control of information sharing in Multics|This document was originally prepared off-line. This file is the result of scan, OCR, and manual touchup, starting
912|Protection|The following paper by Butler Lampson has been frequently referenced. Because the original is not widely available, we are reprinting it here. If the paper is referenced in published work,
913|A hardware architecture for implementing protection rings|Protection of computations and information is an important aspect of a computer utility. In a system which usessegmentation as a memory addressing scheme, protection can be achieved in part by associating concentric rings of decreasing access privilege with a computation. This paper describes hardware processor mechanisms for implementing these rings of protection. The mechanisms allow cross-ring calls and subsequent returns to occur without trapping to the supervisor. Automatic hardware validation of referencesacross ring boundaries is also performed. Thus, a call by a user procedure to a protected subsystem (including the the supervisor) is identical to a call to a companion user procedure. The mechanisms of passing and referencing arguments are the same in both cases as well.
914|The Multics Virtual Memory: Concepts and Design|As experience with use of on-line operating systems has grown, the need to share information among system users has become increasingly apparent. Many contemporary systems permit some degree of sharing. Usually, sharing is accomplished by allowing several users to share data via input and output of information stored in files kept in secondary storage. Through the use of segmentation, however, Multics provides direct hardware addressing by user and system programs of all information, independent of its physical storage location. Information is stored in segments each of which is potentially sharable and carries its own independent attributes of size and access privilege. Here, the design and implementation considerations of segmentation and sharing in Multics are first discussed under the assumption that all information resides in a large, segmented main memory. Since the size of main memory on contemporary systems is rather limited, it is then shown how the Multics software achieves the effect of a large segmented main memory through the use of the Honeywell 645 segmentation and paging hardware.
915|A User Machine in a Time-Sharing System|This paper describes the design of the computer seen by a machine-language programmer in a timesharing system developed at the University of California at Berkeley. Some of the instructions in this machine are executed by the hardware, and some are implemented by software. The user, however, thinks of them all as part of his machine, a machine having extensive and unusual capabilities, many of which might be part of the hardware of a (considerably more expensive) computer. Among the important features of the machine are the arithmetic and string manipulation instructions, the very general memory allocation and configuration mechanism, and the multiple processes which can be created by the program. Facilities are provided for communication among these processes and for the control of exceptional conditions. The input-output system is capable of handling all of the peripheral equipment in a uniform and convenient manner through files having symbolic names. Programs can access files belonging to a number of people, but each person can protect his own files from unauthorized access by others. Some mention is made at various points of the techniques of implementation, but the main emphasis is on the appearance of the user&#039;s machine. 
916|Ongoing research and development on information protection|Many individuals involved in the projects described here spent time patiently explaining their activities to me, putting together written descriptions for me to work from, and reviewing early drafts of my (often muddled) writeups of their research. Thanks are due all of them, but responsibility for mistakes and omissions is my own.
