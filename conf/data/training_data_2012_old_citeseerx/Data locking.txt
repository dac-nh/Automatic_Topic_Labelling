ID|Title|Summary
1|Transactional Memory: Architectural Support for Lock-Free Data Structures |A shared data structure is lock-free if its operations do not require mutual exclusion. If one process is interrupted in the middle of an operation, other processes will not be prevented from operating on that object. In highly concurrent systems, lock-free data structures avoid common problems associated with conventional locking techniques, including priority inversion, convoying, and difficulty of avoiding deadlock. This paper introduces transactional memory, a new multiprocessor architecture intended to make lock-free synchronization as efficient (and easy to use) as conventional techniques based on mutual exclusion. Transactional memory allows programmers to define customized read-modify-write operations that apply to multiple, independently-chosen words of memory. It is implemented by straightforward extensions to any multiprocessor cache-coherence protocol. Simulation results show that transactional memory matches or outperforms the best known locking techniques for simple benchmarks, even in the absence of priority inversion, convoying, and deadlock.  
3|Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors|Scalable shared-memory multiprocessors distribute memory among the processors and use scalable interconnection networks to provide high bandwidth and low latency communication. In addition, memory accesses are cached, buffered, and pipelined to bridge the gap between the slow shared memory and the fast processors. Unless carefully controlled, such architectural optimizations can cause memory accesses to be executed in an order different from what the programmer expects. The set of allowable memory access orderings forms the memory consistency model or event ordering model for an architecture.
4|Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors|Busy-wait techniques are heavily used for mutual exclusion and barrier synchronization in shared-memory parallel programs. Unfortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnect contention, introducing performance bottlenecks that become markedly more pronounced as applications scale. We argue that this problem is not fundamental, and that one can in fact construct busy-wait synchronization algorithms that induce no memory or interconnect contention. The key to these algorithms is for every processor to spin on separate locally-accessible ag variables, and for some other processor to terminate the spin with a single remote write operation at an appropriate time. Flag variables may be locally-accessible as a result of coherent caching, or by virtue of allocation in the local portion of physically distributed shared memory. We present a new scalable algorithm for spin locks that generates O(1) remote references per lock acquisition, independent of the number of processors attempting to acquire the lock. Our algorithm provides reasonable latency in the absence of contention, requires only a constant amount of space per lock, and requires no hardware support other than
5|Distributed packet switching for local computer networks|Abstract: Ethernet is a branching broadcast communication system for carrying digital data packets among locally distributed computing stations. The packet transport mechanism provided by Ethernet has been used to build systems which can be viewed as either local computer networks or loosely coupled multiprocessors. An Ethernet&#039;s shared communication facility, its Ether, is a passive broadcast medium with no central control. Coordination of access to the Ether for packet broadcasts is distributed among the contending transmitting stations using controlled statistical arbitration. Switching of packets to their destinations on the Ether is distributed among the receiving stations using packet address recognition. Design principles and implementation are described based on experience.with an operating Ethernet of1 00 nodes along a kilometer of coaxial cable. A model for estimating performance under heavy loads and a packet protocol for error-controlled communication are included for completeness.
6|A Methodology for Implementing Highly Concurrent Data Objects| A concurrent object is a data structure shared by concurrent processes. Conventional techniques for implementing concurrent objects typically rely on critical sections: ensuring that only one process at a time can operate on the object. Nevertheless, critical sections are poorly suited for asynchronous systems: if one process is halted or delayed in a critical section, other, nonfaulty processes will be unable to progress. By contrast, a concurrent object implementation is lock free if it always guarantees that some process will complete an operation in a finite number of steps, and it is wait free if it guarantees that each process will complete an operation in a finite number of steps. This paper proposes a new methodology for constructing lock-free and wait-free implementations of concurrent objects. The object’s representation and operations are written as stylized sequential programs, with no explicit synchronization. Each sequential operation is automatically transformed into a lock-free or wait-free operation using novel synchronization and memory management algorithms. These algorithms are presented for a multiple instruction/multiple data (MIMD) architecture in which n processes communicate by applying atomic read, wrzte, load_linked, and store_conditional operations to a shared memory.
7|Memory access buffering in multiprocessors|In highly-pipelined machines, instructions and data are prefetched and buffered in both the processor and the cache. This is done to reduce the average memory access la-tency and to take advantage of memory interleaving. Lock-up free caches are designed to avoid processor blocking on a cache miss. Write buffers are often included in a pipelined machine to avoid processor waiting on writes. In a shared memory multiprocessor, there are more advantages in buffering memory requests, since each memory access has to traverse the memory- processor interconnection and has to compete with memory requests issued by different processors. Buffering, however, can cause logical problems in multipro-cessors. These problems are aggravated if each processor has a private memory in which shared writable data may be present, such as in a cache-based system or in a system with a distributed global memory. In this paper, we analyze the benefits and problems associated with the buffering of memory requests in shared memory multiprocessors. We show that the logical problem of buffering is directly related to the problem of synchronization. A simple model is presented to evaluate the performance improvement result-ing from buffering. 1.
8|LimitLESS Directories: A Scalable Cache Coherence Scheme|Caches enhance the performance of multiprocessors by reducing network tra#c and average memory access latency. However, cache-based systems must address the problem of cache coherence. We propose the LimitLESS directory protocol to solve this problem. The LimitLESS scheme uses a combination of hardware and software techniques to realize the performance of a full-map directory with the memory overhead of a limited directory. This protocol is supported by Alewife, a large-scale multiprocessor. We describe the architectural interfaces needed to implement the LimitLESS directory, and evaluate its performance through simulations of the Alewife machine.
9|Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors |The memory consistency model supported by a multiprocessor architecture determines the amount of buffering and pipelining that may be used to hide or reduce the latency of memory accesses. Several different consistency models have been proposed. These range from sequential consistency on one end, allowing very limited buffering, to release consistency on the other end, allowing extensive buffering and pipelining. The processor consistency and weak consistency models fall in between. The advantage of the less strict models is increased performance potential. The disadvantage is increased hardware complexity and a more complex programming model. To make an informed decision on the above tradeoff requires performance data for the various models. This paper addresses the issue of performance benefits from the above four consistency models. Our results are based on simulation studies done for three applications. The results show that in an environment where processor reads are blocking and writes are buffered, a significant performance increase is achieved from allowing reads to bypass previous writes. Pipelining of writes, which determines the rate at which writes are retired from the write buffer, is of secondary importance. As a result, we show that the sequential consistency model performs poorly relative to all other models, while the processor consistency model provides most of the benefits of the weak and release consistency models. 
10|The Expandable Split Window Paradigm for Exploiting Fine-Grain Parallelism|We propose a new processing paradigm, called the Expandable Split Window (ESW) paradigm, for exploiting finegrain parallelism. This paradigm considers a window of instructions (possibly having dependencies) as a single unit, and exploits fine-grain parallelism by overlapping the execution of multiple windows. The basic idea is to connect multiple sequential processors, in a decoupled and decentralized manner, to achieve overall multiple issue. This processing paradigm shares a number of properties of the restricted dataflow machines, but was derived from the sequential von Neumann architecture. We also present an implementation of the Expandable Split Window execution model, and preliminary performance results. 1.
11|A Lock-Free Multiprocessor OS Kernel|Typical shared-memory multiprocessor OS kernels use interlocking, implemented as spinlocks or waiting semaphores. We have implemented a complete multiprocessor OS kernel (including threads, virtual memory, and I/O including a window system and a file system) using only lock-free synchronization methods based on Compare-and-Swap. Lock-free synchronization avoids many serious problems caused by locks: considerable overhead, concurrency bottlenecks, deadlocks, and priority inversion in real-time scheduling. Measured numbers show the low overhead of our implementation, competitive with user-level thread management systems.  Contents  1 Introduction 1 2 Synchronization in OS Kernels 1  2.1 Disabling Interrupts : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 1 2.2 Locking Synchronization Methods : : : : : : : : : : : : : : : : : : : : : : : : : : 2 2.3 Lock-Free Synchronization Methods : : : : : : : : : : : : : : : : : : : : : : : : : 2  3 Lock-Free Quajects 3  3.1 LIFO ...
12|Detecting Violations of Sequential Consistency|The performance of a multiprocessor is directly affected by the choice of the memory consistency model supported. Several different consistency models have been proposed in the literature. These range from sequential consistency on one end, allowing limited buffering of memory accesses, to release consistency on the other end, allowing extensive buffering and pipelining. While the relaxed models such as release consistency provide potential for higher performance, they present a more complex programming model than sequential consistency. Previous research has addressed this tradeoff by showing that a release consistent architecture provides sequentially consistent executions for programs that are free of data races. However, the burden of guaranteeing that the program is free of data races remains with the programmer or compiler.
13|Eraser: a dynamic data race detector for multithreaded programs|Multi-threaded programming is difficult and error prone. It is easy to make a mistake in synchronization that produces a data race, yet it can be extremely hard to locate this mistake during debugging. This paper describes a new tool, called Eraser, for dynamically detecting data races in lock-based multi-threaded programs. Eraser uses binary rewriting techniques to monitor every shared memory reference and verify that consistent locking behavior is observed. We present several case studies, including undergraduate coursework and a multi-threaded Web search engine, that demonstrate the effectiveness of this approach. 1
14|Time, Clocks, and the Ordering of Events in a Distributed System|The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.  
15|Atom: A system for building customized program analysis tools|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
16|Monitors: An Operating System Structuring Concept|This is a digitized copy derived from an ACM copyrighted work. It is not guaranteed to be an accurate copy of the author&#039;s original work. This paper develops Brinch-Hansen&#039;s concept of a monitor as a method of structuring an operating system. It introduces a form of synchronization, describes a possible rnctltotl of implementation in terms of semaphorcs and gives a suitable proof rule. Illustrative examples include a single rcsourcc scheduler, a bounded buffer, an alarm clock, a buffer pool, a disk head optimizer, and a version of the problem of readers and writers. Key Words and Phrases: monitors, operating systems,schcduling, mutual exclusion, synchronization, system implementation langua yes, structured multiprogramming CR Categories:
17|Extensibility, safety and performance in the SPIN operating system|This paper describes the motivation, architecture and performance of SPIN, an extensible operating system. SPIN provides an extension infrastructure, together with a core set of extensible services, that allow applications to safely change the operating system&#039;s interface and implementation. Extensions allow an application to specialize the underlying operating system in order to achieve a particular level of performance and functionality. SPIN uses language and link-time mechanisms to inexpensively export ne-grained interfaces to operating system services. Extensions are written in a type safe language, and are dynamically linked into the operating system kernel. This approach o ers extensions rapid access to system services, while protecting the operating system code executing within the kernel address space. SPIN and its extensions are written in Modula-3 and run on DEC Alpha workstations. 1
18|Petal: Distributed Virtual Disks|The ideal storage system is globally accessible, always available, provides unlimited performance and capacity for a large number of clients, and requires no management. This paper describes the design, implementation, and performance of Petal, a system that attempts to approximate this ideal in practice through a novel combination of features. Petal consists of a collection of networkconnected servers that cooperatively manage a pool of physical disks. To a Petal client, this collection appears as a highly available block-level storage system that provides large abstract containers called virtual disks. A virtual disk is globally accessible to all Petal clients on the network. A client can create a virtual disk on demand to tap the entire capacity and performance of the underlying physical resources. Furthermore, additional resources, such as servers and disks, can be automatically incorporated into Petal. We have an initial Petal prototype consisting of four 225 MHz DEC 3000/700 work...
20|Shasta: A LowOverhead Software-Only Approach to Fine-Grain Shared Memory|Digital Equipment Corporation This paper describes Shasta, a system that supports a shared address space in software on clusters of computers with physically distributed memory. A unique aspect of Shasta compared to most other software distributed shared memory systems is that shared data can be kept coherent at a fine granularity. In addition, the system allows the coherence granularity to vary across different shared data structures in a single application. Shasta implements the shared address space by transparently rewriting the application executable to intercept loads and stores. For each shared load or store, the inserted code checks to see if the data is available locally and communicates with other processors if necessary. The system uses numerous techniques to reduce the run-time overhead of these checks. Since Shasta is implemented entirely in software,
21|Experience with Processes and Monitors in Mesa|The use of monitors for describing concurrency has been much discussed in the literature. When monitors are used in real systems of any size, however, a number of problems arise which have not been adequately dealt with: the semantics of nested monitor calls; the various ways of defining the meaning of WAIT; priority scheduling; handling of timeouts, aborts and other exceptional conditions; interactions with process creation and destruction; monitoring large numbers of small objects. These problems are addressed by the facilities described here for concurrent programming in Mesa. Experience with several substantial applications gives us some confidence in the validity of our solutions.
22|On-the-fly Detection of Data Races for Programs with Nested Fork-Join Parallelism|Detecting data races in shared-memory parallel programs is an important debugging problem. This paper presents a new protocol for run-time detection of data races in executions of shared-memory programs with nested fork-join parallelism and no other interthread synchronization. This protocol has significantly smaller worst-case run-time overhead than previous techniques. The worst-case space required by our protocol when monitoring an execution of a program P is O(V N), where V is the number of shared variables in P , and N is the maximum dynamic nesting of parallel constructs in P &#039;s execution. The worst-case time required to perform any monitoring operation is O(N). We formally prove that our new protocol always reports a non-empty subset of the data races in a monitored program execution and describe how this property leads to an e#ective debugging strategy.
23|Online Data-Race Detection via Coherency Guarantees|We present the design and evaluation of an on-thefly data-race-detection technique that handles applications written for the lazy release consistent (LRC) shared memory model. We require no explicit association between synchronization and shared memory. Hence, shared accesses have to be tracked and compared at the minimum granularity of data accesses, which is typically a single word.  The novel aspect of this system is that we are able to leverage information used to support the underlying memory abstraction to perform on-the-fly data-race detection, without compiler support. Our system consists of a minimally modified version of the CVM distributed shared memory system, and instrumentation code inserted by the ATOM code re-writer.  We present an experimental evaluation of our technique by using our system to look for data races in four unaltered programs. Our system correctly found read-write data races in a program that allows unsynchronized read access to a global tour bound, and a write-write race in a program from a standard benchmark suite. Overall, our mechanism reduced program performance by approximately a factor of two.   
24|Compile-time Support for Efficient Data Race Detection in Shared-Memory Parallel Programs|Data race detection strategies based on software runtime monitoring are notorious for dramatically inflating execution times of shared-memory parallel programs. Without significant reductions in the execution overhead incurred when using these techniques, there is little hope that they will be widely used. A promising approach to this problem is to apply compile-time analysis to identify variable references that need not be monitored at run time because they will never be involved in a data race. In this paper we describe eraser, a data race instrumentation tool that uses aggressive program analysis to prune the number of references to be monitored. To quantify the effectiveness of our analysis techniques, we compare the overhead of race detection with three levels of compile-time analysis ranging from little analysis to aggressive interprocedural analysis for a suite of test programs. For the programs tested, using interprocedural analysis and dependence analysis dramatically reduced ...
25|Implementing data cubes efficiently|Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensional data cubes. Each cell of the data cube is a view consisting of an aggregation of interest, like total sales. The values of many of these cells are dependent on the values of other cells in the data cube..A common and powerful query optimization technique is to materialize some or all of these cells rather than compute them from raw data each time. Commercial systems differ mainly in their approach to materializing the data cube. In this paper, we investigate the issue of which cells (views) to materialize when it is too expensive to materialize all views. A lattice framework is used to express dependencies among views. We present greedy algorithms that work off this lattice and determine a good set of views to materialize. The greedy algorithm performs within a small constant factor of optimal under a variety of models. We then consider the most common case of the hypercube lattice and examine the choice of materialized views for hypercubes in detail, giving some good tradeoffs between the space used and the average time to answer a query. 1
26|Data cube: A relational aggregation operator generalizing group-by, cross-tab, and sub-totals|Abstract. Data analysis applications typically aggregate data across many dimensions looking for anomalies or unusual patterns. The SQL aggregate functions and the GROUP BY operator produce zero-dimensional or one-dimensional aggregates. Applications need the N-dimensional generalization of these operators. This paper defines that operator, called the data cube or simply cube. The cube operator generalizes the histogram, crosstabulation, roll-up, drill-down, and sub-total constructs found in most report writers. The novelty is that cubes are relations. Consequently, the cube operator can be imbedded in more complex non-procedural data analysis programs. The cube operator treats each of the N aggregation attributes as a dimension of N-space. The aggregate of a particular set of attribute values is a point in this space. The set of points forms an N-dimensional cube. Super-aggregates are computed by aggregating the N-cube to lower dimensional spaces. This paper (1) explains the cube and roll-up operators, (2) shows how they fit in SQL, (3) explains how users can define new aggregate functions for cubes, and (4) discusses efficient techniques to compute the cube. Many of these features are being added to the SQL Standard.
27|Query evaluation techniques for large databases|Database management systems will continue to manage large data volumes. Thus, efficient algorithms for accessing and manipulating large sets and sequences will be required to provide acceptable performance. The advent of object-oriented and extensible database systems will not solve this problem. On the contrary, modern data models exacerbate it: In order to manipulate large sets of complex objects as efficiently as today’s database systems manipulate simple records, query processing algorithms and software will become more complex, and a solid understanding of algorithm and architectural issues is essential for the designer of database management software. This survey provides a foundation for the design and implementation of query execution facilities in new database management systems. It describes a wide array of practical query evaluation techniques for both relational and post-relational database systems, including iterative execution of complex query evaluation plans, the duality of sort- and hash-based set matching algorithms, types of parallel query execution and their implementation, and special operators for emerging database application domains.
28|Multi-table joins through bitmapped join indices|This technical note shows how to combine some well-known techniques to create a method that will efficiently execute common multi-table joins. We concentrate on a commonly occurring type of join known as a star-join, although the method presented will generalize to any type of multi-table join. A star-join consists of a central detail table with large cardinality, such as an orders table (where an order row contains a single purchase) with foreign keys that join to descriptive tables, such as customers, products, and (sales) agents. The method presented in this note uses join indices with compressed bitmap representations, which allow predicates restricting columns of descriptive tables to determine an answer set (or foundsef) in the central detail table; the method uses different predicates on different descriptive tables in combination to restrict the detail table through compressed bitmap representations of join indices, and easily completes the join of the fully restricted detail table rows back to the descriptive tables. We outline realistic examples where the combination of these techniques yields substantial performance improvements over alternative, more traditional query evaluation plans. 1.
29|Aggregate-Query Processing in Data Warehousing Environments|In this paper we introduce generalized projections (GPs), an extension of duplicate eliminating projections, that capture aggregations, groupbys, duplicate-eliminating projections (distinct), and duplicate-preserving projections in a common unified framework. Using GPs we extend well known and simple algorithms for SQL queries that use distinct projections to derive algorithms for queries using aggregations like sum, max, min, count, and avg. We develop powerful query rewrite rules for aggregate queries that unify and extend rewrite rules previously known in the literature. We then illustrate the power of our approach by solving a very practical and important problem in data warehousing: how to answer an aggregate query on base tables using materialized aggregate views (summary tables).
30|Including Group-By in Query Optimization|In existing relational database systems, processing of group-by and computation of aggregate functions are always postponed until all joins are performed. In this paper, we present transformations that make it possible to push group-by operation past one or more joins and can potentially reduce the cost of processing a query significantly. Therefore, the placement of group-by should be decided based on cost estimation. We explain how the traditional System-R style optimizers can be modified by incorporating the greedy conservative heuristic that we developed. We prove that applications of greedy conservative heuristic produce plans that are better (or no worse) than the plans generated by a traditional optimizer. Our experimental study shows that the extent of improvement in the quality of plans is significant with only a modest increase in optimization cost. Our technique also applies to optimization of Select Distinct queries by pushing down duplicate elimination in a cost-based fas...
31|Eager Aggregation and Lazy Aggregation|Efficient processing of aggregation queries is essential for decision support applications. This paper describes a class of query transformations, called eager aggregation and lazy aggregation, that allows a query optimizer to move group-by operations up and down the query tree. Eager aggregation partially pushes a group-by past a join. After a group-by is partially pushed down, we still need to perform the original group-by in the upper query block. Eager aggregation reduces the number of input rows to the join and thus may result in a better overall plan. The reverse transformation, lazy aggregation, pulls a group-by above a join and combines two group-by operations into one. This transformation is typically of interest when an aggregation query references a grouped view (a view containing a group-by). Experimental results show that the technique is very beneficial for queries in the TPC-D benchmark. 1 Introduction Aggregation is widely used in decision support systems. All queries...
32|The Lorel Query Language for Semistructured Data|We present the Lorel language, designed for querying semistructured data. Semistructured data is becoming more and more prevalent, e.g., in structured documents such as HTML and when performing simple integration of data from multiple sources. Traditional data models and query languages are inappropriate, since semistructured data often is irregular, some data is missing, similar concepts are represented using different types, heterogeneous sets are present, or object structure is not fully known. Lorel is a user-friendly language in the SQL/OQL style for querying such data effectively. For wide applicability, the simple object model underlying Lorel can be viewed as an extension of ODMG and the language as an extension of OQL. The main novelties of the Lorel language are: (i) extensive use of coercion to relieve the user from the strict typing of OQL, which is inappropriate for semistructured data
33|Querying Semi-Structured Data|

34|Object exchange across heterogeneous information sources|We address the problem of providing integrated access to diverse and dynamic information sources. We explain how this problem differs from the traditional database integration problem and we focus on one aspect of the information integration problem, namely information exchange. We define an object-based information exchange model and a corresponding query language that we believe are well suited for integration of diverse information sources. We describe how, the model and language have been used to integrate heterogeneous bibliographic information sources. We also describe two general-purpose libraries we have implemented for object exchange between clients and servers.
35|Querying object-oriented databases|We present a novel language for querying object-oriented databases. The language is built around the idea of extended path expressions that substantially generalize [ZAN83], and on an adaptation of the first-order formalization of object-oriented languages from [KW89, KLW90, KW92]. The language incorporates features not found in earlier proposals; it is easier to use and has greater expressive power. Some of the salient features of our language are: ffl Precise model-theoretic semantics. ffl A very expressive form of path expressions that not only can do joins, selections and unnesting, but can also be used to explore the database schema. ffl Views can be defined and manipulated in a much more uniform way than in other proposals. ffl Database schema can be explored in the very same language that is used to retrieve data. Unlike in relational languages, the user needs not know anything about the system tables that store schema information. ffl The notions of a type and type-correctness have precise meaning. It accommodates a wide variety of queries that might be deemed well- or ill-typed under different circumstances. In particular, we show that there is more than one way of settling the issue of type correctness. For expository purposes and due to space limitation, we chose to make a number of simplifying assumptions and left some features out. A more complete account can be found in [KSK92].
36|From Structured Documents to Novel Query Facilities|Structured documents (e.g., SGML) can benefit a lot from database support and more specifically from object-oriented database (OODB) management systems. This paper describes a natural mapping from SGML documents into OODB&#039;s and a formal extension of two OODB query languages (one SQL-like and the other calculus) in order to deal with SGML document retrieval. Although motivated by structured documents, the extensions of query languages that we present are general and useful for a variety of other OODB applications. A key element is the introduction of paths as first class citizens. The new features allow to query data (and to some extent schema) without exact knowledge of the schema in a simple and homogeneous fashion. 1 Introduction  Structured documents are central to a wide class of applications such as software engineering, libraries, technical documentation, etc. They are often stored in file systems and document access tools are somewhat limited. We believe that (object-oriented) d...
37|Object fusion in mediator systems|One of the main tasks of mediators is to fuse information from heterogeneous information sources. This may involve, for example, removing redundancies, and resolving inconsistencies in favor of the most reliable source. The problem becomes harder when the sources are unstructured/semistructured and we do not have complete knowledge of their contents and structure. In this paper we show how many common fusion operations can be specified non-procedurally and succinctly. The key to our approach is to assign semantically meaningful object ids to objects as they are &amp;quot;imported &amp;quot; into the mediator.
38|Finding Regular Simple Paths In Graph Databases|We consider the following problem: given a labelled directed graph G and a regular expression R, find all pairs of nodes connected by a simple path such that the concatenation of the labels along the path satisfies R. The problem is motivated by the observation that many recursive queries in relational databases can be expressed in this form, and by the implementation of a query language, G+ , based on this observation. We show that the problem is in general intractable, but present an algorithm than runs in polynomial time in the size of the graph when the regular expression and the graph are free of conflicts. We also present a class of languages whose expressions can always be evaluated in time polynomial in the size of both the graph and the expression, and characterize syntactically the expressions for such languages. 
39|MedMaker: A Mediation System Based on Declarative Specifications|Mediators are used for integration of heterogeneous information sources. We present a system for declaratively specifying mediators. It is targeted for integration of sources with unstructured or semi-structured data and/or sources with changing schemas. We illustrate the main features of the Mediator Specification Language (MSL), show how they facilitate integration, and describe the implementation of the system that interprets the MSL specifications. 
40|Representative Objects: Concise Representations of Semistructured Hierarchical Data|In this paper we introduce the representative object, which uncovers the inherent schema(s) in semistructured, hierarchical data sources and provides a concise description of the structure of the data. Semistructured data, unlike data stored in typical relational or object-oriented databases, does not have fixed schema that is known in advance and stored separately from the data. Withthe rapid growth of the World Wide Web, semistructured hierarchical data sources are becoming widely available to the casual user. The lack of external schema information currently makes browsing and querying these data sources inefficient at best, and impossible at worst. We show how representative objects make schema discovery efficient and facilitate the generation of meaningful queries over the data. 1.
42|Evaluating Queries with Generalized Path Expressions|In the past few years, query languages featuring generalized path expressions have been proposed. These languages allow the interrogation of both data and structure. They are powerful and essential for a number of applications. However, until now, their evaluation has relied on a rather naive and inefficient algorithm. In this paper, we extend an object algebra with two new operators and present some interesting rewriting techniques for queries featuring generalized path expressions. We also show how a query optimizer can integrate the new techniques.  1 Introduction  In the past few years there has been a growing interest in query languages featuring generalized path expressions (GPE) [BRG88, KKS92, CACS94, QRS  +  95]. With these languages, one may issue queries on data without exact knowledge of its structure. A GPE queries data and structure at the same time. Although very useful for standard database applications, these languages are vital for new applications dedicated, for insta...
43|Integrating a Structured-Text Retrieval System with an Object-Oriented Database System|We describe the integration of a structured-text retrieval system (TextMachine) into an  object-oriented database system (OpenODB). Our approach is a light-weight one, using the external  function capability of the database system to encapsulate the text retrieval system as an  external information source. Yet, we are able to provide a tight integration in the query language  and processing; the user can access the text retrieval system using a standard database query  language. The efficient and effective retrieval of structured text performed by the text retrieval  system is seamlessly combined with the rich modeling and general-purpose querying capabilities  of the database system, resulting in an integrated system with querying power beyond those of  the underlying systems. The integrated system also provides uniform access to textual data in  the text retrieval system and structured data in the database system, thereby achieving information  fusion. We discuss the design and imple...
44|Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks|Dryad is a general-purpose distributed execution engine for coarse-grain data-parallel applications. A Dryad applica-tion combines computational “vertices ” with communica-tion “channels ” to form a dataflow graph. Dryad runs the application by executing the vertices of this graph on a set of available computers, communicating as appropriate through files, TCP pipes, and shared-memory FIFOs. The vertices provided by the application developer are quite simple and are usually written as sequential programs with no thread creation or locking. Concurrency arises from Dryad scheduling vertices to run simultaneously on multi-ple computers, or on multiple CPU cores within a computer. The application can discover the size and placement of data at run time, and modify the graph as the computation pro-gresses to make efficient use of the available resources. Dryad is designed to scale from powerful multi-core sin-gle computers, through small clusters of computers, to data centers with thousands of computers. The Dryad execution engine handles all the difficult problems of creating a large distributed, concurrent application: scheduling the use of computers and their CPUs, recovering from communication or computer failures, and transporting data between ver-tices.
45|The Google File System |We have designed and implemented the Google File Sys-tem, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous dis-tributed file systems, our design has been driven by obser-vations of our application workloads and technological envi-ronment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore rad-ically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our ser-vice as well as research and development efforts that require large data sets. The largest cluster to date provides hun-dreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use. Categories and Subject Descriptors D [4]: 3—Distributed file systems
46|The click modular router| Click is a new software architecture for building flexible and configurable routers. A Click router is assembled from packet processing modules called elements. Individual elements implement simple router functions like packet classification, queueing, scheduling, and interfacing with network devices. A router configuration is a directed graph with elements at the vertices; packets flow along the edges of the graph. Configurations are written in a declarative language that supports user-defined abstractions. This language is both readable by humans and easily manipulated by tools. We present language tools that optimize router configurations and ensure they satisfy simple invariants. Due to Click’s architecture and language, Click router configurations are modular and easy to extend. A standards-compliant Click IP router has sixteen elements on its forwarding path. We present extensions to this router that support dropping policies, fairness among flows, quality-of-service, and
47|PVM: A Framework for Parallel Distributed Computing|The PVM system is a programming environment for the development and execution of large concurrent or parallel applications that consist of many interacting, but relatively independent, components. It is intended to operate on a collection of heterogeneous computing elements interconnected by one or more networks. The participating processors may be scalar machines, multiprocessors, or special-purpose computers, enabling application components to execute on the architecture most appropriate to the algorithm. PVM provides a straightforward and general interface that permits the description of various types of algorithms (and their interactions), while the underlying infrastructure permits the execution of applications on a virtual computing environment that supports multiple parallel computation models. PVM contains facilities for concurrent, sequential, or conditional execution of application components, is portable to a variety of architectures, and supports certain forms of error dete...
48|Cilk: An Efficient Multithreaded Runtime System|Cilk (pronounced &#034;silk&#034;) is a C-based runtime system for multithreaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the &#034;work&#034; and &#034;critical-path length&#034; of a Cilk computation can be used to model performance accurately. Consequently, a Cilk programmer can focus on reducing the computation&#039;s work and critical-path length, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of &#034;fully strict&#034; (well-structured) programs, the Cilk scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The Cilk
49|Parallel database systems: the future of high performance database systems|Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1.
50|Distributed Computing in Practice: The Condor Experience|Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational grid. In this chapter, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course traveled by research ideas as they grow into production systems.
51|Cluster-Based Scalable Network Services|This paper has benefited from the detailed and perceptive comments of our reviewers, especially our shepherd Hank Levy. We thank Randy Katz and Eric Anderson for their detailed readings of early drafts of this paper, and David Culler for his ideas on TACC&#039;s potential as a model for cluster programming. Ken Lutz and Eric Fraser configured and administered the test network on which the TranSend scaling experiments were performed. Cliff Frost of the UC Berkeley Data Communications and Networks Services group allowed us to collect traces on the Berkeley dialup IP network and has worked with us to deploy and promote TranSend within Berkeley. Undergraduate researchers Anthony Polito, Benjamin Ling, and Andrew Huang implemented various parts of TranSend&#039;s user profile database and user interface. Ian Goldberg and David Wagner helped us debug TranSend, especially through their implementation of the rewebber
53|Interpreting the Data: Parallel Analysis with Sawzall |Very large data sets often have a flat but regular structure and span multiple disks and machines. Examples include telephone call records, network logs, and web document repositories. These large data sets are not amenable to study using traditional database techniques, if only because they can be too large to fit in a single relational database. On the other hand, many of the analyses done on them can be expressed using simple, easily distributed computations: filtering, aggregation, extraction of statistics, and so on. We present a system for automating such analyses. A filtering phase, in which a query is expressed using a new procedural programming language, emits data to an aggregation phase. Both phases are distributed over hundreds or even thousands of computers. The results are then collated and saved to a file. The design—including the separation into two phases, the form of the programming language, and the properties of the aggregators—exploits the parallelism inherent in having data and computation distributed across many machines. 1
54|The Gamma database machine project|This paper describes the design of the Gamma database machine and the techniques employed in its implementation. Gamma is a relational database machine currently operating on an Intel iPSC/2 hypercube with 32 processors and 32 disk drives. Gamma employs three key technical ideas which enable the architecture to be scaled to 100s of processors. First, all relations are horizontally partitioned across multiple disk drives enabling relations to be scanned in parallel. Second, novel parallel algorithms based on hashing are used to implement the complex relational operators such as join and aggregate functions. Third, dataflow scheduling techniques are used to coordinate multioperator queries. By using these techniques it is possible to control the execution of very complex queries with minimal coordination- a necessity for configurations involving a very large number of processors. In addition to describing the design of the Gamma software, a thorough performance evaluation of the iPSC/2 hypercube version of Gamma is also presented. In addition to measuring the effect of relation size and indices on the response time for selection, join, aggregation, and update queries, we also analyze the performance of Gamma relative to the number of processors employed when the sizes of the input relations are kept constant (speedup) and when the sizes of the input relations are increased proportionally to the number of processors (scaleup). The speedup results obtained for both selection and join queries are linear; thus, doubling the number of processors
55|Programming Parallel Algorithms|In the past 20 years there has been treftlendous progress in developing and analyzing parallel algorithftls. Researchers have developed efficient parallel algorithms to solve most problems for which efficient sequential solutions are known. Although some ofthese algorithms are efficient only in a theoretical framework, many are quite efficient in practice or have key ideas that have been used in efficient implementations. This research on parallel algorithms has not only improved our general understanding ofparallelism but in several cases has led to improvements in sequential algorithms. Unf:ortunately there has been less success in developing good languages f:or prograftlftling parallel algorithftls, particularly languages that are well suited for teaching and prototyping algorithms. There has been a large gap between languages
56|Programmable stream processors|The Imagine Stream Processor is a single-chip programmable media processor with 48 parallel ALUs. At 400 MHz, this translates to a peak arithmetic rate of 16 GFLOPS on single-precision data and 32 GOPS on 16bit fixed-point data. The scalability of Imagine’s programming model and architecture enable it to achieve such high arithmetic rates. Imagine executes applications that have been mapped to the stream programming model. The stream model decomposes applications into a set of computation kernels that operate on data streams. This mapping exposes the inherent locality and parallelism in the application, and Imagine exploits the locality and parallelism to provide a scalable architecture that supports 48 ALUs on a single chip. This paper presents the Imagine architecture and programming model in the first half, and explores the scalability of the Imagine architecture in the second half. 1.
57|Accelerator: using data parallelism to program GPUs for general-purpose uses|GPUs are difficult to program for general-purpose uses. Programmers can either learn graphics APIs and convert their applications to use graphics pipeline operations or they can use stream programming abstractions of GPUs. We describe Accelerator, a system that uses data parallelism to program GPUs for general-purpose uses instead. Programmers use a conventional imperative programming language and a library that provides only high-level data-parallel operations. No aspects of GPUs are exposed to programmers. The library implementation compiles the data-parallel operations on the fly to optimized GPU pixel shader code and API calls. We describe the compilation techniques used to do this. We evaluate the effectiveness of using data parallelism to program GPUs by providing results for a set of compute-intensive benchmarks. We compare the performance of Accelerator versions of the benchmarks against hand-written pixel shaders. The speeds of the Accelerator versions are typically within 50 % of the speeds of hand-written pixel shader code. Some benchmarks significantly outperform C versions on a CPU: they are up to 18 times faster than C code running on a CPU.
58|Using Cohort Scheduling to Enhance Server Performance|A server application is commonly organized as a collection of concurrent threads, each of which executes the code necessary to process a request. This software architecture, which causes frequent control transfers between unrelated pieces of code, decreases instruction and data locality, and consequently reduces the effec- tiveness of hardware mechanisms such as caches, TLBs, and branch predictors. Numerous measurements demonstrate this effect in server applications, which often utilize only a fraction of a modern processor&#039;s computational throughput.
59|The CODE 2.0 Graphical Parallel Programming Language |CODE 2.0 is a graphical parallel programming system that targets the three goals of ease of use, portability, and production of efficient parallel code. Ease of use is provided by an integrated graphical/textual interface, a powerful dynamic model of parallel computation, and an integrated concept of program component reuse. Portability is approached by the declarative expression of synchronization and communication operators at a high level of abstraction in a manner which cleanly separates overall computation structure from the primitive sequential computations that make up a program. Execution efficiency is approached through a systematic class hierarchy that supports hierarchical translation refinement including special case recognition. This paper reports results obtained through experimental use of a prototype implementation of the CODE 2.0 system. CODE 2.0 represents a major conceptual advance over its predecessor systems (CODE 1.0 and CODE 1.2) in terms of the expressive power ...
60|Stream Computations Organized for Reconfigurable Execution (SCORE): Introduction and Tutorial  (2000) |A primary impediment to wide-spread exploitation of reconfigurable computing is the lack of a unifying computational model which allows application portability and longevity without sacrificing a substantial fraction of the raw capabilities. We introduce SCORE (Stream Computation Organized for Reconfigurable Execution), a streambased compute model which virtualizes reconfigurable computing resources (compute, storage, and communication) by dividing a computation up into fixed-size &#034;pages&#034; and time-multiplexing the virtual pages on available physical hardware. Consequently, SCORE applications can scale up or down automatically to exploit a wide range of hardware sizes. We hypothesize that the SCORE model will ease development and deployment of reconfigurable applications and expand the range of applications which can benefit from reconfigurable execution. Further, we believe that a well engineered SCORE implementation can be efficient, wasting little of the capabilities of the raw hardw...
61|Paralex: An Environment for Parallel Programming in Distributed Systems|Modern distributed systems consisting of powerful workstations and high-speed interconnection networks are an economical alternative to special-purpose super computers. The technical issues that need to be addressed in exploiting the parallelism inherent in a distributed system include heterogeneity, high-latency communication, fault tolerance and dynamic load balancing. Current software systems for parallel programming provide little or no automatic support towards these issues and require users to be experts in fault-tolerant distributed computing. The Paralex system is aimed at exploring the extent to which the parallel application programmer can be liberated from the complexities of distributed systems. Paralex is a complete programming environment and makes extensive use of graphics to define, edit, execute and debug parallel scientific applications. All of the necessary code for distributing the computation across a network and replicating it to achieve fault tolerance and dynamic load balancing is automatically generated by the system. In this paper we give an overview of Paralex and present our experiences with a prototype implementation.
62|Parallel and Distributed Haskells|Parallel and distributed languages specify computations on multiple processors and have a computation language to describe the algorithm, i.e. what to compute, and a coordination language to describe how to organise the computations across the processors. Haskell has been used as the computation language for a wide variety of parallel and distributed languages, and this paper is a comprehensive survey of implemented languages. We outline parallel and distributed language concepts and classify Haskell extensions using them. Similar example programs are used to illustrate and contrast the coordination languages, and the comparison is facilitated by the common computation language. A lazy language is not an obvious choice for parallel or distributed computation, and we address the question of why Haskell is a common functional computation language.
63|DB2 Parallel Edition|The rate of increase in database size and response time requirements has outpaced  advancements in processor and mass storage technology. One way to satisfy the increasing  demand for processing power and I/O bandwidth in database applications is to have a  number of processors, loosely or tightly coupled, serving database requests concurrently.  Technologies developed during the last decade have made commercial parallel database  systems a reality and these systems have made an inroad into the stronghold of traditionally  mainframe based large database applications. This paper describes the parallel database  project initiated at IBM Research at Hawthorne and the DB2/AIX-PE product based on  it.  1 Introduction  Large scale parallel processing technology has made giant strides in the past decade and there is no doubt that it has established a place for itself. However, almost all of the applications harnessing this technology are scientific or engineering applications. The lack of com...
64|Highly Available, Fault-Tolerant, Parallel Dataflows|We present a technique that masks failures in a cluster to provide high availability and fault-tolerance for long-running, parallelized dataflows. We can use these dataflows to implement a variety of continuous query (CQ) applications that require high-throughput, 24x7 operation. Examples include network monitoring, phone call processing, click-stream processing, and online financial analysis. Our main contribution is a scheme that carefully integrates traditional query processing techniques for partitioned parallelism with the process-pairs approach for high availability. This delicate integration allows us to tolerate failures of portions of a parallel dataflow  without sacrificing result quality. Upon failure, our technique provides quick fail-over, and automatically recovers the lost pieces on the fly. This piecemeal recovery provides minimal disruption to the ongoing dataflow computation and improved reliability as compared to the straight-forward application of the process-pairs technique on a per dataflow basis. Thus, our technique provides the high availability necessary for critical CQ applications. Our techniques are encapsulated in a reusable dataflow operator called Flux, an extension of the Exchange that is used to compose parallel dataflows. Encapsulating the fault-tolerance logic into Flux minimizes modifications to existing operator code and relieves the burden on the operator writer of repeatedly implementing and verifying this critical logic. We present experiments illustrating these features with an implementation of Flux in the TelegraphCQ code base [8].
65|A comparison of stream-oriented high availability algorithms|Recently, significant efforts have focused on developing novel data-processing systems to support a new class of applications that commonly require sophisticated and timely processing of high-volume data streams. Early work in stream processing has primarily focused on streamoriented languages and resource-constrained, one-pass query-processing. High availability, an increasingly important goal for virtually all data processing systems, is yet to be addressed. In this paper, we first describe how the standard highavailability approaches used in data management systems can be applied to distributed stream processing. We then propose a novel stream-oriented approach that exploits the unique data-flow nature of streaming systems. Using analysis and a detailed simulation study, we characterize the performance of each approach and demonstrate that the stream-oriented algorithm significantly reduces runtime overhead at the expense of a small increase in recovery time. 1
66|Bigtable: A distributed storage system for structured data|Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.  
67|MapReduce: Simplified Data Processing on Large Clusters|MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.  
68|Space/Time Trade-offs in Hash Coding with Allowable Errors|this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash- coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency
69|The ubiquitous B-tree|B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and nnplemented using B-trees This paper reviews B-trees and shows why they have been so successful It discusses the major variations of the B-tree, especially the B+-tree,
70|Resource Containers: A New Facility for Resource Management in Server Systems|General-purpose operating systems provide inadequate support for resource management in large-scale servers. Applications lack sufficient control over scheduling and management of machine resources, which makes it difficult to enforce priority policies, and to provide robust and controlled service. There is a fundamental mismatch between the original design assumptions underlying the resource management mechanisms of current general-purpose operating systems, and the behavior of modern server applications. In particular, the operating system’s notions of protection domain and resource principal coincide in the process abstraction. This coincidence prevents a process that manages large numbers of network connections, for example, from properly allocating system resources among those connections. We propose and evaluate a new operating system abstraction called a resource container, which separates the notion of a protection domain from that of a resource principal. Resource containers enable fine-grained resource management in server systems and allow the development of robust servers, with simple and firm control over priority policies. 1
71|Operating System Support for Planetary-Scale Network Services|PlanetLab is a geographically distributed overlay network designed to support the deployment and evaluation of planetary-scale network services. Two high-level goals shape its design. First, to enable a large research community to share the infrastructure, PlanetLab provides distributed virtualization, whereby each service runs in an isolated slice of PlanetLab’s global resources. Second, to support competition among multiple network services, PlanetLab decouples the operating system running on each node from the networkwide services that define PlanetLab, a principle referred to as unbundled management. This paper describes how Planet-Lab realizes the goals of distributed virtualization and unbundled management, with a focus on the OS running on each node. 1
72|Paxos made live: an engineering perspective|We describe our experience building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected algorithmic and engineering problems encountered, and the solutions we found for them. Our measurements indicate that we have built a competitive system. 1
73|Integrating compression and execution in column-oriented database systems|Column-oriented database system architectures invite a reevaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads. In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in roworiented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.
74|Weaving Relations for Cache Performance|Relational database systems have traditionally optimzed for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM) (a.k.a., slotted pages). Recent research, however, indicates that cache utilization and performance is becoming increasingly important on modern platforms. In this paper, we first demonstrate that in-page data placement is the key to high cache performance and that NSM exhibits low cache utilization on modern platforms. Next, we propose a new data organization model called PAX (Partition Attributes Across), that significantly improves cache performance by grouping together all values of each attribute within each page. Because PAX only affects layout inside the pages, it incurs no storage penalty and does not affect I/O behavior. According to our experimental results, when compared to NSM (a) PAX exhibits superior cache and memory bandwidth utilization, saving at least 75% of NSM&#039;s stall time due to data cache accesses, (b) range selection queries and updates on memoryresident relations execute 17-25% faster, and (c) TPC-H queries involving I/O execute 11-48% faster.
75|Data Placement in Bubba|Thus paper examines the problem of data placement in Bubba, a highly-parallel system for data-intensive applications bemg developed at MCC “Highly-parallel” lmplres that load balancmng IS a cntlcal performance issue “Data-mtenave” means data IS so large that operatrons should be executed where the data resides As a result, data placement becomes a cntlcal performance issue In general, determmmg the optimal placement of data across processmg nodes for performance IS a difficult problem We describe our heuristic approach to solvmg the data placement problem w Bubba We then present expenmental results using a specific workload to provide msrght into the problem Several researchers have argued the benefits of deelustering (1 e, spreading each base relation over many nodes) We show that as declustermg IS increased. load balancing contmues to improve However, for transactions mvolvmg complex Joins, further declusterrng reduces throughput because of communications, startup and termmatron overhead We argue that data placement, especially declustermg, m a highly-parallel system must be considered early in the design, so that mechanrsms can be included for supportmg variable declustermg, for mmtmlzmg the most significant overheads associated with large-scale declustenng, and for gathering the required statistics.
76|On optimistic methods for concurrency control|Most current approaches to concurrency control in database systems rely on locking of data objects as a control mechanism. In this paper, two families of nonlocking concurrency controls are presented. The methods used are “optimistic ” in the sense that they rely mainly on transaction backup as a control mechanism, “hoping ” that conflicts between transactions will not occur. Applications for which these methods should be more efficient than locking are discussed.
77|Efficient Locking for Concurrent Operations on B-Trees|The B-tree and its variants have been found to be highly useful (both theoretically and in practice) for storing large amounts ofinformation, especially on secondary storage devices. We examine the problem of overcoming the inherent difficulty of concurrent operations on such structures, using a practical storage model. A single additional “link ” pointer in each node allows a process to easily recover from tree modifications performed by other concurrent processes. Our solution compares favorably with earlier solutions in that the locking scheme is simpler (no read-locks are used) and only a (small) constant number of nodes are locked by any update process at any given time. An informal correctness proof for our system is given,
78|Concurrent manipulation of binary search trees|The concurrent manipulation of a binary search tree is considered in this paper. The systems presented can support any number of concurrent processes which perform searching, insertion, deletion, and rotation (reorganization) on the tree, but allow any process to lock only a constant number of nodes at any time. Also, in the systems, searches are essentially never blocked. The concurrency control techniques introduced in the paper include the use of special nodes and pointers to redirect searches, and the use of copies of sections of the tree to introduce many changes simultaneously and therefore avoid unpredictable interleaving. Methods developed in this paper may provide new insights into other problems in the area of concurrent database manipulation.
79|An optimality theory of concurrency control for databases|In many database applications it is desirable that the database system be time-shared among multiple users who access the database in an interactive way. In such a systewi the arriving requests for the execution of steps in
80|Static Scheduling of Synchronous Data Flow Programs for Digital Signal Processing|Large grain data flow (LGDF) programming is natural and convenient for describing digital signal processing (DSP) systems, but its runtime overhead is costly in real time or cost-sensitive applications. In some situations, designers are not willing to squander computing resources for the sake of program-mer convenience. This is particularly true when the target machine is a programmable DSP chip. However, the runtime overhead inherent in most LGDF implementations is not required for most signal processing systems because such systems are mostly synchronous (in the DSP sense). Synchronous data flow (SDF) differs from traditional data flow in that the amount of data produced and consumed by a data flow node is specified a priori for each input and output. This is equivalent to specifying the relative sample rates in signal processing system. This means that the scheduling of SDF nodes need not be done at runtime, but can be done at compile time (statically), so the runtime overhead evaporates. The sample rates can all be different, which is not true of most current data-driven digital signal processing programming methodologies. Synchronous data flow is closely related to computation graphs, a special case of Petri nets. This self-contained paper develops the theory necessary to statically schedule SDF programs on single or multiple proces-sors. A class of static (compile time) scheduling algorithms is proven valid, and specific algorithms are given for scheduling SDF systems onto single or multiple processors. 
81|Petri Nets|Over the last decade, the Petri net has gamed increased usage and acceptance as a basic model of systems of asynchronous concurrent computation. This paper surveys the basic concepts and uses of Petm nets. The structure of Petri nets, their markings and execution, several examples of Petm net models of computer hardware and software, and
82|Weak Ordering -- A New Definition|A memory model for a shared memory, multiprocessor commonly and often implicitly assumed by programmers is that of sequential consistency. This model guarantees that all memory accesses will appear to execute atomically and in program order. An alternative model, weak ordering, offers greater performance potential. Weak ordering was first defined by Dubois, Scheurich and Briggs in terms of a set of rules for hardware that have to be made visible to software. The central hypothesis of this work is that programmers prefer to reason about sequentially consistent memory, rather than having to think about weaker memory, or even write buffers. Following this hypothesis, we re-define weak ordering as a contract between software and hardware. By this contract, software agrees to some formally specified constraints, and hardware agrees to appear sequentially consistent to at least the software that obeys those constraints. We illustrate the power of the new definition with a set of software constraints that forbid data races and an imple-mentation for cache-coherent systems chat is not allowed by the old definition. 
83|An Evaluation of Directory Schemes for Cache Coherence|The problem of cache coherence in shared-memory multiprocessors has been addressed using two basic approaches: directory schemes and snoopy cache schemes. Directory schemes have been given less attention in the past several years, while snoopy cache methods have become extremely popular. Directory schemes for cache coherence are potentially attractive in large multiprocessor systems that are beyond the scaling limits of the snoopy cache schemes. Slight modifications to directory schemes can make them competitive in performance with snoopy cache schemes for small multiprocessors. Trace driven simulation, using data collected from several real multiprocessor applications, is used to compare the performance of standard directory schemes, modifications to these schemes, and snoopy cache protocols. 1 Introduction In the past several years, shared-memory multiprocessors have gained wide-spread attention due to the simplicity of the shared-memory parallel programming model. However, allowing...
85|A tree-based algorithm for distributed mutual exclusion|We present an algorithm for distributed mutual exclusion in a computer network of N nodes that communicate by messages rather than shared memory. The algorithm uses a spanning tree of the computer network, and the number of messages exchanged per critical section depends on the topology of this tree. However, typically the number of messages exchanged is O(log N) under light demand, and reduces to approximately four messages under saturated demand. Each node holds information only about its immediate neighbors in the spanning tree rather than information about all nodes, and failed nodes can recover necessary information from their neighbors. The algorithm does not require sequence numbers as it operates correctly despite message overtaking.
86|The Performance Implications of Thread Management Alternatives for Shared-Memory Multiprocessors|Threads (“lightweight ” processes) have become. a common element of new languages and operating systems. This paper examines the performance implications of several data structure and algorithm alternatives for thread management in shared-memory multiprocessors. Both experimental measurements and analytical model projections are presented For applications with fine-grained parallelism, small differences in thread management are shown to have significant performance impact, often posing a tradeoff between throughput and latency. Per-processor data structures can be used to improve throughput, and in some circumstances to avoid locking, improving latency as well. The method used by processors to queue for locks is also shown to affect performance significantly. Normal methods of critical resource waiting can substantially degrade performance with moderate numbers of waiting processors. We present an Ethernet-style backoff algo rithm that largely eliminates~tiis effect. 1.
87|Efficient Synchronization on Multiprocessors with Shared Memory|A new formalism is given for read-modify-write (RMW) synchronization operations. This formalism is used to extend the memory reference combining mechanism, introduced in the NYU Ultracomputer, to arbitrary RMW operations. A formal correctness proof of this combining mechanism is given. General requirements for the practicality of combining are discussed. Combining is shown to be practical for many useful memory access operations. This includes memory updates of the form mem_val := mem_val op val, where op need not be associative, and a variety of synchronization primitives. The computation involved is shown to be closely related to parallel prefix evaluation. 1. INTRODUCTION  Shared memory provides convenient communication between processes in a tightly coupled multiprocessing system. Shared variables can be used for data sharing, information transfer between processes, and, in particular, for coordination and synchronization. Constructs such as the semaphore introduced by Dijkstra in ...
88|Synchronization in Distributed Programs|this paper, one aspect of the construction of distributed programs is ad- This research was supported in part by National Science Foundation Grant MCS 76-22360
89|Multi-model parallel programming in Psyche|Many different parallel programming models, including lightweight processes that communicate with shared memory and heavyweight processes that communicate with messages, have been used to implement parallel applications. Unfortunately, operating systems and languages designed for parallel programming typically support only one model. Multi-model parallel programming is the simultaneous use of several different models, both across programs and within a single program. This paper describes multi-model parallel programming in the Psyche multiprocessor operating system. We explain why multi-model programming is desirable and present an operating system interface designed to support it. Through a series of three examples, we illustrate how the Psyche operating system supports different models of parallelism and how the different models are able to interact. 1.
90|GPFS: A Shared-Disk File System for Large Computing Clusters|GPFS is IBM&#039;s parallel, shared-disk file system for cluster computers, available on the RS/6000 SP parallel supercomputer and on Linux clusters. GPFS is used on many of the largest supercomputers in the world. GPFS was built on many of the ideas that were developed in the academic community over the last several years, particularly distributed locking and recovery technology. To date it has been a matter of conjecture how well these ideas scale. We have had the opportunity to test those limits in the context of a product that runs on the largest systems in existence. While in many cases existing ideas scaled well, new approaches were necessary in many key areas. This paper describes GPFS, and discusses how distributed locking and recovery techniques were extended to scale to large clusters.
91| Frangipani: A Scalable Distributed File System |The ideal distributed file system would provide all its users with coherent, shared access to the same set of files,yet would be arbitrarily scalable to provide more storage space and higher performance to a growing user community. It would be highly available in spite of component failures. It would require minimal human administration, and administration would not become more complex as more components were added. Frangipani is a new file system that approximates this ideal, yet was relatively easy to build because of its two-layer structure. The lower layer is Petal (described in an earlier paper), a distributed storage service that provides incrementally scalable, highly available, automatically managed virtual disks. In the upper layer, multiple machines run the same Frangipani file system code on top of a shared Petal virtual disk, using a distributed lock service to ensure coherence. Frangipaniis meant to run in a cluster of machines that are under a common administration and can communicate securely. Thus the machines trust one another and the shared virtual disk approach is practical. Of course, a Frangipani file system can be exported to untrusted machines using ordinary network file access protocols. We have implemented Frangipani on a collection of Alphas running DIGITAL Unix 4.0. Initial measurements indicate that Frangipani has excellent single-server performance and scales well as servers are added.  
92|Recovery and coherency-control protocols for fast intersystem page transfer and fine-granularity locking in a shared disks transaction environment|llbstract This paper proposes schemes for fast page transfer between transaction system Instances In a shared disks (SD) environment where all the sharing Instances can read and modify the same data Fast page transfer improves transaction response time and concur-rency because one or more disk I/OS are avoided while transferring a page from a system which modified it to another system which needs it. The proposed methods work with the steal and no-force buffer management policies, and fine-granularity (e.g., record) locking For each of the page-transfer schemes, we present both recovery and coherency-control protocols Updates can be made to a page by several systems before the page is written to disk. Many subtleties Involved in correctly recovering such a page in the face of single system or complex-wide failures are also discussed. Assuming that each system maintains its own log, some methods require a merged log for restart recovery while others don’t Our proposals should also apply to dlstrihuted. recoverable file systems and distributed virtual memory in the SD environment, and to the currently oopular client-server object-oriented DBMS environments where the clients cache data. 1.
93|Tiger Shark - a scalable file system for multimedia|Tiger Shark is a scalable, parallel file system designed to support interactive multimedia applications, particularly large-scale ones such as interactive television (ITV). Tiger Shark runs under the IBM AIX operating system, on machines ranging from RS/6000 desktop workstations to the SP2 parallel supercomputer. In addition to supporting continuous-time data, Tiger Shark provides scalability, high availability, and on-line system management, all of which are crucial in large-scale video servers. These latter features also enable Tiger Shark to support non-multimedia applications such as scientific computing, data mining, digital library, and scalable network file servers. Tiger Shark has been employed in a number of customer ITV trials. Based on experience obtained from these trials, Tiger Shark has recently been released in several IBM video server products. This paper describes the architecture and implementation of Tiger Shark, discusses the experience gained from trials, and compa...
94|Software Transactional Memory|As we learn from the literature, flexibility in choosing synchronization operations greatly simplifies the task of designing highly concurrent programs. Unfortunately, existing hardware is inflexible and is at best on the level of a Load Linked/Store Conditional operation on a single word. Building on the hardware based transactional synchronization methodology of Herlihy and Moss, we offer  software transactional memory (STM), a novel software method for supporting flexible transactional programming of synchronization operations. STM is non-blocking, and can be implemented on existing machines using only a  Load Linked/Store Conditional operation. We use STM to provide a general highly concurrent method for translating sequential object implementations to lock-free ones based on implementing a k-word compare&amp;swap STM-transaction. Empirical evidence collected on simulated multiprocessor architectures shows that the our method always outperforms all the lock-free translation methods in ...
95|Linearizability: a correctness condition for concurrent objects|A concurrent object is a data object shared by concurrent processes. Linearizability is a correctness condition for concurrent objects that exploits the semantics of abstract data types. It permits a high degree of concurrency, yet it permits programmers to specify and reason about concurrent objects using known techniques from the sequential domain. Linearizability provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object’s operations can be given by pre- and post-conditions. This paper defines linearizability, compares it to other correctness conditions, presents and demonstrates a method for proving the correctness of implementations, and shows how to reason about concurrent objects, given they are linearizable.
96|Hierarchical correctness proofs for distributed algorithms|Abstract: We introduce the input-output automaton, a simple but powerful model of computation in asynchronous distributed networks. With this model we are able to construct modular, hierarchical correctness proofs for distributed algorithms. We de ne this model, and give aninteresting example of how itcan be used to construct such proofs. 1
97|The Drinking Philosophers Problem|The problem of resolving conflicts between processes in distributed systems is of practical importance. A conflict between a set of processes must be resolved in favor of some (usually one) process and against the others: a favored process must have some property that distinguishes it from others. To guarantee fairness, the distinguishing property must be such that the process selected for favorable treatment is not always the same. A distributed implementation of an acyclic precedence graph, in which the depth of a process (the longest chain of predecessors) is a distinguishing property, is presented. A simple conflict resolution rule coupled with the acyclic graph ensures fair resolution of all conflicts. To make the problem concrete, two paradigms are presented: the well-known distributed dining philosophers problem and a generalization of it, the distributed drinking philosophers problem.
98|The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor|The Alewife multiprocessor project focuses on the architecture and design of a large-scale parallel machine. The machine uses a low-dimensional direct interconnection network to provide scalable communication bandwidth, while allowing the exploitation of locality. Despite its distributed-memory architecture, Alewife allows efficient shared-memory programming through a multilayered approach to locality management. A new scalable cache-coherence scheme called LimitLESS directories allows the use of caches for reducing communication latency and network bandwidth requirements. Alewife also employs run-time and compile-time methods for partitioning and placement of data and processes to enhance communication locality. While the above methods attempt to minimize communication latency, communication with distant processors cannot be completely avoided. Alewife&#039;s processor, Sparcle, is designed to tolerate these latencies by rapidly switching between threads of computation. This paper describe...
99|Adding Networks|  An adding network is a distributed data structure that supports a concurrent, lock-free, low-contention implementation of a fetch&amp;add counter; a counting network is an instance of an adding network that supports only fetch&amp;increment. We present a lower bound showing that adding networks have inherently high latency. Any adding network powerful enough to support addition by at least two values a and b, where |a |&gt; |b |&gt; 0, has sequential executions in which each token traverses ?(n/c) switching elements, where n is the number of concurrent processes, and c is a quantity we call one-shot contention; for a large class of switching networks and for conventional counting networks the one-shot contention is constant. On the contrary, counting networks have O(log n) latency [4,7]. This bound is tight. We present the first concurrent, lock-free, lowcontention networked data structure that supports arbitrary fetch&amp;add operations.  
100|Synchronization Without Contention|Conventional wisdom holds that contention due to busy-wait synchronization is a major obstacle to scalability and acceptable performance in large shared-memory multiprocessors. We argue the contrary, and present fast, simple algorithms for contention-free mutual exclusion, reader-writer control, and barrier synchronization. These algorithms, based on widely available fetch_and_phi instructions, exploit local access to shared memory to avoid contention. We compare our algorithms to previous approaches in both qualitative and quantitative terms, presenting their performance on the Sequent Symmetry and BBN Butterfly multiprocessors. Our results highlight the importance of local access to shared memory, provide a case against the construction of so-called &#034;dance hall&#034; machines, and suggest that special-purpose hardware support for synchronization is unlikely to be cost effective on machines with sequentially consistent memory.
101|A simple load balancing scheme for task allocation in parallel machines |A collection of local workpiles (task queues) and a simple load balancing scheme is well suited for scheduling tasks in shared memory parallel machines. Task scheduling on such machines has usually been done through a single, globally accessible, workpile. The scheme introduced in this paper achieves a balancing comparable to that of a global workpile, while minimizing the overheads. In many parallel computer architectures, each processor has some memory that it can access more efficiently, and so it is desirable that tasks do not mirgrate frequently. The load balancing is simple and distributed: Whenever a processor accesses its local workpile, it performs a balancing operation with probability inversely proportional to the size of its workpile. The balancing operation consists of examining the workpile of a random processor and exchanging tasks so as to equalize the size of the two workpiles. The probabilistic analysis of the performance of the load balancing scheme proves that each tasks in the system receives its fair share of computation time. Specifically, the expected size of each local task queue is within a small constant factor of the average, i.e. total number of tasks in the system divided by the number of processors. 1
102|A method for implementing lock-free shared data structures|barnesQmpi-sb.mpg.de
103|Diffracting trees|Shared counters are among the most basic coordination structures in multiprocessor computation, with applications ranging from barrier synchronization to concurrent-data-structure design. This article introduces diffracting trees, novel data structures for shared counting and load balancing in a distributed/parallel environment. Empirical evidence, collected on a simulated distributed shared-memory machine and several simulated message-passing architectures, shows that diffracting trees scale better and are more robust than both combining trees and counting networks, currently the most effective known methods for implementing concurrent counters in software. The use of a randomized coordination method together with a combinatorial data structure overcomes the resiliency drawbacks of combining trees. Our simulations show that to handle the same load, diffracting trees and counting networks should have a similar width w, yet the depth of a diffracting tree is O(log w), whereas counting networks have depth O(log 2 w). Diffracting trees have already been used to implement highly efficient producer/consumer queues, and we believe diffraction will prove to be an effective alternative paradigm to combining and queue-locking in the design of many concurrent data structures.
104|A performance evaluation of lock-free synchronization protocols|In this paper, we investigate the practical performance of lock-free techniques that provide synchronization on shared-memory multiprocessors. Our goal is to provide a technique to allow designers of new protocols to quickly determine an algorithm’s performance characteristics. We develop a simple analytical performance model based on the architectural observations that memory accesses are expensive, synchronization instructions are more expensive, and that optimistic synchronization policies result in wasted communication bandwidth which can slow the system as a whole. Using our model, we evaluate the performance of five existing lock-free synchronization protocols. We validate our analysis by comparing our results with simulations of a parallel machine. Given this analysis, we identify those protocols which show promise of good performance in practice. In addition, we note that no existing protocols provide insensitivity to common delays while still offering performance equivalent to locks. Accordingly, we introduce a protocol, based on a combination of existing lock-free techniques, which satisfies these criteria. 1
105|Cache Coherence Protocols for Large-Scale Multiprocessors|in partial ful llment of the requirements for the degree of
106|A Fast File System for UNIX|A reimplementation of the UNIX file system is described. The reimplementation provides substantially higher throughput rates by using more flexible allocation policies that allow better locality of reference and can be adapted to a wide range of peripheral and processor characteristics. The new file system clusters data that is sequentially accessed and provides two block sizes to allow fast access to large files while not wasting large amounts of space for small files. File access rates of up to ten times faster than the traditional UNIX file system are experienced. Long needed enhancements to the programmers’ interface are discussed. These include a mechanism to place advisory locks on files, extensions of the name space across file systems, the ability to use long file names, and provisions for administrative control of resource usage.
107|The Unix Time-Sharing System|Unix is a general-purpose, multi-user, interactive operating system for the larger Digital Equipment Corporation PDP-11 and the Interdata 8/32 computers. It offers a number of features seldom found even in larger operating systems, including i A hierarchical file system incorporating demountable volumes, ii Compatible file, device, and inter-process I/O, iii The ability to initiate asynchronous processes, iv System command language selectable on a per-user basis, v Over 100 subsystems including a dozen languages, vi High degree of portability. This paper discusses the nature and implementation of the file system and of the user command interface. I.
108|The Player/Stage Project: Tools for Multi-Robot and Distributed Sensor Systems|This paper describes the Player/Stage software tools applied to multi-robot, distributed-robot and sensor network systems. Player is a robot device server that provides network transparent robot control. Player seeks to constrain controller design as little as possible; it is device independent, non-locking and language- and style-neutral. Stage is a lightweight, highly configurable robot simulator that supports large populations. Player/Stage is a community Free Software project. Current usage of Player and Stage is reviewed, and some interesting research opportunities opened up by this infrastructure are identified.
109|Robust Monte Carlo Localization for Mobile Robots|Mobile robot localization is the problem of determining a robot&#039;s pose from sensor data. This article presents a family of probabilistic localization algorithms known as Monte Carlo Localization (MCL). MCL algorithms represent a robot&#039;s belief by a set of weighted hypotheses (samples), which approximate the posterior under a common Bayesian formulation of the localization problem. Building on the basic MCL algorithm, this article develops a more robust algorithm called MixtureMCL, which integrates two complimentary ways of generating samples in the estimation. To apply this algorithm to mobile robots equipped with range finders, a kernel density tree is learned that permits fast sampling. Systematic empirical results illustrate the robustness and computational efficiency of the approach. 
111|Robomote: A Tiny Mobile Robot Platform for Large-Scale Ad-hoc Sensor Networks|This paper introduces Robomote, a robotic solution developed to explore problems in large-scale distributed robotics and sensor networks. The design explicitly aims at enabling research in sensor networking, adhoc networking, massively distributed robotics, and extended longevity. The platform must meet many demanding criteria not limited to but including: miniature size, low power, low cost, simple fabrication, and a sensor/actuator suite that facilitates navigation and localization. We argue that a robot test bed such as Robomote is necessary for practical research with large networks of mobile robots. Further, we present a preliminary analysis of Robomotes&#039; success to this end.
112|Multiagent Mission Specification and Execution|.  Specifying a reactive behavioral configuration for use by a multiagent team requires both a careful choice of the behavior set and the creation of a temporal chain of behaviors which executes the mission. This difficult task is simplified by applying an object-oriented approach to the design of the mission using a construction called an assemblage and a methodology called temporal sequencing. The assemblage construct allows building high level primitives which provide abstractions for the designer. Assemblages consist of groups of basic behaviors and coordination mechanisms that allow the group to be treated as a new coherent behavior. Upon instantiation, the assemblage is parameterized based on the specific mission requirements. Assemblages can be re-parameterized and used in other states within a mission or archived as high level primitives for use in subsequent projects. Temporal sequencing partitions the mission into discrete operating states with perceptual triggers causing tra...
113|On Device Abstractions for Portable, Reusable Robot Code|We seek to make robot programming more efficient by developing a standard abstract interface for robot hardware, based on familiar techniques from operating systems and network engineering. This paper describes the application of three well known abstractions, the character device model, the interface/driver model, and the client/server model to this purpose. These abstractions underlie Player/Stage, our Open Source project for rapid development of robot control systems. One product of this project is the Player Abstract Device Interface (PADI) specification, which denes a set of interfaces that capture the functionality of logically similar sensors and actuators. This specification is the central abstraction that enables Player-based controllers to run unchanged on a variety of real and simulated devices. We propose that PADI could be a starting point for development of a standard platform for robot interfacing, independent of Player, to enable code portability and re-use, while still providing access to the unique capabilities of individual devices.
114|Tracking Targets using Multiple Robots: The Effect of Environment Occlusion|This paper addresses the problem of tracking multiple targets using a network of communicating robots and stationary sensors. We introduce a Region-based Approach which controls robot deployment at two levels. A coarse deployment controller distributes robots across regions using a topological map which maintains urgency estimates for each region, and a target-following controller attempts to maximize the number of tracked targets within a region. A behavior-based system is presented implementing the Region-Based Approach, which is fully distributed and scalable. We compared the Region-based Approach to a ‘naive ’ local-following strategy in three environments with varying degree of occlusion. The experimental results showed that the Region-based Approach performs better than the naive strategy when the environment has significant occlusion. Second, we performed experiments (the environment was held constant) in which two techniques for computing urgency estimates were compared. Last, different combinations of mobile sensors and stationary sensors were compared in a given environment. Keywords: Multi-target tracking, mobile robotics, embedded sensors
115|An Experiment in Integrated Exploration|Integrated exploration strategy advocated in this paper refers to a tight coupling between the tasks of localization, mapping, and motion control and the effect of this coupling on the overall effectiveness of an exploration strategy. Our approach to exploration calls for a balanced evaluation of alternative motion actions from the point of view of information gain, localization quality, and navigation cost. To provide a uniform basis of comparison of localization quality between different locations, a &#034;localizability&#034; metric is introduced. It is based on the estimate of the lowest vehicle pose covariance attainable from a given location.
116|LOST: Localization-Space Trails for Robot Teams|We describe Localization-Space Trails (LOST), a method that enables a team of robots to navigate between places of interest in an initially unknown environment using a trail of landmarks. The landmarks are not physical; they are waypoint coordinates generated on-line by each robot and shared with team-mates. Waypoints are specified in each robot&#039;s local coordinate system, and contain references to features in the world that are relevant to the team&#039;s task and common to all robots. Using these task-level references, robots can share waypoints without maintaining a global coordinate system.
117|Go Ahead, Make My Day: Robot conflict resolution by aggressive competition|We examine a simulated but realistic multi-robot transport task that suffers from spatial interference.
118|Ayllu: Distributed Port-Arbitrated Behavior-Based Control|. Distributed control of a team of mobile robots presents a number of unique challenges, including highly unreliable communication, real world task and safety constraints, scalability, dynamic reconfigurability, heterogenous platforms, and a lack of standardized tools or techniques. Similar problems plagued development of single robots applications until the &#034;behavior-based&#034; revolution led to new techniques for robot control based on port-arbitrated behaviors (PAB). Though there are now many implementations of systems for behavior-based control of single robots, the potential for distributing such control across robots for multi-agent control has not until now been fully realized.  This paper presents Ayllu, a system for distributed multi-robot behavioral control. Ayllu allows standard PAB interaction (message passing, inhibition, and suppression) to take place over IP networks, and extends the PAB paradigm to provide for arbitrary scalability. We give a brief overview of the Broadcast...
119|Adaptive Spatio-Temporal Organization in Groups of Robots|This paper presents experiments, in simulation, with a group of robots that improve their performance on a straightforward transportation task by using reinforcement learning to associate input states with a set of abstract behaviors. We show that the improvement in performance is a result of the group adapting its spatio-temporal organization to the given environment. Spatio-temporal adaptation is a general form of adaptation in that it can improve performance over a range of di#erent tasks and environments. Hence it increases the general applicability and autonomy of robotic systems. Lastly, we present two communication strategies that improve this ability to adapt by generally improving learning rates for cooperative robots in highly dynamic domains.
120|DCA: A Distributed Control Architecture for Robotics|Many contr olapplications are by nature distributed, not only over different processesbut also over several processors. Managing such a system with respectto the startup of processes, internal communioations and state changes quickly becomesa very complex task. This pap erpresents  a distributed contr olarchite cturohich supports a formal model of computation as described by [lJ. The ar chite ctur is primarily intended for rob ot  contr olbut has a wide range of potential applications. We motivate the design and implementation of the archite ctur by discussing the desired prop ertiesof a rob otsystem cap ableof doing real-time tasks like manipulation. This leads to functionality such as a processalgebra contr olling the life-cycle of the processes, grouping and distribution of pro c esses and internal communication transparent to location. Our implementation do esnot in itself intr odue any bottlenecks due to and modularity, and whid is scalable to enterprise lev el applications.
121|Go ahead, make my day: Robot con¤ict resolution by aggressive competition|We examine a simulated but realistic multi-robot transport task that suers from spatial in-terference. Previously described techniques to re-duce interference are not appropriate for this and related tasks. We demonstrate the utility of an aggressive competition to reduce interference and increase eÆciency in our system. A controller is described which breaks deadlocks in favour of the most `ag-gressive &#039; robot. Simulation trials are performed to evaluate a variety of aggression functions. Our re-sults and subsequent discussion suggest that nei-ther a linear dominance hierarchy nor a simple sensor bias method oer any advantage over a random outcome. Finally we discuss some strategies that might favour the `correct &#039; outcome of competitions to increase the eÆciency of the system. We are cur-rently implementing these controllers on a real robot team.
122|Transactional Locking II|Abstract. The transactional memory programming paradigm is gaining momentum as the approach of choice for replacing locks in concurrent programming. This paper introduces the transactional locking II (TL2) algorithm, a software transactional memory (STM) algorithm based on a combination of commit-time locking and a novel global version-clock based validation technique. TL2 improves on state-of-the-art STMs in the following ways: (1) unlike all other STMs it fits seamlessly with any systems memory life-cycle, including those using malloc/free (2) unlike all other lock-based STMs it efficiently avoids periods of unsafe execution, that is, using its novel version-clock validation, user code is guaranteed to operate only on consistent memory states, and (3) in a sequence of high performance benchmarks, while providing these new properties, it delivered overall performance comparable to (and in many cases better than) that of all former STM algorithms, both lock-based and non-blocking. Perhaps more importantly, on various benchmarks, TL2 delivers performance that is competitive with the best hand-crafted fine-grained concurrent structures. Specifically, it is ten-fold faster than a single lock. We believe these characteristics make TL2 a viable candidate for deployment of transactional memory today, long before hardware transactional support is available. 1
123|Language Support for Lightweight Transactions|Concurrent programming is notoriously di#cult. Current abstractions are intricate and make it hard to design computer systems that are reliable and scalable. We argue that these problems can be addressed by moving to a declarative style of concurrency control in which programmers directly indicate the safety properties that they require.
124|Virtualizing Transactional Memory|Writing concurrent programs is difficult because of the complexity of ensuring proper synchronization. Conventional lock-based synchronization suffers from wellknown limitations, so researchers have considered nonblocking transactions as an alternative. Recent hardware proposals have demonstrated how transactions can achieve high performance while not suffering limitations of lock-based mechanisms. However, current hardware proposals require programmers to be aware of platform-specific resource limitations such as buffer sizes, scheduling quanta, as well as events such as page faults, and process migrations. If the transactional model is to gain wide acceptance, hardware support for transactions must be virtualized to hide these limitations in much the same way that virtual memory shields the programmer from platform-specific limitations of physical memory. This paper proposes Virtual Transactional Memory (VTM), a user-transparent system that shields the programmer from various platform-specific resource limitations. VTM maintains the performance advantage of hardware transactions, incurs low overhead in time, and has modest costs in hardware support. While many system-level challenges remain, VTM takes a step toward making transactional models more widely acceptable.  
125|Unbounded Transactional Memory|Background: Programming in a shared-memory environment often requires the use of atomic regions for program correctness. Traditionally, atomicity is achieved through critical sections protected by locks. Unfortunately, locks are very difficult to program with since they introduce problems such as deadlock and priority inversion. Locks also introduce a significant performance overhead since locking instructions are expensive and performing deadlock avoidance can be slow. In addition, locks are simply memory locations so there is an added space overhead associated with locking as well. Hardware Transactions: To overcome the problems with locks, Herlihy and Moss proposed a hardware transactional memory (HTM) [1] scheme that gives the programmer a more intuitive atomicity primitive, a transaction. A transaction is an atomic region that either completes atomically or fails and has no effect on the global memory state. Two regions are atomic if, after they are run, they can viewed as having run in some serial order with no interleaved instructions. HTM ensures atomicity by simply running the atomic region speculatively. If no other processor accesses any of the same memory locations as the atomic region, the speculative state can be committed since atomicity has been satisfied. On the other hand, HTM must provide the mechanism to detect conflicting memory accesses if they do occur. In such a case, HTM will abort all the
126|McRT-STM: a High Performance Software Transactional Memory System for a Multi-Core Runtime|Applications need to become more concurrent to take advantage of the increased computational power provided by chip level multiprocessing. Programmers have traditionally managed this concurrency using locks (mutex based synchronization). Unfortunately, lock based synchronization often leads to deadlocks, makes fine-grained synchronization difficult, hinders composition of atomic primitives, and provides no support for error recovery. Transactions avoid many of these problems, and therefore, promise to ease concurrent programming. We describe a software transactional memory (STM) system that is part of McRT, an experimental Multi-Core RunTime. The McRT-STM implementation uses a number of novel algorithms, and supports advanced features such as nested transactions with partial aborts, conditional signaling within a transaction, and object based conflict detection for C/C++ applications. The McRT-STM exports interfaces that can be used from C/C++ programs directly or as a target for compilers translating higher level linguistic constructs. We present a detailed performance analysis of various STM design tradeoffs such as pessimistic versus optimistic concurrency, undo logging versus write buffering, and cache line based versus object based conflict detection. We also show a MCAS implementation that works on arbitrary values, coexists with the STM, and can be used as a more efficient form of transactional memory. To provide a baseline we compare the performance of the STM with that of fine-grained and coarsegrained locking using a number of concurrent data structures on a 16-processor SMP system. We also show our STM performance on a non-synthetic workload – the Linux sendmail application.
127|Transactional memory coherence and consistency|In this paper, we propose a new shared memory model: Transactional
128|Atomic Snapshots of Shared Memory|. This paper introduces a general formulation of atomic snapshot memory, a shared memory partitioned into words written (updated) by individual processes, or instantaneously read (scanned) in its entirety. This paper presents three wait-free implementations of atomic snapshot memory. The first implementation in this paper uses unbounded (integer) fields in these registers, and is particularly easy to understand. The second implementation uses bounded registers. Its correctness proof follows the ideas of the unbounded implementation. Both constructions implement a single-writer snapshot memory, in which each word may be updated by only one process, from single-writer, n-reader registers. The third algorithm implements a multi-writer snapshot memory from atomic n-writer, n-reader registers, again echoing key ideas from the earlier constructions. All operations require \Theta(n  2  ) reads and writes to the component shared registers in the worst case.  Categories and Subject Discriptors:...
129|Adaptive Software Transactional Memory|Abstract. Software Transactional Memory (STM) is a generic synchronization construct that enables automatic conversion of correct sequential objects into correct nonblocking concurrent objects. Recent STM systems, though significantly more practical than their predecessors, display inconsistent performance: differing design decisions cause different systems to perform best in different circumstances, often by dramatic margins. In this paper we consider four dimensions of the STM design space: (i) when concurrent objects are acquired by transactions for modification; (ii) how they are acquired; (iii) what they look like when not acquired; and (iv) the non-blocking semantics for transactions (lock-freedom vs. obstruction-freedom). In this 4-dimensional space we highlight the locations of two leading STM systems: the DSTM of Herlihy et al. and the OSTM of Fraser and Harris. Drawing motivation from the performance of a series of application benchmarks, we then present a new Adaptive STM (ASTM) system that adjusts to the offered workload, allowing it to match the performance of the best known existing system on every tested workload. 1
130|A lazy snapshot algorithm with eager validation|Abstract. Most high-performance software transactional memories (STM) use optimistic invisible reads. Consequently, a transaction might have an inconsistent view of the objects it accesses unless the consistency of the view is validated whenever the view changes. Although all STMs usually detect inconsistencies at commit time, a transaction might never reach this point because an inconsistent view can provoke arbitrary behavior in the application (e.g., enter an infinite loop). In this paper, we formally introduce a lazy snapshot algorithm that verifies at each object access that the view observed by a transaction is consistent. Validating previously accessed objects is not necessary for that, however, it can be used on-demand to prolong the view’s validity. We demonstrate both formally and by measurements that the performance of our approach is quite competitive by comparing other STMs with an STM that uses our algorithm. 1
131|Software transactional memory should not be obstruction-free|OTHER INTELLECTUAL PROPERTY RIGHT. Intel products are not intended for use in
132|Transactional monitors for concurrent objects |Abstract. Transactional monitors are proposed as an alternative to monitors based on mutual-exclusion synchronization for object-oriented programming languages. Transactional monitors have execution semantics similar to mutualexclusion monitors but implement monitors as lightweight transactions that can be executed concurrently (or in parallel on multiprocessors). They alleviate many of the constraints that inhibit construction of transparently scalable and robust applications. We undertake a detailed study of alternative implementation schemes for transactional monitors. These different schemes are tailored to different concurrent access patterns, and permit a given application to use potentially different implementations in different contexts. We also examine the performance and scalability of these alternative approaches in the context of the Jikes Research Virtual Machine, a state-of-the-art Java implementation. We show that transactional monitors are competitive with mutualexclusion synchronization and can outperform lock-based approaches up to five times on a wide range of workloads. 1
133|An Efficient Meta-lock for Implementing Ubiquitous Synchronization|Programs written in concurrent object-oriented languages, espe-cially ones that employ thread-safe reusable class libraries, can execute synchronization operations (lock, notify, etc.) at an amaz-ing rate. Unless implemented with utmost care, synchronization can become a performance bottleneck. Furthermore, in languages where every object may have its own monitor, per-object space overhead must be minimized. To address these concerns, we have developed a meta-lock to mediate access to synchronization data. The meta-lock is fast (lock + unlock executes in 11 SPARCTM architecture instructions), compact (uses only two bits of space), robust under contention (no busy-waiting), and flexible (supports a variety of higher-level synchronization operations). We have vali-dated the meta-lock with an implementation of the synchronization operations in a high-performance product-quality JavaTM virtual machine and report performance data for several large programs.
134|Concurrency Control: Methods, Performance, and Analysis|Standard locking (two-phase locking with on-demand lock requests and blocking upon lock conflict) is the primary concurrency control (CC) method for centralized databases. The main source of performance degradation with standard locking is blocking, whereas transaction (txn) restarts to resolve deadlocks have a secondary effect on performance. We provide a performance analysis of standard locking that accurately estimates its performance degradation leading to thrashing. We next introduce two sets of methods to cope with its performance limitations. Restartoriented locking methods selectively abort txns to increase the level of concurrency for active txns with respect to standard locking in high-contention environments. For example, the running-priority method aborts blocked txns based on the essential blocking principle, which only allows blocking by active txns. The waitdepth-limited (WDL) method further minimizes wasted processing by basing abort decisions on the progress made by a txn. Restart waiting serves as a load-control mechanism by deferring the restart of an aborted txn until conflicting txns have left the system. These two methods have performance superior to other restartoriented
135|Design tradeoffs in modern software transactional memory systems|Software Transactional Memory (STM) is a generic nonblocking synchronization construct that enables automatic conversion of correct sequential objects into correct concurrent objects. Because it is nonblocking, STM avoids traditional performance and correctness problems due to thread failure, preemption, page faults, and priority inversion. In this paper we compare and analyze two recent objectbased STM systems, the DSTM of Herlihy et al. and the FSTM of Fraser, both of which support dynamic transactions, in which the set of objects to be modified is not known in advance. We highlight aspects of these systems that lead to performance tradeoffs for various concurrent data structures. More specifically, we consider object ownership acquisition semantics, concurrent object referencing style, the overhead of ordering and bookkeeping, contention management versus helping semantics, and transaction validation. We demonstrate for each system simple benchmarks on which it outperforms the other by a significant margin. This in turn provides us with a preliminary characterization of the applications for which each system is best suited.  
136|The Performance of Concurrent Red-Black Tree Algorithms|Relaxed balancing has become a commonly used concept in the design of concurrent  search tree algorithms. The idea of relaxed balancing is to uncouple the rebalancing from the  updating in order to speed up the update operations and to allow a high degree of concurrency.  Many different relaxed balancing algorithms have been proposed, especially for red-black trees  and AVL trees, but their performance in concurrent environments is not yet well understood.  This paper presents an experimental comparison of three relaxed balancing algorithms  for red-black trees. Using the simulation of a multi processor environment we study the  performance of chromatic trees, the algorithm that is got by applying the general method  of how to make strict balancing schemes relaxed to red-black trees, and the relaxed redblack  tree. Furthermore, we compare the relaxed balancing algorithms with the standard  red-black tree, i.e. the strictly balanced red-black tree combined with the locking scheme  of El...
137|Implementing fast Java monitors with relaxed-locks |The Java  Programming Language permits synchronization operations (lock, unlock, wait, notify) on any object. Synchronization is very common in applications and is endemic in the library code upon which applications depend. It is therefore critical that a monitor implementation be both space-efficient and time-efficient. We present a locking protocol, the Relaxed-Lock, that satisfies those requirements. The Relaxed-Lock is reasonably compact, using only one word in the object header. It is fast, requiring in the uncontested case only one atomic compare-and-swap to lock a monitor and no atomic instructions to release a monitor. The Relaxed-Lock protocol is unique in that it permits a benign data race in the monitor unlock path (hence its name) but detects and recovers from the race and thus maintains correct mutual exclusion. We also introduce speculative deflation, a mechanism for releasing a monitor when it is no longer needed. 1
138|Disconnected Operation in the Coda File System|Disconnected operation is a mode of operation that enables a client to continue accessing critical data during temporary failures of a shared data repository. An important, though not exclusive, application of disconnected operation is in supporting portable computers. In this paper, we show that disconnected operation is feasible, efficient and usable by describing its design and implementation in the Coda File System. The central idea behind our work is that caching of data, now widely used for performance, can also be exploited to improve availability.
139|Scale and performance in a distributed file system|The Andrew File System is a location-transparent distributed tile system that will eventually span more than 5000 workstations at Carnegie Mellon University. Large scale affects performance and complicates system operation. In this paper we present observations of a prototype implementation, motivate changes in the areas of cache validation, server process structure, name translation, and low-level storage representation, and quantitatively demonstrate Andrew’s ability to scale gracefully. We establish the importance of whole-file transfer and caching in Andrew by comparing its performance with that of Sun Microsystem’s NFS tile system. We also show how the aggregation of files into volumes improves the operability of the system.
140|Coda: A Highly available File System for a Distributed Workstation Environment|Abstract- Coda is a file system for a large-scale distributed computing environment composed of Unix workstations. It provides resiliency to server and network failures through the use of two distinct but complementary mechanisms. One mechanism, server replication, stores copies of a file at multiple servers. The other mechanism, disconnected operation, is a mode of execution in which a caching site temporarily assumes the role of a replication site. Disconnected operation is particularly useful for supporting portable workstations. The design of Coda optimizes for availability and performance, and strives to provide the highest degree of consistency attainable in the light of these objectives. Measurements from a prototype show that the performance cost of providing high availability in Coda is reasonable. Index Terms- Andrew, availability, caching, disconnected operation, distributed file system, performance, portable computers, scalability, server replication. I.
141|Design and Implementation or the Sun Network Filesystem|this paper we discuss the design and implementation of the/&#039;fiesystem interface in the kernel and the NF$ virtual/&#039;fiesystem. We describe some interesting design issues and how they were resolved, and point out some of the shortcomings of the current implementation. We conclude with some ideas for future enhancements
142|Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency|Caching introduces the overbead and complexity of ensur-ing consistency, reducing some of its performance bene-fits. In a distributed system, caching must deal,wit.h the additional complications of communication and host fail-ures. Leases are proposed as a time-based mechanism that provides efficient consistent access to cached data in dis-tributed systems. Non-Byzantine failures affect perfor-mance, not correctness, with their effect minimized by short leases. An analytic model and an evaluation for file access in the V system show that leases of short duration provide good performance. The impact of leases on per-formance grows more significant in systems of lar;ger scale and higher processor performance. 
143|Vnodes: An architecture for multiple file system types|sun!srk
144|The locus distributed operating system|LOCUS Is a distributed operating system which supports transparent access to data through a network wide fllesystem, permits automatic replication of storaget supports transparent distributed process execution, supplies a number of high reliability functions such as nested transactions, and is upward compatible with Unix. Partitioned operation of subnetl and their dynamic merge is also supported. The system has been operational for about two years at UCLA and extensive experience In its use has been obtained. The complete system architecture is outlined in this paper, and that experience is summarized. 1
145|Optimism and consistency in partitioned distributed database systems|A protocol for transaction processing during partition failures is presented which guarantees mutual consistency between copies of data-items after repair is completed. The protocol is “optimistic ” in that transactions are processed without restrictions during failure; conflicts are then detected at repair time using a precedence graph, and are resolved by backing out transactions according to some backout strategy. The resulting database state then corresponds to a serial execution of some subset of transactions run during the failure. Results from simulation and probabilistic modeling show that the optimistic protocol is a reasonable alternative in many cases. Conditions under which the protocol performs well are noted, and suggestions are made as to how performance can be improved. In particular, a backout strategy is presented which takes into account individual transaction costs and attempts to minimize total backout cost. Although the problem of choosing transactions to minimize total backout cost is, in general, NP-complete, the backout strategy is efficient and produces very good results.
146|Logbased directory resolution in the Coda file system|optimistic replicltion is m imparrrnt technique for achieving high avdability in distributed file systans. A key problem m optimistic rephtim is using d c bwledge of objects to resolve co&#034;mt updates from multiple partitions. In this paper, we describe how the Coda File System resolves pllrtitionsd updareg to dirCCtOIkS. The CUUd Idt Of Our Work is th.t &amp;g&amp;g Of updates is a simple yet efficient and powerful teclmique for directory resolution m Unix file systems. Mersurementr from our implementatkm show that the time for resolution is typically less than 1096 of the time for paforming the original set of partitioned updates. Analysis based on file traces fnnn our envinm &#034; indicate that a log size of 2 MB per hour of padtion should be ample for typical smers. 1.
147|A Caching File System for a Programmer&#039;s Workstation|This paper describes a workstation file system that supports a group of cooperating programmers by allowing them both to manage local naming environments and to share consistent versions of collections of software. The file system has access to the workstation&#039;s local disk and to remote file servers, and provides a hierarchical name space that includes the files on both. Local names can refer to local files or be attached to remote files. Remote files, which also may be referred to directly, are immutable and cached on the local disk. The file system is part of the Cedar experimental programming environment at Xerox PARC and has been in use since late 1983.
148|Availability and consistency tradeoffs in the Echo distributed file system|Workstations typically depend on remote servers accessed over a network for such services as mail, printing, storing files, booting, and time. The availability of these remote services has a major impact on the usability of the workstation. Availability can be increased by repli-cating the servers. In the Echo distributed file system at DEC SRC, two different replication techniques are employed, one at the upper levels of our hierarchical name space, the name service, and another at the lower levels of the name space, the file volume service. The two replication techniques provide different guarantees of consistency be-tween their replicas and, therefore, different levels of availability. Echo also caches data from the name service and file volume service in client machines (e.g., workstations), with the cache for each service having its own cache consistency guarantee that mimics the guarantee on the consistency of the replicas for that service. The replication and caching consistency guarantees provided by each service are appropriate for its intended use.
149|Distributed Database Systems |this article, we discuss the fundamentals of distributed DBMS technology. We address the data distribution and architectural design issues as well as the algorithms that need to be implemented to provide the basic DBMS functions such as query processing, concurrency control, reliability, and replication control.
150|EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis |Abstract: We have developed a toolbox and graphic user interface, EEGLAB, running under the cross-platform MATLAB environment (The Mathworks, Inc.) for processing collections of single-trial and/or averaged EEG data of any number of channels. Available functions include EEG data, channel and event information importing, data visualization (scrolling, scalp map and dipole model plotting, plus multi-trial ERP-image plots), preprocessing (including artifact rejection, filtering, epoch selection, and averaging), Independent Component Analysis (ICA) and time/frequency decompositions including channel and component cross-coherence supported by bootstrap statistical methods based on data resampling. EEGLAB functions are organized into three layers. Top-layer functions allow users to interact with the data through the graphic interface without needing to use MATLAB syntax. Menu options allow users to tune the behavior of EEGLAB to available memory. Middle-layer functions allow users to customize data processing using command history and interactive ‘pop ’ functions. Experienced MATLAB users can use EEGLAB data structures and stand-alone signal processing functions to write custom and/or batch analysis scripts. Extensive function help and tutorial information are included. A ‘plug-in ’ facility allows easy incorporation of new EEG modules into the main menu. EEGLAB is freely available
151|Blind Beamforming for Non Gaussian Signals|This paper considers an application of blind identification to beamforming. The key point is to use estimates of directional vectors rather than resorting to their hypothesized value. By using estimates of the directional vectors obtained via blind identification i.e. without knowing the arrray manifold, beamforming is made robust with respect to array deformations, distortion of the wave front, pointing errors, etc ... so that neither array calibration nor physical modeling are necessary. Rather surprisingly, `blind beamformers&#039; may outperform `informed beamformers&#039; in a plausible range of parameters, even when the array is perfectly known to the informed beamformer. The key assumption blind identification relies on is the statistical independence of the sources, which we exploit using fourth-order cumulants. A computationally efficient technique is presented for the blind estimation of directional vectors, based on joint diagonalization of 4th-order cumulant matrices
152|Measuring phase-synchrony in brain signals|r r Abstract: This article presents, for the first time, a practical method for the direct quantification of frequency-specific synchronization (i.e., transient phase-locking) between two neuroelectric signals. The motivation for its development is to be able to examine the role of neural synchronies as a putative mechanism for long-range neural integration during cognitive tasks. The method, called phase-locking statistics (PLS), measures the significance of the phase covariance between two signals with a reasonable time-resolution (,100 ms). Unlike the more traditional method of spectral coherence, PLS separates the phase and amplitude components and can be directly interpreted in the framework of neural integration. To validate synchrony values against background fluctuations, PLS uses surrogate data and thus makes no a priori assumptions on the nature of the experimental data. We also apply PLS to investigate intracortical recordings from an epileptic patient performing a visual discrimination task. We find large-scale synchronies in the gamma band (45 Hz), e.g., between hippocampus and frontal gyrus, and local synchronies, within a limbic region, a few cm apart. We argue that whereas long-scale effects do reflect cognitive processing, short-scale synchronies are likely to be due to volume conduction. We discuss ways to separate such conduction
153|Subband-Based Blind Signal Separation for Noisy Speech Recognition|Introduction: Noise robustness is a very important issue in the field of automatic speech recognition. Microphone arrays have been used to achieve noise robustness, and blind source separation has been proposed to enhance the noisy speech signal [1]. For the speech recognition process, however, only clean speech features are required. Therefore, instead of denoising the noisy speech signal in the preprocessing step, it is computationally more efficient to directly extract the clean speech features from noisy speech.  In this letter, we propose an algorithm, which efficiently removes the mixed noise component from the speech signal in the process of extracting features. The denoising process is based on ICA, which linearly separates noise and signal from two noisy speech microphone recordings. A &#034;small-band&#034; approach is implemented to average out fast Fourier transform (FFT) points in a frequency range and apply ICA directly to feature levels. To remove the mixed
154|An Overview of the C++ Programming Language|This overview of C++ presents the key design, programming, and language-technical concepts using examples to give the reader a feel for the language. C++ is a general-purpose programming language with a bias towards systems programming that supports efficient low-level computation, data abstraction, object-oriented programming, and generic programming.  
155|The Spring nucleus: A microkernel for objects|The Spring system is a distributed operating system that supports a distributed, object-oriented application framework. Each individual Spring system is based around a microkernel known as the nucleus, which is structured to support fast cross-address-space object invocations. This paper discusses the design rationale for the nucleus&#039;s IPC facilities and how they fit into the overall Spring programming model. We then describe how the internal structure of the nucleus is organized to support fast crossaddress -space calls, including some specific details and performance information on the current implementation.  
156|High-Performance Scientific Computing Using C++|Concepts from mathematics and physics often map well to object-oriented software since the original concepts are of an abstract nature. We describe our experiences with developing high-performance shock-wave physics simulation codes in C++ and discuss the software engineering issues which we have encountered. The primary enabling technology in C++ for allowed us to share software between our development groups is operator overloading for a number of &#034;numeric&#034; objects. Unfortunately, this enabling feature can also impact the efficiency of our computations. We describe the techniques we have utilized for minimizing this difficulty.  Introduction  Developers of scientific software systems are tasked to implement abstract ideas and concepts. The software implementation of algorithms and ideas from physics, mechanics and mathematics should in principle be complementary to the mathematical abstractions. Often these ideas are very naturally implemented in an object-oriented style. For example...
157|FAST VOLUME RENDERING USING A SHEAR-WARP FACTORIZATION OF THE VIEWING TRANSFORMATION|Volume rendering is a technique for visualizing 3D arrays of sampled data. It has applications in areas such as medical imaging and scientific visualization, but its use has been limited by its high computational expense. Early implementations of volume rendering used brute-force techniques that require on the order of 100 seconds to render typical data sets on a workstation. Algorithms with optimizations that exploit coherence in the data have reduced rendering times to the range of ten seconds but are still not fast enough for interactive visualization applications. In this thesis we present a family of volume rendering algorithms that reduces rendering times to one second. First we present a scanline-order volume rendering algorithm that exploits coherence in both the volume data and the image. We show that scanline-order algorithms are fundamentally more efficient than commonly-used ray casting algorithms because the latter must perform analytic geometry calculations (e.g. intersecting rays with axis-aligned boxes). The new scanline-order algorithm simply streams through the volume and the image in storage order. We describe variants of the algorithm for both parallel and perspective projections and
158|Marching cubes: A high resolution 3D surface construction algorithm|We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.
159|The rendering equation|ABSTRACT. We present an integral equation which generallzes a variety of known rendering algorithms. In the course of discussing a monte carlo solution we also present a new form of variance reduction, called Hierarchical sampling and give a number of elaborations shows that it may be an efficient new technique for a wide variety of monte carlo procedures. The resulting renderlng algorithm extends the range of optical phenomena which can be effectively simulated.
160|Display of Surfaces from Volume Data|The application of volume rendering techniques to the display of surfaces from sampled scalar functions of three spatial dimensions is explored. Fitting of geometric primitives to the sampled data is not required. Images are formed by directly shading each sample and projecting it onto the picture plane. Surface shading calculations are performed at every voxel with local gradient vectors serving as surface normals. In a separate step, surface classification operators are applied to obtain a partial opacity for every voxel. Operators that detect isovalue contour surfaces and region boundary surfaces are presented. Independence of shading and classification calculations insures an undistorted visualization of 3-D shape. Non-binary classification operators insure that small or poorly defined features are not lost. The resulting colors and opacities are composited from back to front along viewing rays to form an image. The technique is simple and fast, yet displays surfaces exhibiting smooth silhouettes and few other aliasing artifacts. The use of selective blurring and super-sampling to further improve image quality is also described. Examples from two applications are given: molecular graphics and medical imaging.
161|Footprint evaluation for volume rendering|This paper presents a forward mapping rendering algo-rithm to display regular volumetric grids that may not have the same spacings in the three grid directions. It takes advantage of the fact that convolution can be thought of as distributing energy from input samples into space. The renderer calculates an image plane footprint for each data sample and uses the footprint to spread the sample&#039;s energy onto the image plane. A result of the technique is that the forward mapping algorithm can support perspective without excessive cost, and support adaptive resampling of the three-dimensional data set during image generation.
162|Volume Rendering|A technique for rendering images Of volumes containing mixtures of materials is presented. The shading model allows both the interior of a material and the boundary between materials to be colored. Image projection is performed by simulating the absorption of light along the ray path to the eye. The algorithms used are designed to avoid artifacts caused by aliasing and quantization and can be efficiently implemented on an image computer. Images from a variety of applications are shown.
163|Efficient ray tracing of volume data|Volume rendering is a technique for visualizing sampled scalar or vector fields of three spatial dimensions without fitting geometric primitives to the data. A subset of these techniques generates images by computing 2-D projections of a colored semitransparent volume, where the color and opacity at each point are derived from the data using local operators. Since all voxels participate in the generation of each image, rendering time grows linearly with the size of the dataset. This paper presents a front-to-back image-order volume-rendering algorithm and discusses two techniques for improving its performance. The first technique employs a pyramid of binary volumes to encode spatial coherence present in the data, and the second technique uses an opacity threshold to adaptively terminate ray tracing. Although the actual time saved depends on the data, speedups of an order of magnitude have been observed for datasets of useful size and complexity. Examples from two applications are given: medical imaging and molecular graphics.
164|Radiosity and Realistic Image Synthesis|this paper, such as the global distribution of radiative energy in the tree crowns, which affects the amount of light reaching the leaves and the local temperature of plant organs. The presented framework itself is also open to further research. To begin, the precise functional specification of the environment, implied by the design of the modeling framework, is suitable for a formal analysis of algorithms that capture various environmental processes. This analysis may highlight tradeoffs between time, memory, and communication complexity, and lead to programs matching the needs of the model to available system resources in an optimal manner. A deeper understanding of the spectrum of processes taking place in the environment may lead to the design of a mini-language for environment specification. Analogous to the language of L-systems for plant specification, this mini-language would simplify the modeling of various environments, relieving the modeler from the burden of low-level programming in a general-purpose language. Fleischer and Barr&#039;s work on the specification of environments supporting collisions and reaction-diffusion processes [20] is an inspiring step in this direction. Complexity issues are not limited to the environment, but also arise in plant models. They become particularly relevant as the scope of modeling increases from individual plants to groups of plants and, eventually, entire plant communities. This raises the problem of selecting the proper level of abstraction for designing plant models, including careful selection of physiological processes incorporated into the model and the spatial resolution of the resulting structures. The complexity of the modeling task can be also addressed at the level of system design, by assigning various components o...
165|Octrees for faster isosurface generation| The large size of many volume data sets often prevents visualization algorithms from providing interactive rendering. The use of hierarchical data structures can ameliorate this problem by storing summary information to prevent useless exploration of regions of little or no current interest within the volume. This paper discusses research into the use of the octree hierarchical data structure when the regions of current interest can vary during the application, and are not known a priori. Octrees are well suited to the six-sided cell structure of many volumes. A new space-efficient design is introduced for octree representations of volumes whose resolutions are not conveniently a power of two; octrees following this design are called branch-on-need octrees (BONOs). Also, a caching method is described that essentially passes information between octree neighbors whose visitation times may be quite different, then discards it when its useful life is over. Using the application of octrees to isosurface generation as a focus, space and time comparisons for octree-based versus more traditional &#034;marching&#034; methods are presented.
167|Ray Tracing Volume Densities|This paper presents new algorithms to trace objects represented by densities within a volume grid, e.g. clouds, fog, flames, dust, particle systems. We develop the light scattering equations, discuss previous methods of solu-tion, and present a new approximate solution to the full three-dimensional radiative scattering problem suitable for use in computer graphics. Additionally we review dynamical models for clouds used to make an animated movie.
168|A Polygonal Approximation to Direct Scalar Volume Rendering|One method of directly rendering a three-dimensional volume of scalar data is to project each cell in a volume onto the screen. Rasterizing a volume cell is more complex than rasterizing a polygon. A method is presented that approximates tetrahedral volume cells with hardware renderable transparent triangles. This method produces results which are visually similar to more exact methods for scalar volume rendering, but is faster and has smaller memory requirements. The method is best suited for display of smoothlychanging data.  CR Categories and Subject Descriptors: I.3.0 [Computer Graphics]: General; I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling.  Additional Key Words and Phrases: Volume rendering, scientific visualization. 1 Introduction  Display of three-dimensional scalar volumes has recently become an active area of research. A scalar volume is described by some function f(x; y; z) defined over some region R of three-dimensional space. In many scientific ap...
169|Survey Of Texture Mapping|This paper appeared in IEEE Computer Graphics and Applications, Nov. 1986, pp. 56-67. An earlier version of thi aper appeared in Graphics Interface &#039;86, May 1986, pp. 207-212. This postscript version is missing all of the paste-up -
170|A language for shading and lighting calculations|A shading language provides a means to extend the shading and lighting formulae used by a rendering system. This paper discusses the design of a new shading language based on previous work of Cook and Perlin. This language has various types of shaders for light sources and surface reflectances, point and color data types, control flow constructs that support the casting of outgoing and the integration of incident light, a clearly specified interface to the rendering system using global state variables, and a host of useful built-in functions. The design issues and their impact on the implementation are also discussed. CR Categories: 1.3.3 [Computer Graphics] Picture/Image Generation- Display algorithms; 1.3.5 [Computer Graphics]
171|Fourier volume rendering|In computer graphics we have traditionally rendered images of data sets specified spatially, Here, we present a volume rendering technique that operates on a frequency domain representation of the data set and that efficiently generates line integral projections of the spatial data it represents, The motivation for this approach is that the Fourier Projection-Slice Theorem allows us to compute 2-D projections of 3-D data seta using only a 2-D slice of the data in the frequency domain. In general, these “X-ray-like ” images can be rendered at a significantly lower computational cost than images generated by current volume rendering techniques, Additionally, assurances of image accuracy can he made.
172|Fast Algorithms for Volume Ray Tracing|We examine various simple algorithms that exploit homogeneity and accumulated opacity for tracing rays through shaded volumes. Most of these methods have error criteria which allow them to trade quality for speed. The time vs. quality tradeoff for these adaptive methods is compared to fixed step multiresolution methods. These methods are also useful for general light transport in volumes. 1 Introduction  We are interested in speeding volume ray tracing computations. We concentrate on the one dimensional problem of tracing a single ray, or computing the intensity at a point from a single direction. In addition to being the kernel of a simple volume ray tracer, this computation can be used to generate shadow volumes and as an element in more general light transport problems. Our data structures will be view independent to speed the production of animations of preshaded volumes and interactive viewing. In [11] Levoy introduced two key concepts which we will be expanding on: presence accel...
173|MemSpy: Analyzing Memory System Bottlenecks in Programs|To cope with the increasing difference between processor and main memory speeds, modern computer systems use deep memory hierarchies. In the presence of such hierarchies, the performance attained by an application is largely determined by its memory reference behavior--- if most references hit in the cache, the performance is significantly higher than if most references have to go to main memory. Frequently, it is possible for the programmer to restructure the data or code to achieve better memory reference behavior. Unfortunately, most existing performance debugging tools do not assist the programmer in this component of the overall performance tuning task. This paper describes MemSpy, a prototype tool that helps programmers identify and fix memory bottlenecks in both sequential and parallel programs. A key aspect of MemSpy is that it introduces the notion of data oriented, in addition to code oriented, performance tuning. Thus, for both source level code objects and data objects, Mem...
174|The DASH Prototype: Logic Overhead and Performance|Abstract-The fundamental premise behind the DASH project is that it is feasible to build large-scale shared-memory multi-processors with hardware cache coherence. While paper studies and software simulators are useful for understanding many high-level design tradeoffs, prototypes are essential to ensure that no critical details are overlooked. A prototype provides convincing evidence of the feasibility of the design, allows one to accurately estimate both the hardware and the complexity cost of various features, and provides a platform for studying real workloads. A 48-processor prototype of the DASH multiprocessor is now operational. In this paper, we first examine the hardware overhead of directory-based cache coherence in the prototype. The data show that the overhead is only about M-15%, which appears to be a small cost for the ease of programming offered by coherent caches and the potential for higher performance. We then discuss the performance of the system and show the speedups obtained by a variety of parallel applications running on the prototype. Using a sophisticated hardware performance monitor, we also characterize the effectiveness of coherent caches and the relationship between an application’s reference behavior and its speedup. Finally, we present an evaluation of the optimizations incorporated in the DASH protocol in terms of their effectiveness on parallel applications and on atomic tests that stress the memory system.’ Index Terms- Directory-based cache coherence, implementa-tion cost, multiprocessor, parallel architecture, performance anal-
175|Feature-Based Volume Metamorphosis|Image metamorphosis, or image morphing, is a popular technique for creating a smooth transition between two images. For synthetic images, transforming and rendering the underlying three-dimensional (3D) models has a number of advantages over morphing between two pre-rendered images. In this paper we consider 3D metamorphosis applied to volume-based representations of objects. We discuss the issues which arise in volume morphing and present a method for creating morphs. Our morphing method has two components: first a warping of the two input volumes, then a blending of the resulting warped volumes. The warping component, an extension of Beier and Neely&#039;s image warping technique to 3D, is feature-based and allows fine user control, thus ensuring realistic looking intermediate objects. In addition, our warping method is amenable to an efficient approximation which gives a 50 times speedup and is computable to arbitrary accuracy. Also, our technique corrects the ghosting problem present in...
176|Volume Rendering on Scalable Shared-Memory MIMD Architectures|Volume rendering is a useful visualization technique for understanding the large amounts of data generated in a variety of scientific disciplines. Routine use of this technique is currently limited by its computational expense. We have designed a parallel volume rendering algorithm for MIMD architectures based on ray tracing and a novel task queue image partitioning technique. The combination of ray tracing and MIMD architectures allows us to employ algorithmic optimizations such as hierarchical opacity enumeration, early ray termination, and adaptive image sampling. The use of task queue image partitioning makes these optimizations efficient in a parallel framework. We have implemented our algorithm on the Stanford DASH Multiprocessor, a scalable shared-memory MIMD machine. Its single address-space and coherent caches provide programming ease and good performance for our algorithm. With only a few days of programming effort, we have obtained nearly linear speedups and near real-time frame update rates on a 48 processor machine. Since DASH is constructed from Silicon Graphics multiprocessors, our code runs on any Silicon Graphics workstation without modification.
177|Template-Based Volume Viewing|We present an efficient three-phase algorithm for volume viewing that is based on exploit- - t ing coherency between rays in parallel projection. The algorithm starts by building a ray emplate and determining a special plane for projection -- the base-plane. Parallel rays are cast t into the volume from within the projected region of the volume on the base-plane, by repeating he sequence of steps specified in the ray-template. We carefully choose the type of line to be s employed and the way the template is being placed on the base-plane in order to assure uniform ampling of the volume by the discrete rays. We conclude by describing an optimized software K  implementation of our algorithm and reporting its performance. eywords: volume rendering, ray casting, template, parallel projection 1. Introduction  Volume visualization is the process of converting complex volume data to a format that is p amenable to human understanding while maintaining the integrity and accuracy of the data. Th...
178|Volume Rendering by Adaptive Refinement|Volume rendering is a technique for visualizing sampled scalar functions of three spatial dimensions by computing 2D projections of a colored semi-transparent gel. This paper presents a volume rendering algorithm in which image quality is adaptively refined over time. An initial image is generated by casting a small number of rays into the data, less than one ray per pixel, and interpolating between the resulting colors. Subsequent images are generated by alternately casting more rays and interpolating. The usefulness of these rays is maximized by distributing them according to measures of local image complexity. Examples from two applications are given: molecular graphics and medical imaging. Key words: Volume rendering, voxel, adaptive refinement, adaptive sampling, ray tracing. 1. Introduction In this paper, we address the problem of visualizing sampled scalar functions of three spatial dimensions, henceforth referred to as volume data. We focus on a relatively new visualization tec...
179|Volume Rendering using the Fourier Projection-Slice Theorem|The Fourier projection-slice theorem states that the inverse transform of a slice extracted from the frequency domain representation of a volume yields a projection of the volume in a direction perpendicular to the slice. This theorem allows the generation of attenuation-only renderings of volume data in O (N  2  log N) time for a volume of size N  3  . In this paper, we show how more realistic renderings can be generated using a class of shading models whose terms are Fourier projections. Models are derived for rendering depth cueing by linear attenuation of variable energy emitters and for rendering directional shading by Lambertian reflection with hemispherical illumination. While the resulting images do not exhibit the occlusion that is characteristic of conventional volume rendering, they provide sufficient depth and shape cues to give a strong illusion that occlusion exists. Keywords: Volume rendering, Fourier projections, Shading models, Scientific visualization, Medical imaging...
180|A Data Distributed, Parallel Algorithm for Ray-Traced Volume Rendering|This paper presents a divide-and-conquer ray-traced volume rendering algorithm and a parallel image compositing method, along with their implementation and performance on the Connection Machine CM-5, and networked workstations. This algorithm distributes both the data and the computations to individual processing units to achieve fast, high-quality rendering of high-resolution data. The volume data, once distributed, is left intact. The processing nodes perform local raytracing of their subvolume concurrently. No communication between processing units is needed during this locally ray-tracing process. A subimage is generated by each processing unit and the #nal image is obtained by compositing subimages in the proper order, which can be determined a priori. Test results on both the CM-5 and a group of networked workstations demonstrate the practicality of our rendering algorithm and compositing method.  y  This researchwas supported in part by the National Aeronautics and Space Administration under NASA contract NAS1-19480 while the author was in residence at the Institute for Computer Application in Science and Engineering #ICASE#, NASA Langley Research Center, Hampton, VA 23681-0001.  i  1 
181|Parallel Volume Visualization on a Hypercube Architecture|A parallel solution to the visualisation of high resolution vol- ume data is presented. Based on the ray tracing (RT) visu- alization technique, the system works on a distributed memory MIMD architecture. A hybrid strategy to ray tracing parallelization is applied, using ray dataflow within an image partition approach. This strategy allows the flexible and effective management of huge dataset on architectures with limited local memory. The dataset is distributed over the nodes using a slice-partitioning technique. The simple data partition chosen implies a straighforward communications pattern of the visualization processes and this improves both software design and eJciency, while providing deadlock prevention. The partitioning technique used and the network interconnection topology allow for the efficient implementation of a statical load balancing technique through pre-rendering of a low resolution image. Details related to the practical issues involved in the parallelization of volumetric RT are discussed, with particular reference to deadlock and termi- nation issues.
182|Parallel Volume Rendering and Data Coherence|The two key issues in implementing a parallel ray-casting volume renderer are the work distribution and the data distribution. We have implemented such a renderer on the Fujitsu AP1000 using an adaptive image-space subdivision algorithm based on the worker-farm paradigm for the work distribution, and a distributed virtual memory, implemented in software, to provide the data distribution. Measurements show that this scheme works efficiently and effectively utilizes the data coherence that is inherent in volume data. Categories and Subject Descriptors: C.1.2 [Proces- sor Architectures]: Multiple Data Stream Architectures -- multiple-instruction-stream, multiple-data-stream (MIMD); I.3.1 [Computer Graphics]: Hardware Architecture -- parallel processing; I.3.7 [Computer Graphics]: ThreeDimensional Graphics and Realism -- ray tracing Key Words: Visualization, volume rendering, worker farm, image space, distributed virtual memory. 1 Introduction Volume rendering using ray-casting is a...
183|Cube-3: A Real-Time Architecture for High-Resolution Volume Visualization|This paper describes a high-performance special-purpose system, Cube-3, for displaying and manipulating high-resolution volumetric datasets in real-time. A primary goal of Cube-3 is to render 512³, 16-bit per voxel, datasets at about 30 frames per second. Cube-3 implements a ray-casting algorithm in a highly-parallel and pipelined architecture, using a 3D skewed volume memory, a modular fast bus, 2D skewed buffers, 3D interpolation and shading units, and a ray projection cone. Cube-3 will allow users to interactively visualize and investigate in real-time static (3D) and dynamic (4D) high-resolution volumetric datasets.
184|Transfer Equations in Global Illumination|The purpose of these notes is to describe some of the physical and mathematical properties of the equations occurring in global illumination. We first examine the physical assumptions that make the particle model of light an appropriate paradigm for computer graphics and then derive a balance equation for photons. In doing this we establish connections with the field of radiative transfer and its more abstract counterpart, transport theory. The resulting balance equation, known as the equation of transfer, accounts for large-scale interaction of light with participating media as well as complex reflecting surfaces. Under various simplifying assumptions the equation of transfer reduces to more conventional equations encountered in global illumination. 1 Introduction  Global illumination connotes a physically-based simulation of light appropriate for synthetic image generation. The task of such a simulation is to model the interplay of light among large-scale objects of an environment in...
185|A Lipschitz Method for Accelerated Volume Rendering|Interpolating discrete volume data into a continuous form adapts implicit surface techniques for rendering volumetric iso-surfaces. One such algorithm uses the Lipschitz condition to create an octree representation that accelerates volume rendering. Furthermore, only one preprocessing step is needed to create the Lipschitzoctree representation that accelerates rendering of isosurfaces for any threshold value.
186|Data Shaders|The process of visualizing a scientific data set requires an extensive knowledge of the domain in which the data set is created. Because an in-depth knowledge of all scientific domains is not available to the creator of visualization software, a flexible and extensible visualization system is essential in providing a productive tool to the scientist. This paper presents a shading language, based on the RenderMan shading language, that extends the shading model used to render volume data sets. Data shaders, written in this shading language, give the users of a volume rendering system a means of specifying how a volume data set is to be rendered. This flexibility is useful both as a visualization tool in the scientific community and as a research tool in the visualization community. 1 Introduction  As science is a diverse and far reaching topic, scientific visualization must be prepared to deal with diverse requirements when scientific data sets are examined, explored, and analyzed. In m...
187|Wait-Free Synchronization|A wait-free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. The problem of constructing a wait-free implementation of one data object from another lies at the heart of much recent work in concurrent algorithms, concurrent data structures, and multiprocessor architectures. In the first part of this paper, we introduce a simple and general technique, based on reduction to a consensus protocol, for proving statements of the form &#034;there is no wait-free implementation of  X by Y .&#034; We derive a hierarchy of objects such that no object at one level has a wait-free implementation in terms of objects at lower levels. In particular, we show that atomic read/write registers, which have been the focus of much recent attention, are at the bottom of the hierarchy: they cannot be used to construct wait-free implementations of many simple and familiar da...
188|Impossibility of Distributed Consensus with One Faulty Process|The consensus problem involves an asynchronous system of processes, some of which may be unreliable. The problem is for the reliable processes to agree on a binary value. We show &#039;that every protocol for this problem has the possibility of nontermination, even with only one faulty process. By way of contrast, solutions are known for the synchronous case, the &#034;Byzantine Generals&#034; problem.
190|On the Minimal Synchronism Needed for Distributed Consensus|Abstract. Reaching agreement is a primitive of distributed computing. Whereas this poses no problem in an ideal, failure-free environment, it imposes certain constraints on the capabilities of an actual system: A system is viable only if it permits the existence of consensus protocols tolerant to some number of failures. Fischer et al. have shown that in a completely asynchronous model, even one failure cannot be tolerated. In this paper their work is extended: Several critical system parameters, including various synchrony conditions, are identified and how varying these affects the number of faults that can be tolerated is examined. The proofs expose general heuristic principles that explain why consensus is possible in certain models but not possible in others.
191|Fast Randomized Consensus using Shared Memory|We give a new randomized algorithm for achieving consensus among  asynchronous processes that communicate by reading and writing shared  registers. The fastest previously known algorithm has exponential expected  running time. Our algorithm is polynomial, requiring an expected   O(n  4  ) operations. Applications of this algorithm include the  elimination of critical sections from concurrent data structures and the  construction of asymptotically unbiased shared coins.
192|Concurrent Reading and Writing|The problem of sharing data among asynchronous processes is considered. It is assumed that only one process at a time can modify the data, but concurrent reading and writing is permitted. Two general theorems are proved, and some algorithms are presented to illustrate their use. These include a solution to the general problem in which a read is repeated if it might have obtained an incorrect result, and two techniques for transmitting messages between processes. These solutions do not assume any synchronizing mechanism other than data which can be written by one process and read by other processes. 
193|Atomic ·shared register access by asynchronous hardware|The contribution of this paper is two-fold. First, we describe two ways to construct multivalued atomic n-writer n-reader registers. The first solution uses atomic 1-writer 1-reader registers and unbounded tags. The other solution uses atomic 1-writer n-reader registers and bounded tags. The second part of the paper develops a general methodology to prove atomicity, by identifying a set of criteria which guaranty an effective construction for the required atomic mapping. We apply the method to prove atomicity of the two implementations for atomic multiwriter multireader registers. 1.
194|Axioms for concurrent objects|The copyright law of the United States (title 17, U.S. Code) governs the making of photocopies or other reproductions of copyrighted material. Any copying of this document without permission of its author may be prohibited by law.
195|Basic Techniques for the Efficient Coordination of Very Large Numbers of Cooperating Sequential Processors|In this paper we implement several basic operating system primitives by using a &#034;replace-add&#034; operation, which can supersede the standard &#034;test and set&#034;, and which appears to be a universal primitive for efficiently coordinating large numbers of independently acting sequential processors. We also present a hardware implementation of replace-add that permits multiple replace-adds to be processed nearly as efficiently as loads and stores. Moreover, the crucial special case of concurrent replace-adds updating the same variable is handled particularly well: If every PE simultaneously addresses a replace-add at the same variable, all these requests are satisfied in the time required to process just one request.
196|Impossibility and universality results for wait-free synchronization|Impossibility and universality results for wait-free synchronization
197|Constructing Two-Writer Atomic Registers|In this paper, we construct a 2-writer, n-reader atomic memory register from two l-writer, (n + l)-reader atomic memory registers. There are no restrictions on the size of the constructed register. The simulation requires only a single extra bit per real register, and can survive the failure of any set of readers and writers. This construction is a part of a systematic investigation of register simulations, by several researchers.  
198|The Virtue of Patience: Concurrent Programming with and without Waiting|We consider the implementation of atomic operations that either write several  shared variables, or that both read and write shared variables. We show that, in  general, such operations cannot be implemented in a wMt-free manner using atomic  registers.
199|Serverless Network File Systems|In this paper, we propose a new paradigm for network file system design, serverless network file systems. While traditional network file systems rely on a central server machine, a serverless system utilizes workstations cooperating as peers to provide all file system services. Any machine in the system can store, cache, or control any block of data. Our approach uses this location independence, in combination with fast local area networks, to provide better performance and scalability than traditional file systems. Further, because any machine in the system can assume the responsibilities of a failed component, our serverless design also provides high availability via redundant data storage. To demonstrate our approach, we have implemented a prototype serverless network file system called xFS. Preliminary performance measurements suggest that our architecture achieves its goal of scalability. For instance, in a 32-node xFS system with 32 active clients, each client receives nearly as much read or write throughput as it would see if it were the only active client. 
200|A cost-effective, high-bandwidth storage architecture|(NASD) storage architecture, prototype implementations oj NASD drives, array management for our architecture, and three,filesystems built on our prototype. NASD provides scal-able storage bandwidth without the cost of servers used primarily,fijr trut&amp;rring data from peripheral networks (e.g. SCSI) to client networks (e.g. ethernet). Increasing datuset sizes, new attachment technologies, the convergence of peripheral and interprocessor switched networks, and the increased availability of on-drive transistors motivate and enable this new architecture. NASD is based on four main principles: direct transfer to clients, secure interfaces via cryptographic support, asynchronous non-critical-path oversight, and variably-sized data objects. Measurements of our prototype system show that these services can be cost-#ectively integrated into a next generation disk drive ASK. End-to-end measurements of our prototype drive andfilesys-terns suggest that NASD cun support conventional distrib-uted filesystems without per$ormance degradation. More importantly, we show scaluble bandwidth for NASD-special-ized filesystems. Using a parallel data mining application, NASD drives deliver u linear scaling of 6.2 MB/s per client-drive pair, tested with up to eight pairs in our lab.
202|Cluster I/O with River: Making the Fast Case Common|We introduce River, a data-flow programming environment and I/O substrate for clusters of computers. River is designed to provide maximum performance in the common case --- even in the face of nonuniformities in hardware, software, and workload. River is based on two simple design features: a high-performance distributed queue, and a storage redundancy mechanism called graduated declustering. We have implemented a number of data-intensive applications on River, which validate our design with near-ideal performance in a variety of non-uniform performance scenarios. 
203|Swift: Using distributed disk striping to provide high I/O data rates|We present an I/O architecture, called Swift, that addresses the problem of data rate mismatches between the requirements of an application, storage devices, and the interconnection medium. The goal of Swift is to support high data rates in general purpose distributed systems. Swift uses a high-speed interconnection medium to provide high data rate transfers by using multiple slower storage devices in parallel. It scales well when using multiple storage devices and interconnections, and can use any appropriate storage technology, including high-performance devices such as disk arrays. To address the problem of partial failures, Swift stores data redundantly. Using the UNIX operating system, we have constructed a simplified prototype of the Swift architecture. The prototype provides data rates that are significantly faster than access to the local SCSI disk, limited by the capacity of a single Ethernet segment, or in the case of multiple Ethernet segments by the ability of the client to drive them. We have constructed a simulation model to demonstrate how the Swift architecture can exploit advances in processor, communication and storage technology. We consider the effects of processor speed, interconnection capacity, and multiple storage agents on the utilization of the components and the data rate of the system. We show that the data rates scale well in the number of storage devices, and that by replacing the most highly stressed components by more powerful ones the data rates of the entire system increase significantly.
204|The Global File System|The Global File System (GFS) is a prototype design for a distributed file system in which cluster nodes physically share storage devices connected via a network like Fibre Channel. Networks and network attached storage devices have advanced to a level of performance and extensibility that the once believed disadvantages of “shared disk ” architectures are no longer valid. This shared storage architecture attempts to exploit the sophistication of device technologies where as the client–server architecture diminishes a device’s role to a simple components. GFS distributes the file system responsibilities across the processing nodes, storage across the devices, and file system resources across the entire storage pool. GFS caches data on the storage devices instead of the main memories of the machines. Consistency is established by using a locking mechanism maintained by the storage device controllers to facilitate atomic read–modify– write operations. The locking mechanism is being prototyped on Seagate disks drives and Ciprico disk arrays. GFS is implemented in the Silicon Graphics IRIX operating system and is accessed using standard Unix commands and utilities.
205|Satisfaction and Comparison Income|This paper is an attempt to test the hypothesis that utility depends on income relative to a &#039;comparison&#039; or reference level. Using data on 5,000 British workers, it provides two findings. First, workers&#039; reported satisfaction levels are shown to be inversely related to their comparison wage rates. Second, holding income constant, satisfaction levels are shown to be strongly declining in the level of education. More generally, the paper tries to help begin the task of constructing an economics of job satisfaction.
206|Job satisfaction as an economic variable|I have benefited from discussions with Robert Pogel,
207|Economics in the Laboratory|Why do economists conduct experiments? To answer that question, it is first necessary briefly to specify the ingredients of an experiment. Every laboratory experiment is defined by an environment, specifying the initial endowments, preferences and costs that motivate exchange. This environment is controlled using monetary rewards to induce the desired specific value/cost configuration (Smith, 1991, 6). 1 An experiment also uses an institution defining the language (messages) of market communication (bids, offers, acceptances), the rules that govern the exchange of information, and the rules under which messages become binding contracts. This institution is defined by the experimental instructions which describe the messages and procedures of the market, which are most often computer controlled. Finally, there is the observed behavior of the participants in the experiments as a function of the environment and institution that constitute the controlled variables. Using this framework of environment, institution, and behavior, I can think of at least seven prominent reasons in the literature as to why economists conduct experiments. Undoubtedly, there are more (Davis and Holt, 1992, chapter 1 and passim).
